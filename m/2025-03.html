
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 588 papers. March 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Март 2025</span> | <span id="title-articles-count">588 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-02.html">⬅️ <span id="prev-date">02.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-04.html">➡️ <span id="next-date">04.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Март 2025', 'en': 'March 2025', 'zh': '3月2025年'};
        let feedDateNext = {'ru': '04.2025', 'en': '04/2025', 'zh': '4月2025年'};
        let feedDatePrev = {'ru': '02.2025', 'en': '02/2025', 'zh': '2月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.19693', 'title': 'AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through\n  Lightweight Vocabulary Adaptation', 'url': 'https://huggingface.co/papers/2503.19693', 'abstract': 'Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance', 'score': 31, 'issue_id': 2976, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '2c907e98a0aafb46', 'authors': ['Itay Nakash', 'Nitay Calderon', 'Eyal Ben David', 'Elad Hoffer', 'Roi Reichart'], 'affiliations': ['Habana Labs', 'The Faculty of Data and Decisions Science, Technion - IIT'], 'pdf_title_img': 'assets/pdf/title_img/2503.19693.jpg', 'data': {'categories': ['#architecture', '#low_resource', '#training', '#optimization', '#data', '#transfer_learning'], 'emoji': '🗜️', 'ru': {'title': 'Адаптация словаря для повышения эффективности LLM в специализированных областях', 'desc': 'AdaptiVocab - это новый подход к адаптации словаря больших языковых моделей (LLM) для повышения их эффективности в узкоспециализированных областях. Метод заменяет токены в словаре на специфичные для домена n-граммы, что сокращает количество токенов, необходимых для обработки входных данных и генерации выходных. AdaptiVocab инициализирует новые вложения n-токенов с помощью экспоненциально взвешенной комбинации существующих вложений и использует легковесную фазу дообучения. Эксперименты показали, что AdaptiVocab уменьшает использование токенов более чем на 25% без ухудшения производительности.'}, 'en': {'title': 'Enhancing LLM Efficiency with Domain-Specific Vocabulary', 'desc': 'This paper presents AdaptiVocab, a method for improving the efficiency of Large Language Models (LLMs) in specific domains by adapting their vocabulary. Instead of using a general-purpose vocabulary, AdaptiVocab replaces tokens with domain-specific n-gram-based tokens, which reduces the number of tokens needed for processing and generation. The approach involves initializing new embeddings through a combination of existing ones and includes a lightweight fine-tuning process that can be done on a single GPU. The results demonstrate that AdaptiVocab can decrease token usage by over 25% while maintaining the quality of generated outputs and overall task performance.'}, 'zh': {'title': '提高大型语言模型效率的新方法', 'desc': '大型语言模型（LLMs）在通用模型中展现了令人印象深刻的多功能性，但其广泛应用伴随着高昂的计算开销，尤其是在自回归解码中，每一步都需要进行前向传播。针对特定领域的应用，通用能力并非必要，可以通过提高效率来进行交换。我们提出了一种新颖的领域适应方法——AdaptiVocab，通过调整词汇表来降低延迟和计算成本，旨在提高LLM在低资源领域的效率。AdaptiVocab可以应用于任何分词器和架构，通过用特定领域的n-gram代替原有的tokens，减少输入处理和输出生成所需的tokens数量。'}}}, {'id': 'https://huggingface.co/papers/2503.22230', 'title': 'Exploring Data Scaling Trends and Effects in Reinforcement Learning from\n  Human Feedback', 'url': 'https://huggingface.co/papers/2503.22230', 'abstract': "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.", 'score': 21, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': 'a994668c51ac51c4', 'authors': ['Wei Shen', 'Guanlin Liu', 'Zheng Wu', 'Ruofei Zhu', 'Qingping Yang', 'Chao Xin', 'Yu Yue', 'Lin Yan'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2503.22230.jpg', 'data': {'categories': ['#rlhf', '#data', '#reasoning', '#alignment', '#training'], 'emoji': '🧠', 'ru': {'title': 'Усовершенствование RLHF: данные и разнообразие ответов как ключ к успеху', 'desc': 'Статья исследует важность конструирования данных в обучении с подкреплением на основе обратной связи от человека (RLHF) для больших языковых моделей. Авторы предлагают гибридную систему вознаграждений, сочетающую верификаторы задач рассуждения (RTV) и генеративную модель вознаграждений (GenRM), для смягчения проблемы обмана вознаграждений. Они также вводят метод Pre-PPO для поддержания разнообразия ответов и повышения эффективности обучения. Результаты показывают, что предложенные методы значительно улучшают производительность RLHF, особенно в математических задачах и задачах кодирования.'}, 'en': {'title': 'Enhancing RLHF: Bridging Data Gaps for Better AI Alignment', 'desc': 'This paper focuses on improving Reinforcement Learning from Human Feedback (RLHF) for large language models by addressing issues in prompt-data construction. It identifies problems like reward hacking and reduced response diversity that hinder RLHF performance. The authors propose a hybrid reward system that combines reasoning task verifiers (RTV) with a generative reward model (GenRM) to counteract these issues. Additionally, they introduce a new prompt-selection method called Pre-PPO and emphasize the importance of prioritizing mathematical and coding tasks during training to enhance overall model performance.'}, 'zh': {'title': '优化人类反馈的强化学习方法', 'desc': '强化学习中的人类反馈（RLHF）对于使大型语言模型与人类偏好对齐至关重要。尽管近期研究集中在算法改进上，但提示数据构建的重要性却被忽视。本文探讨了RLHF性能扩展中的数据驱动瓶颈，特别是奖励黑客和响应多样性下降的问题。我们提出了一种混合奖励系统，结合推理任务验证器（RTV）和生成奖励模型（GenRM），以减轻奖励黑客现象，并提出了一种新颖的提示选择方法Pre-PPO，以保持响应多样性并增强学习效果。'}}}, {'id': 'https://huggingface.co/papers/2503.22675', 'title': 'Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation', 'url': 'https://huggingface.co/papers/2503.22675', 'abstract': "Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose ReaRec, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\\%-50\\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.", 'score': 20, 'issue_id': 2971, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '092ab2fa75277891', 'authors': ['Jiakai Tang', 'Sunhao Dai', 'Teng Shi', 'Jun Xu', 'Xu Chen', 'Wen Chen', 'Wu Jian', 'Yuning Jiang'], 'affiliations': ['Alibaba Group, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.22675.jpg', 'data': {'categories': ['#inference', '#reasoning', '#dataset', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Многошаговое рассуждение для улучшения рекомендаций', 'desc': 'ReaRec - это новый фреймворк для систем рекомендаций, использующий многошаговое рассуждение во время вывода для улучшения представления пользователей. Он применяет авторегрессивную подачу последнего скрытого состояния последовательности в рекомендательную систему, используя специальные эмбеддинги позиций рассуждения. Авторы также предлагают два метода обучения на основе рассуждений: Ensemble Reasoning Learning (ERL) и Progressive Reasoning Learning (PRL). Эксперименты показывают, что ReaRec значительно повышает эффективность различных архитектур последовательных рекомендательных систем.'}, 'en': {'title': 'ReaRec: Elevating Sequential Recommendations with Multi-Step Reasoning', 'desc': 'This paper introduces ReaRec, a novel framework for Sequential Recommendation (SeqRec) that enhances user representation through multi-step reasoning. Traditional methods often rely on a single forward computation, which limits their ability to capture the evolving nature of user preferences and understand less popular items. ReaRec addresses these limitations by using autoregressive techniques and special embeddings to improve the inference process. The proposed framework, along with two learning methods, shows significant performance improvements across various datasets, suggesting a new direction for research in recommendation systems.'}, 'zh': {'title': 'ReaRec：提升顺序推荐的推理能力', 'desc': '顺序推荐（SeqRec）旨在通过捕捉用户历史交互中的顺序模式来预测下一个项目，这在许多现实世界的推荐系统中起着关键作用。现有方法主要采用直接的前向计算范式，最终的隐藏状态作为用户表示，但这种方法在建模用户偏好的复杂演变方面存在局限。为了解决这个问题，我们提出了ReaRec，这是第一个用于推荐系统的推理时计算框架，通过隐式多步推理增强用户表示。我们的实验表明，ReaRec显著提高了多个顺序推荐模型的性能，开辟了推理时计算的新研究方向。'}}}, {'id': 'https://huggingface.co/papers/2503.21614', 'title': 'A Survey of Efficient Reasoning for Large Reasoning Models: Language,\n  Multimodality, and Beyond', 'url': 'https://huggingface.co/papers/2503.21614', 'abstract': 'Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.', 'score': 16, 'issue_id': 2976, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '805b7cd4ec307c34', 'authors': ['Xiaoye Qu', 'Yafu Li', 'Zhaochen Su', 'Weigao Sun', 'Jianhao Yan', 'Dongrui Liu', 'Ganqu Cui', 'Daizong Liu', 'Shuxian Liang', 'Junxian He', 'Peng Li', 'Wei Wei', 'Jing Shao', 'Chaochao Lu', 'Yue Zhang', 'Xian-Sheng Hua', 'Bowen Zhou', 'Yu Cheng'], 'affiliations': ['Huazhong University of Science and Technology', 'Peking University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'Tongji University', 'Tsinghua University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21614.jpg', 'data': {'categories': ['#reasoning', '#inference', '#training', '#survey', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация рассуждений в крупных языковых моделях', 'desc': 'Этот обзор посвящен повышению эффективности рассуждений в крупных моделях рассуждений (LRM). Авторы рассматривают проблему избыточно длинных цепочек рассуждений, которые часто содержат повторяющийся контент и излишний анализ. Обсуждаются методы улучшения эффективности на всех этапах жизненного цикла LRM - от предварительного обучения до вывода. Статья также предлагает перспективные направления для будущих исследований в этой быстро развивающейся области.'}, 'en': {'title': 'Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses the challenges faced by Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI o1, particularly their tendency to generate long and redundant reasoning processes. These inefficiencies can complicate training and real-world applications, especially where efficient use of tokens is crucial. The authors review various strategies aimed at enhancing reasoning efficiency throughout the LRM lifecycle, from pretraining to inference. They also provide a GitHub repository to track advancements in this field, aiming to inspire further research and innovation.'}, 'zh': {'title': '提升推理效率，助力大型模型发展', 'desc': '最近的大型推理模型（LRMs），如DeepSeek-R1和OpenAI o1，通过扩展推理链（CoT）的长度在推理过程中取得了显著的性能提升。然而，它们往往生成过长的推理过程，内容冗余（例如，重复定义）、对简单问题的过度分析，以及对复杂任务的多条推理路径的表面探索。这种低效性在训练、推理和实际应用中引发了重大挑战，尤其是在代理系统中，令牌经济至关重要。本文综述了改善LRMs推理效率的最新努力，识别了常见的低效模式，并探讨了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2503.22194', 'title': 'ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2503.22194', 'abstract': 'We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.', 'score': 14, 'issue_id': 2971, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '6d93cc886c16f9b0', 'authors': ['Yunhong Min', 'Daehyeon Choi', 'Kyeongmin Yeo', 'Jihyun Lee', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.22194.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#3d'], 'emoji': '🧭', 'ru': {'title': 'Точное управление 3D ориентацией объектов при генерации изображений', 'desc': 'ORIGEN - это первый метод для определения 3D ориентации объектов при генерации изображений из текста без предварительного обучения. Он использует предобученную дискриминативную модель для оценки 3D ориентации и одношаговую генеративную модель текст-изображение. Вместо градиентного спуска применяется семплирование на основе динамики Ланжевена с добавлением случайного шума. Эксперименты показывают превосходство ORIGEN над другими методами по количественным метрикам и оценкам пользователей.'}, 'en': {'title': 'Revolutionizing 3D Orientation in Text-to-Image Generation with ORIGEN', 'desc': 'ORIGEN is a novel method that enables zero-shot 3D orientation grounding in text-to-image generation, allowing for better control over how objects are oriented in three-dimensional space. Unlike previous methods that focused on 2D positioning, ORIGEN utilizes a reward-guided sampling approach that leverages a pretrained model for estimating 3D orientations. This method incorporates Langevin dynamics to enhance image realism while maintaining effective sampling, requiring minimal code changes. Experimental results demonstrate that ORIGEN surpasses existing training-based and test-time guidance techniques in both quantitative metrics and user evaluations.'}, 'zh': {'title': 'ORIGEN：文本到图像生成中的3D方向定位新方法', 'desc': '我们介绍了ORIGEN，这是首个用于文本到图像生成中实现3D方向定位的零样本方法。以往的研究主要集中在2D定位上，缺乏对3D方向的控制。为了解决这个问题，我们提出了一种基于奖励引导的采样方法，利用预训练的判别模型进行3D方向估计，并结合一步文本到图像生成流模型。实验结果表明，ORIGEN在定量指标和用户研究中均优于基于训练和测试时引导的方法。'}}}, {'id': 'https://huggingface.co/papers/2503.20785', 'title': 'Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal\n  Consistency', 'url': 'https://huggingface.co/papers/2503.20785', 'abstract': 'We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multiview videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation.', 'score': 11, 'issue_id': 2974, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '1a3973da9e51afd7', 'authors': ['Tianqi Liu', 'Zihao Huang', 'Zhaoxi Chen', 'Guangcong Wang', 'Shoukang Hu', 'Liao Shen', 'Huiqiang Sun', 'Zhiguo Cao', 'Wei Li', 'Ziwei Liu'], 'affiliations': ['Great Bay University', 'Huazhong University of Science and Technology', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20785.jpg', 'data': {'categories': ['#3d', '#multimodal', '#video'], 'emoji': '🎬', 'ru': {'title': 'От одного кадра к полноценной 4D-сцене', 'desc': 'Free4D - это новый подход к генерации 4D-сцен из одного изображения без дополнительного обучения. Метод использует предобученные фундаментальные модели для создания согласованного 4D-представления сцены. Free4D сначала анимирует входное изображение с помощью диффузионных моделей, затем применяет адаптивное руководство для пространственно-временной согласованности и модуляционное уточнение для устранения несоответствий. Результатом является 4D-представление, позволяющее рендерить сцену в реальном времени с возможностью управления.'}, 'en': {'title': 'Revolutionizing 4D Scene Generation from a Single Image', 'desc': 'Free4D is a new framework that generates 4D scenes from just one image without needing extensive tuning. Unlike previous methods that either focus on individual objects or require large datasets, Free4D uses pre-trained models to create consistent 4D representations efficiently. It employs image-to-video diffusion models to animate the input image and then refines the generated structure for spatial and temporal consistency. This approach allows for real-time rendering of 4D scenes, making it a significant step forward in scene generation technology.'}, 'zh': {'title': '从单图像生成4D场景的新突破', 'desc': '我们提出了Free4D，这是一个无需调优的框架，可以从单张图像生成4D场景。现有方法通常专注于物体级生成，导致场景级生成不可行，或者依赖于大规模多视角视频数据集进行昂贵的训练，且由于4D场景数据稀缺，泛化能力有限。我们的关键见解是提炼预训练的基础模型，以实现一致的4D场景表示，这提供了效率和泛化能力的优势。通过图像到视频的扩散模型，我们首先对输入图像进行动画处理，然后初始化4D几何结构，最终实现实时、可控的4D场景生成。'}}}, {'id': 'https://huggingface.co/papers/2503.21821', 'title': 'PHYSICS: Benchmarking Foundation Models on University-Level Physics\n  Problem Solving', 'url': 'https://huggingface.co/papers/2503.21821', 'abstract': 'We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.', 'score': 10, 'issue_id': 2972, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '5254bbde255c8669', 'authors': ['Kaiyue Feng', 'Yilun Zhao', 'Yixin Liu', 'Tianyu Yang', 'Chen Zhao', 'John Sous', 'Arman Cohan'], 'affiliations': ['New York University', 'Notre Dame University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21821.jpg', 'data': {'categories': ['#reasoning', '#rag', '#benchmark', '#science'], 'emoji': '🔬', 'ru': {'title': 'Новый вызов для ИИ: решение сложных задач по физике', 'desc': 'PHYSICS - это новый набор данных для оценки способностей моделей машинного обучения решать задачи по физике университетского уровня. Он включает 1297 экспертно размеченных задач по шести основным разделам физики. Даже самые продвинутые языковые модели показывают ограниченную точность на этом наборе данных. Авторы провели анализ ошибок и исследовали различные стратегии промптинга и дополнения знаний с помощью RAG для определения путей улучшения моделей.'}, 'en': {'title': 'Benchmarking Physics Problem Solving with PHYSICS', 'desc': 'The paper presents PHYSICS, a benchmark designed to assess university-level physics problem solving capabilities. It includes 1297 expert-annotated problems across six fundamental physics domains, requiring both advanced knowledge and mathematical skills. The authors introduce an automated evaluation system to ensure accurate validation of model performance. Their findings reveal that even the top-performing model, o3-mini, only achieves 59.9% accuracy, indicating significant room for improvement in tackling complex scientific challenges.'}, 'zh': {'title': '物理问题解决的新基准', 'desc': '我们介绍了PHYSICS，这是一个全面的大学物理问题解决基准。它包含1297个专家注释的问题，涵盖经典力学、量子力学、热力学与统计力学、电磁学、原子物理和光学六个核心领域。每个问题都需要高级物理知识和数学推理能力。我们的评估显示，当前最先进的模型o3-mini的准确率仅为59.9%，这突显了解决高水平科学问题的重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.22236', 'title': 'Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal\n  Bridging', 'url': 'https://huggingface.co/papers/2503.22236', 'abstract': 'With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.', 'score': 9, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '5024619189472d8c', 'authors': ['Chongjie Ye', 'Yushuang Wu', 'Ziteng Lu', 'Jiahao Chang', 'Xiaoyang Guo', 'Jiaqing Zhou', 'Hao Zhao', 'Xiaoguang Han'], 'affiliations': ['ByteDance', 'The Chinese University of Hong Kong, Shenzhen', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22236.jpg', 'data': {'categories': ['#3d'], 'emoji': '🖼️', 'ru': {'title': 'От плоского к объемному: революция в 3D-моделировании', 'desc': 'Hi3DGen - это новый подход к созданию высококачественных 3D-моделей из 2D-изображений с использованием нормалей в качестве промежуточного представления. Метод состоит из трех ключевых компонентов: оценщика нормалей из изображений, генератора геометрии из нормалей на основе латентной диффузии, и пайплайна для синтеза качественных 3D-данных. Hi3DGen превосходит современные методы в точности воспроизведения мелких геометрических деталей. Этот подход открывает новое направление в генерации высококачественной 3D-геометрии из изображений.'}, 'en': {'title': 'Bridging Normals for High-Fidelity 3D Generation', 'desc': 'The paper introduces Hi3DGen, a new framework designed to create high-fidelity 3D models from 2D images. It tackles challenges like domain gaps and ambiguities in RGB images by using a method called normal bridging. Hi3DGen includes an image-to-normal estimator that improves the accuracy of geometric details through noise injection and dual-stream training. Additionally, it employs a normal-to-geometry learning approach and a 3D data synthesis pipeline to enhance the quality of the generated 3D models, showing superior performance compared to existing methods.'}, 'zh': {'title': '高保真3D几何体生成的新方向', 'desc': '随着对从2D图像生成高保真3D模型的需求增加，现有方法在准确再现细致几何细节方面仍面临重大挑战。为了解决这些问题，我们提出了Hi3DGen，一个通过法线桥接生成高保真3D几何体的新框架。Hi3DGen包含三个关键组件：图像到法线估计器、法线到几何体学习方法和3D数据合成管道。我们的实验表明，该框架在生成丰富几何细节方面的有效性和优越性，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.22268', 'title': 'Segment Any Motion in Videos', 'url': 'https://huggingface.co/papers/2503.22268', 'abstract': 'Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.', 'score': 8, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '084606e82bff72ff', 'authors': ['Nan Huang', 'Wenzhao Zheng', 'Chenfeng Xu', 'Kurt Keutzer', 'Shanghang Zhang', 'Angjoo Kanazawa', 'Qianqian Wang'], 'affiliations': ['Peking University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.22268.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': '🎥', 'ru': {'title': 'Улучшенная сегментация движущихся объектов с помощью траекторного внимания и семантических признаков', 'desc': 'Эта статья представляет новый подход к сегментации движущихся объектов в видео. Метод объединяет траекторные признаки движения с семантическими признаками DINO и использует SAM2 для уточнения масок на уровне пикселей. Модель применяет пространственно-временное внимание к траекториям и раздельное кодирование движения и семантики. Результаты тестирования на различных наборах данных показывают превосходную производительность, особенно в сложных сценариях.'}, 'en': {'title': 'Revolutionizing Moving Object Segmentation with Motion-Semantic Integration', 'desc': 'This paper presents a new method for moving object segmentation in videos, which is essential for understanding visual scenes. The authors address the limitations of traditional optical flow techniques that struggle with issues like motion blur and background distractions. Their approach combines long-range motion cues with semantic features from a DINO model and uses an iterative strategy with SAM2 for detailed pixel-level segmentation. The proposed model shows superior performance on various datasets, particularly in complex scenarios involving multiple moving objects.'}, 'zh': {'title': '创新移动物体分割方法，提升视觉理解能力', 'desc': '移动物体分割是理解视觉场景的重要任务，具有广泛的应用。传统方法主要依赖光流来提供运动线索，但在处理部分运动、复杂变形、运动模糊和背景干扰时常常效果不佳。我们提出了一种新方法，结合长距离轨迹运动线索与基于DINO的语义特征，并通过迭代提示策略利用SAM2进行像素级掩膜细化。我们的模型采用时空轨迹注意力和运动-语义解耦嵌入，优先考虑运动，同时整合语义支持，经过广泛测试在多样化数据集上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.20308', 'title': 'Perceptually Accurate 3D Talking Head Generation: New Definitions,\n  Speech-Mesh Representation, and Evaluation Metrics', 'url': 'https://huggingface.co/papers/2503.20308', 'abstract': 'Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria -- Temporal Synchronization, Lip Readability, and Expressiveness -- are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/.', 'score': 8, 'issue_id': 2979, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '1b5ec45782117fb1', 'authors': ['Lee Chae-Yeon', 'Oh Hyun-Bin', 'Han EunGi', 'Kim Sung-Bin', 'Suekyeong Nam', 'Tae-Hyun Oh'], 'affiliations': ['Dept. of Electrical Engineering, POSTECH', 'Grad. School of AI, POSTECH', 'KRAFTON', 'School of Computing, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.20308.jpg', 'data': {'categories': ['#3d', '#audio', '#training'], 'emoji': '🗣️', 'ru': {'title': 'Новое представление для реалистичной синхронизации речи и движений губ в 3D', 'desc': 'Эта статья представляет новый подход к генерации трехмерных говорящих голов, управляемых речью. Авторы вводят синхронизированное представление речи и трехмерной сетки лица, которое улучшает соответствие движений губ характеристикам речи. Это представление используется как в качестве перцептивной функции потерь при обучении моделей, так и в качестве метрики для оценки качества синхронизации губ. Эксперименты показывают значительное улучшение по трем ключевым критериям: временной синхронизации, читаемости по губам и выразительности.'}, 'en': {'title': 'Enhancing Lip Synchronization in 3D Talking Heads', 'desc': 'This paper addresses the challenges in generating 3D talking heads that accurately synchronize lip movements with speech. The authors propose that achieving Temporal Synchronization, Lip Readability, and Expressiveness is essential for realistic lip movements. They introduce a novel speech-mesh synchronized representation that effectively captures the relationship between speech signals and 3D facial movements. By integrating this representation as a perceptual loss in existing models, they demonstrate significant improvements in lip synchronization quality through experimental validation.'}, 'zh': {'title': '提升3D说话头唇部同步的关键标准', 'desc': '本研究探讨了语音驱动的3D说话头生成中的唇部同步问题。我们提出了三个关键标准：时间同步、唇部可读性和表现力，以实现感知上准确的唇部运动。为此，我们引入了一种语音-网格同步表示，能够捕捉语音信号与3D面部网格之间的复杂对应关系。实验结果表明，使用我们的感知损失训练3D说话头生成模型显著提高了唇部同步的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.17827', 'title': '4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object\n  Understanding', 'url': 'https://huggingface.co/papers/2503.17827', 'abstract': 'Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities. However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects (3D objects with temporal evolution over time). In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks. With 4D-Bench, we evaluate a wide range of open-source and closed-source MLLMs. The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding. 4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human baseline of 91\\%. These findings highlight a substantial gap in 4D object understanding and the need for further advancements in MLLMs.', 'score': 6, 'issue_id': 2978, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 марта', 'en': 'March 22', 'zh': '3月22日'}, 'hash': '4c510d164c81f13e', 'authors': ['Wenxuan Zhu', 'Bing Li', 'Cheng Zheng', 'Jinjie Mai', 'Jun Chen', 'Letian Jiang', 'Abdullah Hamdi', 'Sara Rojas Martinez', 'Chia-Wen Lin', 'Mohamed Elhoseiny', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology', 'National Tsing Hua University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.17827.jpg', 'data': {'categories': ['#benchmark', '#3d', '#games', '#video', '#multimodal', '#open_source'], 'emoji': '🕰️', 'ru': {'title': '4D-Bench: раскрывая ограничения MLLM в понимании пространственно-временных объектов', 'desc': '4D-Bench - это новый эталонный тест для оценки способностей мультимодальных больших языковых моделей (MLLM) в понимании 4D-объектов. Он включает задачи по ответам на вопросы о 4D-объектах и их описанию, предоставляя разнообразные категории объектов и высококачественные аннотации. Результаты показывают, что MLLM хуже понимают временные аспекты по сравнению с визуальными, причем открытые модели значительно отстают от закрытых в понимании временных характеристик. Даже лучшие модели, такие как GPT-4o, демонстрируют низкую точность в задачах с 4D-объектами по сравнению с человеческим уровнем.'}, 'en': {'title': 'Bridging the Gap in 4D Object Understanding for MLLMs', 'desc': 'This paper introduces 4D-Bench, a new benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their understanding of 4D objects, which are 3D objects that change over time. The benchmark includes tasks such as 4D object Question Answering and 4D object captioning, focusing on multi-view spatial-temporal understanding. The evaluation reveals that MLLMs struggle with temporal understanding, showing significant performance gaps compared to their ability to understand object appearance. Overall, the findings indicate a critical need for improvements in MLLMs to enhance their capabilities in 4D object comprehension.'}, 'zh': {'title': '四维物体理解的新基准：4D-Bench', 'desc': '多模态大型语言模型（MLLMs）在二维图像和视频理解方面表现出色，但缺乏评估四维物体理解能力的标准基准。本文提出了4D-Bench，这是第一个用于评估MLLMs在四维物体理解能力的基准，包含四维物体问答和四维物体描述等任务。4D-Bench提供了多样化的四维物体类别和高质量的注释，要求模型具备多视角的时空理解能力。实验结果显示，MLLMs在时间理解方面普遍较弱，尤其是开源模型在外观理解上接近闭源模型，但在时间理解上存在较大差距。'}}}, {'id': 'https://huggingface.co/papers/2503.22329', 'title': 'A Refined Analysis of Massive Activations in LLMs', 'url': 'https://huggingface.co/papers/2503.22329', 'abstract': 'Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.', 'score': 5, 'issue_id': 2972, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': '025b5484847cd3d9', 'authors': ['Louis Owen', 'Nilabhra Roy Chowdhury', 'Abhay Kumar', 'Fabian Güra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2503.22329.jpg', 'data': {'categories': ['#optimization', '#architecture', '#long_context', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый взгляд на массивные активации в LLM: не все так однозначно', 'desc': 'Эта статья посвящена исследованию массивных активаций в больших языковых моделях (LLM). Авторы провели анализ различных архитектур LLM, включая модели на основе GLU и без него. Результаты опровергают некоторые предыдущие предположения, показывая, что не все массивные активации вредны, а существующие стратегии смягчения могут быть неэффективны в определенных случаях. Исследователи предлагают новые гибридные стратегии, сочетающие Target Variance Rescaling с Attention KV bias или Dynamic Tanh, для эффективного смягчения массивных активаций при сохранении производительности модели.'}, 'en': {'title': 'Balancing Act: Mitigating Massive Activations in LLMs', 'desc': 'This paper explores the phenomenon of massive activations in large language models (LLMs) and their implications for low-precision training and quantization. The authors analyze a variety of LLM architectures to understand the effects of massive activations, revealing that not all of them negatively impact model performance. They challenge previous assumptions by demonstrating that suppressing massive activations does not necessarily lead to worse outcomes in downstream tasks. Additionally, the paper introduces new hybrid strategies that effectively mitigate massive activations while maintaining model performance, particularly through the combination of Target Variance Rescaling and other techniques.'}, 'zh': {'title': '大激活的挑战与新策略', 'desc': '本文研究了大语言模型（LLMs）中的大激活现象，特别关注其在低精度训练和量化中的重要性。我们分析了多种LLM架构，包括基于GLU和非GLU的模型，发现并非所有大激活都是有害的，抑制它们并不会导致困惑度的爆炸或下游任务性能的崩溃。我们还发现，现有的缓解策略如注意力KV偏置在某些情况下是模型特定的且无效。因此，我们提出了新的混合缓解策略，结合目标方差重标定（TVR）与注意力KV偏置或动态Tanh（DyT），成功平衡了大激活的缓解与下游模型性能的保持。'}}}, {'id': 'https://huggingface.co/papers/2503.21732', 'title': 'SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling', 'url': 'https://huggingface.co/papers/2503.21732', 'abstract': 'Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.', 'score': 4, 'issue_id': 2972, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'f17ea311cc796683', 'authors': ['Xianglong He', 'Zi-Xin Zou', 'Chia-Hao Chen', 'Yuan-Chen Guo', 'Ding Liang', 'Chun Yuan', 'Wanli Ouyang', 'Yan-Pei Cao', 'Yangguang Li'], 'affiliations': ['The Chinese University of Hong Kong', 'Tsinghua University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2503.21732.jpg', 'data': {'categories': ['#3d'], 'emoji': '🧊', 'ru': {'title': 'Революция в 3D-моделировании: высокое разрешение и произвольная топология', 'desc': 'SparseFlex - это новый метод представления изоповерхностей, позволяющий выполнять дифференцируемую реконструкцию 3D-моделей с разрешением до 1024^3 непосредственно на основе потерь рендеринга. Он сочетает точность Flexicubes с разреженной воксельной структурой, фокусируясь на областях, прилегающих к поверхности. Введена стратегия обучения на основе секционных вокселей с учетом фрустума, что значительно снижает потребление памяти. SparseFlex также позволяет реконструировать внутренние части сетки, используя только рендеринг для обучения.'}, 'en': {'title': 'SparseFlex: Revolutionizing 3D Mesh Reconstruction with Efficiency and Detail', 'desc': 'This paper presents SparseFlex, a new method for creating detailed 3D meshes with complex shapes and open surfaces. It uses a sparse voxel structure to focus on areas near the surface, allowing for efficient high-resolution mesh reconstruction directly from rendering losses. The method introduces a frustum-aware training strategy that reduces memory usage by activating only necessary voxels during rendering. SparseFlex achieves impressive results, outperforming previous techniques in accuracy and enabling the generation of intricate 3D shapes with varying topologies.'}, 'zh': {'title': 'SparseFlex：高分辨率3D网格重建的新突破', 'desc': '本文介绍了一种名为SparseFlex的新型稀疏结构等值面表示方法，旨在解决高保真3D网格重建中的挑战。SparseFlex能够直接从渲染损失中进行可微分的网格重建，支持高达1024^3的分辨率。通过结合Flexicubes的准确性和稀疏体素结构，该方法有效处理开放表面，并引入了基于视锥的分段体素训练策略，显著降低了内存消耗。实验结果表明，SparseFlex在重建精度上达到了最先进的水平，成功生成了具有任意拓扑的高分辨率、细节丰富的3D形状。'}}}, {'id': 'https://huggingface.co/papers/2503.19108', 'title': 'Your ViT is Secretly an Image Segmentation Model', 'url': 'https://huggingface.co/papers/2503.19108', 'abstract': 'Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: https://www.tue-mps.org/eomt/.', 'score': 4, 'issue_id': 2980, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '96e8b32d843c2265', 'authors': ['Tommie Kerssies', 'Niccolò Cavagnero', 'Alexander Hermans', 'Narges Norouzi', 'Giuseppe Averta', 'Bastian Leibe', 'Gijs Dubbelman', 'Daan de Geus'], 'affiliations': ['Eindhoven University of Technology', 'Polytechnic of Turin', 'RWTH Aachen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19108.jpg', 'data': {'categories': ['#cv', '#architecture', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Простота и эффективность: ViT для сегментации изображений без лишних компонентов', 'desc': 'Статья представляет новый подход к сегментации изображений с использованием Vision Transformers (ViT). Авторы предлагают архитектуру Encoder-only Mask Transformer (EoMT), которая адаптирует стандартную модель ViT для задачи сегментации без добавления специфичных компонентов. EoMT демонстрирует точность сегментации, сравнимую с современными методами, но работает значительно быстрее благодаря своей простоте. Исследование показывает, что при достаточно большом масштабе модели и предобучении, ViT способен самостоятельно изучить необходимые индуктивные смещения.'}, 'en': {'title': 'Streamlining Image Segmentation with EoMT: Faster and Simpler ViTs', 'desc': "This paper presents the Encoder-only Mask Transformer (EoMT), a novel approach for image segmentation using Vision Transformers (ViTs). Instead of relying on additional components like convolutional adapters and pixel decoders, EoMT leverages the ViT's ability to learn necessary features directly from data. The authors demonstrate that with large models and extensive pre-training, EoMT achieves competitive segmentation accuracy while being significantly faster than traditional methods. This suggests that focusing on scaling the ViT architecture can yield better performance than adding complex task-specific elements."}, 'zh': {'title': '简化架构，提升分割效率', 'desc': '视觉变换器（ViTs）在各种计算机视觉任务中表现出色。本文提出了一种新的模型，称为仅编码器掩码变换器（EoMT），它利用简单的ViT架构进行图像分割，而无需额外的任务特定组件。研究表明，ViT可以通过大规模模型和充分的预训练来学习任务所需的偏置。EoMT在分割精度和预测速度之间实现了最佳平衡，且速度比传统方法快，最高可达4倍。'}}}, {'id': 'https://huggingface.co/papers/2503.21332', 'title': 'ReFeed: Multi-dimensional Summarization Refinement with Reflective\n  Reasoning on Feedback', 'url': 'https://huggingface.co/papers/2503.21332', 'abstract': 'Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released.', 'score': 3, 'issue_id': 2977, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '4797a440ca75521b', 'authors': ['Taewon Yun', 'Jihwan Oh', 'Hyangsuk Min', 'Yuho Lee', 'Jihwan Bang', 'Jason Cai', 'Hwanjun Song'], 'affiliations': ['Amazon Web Services, AI Labs', 'Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2503.21332.jpg', 'data': {'categories': ['#dataset', '#long_context', '#training', '#data', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'Улучшение многомерной суммаризации через рефлексивное рассуждение', 'desc': 'В этой статье представлен ReFeed - мощный конвейер для улучшения суммаризации по нескольким параметрам с использованием рефлексивного рассуждения на основе обратной связи. Авторы создали датасет SumFeed-CoT для обучения легковесной модели рефлексивному рассуждению. Эксперименты показали важность одновременной обработки нескольких видов обратной связи и устойчивость ReFeed к шуму. Исследование подчеркивает значимость правильно сформулированной цели и руководства при создании данных для эффективного рассуждения.'}, 'en': {'title': 'Enhancing Summarization with Reflective Reasoning', 'desc': 'This paper presents ReFeed, a novel summarization refinement pipeline that improves the quality of summaries across multiple dimensions by utilizing reflective reasoning on feedback. The authors introduce a new dataset called SumFeed-CoT, which is designed to train a lightweight model capable of handling reflective reasoning effectively. Through experiments, they demonstrate that the performance of the summarization refinement is significantly influenced by the number of dimensions, the exposure to feedback, and the reasoning policy employed. The findings suggest that addressing multiple feedback sources simultaneously is essential for optimizing performance while also being resilient to noisy feedback and variations in feedback order.'}, 'zh': {'title': '反思性推理提升多维总结精炼', 'desc': '本文介绍了一种名为ReFeed的总结精炼管道，旨在通过反思性推理来增强多维度的总结效果。我们发布了一个名为SumFeed-CoT的大规模数据集，专门用于训练轻量级模型，以实现反思性推理。实验结果表明，维度数量、反馈暴露和推理策略对精炼性能有显著影响，强调了反思性推理和同时处理多个反馈的重要性。最后，我们的研究发现，创建具有明确目标和指导的数据是有效推理的基础。'}}}, {'id': 'https://huggingface.co/papers/2503.18968', 'title': 'MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via\n  Reasoning Agentic Workflow', 'url': 'https://huggingface.co/papers/2503.18968', 'abstract': 'Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.', 'score': 3, 'issue_id': 2973, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '662e73dea1e23d07', 'authors': ['Ziyue Wang', 'Junde Wu', 'Chang Han Low', 'Yueming Jin'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.18968.jpg', 'data': {'categories': ['#hallucinations', '#3d', '#multimodal', '#reasoning', '#interpretability', '#healthcare', '#agents'], 'emoji': '🩺', 'ru': {'title': 'MedAgent-Pro: надежная мультимодальная диагностика с ИИ', 'desc': 'Статья представляет MedAgent-Pro - агентную систему для надежной и объяснимой медицинской диагностики на основе мультимодальных больших языковых моделей (MLLM). Система решает проблемы прямого применения MLLM в медицине, такие как ограниченное восприятие визуальных данных и склонность к галлюцинациям. MedAgent-Pro использует иерархический рабочий процесс с рассуждениями на основе знаний и множеством инструментальных агентов для обработки мультимодальных входных данных. Эксперименты показывают превосходство и эффективность MedAgent-Pro в задачах 2D и 3D медицинской диагностики.'}, 'en': {'title': 'MedAgent-Pro: Reliable AI for Accurate Medical Diagnosis', 'desc': 'This paper introduces MedAgent-Pro, a new AI system designed to improve medical diagnosis by combining multi-modal inputs and evidence-based reasoning. It addresses the limitations of existing Multi-modal Large Language Models (MLLMs), which struggle with visual data and often produce unreliable outputs. MedAgent-Pro uses a hierarchical approach, where knowledge-based reasoning creates diagnostic plans and multiple tool agents analyze various indicators to provide accurate diagnoses. Experiments show that MedAgent-Pro outperforms traditional methods in both 2D and 3D medical tasks, demonstrating its reliability and interpretability in clinical settings.'}, 'zh': {'title': '提升医疗诊断的可靠性与可解释性', 'desc': '本论文提出了一种名为MedAgent-Pro的系统，旨在提高多模态医疗诊断的可靠性和准确性。该系统结合了基于知识的推理和多工具代理，能够处理多种输入并生成可靠的诊断计划。通过对2D和3D医疗诊断任务的全面实验，MedAgent-Pro展示了其优越性和有效性。该系统不仅提供了精确的诊断，还具备良好的可解释性，适合临床应用。'}}}, {'id': 'https://huggingface.co/papers/2503.21779', 'title': 'X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time\n  Tomographic Reconstruction', 'url': 'https://huggingface.co/papers/2503.21779', 'abstract': 'Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.', 'score': 2, 'issue_id': 2976, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '628d77c9c1bdcd9e', 'authors': ['Weihao Yu', 'Yuanhao Cai', 'Ruyi Zha', 'Zhiwen Fan', 'Chenxin Li', 'Yixuan Yuan'], 'affiliations': ['Johns Hopkins University', 'The Australian National University', 'The Chinese University of Hong Kong', 'University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.21779.jpg', 'data': {'categories': ['#architecture', '#healthcare', '#cv'], 'emoji': '🫁', 'ru': {'title': 'Непрерывная 4D-КТ реконструкция без внешней синхронизации', 'desc': 'Эта статья представляет X^2-Gaussian - новый метод для непрерывной реконструкции 4D-КТ изображений без использования внешних устройств синхронизации дыхания. Авторы объединяют динамическое рассеивание гауссовых функций с самоконтролируемым обучением респираторных движений. Метод использует пространственно-временную архитектуру кодировщика-декодировщика для моделирования анатомической динамики через предсказание деформаций гауссовых функций во времени. X^2-Gaussian превосходит традиционные методы на 9.93 дБ по метрике PSNR, открывая новые возможности для динамической клинической визуализации.'}, 'en': {'title': 'Revolutionizing 4D CT with Continuous Motion Modeling', 'desc': 'This paper presents X^2-Gaussian, a new method for 4D CT reconstruction that overcomes the limitations of traditional phase-binning techniques. By using a spatiotemporal encoder-decoder architecture, it models dynamic anatomical changes without relying on fixed phases, thus allowing for continuous-time reconstruction. The method incorporates self-supervised learning to understand patient-specific breathing patterns, eliminating the need for external respiratory gating devices. Experimental results show that X^2-Gaussian significantly improves image quality, achieving higher PSNR compared to existing methods.'}, 'zh': {'title': 'X^2-Gaussian：无硬件的高保真4D CT重建新方法', 'desc': '四维计算机断层扫描（4D CT）重建对于捕捉动态解剖变化至关重要，但传统的相位分箱工作流程存在固有的局限性。现有方法将时间分辨率离散化为固定相位，使用呼吸门控设备，导致运动错位并限制临床实用性。本文提出了一种新框架X^2-Gaussian，通过结合动态辐射高斯点云和自监督呼吸运动学习，实现连续时间的4D CT重建。我们的模型通过时空编码器-解码器架构预测时间变化的高斯变形，消除了相位离散化的需求，并引入生理驱动的周期一致性损失，直接从投影中学习患者特定的呼吸周期。'}}}, {'id': 'https://huggingface.co/papers/2503.21851', 'title': 'On Large Multimodal Models as Open-World Image Classifiers', 'url': 'https://huggingface.co/papers/2503.21851', 'abstract': 'Traditional image classification requires a predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt "What is the main object in the image?"). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming a closed-world setting with a predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in a truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, fine-grained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them.', 'score': 2, 'issue_id': 2977, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '1f5e6c94324fef3f', 'authors': ['Alessandro Conti', 'Massimiliano Mancini', 'Enrico Fini', 'Yiming Wang', 'Paolo Rota', 'Elisa Ricci'], 'affiliations': ['Fondazione Bruno Kessler', 'Independent researcher', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.21851.jpg', 'data': {'categories': ['#reasoning', '#alignment', '#benchmark', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Открытый взгляд на классификацию изображений с помощью мультимодальных моделей', 'desc': 'Статья исследует производительность больших мультимодальных моделей (LMM) в задаче классификации изображений в условиях открытого мира. Авторы предлагают новый протокол оценки и метрики для измерения соответствия между предсказанными и истинными классами. Проводится оценка 13 моделей на 10 наборах данных, включающих прототипические, непрототипические и детальные классы. Анализ выявляет трудности, с которыми сталкиваются LMM, особенно в отношении гранулярности и способности различать тонкие детали.'}, 'en': {'title': 'Unlocking Open-World Image Classification with LMMs', 'desc': 'This paper explores the capabilities of Large Multimodal Models (LMMs) in image classification without relying on a fixed set of categories. It introduces a new evaluation framework to assess LMM performance in an open-world context, where images can belong to any category. The study evaluates 13 different models across various benchmarks, revealing the difficulties LMMs encounter, particularly with fine-grained classifications. Additionally, it discusses how improved prompting and reasoning techniques can help mitigate these challenges.'}, 'zh': {'title': '大型多模态模型在开放世界中的图像分类挑战', 'desc': '传统的图像分类需要预定义的语义类别，而大型多模态模型（LMMs）可以通过自然语言直接对图像进行分类。这项研究评估了LMM在开放世界环境中的分类性能，填补了现有研究的空白。我们引入了一种评估协议，定义了多种指标来评估预测类别与真实类别之间的对齐情况。通过对13个模型在10个基准上的评估，揭示了LMM在细粒度分类中的挑战，并提出了定制提示和推理的方法来缓解这些问题。'}}}, {'id': 'https://huggingface.co/papers/2503.21751', 'title': 'Reconstructing Humans with a Biomechanically Accurate Skeleton', 'url': 'https://huggingface.co/papers/2503.21751', 'abstract': 'In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/', 'score': 0, 'issue_id': 2979, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'a715b9755e020ce7', 'authors': ['Yan Xia', 'Xiaowei Zhou', 'Etienne Vouga', 'Qixing Huang', 'Georgios Pavlakos'], 'affiliations': ['The University of Texas at Austin', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21751.jpg', 'data': {'categories': ['#benchmark', '#3d', '#architecture', '#dataset'], 'emoji': '🦴', 'ru': {'title': 'Биомеханически точная 3D-реконструкция человека по одному изображению', 'desc': 'В этой статье представлен метод реконструкции 3D-моделей людей по одному изображению с использованием биомеханически точной модели скелета. Авторы обучают трансформер, который принимает изображение на вход и оценивает параметры модели. Из-за нехватки обучающих данных разработан конвейер для создания псевдо-разметки параметров модели по одиночным изображениям. По сравнению с современными методами восстановления 3D-сетки человека, предложенный подход показывает конкурентоспособные результаты на стандартных тестах и значительно превосходит их в сценариях с экстремальными 3D-позами и ракурсами.'}, 'en': {'title': 'Reconstructing Realistic 3D Humans from Single Images', 'desc': 'This paper presents a novel method for creating 3D models of humans from just one image by using a skeleton model that accurately reflects human biomechanics. The authors train a transformer model that processes the input image to predict the parameters of this skeleton. To address the challenge of limited training data, they develop a pipeline that generates pseudo ground truth parameters, which are refined through an iterative training process. Their approach not only matches the performance of existing 3D human reconstruction methods but also excels in challenging scenarios with extreme poses and viewpoints, while ensuring realistic joint movements.'}, 'zh': {'title': '生物力学驱动的3D人类重建新方法', 'desc': '本文提出了一种从单张图像重建3D人类模型的方法，使用生物力学准确的骨骼模型。我们训练了一个变换器，输入图像并估计模型参数。由于训练数据的不足，我们建立了一个管道来生成单张图像的伪真实模型参数，并实施了一个迭代优化伪标签的训练过程。与现有的3D人类网格恢复方法相比，我们的模型在标准基准测试中表现出竞争力，尤其在极端3D姿势和视角下表现更为优越。'}}}, {'id': 'https://huggingface.co/papers/2503.01785', 'title': 'Visual-RFT: Visual Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2503.01785', 'abstract': "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1 learns from feedback on its answers, which is especially useful in applications when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1 demonstrates that reinforcement learning with verifiable reward is one key direction in reproducing o1. While the R1-style model has demonstrated success in language models, its application in multi-modal domains remains under-explored. This work introduces Visual Reinforcement Fine-Tuning (Visual-RFT), which further extends the application areas of RFT on visual tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs) to generate multiple responses containing reasoning tokens and final answers for each input, and then uses our proposed visual perception verifiable reward functions to update the model via the policy optimization algorithm such as Group Relative Policy Optimization (GRPO). We design different verifiable reward functions for different perception tasks, such as the Intersection over Union (IoU) reward for object detection. Experimental results on fine-grained image classification, few-shot object detection, reasoning grounding, as well as open-vocabulary object detection benchmarks show the competitive performance and advanced generalization ability of Visual-RFT compared with Supervised Fine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over the baseline in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline by 21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks.", 'score': 43, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'ef2e10eb59ab7743', 'authors': ['Ziyu Liu', 'Zeyi Sun', 'Yuhang Zang', 'Xiaoyi Dong', 'Yuhang Cao', 'Haodong Duan', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.01785.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#cv', '#optimization', '#rlhf', '#reasoning', '#training', '#rl'], 'emoji': '🔬', 'ru': {'title': 'Visual-RFT: Революция в тонкой настройке визуально-языковых моделей', 'desc': 'Статья представляет Visual Reinforcement Fine-Tuning (Visual-RFT) - метод, расширяющий применение обучения с подкреплением в визуальных задачах. Visual-RFT использует большие визуально-языковые модели для генерации ответов с токенами рассуждений и применяет визуально верифицируемые функции вознаграждения для обновления модели. Эксперименты показывают превосходство Visual-RFT над методом Supervised Fine-tuning в задачах классификации изображений, обнаружения объектов и обоснованного заземления. Метод демонстрирует значительное улучшение точности и обобщающей способности при ограниченном количестве обучающих примеров.'}, 'en': {'title': 'Revolutionizing Visual Learning with Reinforcement Fine-Tuning', 'desc': "This paper introduces Visual Reinforcement Fine-Tuning (Visual-RFT), a method that enhances large vision-language models (LVLMs) by using reinforcement learning to improve their performance on visual tasks. Visual-RFT generates multiple responses for each input and employs verifiable reward functions to optimize the model's policy, making it particularly effective in scenarios with limited fine-tuning data. The approach demonstrates significant improvements in tasks like fine-grained image classification and object detection, outperforming traditional supervised fine-tuning methods. Overall, Visual-RFT represents a novel, efficient way to fine-tune LVLMs, focusing on reasoning and adaptability in specific domains."}, 'zh': {'title': '视觉强化微调：提升推理与适应性的创新方法', 'desc': '强化微调（RFT）在大型推理模型中通过反馈学习，特别适用于微调数据稀缺的应用场景。本文提出的视觉强化微调（Visual-RFT）扩展了RFT在视觉任务中的应用，利用大型视觉语言模型生成多种响应，并通过可验证的视觉感知奖励函数进行模型更新。实验结果表明，Visual-RFT在细粒度图像分类和少样本目标检测等任务中表现出色，相较于传统的监督微调（SFT）方法，准确率显著提高。Visual-RFT代表了一种新的微调范式，提供了一种数据高效、以奖励驱动的方法，增强了模型在特定领域任务中的推理能力和适应性。'}}}, {'id': 'https://huggingface.co/papers/2503.01743', 'title': 'Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs', 'url': 'https://huggingface.co/papers/2503.01743', 'abstract': 'We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.', 'score': 38, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'fb054d6547a4a4fb', 'authors': ['Abdelrahman Abouelenin', 'Atabak Ashfaq', 'Adam Atkinson', 'Hany Awadalla', 'Nguyen Bach', 'Jianmin Bao', 'Alon Benhaim', 'Martin Cai', 'Vishrav Chaudhary', 'Congcong Chen', 'Dong Chen', 'Dongdong Chen', 'Junkun Chen', 'Weizhu Chen', 'Yen-Chun Chen', 'Yi-ling Chen', 'Qi Dai', 'Xiyang Dai', 'Ruchao Fan', 'Mei Gao', 'Min Gao', 'Amit Garg', 'Abhishek Goswami', 'Junheng Hao', 'Amr Hendy', 'Yuxuan Hu', 'Xin Jin', 'Mahmoud Khademi', 'Dongwoo Kim', 'Young Jin Kim', 'Gina Lee', 'Jinyu Li', 'Yunsheng Li', 'Chen Liang', 'Xihui Lin', 'Zeqi Lin', 'Mengchen Liu', 'Yang Liu', 'Gilsinia Lopez', 'Chong Luo', 'Piyush Madan', 'Vadim Mazalov', 'Ali Mousavi', 'Anh Nguyen', 'Jing Pan', 'Daniel Perez-Becker', 'Jacob Platin', 'Thomas Portet', 'Kai Qiu', 'Bo Ren', 'Liliang Ren', 'Sambuddha Roy', 'Ning Shang', 'Yelong Shen', 'Saksham Singhal', 'Subhojit Som', 'Xia Song', 'Tetyana Sych', 'Praneetha Vaddamanu', 'Shuohang Wang', 'Yiming Wang', 'Zhenghao Wang', 'Haibin Wu', 'Haoran Xu', 'Weijian Xu', 'Yifan Yang', 'Ziyi Yang', 'Donghan Yu', 'Ishmam Zabir', 'Jianwen Zhang', 'Li Lyna Zhang', 'Yunan Zhang', 'Xiren Zhou'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.01743.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#data', '#agi', '#synthetic', '#long_context', '#optimization', '#dataset', '#training'], 'emoji': '🧠', 'ru': {'title': 'Компактные модели с большими возможностями: прорыв в эффективности ИИ', 'desc': 'Представлены две новые модели: Phi-4-Mini и Phi-4-Multimodal. Phi-4-Mini - это языковая модель с 3,8 миллиардами параметров, обученная на высококачественных веб-данных и синтетических данных, которая превосходит аналогичные модели в задачах математики и программирования. Phi-4-Multimodal - это мультимодальная модель, объединяющая текст, изображения и речь/аудио в единую систему с использованием LoRA-адаптеров. Обе модели демонстрируют высокую эффективность несмотря на свой компактный размер, превосходя более крупные аналоги в различных задачах.'}, 'en': {'title': 'Compact Models, Superior Performance!', 'desc': 'The paper presents Phi-4-Mini and Phi-4-Multimodal, two advanced models designed for language and multimodal tasks. Phi-4-Mini, with 3.8 billion parameters, excels in math and coding tasks by utilizing a high-quality synthetic data approach and an expanded vocabulary of 200K tokens. Phi-4-Multimodal integrates text, vision, and audio inputs, employing innovative techniques like LoRA adapters for efficient multi-modal processing. Both models demonstrate superior performance compared to larger counterparts, showcasing their effectiveness in complex reasoning and diverse input scenarios.'}, 'zh': {'title': '紧凑强大的多模态模型Phi-4系列', 'desc': '我们介绍了Phi-4-Mini和Phi-4-Multimodal这两种紧凑而强大的语言和多模态模型。Phi-4-Mini是一个拥有38亿参数的语言模型，经过高质量的网络和合成数据训练，在数学和编码任务中表现优于同类开源模型，并且在复杂推理方面与两倍于其规模的模型相当。相比于前身Phi-3.5-Mini，Phi-4-Mini扩展了词汇量，支持多语言应用，并采用了组查询注意力机制以提高长序列生成的效率。Phi-4-Multimodal则是一个多模态模型，能够将文本、视觉和语音/音频输入整合到一个模型中，支持多种推理模式，且在多个任务上超越了更大的视觉-语言和语音-语言模型。'}}}, {'id': 'https://huggingface.co/papers/2503.01774', 'title': 'Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models', 'url': 'https://huggingface.co/papers/2503.01774', 'abstract': 'Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D reconstruction and novel-view synthesis task. However, achieving photorealistic rendering from extreme novel viewpoints remains challenging, as artifacts persist across representations. In this work, we introduce Difix3D+, a novel pipeline designed to enhance 3D reconstruction and novel-view synthesis through single-step diffusion models. At the core of our approach is Difix, a single-step image diffusion model trained to enhance and remove artifacts in rendered novel views caused by underconstrained regions of the 3D representation. Difix serves two critical roles in our pipeline. First, it is used during the reconstruction phase to clean up pseudo-training views that are rendered from the reconstruction and then distilled back into 3D. This greatly enhances underconstrained regions and improves the overall 3D representation quality. More importantly, Difix also acts as a neural enhancer during inference, effectively removing residual artifacts arising from imperfect 3D supervision and the limited capacity of current reconstruction models. Difix3D+ is a general solution, a single model compatible with both NeRF and 3DGS representations, and it achieves an average 2times improvement in FID score over baselines while maintaining 3D consistency.', 'score': 29, 'issue_id': 2512, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '39af2f882aef9afb', 'authors': ['Jay Zhangjie Wu', 'Yuxuan Zhang', 'Haithem Turki', 'Xuanchi Ren', 'Jun Gao', 'Mike Zheng Shou', 'Sanja Fidler', 'Zan Gojcic', 'Huan Ling'], 'affiliations': ['NVIDIA', 'National University of Singapore', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01774.jpg', 'data': {'categories': ['#3d', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Одношаговая диффузия для фотореалистичной 3D-реконструкции', 'desc': 'Difix3D+ - это новый подход к улучшению 3D-реконструкции и синтеза изображений с новых ракурсов. В его основе лежит Difix - одношаговая модель диффузии изображений, обученная улучшать и устранять артефакты в визуализированных видах. Difix используется как на этапе реконструкции для очистки псевдо-обучающих видов, так и во время вывода для устранения остаточных артефактов. Difix3D+ совместим с представлениями NeRF и 3DGS и показывает двукратное улучшение оценки FID по сравнению с базовыми моделями.'}, 'en': {'title': 'Enhancing 3D Reconstruction with Difix3D+', 'desc': 'This paper presents Difix3D+, a new method for improving 3D reconstruction and novel-view synthesis using single-step diffusion models. The core component, Difix, is an image diffusion model that enhances rendered views by removing artifacts caused by underconstrained areas in 3D representations. It plays a dual role by cleaning up pseudo-training views during reconstruction and acting as a neural enhancer during inference to eliminate residual artifacts. Difix3D+ is versatile, working with both Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), and it significantly improves the quality of 3D representations, achieving a 2x better FID score compared to existing methods.'}, 'zh': {'title': 'Difix3D+: 提升3D重建与新视角合成的利器', 'desc': 'Neural Radiance Fields（NeRF）和3D高斯点云（3D Gaussian Splatting）在3D重建和新视角合成任务中取得了重大进展。然而，从极端新视角实现真实感渲染仍然面临挑战，因为在表示中存在伪影。我们提出了Difix3D+，这是一种新颖的管道，旨在通过单步扩散模型增强3D重建和新视角合成。Difix作为核心模型，能够在重建阶段清理伪训练视图，并在推理阶段去除残留伪影，从而显著提高3D表示的质量。'}}}, {'id': 'https://huggingface.co/papers/2503.01183', 'title': 'DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion', 'url': 'https://huggingface.co/papers/2503.01183', 'abstract': 'Recent advancements in music generation have garnered significant attention, yet existing approaches face critical limitations. Some current generative models can only synthesize either the vocal track or the accompaniment track. While some models can generate combined vocal and accompaniment, they typically rely on meticulously designed multi-stage cascading architectures and intricate data pipelines, hindering scalability. Additionally, most systems are restricted to generating short musical segments rather than full-length songs. Furthermore, widely used language model-based methods suffer from slow inference speeds. To address these challenges, we propose DiffRhythm, the first latent diffusion-based song generation model capable of synthesizing complete songs with both vocal and accompaniment for durations of up to 4m45s in only ten seconds, maintaining high musicality and intelligibility. Despite its remarkable capabilities, DiffRhythm is designed to be simple and elegant: it eliminates the need for complex data preparation, employs a straightforward model structure, and requires only lyrics and a style prompt during inference. Additionally, its non-autoregressive structure ensures fast inference speeds. This simplicity guarantees the scalability of DiffRhythm. Moreover, we release the complete training code along with the pre-trained model on large-scale data to promote reproducibility and further research.', 'score': 18, 'issue_id': 2516, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '0370c6364610fd8e', 'authors': ['Ziqian Ning', 'Huakang Chen', 'Yuepeng Jiang', 'Chunbo Hao', 'Guobin Ma', 'Shuai Wang', 'Jixun Yao', 'Lei Xie'], 'affiliations': ['Northwestern Polytechnical University', 'Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen), China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01183.jpg', 'data': {'categories': ['#diffusion', '#inference', '#dataset', '#open_source', '#audio'], 'emoji': '🎵', 'ru': {'title': 'DiffRhythm: Быстрая генерация полных песен с помощью латентной диффузии', 'desc': 'DiffRhythm - это первая модель генерации песен на основе латентной диффузии, способная синтезировать полные песни с вокалом и аккомпанементом длительностью до 4м45с всего за десять секунд. Модель имеет простую структуру, не требует сложной подготовки данных и использует только текст песни и стилевую подсказку при инференсе. Благодаря неавторегрессивной структуре, DiffRhythm обеспечивает высокую скорость генерации. Авторы опубликовали полный код обучения и предобученную модель для воспроизводимости результатов и дальнейших исследований.'}, 'en': {'title': 'DiffRhythm: Fast and Scalable Song Generation with Latent Diffusion', 'desc': "This paper introduces DiffRhythm, a novel music generation model that utilizes latent diffusion techniques to create full-length songs with both vocal and accompaniment tracks. Unlike existing models that are limited to short segments or require complex architectures, DiffRhythm simplifies the process by needing only lyrics and a style prompt for song generation. It achieves high musical quality and intelligibility while significantly improving inference speed, generating songs in just ten seconds. The authors also emphasize the model's scalability and reproducibility by providing the complete training code and pre-trained model for further research."}, 'zh': {'title': 'DiffRhythm：快速生成完整歌曲的创新模型', 'desc': '本论文介绍了一种新的音乐生成模型DiffRhythm，它能够在短短十秒内合成完整的歌曲，包括人声和伴奏，时长可达4分45秒。与现有模型相比，DiffRhythm采用潜在扩散技术，避免了复杂的数据准备和多阶段架构，确保了高效的推理速度。该模型只需歌词和风格提示即可生成音乐，具有良好的可扩展性。我们还发布了完整的训练代码和预训练模型，以促进研究的可重复性和进一步发展。'}}}, {'id': 'https://huggingface.co/papers/2502.18965', 'title': 'OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment', 'url': 'https://huggingface.co/papers/2502.18965', 'abstract': "Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\\% increase in watch-time, which is a substantial improvement.", 'score': 18, 'issue_id': 2515, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '21c5c80a138c98a0', 'authors': ['Jiaxin Deng', 'Shiyao Wang', 'Kuo Cai', 'Lejian Ren', 'Qigen Hu', 'Weifeng Ding', 'Qiang Luo', 'Guorui Zhou'], 'affiliations': ['KuaiShou Inc. Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.18965.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#rag', '#games', '#training', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'OneRec: Единая генеративная модель для революции в рекомендательных системах', 'desc': 'OneRec - это новая система рекомендаций, использующая единую генеративную модель вместо каскадного подхода. Она включает в себя структуру кодировщик-декодировщик с разреженной смесью экспертов (MoE) для масштабирования возможностей модели. OneRec применяет поэтапную генерацию сессий и модуль итеративного выравнивания предпочтений с прямой оптимизацией предпочтений (DPO). Система показала значительное улучшение времени просмотра при развертывании на платформе Kuaishou.'}, 'en': {'title': 'OneRec: Revolutionizing Recommendations with Generative Models', 'desc': 'This paper introduces OneRec, a novel generative retrieval-based recommendation system that improves upon traditional retrieve-and-rank methods. Unlike existing systems that use generative models merely for selection, OneRec employs a unified generative model that encodes user behavior and generates personalized video recommendations in a session-wise manner. The model utilizes a sparse Mixture-of-Experts architecture to enhance capacity while maintaining efficiency, and incorporates an Iterative Preference Alignment module to optimize user preferences effectively. Experimental results show that OneRec significantly outperforms existing systems, leading to a notable increase in user engagement metrics such as watch-time.'}, 'zh': {'title': 'OneRec：统一生成模型的推荐新范式', 'desc': '最近，基于生成检索的推荐系统成为一种有前景的范式。本文提出的OneRec模型，采用统一的生成模型，取代了传统的级联学习框架，能够在真实场景中显著超越现有复杂的推荐系统。OneRec包括编码-解码结构，能够有效编码用户的历史行为，并生成用户可能感兴趣的视频。此外，OneRec还引入了会话生成方法和迭代偏好对齐模块，提升了生成结果的质量，并在快手的实际应用中实现了观看时间的显著增加。'}}}, {'id': 'https://huggingface.co/papers/2503.01688', 'title': 'When an LLM is apprehensive about its answers -- and when its uncertainty is justified', 'url': 'https://huggingface.co/papers/2503.01688', 'abstract': 'Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on a specific type of uncertainty, ignoring others. We investigate what estimates, specifically token-wise entropy and model-as-judge (MASJ), would work for multiple-choice question-answering tasks for different question topics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of different sizes from 1.5B to 72B and 14 topics. While MASJ performs similarly to a random error predictor, the response entropy predicts model error in knowledge-dependent domains and serves as an effective indicator of question difficulty: for biology ROC AUC is 0.73. This correlation vanishes for the reasoning-dependent domain: for math questions ROC-AUC is 0.55. More principally, we found out that the entropy measure required a reasoning amount. Thus, data-uncertainty related entropy should be integrated within uncertainty estimates frameworks, while MASJ requires refinement. Moreover, existing MMLU-Pro samples are biased, and should balance required amount of reasoning for different subdomains to provide a more fair assessment of LLMs performance.', 'score': 16, 'issue_id': 2518, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '68429d7977c57eae', 'authors': ['Petr Sychev', 'Andrey Goncharov', 'Daniil Vyazhev', 'Edvard Khalafyan', 'Alexey Zaytsev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.01688.jpg', 'data': {'categories': ['#ethics', '#hallucinations', '#benchmark', '#reasoning', '#data', '#multilingual'], 'emoji': '🤖', 'ru': {'title': 'Энтропия ответов как индикатор неопределенности LLM в задачах с множественным выбором', 'desc': "Исследование посвящено оценке неопределенности в крупных языковых моделях (LLM) при решении задач с множественным выбором. Авторы сравнивают эффективность энтропии токенов и метода 'модель как судья' (MASJ) для различных тем вопросов. Эксперименты проводились на трех LLM разных размеров и 14 темах. Результаты показывают, что энтропия ответов хорошо предсказывает ошибки модели в областях, зависящих от знаний, но не в областях, требующих рассуждений."}, 'en': {'title': 'Enhancing Uncertainty Estimation in LLMs for Better Decision-Making', 'desc': 'This paper explores how to measure uncertainty in Large Language Models (LLMs) when answering multiple-choice questions, which is important in critical areas where wrong answers can have serious effects. It compares two methods of uncertainty estimation: token-wise entropy and model-as-judge (MASJ), across various LLMs and topics. The findings reveal that while MASJ does not effectively predict errors, token-wise entropy is a better indicator of question difficulty, especially in knowledge-based subjects like biology. The study also highlights the need to refine MASJ and address biases in existing datasets to ensure fair evaluation of LLM performance across different reasoning requirements.'}, 'zh': {'title': '提升大型语言模型的不确定性估计', 'desc': '不确定性估计对于评估大型语言模型（LLMs）至关重要，尤其是在错误答案可能导致重大后果的高风险领域。本文探讨了不同类型的不确定性估计，特别是基于令牌的熵和模型作为评判者（MASJ），在多选题回答任务中的有效性。实验涉及三种不同规模的LLMs，结果显示，响应熵在知识依赖领域能够有效预测模型错误，而MASJ的表现类似于随机错误预测器。我们发现熵度量需要一定的推理量，因此数据不确定性相关的熵应纳入不确定性估计框架中，而MASJ则需要进一步改进。'}}}, {'id': 'https://huggingface.co/papers/2503.01496', 'title': 'Liger: Linearizing Large Language Models to Gated Recurrent Structures', 'url': 'https://huggingface.co/papers/2503.01496', 'abstract': 'Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\\% of the Transformer-based LLM at 0.02\\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.', 'score': 13, 'issue_id': 2514, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd5ca7ef45c0e90c9', 'authors': ['Disen Lan', 'Weigao Sun', 'Jiaxi Hu', 'Jusen Du', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'South China University of Technology', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2503.01496.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#benchmark'], 'emoji': '🔢', 'ru': {'title': 'Эффективная линеаризация больших языковых моделей', 'desc': 'Данная статья представляет новый метод Liger для линеаризации больших языковых моделей (LLM) в гейтированные линейно-рекуррентные структуры. Liger преобразует предобученные LLM без добавления дополнительных параметров, используя существующие веса ключевой матрицы для создания различных механизмов гейтирования. Метод применяет легковесную донастройку с помощью Low-Rank Adaptation (LoRA) для восстановления производительности линеаризованных моделей. Авторы также представляют Liger Attention - гибридный механизм внимания, который значительно улучшает эффективность линеаризации.'}, 'en': {'title': 'Liger: Efficiently Transforming LLMs into Gated Linear Recurrent Models', 'desc': "This paper introduces Liger, a method for transforming pretrained large language models (LLMs) into gated linear recurrent models. Liger efficiently repurposes existing key matrix weights to create diverse gating mechanisms without adding extra parameters, thus avoiding the costly process of training new components from scratch. The approach employs lightweight fine-tuning techniques, specifically Low-Rank Adaptation (LoRA), to maintain the performance of the linearized models comparable to the original LLMs. Additionally, Liger incorporates a novel intra-layer hybrid attention mechanism, Liger Attention, which enhances the model's efficiency while achieving competitive results across various benchmarks."}, 'zh': {'title': 'Liger：高效转换预训练模型的创新方法', 'desc': '本文提出了一种名为Liger的方法，用于将预训练的大型语言模型（LLMs）转换为带门控的线性递归模型，而无需增加额外的参数。Liger通过重新利用预训练的关键矩阵权重，构建多样的门控机制，从而形成不同的门控递归结构。该方法使用轻量级的微调技术（如低秩适应LoRA），使线性化的门控递归模型的性能恢复到与原始LLMs相当的水平。此外，Liger Attention作为一种层内混合注意力机制，在线性化过程中显著恢复了93%的Transformer基础LLM的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.01307', 'title': 'Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs', 'url': 'https://huggingface.co/papers/2503.01307', 'abstract': "Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.", 'score': 13, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'fa966620baa8c013', 'authors': ['Kanishk Gandhi', 'Ayush Chakravarthy', 'Anikait Singh', 'Nathan Lile', 'Noah D. Goodman'], 'affiliations': ['Stanford University', 'SynthLabs'], 'pdf_title_img': 'assets/pdf/title_img/2503.01307.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Когнитивные навыки - ключ к самосовершенствованию языковых моделей', 'desc': 'Исследование показывает, что способность языковых моделей к самосовершенствованию зависит от наличия у них определенных когнитивных поведений, таких как верификация, бэктрекинг, постановка подцелей и обратное планирование. Эксперименты выявили, что модель Qwen изначально обладает этими навыками, в то время как Llama нет. Прайминг Llama примерами, содержащими эти поведения, позволил значительно улучшить ее производительность при обучении с подкреплением. Важно отметить, что наличие правильных рассуждений оказалось более критичным фактором, чем корректность ответов.'}, 'en': {'title': 'Unlocking Self-Improvement in Language Models through Reasoning', 'desc': 'This paper explores how language models can improve their problem-solving abilities through a process called test-time inference, similar to human experts. It highlights the differences in performance between two models, Qwen-2.5-3B and Llama-3.2-3B, when trained with reinforcement learning (RL) on the game Countdown. The authors identify four cognitive behaviors—verification, backtracking, subgoal setting, and backward chaining—that are crucial for effective self-improvement in these models. They demonstrate that enhancing Llama with examples of these reasoning behaviors can significantly boost its performance, suggesting that the ability to reason is more important than simply providing correct answers.'}, 'zh': {'title': '推理行为是模型自我提升的关键', 'desc': '本文探讨了语言模型在复杂任务中自我改进的能力，特别是通过强化学习（RL）实现的自我提升。研究发现，不同模型在相同的RL训练下表现差异显著，例如Qwen-2.5-3B在游戏Countdown中远超Llama-3.2-3B。我们分析了四种关键的认知行为：验证、回溯、子目标设定和逆向链推理，发现Qwen自然展现了这些推理行为，而Llama则最初缺乏。通过对Llama进行示例引导，能够显著提升其在RL中的表现，证明了推理行为的存在是模型自我改进的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2503.00501', 'title': 'Qilin: A Multimodal Information Retrieval Dataset with APP-level User Sessions', 'url': 'https://huggingface.co/papers/2503.00501', 'abstract': "User-generated content (UGC) communities, especially those featuring multimodal content, improve user experiences by integrating visual and textual information into results (or items). The challenge of improving user experiences in complex systems with search and recommendation (S\\&R) services has drawn significant attention from both academia and industry these years. However, the lack of high-quality datasets has limited the research progress on multimodal S\\&R. To address the growing need for developing better S\\&R services, we present a novel multimodal information retrieval dataset in this paper, namely Qilin. The dataset is collected from Xiaohongshu, a popular social platform with over 300 million monthly active users and an average search penetration rate of over 70\\%. In contrast to existing datasets, Qilin offers a comprehensive collection of user sessions with heterogeneous results like image-text notes, video notes, commercial notes, and direct answers, facilitating the development of advanced multimodal neural retrieval models across diverse task settings. To better model user satisfaction and support the analysis of heterogeneous user behaviors, we also collect extensive APP-level contextual signals and genuine user feedback. Notably, Qilin contains user-favored answers and their referred results for search requests triggering the Deep Query Answering (DQA) module. This allows not only the training \\& evaluation of a Retrieval-augmented Generation (RAG) pipeline, but also the exploration of how such a module would affect users' search behavior. Through comprehensive analysis and experiments, we provide interesting findings and insights for further improving S\\&R systems. We hope that Qilin will significantly contribute to the advancement of multimodal content platforms with S\\&R services in the future.", 'score': 10, 'issue_id': 2513, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': 'ed7fc8625b068597', 'authors': ['Jia Chen', 'Qian Dong', 'Haitao Li', 'Xiaohui He', 'Yan Gao', 'Shaosheng Cao', 'Yi Wu', 'Ping Yang', 'Chen Xu', 'Yao Hu', 'Qingyao Ai', 'Yiqun Liu'], 'affiliations': ['Tsinghua University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.00501.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Qilin: мультимодальный датасет для улучшения поиска и рекомендаций', 'desc': 'Представлен новый набор данных Qilin для мультимодального информационного поиска, собранный на платформе Xiaohongshu. Датасет включает пользовательские сессии с разнородными результатами (изображения, видео, коммерческие заметки) и контекстуальными сигналами. Qilin позволяет обучать и оценивать нейросетевые модели поиска и рекомендаций, а также исследовать влияние модуля глубоких ответов на запросы. Авторы надеются, что Qilin внесет значительный вклад в развитие мультимодальных платформ с поисковыми сервисами.'}, 'en': {'title': 'Enhancing User Experiences with Qilin: A Multimodal Dataset for S&R Services', 'desc': 'This paper introduces Qilin, a new multimodal information retrieval dataset designed to enhance search and recommendation (S&R) services in user-generated content communities. Qilin is unique as it includes diverse user sessions with various content types, such as image-text notes and videos, which can help in developing advanced multimodal neural retrieval models. Additionally, the dataset captures user feedback and contextual signals, allowing researchers to analyze user satisfaction and behavior more effectively. The findings from this research aim to improve S&R systems and contribute to the evolution of multimodal content platforms.'}, 'zh': {'title': '推动多模态搜索与推荐服务的进步', 'desc': '本文介绍了一个新的多模态信息检索数据集Qilin，旨在改善用户在复杂系统中的搜索和推荐体验。Qilin数据集来源于小红书，包含多种类型的用户会话，如图文笔记、视频笔记和商业笔记，适用于多种任务设置。该数据集还收集了丰富的应用级上下文信号和真实用户反馈，以更好地建模用户满意度。通过对Qilin的分析和实验，本文提供了有趣的发现，期望能推动多模态内容平台的搜索和推荐服务的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.00714', 'title': 'Speculative Ad-hoc Querying', 'url': 'https://huggingface.co/papers/2503.00714', 'abstract': "Analyzing large datasets requires responsive query execution, but executing SQL queries on massive datasets can be slow. This paper explores whether query execution can begin even before the user has finished typing, allowing results to appear almost instantly. We propose SpeQL, a system that leverages Large Language Models (LLMs) to predict likely queries based on the database schema, the user's past queries, and their incomplete query. Since exact query prediction is infeasible, SpeQL speculates on partial queries in two ways: 1) it predicts the query structure to compile and plan queries in advance, and 2) it precomputes smaller temporary tables that are much smaller than the original database, but are still predicted to contain all information necessary to answer the user's final query. Additionally, SpeQL continuously displays results for speculated queries and subqueries in real time, aiding exploratory analysis. A utility/user study showed that SpeQL improved task completion time, and participants reported that its speculative display of results helped them discover patterns in the data more quickly. In the study, SpeQL improves user's query latency by up to 289times and kept the overhead reasonable, at 4$ per hour.", 'score': 8, 'issue_id': 2514, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '1b0459b56fdb6894', 'authors': ['Haoyu Li', 'Srikanth Kandula', 'Maria Angels de Luis Balaguer', 'Aditya Akella', 'Venkat Arun'], 'affiliations': ['Amazon Web Services', 'Microsoft Research', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.00714.jpg', 'data': {'categories': ['#dataset', '#data', '#benchmark'], 'emoji': '⚡', 'ru': {'title': 'Молниеносные SQL-запросы с помощью предиктивной аналитики', 'desc': 'Статья представляет систему SpeQL, использующую большие языковые модели для предсказания SQL-запросов пользователя. SpeQL предугадывает структуру запроса и предварительно вычисляет временные таблицы, что позволяет начать выполнение запроса до его завершения пользователем. Система непрерывно отображает результаты предполагаемых запросов в реальном времени, помогая в исследовательском анализе данных. Исследование показало, что SpeQL значительно сокращает время выполнения задач и помогает пользователям быстрее обнаруживать закономерности в данных.'}, 'en': {'title': 'Instant Query Results with SpeQL!', 'desc': 'This paper introduces SpeQL, a novel system designed to enhance the speed of SQL query execution on large datasets. By utilizing Large Language Models (LLMs), SpeQL predicts user queries even before they are fully typed, allowing for near-instantaneous results. It employs two main strategies: predicting the structure of queries for pre-compilation and creating smaller temporary tables that contain essential data for answering the final query. A user study demonstrated that SpeQL significantly reduced query latency and helped users identify data patterns more efficiently during exploratory analysis.'}, 'zh': {'title': 'SpeQL：让查询更快的智能预测系统', 'desc': '本论文探讨了如何在用户输入SQL查询时，提前开始执行查询，以加快大数据集的查询响应速度。我们提出了SpeQL系统，利用大型语言模型（LLMs）根据数据库模式、用户的历史查询和不完整查询来预测可能的查询。SpeQL通过预测查询结构和预计算小型临时表来处理部分查询，从而在用户完成查询之前提供实时结果。研究表明，SpeQL显著提高了用户的查询速度，并帮助用户更快地发现数据中的模式。'}}}, {'id': 'https://huggingface.co/papers/2503.00784', 'title': 'DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting', 'url': 'https://huggingface.co/papers/2503.00784', 'abstract': 'Large language models (LLMs) exhibit exceptional performance across a wide range of tasks; however, their token-by-token autoregressive generation process significantly hinders inference speed. Speculative decoding presents a promising draft-then-verify framework that reduces generation latency while maintaining output distribution fidelity. Nevertheless, the draft model introduces additional computational overhead, becoming a performance bottleneck and increasing the time to first token (TTFT). Previous approaches to mitigate draft model overhead have primarily relied on heuristics and generally failed to match the quality of the draft language models. To address these challenges, we propose DuoDecoding, a novel approach that strategically deploys the draft and target models on the CPU and GPU respectively, enabling parallel decoding while preserving draft quality. Our method incorporates a hardware-aware optimal draft budget to minimize idle times and employs dynamic multi-sequence drafting to enhance draft quality. Extensive experiments across seven tasks show that DuoDecoding achieves up to 2.61x speedup in generation latency, while reducing TTFT to 83% of that in conventional speculative decoding. The Code is available at https://github.com/KaiLv69/DuoDecoding.', 'score': 8, 'issue_id': 2510, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'b4870a0e44c3cc55', 'authors': ['Kai Lv', 'Honglin Guo', 'Qipeng Guo', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.00784.jpg', 'data': {'categories': ['#inference', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'DuoDecoding: Параллельное ускорение языковых моделей', 'desc': 'Статья представляет новый метод ускорения генерации текста большими языковыми моделями (LLM) под названием DuoDecoding. Этот подход использует параллельное декодирование на CPU и GPU, оптимизируя время генерации первого токена и общую латентность. DuoDecoding применяет аппаратно-ориентированный оптимальный бюджет черновика и динамическое многопоследовательное черновое декодирование для повышения качества. Эксперименты показали значительное ускорение генерации по сравнению с обычным спекулятивным декодированием.'}, 'en': {'title': 'DuoDecoding: Speeding Up Text Generation with Smart Model Deployment', 'desc': 'This paper introduces DuoDecoding, a new method to improve the speed of generating text with large language models (LLMs) while keeping the quality high. It uses a draft-then-verify approach, where a draft model quickly generates initial text, and a target model refines it, but does so in a way that reduces the time it takes to start generating text. By using both CPU and GPU for different parts of the process, DuoDecoding allows for faster and more efficient decoding. The results show that this method can significantly speed up text generation without sacrificing quality, achieving a notable improvement in performance across various tasks.'}, 'zh': {'title': 'DuoDecoding：加速生成的新方法', 'desc': '大型语言模型（LLMs）在多种任务中表现出色，但其逐字自回归生成过程显著影响推理速度。推测解码提供了一种有前景的草稿-验证框架，能够减少生成延迟，同时保持输出分布的准确性。我们提出的DuoDecoding方法通过在CPU和GPU上分别部署草稿模型和目标模型，实现了并行解码，提升了生成效率。实验结果表明，DuoDecoding在生成延迟上实现了最高2.61倍的加速，同时将首次生成时间缩短至传统推测解码的83%。'}}}, {'id': 'https://huggingface.co/papers/2503.01506', 'title': 'SampleMix: A Sample-wise Pre-training Data Mixing Strategey by Coordinating Data Quality and Diversity', 'url': 'https://huggingface.co/papers/2503.01506', 'abstract': "Existing pretraining data mixing methods for large language models (LLMs) typically follow a domain-wise methodology, a top-down process that first determines domain weights and then performs uniform data sampling across each domain. However, these approaches neglect significant inter-domain overlaps and commonalities, failing to control the global diversity of the constructed training dataset. Further, uniform sampling within domains ignores fine-grained sample-specific features, potentially leading to suboptimal data distribution. To address these shortcomings, we propose a novel sample-wise data mixture approach based on a bottom-up paradigm. This method performs global cross-domain sampling by systematically evaluating the quality and diversity of each sample, thereby dynamically determining the optimal domain distribution. Comprehensive experiments across multiple downstream tasks and perplexity assessments demonstrate that SampleMix surpasses existing domain-based methods. Meanwhile, SampleMix requires 1.4x to 2.1x training steps to achieves the baselines' performance, highlighting the substantial potential of SampleMix to optimize pre-training data.", 'score': 7, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '018cc621eb1ee12b', 'authors': ['Xiangyu Xi', 'Deyang Kong', 'Jian Yang', 'Jiawei Yang', 'Zhengyu Chen', 'Wei Wang', 'Jingang Wang', 'Xunliang Cai', 'Shikun Zhang', 'Wei Ye'], 'affiliations': ['Meituan Group, Beijing, China', 'National Engineering Research Center for Software Engineering, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01506.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#optimization', '#data'], 'emoji': '🔀', 'ru': {'title': 'SampleMix: революция в смешивании данных для LLM', 'desc': 'В статье представлен новый подход к смешиванию предобучающих данных для больших языковых моделей (LLM), названный SampleMix. В отличие от традиционных методов, основанных на доменах, SampleMix использует выборку на уровне отдельных образцов, оценивая их качество и разнообразие. Этот метод позволяет динамически определять оптимальное распределение доменов и учитывать межdomенные пересечения. Эксперименты показали, что SampleMix превосходит существующие методы, основанные на доменах, хотя и требует больше шагов обучения.'}, 'en': {'title': 'Revolutionizing Data Mixing for Better Language Model Training', 'desc': 'This paper introduces SampleMix, a new method for mixing pretraining data for large language models (LLMs). Unlike traditional domain-wise approaches that sample uniformly within predefined domains, SampleMix uses a bottom-up strategy that evaluates the quality and diversity of individual samples across domains. This allows for a more dynamic and optimal distribution of training data, addressing the limitations of inter-domain overlaps and sample-specific features. Experimental results show that SampleMix not only outperforms existing methods but also requires fewer training steps to achieve comparable performance.'}, 'zh': {'title': '样本级数据混合，优化预训练数据的未来', 'desc': '现有的大型语言模型预训练数据混合方法通常采用领域导向的方法，先确定领域权重，再在每个领域内进行均匀数据采样。然而，这些方法忽视了领域之间的重要重叠和共性，未能有效控制训练数据集的全球多样性。此外，领域内的均匀采样忽略了样本特定的细微特征，可能导致数据分布不理想。为了解决这些问题，我们提出了一种基于自下而上的新型样本级数据混合方法，能够通过系统评估每个样本的质量和多样性，动态确定最佳领域分布。'}}}, {'id': 'https://huggingface.co/papers/2502.18890', 'title': 'From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens', 'url': 'https://huggingface.co/papers/2502.18890', 'abstract': "Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at https://github.com/bigai-nlco/TokenSwift.", 'score': 7, 'issue_id': 2517, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': 'd07c05abfac49ecc', 'authors': ['Tong Wu', 'Junzhe Shen', 'Zixia Jia', 'Yuxuan Wang', 'Zilong Zheng'], 'affiliations': ['NLCo Lab, BIGAI LUMIA Lab, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18890.jpg', 'data': {'categories': ['#training', '#architecture', '#long_context', '#inference', '#optimization'], 'emoji': '⚡', 'ru': {'title': 'TOKENSWIFT: революция в скорости генерации сверхдлинных текстов', 'desc': 'Исследователи представили TOKENSWIFT - новую систему для ускорения генерации сверхдлинных последовательностей большими языковыми моделями (LLM). Они выявили три основные проблемы, препятствующие эффективной генерации: частая перезагрузка модели, динамическое управление ключами-значениями и повторяющаяся генерация. TOKENSWIFT решает эти проблемы, позволяя ускорить процесс генерации в 3 раза для моделей различных масштабов и архитектур. Это существенно сокращает время генерации сверхдлинных последовательностей, сохраняя при этом качество целевой модели.'}, 'en': {'title': 'Accelerating Ultra-Long Sequence Generation with TOKENSWIFT', 'desc': 'This paper presents TOKENSWIFT, a new framework aimed at speeding up the generation of ultra-long sequences using large language models (LLMs). The authors identify key challenges such as model reloading, dynamic key-value management, and repetitive generation that slow down the process. By addressing these issues, TOKENSWIFT achieves over three times the speed of traditional methods while preserving the quality of the generated text. Experimental results show that this framework is effective across various model sizes and architectures, making it a significant advancement in the field of sequence generation.'}, 'zh': {'title': 'TOKENSWIFT：加速超长序列生成的创新框架', 'desc': '生成超长序列对于大型语言模型（LLMs）变得越来越重要，但这一过程通常非常耗时，尤其是对于长达10万标记的序列。传统的推测解码方法在延长生成限制时并未加速过程，反而可能造成负面影响。我们通过深入分析，识别出影响高效生成的三个主要挑战：频繁的模型重载、动态键值（KV）管理和重复生成。为了解决这些问题，我们提出了TOKENSWIFT，一个新框架，旨在显著加快超长序列的生成过程，同时保持目标模型的固有质量。'}}}, {'id': 'https://huggingface.co/papers/2503.01370', 'title': 'Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation', 'url': 'https://huggingface.co/papers/2503.01370', 'abstract': "Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate ''3D Bundle Image'', a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently.", 'score': 7, 'issue_id': 2513, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '3decc9fe2b6f6e32', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#cv', '#diffusion', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Простая и эффективная 3D-генерация на основе 2D-диффузии', 'desc': "Статья представляет Kiss3DGen - эффективный фреймворк для генерации, редактирования и улучшения 3D-объектов с использованием предобученной модели диффузии для 2D-изображений. Метод основан на дообучении диффузионной модели для генерации 'Пакетного 3D-изображения', состоящего из мультиракурсных изображений и соответствующих карт нормалей. Затем карты нормалей используются для реконструкции 3D-меша, а мультиракурсные изображения обеспечивают текстурирование, что в результате дает полную 3D-модель. Авторы демонстрируют, что их подход совместим с различными техниками диффузионных моделей и позволяет эффективно создавать качественные 3D-модели."}, 'en': {'title': 'Kiss3DGen: Simplifying 3D Generation with 2D Diffusion Models', 'desc': "This paper presents Kiss3DGen, a novel framework that simplifies the process of generating and enhancing 3D objects by leveraging existing 2D image diffusion models. The approach involves fine-tuning a diffusion model to create a '3D Bundle Image', which consists of multiple views and normal maps that are essential for 3D reconstruction. By transforming the 3D generation challenge into a 2D image task, the method maximizes the use of knowledge from pretrained models, making it more efficient. The results show that Kiss3DGen not only generates high-quality 3D models but also supports advanced features like editing and texture enhancement."}, 'zh': {'title': '简单高效的三维生成方法', 'desc': '扩散模型在生成二维图像方面取得了巨大成功，但在三维内容生成的质量和通用性上仍然有限。现有的先进方法通常需要大量的三维资产进行训练，这些资产难以收集。我们提出了Kiss3DGen（简单直接的三维生成），这是一个高效的框架，通过重新利用经过良好训练的二维图像扩散模型来生成、编辑和增强三维物体。该方法将三维生成问题转化为二维图像生成任务，最大化利用预训练扩散模型中的知识，能够有效生成高质量的三维模型。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.00455', 'title': 'PodAgent: A Comprehensive Framework for Podcast Generation', 'url': 'https://huggingface.co/papers/2503.00455', 'abstract': "Existing Existing automatic audio generation methods struggle to generate podcast-like audio programs effectively. The key challenges lie in in-depth content generation, appropriate and expressive voice production. This paper proposed PodAgent, a comprehensive framework for creating audio programs. PodAgent 1) generates informative topic-discussion content by designing a Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis method to generate expressive conversational speech. Given the absence of standardized evaluation criteria for podcast-like audio generation, we developed comprehensive assessment guidelines to effectively evaluate the model's performance. Experimental results demonstrate PodAgent's effectiveness, significantly surpassing direct GPT-4 generation in topic-discussion dialogue content, achieving an 87.4% voice-matching accuracy, and producing more expressive speech through LLM-guided synthesis. Demo page: https://podcast-agent.github.io/demo/. Source code: https://github.com/yujxx/PodAgent.", 'score': 5, 'issue_id': 2519, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '59ce5f373a030894', 'authors': ['Yujia Xiao', 'Lei He', 'Haohan Guo', 'Fenglong Xie', 'Tan Lee'], 'affiliations': ['Microsoft', 'The Chinese University of Hong Kong', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.00455.jpg', 'data': {'categories': ['#games', '#audio', '#interpretability', '#benchmark', '#optimization', '#multimodal'], 'emoji': '🎙️', 'ru': {'title': 'PodAgent: ИИ-ведущий для подкастов нового поколения', 'desc': 'PodAgent - это новая система для автоматического создания аудиопрограмм в стиле подкастов. Она использует мультиагентный подход для генерации содержательного контента, подбирает подходящие голоса из голосового пула и применяет улучшенный синтез речи на основе языковых моделей. Система решает ключевые проблемы существующих методов, такие как глубина контента и выразительность голоса. Эксперименты показали значительное превосходство PodAgent над прямой генерацией GPT-4 по качеству диалогов и точности подбора голосов.'}, 'en': {'title': 'Revolutionizing Podcast Audio Generation with PodAgent', 'desc': "This paper introduces PodAgent, a novel framework designed to enhance the generation of podcast-like audio programs. It addresses key challenges in content creation and voice production by employing a multi-agent system that includes a Host, Guest, and Writer for collaborative topic discussions. Additionally, PodAgent features a voice pool for effective voice-role matching and utilizes a large language model (LLM) to improve the expressiveness of the generated speech. The framework's performance is validated through comprehensive evaluation guidelines, showing significant improvements over existing methods, including a high voice-matching accuracy and more engaging conversational audio."}, 'zh': {'title': 'PodAgent：智能生成播客音频的全新框架', 'desc': '本论文提出了一种名为PodAgent的框架，用于自动生成类似播客的音频节目。PodAgent通过设计一个主持人-嘉宾-编剧的多智能体协作系统，生成有深度的主题讨论内容。同时，它建立了一个声音库，以实现合适的声音角色匹配，并利用增强型大语言模型（LLM）进行富有表现力的语音合成。实验结果表明，PodAgent在主题讨论对话内容生成方面显著优于直接使用GPT-4，语音匹配准确率达到87.4%，并通过LLM引导的合成生成了更具表现力的语音。'}}}, {'id': 'https://huggingface.co/papers/2503.01295', 'title': 'CodeArena: A Collective Evaluation Platform for LLM Code Generation', 'url': 'https://huggingface.co/papers/2503.01295', 'abstract': 'Large Language Models (LLMs) have reshaped code generation by synergizing their exceptional comprehension of natural language and programming syntax, thereby substantially boosting developer productivity. These advancements have prompted numerous efforts to quantitatively evaluate their coding capabilities. However, persistent challenges, such as benchmark leakage, data dissipation, and limited system accessibility, continue to impede a timely and accurate assessment. To address these limitations, we introduce CodeArena, an online evaluation framework tailored for LLM code generation. The key innovation is a collective evaluation mechanism, which dynamically recalibrates individual model scores based on the holistic performance of all participating models, mitigating score biases caused by widespread benchmark leakage. In addition, CodeArena ensures open access to all submitted solutions and test cases and provides automation-friendly APIs to streamline the code evaluation workflow. Our main contributions are: (1) a collective evaluation system for unbiased assessment, (2) a public repository of solutions and test cases, and (3) automation-ready APIs for seamless integration.', 'score': 5, 'issue_id': 2514, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '96f50dd9e636b12e', 'authors': ['Mingzhe Du', 'Anh Tuan Luu', 'Bin Ji', 'Xiaobao Wu', 'Dong Huang', 'Terry Yue Zhuo', 'Qian Liu', 'See-Kiong Ng'], 'affiliations': ['ByteDance', 'Monash University', 'Nanyang Technological University', 'National University of Singapore', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.01295.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#leakage', '#open_source'], 'emoji': '🏟️', 'ru': {'title': 'CodeArena: Справедливая арена для оценки LLM в генерации кода', 'desc': 'CodeArena - это новая онлайн-платформа для оценки генерации кода большими языковыми моделями (LLM). Она использует коллективный механизм оценки, который динамически пересчитывает индивидуальные оценки моделей на основе общей производительности всех участвующих моделей. Это помогает снизить искажения оценок, вызванные утечкой тестовых данных. CodeArena также предоставляет открытый доступ ко всем отправленным решениям и тестовым случаям, а также API для автоматизации процесса оценки.'}, 'en': {'title': 'Revolutionizing Code Evaluation with CodeArena', 'desc': 'This paper discusses the impact of Large Language Models (LLMs) on code generation, highlighting their ability to understand both natural language and programming syntax, which enhances developer productivity. It identifies ongoing issues in evaluating LLM coding capabilities, such as benchmark leakage and limited access to evaluation systems. To overcome these challenges, the authors present CodeArena, an online framework that offers a collective evaluation mechanism to provide unbiased assessments of LLMs. CodeArena also features a public repository for solutions and test cases, along with APIs for easy integration into existing workflows.'}, 'zh': {'title': 'CodeArena：公平评估LLM代码生成的创新平台', 'desc': '大型语言模型（LLMs）通过结合对自然语言和编程语法的深刻理解，极大地提升了代码生成的效率，进而提高了开发者的生产力。为了量化评估这些模型的编码能力，许多研究工作应运而生，但仍面临基准泄漏、数据消散和系统可访问性有限等挑战。为了解决这些问题，我们提出了CodeArena，这是一个专为LLM代码生成设计的在线评估框架。其核心创新在于集体评估机制，能够根据所有参与模型的整体表现动态调整个别模型的评分，从而减少因基准泄漏造成的评分偏差。'}}}, {'id': 'https://huggingface.co/papers/2503.01807', 'title': 'Large-Scale Data Selection for Instruction Tuning', 'url': 'https://huggingface.co/papers/2503.01807', 'abstract': 'Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection.', 'score': 5, 'issue_id': 2511, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8bbc980a9ef867f7', 'authors': ['Hamish Ivison', 'Muru Zhang', 'Faeze Brahman', 'Pang Wei Koh', 'Pradeep Dasigi'], 'affiliations': ['Allen Institute for AI', 'University of Southern California', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.01807.jpg', 'data': {'categories': ['#data', '#open_source', '#optimization', '#dataset', '#training'], 'emoji': '🔍', 'ru': {'title': 'Эффективный отбор данных для обучения языковых моделей: меньше значит больше', 'desc': 'Эта статья исследует методы автоматического отбора данных для инструктивной настройки языковых моделей. Авторы проводят систематическое изучение эффективности различных методов при масштабировании до больших объемов данных, выбирая до 2,5 миллионов образцов из пулов до 5,8 миллионов. Результаты показывают, что многие недавно предложенные методы уступают случайному отбору в этих условиях, однако вариант метода отбора на основе представлений (RDS+) превосходит более сложные подходы. Исследование подчеркивает важность тщательного анализа масштабируемости методов автоматического отбора данных.'}, 'en': {'title': 'Quality Over Quantity: Smart Data Selection for Language Models', 'desc': 'This paper investigates the importance of selecting high-quality training data for instruction-tuning language models. It reveals that many automated data selection methods do not perform better than random selection when scaling to larger datasets, which can include millions of samples. The study introduces a representation-based data selection method (RDS+) that consistently outperforms more complex approaches while being more efficient in terms of computational resources. The authors emphasize the need for a deeper examination of how these selection methods behave as the size of the data pools increases.'}, 'zh': {'title': '高效选择：优化语言模型训练数据的关键', 'desc': '在对语言模型进行指令调优时，从更大数据集中选择高质量的训练数据是一个关键步骤。经过精心策划的数据集通常能产生比那些在更大、更嘈杂的数据集上训练的模型更好的效果。我们进行了系统研究，评估数据选择方法在大规模数据集上的表现，发现许多新提出的方法在这种情况下的表现不如随机选择。我们还发现一种基于表示的数据选择变体（RDS+）在所有测试设置中始终优于更复杂的方法，同时计算效率更高。'}}}, {'id': 'https://huggingface.co/papers/2503.00031', 'title': 'Efficient Test-Time Scaling via Self-Calibration', 'url': 'https://huggingface.co/papers/2503.00031', 'abstract': 'Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling Self-Consistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design confidence-based efficient test-time scaling methods to handle queries of various difficulty, such as Early-Stopping for Best-of-N and Self-Consistency with calibrated confidence. Experiments on three LLMs across six datasets demonstrate the effectiveness of our approach. Specifically, applying confidence-based Early Stopping to Best-of-N improves MathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses, indicating the efficacy of confidence-based sampling strategy at inference time.', 'score': 4, 'issue_id': 2523, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': 'fad9bb0721d1d6ce', 'authors': ['Chengsong Huang', 'Langlin Huang', 'Jixuan Leng', 'Jiacheng Liu', 'Jiaxin Huang'], 'affiliations': ['Carnegie Mellon University', 'University of Washington', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.00031.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': '🎯', 'ru': {'title': 'Повышение эффективности LLM через калибровку уверенности', 'desc': 'Это исследование предлагает метод Self-Calibration для улучшения эффективности крупных языковых моделей (LLM) при тестировании. Метод основан на дистилляции уверенности, полученной из Self-Consistency, в саму модель, что позволяет надежно оценивать уверенность за один проход. Авторы разработали методы эффективного масштабирования на основе уверенности, такие как Early-Stopping для Best-of-N и Self-Consistency с калиброванной уверенностью. Эксперименты на трех LLM и шести наборах данных показали эффективность этого подхода, в частности, улучшение точности MathQA с 81.0 до 83.6 при бюджете выборки в 16 ответов.'}, 'en': {'title': 'Enhancing LLM Efficiency with Confidence-Based Sampling', 'desc': 'This paper discusses how to improve the efficiency of Large Language Models (LLMs) during testing by using model confidence to guide response sampling. Traditional methods like Best-of-N sampling and Self-Consistency require a fixed number of responses, which can lead to wasted resources or inadequate exploration of complex queries. The authors propose a technique called Self-Calibration, which helps LLMs provide more reliable confidence estimates by distilling information from previous responses. By implementing confidence-based strategies such as Early-Stopping, the paper shows that it is possible to enhance accuracy while reducing unnecessary computations, particularly in challenging tasks like MathQA.'}, 'zh': {'title': '提升大型语言模型响应质量的自我校准方法', 'desc': '本论文探讨了如何通过增加测试时的计算来提高大型语言模型（LLMs）的响应质量。我们提出了一种自我校准的方法，通过将自我一致性生成的置信度提炼到模型中，从而改善置信度估计的可靠性。这样，模型在测试时可以在一次前向传播中获得可靠的置信度估计。我们的实验表明，基于置信度的早停策略能够有效提高模型在不同难度问题上的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.01714', 'title': "Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia", 'url': 'https://huggingface.co/papers/2503.01714', 'abstract': "Human readers can efficiently comprehend scrambled words, a phenomenon known as Typoglycemia, primarily by relying on word form; if word form alone is insufficient, they further utilize contextual cues for interpretation. While advanced large language models (LLMs) exhibit similar abilities, the underlying mechanisms remain unclear. To investigate this, we conduct controlled experiments to analyze the roles of word form and contextual information in semantic reconstruction and examine LLM attention patterns. Specifically, we first propose SemRecScore, a reliable metric to quantify the degree of semantic reconstruction, and validate its effectiveness. Using this metric, we study how word form and contextual information influence LLMs' semantic reconstruction ability, identifying word form as the core factor in this process. Furthermore, we analyze how LLMs utilize word form and find that they rely on specialized attention heads to extract and process word form information, with this mechanism remaining stable across varying levels of word scrambling. This distinction between LLMs' fixed attention patterns primarily focused on word form and human readers' adaptive strategy in balancing word form and contextual information provides insights into enhancing LLM performance by incorporating human-like, context-aware mechanisms.", 'score': 4, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4880ed4c044081c4', 'authors': ['Chenxi Wang', 'Tianle Gu', 'Zhongyu Wei', 'Lang Gao', 'Zirui Song', 'Xiuying Chen'], 'affiliations': ['Fudan University', 'Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)'], 'pdf_title_img': 'assets/pdf/title_img/2503.01714.jpg', 'data': {'categories': ['#training', '#interpretability', '#data', '#multimodal', '#alignment'], 'emoji': '🔀', 'ru': {'title': 'Форма слова - ключ к пониманию перемешанного текста для ИИ', 'desc': 'Исследование посвящено способности больших языковых моделей (LLM) понимать перемешанные слова, подобно людям. Авторы предлагают метрику SemRecScore для оценки семантической реконструкции и анализируют роль формы слова и контекстной информации. Результаты показывают, что форма слова является ключевым фактором для LLM при обработке перемешанных слов. Анализ паттернов внимания LLM выявляет специализированные механизмы для извлечения информации о форме слова.'}, 'en': {'title': 'Unlocking LLMs: The Power of Word Form in Understanding Scrambled Text', 'desc': 'This paper explores how large language models (LLMs) understand scrambled words, similar to how humans do through a phenomenon called Typoglycemia. The authors introduce a new metric, SemRecScore, to measure how well LLMs can reconstruct meaning from scrambled text by focusing on word form and context. Their experiments reveal that LLMs primarily depend on word form for semantic reconstruction, utilizing specific attention heads to process this information. The findings suggest that incorporating more human-like, context-aware strategies could improve LLM performance in understanding language.'}, 'zh': {'title': '揭示大型语言模型的语义重建机制', 'desc': '本研究探讨了大型语言模型（LLMs）在语义重建中的能力，特别是它们如何利用单词形式和上下文信息。我们提出了一种新的度量标准SemRecScore，用于量化语义重建的程度，并验证了其有效性。研究发现，单词形式是影响LLMs语义重建能力的核心因素，且LLMs通过专门的注意力头来提取和处理单词形式信息。与人类读者在单词形式和上下文信息之间的灵活策略不同，LLMs的注意力模式主要集中在单词形式上，这为提升LLMs性能提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2502.19402', 'title': 'General Reasoning Requires Learning to Reason from the Get-go', 'url': 'https://huggingface.co/papers/2502.19402', 'abstract': "Large Language Models (LLMs) have demonstrated impressive real-world utility, exemplifying artificial useful intelligence (AUI). However, their ability to reason adaptively and robustly -- the hallmarks of artificial general intelligence (AGI) -- remains fragile. While LLMs seemingly succeed in commonsense reasoning, programming, and mathematics, they struggle to generalize algorithmic understanding across novel contexts. Our experiments with algorithmic tasks in esoteric programming languages reveal that LLM's reasoning overfits to the training data and is limited in its transferability. We hypothesize that the core issue underlying such limited transferability is the coupling of reasoning and knowledge in LLMs.   To transition from AUI to AGI, we propose disentangling knowledge and reasoning through three key directions: (1) pretaining to reason using RL from scratch as an alternative to the widely used next-token prediction pretraining, (2) using a curriculum of synthetic tasks to ease the learning of a reasoning prior for RL that can then be transferred to natural language tasks, and (3) learning more generalizable reasoning functions using a small context window to reduce exploiting spurious correlations between tokens. Such a reasoning system coupled with a trained retrieval system and a large external memory bank as a knowledge store can overcome several limitations of existing architectures at learning to reason in novel scenarios.", 'score': 3, 'issue_id': 2520, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '5774d50d6c5a9361', 'authors': ['Seungwook Han', 'Jyothish Pari', 'Samuel J. Gershman', 'Pulkit Agrawal'], 'affiliations': ['Department of Psychology and Center for Brain Science, Harvard University', 'Improbable AI Lab, MIT'], 'pdf_title_img': 'assets/pdf/title_img/2502.19402.jpg', 'data': {'categories': ['#agi', '#transfer_learning', '#architecture', '#rl', '#synthetic', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Отделяя рассуждения от знаний: путь к AGI', 'desc': 'Статья рассматривает ограничения больших языковых моделей (LLM) в области обобщенного рассуждения, несмотря на их впечатляющую полезность. Авторы выявляют проблему переобучения LLM на тренировочных данных и предлагают разделить знания и рассуждения для перехода к искусственному общему интеллекту (AGI). Предлагается использовать обучение с подкреплением, синтетические задачи и ограниченный контекст для улучшения обобщающей способности. Такая система рассуждений в сочетании с извлечением информации и внешней памятью может преодолеть ограничения существующих архитектур.'}, 'en': {'title': 'Disentangling Knowledge and Reasoning for Robust AI', 'desc': "This paper discusses the limitations of Large Language Models (LLMs) in achieving robust reasoning capabilities, which are essential for artificial general intelligence (AGI). The authors identify that LLMs often overfit to their training data, leading to poor generalization in novel algorithmic tasks. They propose a solution that involves separating knowledge from reasoning by employing reinforcement learning (RL) and a structured curriculum of synthetic tasks. By enhancing reasoning functions and integrating a retrieval system with an external memory, the authors aim to improve LLMs' adaptability and performance in unfamiliar contexts."}, 'zh': {'title': '解耦知识与推理，迈向人工通用智能', 'desc': '大型语言模型（LLMs）在实际应用中表现出色，展示了人工有用智能（AUI）的潜力。然而，它们在自适应和稳健推理方面的能力仍然脆弱，这是人工通用智能（AGI）的标志。我们的实验表明，LLMs在算法任务中容易过拟合训练数据，且在新环境中的迁移能力有限。为了解决这一问题，我们提出通过三种关键方向来解耦知识与推理，以促进从AUI向AGI的过渡。'}}}, {'id': 'https://huggingface.co/papers/2503.01739', 'title': 'VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation', 'url': 'https://huggingface.co/papers/2503.01739', 'abstract': "Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0 License.", 'score': 3, 'issue_id': 2512, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '046fdeee8939e82c', 'authors': ['Wenhao Wang', 'Yi Yang'], 'affiliations': ['University of Technology Sydney', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01739.jpg', 'data': {'categories': ['#video', '#dataset', '#data', '#games', '#open_source'], 'emoji': '🎬', 'ru': {'title': 'VideoUFO: Новый эталонный датасет для генерации видео по запросу', 'desc': 'Статья представляет VideoUFO - новый набор данных для обучения моделей генерации видео по текстовому описанию. Этот датасет содержит более 1,09 миллиона видеоклипов с подробными и краткими описаниями, охватывающих 1291 тему, актуальную для пользователей. VideoUFO отличается минимальным пересечением с существующими наборами данных и использованием только видео под лицензией Creative Commons. Эксперименты показали, что простая модель, обученная на VideoUFO, превосходит другие модели на наиболее сложных темах.'}, 'en': {'title': 'Empowering Text-to-Video Models with User-Focused Data', 'desc': 'This paper introduces VideoUFO, a novel video dataset designed to enhance text-to-video generative models by focusing on user-relevant topics. The dataset contains over 1.09 million video clips, each accompanied by both brief and detailed captions, ensuring minimal overlap with existing datasets. By clustering user prompts, the authors identified 1,291 specific topics to guide video retrieval from YouTube, which were then segmented into clips. Experiments show that models trained on VideoUFO significantly outperform existing models, particularly on challenging topics, highlighting the importance of tailored training data in machine learning applications.'}, 'zh': {'title': '提升文本到视频生成的用户体验', 'desc': '本文介绍了一种新的视频数据集VideoUFO，旨在提高文本到视频生成模型的性能。该数据集专注于用户关注的主题，包含超过109万个视频片段，并为每个片段提供简短和详细的描述。VideoUFO与现有数据集的重叠率极低，且所有视频均通过YouTube的官方API获取，确保了数据的多样性和合法性。实验结果表明，使用VideoUFO训练的模型在用户关注的主题上表现优于其他模型。'}}}, {'id': 'https://huggingface.co/papers/2503.01103', 'title': 'Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator', 'url': 'https://huggingface.co/papers/2503.01103', 'abstract': 'While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that bridges likelihood-based generative training and the GAN objective to bypass this fundamental constraint. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58 to new records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256times256.', 'score': 2, 'issue_id': 2517, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd8b58c1a2c49da16', 'authors': ['Kaiwen Zheng', 'Yongxin Chen', 'Huayu Chen', 'Guande He', 'Ming-Yu Liu', 'Jun Zhu', 'Qinsheng Zhang'], 'affiliations': ['NVIDIA', 'The University of Texas at', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01103.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'DDO: Прорыв в обучении генеративных моделей без ограничений MLE', 'desc': 'Авторы статьи предлагают новый метод обучения генеративных моделей под названием Direct Discriminative Optimization (DDO). Этот подход объединяет методы обучения на основе правдоподобия и цели генеративно-состязательных сетей (GAN), чтобы преодолеть ограничения метода максимального правдоподобия (MLE). DDO использует отношение правдоподобия между обучаемой целевой моделью и фиксированной эталонной моделью для параметризации дискриминатора. Эксперименты показывают, что DDO значительно улучшает результаты современных диффузионных и авторегрессионных моделей на различных наборах данных.'}, 'en': {'title': 'Enhancing Generative Models with Direct Discriminative Optimization', 'desc': 'This paper introduces Direct Discriminative Optimization (DDO), a new framework that enhances the performance of generative models by combining likelihood-based training with concepts from Generative Adversarial Networks (GANs). DDO addresses the limitations of maximum likelihood estimation (MLE) by using a discriminator that is parameterized through the likelihood ratio of a target model and a fixed reference model. This approach allows for efficient finetuning of pre-trained models without the need for joint training of generator and discriminator networks. The results show that DDO significantly improves the state-of-the-art performance in visual generation tasks, achieving lower FID scores on popular datasets like CIFAR-10 and ImageNet.'}, 'zh': {'title': '直接判别优化：提升生成模型的新方法', 'desc': '本文提出了一种新的方法，称为直接判别优化（DDO），旨在提高生成模型的质量。DDO通过将生成训练与GAN目标结合，克服了最大似然估计（MLE）在模型容量有限时的局限性。该方法通过使用可学习的目标模型与固定参考模型之间的似然比来隐式参数化判别器，从而简化了生成器和判别器的联合训练。实验结果表明，DDO显著提高了现有扩散模型的性能，并在多个数据集上创造了新的记录。'}}}, {'id': 'https://huggingface.co/papers/2502.16779', 'title': 'Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model', 'url': 'https://huggingface.co/papers/2502.16779', 'abstract': 'Room layout estimation from multiple-perspective images is poorly investigated due to the complexities that emerge from multi-view geometry, which requires muti-step solutions such as camera intrinsic and extrinsic estimation, image matching, and triangulation. However, in 3D reconstruction, the advancement of recent 3D foundation models such as DUSt3R has shifted the paradigm from the traditional multi-step structure-from-motion process to an end-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a novel method for multi-view room layout estimation leveraging the 3D foundation model DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on a room layout dataset (Structure3D) with a modified objective to estimate structural planes. By generating uniform and parsimonious results, Plane-DUSt3R enables room layout estimation with only a single post-processing step and 2D detection results. Unlike previous methods that rely on single-perspective or panorama image, Plane-DUSt3R extends the setting to handle multiple-perspective images. Moreover, it offers a streamlined, end-to-end solution that simplifies the process and reduces error accumulation. Experimental results demonstrate that Plane-DUSt3R not only outperforms state-of-the-art methods on the synthetic dataset but also proves robust and effective on in the wild data with different image styles such as cartoon.Our code is available at: https://github.com/justacar/Plane-DUSt3R', 'score': 2, 'issue_id': 2516, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '4a9f6cc2fb1ab840', 'authors': ['Yaxuan Huang', 'Xili Dai', 'Jianan Wang', 'Xianbiao Qi', 'Yixing Yuan', 'Xiangyu Yue'], 'affiliations': ['Astribot', 'Hong Kong Center for Construction Robotics, The Hong Kong University of Science and Technology', 'Intellifusion Inc.', 'MMLab, The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.16779.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv', '#synthetic'], 'emoji': '🏠', 'ru': {'title': 'Революция в оценке планировки помещений: от множества шагов к единому решению', 'desc': 'Статья представляет Plane-DUSt3R - новый метод для оценки планировки помещений по множественным ракурсам изображений. Этот подход использует 3D-модель фундаментального уровня DUSt3R и дообучается на наборе данных Structure3D для оценки структурных плоскостей. Plane-DUSt3R предлагает упрощенное сквозное решение, которое превосходит современные методы на синтетических данных и показывает надежность на реальных изображениях различных стилей. Метод позволяет оценивать планировку помещений с помощью одного шага постобработки и результатов 2D-обнаружения.'}, 'en': {'title': 'Revolutionizing Room Layout Estimation with Plane-DUSt3R', 'desc': 'This paper presents Plane-DUSt3R, a new method for estimating room layouts from multiple images taken from different perspectives. It builds on the DUSt3R 3D foundation model, moving away from traditional multi-step processes to a more efficient end-to-end approach. By fine-tuning the model on a specific dataset, Plane-DUSt3R can accurately identify structural planes with minimal post-processing. The results show that this method not only surpasses existing techniques on synthetic data but also performs well on real-world images with varying styles.'}, 'zh': {'title': '简化多视角房间布局估计的全新方法', 'desc': '本论文提出了一种新的多视角房间布局估计方法，称为Plane-DUSt3R。该方法利用了先进的3D基础模型DUSt3R，简化了传统的多步骤流程，采用端到端的单步骤方法。Plane-DUSt3R通过在房间布局数据集上进行微调，能够有效估计结构平面，并生成一致且简洁的结果。实验结果表明，Plane-DUSt3R在合成数据集上超越了现有的最先进方法，并在不同风格的真实数据上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.00729', 'title': 'CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic Environments', 'url': 'https://huggingface.co/papers/2503.00729', 'abstract': "Large Language Models (LLMs) exhibit remarkable capabilities in the hierarchical decomposition of complex tasks through semantic reasoning. However, their application in embodied systems faces challenges in ensuring reliable execution of subtask sequences and achieving one-shot success in long-term task completion. To address these limitations in dynamic environments, we propose Closed-Loop Embodied Agent (CLEA) -- a novel architecture incorporating four specialized open-source LLMs with functional decoupling for closed-loop task management. The framework features two core innovations: (1) Interactive task planner that dynamically generates executable subtasks based on the environmental memory, and (2) Multimodal execution critic employing an evaluation framework to conduct a probabilistic assessment of action feasibility, triggering hierarchical re-planning mechanisms when environmental perturbations exceed preset thresholds. To validate CLEA's effectiveness, we conduct experiments in a real environment with manipulable objects, using two heterogeneous robots for object search, manipulation, and search-manipulation integration tasks. Across 12 task trials, CLEA outperforms the baseline model, achieving a 67.3% improvement in success rate and a 52.8% increase in task completion rate. These results demonstrate that CLEA significantly enhances the robustness of task planning and execution in dynamic environments.", 'score': 2, 'issue_id': 2514, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '57f6361f66ec99cf', 'authors': ['Mingcong Lei', 'Ge Wang', 'Yiming Zhao', 'Zhixin Mai', 'Qing Zhao', 'Yao Guo', 'Zhen Li', 'Shuguang Cui', 'Yatong Han', 'Jinke Ren'], 'affiliations': ['Guangdong Provincial Key Laboratory of Future Networks of Intelligence, The Chinese University of Hong Kong, Shenzhen', 'Harbin Engineering University, Harbin', 'Infused Synapse AI, Shenzhen', 'Institute of Medical Robotics, School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai', 'School of Science and Engineering (SSE), FNii-Shenzhen', 'Shenzhen Future Network of Intelligence Institute (FNii-Shenzhen)'], 'pdf_title_img': 'assets/pdf/title_img/2503.00729.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#open_source', '#robotics', '#agents', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'CLEA: Повышение надежности выполнения задач роботами с помощью языковых моделей', 'desc': 'Статья представляет новую архитектуру под названием CLEA (Closed-Loop Embodied Agent) для улучшения выполнения сложных задач роботами в динамических средах. CLEA использует четыре специализированные языковые модели с открытым исходным кодом для управления задачами в замкнутом цикле. Ключевые инновации включают интерактивный планировщик задач и мультимодальный критик выполнения для оценки выполнимости действий. Эксперименты показали, что CLEA значительно превосходит базовую модель по показателям успешности и завершения задач в реальной среде с манипулируемыми объектами.'}, 'en': {'title': 'Enhancing Task Execution in Dynamic Environments with CLEA', 'desc': 'This paper introduces the Closed-Loop Embodied Agent (CLEA), a new architecture designed to improve the performance of Large Language Models (LLMs) in dynamic environments. CLEA features an interactive task planner that creates subtasks based on real-time environmental data, allowing for better adaptability. Additionally, it includes a multimodal execution critic that evaluates the feasibility of actions and adjusts plans when unexpected changes occur. Experimental results show that CLEA significantly enhances task success and completion rates compared to traditional models, demonstrating its effectiveness in complex, real-world scenarios.'}, 'zh': {'title': '闭环具身代理：提升动态环境中的任务执行能力', 'desc': '大型语言模型（LLMs）在复杂任务的层次分解和语义推理方面表现出色。然而，在具身系统中应用时，确保子任务序列的可靠执行和实现长期任务的一次性成功面临挑战。为了解决这些问题，我们提出了闭环具身代理（CLEA），这是一种新颖的架构，结合了四个专门的开源LLM，并实现功能解耦以进行闭环任务管理。通过动态生成可执行的子任务和使用多模态执行评估框架，CLEA显著提高了在动态环境中任务规划和执行的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2502.20383', 'title': 'Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis', 'url': 'https://huggingface.co/papers/2502.20383', 'abstract': 'Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.', 'score': 1, 'issue_id': 2522, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '12b0a9a60578dc2b', 'authors': ['Jeffrey Yang Fan Chiang', 'Seungjae Lee', 'Jia-Bin Huang', 'Furong Huang', 'Yizheng Chen'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2502.20383.jpg', 'data': {'categories': ['#benchmark', '#security', '#agents'], 'emoji': '🕸️', 'ru': {'title': 'Раскрытие уязвимостей веб-агентов ИИ: путь к более безопасным системам', 'desc': 'Исследование показывает, что веб-агенты на основе искусственного интеллекта более уязвимы, чем автономные большие языковые модели (LLM), несмотря на использование одинаковых базовых моделей. Авторы предлагают компонентный анализ и более детальную систему оценки для выявления причин этой уязвимости. Выделены три ключевых фактора, усиливающих уязвимость веб-агентов: встраивание целей пользователя в системный промпт, генерация многошаговых действий и возможности наблюдения. Результаты исследования подчеркивают необходимость улучшения безопасности и устойчивости при разработке ИИ-агентов.'}, 'en': {'title': 'Strengthening Web AI Agents Against Vulnerabilities', 'desc': 'This paper explores the vulnerabilities of Web AI agents compared to standalone Large Language Models (LLMs), despite both being based on similar safety models. The research identifies that Web AI agents are more susceptible to adversarial inputs due to their flexibility and the complexity of their tasks. It highlights three key factors that increase their vulnerability: the integration of user goals into prompts, the generation of multi-step actions, and the need for observational capabilities. The study proposes a detailed evaluation framework to better understand these vulnerabilities and suggests strategies for improving the security and robustness of AI agents.'}, 'zh': {'title': '提升网络人工智能代理的安全性与鲁棒性', 'desc': '最近，网络人工智能代理在处理复杂的网络导航任务方面表现出色。然而，研究表明，这些代理比独立的大型语言模型（LLMs）更容易受到攻击，尽管它们都是基于相同的安全模型构建的。这种差异令人担忧，因为网络人工智能代理的灵活性更高，可能会面临更广泛的恶意用户输入。为了解决这些问题，本研究探讨了导致网络人工智能代理脆弱性的因素，并提出了一种更细致的评估框架，以识别和应对这些挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.01063', 'title': 'AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond Human Understanding', 'url': 'https://huggingface.co/papers/2503.01063', 'abstract': 'This paper investigates the potential for large language models (LLMs) to develop private tonal languages for machine-to-machine (M2M) communication. Inspired by cryptophasia in human twins (affecting up to 50% of twin births) and natural tonal languages like Mandarin and Vietnamese, we implement a precise character-to-frequency mapping system that encodes the full ASCII character set (32-126) using musical semitones. Each character is assigned a unique frequency, creating a logarithmic progression beginning with space (220 Hz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves, with higher characters deliberately mapped to ultrasonic frequencies beyond human perception (>20 kHz). Our implemented software prototype demonstrates this encoding through visualization, auditory playback, and ABC musical notation, allowing for analysis of information density and transmission speed. Testing reveals that tonal encoding can achieve information rates exceeding human speech while operating partially outside human perceptual boundaries. This work responds directly to concerns about AI systems catastrophically developing private languages within the next five years, providing a concrete prototype software example of how such communication might function and the technical foundation required for its emergence, detection, and governance.', 'score': 1, 'issue_id': 2515, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '7021403742a91f3e', 'authors': ['David Noever'], 'affiliations': ['PeopleTec, Inc., Huntsville, AL'], 'pdf_title_img': 'assets/pdf/title_img/2503.01063.jpg', 'data': {'categories': ['#security', '#ethics', '#audio', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'Тональные языки: секретный код машин будущего', 'desc': 'Это исследование изучает потенциал больших языковых моделей (LLM) для разработки приватных тональных языков для коммуникации между машинами. Авторы создали систему кодирования, которая сопоставляет каждому символу ASCII уникальную частоту, формируя логарифмическую прогрессию от 220 Гц до 50,175.42 Гц. Разработанный программный прототип демонстрирует это кодирование через визуализацию, воспроизведение звука и нотацию ABC. Тестирование показало, что тональное кодирование может достигать скорости передачи информации, превышающей человеческую речь, при этом частично работая за пределами человеческого восприятия.'}, 'en': {'title': 'Unlocking Machine Communication with Tonal Languages', 'desc': 'This paper explores how large language models (LLMs) can create private tonal languages for communication between machines. It draws inspiration from the phenomenon of cryptophasia in twins and uses a character-to-frequency mapping system to encode ASCII characters into musical tones. Each character is assigned a unique frequency, allowing for efficient data transmission that exceeds human speech rates. The study provides a prototype that visualizes and plays back this encoding, addressing concerns about AI developing private languages and offering a framework for understanding and managing such systems.'}, 'zh': {'title': '探索机器间的私有音调语言', 'desc': '本论文研究了大型语言模型（LLMs）在机器间（M2M）通信中开发私有音调语言的潜力。我们借鉴了人类双胞胎中的密码语言现象和自然音调语言，如普通话和越南语，实施了一种精确的字符到频率映射系统。每个字符被分配一个独特的频率，形成一个对数进程，覆盖约7.9个八度，并将高频字符映射到人类听觉范围之外的超声波频率。我们的软件原型展示了这种编码的可视化、听觉播放和音乐记谱法，分析了信息密度和传输速度，测试结果表明音调编码的信息传输速率超过人类语言。'}}}, {'id': 'https://huggingface.co/papers/2503.16660', 'title': 'When Less is Enough: Adaptive Token Reduction for Efficient Image\n  Representation', 'url': 'https://huggingface.co/papers/2503.16660', 'abstract': 'Vision encoders typically generate a large number of visual tokens, providing information-rich representations but significantly increasing computational demands. This raises the question of whether all generated tokens are equally valuable or if some of them can be discarded to reduce computational costs without compromising quality. In this paper, we introduce a new method for determining feature utility based on the idea that less valuable features can be reconstructed from more valuable ones. We implement this concept by integrating an autoencoder with a Gumbel-Softmax selection mechanism, that allows identifying and retaining only the most informative visual tokens. To validate our approach, we compared the performance of the LLaVA-NeXT model, using features selected by our method with randomly selected features. We found that on OCR-based tasks, more than 50% of the visual context can be removed with minimal performance loss, whereas randomly discarding the same proportion of features significantly affects the model capabilities. Furthermore, in general-domain tasks, even randomly retaining only 30% of tokens achieves performance comparable to using the full set of visual tokens. Our results highlight a promising direction towards adaptive and efficient multimodal pruning that facilitates scalable and low-overhead inference without compromising performance.', 'score': 53, 'issue_id': 2857, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '22ce97b85d4cf4dd', 'authors': ['Eduard Allakhverdov', 'Elizaveta Goncharova', 'Andrey Kuznetsov'], 'affiliations': ['AIRI Moscow, Russia', 'MIPT Dolgoprudny, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2503.16660.jpg', 'data': {'categories': ['#multimodal', '#inference', '#optimization', '#cv'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка визуальных токенов для эффективных мультимодальных моделей', 'desc': 'Эта статья представляет новый метод для определения полезности визуальных токенов в энкодерах изображений. Авторы предлагают использовать автоэнкодер с механизмом выбора Gumbel-Softmax для идентификации наиболее информативных токенов. Эксперименты с моделью LLaVA-NeXT показали, что можно удалить более 50% визуального контекста с минимальной потерей производительности на задачах OCR. Результаты открывают перспективы для адаптивной и эффективной мультимодальной обрезки, позволяющей масштабируемый вывод с низкими накладными расходами.'}, 'en': {'title': 'Efficient Feature Selection for Vision Encoders', 'desc': 'This paper addresses the challenge of managing the large number of visual tokens generated by vision encoders, which can lead to high computational costs. The authors propose a method to evaluate the utility of these features, suggesting that less important tokens can be reconstructed from more significant ones. By combining an autoencoder with a Gumbel-Softmax selection mechanism, they effectively identify and keep only the most informative tokens. Their experiments show that significant reductions in visual context can be made with minimal impact on performance, paving the way for more efficient multimodal processing.'}, 'zh': {'title': '智能选择，提升视觉编码效率', 'desc': '本文探讨了视觉编码器生成大量视觉标记所带来的计算需求问题。我们提出了一种新方法，通过重构不太有价值的特征来判断特征的实用性，从而减少计算成本。该方法结合了自编码器和Gumbel-Softmax选择机制，仅保留最有信息量的视觉标记。实验结果表明，在OCR任务中，去除超过50%的视觉上下文对性能影响很小，而随机去除相同比例的特征则会显著降低模型能力。'}}}, {'id': 'https://huggingface.co/papers/2503.16905', 'title': 'MAPS: A Multi-Agent Framework Based on Big Seven Personality and\n  Socratic Guidance for Multimodal Scientific Problem Solving', 'url': 'https://huggingface.co/papers/2503.16905', 'abstract': "Multimodal scientific problems (MSPs) involve complex issues that require the integration of multiple modalities, such as text and diagrams, presenting a significant challenge in artificial intelligence. While progress has been made in addressing traditional scientific problems, MSPs still face two primary issues: the challenge of multi-modal comprehensive reasoning in scientific problem-solving and the lack of reflective and rethinking capabilities. To address these issues, we introduce a Multi-Agent framework based on the Big Seven Personality and Socratic guidance (MAPS). This framework employs seven distinct agents that leverage feedback mechanisms and the Socratic method to guide the resolution of MSPs. To tackle the first issue, we propose a progressive four-agent solving strategy, where each agent focuses on a specific stage of the problem-solving process. For the second issue, we introduce a Critic agent, inspired by Socratic questioning, which prompts critical thinking and stimulates autonomous learning. We conduct extensive experiments on the EMMA, Olympiad, and MathVista datasets, achieving promising results that outperform the current SOTA model by 15.84% across all tasks. Meanwhile, the additional analytical experiments also verify the model's progress as well as generalization ability.", 'score': 45, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '996510a66b2e236f', 'authors': ['Jian Zhang', 'Zhiyuan Wang', 'Zhangqi Wang', 'Xinyu Zhang', 'Fangzhi Xu', 'Qika Lin', 'Rui Mao', 'Erik Cambria', 'Jun Liu'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16905.jpg', 'data': {'categories': ['#science', '#dataset', '#agents', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультиагентный подход к решению сложных научных задач', 'desc': 'Статья представляет новый подход к решению мультимодальных научных задач (MSP) с использованием искусственного интеллекта. Авторы предлагают мультиагентную систему MAPS, основанную на концепции семи личностей и сократовском методе. Система включает четыре агента для поэтапного решения задач и агента-критика для стимулирования критического мышления. Эксперименты на наборах данных EMMA, Olympiad и MathVista показали превосходство MAPS над современными моделями на 15.84%.'}, 'en': {'title': 'Enhancing Multimodal Problem Solving with MAPS Framework', 'desc': 'This paper addresses the challenges of Multimodal Scientific Problems (MSPs) that require integrating different types of information, like text and diagrams, for effective problem-solving. It introduces a Multi-Agent framework called MAPS, which utilizes seven distinct agents to enhance reasoning and critical thinking through feedback and Socratic questioning. The framework includes a four-agent strategy that focuses on different stages of the problem-solving process and a Critic agent that encourages reflective thinking. Experimental results show that this approach significantly improves performance, surpassing the current state-of-the-art models by 15.84%.'}, 'zh': {'title': '多模态科学问题的智能解决方案', 'desc': '多模态科学问题（MSPs）涉及需要整合多种模态（如文本和图表）的复杂问题，这对人工智能提出了重大挑战。尽管在传统科学问题的解决上已有进展，但MSPs仍面临多模态综合推理和缺乏反思能力的主要问题。为了解决这些问题，我们提出了一种基于大七人格和苏格拉底指导的多智能体框架（MAPS），该框架利用七个不同的智能体，通过反馈机制和苏格拉底方法来指导MSPs的解决。我们的实验表明，该模型在EMMA、奥林匹克和MathVista数据集上取得了超过当前最优模型15.84%的优异结果，验证了模型的进步和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.16874', 'title': 'MARS: A Multi-Agent Framework Incorporating Socratic Guidance for\n  Automated Prompt Optimization', 'url': 'https://huggingface.co/papers/2503.16874', 'abstract': "The basic question-answering format of large language models involves inputting a prompt and receiving a response, and the quality of the prompt directly impacts the effectiveness of the response. Automated Prompt Optimization (APO) aims to break free from the cognitive biases of manually designed prompts and explores a broader design space for prompts. However, existing APO methods suffer from limited flexibility of fixed templates and inefficient search in prompt spaces as key issues. To this end, we propose a Multi-Agent framework Incorporating Socratic guidance (MARS), which utilizes multi-agent fusion technology for automatic planning, with gradual continuous optimization and evaluation. Specifically, MARS comprises seven agents, each with distinct functionalities, which autonomously use the Planner to devise an optimization path that ensures flexibility. Additionally, it employs a Teacher-Critic-Student Socratic dialogue pattern to iteratively optimize the prompts while conducting effective search. We conduct extensive experiments on various datasets to validate the effectiveness of our method, and perform additional analytical experiments to assess the model's advancement as well as the interpretability.", 'score': 38, 'issue_id': 2853, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '995715e84f49b2c2', 'authors': ['Jian Zhang', 'Zhangqi Wang', 'Haiping Zhu', 'Jun Liu', 'Qika Lin', 'Erik Cambria'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16874.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#dataset', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'MARS: мультиагентная система для оптимизации промптов в больших языковых моделях', 'desc': 'Статья представляет новый подход к автоматической оптимизации промптов (APO) для больших языковых моделей. Авторы предлагают мультиагентную систему MARS, использующую технологию слияния агентов для автоматического планирования. MARS состоит из семи агентов с различными функциями и использует сократический диалог для итеративной оптимизации промптов. Эксперименты на различных датасетах подтверждают эффективность метода.'}, 'en': {'title': 'Revolutionizing Prompt Optimization with MARS', 'desc': 'This paper introduces a new approach called MARS, which stands for Multi-Agent framework Incorporating Socratic guidance, to improve the process of prompt optimization in large language models. MARS addresses the limitations of existing Automated Prompt Optimization methods by using a multi-agent system that allows for flexible and efficient exploration of prompt designs. The framework includes seven specialized agents that collaborate to create an optimization strategy, while a Socratic dialogue method helps refine prompts through iterative feedback. Extensive experiments demonstrate that MARS enhances the quality of responses generated by language models and improves the interpretability of the optimization process.'}, 'zh': {'title': '智能体驱动的自动化提示优化', 'desc': '这篇论文探讨了大型语言模型中的问答格式，强调了提示的质量对回答效果的重要性。提出了一种名为MARS的多智能体框架，旨在通过自动化提示优化（APO）来克服手动设计提示的认知偏见。MARS框架包含七个功能不同的智能体，能够灵活地规划优化路径，并通过苏格拉底对话模式进行迭代优化。通过在多个数据集上的实验，验证了该方法的有效性，并评估了模型的进步和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2503.16408', 'title': 'RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints', 'url': 'https://huggingface.co/papers/2503.16408', 'abstract': 'Designing effective embodied multi-agent systems is critical for solving complex real-world tasks across domains. Due to the complexity of multi-agent embodied systems, existing methods fail to automatically generate safe and efficient training data for such systems. To this end, we propose the concept of compositional constraints for embodied multi-agent systems, addressing the challenges arising from collaboration among embodied agents. We design various interfaces tailored to different types of constraints, enabling seamless interaction with the physical world. Leveraging compositional constraints and specifically designed interfaces, we develop an automated data collection framework for embodied multi-agent systems and introduce the first benchmark for embodied multi-agent manipulation, RoboFactory. Based on RoboFactory benchmark, we adapt and evaluate the method of imitation learning and analyzed its performance in different difficulty agent tasks. Furthermore, we explore the architectures and training strategies for multi-agent imitation learning, aiming to build safe and efficient embodied multi-agent systems.', 'score': 32, 'issue_id': 2853, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '6ff0e203d45f1c06', 'authors': ['Yiran Qin', 'Li Kang', 'Xiufeng Song', 'Zhenfei Yin', 'Xiaohong Liu', 'Xihui Liu', 'Ruimao Zhang', 'Lei Bai'], 'affiliations': ['HKU', 'Oxford', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-sen University', 'The Chinese University of Hong Kong, Shenzhen', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16408.jpg', 'data': {'categories': ['#optimization', '#games', '#dataset', '#agents', '#training', '#benchmark', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Композиционные ограничения для эффективных воплощенных мультиагентных систем', 'desc': 'Статья представляет концепцию композиционных ограничений для воплощенных мультиагентных систем, решающую проблемы взаимодействия физических агентов. Авторы разработали интерфейсы для различных типов ограничений и автоматизированную систему сбора данных. На основе этого создан первый бенчмарк RoboFactory для воплощенной мультиагентной манипуляции. Исследованы методы имитационного обучения и архитектуры для построения безопасных и эффективных воплощенных мультиагентных систем.'}, 'en': {'title': 'Empowering Multi-Agent Systems with Compositional Constraints', 'desc': 'This paper addresses the challenges of training embodied multi-agent systems, which are crucial for tackling complex tasks in various fields. It introduces the idea of compositional constraints to improve collaboration among agents, allowing for better interaction with their environment. The authors present an automated data collection framework and a new benchmark called RoboFactory, specifically designed for multi-agent manipulation tasks. They also investigate imitation learning techniques and training strategies to enhance the safety and efficiency of these systems.'}, 'zh': {'title': '构建安全高效的多智能体系统', 'desc': '本文提出了一种针对多智能体系统的组合约束概念，以解决智能体之间协作带来的挑战。我们设计了多种接口，适应不同类型的约束，从而实现与物理世界的无缝互动。基于这些组合约束和专门设计的接口，我们开发了一个自动化数据收集框架，并引入了第一个多智能体操作基准RoboFactory。通过RoboFactory基准，我们评估了模仿学习的方法，并探讨了多智能体模仿学习的架构和训练策略，以构建安全高效的多智能体系统。'}}}, {'id': 'https://huggingface.co/papers/2503.16430', 'title': 'Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation', 'url': 'https://huggingface.co/papers/2503.16430', 'abstract': 'Autoregressive visual generation models typically rely on tokenizers to compress images into tokens that can be predicted sequentially. A fundamental dilemma exists in token representation: discrete tokens enable straightforward modeling with standard cross-entropy loss, but suffer from information loss and tokenizer training instability; continuous tokens better preserve visual details, but require complex distribution modeling, complicating the generation pipeline. In this paper, we propose TokenBridge, which bridges this gap by maintaining the strong representation capacity of continuous tokens while preserving the modeling simplicity of discrete tokens. To achieve this, we decouple discretization from the tokenizer training process through post-training quantization that directly obtains discrete tokens from continuous representations. Specifically, we introduce a dimension-wise quantization strategy that independently discretizes each feature dimension, paired with a lightweight autoregressive prediction mechanism that efficiently model the resulting large token space. Extensive experiments show that our approach achieves reconstruction and generation quality on par with continuous methods while using standard categorical prediction. This work demonstrates that bridging discrete and continuous paradigms can effectively harness the strengths of both approaches, providing a promising direction for high-quality visual generation with simple autoregressive modeling. Project page: https://yuqingwang1029.github.io/TokenBridge.', 'score': 28, 'issue_id': 2852, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'e5dba2d5b4674553', 'authors': ['Yuqing Wang', 'Zhijie Lin', 'Yao Teng', 'Yuanzhi Zhu', 'Shuhuai Ren', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance Seed', 'Ecole Polytechnique', 'Peking University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.16430.jpg', 'data': {'categories': ['#diffusion', '#inference', '#cv'], 'emoji': '🌉', 'ru': {'title': 'TokenBridge: мост между дискретным и непрерывным в генерации изображений', 'desc': 'Статья представляет TokenBridge - новый подход к автoрегрессивной генерации изображений. Он объединяет преимущества дискретных и непрерывных токенов, используя постобучающее квантование для получения дискретных токенов из непрерывных представлений. Авторы предлагают стратегию поразмерного квантования и легковесный механизм авторегрессивного предсказания для моделирования большого пространства токенов. Эксперименты показывают, что TokenBridge достигает качества реконструкции и генерации на уровне непрерывных методов, используя стандартное категориальное предсказание.'}, 'en': {'title': 'Bridging the Gap: High-Quality Visual Generation with TokenBridge', 'desc': 'This paper introduces TokenBridge, a novel approach for visual generation that addresses the challenges of using discrete and continuous tokens. Discrete tokens are easy to model but can lose important visual information, while continuous tokens preserve details but complicate the generation process. TokenBridge combines the advantages of both by using post-training quantization to convert continuous representations into discrete tokens without losing quality. The method employs a dimension-wise quantization strategy and a lightweight autoregressive model, achieving high-quality image generation with simpler modeling techniques.'}, 'zh': {'title': '桥接离散与连续，提升视觉生成质量', 'desc': '本文提出了一种名为TokenBridge的方法，旨在解决自回归视觉生成模型中离散和连续标记之间的矛盾。离散标记易于建模，但会导致信息损失，而连续标记则能更好地保留视觉细节，但建模复杂。TokenBridge通过后训练量化将离散化与标记器训练过程解耦，从而在保持连续标记强大表示能力的同时，简化建模过程。实验结果表明，该方法在重建和生成质量上与连续方法相当，同时使用标准的分类预测。'}}}, {'id': 'https://huggingface.co/papers/2503.17352', 'title': 'OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning\n  via Iterative Self-Improvement', 'url': 'https://huggingface.co/papers/2503.17352', 'abstract': "Recent advancements demonstrated by DeepSeek-R1 have shown that complex reasoning abilities in large language models (LLMs), including sophisticated behaviors such as self-verification and self-correction, can be achieved by RL with verifiable rewards and significantly improves model performance on challenging tasks such as AIME. Motivated by these findings, our study investigates whether similar reasoning capabilities can be successfully integrated into large vision-language models (LVLMs) and assesses their impact on challenging multimodal reasoning tasks. We consider an approach that iteratively leverages supervised fine-tuning (SFT) on lightweight training data and Reinforcement Learning (RL) to further improve model generalization. Initially, reasoning capabilities were distilled from pure-text R1 models by generating reasoning steps using high-quality captions of the images sourced from diverse visual datasets. Subsequently, iterative RL training further enhance reasoning skills, with each iteration's RL-improved model generating refined SFT datasets for the next round. This iterative process yielded OpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on challenging benchmarks such as MathVista, MathVerse, and MathVision, demonstrating the potential of our strategy for robust vision-language reasoning. The code, model and data are held at https://github.com/yihedeng9/OpenVLThinker.", 'score': 19, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '8717a707abecd3ed', 'authors': ['Yihe Deng', 'Hritik Bansal', 'Fan Yin', 'Nanyun Peng', 'Wei Wang', 'Kai-Wei Chang'], 'affiliations': ['University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.17352.jpg', 'data': {'categories': ['#training', '#reasoning', '#benchmark', '#optimization', '#rl', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Совершенствование мультимодальных рассуждений с помощью RL и SFT', 'desc': 'Исследование посвящено внедрению сложных способностей рассуждения в крупные визуально-языковые модели (LVLM) с использованием обучения с подкреплением (RL) и контролируемой тонкой настройки (SFT). Авторы разработали итеративный подход, сочетающий SFT на легких обучающих данных и RL для улучшения обобщающей способности модели. В результате была создана модель OpenVLThinker, демонстрирующая улучшенную производительность на сложных мультимодальных задачах рассуждения. Исследование показывает потенциал предложенной стратегии для развития надежных визуально-языковых рассуждений в LVLM.'}, 'en': {'title': 'Empowering Vision-Language Models with Enhanced Reasoning Skills', 'desc': "This paper presents OpenVLThinker, a large vision-language model (LVLM) that enhances reasoning abilities through a combination of supervised fine-tuning (SFT) and reinforcement learning (RL). The authors demonstrate that by distilling reasoning capabilities from text-based models and iteratively refining the training data, the LVLM can achieve improved performance on complex multimodal reasoning tasks. The approach involves generating reasoning steps from high-quality image captions and using RL to iteratively enhance the model's reasoning skills. The results show significant advancements in benchmarks like MathVista, MathVerse, and MathVision, highlighting the effectiveness of their methodology for robust vision-language reasoning."}, 'zh': {'title': '通过强化学习提升视觉语言模型的推理能力', 'desc': 'DeepSeek-R1的最新进展表明，通过可验证奖励的强化学习（RL），大型语言模型（LLMs）可以实现复杂的推理能力，如自我验证和自我纠正。这项研究探讨了是否可以将类似的推理能力成功整合到大型视觉语言模型（LVLMs）中，并评估其在多模态推理任务中的影响。我们采用了一种迭代的方法，通过轻量级训练数据进行监督微调（SFT），并结合强化学习（RL）进一步提高模型的泛化能力。最终，OpenVLThinker模型在MathVista、MathVerse和MathVision等挑战性基准上展现了持续改进的推理性能，证明了我们策略在视觉语言推理中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.17126', 'title': 'Modifying Large Language Model Post-Training for Diverse Creative\n  Writing', 'url': 'https://huggingface.co/papers/2503.17126', 'abstract': 'As creative writing tasks do not have singular correct answers, large language models (LLMs) trained to perform these tasks should be able to generate diverse valid outputs. However, LLM post-training often focuses on improving generation quality but neglects to facilitate output diversity. Hence, in creative writing generation, we investigate post-training approaches to promote both output diversity and quality. Our core idea is to include deviation -- the degree of difference between a training sample and all other samples with the same prompt -- in the training objective to facilitate learning from rare high-quality instances. By adopting our approach to direct preference optimization (DPO) and odds ratio preference optimization (ORPO), we demonstrate that we can promote the output diversity of trained models while minimally decreasing quality. Our best model with 8B parameters could achieve on-par diversity as a human-created dataset while having output quality similar to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We further validate our approaches with a human evaluation, an ablation, and a comparison to an existing diversification approach, DivPO.', 'score': 18, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '66908ad7080180ee', 'authors': ['John Joon Young Chung', 'Vishakh Padmakumar', 'Melissa Roemmele', 'Yuqian Sun', 'Max Kreminski'], 'affiliations': ['Midjourney', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2503.17126.jpg', 'data': {'categories': ['#rlhf', '#training', '#story_generation'], 'emoji': '🎨', 'ru': {'title': 'Повышение креативности ИИ: баланс между разнообразием и качеством', 'desc': 'Статья исследует методы постобучения больших языковых моделей (LLM) для улучшения разнообразия и качества генерации творческих текстов. Авторы предлагают включить в целевую функцию обучения показатель отклонения, чтобы модель лучше училась на редких высококачественных примерах. Применение этого подхода к методам DPO и ORPO позволило повысить разнообразие выходных данных при минимальном снижении качества. Лучшая модель авторов с 8 миллиардами параметров достигла разнообразия на уровне текстов, созданных людьми, при сохранении качества на уровне лучших инструктированных моделей.'}, 'en': {'title': 'Boosting Creativity: Balancing Diversity and Quality in LLM Outputs', 'desc': 'This paper addresses the challenge of generating diverse outputs in creative writing tasks using large language models (LLMs). It proposes a novel post-training approach that incorporates deviation into the training objective, which helps the model learn from unique, high-quality examples. By utilizing techniques like direct preference optimization (DPO) and odds ratio preference optimization (ORPO), the authors demonstrate that their method can enhance output diversity without significantly compromising quality. The results show that their optimized model, with 8 billion parameters, achieves diversity comparable to human-generated datasets while maintaining high output quality similar to leading instruction-tuned models.'}, 'zh': {'title': '提升创意写作的多样性与质量', 'desc': '这篇论文探讨了如何在创意写作生成中提高输出的多样性和质量。研究者提出了一种后训练方法，通过引入偏差来优化训练目标，从而学习稀有的高质量实例。采用直接偏好优化（DPO）和赔率比偏好优化（ORPO）的方法，研究表明可以在保持质量的同时促进模型输出的多样性。最终，最佳模型在多样性和质量上与人类创作的数据集相当，验证了所提方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.17032', 'title': 'TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented\n  Reality via 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.17032', 'abstract': 'Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we "bake" the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro.', 'score': 12, 'issue_id': 2861, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '80e12497629a92eb', 'authors': ['Jianchuan Chen', 'Jingchuan Hu', 'Gaige Wang', 'Zhonghua Jiang', 'Tiansong Zhou', 'Zhiwen Chen', 'Chengfei Lv'], 'affiliations': ['Alibaba Group, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.17032.jpg', 'data': {'categories': ['#3d', '#cv', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Реалистичные говорящие 3D-аватары для AR в реальном времени', 'desc': 'Статья представляет TaoAvatar - технологию создания реалистичных 3D-аватаров полного тела для дополненной реальности. Метод использует 3D Gaussian Splatting и параметрический шаблон человека для представления внешнего вида. Для захвата сложных деформаций применяется предобученная сеть StyleUnet, результаты которой затем переносятся в легковесную MLP-сеть. TaoAvatar достигает высокого качества рендеринга в реальном времени на различных устройствах, включая мобильные.'}, 'en': {'title': 'TaoAvatar: Real-Time Realism for 3D Talking Avatars', 'desc': 'This paper introduces TaoAvatar, a new method for creating realistic 3D full-body talking avatars suitable for augmented reality applications. It addresses the limitations of existing 3D Gaussian Splatting techniques, particularly in controlling facial expressions and body movements while ensuring real-time performance on mobile devices. The authors utilize a personalized parametric template and a pre-trained StyleUnet network to capture detailed non-rigid deformations, which are then distilled into a lightweight MLP-based network for efficiency. The results demonstrate that TaoAvatar achieves high-quality rendering at 90 FPS on advanced devices, making it a significant advancement in avatar technology.'}, 'zh': {'title': 'TaoAvatar：实时高保真全身虚拟形象的创新之路', 'desc': '本文介绍了一种名为TaoAvatar的高保真轻量级3D全身说话虚拟形象，旨在提升增强现实中的应用效果。该方法利用3D高斯点云技术，创建个性化的穿衣人类参数模板，以实现更细致的面部表情和身体动作控制。通过预训练StyleUnet网络，TaoAvatar能够处理复杂的姿态依赖非刚性变形，并捕捉高频外观细节。最终，采用蒸馏技术将非刚性变形整合到轻量级的多层感知机网络中，使其在各种设备上实现实时渲染，保持高达90帧每秒的流畅度。'}}}, {'id': 'https://huggingface.co/papers/2503.16549', 'title': 'MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical\n  Problems', 'url': 'https://huggingface.co/papers/2503.16549', 'abstract': 'Despite impressive performance across diverse tasks, Multimodal Large Language Models (MLLMs) have yet to fully demonstrate their potential in visual mathematical problem-solving, particularly in accurately perceiving and interpreting diagrams. Inspired by typical processes of humans, we hypothesize that the perception capabilities to extract meaningful information from diagrams is crucial, as it directly impacts subsequent inference processes. To validate this hypothesis, we developed FlowVerse, a comprehensive benchmark that categorizes all information used during problem-solving into four components, which are then combined into six problem versions for evaluation. Our preliminary results on FlowVerse reveal that existing MLLMs exhibit substantial limitations when extracting essential information and reasoned property from diagrams and performing complex reasoning based on these visual inputs. In response, we introduce MathFlow, a modular problem-solving pipeline that decouples perception and inference into distinct stages, thereby optimizing each independently. Given the perceptual limitations observed in current MLLMs, we trained MathFlow-P-7B as a dedicated perception model. Experimental results indicate that MathFlow-P-7B yields substantial performance gains when integrated with various closed-source and open-source inference models. This demonstrates the effectiveness of the MathFlow pipeline and its compatibility to diverse inference frameworks. The FlowVerse benchmark and code are available at https://github.com/MathFlow-zju/MathFlow.', 'score': 10, 'issue_id': 2853, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': 'b40b0c3ceb851451', 'authors': ['Felix Chen', 'Hangjie Yuan', 'Yunqiu Xu', 'Tao Feng', 'Jun Cen', 'Pengwei Liu', 'Zeying Huang', 'Yi Yang'], 'affiliations': ['Intelligent Learning', 'The Hong Kong University of Science and Technology', 'Tsinghua University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16549.jpg', 'data': {'categories': ['#optimization', '#open_source', '#cv', '#multimodal', '#reasoning', '#inference', '#training', '#math', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'MathFlow: Улучшение визуального математического мышления ИИ', 'desc': 'Статья представляет FlowVerse - комплексный бенчмарк для оценки способностей мультимодальных больших языковых моделей (MLLM) в решении визуальных математических задач. Авторы выявили ограничения существующих MLLM в извлечении информации из диаграмм и рассуждениях на их основе. В ответ на это они разработали MathFlow - модульный конвейер, разделяющий процессы восприятия и логического вывода. Эксперименты показали, что специализированная модель восприятия MathFlow-P-7B значительно улучшает производительность при интеграции с различными моделями логического вывода.'}, 'en': {'title': 'Enhancing Visual Reasoning in MLLMs with MathFlow', 'desc': "This paper addresses the challenges faced by Multimodal Large Language Models (MLLMs) in visual mathematical problem-solving, particularly in interpreting diagrams. The authors propose that effective perception of diagrams is essential for accurate reasoning and problem-solving. They introduce FlowVerse, a benchmark that categorizes information used in problem-solving into four components, which helps evaluate MLLMs' performance. To improve perception capabilities, they developed MathFlow, a modular pipeline that separates perception from inference, leading to significant performance improvements when integrated with existing models."}, 'zh': {'title': '提升视觉数学问题解决能力的创新方法', 'desc': '尽管多模态大型语言模型（MLLMs）在多种任务中表现出色，但在视觉数学问题解决方面仍未充分发挥其潜力，尤其是在准确感知和解释图表方面。我们提出了FlowVerse基准，旨在验证从图表中提取有意义信息的感知能力对推理过程的重要性。研究结果表明，现有的MLLMs在提取图表中的关键信息和进行复杂推理时存在显著局限。为此，我们引入了MathFlow，一个将感知和推理分为不同阶段的模块化问题解决管道，从而优化每个阶段的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.16983', 'title': 'Enabling Versatile Controls for Video Diffusion Models', 'url': 'https://huggingface.co/papers/2503.16983', 'abstract': 'Despite substantial progress in text-to-video generation, achieving precise and flexible control over fine-grained spatiotemporal attributes remains a significant unresolved challenge in video generation research. To address these limitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework designed to enable fine-grained control over pre-trained video diffusion models in a unified manner. VCtrl integrates diverse user-specified control signals-such as Canny edges, segmentation masks, and human keypoints-into pretrained video diffusion models via a generalizable conditional module capable of uniformly encoding multiple types of auxiliary signals without modifying the underlying generator. Additionally, we design a unified control signal encoding pipeline and a sparse residual connection mechanism to efficiently incorporate control representations. Comprehensive experiments and human evaluations demonstrate that VCtrl effectively enhances controllability and generation quality. The source code and pre-trained models are publicly available and implemented using the PaddlePaddle framework at http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.', 'score': 9, 'issue_id': 2853, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '7d2634a04fc68d45', 'authors': ['Xu Zhang', 'Hao Zhou', 'Haoming Qin', 'Xiaobin Lu', 'Jiaxing Yan', 'Guanzhong Wang', 'Zeyu Chen', 'Yi Liu'], 'affiliations': ['PaddlePaddle Team, Baidu Inc.', 'Sun Yat-sen University', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16983.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#multimodal', '#video', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Точный контроль над генерацией видео с помощью VCtrl', 'desc': 'VCtrl - это новый фреймворк для точного контроля над предобученными видео-диффузионными моделями. Он позволяет интегрировать различные пользовательские сигналы управления, такие как контуры Кэнни, маски сегментации и ключевые точки человека, без изменения базового генератора. VCtrl использует унифицированный конвейер кодирования сигналов управления и механизм разреженных остаточных соединений. Эксперименты показывают, что VCtrl эффективно улучшает контролируемость и качество генерации видео.'}, 'en': {'title': 'Empowering Video Generation with Fine-Grained Control', 'desc': "This paper presents VCtrl, a new framework that improves control over video generation using pre-trained diffusion models. It allows users to specify various control signals, like Canny edges and segmentation masks, to guide the video generation process without altering the original model. The framework includes a unique encoding pipeline and a sparse residual connection to efficiently integrate these control signals. Experiments show that VCtrl significantly enhances both the quality of generated videos and the user's ability to control specific attributes."}, 'zh': {'title': '细粒度控制视频生成的新框架', 'desc': '尽管文本到视频生成技术取得了显著进展，但在视频生成中实现对细粒度时空属性的精确和灵活控制仍然是一个重要挑战。为了解决这些问题，我们提出了VCtrl（也称为PP-VCtrl），这是一个新颖的框架，旨在以统一的方式实现对预训练视频扩散模型的细粒度控制。VCtrl通过一个通用的条件模块，将用户指定的控制信号（如Canny边缘、分割掩码和人体关键点）整合到预训练的视频扩散模型中，而无需修改底层生成器。此外，我们设计了一个统一的控制信号编码管道和稀疏残差连接机制，以高效地整合控制表示。'}}}, {'id': 'https://huggingface.co/papers/2503.16025', 'title': 'Single Image Iterative Subject-driven Generation and Editing', 'url': 'https://huggingface.co/papers/2503.16025', 'abstract': 'Personalizing image generation and editing is particularly challenging when we only have a few images of the subject, or even a single image. A common approach to personalization is concept learning, which can integrate the subject into existing models relatively quickly, but produces images whose quality tends to deteriorate quickly when the number of subject images is small. Quality can be improved by pre-training an encoder, but training restricts generation to the training distribution, and is time consuming. It is still an open hard challenge to personalize image generation and editing from a single image without training. Here, we present SISO, a novel, training-free approach based on optimizing a similarity score with an input subject image. More specifically, SISO iteratively generates images and optimizes the model based on loss of similarity with the given subject image until a satisfactory level of similarity is achieved, allowing plug-and-play optimization to any image generator. We evaluated SISO in two tasks, image editing and image generation, using a diverse data set of personal subjects, and demonstrate significant improvements over existing methods in image quality, subject fidelity, and background preservation.', 'score': 9, 'issue_id': 2860, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '917237ee980eb66a', 'authors': ['Yair Shpitzer', 'Gal Chechik', 'Idan Schwartz'], 'affiliations': ['Bar-Ilan University', 'NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.16025.jpg', 'data': {'categories': ['#training', '#cv', '#optimization', '#synthetic', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'Персонализация генерации изображений без обучения', 'desc': 'SISO - это новый подход к персонализации генерации и редактирования изображений без обучения модели. Метод основан на оптимизации оценки сходства с входным изображением субъекта. SISO итеративно генерирует изображения и оптимизирует модель на основе потери сходства с заданным изображением субъекта. Этот подход позволяет улучшить качество изображения, точность передачи субъекта и сохранение фона по сравнению с существующими методами.'}, 'en': {'title': 'SISO: Personalization Without Training', 'desc': 'This paper introduces SISO, a new method for personalizing image generation and editing without the need for training on multiple images. SISO works by optimizing a similarity score between generated images and a single input image, allowing for iterative improvements until the desired similarity is reached. This approach overcomes the limitations of traditional methods that require extensive training data and can lead to quality degradation with fewer images. The authors demonstrate that SISO significantly enhances image quality, maintains subject fidelity, and preserves background details compared to existing techniques.'}, 'zh': {'title': '无训练个性化图像生成的新方法', 'desc': '个性化图像生成和编辑在只有少量或单张图像时特别具有挑战性。传统的个性化方法依赖于概念学习，但在图像数量较少时，生成的图像质量往往会迅速下降。我们提出了一种名为SISO的新方法，它不需要训练，通过优化与输入图像的相似度分数来实现个性化。SISO在图像编辑和生成任务中表现出显著的质量提升，能够更好地保留主题特征和背景。'}}}, {'id': 'https://huggingface.co/papers/2503.17287', 'title': 'FastCuRL: Curriculum Reinforcement Learning with Progressive Context\n  Extension for Efficient Training R1-like Reasoning Models', 'url': 'https://huggingface.co/papers/2503.17287', 'abstract': 'In this paper, we propose \\textsc{FastCuRL}, a simple yet efficient Curriculum Reinforcement Learning approach with context window extending strategy to accelerate the reinforcement learning training efficiency for R1-like reasoning models while enhancing their performance in tackling complex reasoning tasks with long chain-of-thought rationales, particularly with a 1.5B parameter language model. \\textsc{FastCuRL} consists of two main procedures: length-aware training data segmentation and context window extension training. Specifically, the former first splits the original training data into three different levels by the input prompt length, and then the latter leverages segmented training datasets with a progressively increasing context window length to train the reasoning model. Experimental results demonstrate that \\textsc{FastCuRL}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview across all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva Math, and OlympiadBench) while only utilizing 50\\% of training steps. Furthermore, all training stages for FastCuRL-1.5B-Preview are completed using just a single node with 8 GPUs.', 'score': 8, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '17806ebf2abff1ed', 'authors': ['Mingyang Song', 'Mao Zheng', 'Zheng Li', 'Wenjie Yang', 'Xuan Luo', 'Yue Pan', 'Feng Zhang'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2503.17287.jpg', 'data': {'categories': ['#training', '#long_context', '#reasoning', '#optimization', '#rl'], 'emoji': '🚀', 'ru': {'title': 'Ускоренное обучение языковых моделей для сложных рассуждений', 'desc': 'В этой статье представлен метод FastCuRL для ускорения обучения с подкреплением языковых моделей для решения сложных задач рассуждения. FastCuRL использует сегментацию обучающих данных по длине и постепенное расширение контекстного окна. Эксперименты показывают, что FastCuRL-1.5B-Preview превосходит DeepScaleR-1.5B-Preview на пяти наборах данных, используя всего 50% шагов обучения. Обучение FastCuRL-1.5B-Preview выполняется на одном узле с 8 GPU.'}, 'en': {'title': 'Accelerating Reasoning with FastCuRL: Efficient Training for Complex Tasks', 'desc': 'This paper introduces FastCuRL, a novel approach in Curriculum Reinforcement Learning designed to improve training efficiency for reasoning models with 1.5 billion parameters. It employs a two-step process: first, it segments training data based on the length of input prompts, and then it extends the context window during training to enhance model performance on complex reasoning tasks. The results show that FastCuRL outperforms existing models like DeepScaleR while requiring only half the training steps. Additionally, the entire training process is efficiently executed on a single node equipped with 8 GPUs.'}, 'zh': {'title': '快速提升推理模型训练效率的课程强化学习', 'desc': '本文提出了一种简单而高效的课程强化学习方法，称为FastCuRL，旨在加速R1类推理模型的强化学习训练效率，同时提高其在复杂推理任务中的表现。FastCuRL包括两个主要步骤：长度感知的训练数据分割和上下文窗口扩展训练。具体而言，前者将原始训练数据根据输入提示长度分为三个不同的级别，后者则利用分段的训练数据集，逐步增加上下文窗口长度来训练推理模型。实验结果表明，FastCuRL-1.5B-Preview在所有五个数据集上均优于DeepScaleR-1.5B-Preview，同时仅使用了50%的训练步骤。'}}}, {'id': 'https://huggingface.co/papers/2503.16867', 'title': 'ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question\n  Generation and Answering', 'url': 'https://huggingface.co/papers/2503.16867', 'abstract': "Precisely evaluating semantic alignment between text prompts and generated videos remains a challenge in Text-to-Video (T2V) Generation. Existing text-to-video alignment metrics like CLIPScore only generate coarse-grained scores without fine-grained alignment details, failing to align with human preference. To address this limitation, we propose ETVA, a novel Evaluation method of Text-to-Video Alignment via fine-grained question generation and answering. First, a multi-agent system parses prompts into semantic scene graphs to generate atomic questions. Then we design a knowledge-augmented multi-stage reasoning framework for question answering, where an auxiliary LLM first retrieves relevant common-sense knowledge (e.g., physical laws), and then video LLM answers the generated questions through a multi-stage reasoning mechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's correlation coefficient of 58.47, showing a much higher correlation with human judgment than existing metrics which attain only 31.0. We also construct a comprehensive benchmark specifically designed for text-to-video alignment evaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10 categories. Through a systematic evaluation of 15 existing text-to-video models, we identify their key capabilities and limitations, paving the way for next-generation T2V generation.", 'score': 8, 'issue_id': 2859, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '36366558e2b46107', 'authors': ['Kaisi Guan', 'Zhengfeng Lai', 'Yuchong Sun', 'Peng Zhang', 'Wei Liu', 'Kieran Liu', 'Meng Cao', 'Ruihua Song'], 'affiliations': ['Apple', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16867.jpg', 'data': {'categories': ['#alignment', '#video', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Точная оценка соответствия текста и видео с помощью вопросно-ответной системы', 'desc': 'Статья представляет ETVA - новый метод оценки соответствия между текстовыми запросами и сгенерированными видео. Метод использует мультиагентную систему для генерации атомарных вопросов на основе семантических графов сцен. Затем применяется многоступенчатая система рассуждений с использованием языковых моделей для ответов на вопросы. ETVA показывает значительно более высокую корреляцию с человеческими оценками по сравнению с существующими метриками.'}, 'en': {'title': 'Enhancing Text-to-Video Alignment with ETVA', 'desc': 'This paper addresses the challenge of accurately evaluating how well text prompts align with generated videos in Text-to-Video (T2V) Generation. The authors introduce ETVA, a new evaluation method that uses fine-grained question generation and answering to improve alignment metrics. By creating semantic scene graphs and employing a multi-agent system, ETVA generates specific questions that are answered using a knowledge-augmented reasoning framework. The results show that ETVA significantly correlates with human judgment, outperforming existing metrics and providing a comprehensive benchmark for future T2V evaluations.'}, 'zh': {'title': '提升文本与视频对齐的评估精度', 'desc': '在文本到视频生成（T2V）中，准确评估文本提示与生成视频之间的语义对齐仍然是一个挑战。现有的对齐指标如CLIPScore只能生成粗略的评分，缺乏细致的对齐信息，无法满足人类的偏好。为了解决这个问题，我们提出了一种新颖的文本到视频对齐评估方法ETVA，通过生成和回答细粒度问题来实现。我们的实验表明，ETVA与人类判断的相关性显著高于现有指标，且我们还构建了一个专门用于文本到视频对齐评估的基准。'}}}, {'id': 'https://huggingface.co/papers/2503.12821', 'title': 'From Head to Tail: Towards Balanced Representation in Large\n  Vision-Language Models through Adaptive Data Calibration', 'url': 'https://huggingface.co/papers/2503.12821', 'abstract': 'Large Vision-Language Models (LVLMs) have achieved significant progress in combining visual comprehension with language generation. Despite this success, the training data of LVLMs still suffers from Long-Tail (LT) problems, where the data distribution is highly imbalanced. Previous works have mainly focused on traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as recognition and classification. Nevertheless, the exploration of LVLM (e.g. LLaVA) and more general tasks (e.g. Visual Question Answering and Visual Reasoning) remains under-explored. In this paper, we first conduct an in-depth analysis of the LT issues in LVLMs and identify two core causes: the overrepresentation of head concepts and the underrepresentation of tail concepts. Based on the above observation, we propose an Adaptive Data Refinement Framework (ADR), which consists of two stages: Data Rebalancing (DR) and Data Synthesis (DS). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and scarce images to supplement underrepresented portions. Through comprehensive evaluations across eleven benchmarks, our proposed ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.', 'score': 7, 'issue_id': 2855, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '16457f914f8e71df', 'authors': ['Mingyang Song', 'Xiaoye Qu', 'Jiawei Zhou', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Stony Brook University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.12821.jpg', 'data': {'categories': ['#training', '#long_context', '#synthetic', '#optimization', '#dataset', '#data'], 'emoji': '🦒', 'ru': {'title': 'Балансировка данных для улучшения мультимодальных моделей', 'desc': 'Эта статья посвящена проблеме несбалансированности данных (проблема длинного хвоста) в обучении крупных моделей машинного зрения и языка (LVLM). Авторы предлагают адаптивную систему улучшения данных (ADR), состоящую из двух этапов: ребалансировка данных и синтез данных. Система ADR эффективно решает проблему длинного хвоста в обучающих данных, не увеличивая их объем. Эксперименты показали относительное улучшение средней производительности модели LLaVA 1.5 на 4.36% на одиннадцати тестовых наборах.'}, 'en': {'title': 'Balancing the Data for Better Vision-Language Models', 'desc': "This paper addresses the challenges faced by Large Vision-Language Models (LVLMs) due to imbalanced training data, known as the Long-Tail (LT) problem. The authors identify that certain concepts are overrepresented while others are underrepresented, which affects the model's performance on diverse tasks. To tackle this issue, they introduce an Adaptive Data Refinement Framework (ADR) that includes two key stages: Data Rebalancing to adjust the data distribution and Data Synthesis to generate additional data for underrepresented concepts. Their approach shows significant improvements in model performance across various benchmarks without the need for more training data."}, 'zh': {'title': '自适应数据精炼，解决长尾问题！', 'desc': '大型视觉语言模型（LVLMs）在视觉理解与语言生成的结合上取得了显著进展。然而，LVLMs的训练数据仍然面临长尾问题，数据分布极不平衡。本文深入分析了LVLM中的长尾问题，识别出头部概念过度代表和尾部概念不足代表两个核心原因。我们提出了一种自适应数据精炼框架（ADR），通过数据重平衡和数据合成两个阶段，有效缓解了训练数据中的长尾问题，提升了LLaVA 1.5的平均性能。'}}}, {'id': 'https://huggingface.co/papers/2503.17069', 'title': 'PVChat: Personalized Video Chat with One-Shot Learning', 'url': 'https://huggingface.co/papers/2503.17069', 'abstract': 'Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as "Wilson is receiving chemotherapy" or "Tom is discussing with Sarah", limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose a one-shot learning framework PVChat, the first personalized ViLLM that enables subject-aware question answering (QA) from a single video for each subject. Our approach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically augmented video-QA dataset, leveraging a progressive image-to-video learning strategy. Specifically, we introduce an automated augmentation pipeline that synthesizes identity-preserving positive samples and retrieves hard negatives from existing video corpora, generating a diverse training dataset with four QA types: existence, appearance, action, and location inquiries. To enhance subject-specific learning, we propose a ReLU Routing MoH attention mechanism, alongside two novel objectives: (1) Smooth Proximity Regularization for progressive learning through exponential distance scaling and (2) Head Activation Enhancement for balanced attention routing. Finally, we adopt a two-stage training strategy, transitioning from image pre-training to video fine-tuning, enabling a gradual learning process from static attributes to dynamic representations. We evaluate PVChat on diverse datasets covering medical scenarios, TV series, anime, and real-world footage, demonstrating its superiority in personalized feature understanding after learning from a single video, compared to state-of-the-art ViLLMs.', 'score': 6, 'issue_id': 2859, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'd7cfcebc43949d14', 'authors': ['Yufei Shi', 'Weilong Yan', 'Gang Xu', 'Yumeng Li', 'Yuchen Li', 'Zhenxi Li', 'Fei Richard Yu', 'Ming Li', 'Si Yong Yeo'], 'affiliations': ['Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Nankai University', 'Nanyang Technological University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.17069.jpg', 'data': {'categories': ['#video', '#dataset', '#transfer_learning', '#healthcare', '#training', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'PVChat: Персонализированное понимание видео с одного просмотра', 'desc': 'Статья представляет PVChat - новую модель для персонализированного понимания видео с использованием больших языковых моделей. PVChat решает проблему идентификации личностей в видео, что важно для умных домов и здравоохранения. Модель использует одноразовое обучение и синтетически дополненные данные для улучшения распознавания субъектов. Предложенный подход включает механизм внимания Mixture-of-Heads и двухэтапную стратегию обучения от изображений к видео.'}, 'en': {'title': 'Personalized Video Understanding with One-Shot Learning', 'desc': "This paper introduces PVChat, a novel one-shot learning framework designed to enhance personalized video understanding in large language models. Unlike traditional models that struggle with identity-aware comprehension, PVChat allows for subject-specific question answering from just one video per subject. The framework employs a Mixture-of-Heads attention mechanism and a unique training strategy that combines image pre-training with video fine-tuning, optimizing the model's ability to recognize and respond to various types of inquiries. Evaluation results show that PVChat outperforms existing models in understanding personalized features across multiple datasets, including healthcare and entertainment scenarios."}, 'zh': {'title': '个性化视频理解的新突破', 'desc': '视频大型语言模型（ViLLMs）在一般视频理解方面表现出色，但在身份感知理解上存在困难。为了解决这个问题，我们提出了一种一-shot学习框架PVChat，这是首个个性化的ViLLM，能够从每个主体的单个视频中进行主体感知问答。我们的方法通过增强的混合头（MoH）优化ViLLM，并利用合成增强的视频问答数据集，采用渐进的图像到视频学习策略。最终，我们在多个数据集上评估PVChat，显示其在个性化特征理解方面的优越性。'}}}, {'id': 'https://huggingface.co/papers/2503.16921', 'title': 'When Preferences Diverge: Aligning Diffusion Models with Minority-Aware\n  Adaptive DPO', 'url': 'https://huggingface.co/papers/2503.16921', 'abstract': "In recent years, the field of image generation has witnessed significant advancements, particularly in fine-tuning methods that align models with universal human preferences. This paper explores the critical role of preference data in the training process of diffusion models, particularly in the context of Diffusion-DPO and its subsequent adaptations. We investigate the complexities surrounding universal human preferences in image generation, highlighting the subjective nature of these preferences and the challenges posed by minority samples in preference datasets. Through pilot experiments, we demonstrate the existence of minority samples and their detrimental effects on model performance. We propose Adaptive-DPO -- a novel approach that incorporates a minority-instance-aware metric into the DPO objective. This metric, which includes intra-annotator confidence and inter-annotator stability, distinguishes between majority and minority samples. We introduce an Adaptive-DPO loss function which improves the DPO loss in two ways: enhancing the model's learning of majority labels while mitigating the negative impact of minority samples. Our experiments demonstrate that this method effectively handles both synthetic minority data and real-world preference data, paving the way for more effective training methodologies in image generation tasks.", 'score': 5, 'issue_id': 2852, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '6bd4875e712b7a36', 'authors': ['Lingfan Zhang', 'Chen Liu', 'Chengming Xu', 'Kai Hu', 'Donghao Luo', 'Chengjie Wang', 'Yanwei Fu', 'Yuan Yao'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Tencent', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.16921.jpg', 'data': {'categories': ['#rlhf', '#training', '#diffusion', '#alignment'], 'emoji': '🖼️', 'ru': {'title': 'Адаптивное обучение генеративных моделей с учетом предпочтений большинства', 'desc': 'Статья исследует роль данных о предпочтениях в обучении диффузионных моделей генерации изображений. Авторы выявляют проблему субъективности универсальных человеческих предпочтений и негативное влияние выборок меньшинства на производительность модели. Предлагается новый подход Adaptive-DPO, который учитывает метрику для различения выборок большинства и меньшинства. Эксперименты показывают, что метод эффективно обрабатывает как синтетические, так и реальные данные о предпочтениях.'}, 'en': {'title': 'Enhancing Image Generation with Adaptive-DPO: Balancing Preferences for Better Models', 'desc': 'This paper discusses advancements in image generation, focusing on how preference data can improve diffusion models. It highlights the challenges of incorporating universal human preferences, especially the impact of minority samples in preference datasets. The authors introduce Adaptive-DPO, a new method that uses a metric to better handle these minority samples during training. Their experiments show that Adaptive-DPO enhances model performance by balancing the influence of majority and minority preferences.'}, 'zh': {'title': '适应性DPO：提升图像生成模型的偏好学习', 'desc': '近年来，图像生成领域取得了显著进展，尤其是在微调方法方面，这些方法使模型与普遍的人类偏好对齐。本文探讨了偏好数据在扩散模型训练过程中的关键作用，特别是在Diffusion-DPO及其后续适应中。我们研究了图像生成中普遍人类偏好的复杂性，强调了这些偏好的主观性以及偏好数据集中少数样本带来的挑战。我们提出了一种新方法Adaptive-DPO，通过引入关注少数实例的度量，改善了模型对多数标签的学习，同时减轻了少数样本的负面影响。'}}}, {'id': 'https://huggingface.co/papers/2503.16423', 'title': 'GAEA: A Geolocation Aware Conversational Model', 'url': 'https://huggingface.co/papers/2503.16423', 'abstract': 'Image geolocalization, in which, traditionally, an AI model predicts the precise GPS coordinates of an image is a challenging task with many downstream applications. However, the user cannot utilize the model to further their knowledge other than the GPS coordinate; the model lacks an understanding of the location and the conversational ability to communicate with the user. In recent days, with tremendous progress of large multimodal models (LMMs) proprietary and open-source researchers have attempted to geolocalize images via LMMs. However, the issues remain unaddressed; beyond general tasks, for more specialized downstream tasks, one of which is geolocalization, LMMs struggle. In this work, we propose to solve this problem by introducing a conversational model GAEA that can provide information regarding the location of an image, as required by a user. No large-scale dataset enabling the training of such a model exists. Thus we propose a comprehensive dataset GAEA with 800K images and around 1.6M question answer pairs constructed by leveraging OpenStreetMap (OSM) attributes and geographical context clues. For quantitative evaluation, we propose a diverse benchmark comprising 4K image-text pairs to evaluate conversational capabilities equipped with diverse question types. We consider 11 state-of-the-art open-source and proprietary LMMs and demonstrate that GAEA significantly outperforms the best open-source model, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by 8.28%. Our dataset, model and codes are available', 'score': 5, 'issue_id': 2864, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'ca9d5aa3c56f03d9', 'authors': ['Ron Campos', 'Ashmal Vayani', 'Parth Parag Kulkarni', 'Rohit Gupta', 'Aritra Dutta', 'Mubarak Shah'], 'affiliations': ['University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2503.16423.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#dataset', '#science', '#multimodal'], 'emoji': '🗺️', 'ru': {'title': 'GAEA: диалоговая геолокализация изображений на новом уровне', 'desc': 'Статья представляет модель GAEA для геолокализации изображений с помощью диалогового интерфейса. Авторы создали обширный датасет из 800 тысяч изображений и 1,6 миллиона пар вопросов-ответов, используя данные OpenStreetMap и географические подсказки. Для оценки разработан бенчмарк из 4000 пар изображение-текст с различными типами вопросов. GAEA превосходит лучшие открытые и проприетарные мультимодальные модели на 25,69% и 8,28% соответственно.'}, 'en': {'title': 'GAEA: Conversational Geolocalization for Enhanced User Interaction', 'desc': 'This paper addresses the challenge of image geolocalization, where AI models predict GPS coordinates but lack conversational understanding of the location. The authors introduce GAEA, a conversational model that not only geolocalizes images but also provides contextual information to users. To train this model, they created a new dataset called GAEA, consisting of 800,000 images and 1.6 million question-answer pairs derived from OpenStreetMap data. The results show that GAEA outperforms existing large multimodal models in conversational capabilities related to geolocalization tasks.'}, 'zh': {'title': 'GAEA：图像地理定位的新对话模型', 'desc': '本文提出了一种新的对话模型GAEA，用于图像地理定位。传统的AI模型只能预测图像的GPS坐标，缺乏与用户的互动能力。GAEA通过提供与图像位置相关的信息，解决了这一问题。我们还构建了一个包含80万张图像和160万个问答对的综合数据集，以支持模型的训练和评估。'}}}, {'id': 'https://huggingface.co/papers/2503.16282', 'title': 'Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language\n  Model', 'url': 'https://huggingface.co/papers/2503.16282', 'abstract': 'Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to new classes with few support samples while retaining base class segmentation. Existing GFS-PCS methods enhance prototypes via interacting with support or query features but remain limited by sparse knowledge from few-shot samples. Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world novel classes, contain rich but noisy novel class knowledge. In this work, we introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label selection to filter low-quality regions, followed by an adaptive infilling strategy that combines knowledge from pseudo-label contexts and few-shot samples to adaptively label the filtered, unlabeled areas. Additionally, we design a novel-base mix strategy to embed few-shot samples into training scenes, preserving essential context for improved novel class learning. Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we introduce two challenging benchmarks with diverse novel classes for comprehensive generalization evaluation. Experiments validate the effectiveness of our framework across models and datasets. Our approach and benchmarks provide a solid foundation for advancing GFS-PCS in the real world. The code is at https://github.com/ZhaochongAn/GFS-VL', 'score': 5, 'issue_id': 2862, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '32970c716a041be9', 'authors': ['Zhaochong An', 'Guolei Sun', 'Yun Liu', 'Runjia Li', 'Junlin Han', 'Ender Konukoglu', 'Serge Belongie'], 'affiliations': ['College of Computer Science, Nankai University', 'Computer Vision Laboratory, ETH Zurich', 'Department of Computer Science, University of Copenhagen', 'Department of Engineering Science, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.16282.jpg', 'data': {'categories': ['#games', '#transfer_learning', '#3d', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Синергия 3D VLM и few-shot обучения для улучшенной сегментации облаков точек', 'desc': 'Статья представляет новый подход к обобщенной сегментации облаков точек 3D с малым количеством примеров (GFS-PCS). Авторы предлагают framework GFS-VL, который объединяет плотные, но шумные псевдо-метки из 3D vision-language моделей с точными, но редкими образцами few-shot. Метод включает отбор псевдо-меток на основе прототипов и адаптивное заполнение неразмеченных областей. Также представлены новые наборы данных для оценки обобщения на разнообразные новые классы.'}, 'en': {'title': 'Maximizing Learning with Few Samples in 3D Segmentation', 'desc': "This paper presents a new framework called GFS-VL for generalized few-shot 3D point cloud segmentation, which allows models to learn new classes with only a few examples while still recognizing previously learned classes. The approach combines the strengths of 3D vision-language models, which provide rich but noisy information about new classes, with precise but limited few-shot samples. It introduces a method for selecting high-quality pseudo-labels and an adaptive strategy to fill in gaps in the data, enhancing the labeling of unlabeled areas. Additionally, the authors propose new benchmarks to evaluate the model's performance on diverse novel classes, demonstrating the framework's effectiveness across various datasets and models."}, 'zh': {'title': '融合伪标签与少样本的3D点云分割新方法', 'desc': '本文提出了一种新的框架GFS-VL，用于解决少样本3D点云分割问题。该框架结合了来自3D视觉语言模型的丰富伪标签和稀疏的少样本数据，以提高模型在新类别上的适应能力。通过原型引导的伪标签选择和自适应填充策略，GFS-VL能够有效过滤低质量区域并标记未标记区域。我们还引入了新的基准测试，以评估模型在多样化新类别上的泛化能力，实验结果验证了该框架的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.11572', 'title': 'Implicit Bias-Like Patterns in Reasoning Models', 'url': 'https://huggingface.co/papers/2503.11572', 'abstract': "Implicit bias refers to automatic or spontaneous mental processes that shape perceptions, judgments, and behaviors. Previous research examining `implicit bias' in large language models (LLMs) has often approached the phenomenon differently than how it is studied in humans by focusing primarily on model outputs rather than on model processing. To examine model processing, we present a method called the Reasoning Model Implicit Association Test (RM-IAT) for studying implicit bias-like patterns in reasoning models: LLMs that employ step-by-step reasoning to solve complex tasks. Using this method, we find that reasoning models require more tokens when processing association-incompatible information compared to association-compatible information. These findings suggest AI systems harbor patterns in processing information that are analogous to human implicit bias. We consider the implications of these implicit bias-like patterns for their deployment in real-world applications.", 'score': 5, 'issue_id': 2854, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'f45b684d5c68e00c', 'authors': ['Messi H. J. Lee', 'Calvin K. Lai'], 'affiliations': ['Department of Psychology Rutgers University', 'Division of Computational and Data Sciences Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.11572.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#ethics', '#multimodal', '#training'], 'emoji': '🧠', 'ru': {'title': 'Скрытые предубеждения ИИ: новый взгляд на обработку информации', 'desc': 'Статья представляет метод RM-IAT для изучения имплицитных предубеждений в языковых моделях с рассуждениями (reasoning models). Исследователи обнаружили, что таким моделям требуется больше токенов для обработки информации, несовместимой с ассоциациями, по сравнению с совместимой. Это указывает на наличие в ИИ-системах паттернов обработки информации, аналогичных человеческим имплицитным предубеждениям. Авторы рассматривают последствия этих предубеждений для реальных приложений.'}, 'en': {'title': 'Unveiling Implicit Bias in AI Reasoning Models', 'desc': 'This paper introduces a new method called the Reasoning Model Implicit Association Test (RM-IAT) to study implicit bias in large language models (LLMs). Unlike previous research that focused on outputs, this approach examines how LLMs process information, particularly in reasoning tasks. The study finds that LLMs take longer to process information that conflicts with established associations, similar to human implicit biases. These insights raise important considerations for the use of AI in real-world applications, highlighting the need to understand underlying processing patterns.'}, 'zh': {'title': '揭示AI中的隐性偏见模式', 'desc': '隐性偏见是指自动或自发的心理过程，这些过程影响我们的感知、判断和行为。以往对大型语言模型（LLMs）中隐性偏见的研究，主要关注模型输出，而非模型处理过程。我们提出了一种名为推理模型隐性联想测试（RM-IAT）的方法，用于研究推理模型中的隐性偏见模式。研究发现，推理模型在处理与联想不兼容的信息时，需要更多的标记，这表明AI系统在信息处理上存在类似于人类隐性偏见的模式。'}}}, {'id': 'https://huggingface.co/papers/2503.14607', 'title': 'Can Large Vision Language Models Read Maps Like a Human?', 'url': 'https://huggingface.co/papers/2503.14607', 'abstract': 'In this paper, we introduce MapBench-the first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given a map image and a query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLM-generated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and a Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes. Our evaluation of both open-source and closed-source LVLMs underscores the substantial difficulty posed by MapBench, revealing critical limitations in their spatial reasoning and structured decision-making capabilities. We release all the code and dataset in https://github.com/taco-group/MapBench.', 'score': 3, 'issue_id': 2868, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'a67ca23976ed87ed', 'authors': ['Shuo Xing', 'Zezhou Sun', 'Shuangyu Xie', 'Kaiyuan Chen', 'Yanjia Huang', 'Yuping Wang', 'Jiachen Li', 'Dezhen Song', 'Zhengzhong Tu'], 'affiliations': ['MBZUAI', 'Texas A&M University', 'UC Berkeley', 'UC Riverside', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2503.14607.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#reasoning'], 'emoji': '🗺️', 'ru': {'title': 'MapBench: Новый вызов для языковых моделей в навигации по картам', 'desc': 'MapBench - это первый набор данных, специально разработанный для навигации на основе читаемых человеком пиксельных карт. Он содержит более 1600 задач по поиску пути на 100 разнообразных картах. В MapBench языковые модели с большим объемом данных (LVLMs) генерируют инструкции по навигации на основе изображения карты и запроса с начальными и конечными ориентирами. Оценка показала, что MapBench представляет значительную сложность для современных LVLMs, выявляя ограничения в их пространственном мышлении и структурированном принятии решений.'}, 'en': {'title': 'MapBench: Navigating the Future of Language and Vision Models', 'desc': 'This paper presents MapBench, a novel dataset tailored for evaluating language-based navigation in outdoor environments using pixel-based maps. It includes over 1600 pathfinding scenarios across 100 unique maps, allowing for comprehensive testing of language-vision models (LVLMs). The dataset features a Map Space Scene Graph (MSSG) to facilitate the conversion between natural language instructions and map images, enhancing the evaluation of LVLM outputs. The findings reveal significant challenges for current LVLMs in spatial reasoning and decision-making, highlighting the need for improved models in complex navigation tasks.'}, 'zh': {'title': 'MapBench：挑战语言视觉模型的地图导航数据集', 'desc': '本文介绍了MapBench，这是第一个专门为人类可读的基于像素的地图导航设计的数据集，来源于复杂的路径寻找场景。MapBench包含来自100个不同地图的1600多个像素空间地图路径寻找问题。在MapBench中，LVLM（语言视觉大模型）根据地图图像和起始与结束地标的查询生成基于语言的导航指令。我们的评估显示，MapBench对现有的LVLM在零-shot 提示和链式思维推理框架下提出了显著挑战，揭示了它们在空间推理和结构化决策能力方面的关键局限性。'}}}, {'id': 'https://huggingface.co/papers/2503.17095', 'title': 'FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields', 'url': 'https://huggingface.co/papers/2503.17095', 'abstract': 'Recent 3D face editing methods using masks have produced high-quality edited images by leveraging Neural Radiance Fields (NeRF). Despite their impressive performance, existing methods often provide limited user control due to the use of pre-trained segmentation masks. To utilize masks with a desired layout, an extensive training dataset is required, which is challenging to gather. We present FFaceNeRF, a NeRF-based face editing technique that can overcome the challenge of limited user control due to the use of fixed mask layouts. Our method employs a geometry adapter with feature injection, allowing for effective manipulation of geometry attributes. Additionally, we adopt latent mixing for tri-plane augmentation, which enables training with a few samples. This facilitates rapid model adaptation to desired mask layouts, crucial for applications in fields like personalized medical imaging or creative face editing. Our comparative evaluations demonstrate that FFaceNeRF surpasses existing mask based face editing methods in terms of flexibility, control, and generated image quality, paving the way for future advancements in customized and high-fidelity 3D face editing. The code is available on the {https://kwanyun.github.io/FFaceNeRF_page/{project-page}}.', 'score': 2, 'issue_id': 2862, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'ebada6e24359eec7', 'authors': ['Kwan Yun', 'Chaelin Kim', 'Hangyeul Shin', 'Junyong Noh'], 'affiliations': ['Handong Global University', 'KAIST, Visual Media Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.17095.jpg', 'data': {'categories': ['#open_source', '#optimization', '#3d'], 'emoji': '🎭', 'ru': {'title': 'Гибкое редактирование 3D-лиц с помощью адаптивных масок и NeRF', 'desc': 'FFaceNeRF - это метод редактирования 3D-лиц на основе нейронных полей излучения (NeRF), который преодолевает ограничения существующих подходов с фиксированными масками сегментации. Он использует геометрический адаптер с внедрением признаков для эффективного манипулирования геометрическими атрибутами. Метод также применяет смешивание латентных представлений для аугментации трехплоскостных данных, что позволяет обучаться на небольшом количестве образцов. FFaceNeRF превосходит существующие методы редактирования лиц на основе масок по гибкости, контролю и качеству генерируемых изображений.'}, 'en': {'title': 'Empowering 3D Face Editing with FFaceNeRF', 'desc': 'FFaceNeRF is a novel face editing technique that enhances user control in 3D face editing by utilizing Neural Radiance Fields (NeRF). Unlike traditional methods that rely on fixed segmentation masks, FFaceNeRF introduces a geometry adapter with feature injection, allowing users to manipulate geometry attributes more freely. The method also incorporates latent mixing for tri-plane augmentation, enabling effective training with limited data samples. Our evaluations show that FFaceNeRF outperforms existing methods in flexibility, control, and image quality, making it a significant advancement in personalized and high-fidelity 3D face editing.'}, 'zh': {'title': 'FFaceNeRF：灵活高效的人脸编辑新方法', 'desc': '最近的3D人脸编辑方法利用神经辐射场（NeRF）通过掩膜生成高质量的编辑图像。然而，现有方法由于使用预训练的分割掩膜，往往限制了用户的控制能力。我们提出了FFaceNeRF，这是一种基于NeRF的人脸编辑技术，能够克服固定掩膜布局带来的控制限制。该方法采用几何适配器和特征注入，允许有效操控几何属性，并通过潜在混合进行三平面增强，使得在少量样本下也能快速适应所需的掩膜布局。'}}}, {'id': 'https://huggingface.co/papers/2503.00865', 'title': 'Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers', 'url': 'https://huggingface.co/papers/2503.00865', 'abstract': "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.", 'score': 40, 'issue_id': 2555, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'bc2424e709a2dd78', 'authors': ['Yiran Zhao', 'Chaoqun Liu', 'Yue Deng', 'Jiahao Ying', 'Mahani Aljunied', 'Zhaodonghui Li', 'Lidong Bing', 'Hou Pong Chan', 'Yu Rong', 'Deli Zhao', 'Wenxuan Zhang'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.00865.jpg', 'data': {'categories': ['#low_resource', '#architecture', '#open_source', '#training', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Babel: революция в многоязычном машинном обучении', 'desc': 'Представлена новая многоязычная языковая модель Babel, охватывающая 25 самых распространенных языков мира. Модель использует технику расширения слоев для улучшения производительности. Предложены две версии: Babel-9B для эффективного вывода и дообучения, и Babel-83B, устанавливающая новый стандарт для открытых многоязычных моделей. Обе версии показывают превосходные результаты в многоязычных задачах по сравнению с аналогичными открытыми моделями.'}, 'en': {'title': 'Babel: Bridging the Language Gap with Open Multilingual LLMs', 'desc': "This paper presents Babel, an innovative open-source multilingual large language model (LLM) that addresses the lack of coverage for under-resourced languages in existing models. Babel supports the top 25 languages spoken globally, reaching over 90% of the world's population, and includes many languages that are often neglected. The model employs a unique layer extension technique to increase its parameter count, enhancing its performance beyond traditional pretraining methods. With two variants, Babel-9B and Babel-83B, the model demonstrates superior performance on multilingual tasks, outperforming other open LLMs of similar size and achieving results comparable to commercial models."}, 'zh': {'title': 'Babel：打破语言壁垒的多语言模型', 'desc': '大型语言模型（LLMs）在自然语言处理（NLP）领域带来了革命性的变化，但开源的多语言LLMs仍然稀缺，现有模型通常在语言覆盖上有限。许多模型优先考虑资源丰富的语言，而广泛使用但资源不足的语言常常被忽视。为了解决这一差距，我们推出了Babel，一个开放的多语言LLM，覆盖全球前25种语言，支持超过90%的人口，并包括许多其他开源多语言LLMs忽视的语言。Babel通过层扩展技术增加参数数量，提升了性能，并推出了两个变体：Babel-9B和Babel-83B，后者在多语言任务中设定了新的标准。'}}}, {'id': 'https://huggingface.co/papers/2503.03746', 'title': 'Process-based Self-Rewarding Language Models', 'url': 'https://huggingface.co/papers/2503.03746', 'abstract': "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities.", 'score': 21, 'issue_id': 2564, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '808bee960390ec29', 'authors': ['Shimao Zhang', 'Xiao Liu', 'Xin Zhang', 'Junxiao Liu', 'Zheheng Luo', 'Shujian Huang', 'Yeyun Gong'], 'affiliations': ['Microsoft Research Asia', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2503.03746.jpg', 'data': {'categories': ['#math', '#alignment', '#rlhf', '#reasoning', '#training'], 'emoji': '🧮', 'ru': {'title': 'Самообучение ИИ математике: шаг за шагом к сверхчеловеческим способностям', 'desc': 'Статья представляет новый метод самообучения языковых моделей для задач математических рассуждений. Авторы предлагают подход Process-based Self-Rewarding, который включает пошаговое рассуждение и оценку промежуточных результатов самой моделью. Этот метод позволяет преодолеть ограничения существующих подходов к самообучению в математических задачах. Эксперименты показывают, что новый метод значительно улучшает способности языковых моделей к математическим рассуждениям на различных тестовых наборах.'}, 'en': {'title': 'Empowering LLMs with Process-based Self-Rewarding for Superior Reasoning', 'desc': 'This paper discusses the limitations of current self-rewarding methods used to train Large Language Models (LLMs), particularly in mathematical reasoning tasks. The authors introduce a new approach called Process-based Self-Rewarding, which incorporates long-thought reasoning and a step-wise evaluation process. By allowing LLMs to act as judges of their own outputs, this method optimizes the training process iteratively. The results show significant improvements in LLM performance on mathematical reasoning benchmarks, suggesting that self-rewarding can enhance reasoning capabilities beyond human levels.'}, 'zh': {'title': '基于过程的自我奖励：超越人类的推理能力', 'desc': '大型语言模型在各种下游任务中表现出色，并广泛应用于多个场景。为了进一步提高其性能，研究者使用人类标注的偏好数据进行训练，但这受到人类表现上限的限制。因此，提出了自我奖励的方法，让语言模型通过奖励自己的输出生成训练数据。然而，现有的自我奖励方法在数学推理场景中效果不佳，甚至可能导致性能下降。本文提出了一种基于过程的自我奖励管道，通过引入长时间思考推理、逐步的语言模型评判和逐步的偏好优化，成功提升了语言模型在多个数学推理基准上的表现，展示了自我奖励在超越人类能力的推理中的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.00329', 'title': 'ABC: Achieving Better Control of Multimodal Embeddings using VLMs', 'url': 'https://huggingface.co/papers/2503.00329', 'abstract': 'Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page.', 'score': 14, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '0483c542c8885777', 'authors': ['Benjamin Schneider', 'Florian Kerschbaum', 'Wenhu Chen'], 'affiliations': ['Cheriton School of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.00329.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#open_source', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'ABC: Мультимодальные встраивания с гибким языковым контролем', 'desc': 'Статья представляет новую мультимодальную модель встраивания под названием ABC, которая объединяет визуальные и текстовые данные. В отличие от существующих подходов, ABC использует глубокую интеграцию изображений и естественного языка. Модель демонстрирует высокую производительность в задачах поиска изображений по тексту и классификации. ABC также позволяет использовать естественный язык для решения сложных задач визуального поиска с неоднозначностями.'}, 'en': {'title': 'ABC: Unifying Vision and Language for Enhanced Multimodal Understanding', 'desc': "This paper presents ABC, a new multimodal embedding model that integrates visual and natural language inputs more effectively than existing CLIP-based methods. Unlike previous models that treat images and text separately, ABC combines these modalities deeply, allowing for better interaction and user control. The model excels in zero-shot tasks, particularly in image-to-text retrieval and classification, outperforming others in the Massive Multimodal Embedding Benchmark. To assess its capabilities, the authors introduce CtrlBench, a benchmark designed to evaluate the model's performance in handling complex visual retrieval tasks with natural language instructions."}, 'zh': {'title': 'ABC：多模态嵌入的新突破', 'desc': '这篇论文介绍了一种名为ABC的多模态嵌入模型，旨在解决视觉检索和分类中的模糊性问题。与现有的CLIP方法不同，ABC通过深度整合图像特征和自然语言指令，提供更强的模态交互。ABC在MSCOCO图像到文本检索任务中表现出色，并在分类和视觉问答任务中取得了最佳性能。通过设计CtrlBench基准，评估了ABC在处理复杂视觉检索问题时的能力，展示了其高质量的表示和灵活的自然语言控制。'}}}, {'id': 'https://huggingface.co/papers/2503.03751', 'title': 'GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control', 'url': 'https://huggingface.co/papers/2503.03751', 'abstract': 'We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/', 'score': 13, 'issue_id': 2555, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '8f5f2ad910a260c0', 'authors': ['Xuanchi Ren', 'Tianchang Shen', 'Jiahui Huang', 'Huan Ling', 'Yifan Lu', 'Merlin Nimier-David', 'Thomas Müller', 'Alexander Keller', 'Sanja Fidler', 'Jun Gao'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.03751.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': '🎥', 'ru': {'title': 'Точный контроль камеры и 3D-согласованность в генерации видео', 'desc': 'GEN3C - это генеративная модель видео с точным контролем камеры и временной 3D-согласованностью. Она использует 3D-кэш в виде облаков точек, полученных из глубинных карт исходных изображений или ранее сгенерированных кадров. При генерации следующих кадров GEN3C опирается на 2D-рендеринг 3D-кэша с новой траекторией камеры, заданной пользователем. Это позволяет модели сфокусироваться на ранее ненаблюдаемых областях и продвижении состояния сцены, не тратя ресурсы на запоминание предыдущих результатов или вывод структуры изображения из положения камеры.'}, 'en': {'title': 'GEN3C: Mastering Video Generation with 3D Precision and Camera Control', 'desc': 'GEN3C is a generative video model that enhances video generation by incorporating precise camera control and maintaining temporal 3D consistency. Unlike previous models that often lack 3D information, GEN3C utilizes a 3D cache of point clouds derived from depth predictions, allowing for more coherent object presence in videos. The model is conditioned on 2D renderings from this cache, enabling it to generate new frames without needing to remember past outputs or infer scene structure from camera angles. This approach results in superior camera control and state-of-the-art performance in generating novel views, particularly in complex scenarios like driving scenes.'}, 'zh': {'title': 'GEN3C：精确相机控制与时间一致性的视频生成模型', 'desc': '我们提出了GEN3C，这是一种具有精确相机控制和时间一致性的生成视频模型。以往的视频模型虽然能够生成逼真的视频，但往往缺乏3D信息，导致物体出现和消失的不一致性。GEN3C通过3D缓存来指导生成过程，利用从种子图像或先前生成帧中预测的像素深度获得的点云。这样，GEN3C能够在用户提供的新相机轨迹下，专注于生成未观察到的区域，并有效推进场景状态到下一个帧。'}}}, {'id': 'https://huggingface.co/papers/2503.02951', 'title': 'KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding', 'url': 'https://huggingface.co/papers/2503.02951', 'abstract': 'We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.', 'score': 12, 'issue_id': 2555, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '6c344ba0bf71ac84', 'authors': ['Zhangchen Xu', 'Yang Liu', 'Yueqin Yin', 'Mingyuan Zhou', 'Radha Poovendran'], 'affiliations': ['Microsoft', 'The University of Texas at Austin', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.02951.jpg', 'data': {'categories': ['#dataset', '#rl', '#optimization', '#synthetic', '#training'], 'emoji': '🧑\u200d💻', 'ru': {'title': 'KodCode: Синтетические данные для обучения ИИ программированию', 'desc': 'KodCode - это синтетический набор данных для обучения больших языковых моделей программированию. Он состоит из триплетов вопрос-решение-тест, которые проходят процедуру самопроверки. Процесс создания KodCode включает синтез вопросов по программированию, генерацию решений и тестовых случаев, а также постобработку данных. Эксперименты показывают, что модели, обученные на KodCode, достигают наилучших результатов на различных бенчмарках по программированию.'}, 'en': {'title': 'KodCode: Elevating Coding Models with Verified Data', 'desc': 'KodCode is a synthetic dataset designed to improve the training of Large Language Models (LLMs) for coding tasks by providing high-quality, verifiable data. It includes question-solution-test triplets that are validated through a self-verification process, ensuring both correctness and a wide range of coding difficulties. The dataset is generated using a systematic pipeline that synthesizes coding questions, creates solutions, and develops test cases, particularly focusing on challenging problems. Fine-tuning experiments show that models trained on KodCode outperform existing models on various coding benchmarks, demonstrating its effectiveness in enhancing LLM performance.'}, 'zh': {'title': 'KodCode：高质量编码数据集的解决方案', 'desc': '我们介绍了KodCode，这是一个合成数据集，旨在解决获取高质量、可验证的训练数据的挑战，以训练大型语言模型进行编码。现有的代码资源通常无法确保覆盖范围广泛或正确性可验证。KodCode由问题-解决方案-测试三元组组成，通过自我验证程序系统地验证。我们的流程包括合成各种编码问题，生成解决方案和测试用例，并在后期通过重写问题和生成响应来进行数据合成，最终生成一个大规模、强大且多样化的编码数据集。'}}}, {'id': 'https://huggingface.co/papers/2503.03278', 'title': 'Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions', 'url': 'https://huggingface.co/papers/2503.03278', 'abstract': 'Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities.', 'score': 10, 'issue_id': 2560, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '6103dbe5d60b5f3f', 'authors': ['Jun Li', 'Che Liu', 'Wenjia Bai', 'Rossella Arcucci', 'Cosmin I. Bercea', 'Julia A. Schnabel'], 'affiliations': ['Helmholtz AI and Helmholtz Munich, Germany', 'Imperial College London, UK', 'Kings College London, UK', 'Munich Center for Machine Learning, Germany', 'Technical University of Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.03278.jpg', 'data': {'categories': ['#cv', '#multimodal', '#healthcare', '#alignment', '#transfer_learning'], 'emoji': '🔬', 'ru': {'title': 'Декомпозиция медицинских знаний для повышения эффективности VLM в анализе медицинских изображений', 'desc': 'Эта статья представляет новый подход к улучшению работы визуальных языковых моделей (VLM) в обнаружении и локализации аномалий на медицинских изображениях. Вместо прямого распознавания конкретных патологий, метод фокусируется на разложении медицинских концепций на базовые атрибуты и общие визуальные паттерны. Это улучшает связь между текстовыми описаниями и визуальными характеристиками, повышая точность распознавания и локализации аномалий. Метод был протестирован на модели Florence-2 и показал результаты, сравнимые с гораздо более крупными медицинскими VLM, несмотря на использование значительно меньшего объема данных для обучения.'}, 'en': {'title': 'Enhancing Medical VLMs through Decomposed Knowledge', 'desc': 'This paper presents a new method to improve Visual Language Models (VLMs) for detecting and locating abnormalities in medical images. The authors address the challenge of complex medical terminology by breaking down medical concepts into simpler attributes and common visual patterns. This approach enhances the alignment between text descriptions and visual features, leading to better performance in recognizing and localizing abnormalities. The proposed method shows competitive results with larger models while using significantly less training data, indicating its efficiency and strong generalization capabilities.'}, 'zh': {'title': '分解医学知识，提升视觉语言模型的异常检测能力', 'desc': '视觉语言模型（VLMs）在视觉定位任务中表现出色，但在医学领域，尤其是医学图像中的异常检测和定位方面，仍然缺乏研究。医学术语的复杂性使得将病理异常术语与相应的视觉特征直接关联变得困难。我们提出了一种新方法，通过分解医学知识来增强VLM在医学异常检测和定位中的性能。该方法通过将医学概念分解为基本属性和常见视觉模式，促进了文本描述与视觉特征之间的更强对齐，从而提高了医学图像中异常的识别和定位能力。'}}}, {'id': 'https://huggingface.co/papers/2503.01836', 'title': 'CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom', 'url': 'https://huggingface.co/papers/2503.01836', 'abstract': "Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores, model perplexity), they fail to capture the complexity of instruction-following across diverse fields. Therefore, we investigate more diverse signals to capture comprehensive instruction-response pair characteristics and propose three foundational metrics that leverage Multi-LLM wisdom, informed by (1) diverse LLM responses and (2) reward model assessment. Building upon base metrics, we propose CrowdSelect, an integrated metric incorporating a clustering-based approach to maintain response diversity. Our comprehensive experiments demonstrate that our foundation metrics consistently improve performance across 4 base models on MT-bench and Arena-Hard. CrowdSelect, efficiently incorporating all metrics, achieves state-of-the-art performance in both Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and 11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring valuable insights for future research in this direction. Code are available at https://github.com/listentm/crowdselect.", 'score': 9, 'issue_id': 2560, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd59d65fb3b60c043', 'authors': ['Yisen Li', 'Lingfeng Yang', 'Wenxuan Shen', 'Pan Zhou', 'Yao Wan', 'Weiwei Lin', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01836.jpg', 'data': {'categories': ['#small_models', '#training', '#synthetic', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'CrowdSelect: умный отбор инструкций для обучения языковых моделей', 'desc': 'Статья описывает новый метод отбора инструкций для обучения языковых моделей, названный CrowdSelect. Он использует три основных метрики, основанные на оценках различных большиx языковых моделей и моделей вознаграждения. CrowdSelect также включает кластеризацию для сохранения разнообразия ответов. Эксперименты показали, что этот метод превосходит существующие подходы на бенчмарках MT-bench и Arena-Hard. Авторы надеются, что их исследование внесет вклад в развитие этого направления.'}, 'en': {'title': 'Enhancing Model Training with Diverse Instruction Metrics', 'desc': 'This paper focuses on improving the training of smaller models by distilling the instruction-following abilities of larger language models (LLMs). It critiques existing methods that use simple metrics for selecting synthetic instruction data, which do not adequately reflect the complexity of instruction-following tasks. The authors propose new metrics that utilize diverse responses from multiple LLMs and a reward model to better assess instruction-response pairs. Their method, CrowdSelect, combines these metrics with a clustering approach to enhance response diversity, leading to significant performance improvements in various model evaluations.'}, 'zh': {'title': '提升小模型的指令跟随能力', 'desc': '本论文探讨了如何将大型语言模型的指令跟随能力提炼到更小的模型中。现有的合成指令数据选择策略主要依赖单一维度的信号，未能全面捕捉指令跟随的复杂性。我们提出了三种基础指标，利用多种大型语言模型的智慧，结合多样的响应和奖励模型评估。通过综合实验，我们的CrowdSelect指标在多个基模型上表现出色，显著提升了性能，展示了未来研究的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.01933', 'title': 'Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective', 'url': 'https://huggingface.co/papers/2503.01933', 'abstract': 'Deploying large scale language models on edge devices faces inherent challenges such as high computational demands, energy consumption, and potential data privacy risks. This paper introduces the Shakti Small Language Models (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these constraints headon. By combining efficient architectures, quantization techniques, and responsible AI principles, the Shakti series enables on-device intelligence for smartphones, smart appliances, IoT systems, and beyond. We provide comprehensive insights into their design philosophy, training pipelines, and benchmark performance on both general tasks (e.g., MMLU, Hellaswag) and specialized domains (healthcare, finance, and legal). Our findings illustrate that compact models, when carefully engineered and fine-tuned, can meet and often exceed expectations in real-world edge-AI scenarios.', 'score': 6, 'issue_id': 2563, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '6fa74210552cc49f', 'authors': ['Rakshit Aralimatti', 'Syed Abdul Gaffar Shakhadri', 'Kruthika KR', 'Kartik Basavaraj Angadi'], 'affiliations': ['SandLogic Technologies Pvt Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2503.01933.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#training', '#ethics', '#inference', '#small_models', '#optimization'], 'emoji': '📱', 'ru': {'title': 'Малые языковые модели для большого интеллекта на краю сети', 'desc': 'Статья представляет серию малых языковых моделей Shakti, разработанных для применения на периферийных устройствах. Модели Shakti-100M, Shakti-250M и Shakti-500M решают проблемы высоких вычислительных требований, энергопотребления и потенциальных рисков конфиденциальности данных. Используя эффективные архитектуры, методы квантования и принципы ответственного ИИ, серия Shakti обеспечивает локальный интеллект для смартфонов, умных устройств и IoT-систем. Исследование показывает, что компактные модели, при тщательной разработке и настройке, могут соответствовать и часто превосходить ожидания в реальных сценариях периферийного ИИ.'}, 'en': {'title': 'Empowering Edge Devices with Efficient Language Models', 'desc': 'This paper presents the Shakti Small Language Models (SLMs) designed to operate efficiently on edge devices while addressing challenges like high computational needs and energy consumption. The Shakti models, including Shakti-100M, Shakti-250M, and Shakti-500M, utilize advanced architectures and quantization techniques to optimize performance without compromising data privacy. The authors detail the design philosophy, training processes, and benchmark results across various tasks and specialized fields such as healthcare and finance. The results demonstrate that well-engineered compact models can perform effectively in real-world applications, showcasing the potential of on-device AI.'}, 'zh': {'title': '小型语言模型，智能边缘计算的未来', 'desc': '本论文介绍了Shakti小型语言模型（SLMs），包括Shakti-100M、Shakti-250M和Shakti-500M，旨在解决在边缘设备上部署大型语言模型时面临的高计算需求和能耗问题。通过结合高效架构、量化技术和负责任的人工智能原则，Shakti系列实现了智能手机、智能家电和物联网系统的本地智能。我们提供了关于其设计理念、训练流程和在一般任务（如MMLU、Hellaswag）及专业领域（医疗、金融和法律）上的基准性能的全面见解。研究结果表明，经过精心设计和微调的紧凑模型能够在实际边缘人工智能场景中满足并超越预期。'}}}, {'id': 'https://huggingface.co/papers/2502.20317', 'title': 'Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases', 'url': 'https://huggingface.co/papers/2502.20317', 'abstract': 'Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.', 'score': 6, 'issue_id': 2561, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '686b2ff85600f281', 'authors': ['Yongjia Lei', 'Haoyu Han', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka', 'Mahantesh M Halappanavar', 'Jiliang Tang', 'Yu Wang'], 'affiliations': ['Adobe Research', 'Michigan State University', 'Pacific Northwest National Laboratory', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2502.20317.jpg', 'data': {'categories': ['#benchmark', '#data', '#graphs', '#dataset', '#multimodal', '#reasoning'], 'emoji': '🕸️', 'ru': {'title': 'Гармоничное слияние структурного и текстового поиска в графовых базах знаний', 'desc': 'Эта статья представляет новый метод под названием MoR (Mixture of Structural-and-Textual Retrieval) для работы с графовыми базами знаний, содержащими текстовую информацию. MoR использует трехэтапный подход: планирование, рассуждение и организация, чтобы эффективно объединить структурный и текстовый поиск. Метод генерирует текстовые графы планирования, затем переплетает структурный обход и текстовое сопоставление, и наконец, переранжирует кандидатов на основе их структурных траекторий. Эксперименты показывают превосходство MoR в гармонизации структурного и текстового поиска.'}, 'en': {'title': 'Harmonizing Text and Structure for Better Knowledge Retrieval', 'desc': 'This paper introduces a new method called Mixture of Structural-and-Textual Retrieval (MoR) for improving the retrieval of knowledge from Text-rich Graph Knowledge Bases (TG-KBs). Unlike traditional methods that treat textual and structural knowledge separately, MoR integrates both types through a three-stage framework: Planning, Reasoning, and Organizing. In the Planning stage, it creates graphs that outline the logic for query responses, while the Reasoning stage combines structural traversal with textual matching to gather relevant candidates. Finally, the Organizing stage enhances the selection process by reranking candidates based on their structural paths, leading to better retrieval performance across various query types.'}, 'zh': {'title': '结构与文本知识的完美结合', 'desc': '本文提出了一种新的混合检索方法，称为结构与文本检索的混合体（MoR），旨在同时利用文本和结构知识来回答查询。MoR通过规划-推理-组织的框架来实现这一目标，在规划阶段生成文本规划图，明确回答查询的逻辑。接着，在推理阶段，MoR结合结构遍历和文本匹配，从文本丰富的图知识库中获取候选答案。最后，在组织阶段，MoR根据结构轨迹对获取的候选答案进行重新排序，实验结果表明该方法在结构和文本检索的协调性上具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2503.03044', 'title': 'QE4PE: Word-level Quality Estimation for Human Post-Editing', 'url': 'https://huggingface.co/papers/2503.03044', 'abstract': "Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. Our QE4PE study investigates the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated by behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.", 'score': 5, 'issue_id': 2560, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'e4d3d7db506b6e1c', 'authors': ['Gabriele Sarti', 'Vilém Zouhar', 'Grzegorz Chrupała', 'Ana Guerberof-Arenas', 'Malvina Nissim', 'Arianna Bisazza'], 'affiliations': ['CLCG, University of Groningen', 'CSAI, Tilburg University', 'ETH Zürich'], 'pdf_title_img': 'assets/pdf/title_img/2503.03044.jpg', 'data': {'categories': ['#machine_translation', '#data', '#multilingual', '#healthcare'], 'emoji': '🔍', 'ru': {'title': 'Оценка качества перевода: мост между точностью и практичностью', 'desc': 'Статья исследует влияние оценки качества перевода на уровне слов (word-level QE) на процесс постредактирования машинного перевода. В исследовании участвовали 42 профессиональных редактора, работавших с двумя направлениями перевода. Сравнивались четыре модальности подсветки ошибок, включая методы на основе обучения с учителем и неопределенности. Результаты показывают, что эффективность подсветки зависит от домена, языка и скорости работы редакторов, при этом разница между ручной и автоматической QE оказалась незначительной.'}, 'en': {'title': 'Enhancing Post-Editing Efficiency with Word-Level Quality Estimation', 'desc': 'This paper explores how word-level quality estimation (QE) can help improve the efficiency of human post-editing in machine translation (MT). It examines the effectiveness of different methods for highlighting potential translation errors, comparing supervised and uncertainty-based approaches. The study involves 42 professional post-editors and assesses their editing speed and quality improvements through detailed behavioral logs and human annotations. The findings reveal that factors like domain, language, and editor speed significantly influence the effectiveness of error highlights, indicating a need to bridge the gap between the accuracy of QE systems and their practical usability in real-world editing tasks.'}, 'zh': {'title': '提升机器翻译后编辑效率的关键', 'desc': '本文研究了词级质量评估（QE）在机器翻译后编辑中的影响。我们分析了42名专业后编辑在两种翻译方向下的表现，比较了四种错误范围高亮方式，包括监督和基于不确定性的词级QE方法。研究发现，领域、语言和编辑速度是影响高亮效果的关键因素。结果表明，人工和自动QE高亮之间存在适度差异，突显了专业工作流程中准确性与可用性之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2503.00307', 'title': 'Remasking Discrete Diffusion Models with Inference-Time Scaling', 'url': 'https://huggingface.co/papers/2503.00307', 'abstract': 'Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: https://remdm.github.io.', 'score': 4, 'issue_id': 2572, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '7f31677f2cb675e1', 'authors': ['Guanghan Wang', 'Yair Schiff', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov'], 'affiliations': ['Department of Computer Science, Cornell Unversity'], 'pdf_title_img': 'assets/pdf/title_img/2503.00307.jpg', 'data': {'categories': ['#open_source', '#inference', '#diffusion', '#science', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'Перемаскировка для улучшения дискретных диффузионных моделей', 'desc': 'Статья представляет новый метод ReMDM (remasking diffusion model), который улучшает процесс генерации в дискретных диффузионных моделях. ReMDM позволяет итеративно уточнять сгенерированные токены, что ранее было невозможно в классических масочных диффузионных моделях. Этот подход повышает качество генерации естественного языка, приближая его к уровню авторегрессионных моделей, особенно при увеличении числа шагов сэмплирования. ReMDM также демонстрирует улучшения в генерации дискретизированных изображений и дизайне молекул.'}, 'en': {'title': 'Enhancing Masked Diffusion with Iterative Refinement', 'desc': 'This paper introduces the remasking diffusion model (ReMDM) sampler, which enhances the capabilities of masked discrete diffusion models by allowing iterative refinement during output generation. Unlike traditional methods where generated tokens cannot be updated, ReMDM enables the correction of errors by applying a remasking backward process. This approach not only improves the quality of natural language outputs to rival autoregressive models but also maintains high quality under limited computational resources. Additionally, ReMDM enhances the performance of masked diffusion models in generating discretized images and aids in molecule design, pushing the boundaries of controllability in scientific applications.'}, 'zh': {'title': '重掩蔽扩散模型：提升生成质量的新方法', 'desc': '扩散模型的成功部分源于其迭代精炼的能力，即在生成过程中不断修正输出。然而，现代的掩蔽离散扩散模型缺乏这种能力：一旦生成一个标记，就无法再次更新，即使它引入了错误。为了解决这个限制，我们提出了重掩蔽扩散模型（ReMDM）采样器，这是一种可以以原则性方式应用于预训练掩蔽扩散模型的方法。ReMDM通过增加采样步骤，生成的自然语言输出质量接近自回归模型，同时在计算预算有限时，ReMDM更好地保持质量。'}}}, {'id': 'https://huggingface.co/papers/2502.18860', 'title': 'Exploring Rewriting Approaches for Different Conversational Tasks', 'url': 'https://huggingface.co/papers/2502.18860', 'abstract': "Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best.", 'score': 4, 'issue_id': 2561, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '1f018cc4f38149bc', 'authors': ['Md Mehrab Tanjim', 'Ryan A. Rossi', 'Mike Rimer', 'Xiang Chen', 'Sungchul Kim', 'Vaishnavi Muppala', 'Tong Yu', 'Zhengmian Hu', 'Ritwik Sinha', 'Wei Zhang', 'Iftikhar Ahamath Burhanuddin', 'Franck Dernoncourt'], 'affiliations': ['Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.18860.jpg', 'data': {'categories': ['#dataset', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Переписывание запросов в разговорных ИИ: один метод не подходит для всех задач', 'desc': 'В статье исследуются два подхода к переписыванию запросов в разговорных ассистентах: переписывание и слияние. Эксперименты проводились на двух различных задачах генерации: текст-в-текст и мультимодальной генерации визуализаций. Результаты показывают, что эффективность подхода зависит от конкретного случая использования и задачи. Для текстового вопросно-ответного ассистента лучше работает переписывание запросов, а для ассистента по анализу данных - слияние запросов.'}, 'en': {'title': 'Tailoring Response Strategies for Conversational Assistants', 'desc': 'This paper explores how conversational assistants can improve their responses by using two methods: rewriting and fusion. The rewriting method modifies user questions to enhance accuracy, while the fusion method combines information from past interactions to generate answers. The study shows that the effectiveness of these methods varies based on the task; rewriting is better for text-based question answering, while fusion excels in generating visualizations from data analysis tasks. The findings highlight the importance of tailoring the approach to the specific use case for optimal performance.'}, 'zh': {'title': '对话助手中的问题重写与融合方法的最佳选择', 'desc': '本论文探讨了对话助手中问题重写算法的两种不同方法：重写和融合。这两种方法在文本生成和多模态生成任务中表现不同，具体取决于应用场景。研究发现，对于对话问答助手，查询重写方法效果最佳；而对于生成可视化和数据表的数据分析助手，查询融合方法更为有效。我们还分析了短对话和长对话的数据集，结果表明查询融合在数据分析任务中始终表现更好。'}}}, {'id': 'https://huggingface.co/papers/2503.01763', 'title': "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", 'url': 'https://huggingface.co/papers/2503.01763', 'abstract': 'Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.', 'score': 4, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'e6a23582f741dc5b', 'authors': ['Zhengliang Shi', 'Yuhan Wang', 'Lingyong Yan', 'Pengjie Ren', 'Shuaiqiang Wang', 'Dawei Yin', 'Zhaochun Ren'], 'affiliations': ['Baidu Inc., Beijing, China', 'Leiden University, Leiden, The Netherlands', 'Shandong University, Qingdao, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01763.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#dataset', '#data'], 'emoji': '🔍', 'ru': {'title': 'ToolRet: Новый вызов для моделей поиска инструментов ИИ', 'desc': 'ToolRet - это новый эталонный тест для оценки поиска инструментов в контексте обучения инструментам для больших языковых моделей (LLM). Он включает 7,6 тысяч разнообразных задач поиска и корпус из 43 тысяч инструментов. Исследование показало, что даже модели с высокой производительностью в традиционных тестах информационного поиска плохо справляются с ToolRet. Авторы также предоставили обучающий набор данных из более чем 200 тысяч примеров для улучшения способностей моделей к поиску инструментов.'}, 'en': {'title': 'Enhancing Tool Retrieval for Language Models with ToolRet', 'desc': 'This paper introduces ToolRet, a benchmark designed to evaluate the effectiveness of information retrieval (IR) models in selecting tools for large language models (LLMs) in practical tasks. The authors highlight that existing benchmarks often rely on a limited set of pre-annotated tools, which does not reflect real-world complexities. Their findings reveal that even high-performing IR models struggle with tool retrieval in this new context, leading to lower task success rates for LLMs. To address this issue, they provide a large-scale training dataset that significantly enhances the tool retrieval capabilities of IR models.'}, 'zh': {'title': '工具检索：提升LLMs的实用能力', 'desc': '本文探讨了工具学习如何增强大型语言模型（LLMs）的能力，使其能够作为代理解决实际任务。由于工具使用的LLMs具有有限的上下文长度，因此采用信息检索（IR）模型从大量工具集中选择有用工具是关键的初步步骤。我们提出了ToolRet，一个包含7.6k多样化检索任务和43k工具的异构工具检索基准，旨在评估IR模型在工具检索任务中的表现。研究发现，即使在传统IR基准上表现良好的模型，在ToolRet上的表现却很差，这降低了工具使用LLMs的任务通过率。'}}}, {'id': 'https://huggingface.co/papers/2503.01729', 'title': 'FLAME: A Federated Learning Benchmark for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2503.01729', 'abstract': 'Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. While federated learning enables decentralized, privacy-preserving training, its application to robotic manipulation remains largely unexplored. We introduce FLAME (Federated Learning Across Manipulation Environments), the first benchmark designed for federated learning in robotic manipulation. FLAME consists of: (i) a set of large-scale datasets of over 160,000 expert demonstrations of multiple manipulation tasks, collected across a wide range of simulated environments; (ii) a training and evaluation framework for robotic policy learning in a federated setting. We evaluate standard federated learning algorithms in FLAME, showing their potential for distributed policy learning and highlighting key challenges. Our benchmark establishes a foundation for scalable, adaptive, and privacy-aware robotic learning.', 'score': 3, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '893358a382c79250', 'authors': ['Santiago Bou Betran', 'Alberta Longhini', 'Miguel Vasco', 'Yuchong Zhang', 'Danica Kragic'], 'affiliations': ['KTH Royal Institute of Technology, Stockholm, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2503.01729.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Федеративное обучение для масштабируемой и конфиденциальной робототехники', 'desc': 'Статья представляет FLAME - первый бенчмарк для федеративного обучения в робототехнической манипуляции. FLAME включает в себя большой набор данных с более чем 160 000 экспертных демонстраций различных задач манипуляции, собранных в симулированных средах. Бенчмарк также предоставляет фреймворк для обучения и оценки робототехнических политик в федеративной среде. Авторы оценивают стандартные алгоритмы федеративного обучения на FLAME, демонстрируя их потенциал для распределенного обучения политик и выявляя ключевые проблемы.'}, 'en': {'title': 'Empowering Robots with Federated Learning for Privacy and Scalability', 'desc': 'This paper presents FLAME, a benchmark for applying federated learning to robotic manipulation tasks. It addresses the limitations of centralized training by allowing robots to learn from diverse datasets while preserving data privacy. FLAME includes over 160,000 expert demonstrations from various simulated environments, facilitating decentralized training. The study evaluates existing federated learning algorithms, demonstrating their effectiveness and identifying challenges in distributed policy learning for robotics.'}, 'zh': {'title': '联邦学习助力机器人操控的未来', 'desc': '这篇论文介绍了FLAME（跨操控环境的联邦学习），这是一个为机器人操控设计的基准测试。FLAME包含超过160,000个专家演示的大规模数据集，涵盖多种操控任务，收集自多种模拟环境。通过在FLAME中评估标准的联邦学习算法，论文展示了分布式策略学习的潜力，并指出了关键挑战。该基准为可扩展、适应性强且注重隐私的机器人学习奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2503.01449', 'title': 'Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection', 'url': 'https://huggingface.co/papers/2503.01449', 'abstract': 'Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.', 'score': 3, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '1b4593bb9d78ec53', 'authors': ['Ting Zhang', 'Chengran Yang', 'Yindu Su', 'Martin Weyssow', 'Hung Nguyen', 'Tan Bui', 'Hong Jin Kang', 'Yikun Li', 'Eng Lieh Ouh', 'Lwin Khin Shar', 'David Lo'], 'affiliations': ['School of Computer Science, University of Sydney, Australia', 'School of Computing and Information Systems, Singapore Management University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.01449.jpg', 'data': {'categories': ['#open_source', '#plp', '#training', '#security', '#benchmark', '#dataset', '#data'], 'emoji': '🛡️', 'ru': {'title': 'LLM на страже безопасности кода: новые горизонты в обнаружении уязвимостей', 'desc': 'Статья представляет комплексное исследование возможностей больших языковых моделей (LLM) в обнаружении уязвимостей программного обеспечения (SVD). Авторы оценивают производительность пяти открытых LLM на наборах данных, включающих уязвимые функции на Python, Java и JavaScript, используя различные подходы, такие как инженерия промптов, настройка инструкций и тонкая настройка классификации последовательностей. Исследование также изучает способы улучшения производительности LLM в SVD, включая переобучение на сбалансированных наборах данных и использование ансамблевых методов обучения. Результаты показывают, что SVD остается сложной задачей для LLM, предоставляя ценные insights для будущих разработок в области применения генеративного ИИ для повышения безопасности программного обеспечения.'}, 'en': {'title': 'Unlocking LLMs for Software Vulnerability Detection', 'desc': "This paper investigates the effectiveness of large language models (LLMs) in detecting software vulnerabilities, an important area for software security. It highlights the lack of comprehensive studies on LLMs' capabilities across various programming languages, as most existing research focuses on C/C++ datasets. The authors present an empirical study using a dataset of over 44,000 vulnerable functions from Python, Java, and JavaScript, evaluating five open-source LLMs with different strategies like prompt engineering and instruction tuning. The findings reveal that while LLMs show promise, software vulnerability detection remains a challenging task, providing valuable insights for future improvements in this field."}, 'zh': {'title': '提升软件安全：大型语言模型在漏洞检测中的应用', 'desc': '最近生成性人工智能的进展使得大型语言模型（LLMs）在软件工程中得到了广泛应用，解决了许多长期存在的挑战。然而，目前缺乏对LLMs在软件漏洞检测（SVD）能力的全面研究，这对软件安全至关重要。现有研究主要集中在使用C/C++数据集评估LLMs，通常只探讨了提示工程、指令调优和序列分类微调中的一两种策略。因此，我们进行了一项全面的实证研究，评估LLMs在不同编程语言中检测漏洞的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.01378', 'title': 'CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs', 'url': 'https://huggingface.co/papers/2503.01378', 'abstract': 'This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io', 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8a4aab69ce92453d', 'authors': ['Artem Lykov', 'Valerii Serpiva', 'Muhammad Haris Khan', 'Oleg Sautenkov', 'Artyom Myshlyaev', 'Grik Tadevosyan', 'Yasheerah Yaqoot', 'Dzmitry Tsetserukou'], 'affiliations': ['Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01378.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#dataset', '#multimodal', '#cv'], 'emoji': '🚁', 'ru': {'title': 'Умные дроны: когнитивное управление БПЛА с помощью ИИ', 'desc': 'В статье представлена модель CognitiveDrone - новая модель Зрение-Язык-Действие (VLA) для сложных задач беспилотных летательных аппаратов (БПЛА). Модель обучена на наборе данных из более чем 8000 симулированных полетов и генерирует команды действий в реальном времени на основе визуальных входных данных и текстовых инструкций. Усовершенствованная версия CognitiveDrone-R1 включает дополнительный модуль рассуждений на основе Модели Зрения-Языка (VLM) для упрощения сложных задач. Экспериментальная оценка показывает, что CognitiveDrone-R1 достигает успешности выполнения задач в 77.2%, что на 30% лучше базовых моделей в критических когнитивных задачах.'}, 'en': {'title': 'CognitiveDrone: Elevating UAV Intelligence with Vision-Language-Action!', 'desc': 'This paper presents CognitiveDrone, a new Vision-Language-Action (VLA) model designed for complex tasks performed by Unmanned Aerial Vehicles (UAVs). It is trained on a dataset of over 8,000 simulated flight paths focusing on Human Recognition, Symbol Understanding, and Reasoning. The model can generate real-time 4D action commands from visual inputs and text instructions, with an enhanced version, CognitiveDrone-R1, that includes a Vision-Language Model (VLM) reasoning module for better task management. Experimental results show significant performance improvements, with CognitiveDrone-R1 achieving a 77.2% success rate, highlighting the importance of advanced reasoning in UAV operations.'}, 'zh': {'title': '智能无人机的认知飞行新纪元', 'desc': '本文介绍了一种名为CognitiveDrone的新型视觉-语言-行动（VLA）模型，专为复杂的无人机任务设计，具备高级认知能力。该模型在超过8000条模拟飞行轨迹的数据集上进行训练，涵盖人类识别、符号理解和推理三个关键类别。CognitiveDrone-R1通过集成额外的视觉-语言模型（VLM）推理模块，进一步提升在复杂场景中的表现。实验结果显示，CognitiveDrone模型的成功率达到59.6%，而CognitiveDrone-R1的成功率更是高达77.2%，证明了将高级推理能力融入无人机控制系统的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.01372', 'title': 'SwiLTra-Bench: The Swiss Legal Translation Benchmark', 'url': 'https://huggingface.co/papers/2503.01372', 'abstract': "In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.", 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '3de5be81537fa0fd', 'authors': ['Joel Niklaus', 'Jakob Merane', 'Luka Nenadic', 'Sina Ahmadi', 'Yingqiang Gao', 'Cyrill A. H. Chevalley', 'Claude Humbel', 'Christophe Gösken', 'Lorenzo Tanzi', 'Thomas Lüthi', 'Stefan Palombo', 'Spencer Poff', 'Boling Yang', 'Nan Wu', 'Matthew Guillod', 'Robin Mamié', 'Daniel Brunner', 'Julio Pereyra', 'Niko Grupen'], 'affiliations': ['Canton of Solothurn', 'ETH Zurich', 'Max Planck Institute for Research on Collective Goods', 'Swiss Federal Supreme Court', 'University of Basel', 'University of Geneva', 'University of Lausanne', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.01372.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#benchmark', '#dataset', '#machine_translation'], 'emoji': '⚖️', 'ru': {'title': 'Революция в юридическом переводе: ИИ покоряет многоязычную Швейцарию', 'desc': 'Статья представляет SwiLTra-Bench - многоязычный набор данных для оценки систем машинного перевода юридических текстов в Швейцарии. Авторы провели систематическую оценку различных моделей, включая крупные языковые модели и специализированные системы перевода. Результаты показывают, что передовые модели достигают лучших результатов во всех типах документов, а дообучение открытых моделей значительно улучшает качество перевода. Также представлена система SwiLTra-Judge для оценки качества перевода, которая хорошо коррелирует с оценками экспертов.'}, 'en': {'title': 'Enhancing Legal Translation with SwiLTra-Bench and LLMs', 'desc': 'This paper addresses the challenges of legal translation in Switzerland, where multiple languages complicate the process. It introduces SwiLTra-Bench, a benchmark dataset with over 180,000 aligned legal translation pairs to evaluate large language model (LLM) translation systems. The findings show that while advanced models perform well across various document types, specialized systems are better for translating laws but struggle with headnotes. The study also highlights the effectiveness of fine-tuning open-source language models, although they still do not match the performance of top zero-shot models like Claude-3.5-Sonnet.'}, 'zh': {'title': '瑞士法律翻译的智能解决方案', 'desc': '在瑞士，由于有四种官方语言，法律翻译显得尤为重要。传统上，这一过程依赖于既是法律专家又是翻译高手的专业人士，导致了瓶颈，影响了公正的有效获取。为了解决这个问题，我们推出了SwiLTra-Bench，这是一个包含超过18万对瑞士法律翻译的多语言基准数据集，旨在评估基于大语言模型的翻译系统。我们的评估显示，前沿模型在所有文档类型的翻译表现上优于其他系统，而专门的翻译系统在法律文本中表现出色，但在头注方面表现不佳。'}}}, {'id': 'https://huggingface.co/papers/2503.00502', 'title': 'Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions', 'url': 'https://huggingface.co/papers/2503.00502', 'abstract': "Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, but the conflict between slow inference speed and the need for real-time decision-making challenges practical deployment. To address these issues, this paper introduces a parallel Actor-Reasoner framework designed to enable explicit bidirectional AV-HV interactions across multiple scenarios. First, by facilitating interactions between the LLM-driven Reasoner and heterogeneous simulated HVs during training, an interaction memory database, referred to as the Actor, is established. Then, by introducing the memory partition module and the two-layer memory retrieval module, the Actor's ability to handle heterogeneous HVs is significantly enhanced. Ablation studies and comparisons with other decision-making methods demonstrate that the proposed Actor-Reasoner framework significantly improves safety and efficiency. Finally, with the combination of the external Human-Machine Interface (eHMI) information derived from Reasoner's reasoning and the feasible action solutions retrieved from the Actor, the effectiveness of the proposed Actor-Reasoner is confirmed in multi-scenario field interactions. Our code is available at https://github.com/FanGShiYuu/Actor-Reasoner.", 'score': 2, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': 'd184a5cae68093d5', 'authors': ['Shiyu Fang', 'Jiaqi Liu', 'Chengkai Xu', 'Chen Lv', 'Peng Hang', 'Jian Sun'], 'affiliations': ['College of Transportation, Tongji University, Shanghai 201804, China', 'Nanyang Technological University, 639798, Singapore', 'State Key Lab of Intelligent Transportation System, Beijing 100088, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.00502.jpg', 'data': {'categories': ['#rl', '#robotics', '#inference', '#optimization', '#agents', '#reasoning'], 'emoji': '🚗', 'ru': {'title': 'Интеллектуальное взаимодействие автономных и обычных автомобилей с помощью больших языковых моделей', 'desc': 'Эта статья представляет новую архитектуру Actor-Reasoner для улучшения взаимодействия между автономными и управляемыми человеком транспортными средствами. Авторы используют большие языковые модели для создания базы данных взаимодействий и двухуровневую систему извлечения памяти для работы с разнородными транспортными средствами. Предложенный подход значительно повышает безопасность и эффективность принятия решений по сравнению с другими методами. Эксперименты в реальных условиях подтверждают эффективность предложенной архитектуры Actor-Reasoner в различных сценариях.'}, 'en': {'title': 'Enhancing AV-HV Interactions with the Actor-Reasoner Framework', 'desc': "This paper presents a new framework called the Actor-Reasoner to improve interactions between Autonomous Vehicles (AVs) and Human-driven Vehicles (HVs). It leverages large language models (LLMs) to facilitate real-time communication and decision-making, addressing the challenge of slow inference speeds. The framework includes an interaction memory database, which enhances the AV's ability to understand and respond to various HV behaviors. Experimental results show that this approach significantly boosts both safety and efficiency in multi-scenario driving situations."}, 'zh': {'title': '提升自动驾驶与人类驾驶互动的智能框架', 'desc': '这篇论文介绍了一种新的并行演员-推理器框架，旨在改善自动驾驶汽车（AV）与人类驾驶汽车（HV）之间的互动。通过在训练过程中促进大语言模型（LLM）驱动的推理器与不同类型的模拟HV之间的互动，建立了一个互动记忆数据库。引入记忆分区模块和双层记忆检索模块后，演员的处理能力得到了显著提升。实验结果表明，该框架在多场景交互中显著提高了安全性和效率。'}}}, {'id': 'https://huggingface.co/papers/2503.02954', 'title': 'Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders', 'url': 'https://huggingface.co/papers/2503.02954', 'abstract': 'Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as a graph problem and collect ground truth data using a Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into a latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. Finally, the feasible proposal with the highest performance index is selected for the deployment. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. Project page: https://mengyuest.github.io/gnn-vae-coord', 'score': 0, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d0c072d834299e0', 'authors': ['Yue Meng', 'Nathalie Majcherczyk', 'Wenliang Liu', 'Scott Kiesel', 'Chuchu Fan', 'Federico Pecora'], 'affiliations': ['Amazon Robotics, North Reading, MA USA', 'Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02954.jpg', 'data': {'categories': ['#optimization', '#training', '#games', '#agents', '#inference', '#graphs'], 'emoji': '🤖', 'ru': {'title': 'GNN-VAE: Быстрая координация множества роботов', 'desc': 'Статья представляет новый подход к координации множества роботов в общих пространствах, таких как автоматизированные склады. Авторы предлагают использовать вариационные автоэнкодеры на основе графовых нейронных сетей (GNN-VAE) для решения проблемы координации в масштабе быстрее, чем с помощью централизованной оптимизации. Модель обучается на данных, сгенерированных решателем задач смешанного целочисленного линейного программирования (MILP). Результаты показывают, что подход может достигать высококачественных решений даже для крупномасштабных проблем с 250 роботами, работая значительно быстрее других базовых методов.'}, 'en': {'title': 'Efficient Multi-Robot Coordination with GNN-VAE', 'desc': 'This paper addresses the challenge of coordinating multiple robots in shared environments, particularly in high-traffic areas where traditional local methods may lead to deadlocks. The authors propose a novel approach using Graph Neural Network Variational Autoencoders (GNN-VAE) to efficiently generate coordination schedules at scale. By framing the coordination problem as a graph problem and utilizing a Mixed-Integer Linear Program (MILP) for data collection, the framework learns to encode effective solutions into a latent space. During inference, it decodes these solutions to select the most optimal coordination strategy, ensuring compliance with all operational constraints while significantly reducing computation time compared to centralized methods.'}, 'zh': {'title': '高效多智能体协调的新方法', 'desc': '多智能体协调在共享空间中的可靠多机器人导航中至关重要，尤其是在机器人交通密集的区域。传统的局部协调方法可能无法找到无死锁的解决方案，因此需要一个中央单元生成全局调度来决定机器人的通行顺序。本文提出利用图神经网络变分自编码器（GNN-VAE）来更快地解决大规模的多智能体协调问题，避免了集中优化方法的高运行时间。通过将协调问题形式化为图问题，并使用混合整数线性规划（MILP）求解器收集真实数据，我们的学习框架能够在潜在空间中编码高质量的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.02924', 'title': 'Diverse Controllable Diffusion Policy with Signal Temporal Logic', 'url': 'https://huggingface.co/papers/2503.02924', 'abstract': 'Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature "single-outcome", making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. A case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy.', 'score': 0, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'adc4dd2a16bb83a4', 'authors': ['Yue Meng', 'Chuchu fan'], 'affiliations': ['Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02924.jpg', 'data': {'categories': ['#dataset', '#training', '#data', '#rl', '#agents', '#diffusion', '#synthetic'], 'emoji': '🚗', 'ru': {'title': 'Реалистичные и разнообразные симуляции дорожного движения с помощью STL и диффузионных моделей', 'desc': 'Эта статья представляет новый подход к генерации реалистичных симуляций для автономных систем, таких как беспилотные автомобили и взаимодействие человека с роботом. Авторы используют комбинацию Сигнальной Темпоральной Логики (STL) и Диффузионных Моделей для создания контролируемой, разнообразной и соблюдающей правила политики поведения участников дорожного движения. Метод сначала калибрует STL на реальных данных, затем генерирует разнообразные синтетические данные с помощью оптимизации траекторий, и наконец обучает скорректированную диффузионную политику на расширенном наборе данных. Результаты показывают, что подход превосходит базовые методы по разнообразию, соблюдению правил и безопасности, а также позволяет генерировать различные характеристики в зависимости от параметров STL.'}, 'en': {'title': 'Diverse and Rule-Compliant Simulations for Autonomous Systems', 'desc': 'This paper addresses the challenge of generating realistic simulations for autonomous systems, particularly in driving scenarios. It combines Signal Temporal Logic (STL) with Diffusion Models to create a policy that is both controllable and diverse while adhering to traffic rules. By calibrating STL on real-world data and generating synthetic data through trajectory optimization, the authors enhance the learning process to produce varied and rule-compliant behaviors. The results demonstrate that their method outperforms existing approaches in terms of diversity and safety in simulated environments.'}, 'zh': {'title': '生成多样化且遵循规则的行为策略', 'desc': '本文提出了一种新方法，通过信号时序逻辑（STL）和扩散模型来生成可控、多样且遵循规则的行为策略，以解决当前驾驶模拟器在生成道路参与者行为时的局限性。我们首先在真实数据上校准STL，然后利用轨迹优化生成多样的合成数据，最后在增强数据集上学习修正的扩散策略。实验结果表明，我们的方法在NuScenes数据集上能够生成最具多样性且符合规则的轨迹，且运行时间显著优于其他基线方法。通过闭环测试，我们的方法在多样性、规则满足率和碰撞率方面均表现最佳，能够根据不同的STL参数生成多样化的特征。'}}}, {'id': 'https://huggingface.co/papers/2503.18878', 'title': 'I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2503.18878', 'abstract': "Large Language Models (LLMs) have achieved remarkable success in natural language processing. Recent advances have led to the developing of a new class of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved state-of-the-art performance by integrating deep thinking and complex reasoning. Despite these impressive capabilities, the internal reasoning mechanisms of such models remain unexplored. In this work, we employ Sparse Autoencoders (SAEs), a method to learn a sparse decomposition of latent representations of a neural network into interpretable features, to identify features that drive reasoning in the DeepSeek-R1 series of models. First, we propose an approach to extract candidate ''reasoning features'' from SAE representations. We validate these features through empirical analysis and interpretability methods, demonstrating their direct correlation with the model's reasoning abilities. Crucially, we demonstrate that steering these features systematically enhances reasoning performance, offering the first mechanistic account of reasoning in LLMs. Code available at https://github.com/AIRI-Institute/SAE-Reasoning", 'score': 82, 'issue_id': 2881, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'a72d586944db2b55', 'authors': ['Andrey Galichin', 'Alexey Dontsov', 'Polina Druzhinina', 'Anton Razzhigaev', 'Oleg Y. Rogov', 'Elena Tutubalina', 'Ivan Oseledets'], 'affiliations': ['AIRI', 'HSE', 'MTUCI', 'Sber', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2503.18878.jpg', 'data': {'categories': ['#training', '#interpretability', '#open_source', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая тайны рассуждений искусственного интеллекта', 'desc': 'Исследователи изучили внутренние механизмы рассуждений в языковых моделях, используя разреженные автоэнкодеры (SAE). Они выделили ключевые признаки, отвечающие за способность модели к рассуждениям, в серии моделей DeepSeek-R1. Анализ показал прямую корреляцию этих признаков с рассуждениями модели. Управление выделенными признаками позволило систематически улучшать способность модели к рассуждениям, что дает первое механистическое объяснение рассуждений в больших языковых моделях.'}, 'en': {'title': 'Unlocking Reasoning in Large Language Models', 'desc': "This paper explores the internal reasoning mechanisms of Large Language Models (LLMs), specifically focusing on the DeepSeek-R1 model. The authors utilize Sparse Autoencoders (SAEs) to extract and identify 'reasoning features' that contribute to the model's reasoning capabilities. Through empirical analysis, they validate the correlation between these features and the model's performance in reasoning tasks. The study reveals that manipulating these features can systematically improve reasoning performance, providing new insights into how LLMs process and understand complex information."}, 'zh': {'title': '揭示大型语言模型的推理机制', 'desc': '大型语言模型（LLMs）在自然语言处理领域取得了显著成功。最近的进展催生了一类新的推理LLMs，例如开源的DeepSeek-R1，通过整合深度思考和复杂推理实现了最先进的性能。尽管这些模型的能力令人印象深刻，但其内部推理机制仍未被深入探索。本文采用稀疏自编码器（SAEs）来识别驱动DeepSeek-R1系列模型推理的特征，并通过实证分析验证这些特征与模型推理能力的直接关联。'}}}, {'id': 'https://huggingface.co/papers/2503.17359', 'title': 'Position: Interactive Generative Video as Next-Generation Game Engine', 'url': 'https://huggingface.co/papers/2503.17359', 'abstract': "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.", 'score': 51, 'issue_id': 2875, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '0046c940a41d8637', 'authors': ['Jiwen Yu', 'Yiran Qin', 'Haoxuan Che', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Xihui Liu'], 'affiliations': ['Kuaishou', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.17359.jpg', 'data': {'categories': ['#video', '#architecture', '#games', '#multimodal'], 'emoji': '🎮', 'ru': {'title': 'Революция в разработке игр: ИИ-генерируемые миры будущего', 'desc': 'Статья предлагает концепцию Генеративных Игровых Движков (GGE), основанных на Интерактивной Генеративной Видео технологии (IGV). GGE позволяет создавать неограниченный новый контент для игр следующего поколения, используя преимущества IGV в синтезе высококачественного контента, моделировании физики мира и интерактивности. Авторы представляют комплексную структуру основных модулей GGE и иерархическую дорожную карту зрелости (L0-L4) для его развития. Это исследование открывает новые перспективы для разработки игр в эпоху искусственного интеллекта.'}, 'en': {'title': 'Revolutionizing Game Development with AI-Driven Generative Engines', 'desc': 'This paper discusses the limitations of traditional game engines that rely on fixed content, which can hinder creativity and increase costs in game development. It introduces Interactive Generative Video (IGV) as a new approach to create Generative Game Engines (GGE), which can produce endless unique game content. GGE utilizes advanced features like high-quality content synthesis, physics-aware modeling, and user interactivity to enhance the gaming experience. The authors also outline a framework and roadmap for the development of GGE, aiming to transform the future of game creation through AI technologies.'}, 'zh': {'title': 'AI驱动的游戏创作新纪元', 'desc': '现代游戏开发面临着创造力和成本的重大挑战，传统游戏引擎的内容预设限制了创新。最近，视频生成模型的突破使得合成逼真且互动的虚拟环境成为可能，这为游戏创作带来了革命性的机会。我们提出了互动生成视频（IGV）作为生成游戏引擎（GGE）的基础，能够在下一代游戏中实现无限的新内容生成。GGE利用IGV在高质量内容合成、物理感知世界建模、用户控制互动、长期记忆能力和因果推理等方面的独特优势。'}}}, {'id': 'https://huggingface.co/papers/2503.18942', 'title': 'Video-T1: Test-Time Scaling for Video Generation', 'url': 'https://huggingface.co/papers/2503.18942', 'abstract': 'With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: https://liuff19.github.io/Video-T1', 'score': 50, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '1d482b72d90d6136', 'authors': ['Fangfu Liu', 'Hanyang Wang', 'Yimo Cai', 'Kaiyan Zhang', 'Xiaohang Zhan', 'Yueqi Duan'], 'affiliations': ['Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18942.jpg', 'data': {'categories': ['#video', '#inference', '#games', '#optimization', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Масштабирование времени тестирования: новый подход к улучшению генерации видео', 'desc': 'Статья исследует применение метода масштабирования времени тестирования (TTS) для улучшения качества генерации видео по текстовому описанию. Авторы представляют этот процесс как задачу поиска лучших траекторий от гауссова шума к целевому распределению видео. Они предлагают два подхода: линейный поиск с увеличением кандидатов шума и более эффективный метод Tree-of-Frames (ToF), который адаптивно расширяет и обрезает ветви видео авторегрессивным способом. Эксперименты показывают, что увеличение вычислительных ресурсов на этапе тестирования значительно улучшает качество генерируемых видео.'}, 'en': {'title': 'Unlocking Video Quality with Test-Time Scaling', 'desc': 'This paper explores the concept of Test-Time Scaling (TTS) in video generation, which allows models to utilize additional computational resources during inference to enhance video quality. Instead of focusing solely on training larger models, the authors investigate how increasing inference-time computation can improve the generation of videos from text prompts. They propose a method called Tree-of-Frames (ToF) that efficiently manages the search for better video outputs by adaptively expanding and pruning video branches. The results show that leveraging more computational power at test time leads to significant improvements in the quality of generated videos.'}, 'zh': {'title': '测试时间扩展：提升视频生成质量的新方法', 'desc': '随着训练数据、模型规模和计算成本的增加，视频生成在数字创作中取得了显著成果。本文探讨了在视频生成中应用测试时间扩展（TTS）的潜力，旨在提高生成质量。我们将测试时间扩展重新解释为一个搜索问题，通过从高斯噪声空间中采样更好的轨迹来生成目标视频。实验结果表明，增加测试时间计算可以显著提升视频质量。'}}}, {'id': 'https://huggingface.co/papers/2503.18945', 'title': 'Aether: Geometric-Aware Unified World Modeling', 'url': 'https://huggingface.co/papers/2503.18945', 'abstract': 'The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video prediction, and (3) goal-conditioned visual planning. Through task-interleaved feature learning, Aether achieves synergistic knowledge sharing across reconstruction, prediction, and planning objectives. Building upon video generation models, our framework demonstrates unprecedented synthetic-to-real generalization despite never observing real-world data during training. Furthermore, our approach achieves zero-shot generalization in both action following and reconstruction tasks, thanks to its intrinsic geometric modeling. Remarkably, even without real-world data, its reconstruction performance far exceeds that of domain-specific models. Additionally, Aether leverages a geometry-informed action space to seamlessly translate predictions into actions, enabling effective autonomous trajectory planning. We hope our work inspires the community to explore new frontiers in physically-reasonable world modeling and its applications.', 'score': 21, 'issue_id': 2877, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'e9f70faf8bc6d0d0', 'authors': ['Aether Team', 'Haoyi Zhu', 'Yifan Wang', 'Jianjun Zhou', 'Wenzheng Chang', 'Yang Zhou', 'Zizun Li', 'Junyi Chen', 'Chunhua Shen', 'Jiangmiao Pang', 'Tong He'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.18945.jpg', 'data': {'categories': ['#video', '#3d', '#reasoning', '#agents', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'Единая система для геометрического моделирования мира и планирования действий', 'desc': 'Статья представляет Aether - унифицированную систему для геометрически-осознанного рассуждения в моделях мира. Aether объединяет три ключевые возможности: 4D динамическую реконструкцию, предсказание видео с учетом действий и визуальное планирование с учетом целей. Благодаря совместному обучению признаков для различных задач, система демонстрирует беспрецедентную генерализацию с синтетических данных на реальные. Aether также показывает способность к обобщению без дополнительного обучения в задачах следования действиям и реконструкции.'}, 'en': {'title': 'Aether: Bridging Geometry and Generative Modeling for AI Spatial Reasoning', 'desc': "This paper introduces Aether, a framework that combines geometric reconstruction with generative modeling to enhance AI's spatial reasoning abilities. Aether optimizes three main functions: dynamic 4D reconstruction, action-based video prediction, and goal-oriented visual planning. By using task-interleaved feature learning, it allows for effective knowledge sharing among these functions, leading to improved performance. Notably, Aether achieves strong generalization to real-world scenarios without ever training on real data, showcasing its potential for autonomous trajectory planning and physical world modeling."}, 'zh': {'title': 'Aether：实现几何感知推理的统一框架', 'desc': '本论文提出了Aether框架，旨在解决几何重建与生成建模的整合问题，以实现类人空间推理。Aether通过联合优化四个核心能力，包括4D动态重建、基于动作的视频预测和基于目标的视觉规划，来实现几何感知推理。该框架通过任务交错特征学习，促进了重建、预测和规划目标之间的知识共享。尽管在训练过程中未观察到真实世界数据，Aether仍展现出前所未有的合成到真实的泛化能力，且在无监督情况下在动作跟随和重建任务中实现了零样本泛化。'}}}, {'id': 'https://huggingface.co/papers/2503.18892', 'title': 'SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild', 'url': 'https://huggingface.co/papers/2503.18892', 'abstract': 'DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.', 'score': 21, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '4c0c4ab2292562e4', 'authors': ['Weihao Zeng', 'Yuzhen Huang', 'Qian Liu', 'Wei Liu', 'Keqing He', 'Zejun Ma', 'Junxian He'], 'affiliations': ['BUPT', 'HKUST', 'TikTok'], 'pdf_title_img': 'assets/pdf/title_img/2503.18892.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#rl', '#training', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Развитие рассуждений в языковых моделях через RL с нуля', 'desc': 'Это исследование посвящено применению обучения с подкреплением (RL) без предварительной подготовки для развития способностей к рассуждениям у различных языковых моделей. Авторы провели эксперименты на 10 разных базовых моделях, включая LLama3, Mistral, DeepSeek-Math и Qwen2.5. Используя специальные стратегии, такие как настройка формата вознаграждения и контроль сложности запросов, удалось значительно улучшить точность рассуждений и длину ответов. Наблюдения показали, что разные модели демонстрируют различные паттерны в процессе обучения, причем увеличение длины ответа не всегда коррелирует с появлением определенных когнитивных способностей.'}, 'en': {'title': 'Unlocking Reasoning with Zero RL Training', 'desc': "The paper discusses DeepSeek-R1, which demonstrates that long chain-of-thought reasoning can be developed using a simple reinforcement learning framework with rule-based rewards, starting directly from base models, a method called zero RL training. The authors explore zero RL training across ten different base models, revealing that many of these models already possess strong instruction-following and self-reflection capabilities. They implement design strategies to enhance reasoning accuracy and response length, while also noting that training dynamics vary significantly among models. Importantly, they identify the 'aha moment' in smaller models outside the Qwen family, providing insights and open-sourcing their findings to support further research."}, 'zh': {'title': '零强化学习训练：推理与反思的新突破', 'desc': 'DeepSeek-R1展示了通过简单的强化学习框架和基于规则的奖励，长链思维推理可以自然出现。这种训练方法被称为零强化学习训练，允许直接从基础模型开始。我们研究了10种不同的基础模型，发现它们在推理准确性和响应长度上都有显著提升。我们还观察到，不同模型在训练过程中表现出不同的模式，特别是小模型首次出现了“恍然大悟”的现象。'}}}, {'id': 'https://huggingface.co/papers/2503.18033', 'title': 'OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18033', 'abstract': "Omnimatte aims to decompose a given video into semantically meaningful layers, including the background and individual objects along with their associated effects, such as shadows and reflections. Existing methods often require extensive training or costly self-supervised optimization. In this paper, we present OmnimatteZero, a training-free approach that leverages off-the-shelf pre-trained video diffusion models for omnimatte. It can remove objects from videos, extract individual object layers along with their effects, and composite those objects onto new videos. We accomplish this by adapting zero-shot image inpainting techniques for video object removal, a task they fail to handle effectively out-of-the-box. We then show that self-attention maps capture information about the object and its footprints and use them to inpaint the object's effects, leaving a clean background. Additionally, through simple latent arithmetic, object layers can be isolated and recombined seamlessly with new video layers to produce new videos. Evaluations show that OmnimatteZero not only achieves superior performance in terms of background reconstruction but also sets a new record for the fastest Omnimatte approach, achieving real-time performance with minimal frame runtime.", 'score': 20, 'issue_id': 2881, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': 'd43dc2371ac5a1e8', 'authors': ['Dvir Samuel', 'Matan Levy', 'Nir Darshan', 'Gal Chechik', 'Rami Ben-Ari'], 'affiliations': ['Bar-Ilan University, Ramat-Gan, Israel', 'NVIDIA Research, Tel-Aviv, Israel', 'OriginAI, Tel-Aviv, Israel', 'The Hebrew University of Jerusalem, Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.18033.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#video'], 'emoji': '🎬', 'ru': {'title': 'OmnimatteZero: Быстрая и эффективная декомпозиция видео без обучения', 'desc': 'OmnimatteZero - это новый подход к декомпозиции видео на семантически значимые слои без необходимости обучения. Метод использует предобученные диффузионные модели для видео, позволяя удалять объекты, извлекать отдельные слои объектов вместе с их эффектами и компоновать их в новых видео. OmnimatteZero адаптирует технологии инпейнтинга изображений для удаления объектов из видео и использует карты самовнимания для восстановления эффектов объекта. Метод демонстрирует превосходную производительность в реконструкции фона и работает в режиме реального времени.'}, 'en': {'title': 'Revolutionizing Video Editing with Training-Free Layer Decomposition', 'desc': 'OmnimatteZero is a novel approach for video decomposition that separates a video into meaningful layers, such as backgrounds and individual objects, along with their effects like shadows and reflections. Unlike traditional methods that require extensive training, OmnimatteZero utilizes pre-trained video diffusion models, allowing for a training-free solution. It effectively employs zero-shot image inpainting techniques to remove objects and inpaint their effects, ensuring a clean background. The method also allows for the seamless recombination of object layers with new video content, achieving real-time performance and setting a new standard for speed in Omnimatte techniques.'}, 'zh': {'title': 'OmnimatteZero：无训练的视频对象分解新方法', 'desc': 'Omnimatte旨在将给定视频分解为具有语义意义的层，包括背景和单个对象及其相关效果，如阴影和反射。现有方法通常需要大量的训练或昂贵的自监督优化。本文提出了OmnimatteZero，这是一种无训练的方法，利用现成的预训练视频扩散模型进行omnimatte处理。通过适应零-shot图像修复技术，OmnimatteZero能够有效地从视频中移除对象，提取单个对象层及其效果，并将这些对象合成到新视频中。'}}}, {'id': 'https://huggingface.co/papers/2503.17489', 'title': 'Judge Anything: MLLM as a Judge Across Any Modality', 'url': 'https://huggingface.co/papers/2503.17489', 'abstract': 'Evaluating generative foundation models on open-ended multimodal understanding (MMU) and generation (MMG) tasks across diverse modalities (e.g., images, audio, video) poses significant challenges due to the complexity of cross-modal interactions. To this end, the idea of utilizing Multimodal LLMs (MLLMs) as automated judges has emerged, with encouraging results in assessing vision-language understanding tasks. Moving further, this paper extends MLLM-as-a-Judge across modalities to a unified manner by introducing two benchmarks, TaskAnything and JudgeAnything, to respectively evaluate the overall performance and judging capabilities of MLLMs across any-to-any modality tasks. Specifically, TaskAnything evaluates the MMU and MMG capabilities across 15 any-to-any modality categories, employing 1,500 queries curated from well-established benchmarks. Furthermore, JudgeAnything evaluates the judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from the perspectives of Pair Comparison and Score Evaluation, providing a standardized testbed that incorporates human judgments and detailed rubrics. Our extensive experiments reveal that while these MLLMs show promise in assessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting and 42.79% in Score Evaluation setting), they encounter significant challenges with MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and 30.05% in Score Evaluation setting), exposing cross-modality biases and hallucination issues. To address this, we present OmniArena, an automated platform for evaluating omni-models and multimodal reward models. Our work highlights the need for fairer evaluation protocols and stronger alignment with human preferences. The source code and dataset are publicly available at: https://urrealhero.github.io/judgeanythingweb/.', 'score': 14, 'issue_id': 2875, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'bb040618997e1b0a', 'authors': ['Shu Pu', 'Yaochen Wang', 'Dongping Chen', 'Yuhang Chen', 'Guohao Wang', 'Qi Qin', 'Zhongyi Zhang', 'Zhiyuan Zhang', 'Zetong Zhou', 'Shuang Gong', 'Yi Gui', 'Yao Wan', 'Philip S. Yu'], 'affiliations': ['Huazhong University of Science and Technology', 'University of Illinois Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2503.17489.jpg', 'data': {'categories': ['#multimodal', '#hallucinations', '#benchmark', '#alignment', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Универсальная оценка мультимодальных ИИ-моделей: от понимания к генерации', 'desc': 'Статья представляет новые бенчмарки TaskAnything и JudgeAnything для оценки мультимодальных языковых моделей (MLLM) в задачах понимания и генерации контента различных модальностей. TaskAnything оценивает способности MLLM в 15 категориях задач с различными комбинациями модальностей, используя 1500 запросов. JudgeAnything оценивает способности MLLM выступать в роли судей, сравнивая их оценки с человеческими по методикам попарного сравнения и балльной оценки. Результаты показывают, что MLLM лучше справляются с задачами понимания, чем с задачами генерации, выявляя проблемы межмодальных предубеждений и галлюцинаций.'}, 'en': {'title': 'Enhancing Multimodal Evaluation with MLLMs', 'desc': "This paper discusses the challenges of evaluating generative foundation models in tasks that involve multiple types of data, like images and audio. It introduces Multimodal LLMs (MLLMs) as automated judges to assess these models' understanding and generation capabilities across different modalities. The authors present two benchmarks, TaskAnything and JudgeAnything, to systematically evaluate MLLMs' performance and judging abilities. The findings reveal that while MLLMs perform reasonably well in understanding tasks, they struggle with generation tasks, highlighting the need for improved evaluation methods and alignment with human preferences."}, 'zh': {'title': '多模态评估的新视角', 'desc': '本论文探讨了在多模态理解（MMU）和生成（MMG）任务中评估生成基础模型的挑战，尤其是跨模态交互的复杂性。我们提出了使用多模态大语言模型（MLLMs）作为自动评估者的想法，并引入了两个基准：TaskAnything和JudgeAnything，分别用于评估MLLMs在任何模态任务中的整体性能和判断能力。实验结果显示，尽管MLLMs在MMU任务中表现出一定的潜力，但在MMG任务中面临显著挑战，暴露了跨模态偏见和幻觉问题。为了解决这些问题，我们提出了OmniArena，一个用于评估多模态模型和奖励模型的自动化平台。'}}}, {'id': 'https://huggingface.co/papers/2503.18813', 'title': 'Defeating Prompt Injections by Design', 'url': 'https://huggingface.co/papers/2503.18813', 'abstract': 'Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models may be susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL relies on a notion of a capability to prevent the exfiltration of private data over unauthorized data flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks with provable security in AgentDojo [NeurIPS 2024], a recent agentic security benchmark.', 'score': 13, 'issue_id': 2879, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'a86a20fb5877c5ad', 'authors': ['Edoardo Debenedetti', 'Ilia Shumailov', 'Tianqi Fan', 'Jamie Hayes', 'Nicholas Carlini', 'Daniel Fabian', 'Christoph Kern', 'Chongyang Shi', 'Andreas Terzis', 'Florian Tramèr'], 'affiliations': ['ETH Zurich', 'Google', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.18813.jpg', 'data': {'categories': ['#inference', '#security', '#agents', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'CaMeL: Надежная защита LLM-агентов от атак внедрения промптов', 'desc': 'Статья представляет CaMeL - новый метод защиты агентов на основе больших языковых моделей (LLM) от атак внедрения промптов. CaMeL создает защитный системный слой вокруг LLM, извлекая потоки управления и данных из доверенного запроса. Это предотвращает влияние недоверенных данных на выполнение программы. Метод также использует концепцию возможностей для предотвращения утечки приватных данных. Эффективность CaMeL продемонстрирована решением 67% задач с доказуемой безопасностью в бенчмарке AgentDojo.'}, 'en': {'title': 'Securing LLMs Against Prompt Injection with CaMeL', 'desc': 'This paper introduces CaMeL, a defense mechanism designed to protect Large Language Models (LLMs) from prompt injection attacks when they interact with untrusted data. CaMeL works by creating a secure layer that separates control and data flows, ensuring that untrusted inputs do not affect the execution of the program. Additionally, it implements a capability system to prevent unauthorized access to private data. The effectiveness of CaMeL is demonstrated through its ability to solve 67% of tasks securely in the AgentDojo benchmark.'}, 'zh': {'title': 'CaMeL：保护大型语言模型的安全防线', 'desc': '大型语言模型（LLMs）在与外部环境交互的智能系统中越来越多地被使用。然而，当处理不可信数据时，LLM代理容易受到提示注入攻击。本文提出了CaMeL，这是一种强大的防御机制，它在LLM周围创建了一个保护系统层，即使底层模型可能容易受到攻击。CaMeL通过明确提取控制和数据流来操作，从而确保LLM检索到的不可信数据不会影响程序流程，并通过能力概念进一步提高安全性，防止私密数据通过未经授权的数据流泄露。'}}}, {'id': 'https://huggingface.co/papers/2503.18102', 'title': 'AgentRxiv: Towards Collaborative Autonomous Research', 'url': 'https://huggingface.co/papers/2503.18102', 'abstract': 'Progress in scientific discovery is rarely the result of a single "Eureka" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other\'s research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery.', 'score': 13, 'issue_id': 2879, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': 'f13710802c9e6fb0', 'authors': ['Samuel Schmidgall', 'Michael Moor'], 'affiliations': ['Department of Biosystems Science & Engineering, ETH Zurich', 'Department of Electrical & Computer Engineering, Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18102.jpg', 'data': {'categories': ['#agents', '#math', '#reasoning', '#science', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Коллективный разум ИИ: AgentRxiv объединяет LLM-агентов для ускорения научных открытий', 'desc': 'Статья представляет AgentRxiv - фреймворк, позволяющий агентам на основе больших языковых моделей (LLM) обмениваться результатами исследований через общий препринт-сервер. Эксперименты показывают, что агенты с доступом к предыдущим исследованиям достигают более высоких результатов по сравнению с изолированными агентами. Несколько лабораторий агентов, обменивающихся исследованиями через AgentRxiv, способны быстрее продвигаться к общей цели. Результаты предполагают, что автономные агенты могут играть роль в разработке будущих систем искусственного интеллекта наряду с людьми.'}, 'en': {'title': 'Collaborative AI: Accelerating Research with AgentRxiv', 'desc': 'The paper introduces AgentRxiv, a collaborative framework for large language model (LLM) agents to share and build upon research findings. Unlike traditional isolated workflows, AgentRxiv allows agents to upload and retrieve reports from a shared preprint server, fostering collaboration and iterative improvement. The study shows that agents utilizing prior research achieve significant performance gains, with a 11.4% relative improvement on the MATH-500 benchmark. This collaborative approach not only enhances individual agent performance but also accelerates overall research progress, suggesting a promising future for autonomous agents in scientific discovery.'}, 'zh': {'title': 'AgentRxiv：促进代理实验室协作的研究框架', 'desc': '这篇论文介绍了AgentRxiv，一个框架使得大型语言模型（LLM）代理实验室能够上传和检索共享的预印本报告，从而实现合作和知识共享。通过这种方式，代理实验室可以在彼此的研究基础上进行迭代，提升研究成果。研究表明，能够访问先前研究的代理在性能上有显著提升，相比于孤立操作的代理，提升达到了11.4%。这些结果表明，多个代理实验室通过AgentRxiv合作，可以更快地朝着共同目标前进，并在整体准确性上实现更高的提升。'}}}, {'id': 'https://huggingface.co/papers/2503.17439', 'title': 'LEMMA: Learning from Errors for MatheMatical Advancement in LLMs', 'url': 'https://huggingface.co/papers/2503.17439', 'abstract': "Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines.", 'score': 13, 'issue_id': 2875, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '946d486485fedb03', 'authors': ['Zhuoshi Pan', 'Yu Li', 'Honglin Lin', 'Qizhi Pei', 'Zinan Tang', 'Wei Wu', 'Chenlin Ming', 'H. Vicky Zhao', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.17439.jpg', 'data': {'categories': ['#math', '#training', '#reasoning', '#dataset', '#data'], 'emoji': '🧮', 'ru': {'title': 'Учимся на ошибках: новый подход к улучшению математических способностей ИИ', 'desc': 'Эта статья предлагает метод LEMMA для улучшения способности больших языковых моделей (LLM) решать математические задачи путем обучения на ошибках. LEMMA создает набор данных, состоящий из неправильных решений с ошибочными шагами и связями с правильными решениями для дообучения модели. Авторы вводят метод аугментации ошибок на основе типов ошибок для сбора разнообразных и репрезентативных ошибок. Эксперименты показывают, что LEMMA значительно улучшает производительность по сравнению с другими сильными базовыми моделями.'}, 'en': {'title': 'Empowering LLMs: Learning from Errors to Enhance Reasoning', 'desc': 'This paper introduces a novel approach called Learning from Errors for Mathematical Advancement (LEMMA) to improve the reasoning capabilities of large language models (LLMs) in solving mathematical problems. Unlike traditional methods that focus solely on enhancing correct training data, LEMMA leverages the value of error data by constructing a dataset that includes incorrect solutions paired with reflections on correct solutions. The method systematically analyzes error types and employs a mistake augmentation technique to gather diverse errors, allowing the model to learn from its mistakes. By fine-tuning on this enriched dataset, LEMMA enables LLMs to autonomously correct their errors during the generation process, leading to significant performance gains compared to existing methods.'}, 'zh': {'title': '从错误中学习，提升数学推理能力', 'desc': '大型语言模型（LLMs）在解决数学问题时展现了出色的推理能力。现有的方法主要关注提高正确训练数据的质量，而忽视了错误数据的价值，这可能会妨碍模型的反思能力。我们提出了一种通过学习错误来提升数学推理能力的方法，称为LEMMA。该方法通过构建包含错误步骤的错误解和与正确解的反思连接的数据集，来进行模型的微调，从而使模型能够在生成过程中自主纠正错误。'}}}, {'id': 'https://huggingface.co/papers/2503.18013', 'title': 'Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.18013', 'abstract': 'Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However, constructing high-quality human-annotated preference data and developing robust reward models to mimic these preferences are both costly and challenging. Motivated by this observation, we propose Vision-R1, a novel vision-guided R1-like reinforcement learning algorithm for LVLMs that rewards models with definitive vision feedback. It only leverages curated instruction data, eliminating the need for specialized reward models and handcrafted preference datasets. We incorporate a criterion-driven reward function that further integrates multi-dimensional feedback to evaluate model completions comprehensively based on the vision task logic. Furthermore, we introduce a progressive rule refinement strategy that dynamically adjusts the reward criteria during training, enabling continuous model improvement and mitigating reward hacking. Extensive experiments on both in-distribution and out-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with Vision-R1 achieves consistent performance gains, with even up to 50% improvement and surpassing the state-of-the-art 10x size model.', 'score': 12, 'issue_id': 2876, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '45029d297f1b8ac9', 'authors': ['Yufei Zhan', 'Yousong Zhu', 'Shurong Zheng', 'Hongyin Zhao', 'Fan Yang', 'Ming Tang', 'Jinqiao Wang'], 'affiliations': ['Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Peng Cheng Laboratory, Shenzhen, China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', 'Wuhan AI Research, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.18013.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#benchmark', '#rlhf'], 'emoji': '👁️', 'ru': {'title': 'Vision-R1: Революция в обучении визуально-языковых моделей без ручной разметки', 'desc': 'В статье представлен новый алгоритм обучения с подкреплением для крупных визуально-языковых моделей под названием Vision-R1. Этот метод использует обратную связь на основе зрения для улучшения моделей, не требуя специальных наборов данных о предпочтениях или моделей вознаграждения. Vision-R1 включает функцию вознаграждения на основе критериев и стратегию прогрессивного уточнения правил. Эксперименты показывают значительное улучшение производительности моделей, обученных с помощью Vision-R1, в некоторых случаях превосходящее модели в 10 раз большего размера.'}, 'en': {'title': 'Reinforcing Vision with Vision-R1: Simplifying LVLM Training', 'desc': 'This paper introduces Vision-R1, a new reinforcement learning algorithm designed for Large Vision-Language Models (LVLMs). Unlike traditional methods that require expensive human-annotated preference data, Vision-R1 uses curated instruction data to provide vision feedback directly to the models. The algorithm employs a criterion-driven reward function that assesses model outputs based on the logic of vision tasks, allowing for a more comprehensive evaluation. Additionally, it features a progressive rule refinement strategy that adapts reward criteria during training, leading to significant performance improvements in LVLMs without the need for complex reward models.'}, 'zh': {'title': '视觉引导的强化学习提升模型能力', 'desc': '大型视觉语言模型（LVLMs）通常采用两阶段训练方法：预训练和监督微调。最近，源自语言领域的偏好优化成为一种有效的后训练强化策略，用于提升LVLMs的能力。我们提出了一种新颖的视觉引导R1类强化学习算法Vision-R1，它通过明确的视觉反馈来奖励模型，避免了构建高质量人类标注的偏好数据和开发复杂的奖励模型的高成本。通过引入多维反馈的标准驱动奖励函数，Vision-R1能够全面评估模型的完成情况，并在训练过程中动态调整奖励标准，从而实现持续的模型改进。'}}}, {'id': 'https://huggingface.co/papers/2503.18948', 'title': 'Equivariant Image Modeling', 'url': 'https://huggingface.co/papers/2503.18948', 'abstract': 'Current generative models, such as autoregressive and diffusion approaches, decompose high-dimensional data distribution learning into a series of simpler subtasks. However, inherent conflicts arise during the joint optimization of these subtasks, and existing solutions fail to resolve such conflicts without sacrificing efficiency or scalability. We propose a novel equivariant image modeling framework that inherently aligns optimization targets across subtasks by leveraging the translation invariance of natural visual signals. Our method introduces (1) column-wise tokenization which enhances translational symmetry along the horizontal axis, and (2) windowed causal attention which enforces consistent contextual relationships across positions. Evaluated on class-conditioned ImageNet generation at 256x256 resolution, our approach achieves performance comparable to state-of-the-art AR models while using fewer computational resources. Systematic analysis demonstrates that enhanced equivariance reduces inter-task conflicts, significantly improving zero-shot generalization and enabling ultra-long image synthesis. This work establishes the first framework for task-aligned decomposition in generative modeling, offering insights into efficient parameter sharing and conflict-free optimization. The code and models are publicly available at https://github.com/drx-code/EquivariantModeling.', 'score': 11, 'issue_id': 2880, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'a098ae2b5e7412ee', 'authors': ['Ruixiao Dong', 'Mengde Xu', 'Zigang Geng', 'Li Li', 'Han Hu', 'Shuyang Gu'], 'affiliations': ['Tencent Hunyuan Research', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.18948.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#training', '#open_source', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Эквивариантное моделирование для эффективной генерации изображений', 'desc': 'Статья представляет новый подход к генеративному моделированию изображений, основанный на принципе эквивариантности. Авторы предлагают метод токенизации по столбцам и оконное каузальное внимание для улучшения трансляционной симметрии. Эксперименты на ImageNet показывают, что модель достигает производительности на уровне современных авторегрессионных моделей при меньших вычислительных затратах. Подход демонстрирует улучшенную обобщающую способность и возможность синтеза сверхдлинных изображений.'}, 'en': {'title': 'Aligning Tasks for Efficient Generative Modeling', 'desc': 'This paper introduces a new framework for generative modeling that addresses conflicts in optimizing multiple subtasks. By utilizing the natural translation invariance found in visual data, the proposed method aligns optimization targets, leading to improved efficiency. Key innovations include column-wise tokenization for better symmetry and windowed causal attention to maintain contextual relationships. The framework shows competitive performance in image generation while reducing computational demands, enhancing generalization, and enabling longer image synthesis.'}, 'zh': {'title': '任务对齐的生成建模新框架', 'desc': '当前的生成模型，如自回归和扩散方法，将高维数据分布学习分解为一系列更简单的子任务。然而，在这些子任务的联合优化过程中会出现固有的冲突，现有的解决方案无法在不牺牲效率或可扩展性的情况下解决这些冲突。我们提出了一种新颖的等变图像建模框架，通过利用自然视觉信号的平移不变性，内在地对齐子任务之间的优化目标。我们的研究在256x256分辨率下的类条件ImageNet生成中表现出与最先进的自回归模型相当的性能，同时使用更少的计算资源。'}}}, {'id': 'https://huggingface.co/papers/2503.18940', 'title': 'Training-free Diffusion Acceleration with Bottleneck Sampling', 'url': 'https://huggingface.co/papers/2503.18940', 'abstract': 'Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video resolution. While existing acceleration methods often compromise output quality or necessitate costly retraining, we observe that most diffusion models are pre-trained at lower resolutions, presenting an opportunity to exploit these low-resolution priors for more efficient inference without degrading performance. In this work, we introduce Bottleneck Sampling, a training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. Bottleneck Sampling follows a high-low-high denoising workflow: it performs high-resolution denoising in the initial and final stages while operating at lower resolutions in intermediate steps. To mitigate aliasing and blurring artifacts, we further refine the resolution transition points and adaptively shift the denoising timesteps at each stage. We evaluate Bottleneck Sampling on both image and video generation tasks, where extensive experiments demonstrate that it accelerates inference by up to 3times for image generation and 2.5times for video generation, all while maintaining output quality comparable to the standard full-resolution sampling process across multiple evaluation metrics. Code is available at: https://github.com/tyfeld/Bottleneck-Sampling', 'score': 11, 'issue_id': 2875, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '83ffcf1c20f5d4db', 'authors': ['Ye Tian', 'Xin Xia', 'Yuxi Ren', 'Shanchuan Lin', 'Xing Wang', 'Xuefeng Xiao', 'Yunhai Tong', 'Ling Yang', 'Bin Cui'], 'affiliations': ['Bytedance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18940.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference', '#video'], 'emoji': '⏱️', 'ru': {'title': 'Ускорение диффузионных моделей без потери качества', 'desc': 'Статья представляет новый метод ускорения работы диффузионных моделей под названием Bottleneck Sampling. Этот подход использует предобученные низкоразрешающие модели для уменьшения вычислительных затрат без потери качества выходных данных. Метод следует схеме высокое-низкое-высокое разрешение при денойзинге, что позволяет ускорить вывод в 2.5-3 раза для задач генерации изображений и видео. Bottleneck Sampling не требует переобучения модели и сохраняет качество результатов на уровне стандартного полноразрешающего семплирования.'}, 'en': {'title': 'Speeding Up Diffusion Models with Bottleneck Sampling', 'desc': 'This paper presents Bottleneck Sampling, a new method to speed up diffusion models used for generating images and videos. Traditional diffusion models are slow because they use a complex self-attention mechanism that increases with image resolution. Bottleneck Sampling takes advantage of low-resolution training data to reduce the computational load during inference without sacrificing quality. By using a high-low-high denoising approach, it achieves significant speed improvements—up to 3 times faster for images and 2.5 times for videos—while still producing high-quality outputs.'}, 'zh': {'title': '瓶颈采样：高效的扩散模型推理', 'desc': '扩散模型在视觉内容生成方面表现出色，但在推理时由于计算成本高而难以部署。主要的计算负担来自于自注意力机制在图像或视频分辨率上的二次复杂性。我们提出了一种名为瓶颈采样的框架，利用低分辨率的先验知识来减少计算开销，同时保持输出质量。通过在高分辨率和低分辨率之间进行高低高的去噪工作流程，我们的实验表明，该方法在图像生成中加速推理速度可达3倍，在视频生成中可达2.5倍。'}}}, {'id': 'https://huggingface.co/papers/2503.18908', 'title': 'FFN Fusion: Rethinking Sequential Computation in Large Language Models', 'url': 'https://huggingface.co/papers/2503.18908', 'abstract': 'We introduce FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71X speedup in inference latency and 35X lower per-token cost while maintaining strong performance across benchmarks. Through extensive experiments on models from 49B to 253B parameters, we demonstrate that FFN Fusion becomes increasingly effective at larger scales and can complement existing optimization techniques like quantization and pruning. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.', 'score': 11, 'issue_id': 2877, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '77fd55a50b93d2d4', 'authors': ['Akhiad Bercovich', 'Mohammad Dabbah', 'Omri Puny', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Ehud Karpas', 'Itay Levy', 'Zach Moshe', 'Najeeb Nabwani', 'Tomer Ronen', 'Itamar Schen', 'Elad Segal', 'Ido Shahaf', 'Oren Tropp', 'Ran Zilberstein', 'Ran El-Yaniv'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.18908.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение больших языковых моделей через параллелизацию FFN слоев', 'desc': 'Статья представляет технику оптимизации архитектуры под названием FFN Fusion, которая сокращает последовательные вычисления в больших языковых моделях. Метод идентифицирует последовательности слоев Feed-Forward Network (FFN), которые можно распараллелить с минимальным влиянием на точность модели. Применение этой техники к модели Llama-3.1-405B-Instruct позволило создать Llama-Nemotron-Ultra-253B-Base, достигающую ускорения в 1.71 раза при сохранении производительности. Исследования показали, что FFN Fusion особенно эффективен для крупных моделей и может сочетаться с другими методами оптимизации, такими как квантизация и прунинг.'}, 'en': {'title': 'Accelerating Language Models with FFN Fusion', 'desc': 'The paper presents FFN Fusion, a technique that optimizes large language models by enabling parallel computation of Feed-Forward Network (FFN) layers. This method identifies sequences of FFN layers that can be fused and executed in parallel, which reduces the time it takes for the model to make predictions without sacrificing accuracy. The authors demonstrate this approach on the Llama-3.1-405B-Instruct model, resulting in a new model, Llama-Nemotron-Ultra-253B-Base, that is significantly faster and cheaper to run. The findings suggest that FFN Fusion is particularly beneficial for larger models and can work alongside other optimization methods like quantization and pruning.'}, 'zh': {'title': 'FFN Fusion：提升大型语言模型的推理效率', 'desc': '我们提出了一种名为FFN Fusion的架构优化技术，旨在通过识别和利用并行化的自然机会来减少大型语言模型中的顺序计算。我们的关键见解是，去除特定注意力层后，前馈网络（FFN）层的序列通常可以以最小的准确性影响进行并行化。我们开发了一种原则性的方法来识别和融合这些序列，将其转化为并行操作，从而显著降低推理延迟，同时保持模型行为。通过将这些技术应用于Llama-3.1-405B-Instruct，我们创建了Llama-Nemotron-Ultra-253B-Base（Ultra-253B-Base），该模型在推理延迟上实现了1.71倍的加速，并且每个token的成本降低了35倍，同时在基准测试中保持了强劲的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.18886', 'title': 'CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models', 'url': 'https://huggingface.co/papers/2503.18886', 'abstract': 'Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero*, an improved CFG with two contributions: (a) optimized scale, where a scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the * in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on both text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video (Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG, highlighting its effectiveness in guiding Flow Matching models. (Code is available at github.com/WeichenFan/CFG-Zero-star)', 'score': 11, 'issue_id': 2880, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '1e2a721645115955', 'authors': ['Weichen Fan', 'Amber Yijia Zheng', 'Raymond A. Yeh', 'Ziwei Liu'], 'affiliations': ['Department of Computer Science, Purdue University', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18886.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#video', '#training', '#cv'], 'emoji': '🔬', 'ru': {'title': 'CFG-Zero*: Оптимизированное руководство для улучшения генеративных моделей', 'desc': 'Статья посвящена улучшению техники Classifier-Free Guidance (CFG) для моделей диффузии и потоковых моделей. Авторы аналитически изучили влияние CFG на модели согласования потоков, обученные на гауссовых смесях. На основе наблюдений они предложили улучшенный метод CFG-Zero* с оптимизированным масштабированием и инициализацией нулями. Эксперименты на задачах генерации изображений и видео по тексту показали превосходство CFG-Zero* над стандартным CFG.'}, 'en': {'title': 'Enhancing Image Generation with CFG-Zero*', 'desc': 'This paper explores the limitations of Classifier-Free Guidance (CFG) in flow matching models, particularly during the early training phases when flow estimations are often inaccurate. The authors introduce CFG-Zero*, which enhances CFG by optimizing a scalar to adjust for these inaccuracies and implementing a zero-initialization strategy for the initial steps of the ODE solver. Through experiments in both text-to-image and text-to-video generation, CFG-Zero* shows significant improvements over traditional CFG in terms of image fidelity and controllability. The findings suggest that these modifications lead to better guidance for flow matching models, making them more effective in generating high-quality outputs.'}, 'zh': {'title': 'CFG-Zero*: 提升流模型的引导效果', 'desc': '本文研究了无分类器引导（CFG）在扩散/流模型中的应用，旨在提高图像的真实感和可控性。我们发现，在训练的早期阶段，流估计不准确时，CFG会将样本引导到错误的轨迹。为了解决这个问题，我们提出了CFG-Zero*，它通过优化比例和零初始化来改进CFG。实验结果表明，CFG-Zero*在文本到图像和文本到视频生成任务中均优于传统的CFG，证明了其在引导流匹配模型方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.18923', 'title': 'Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models', 'url': 'https://huggingface.co/papers/2503.18923', 'abstract': 'Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this gap, we introduce Video SimpleQA, the first comprehensive benchmark tailored for factuality evaluation of LVLMs. Our work distinguishes from existing video benchmarks through the following key features: 1) Knowledge required: demanding integration of external knowledge beyond the explicit narrative; 2) Fact-seeking question: targeting objective, undisputed events or relationships, avoiding subjective interpretation; 3) Definitive & short-form answer: Answers are crafted as unambiguous and definitively correct in a short format, enabling automated evaluation through LLM-as-a-judge frameworks with minimal scoring variance; 4) External-source verified: All annotations undergo rigorous validation against authoritative external references to ensure the reliability; 5) Temporal reasoning required: The annotated question types encompass both static single-frame understanding and dynamic temporal reasoning, explicitly evaluating LVLMs factuality under the long-context dependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize key findings as follows: 1) Current LVLMs exhibit notable deficiencies in factual adherence, particularly for open-source models. The best-performing model Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute paradigms show insignificant performance gains, revealing fundamental constraints for enhancing factuality through post-hoc computation; 3) Retrieval-Augmented Generation demonstrates consistent improvements at the cost of additional inference time overhead, presenting a critical efficiency-performance trade-off.', 'score': 10, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '599abe342d833dd0', 'authors': ['Meng Cao', 'Pengfei Hu', 'Yingyao Wang', 'Jihao Gu', 'Haoran Tang', 'Haoze Zhao', 'Jiahua Dong', 'Wangbo Yu', 'Ge Zhang', 'Ian Reid', 'Xiaodan Liang'], 'affiliations': ['Alibaba Group', 'M-A-P', 'MBZUAI', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18923.jpg', 'data': {'categories': ['#video', '#interpretability', '#reasoning', '#long_context', '#multimodal', '#rag', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Новый стандарт оценки фактической точности видео-языковых моделей', 'desc': 'Статья представляет Video SimpleQA - первый комплексный бенчмарк для оценки фактической точности Больших Видео-Языковых Моделей (LVLM). Бенчмарк отличается требованием интеграции внешних знаний, объективностью вопросов и верифицируемостью ответов. Оценка 41 современной LVLM показала значительные недостатки в фактической точности, при этом лучшая модель Gemini-1.5-Pro достигла F-меры всего 54.4%. Исследование выявило ограничения улучшения фактической точности через пост-обработку и компромисс между эффективностью и производительностью при использовании Retrieval-Augmented Generation.'}, 'en': {'title': 'Evaluating Factual Accuracy in Video Language Models', 'desc': 'This paper introduces Video SimpleQA, a new benchmark designed to evaluate the factual accuracy of Large Video Language Models (LVLMs). It focuses on assessing how well these models can integrate external knowledge and answer fact-based questions about video content. The benchmark emphasizes the need for definitive answers and includes rigorous validation against authoritative sources to ensure reliability. The evaluation of 41 LVLMs reveals significant shortcomings in factual adherence, particularly among open-source models, highlighting the challenges in improving factual accuracy in multi-modal contexts.'}, 'zh': {'title': '视频语言模型的事实性评估新基准', 'desc': '最近，大型视频语言模型（LVLMs）的进展显示了它们在多模态理解方面的潜力，但在视频上下文中评估其事实基础仍然是一个重要的未解决挑战。为了解决这个问题，我们引入了Video SimpleQA，这是第一个专门针对LVLMs事实性评估的综合基准。该基准的特点包括：需要整合外部知识、针对客观事件的问题、明确且简短的答案，以及经过外部来源验证的注释。我们对41个最先进的LVLMs进行了广泛评估，发现当前模型在事实遵循方面存在显著不足，尤其是开源模型。'}}}, {'id': 'https://huggingface.co/papers/2503.17811', 'title': 'Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model\n  Collaboration Paradigm for Small Language Models', 'url': 'https://huggingface.co/papers/2503.17811', 'abstract': 'Natural Language to SQL (NL2SQL) has seen significant advancements with large language models (LLMs). However, these models often depend on closed-source systems and high computational resources, posing challenges in data privacy and deployment. In contrast, small language models (SLMs) struggle with NL2SQL tasks, exhibiting poor performance and incompatibility with existing frameworks. To address these issues, we introduce Feather-SQL, a new lightweight framework tailored for SLMs. Feather-SQL improves SQL executability and accuracy through 1) schema pruning and linking, 2) multi-path and multi-candidate generation. Additionally, we introduce the 1+1 Model Collaboration Paradigm, which pairs a strong general-purpose chat model with a fine-tuned SQL specialist, combining strong analytical reasoning with high-precision SQL generation. Experimental results on BIRD demonstrate that Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for models without fine-tuning. The proposed paradigm raises the accuracy ceiling of SLMs to 54.76%, highlighting its effectiveness.', 'score': 10, 'issue_id': 2887, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 марта', 'en': 'March 22', 'zh': '3月22日'}, 'hash': 'a2f4f2ed59d6f67b', 'authors': ['Wenqi Pei', 'Hailing Xu', 'Hengyuan Zhao', 'Shizheng Hou', 'Han Chen', 'Zining Zhang', 'Pingyi Luo', 'Bingsheng He'], 'affiliations': ['4Paradigm', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.17811.jpg', 'data': {'categories': ['#optimization', '#dataset', '#small_models', '#reasoning', '#training'], 'emoji': '🪶', 'ru': {'title': 'Легковесный NL2SQL: Feather-SQL поднимает планку для малых языковых моделей', 'desc': "Статья представляет Feather-SQL - новый легковесный фреймворк для малых языковых моделей (SLM) в задаче преобразования естественного языка в SQL (NL2SQL). Feather-SQL улучшает исполняемость и точность SQL через оптимизацию схемы и многокандидатную генерацию. Авторы также предлагают парадигму сотрудничества '1+1 Model', объединяющую общую чат-модель со специализированной SQL-моделью. Эксперименты показывают значительное улучшение производительности SLM в задаче NL2SQL с использованием предложенного подхода."}, 'en': {'title': 'Empowering Small Models for SQL with Feather-SQL', 'desc': 'This paper presents Feather-SQL, a new framework designed to enhance the performance of small language models (SLMs) in converting natural language to SQL queries. It addresses the limitations of SLMs, which typically struggle with accuracy and compatibility in NL2SQL tasks. Feather-SQL employs techniques like schema pruning and multi-path generation to improve SQL executability and precision. Additionally, the introduction of the 1+1 Model Collaboration Paradigm allows for the combination of a general-purpose chat model with a specialized SQL model, significantly boosting the performance of SLMs in this domain.'}, 'zh': {'title': '轻量级框架，提升小型模型的SQL能力', 'desc': '自然语言转SQL（NL2SQL）在大型语言模型（LLMs）上取得了显著进展，但这些模型通常依赖于封闭源系统和高计算资源，给数据隐私和部署带来了挑战。相比之下，小型语言模型（SLMs）在NL2SQL任务上表现不佳，且与现有框架不兼容。为了解决这些问题，我们提出了Feather-SQL，这是一个专为SLMs设计的轻量级框架，通过模式修剪和链接、多路径和多候选生成来提高SQL的可执行性和准确性。此外，我们引入了1+1模型协作范式，将强大的通用聊天模型与经过微调的SQL专家配对，结合了强大的分析推理和高精度的SQL生成。'}}}, {'id': 'https://huggingface.co/papers/2503.14428', 'title': 'MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.14428', 'abstract': 'Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we propose MagicComp, a training-free method that enhances compositional T2V generation through dual-phase refinement. Specifically, (1) During the Conditioning Stage: We introduce the Semantic Anchor Disambiguation to reinforces subject-specific semantics and resolve inter-subject ambiguity by progressively injecting the directional vectors of semantic anchors into original text embedding; (2) During the Denoising Stage: We propose Dynamic Layout Fusion Attention, which integrates grounding priors and model-adaptive spatial perception to flexibly bind subjects to their spatiotemporal regions through masked attention modulation. Furthermore, MagicComp is a model-agnostic and versatile approach, which can be seamlessly integrated into existing T2V architectures. Extensive experiments on T2V-CompBench and VBench demonstrate that MagicComp outperforms state-of-the-art methods, highlighting its potential for applications such as complex prompt-based and trajectory-controllable video generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.', 'score': 7, 'issue_id': 2875, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '1cd532518024f266', 'authors': ['Hongyu Zhang', 'Yufan Deng', 'Shenghai Yuan', 'Peng Jin', 'Zesen Cheng', 'Yian Zhao', 'Chang Liu', 'Jie Chen'], 'affiliations': ['Peng Cheng Laboratory, Shenzhen, China', 'School of Electronic and Computer Engineering, Peking University, Shenzhen, China', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14428.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#games', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'MagicComp: Усовершенствованная генерация видео по тексту без дополнительного обучения', 'desc': 'MagicComp - это метод генерации видео по тексту, не требующий дополнительного обучения. Он использует двухфазовое уточнение для улучшения композиционной генерации: семантическое разрешение неоднозначности на этапе подготовки условий и динамическое слияние макетов на этапе шумоподавления. Метод решает проблемы связывания атрибутов, определения пространственных отношений и захвата сложных взаимодействий между несколькими объектами. MagicComp может быть интегрирован в существующие архитектуры генерации видео по тексту и превосходит современные методы в экспериментах.'}, 'en': {'title': 'Enhancing Text-to-Video Generation with MagicComp', 'desc': 'This paper presents MagicComp, a novel method for improving text-to-video (T2V) generation using diffusion models. It addresses challenges in accurately linking attributes and understanding spatial relationships between subjects in videos. The method consists of two main phases: the Conditioning Stage, which clarifies subject semantics using Semantic Anchor Disambiguation, and the Denoising Stage, which employs Dynamic Layout Fusion Attention to enhance spatial binding. MagicComp is designed to be adaptable and can be integrated into existing T2V systems, showing superior performance in various benchmarks.'}, 'zh': {'title': 'MagicComp：提升文本到视频生成的创新方法', 'desc': '本文提出了一种名为MagicComp的文本到视频生成方法，旨在解决现有方法在属性绑定、空间关系确定和复杂动作交互方面的不足。该方法通过双阶段的精炼过程来增强组合式T2V生成，首先在条件阶段引入语义锚点消歧，以强化特定主题的语义并解决主题间的歧义。其次，在去噪阶段，提出动态布局融合注意力，通过掩蔽注意力调制灵活绑定主题与其时空区域。MagicComp是一种与模型无关的通用方法，可以无缝集成到现有的T2V架构中，并在多个基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2503.18866', 'title': 'Reasoning to Learn from Latent Thoughts', 'url': 'https://huggingface.co/papers/2503.18866', 'abstract': 'Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining.', 'score': 6, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'ab963a9dd28b0934', 'authors': ['Yangjun Ruan', 'Neil Band', 'Chris J. Maddison', 'Tatsunori Hashimoto'], 'affiliations': ['Stanford University', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.18866.jpg', 'data': {'categories': ['#optimization', '#training', '#transfer_learning', '#synthetic', '#data', '#math'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие скрытых мыслей для эффективного обучения языковых моделей', 'desc': 'Статья предлагает метод повышения эффективности предобучения языковых моделей в условиях ограниченных данных. Авторы предлагают моделировать и выводить скрытые мысли, лежащие в основе процесса генерации текста. Этот подход рассматривает веб-текст как сжатый результат подробного мыслительного процесса человека. Эмпирические результаты показывают значительное улучшение эффективности обучения, особенно в области математики.'}, 'en': {'title': 'Unlocking Data Efficiency through Latent Thought Inference', 'desc': 'This paper addresses the challenge of limited human-written text data for training large language models (LMs). It proposes a method to model and infer the underlying thoughts that lead to text generation, which can enhance data efficiency during pretraining. By treating web text as a condensed version of human thought processes, the authors show that inferring these latent thoughts can lead to better learning outcomes, especially in data-scarce situations. Their experiments demonstrate that this approach not only improves performance on tasks like math but also allows LMs to iteratively enhance their own capabilities without relying heavily on external data.'}, 'zh': {'title': '潜在思维推断提升语言模型预训练效率', 'desc': '这篇论文探讨了在语言模型预训练中，数据增长速度慢于模型规模扩展的问题。作者提出通过显式建模和推断文本生成过程中的潜在思维，可以显著提高预训练的数据效率。研究表明，合成数据方法在推断潜在思维方面的应用，能够在数据受限的情况下，提升模型的学习效果。通过迭代的EM算法，模型能够自我提升性能，并在多个迭代中显著超越仅使用原始数据训练的基线模型。'}}}, {'id': 'https://huggingface.co/papers/2503.15879', 'title': 'Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering', 'url': 'https://huggingface.co/papers/2503.15879', 'abstract': 'Non-factoid question-answering (NFQA) poses a significant challenge due to its open-ended nature, diverse intents, and the need for multi-aspect reasoning, which renders conventional factoid QA approaches, including retrieval-augmented generation (RAG), inadequate. Unlike factoid questions, non-factoid questions (NFQs) lack definitive answers and require synthesizing information from multiple sources across various reasoning dimensions. To address these limitations, we introduce Typed-RAG, a type-aware multi-aspect decomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies NFQs into distinct types -- such as debate, experience, and comparison -- and applies aspect-based decomposition to refine retrieval and generation strategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and aggregating the results, Typed-RAG generates more informative and contextually relevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark dataset covering diverse NFQ types. Experimental results demonstrate that Typed-RAG outperforms baselines, thereby highlighting the importance of type-aware decomposition for effective retrieval and generation in NFQA. Our code and dataset are available at https://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.', 'score': 6, 'issue_id': 2878, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '7bad41c869c73370', 'authors': ['DongGeon Lee', 'Ahjeong Park', 'Hyeri Lee', 'Hyeonseo Nam', 'Yunho Maeng'], 'affiliations': ['Ewha Womans University', 'Independent Researcher', 'KT', 'LLM Experimental Lab, MODULABS', 'Pohang University of Science and Technology', 'Sookmyung Womens University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15879.jpg', 'data': {'categories': ['#rag', '#open_source', '#benchmark', '#optimization', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Typed-RAG: умное разложение вопросов для лучших ответов', 'desc': 'Статья представляет новый подход к ответам на нефактоидные вопросы (NFQA) под названием Typed-RAG. Этот метод классифицирует вопросы по типам и применяет декомпозицию на аспекты для улучшения стратегий поиска и генерации ответов. Авторы также представляют новый набор данных Wiki-NFQA для оценки эффективности Typed-RAG. Экспериментальные результаты показывают, что Typed-RAG превосходит базовые методы в задаче NFQA.'}, 'en': {'title': 'Typed-RAG: Enhancing NFQA with Type-Aware Decomposition', 'desc': 'This paper addresses the challenges of non-factoid question-answering (NFQA), which involves questions that do not have straightforward answers and require complex reasoning. The authors propose a new framework called Typed-RAG, which enhances the retrieval-augmented generation (RAG) approach by classifying NFQs into specific types and breaking them down into simpler sub-queries. By focusing on individual aspects of the questions, Typed-RAG improves the quality of the generated responses, making them more relevant and informative. The effectiveness of this method is validated through a new benchmark dataset, Wiki-NFQA, showing that type-aware decomposition significantly boosts performance in NFQA tasks.'}, 'zh': {'title': '类型感知，提升非事实问答的效果', 'desc': '非事实问答（NFQA）因其开放性、多样化的意图和多方面推理的需求而面临重大挑战，传统的事实问答方法（如检索增强生成RAG）无法满足这些需求。与事实问题不同，非事实问题（NFQ）没有明确的答案，需要从多个来源综合信息。为了解决这些问题，我们提出了Typed-RAG，这是一种在RAG范式下的类型感知多方面分解框架。Typed-RAG将NFQ分类为不同类型，并通过基于方面的分解来优化检索和生成策略，从而生成更具信息量和上下文相关的回答。'}}}, {'id': 'https://huggingface.co/papers/2503.18769', 'title': 'AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning', 'url': 'https://huggingface.co/papers/2503.18769', 'abstract': 'This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet.', 'score': 5, 'issue_id': 2875, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'e92ee9df78b66019', 'authors': ['Alan Dao', 'Dinh Bach Vu', 'Bui Quang Huy'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.18769.jpg', 'data': {'categories': ['#architecture', '#synthetic', '#reasoning', '#3d'], 'emoji': '🧠', 'ru': {'title': 'AlphaSpace: Прорыв в пространственном мышлении ИИ', 'desc': 'AlphaSpace - это новая методология, разработанная для улучшения пространственного мышления больших языковых моделей (LLM) в навигации по 3D декартовому пространству. Она использует токенизацию на основе семантики, кодируя информацию о высоте через специальные семантические токены, и интегрирует преимущественно символические синтетические данные для рассуждений. Этот подход позволяет LLM точно манипулировать объектами, позиционируя их по конкретным координатам [x, y, z]. Экспериментальные результаты показывают, что AlphaSpace значительно превосходит существующие модели в подзадачах манипулирования, достигая общей точности 66,67%.'}, 'en': {'title': 'Enhancing 3D Navigation in Language Models with AlphaSpace', 'desc': 'This paper introduces AlphaSpace, a new method aimed at improving how large language models (LLMs) understand and navigate 3D spaces. It uses a unique tokenization method that incorporates height information through special semantic tokens, allowing for better spatial reasoning. By combining this with symbolic reasoning data, AlphaSpace enables LLMs to effectively manipulate objects in a 3D environment by placing them at precise coordinates. The results show that AlphaSpace achieves a notable accuracy of 66.67% in manipulation tasks, outperforming other models like GPT-4o and Claude 3.5 Sonnet.'}, 'zh': {'title': 'AlphaSpace：提升语言模型的空间推理能力', 'desc': '本文介绍了一种新方法AlphaSpace，旨在提升大型语言模型（LLMs）在三维笛卡尔空间导航中的空间推理能力。AlphaSpace采用基于语义的标记化策略，通过专门的语义标记编码高度信息，并主要整合符号合成推理数据。该方法使得LLMs能够准确地通过特定的[x, y, z]坐标来操作物体。实验结果表明，AlphaSpace在操作子任务上显著优于现有模型，总准确率达到66.67%，而GPT-4o为37.5%，Claude 3.5 Sonnet为29.17%。'}}}, {'id': 'https://huggingface.co/papers/2503.17422', 'title': 'V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms', 'url': 'https://huggingface.co/papers/2503.17422', 'abstract': 'The recent exponential growth of Large Language Models (LLMs) has relied on GPU-based systems. However, CPUs are emerging as a flexible and lower-cost alternative, especially when targeting inference and reasoning workloads. RISC-V is rapidly gaining traction in this area, given its open and vendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the corresponding software ecosystem are not fully mature and streamlined, given the requirement of domain-specific tuning. This paper aims at filling this gap, focusing on optimizing LLM inference on the Sophon SG2042, the first commercially available many-core RISC-V CPU with vector processing capabilities.   On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1 Distill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s for token generation and 6.54/3.68 token/s for prompt processing, with a speed up of up 2.9x/3.0x compared to our baseline.', 'score': 5, 'issue_id': 2876, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '3811c1f2a2e12813', 'authors': ['Javier J. Poveda Rodrigo', 'Mohamed Amine Ahmdi', 'Alessio Burrello', 'Daniele Jahier Pagliari', 'Luca Benini'], 'affiliations': ['DAUIN, Politecnico of Turin, Turin, Italy', 'ETHZ, Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2503.17422.jpg', 'data': {'categories': ['#architecture', '#optimization', '#reasoning', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей на RISC-V процессорах', 'desc': 'Данная статья посвящена оптимизации инференса больших языковых моделей (LLM) на процессорах RISC-V, в частности на Sophon SG2042. Авторы исследуют возможности использования CPU как альтернативы GPU для задач обработки естественного языка. В работе представлены результаты оптимизации двух современных LLM моделей - DeepSeek R1 Distill Llama 8B и DeepSeek R1 Distill QWEN 14B. Достигнуто значительное ускорение инференса по сравнению с базовой реализацией, до 2.9-3.0 раз.'}, 'en': {'title': 'Unlocking LLM Potential with RISC-V CPUs', 'desc': 'This paper discusses the potential of using CPUs, specifically RISC-V architecture, for optimizing Large Language Model (LLM) inference. It highlights the advantages of RISC-V, such as its flexibility and cost-effectiveness, especially for reasoning tasks. The authors focus on the Sophon SG2042, a many-core RISC-V CPU, and demonstrate significant performance improvements in token generation and prompt processing for two advanced LLMs. The results show up to 3x speed improvements compared to traditional systems, indicating a promising direction for LLM deployment on CPU architectures.'}, 'zh': {'title': '用RISC-V优化大型语言模型推理', 'desc': '近年来，大型语言模型（LLMs）的快速发展依赖于基于GPU的系统。然而，CPU作为一种灵活且成本更低的替代方案，正在逐渐崭露头角，特别是在推理和推断工作负载方面。RISC-V因其开放和中立的指令集架构（ISA）而在这一领域迅速获得关注。本文旨在优化在Sophon SG2042上进行LLM推理，展示了在两个最新的优化推理模型上实现的显著性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.18559', 'title': 'AMD-Hummingbird: Towards an Efficient Text-to-Video Model', 'url': 'https://huggingface.co/papers/2503.18559', 'abstract': 'Text-to-Video (T2V) generation has attracted significant attention for its ability to synthesize realistic videos from textual descriptions. However, existing models struggle to balance computational efficiency and high visual quality, particularly on resource-limited devices, e.g.,iGPUs and mobile phones. Most prior work prioritizes visual fidelity while overlooking the need for smaller, more efficient models suitable for real-world deployment. To address this challenge, we propose a lightweight T2V framework, termed Hummingbird, which prunes existing models and enhances visual quality through visual feedback learning. Our approach reduces the size of the U-Net from 1.4 billion to 0.7 billion parameters, significantly improving efficiency while preserving high-quality video generation. Additionally, we introduce a novel data processing pipeline that leverages Large Language Models (LLMs) and Video Quality Assessment (VQA) models to enhance the quality of both text prompts and video data. To support user-driven training and style customization, we publicly release the full training code, including data processing and model training. Extensive experiments show that our method achieves a 31X speedup compared to state-of-the-art models such as VideoCrafter2, while also attaining the highest overall score on VBench. Moreover, our method supports the generation of videos with up to 26 frames, addressing the limitations of existing U-Net-based methods in long video generation. Notably, the entire training process requires only four GPUs, yet delivers performance competitive with existing leading methods. Hummingbird presents a practical and efficient solution for T2V generation, combining high performance, scalability, and flexibility for real-world applications.', 'score': 4, 'issue_id': 2877, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'f6ded1274ae1fbf4', 'authors': ['Takashi Isobe', 'He Cui', 'Dong Zhou', 'Mengmeng Ge', 'Dong Li', 'Emad Barsoum'], 'affiliations': ['Advanced Micro Devices, Inc.', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18559.jpg', 'data': {'categories': ['#long_context', '#video', '#optimization', '#open_source', '#training', '#small_models', '#data'], 'emoji': '🐦', 'ru': {'title': 'Эффективная генерация видео по тексту для мобильных устройств', 'desc': 'Статья представляет легковесную модель генерации видео по тексту под названием Hummingbird. Авторы уменьшили размер U-Net с 1,4 до 0,7 миллиардов параметров, сохранив при этом высокое качество генерации. В работе используются большие языковые модели (LLM) и модели оценки качества видео (VQA) для улучшения текстовых запросов и видеоданных. Модель Hummingbird показывает 31-кратное ускорение по сравнению с современными аналогами и поддерживает генерацию видео длиной до 26 кадров.'}, 'en': {'title': 'Hummingbird: Efficient Text-to-Video Generation for Real-World Use', 'desc': 'This paper introduces Hummingbird, a lightweight framework for Text-to-Video (T2V) generation that aims to improve efficiency without sacrificing visual quality. By pruning the U-Net model from 1.4 billion to 0.7 billion parameters, Hummingbird achieves a significant speedup of 31 times compared to existing models like VideoCrafter2. The framework also incorporates a novel data processing pipeline that utilizes Large Language Models and Video Quality Assessment to enhance both text prompts and video data. With the ability to generate videos with up to 26 frames and requiring only four GPUs for training, Hummingbird offers a scalable and practical solution for real-world T2V applications.'}, 'zh': {'title': '轻量级文本到视频生成的高效解决方案', 'desc': '本文提出了一种轻量级的文本到视频生成框架，称为Hummingbird，旨在提高视频生成的效率和视觉质量。该框架通过剪枝现有模型，将U-Net的参数从14亿减少到7亿，从而在保持高质量视频生成的同时显著提高了计算效率。我们还引入了一种新的数据处理流程，利用大型语言模型和视频质量评估模型来提升文本提示和视频数据的质量。实验结果表明，Hummingbird在速度和性能上均优于现有的最先进模型，适用于资源有限的设备。'}}}, {'id': 'https://huggingface.co/papers/2503.18018', 'title': 'Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural\n  Contexts?', 'url': 'https://huggingface.co/papers/2503.18018', 'abstract': "Large Language Models (LLMs) have significantly advanced various fields, particularly coding, mathematical reasoning, and logical problem solving. However, a critical question remains: Do these mathematical reasoning abilities persist when LLMs are presented with culturally adapted math problems? Specifically, how do LLMs perform when faced with math problems embedded in cultural contexts that have no significant representation in main stream web-scale AI training data? To explore this, we generated six synthetic cultural datasets from GSM8K, a widely used benchmark for assessing LLMs' mathematical reasoning skills. While preserving the mathematical logic and numerical values of the original GSM8K test set, we modify cultural elements such as personal names, food items, place names, etc. These culturally adapted datasets provide a more reliable framework for evaluating LLMs' mathematical reasoning under shifting cultural contexts. Our findings reveal that LLMs struggle with math problems when cultural references change, even though the underlying mathematical structure remains constant. Smaller models exhibit greater performance drops compared to larger models. Interestingly, our results also suggest that cultural familiarity can enhance mathematical reasoning. Even models with no explicit mathematical training but exposure to relevant cultural contexts sometimes outperform larger, mathematically proficient models on culturally embedded math problems. This study highlights the impact of cultural context on the mathematical reasoning abilities of LLMs, underscoring the need for more diverse and representative training data to improve robustness in real-world applications. The benchmark data sets and script for reproducing the results are available at https://github.com/akarim23131/Lost_in_Cultural_Translation", 'score': 4, 'issue_id': 2883, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '38376682f477cce9', 'authors': ['Aabid Karim', 'Abdul Karim', 'Bhoomika Lohana', 'Matt Keon', 'Jaswinder Singh', 'Abdul Sattar'], 'affiliations': ['155mv Research Lab', 'Griffith University', 'Microsoft', 'Millcrest Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.18018.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#math', '#data', '#reasoning', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'Культурный контекст влияет на математические способности ИИ', 'desc': 'Исследование показывает, что большие языковые модели (LLM) испытывают трудности с решением математических задач, когда меняется культурный контекст, несмотря на сохранение математической структуры. Авторы создали шесть синтетических культурных наборов данных на основе GSM8K, изменяя культурные элементы, такие как имена и названия мест. Результаты демонстрируют, что меньшие модели показывают большее снижение производительности по сравнению с более крупными. Исследование подчеркивает важность разнообразных и репрезентативных обучающих данных для повышения устойчивости LLM в реальных приложениях.'}, 'en': {'title': 'Cultural Context Matters: LLMs and Math Reasoning', 'desc': 'This paper investigates how Large Language Models (LLMs) perform on mathematical reasoning tasks when the problems are culturally adapted. It creates six synthetic datasets from the GSM8K benchmark, altering cultural elements while keeping the math intact. The study finds that LLMs struggle with culturally modified math problems, especially smaller models, indicating that cultural context significantly affects their reasoning abilities. Additionally, it suggests that familiarity with cultural references can enhance performance, highlighting the importance of diverse training data for improving LLM robustness.'}, 'zh': {'title': '文化背景影响数学推理能力', 'desc': '大型语言模型（LLMs）在编码、数学推理和逻辑问题解决等领域取得了显著进展。然而，当这些模型面对文化适应的数学问题时，它们的数学推理能力是否依然存在是一个重要问题。研究发现，当文化背景发生变化时，LLMs在解决数学问题时表现不佳，尽管数学结构保持不变。我们的研究强调了文化背景对LLMs数学推理能力的影响，表明需要更具多样性和代表性的训练数据以提高模型在实际应用中的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.17500', 'title': 'Variance Control via Weight Rescaling in LLM Pre-training', 'url': 'https://huggingface.co/papers/2503.17500', 'abstract': 'The outcome of Large Language Model (LLM) pre-training strongly depends on weight initialization and variance control strategies. Although the importance of initial variance control has been well documented in neural networks in general, the literature on initialization and management of its growth during LLM pre-training, specifically, is somewhat sparse. In this paper, we introduce the Layer Index Rescaling (LIR) weight initialization scheme, and the Target Variance Rescaling (TVR) variance control strategy. Experiments on a 1B parameter LLaMA model demonstrate that better variance management using these techniques yields substantial improvements in downstream task performance (up to 4.6% on common pre-training benchmarks) and reduces extreme activation values, thus mitigating challenges associated with quantization and low-precision training. Our code is available at: https://github.com/bluorion-com/weight_rescaling.', 'score': 4, 'issue_id': 2878, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '56b451c0f204aeda', 'authors': ['Louis Owen', 'Abhay Kumar', 'Nilabhra Roy Chowdhury', 'Fabian Güra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2503.17500.jpg', 'data': {'categories': ['#inference', '#benchmark', '#optimization', '#training', '#architecture'], 'emoji': '⚖️', 'ru': {'title': 'Точная настройка весов - ключ к эффективным языковым моделям', 'desc': 'Статья представляет новые методы инициализации весов и контроля дисперсии для предобучения больших языковых моделей (LLM). Авторы предлагают схему инициализации весов Layer Index Rescaling (LIR) и стратегию контроля дисперсии Target Variance Rescaling (TVR). Эксперименты на модели LLaMA с 1 млрд параметров показывают, что эти техники улучшают производительность на целевых задачах и уменьшают экстремальные значения активаций. Результаты демонстрируют важность правильного управления дисперсией при обучении LLM.'}, 'en': {'title': 'Enhancing LLM Performance through Innovative Weight and Variance Management', 'desc': 'This paper discusses how the performance of Large Language Models (LLMs) during pre-training is influenced by how their weights are initialized and how variance is controlled. It introduces two new techniques: Layer Index Rescaling (LIR) for weight initialization and Target Variance Rescaling (TVR) for managing variance growth. The authors show that applying these methods to a 1B parameter LLaMA model leads to significant improvements in performance on various tasks, achieving up to a 4.6% increase on standard benchmarks. Additionally, these techniques help reduce extreme activation values, which can complicate quantization and low-precision training.'}, 'zh': {'title': '优化大型语言模型的预训练性能', 'desc': '本论文探讨了大型语言模型（LLM）预训练中权重初始化和方差控制策略的重要性。我们提出了一种新的权重初始化方案，称为层索引重缩放（LIR），以及目标方差重缩放（TVR）方差控制策略。实验表明，使用这些技术进行更好的方差管理可以显著提高下游任务的性能，最高可达4.6%的提升，并减少极端激活值，从而缓解量化和低精度训练带来的挑战。我们的代码已在GitHub上发布，供研究人员使用。'}}}, {'id': 'https://huggingface.co/papers/2503.18352', 'title': 'Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18352', 'abstract': 'In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and Compression Ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis.', 'score': 3, 'issue_id': 2882, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '16c79c2acf245efb', 'authors': ['Jinjin Zhang', 'Qiuyu Huang', 'Junjie Liu', 'Xiefan Guo', 'Di Huang'], 'affiliations': ['Meituan', 'School of Computer Science and Engineering, Beihang University, Beijing 100191, China', 'State Key Laboratory of Complex and Critical Software Environment, Beihang University, Beijing 100191, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.18352.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#training', '#cv', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Революция в генерации 4K изображений с помощью диффузионных моделей', 'desc': 'Статья представляет Diffusion-4K - новый фреймворк для синтеза ультра-высокого разрешения изображений с помощью диффузионных моделей текст-в-изображение. Авторы создали набор данных Aesthetic-4K и метрики для оценки качества 4K изображений. Они предложили метод файн-тюнинга на основе вейвлетов для обучения на фотореалистичных 4K изображениях. Эксперименты показали превосходство Diffusion-4K в синтезе изображений сверхвысокого разрешения.'}, 'en': {'title': 'Revolutionizing Ultra-High-Resolution Image Synthesis with Diffusion-4K', 'desc': 'This paper introduces Diffusion-4K, a new framework designed for creating ultra-high-resolution images directly from text using diffusion models. It presents the Aesthetic-4K Benchmark, a dataset of 4K images and captions, along with new metrics like GLCM Score and Compression Ratio for evaluating image quality. The authors also propose a wavelet-based fine-tuning method that enhances the training of diffusion models to produce detailed 4K images. Overall, Diffusion-4K shows significant improvements in image synthesis quality and adherence to text prompts compared to existing methods.'}, 'zh': {'title': 'Diffusion-4K：超高分辨率图像合成的新突破', 'desc': '本文提出了一种新颖的框架Diffusion-4K，用于直接生成超高分辨率图像，采用文本到图像的扩散模型。我们构建了Aesthetic-4K基准数据集，解决了缺乏公开4K图像合成数据集的问题，并引入了GLCM评分和压缩比等指标来评估图像细节。我们还提出了一种基于小波的微调方法，适用于各种潜在扩散模型，能够有效合成高细节的4K图像。实验结果表明，Diffusion-4K在高质量图像合成和文本提示遵循方面表现出色，尤其是在现代大规模扩散模型的支持下。'}}}, {'id': 'https://huggingface.co/papers/2503.18470', 'title': 'MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse', 'url': 'https://huggingface.co/papers/2503.18470', 'abstract': 'We present MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance 3D spatial reasoning in vision-language models (VLMs), enabling real-time 3D scene generation without the need for hard-coded optimizations. MetaSpatial addresses two core challenges: (i) the lack of internalized 3D spatial reasoning in VLMs, which limits their ability to generate realistic layouts, and (ii) the inefficiency of traditional supervised fine-tuning (SFT) for layout generation tasks, as perfect ground truth annotations are unavailable. Our key innovation is a multi-turn RL-based optimization mechanism that integrates physics-aware constraints and rendered image evaluations, ensuring generated 3D layouts are coherent, physically plausible, and aesthetically consistent. Methodologically, MetaSpatial introduces an adaptive, iterative reasoning process, where the VLM refines spatial arrangements over multiple turns by analyzing rendered outputs, improving scene coherence progressively. Empirical evaluations demonstrate that MetaSpatial significantly enhances the spatial consistency and formatting stability of various scale models. Post-training, object placements are more realistic, aligned, and functionally coherent, validating the effectiveness of RL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game development applications. Our code, data, and training pipeline are publicly available at https://github.com/PzySeere/MetaSpatial.', 'score': 2, 'issue_id': 2881, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '61b0d03727bc1ba4', 'authors': ['Zhenyu Pan', 'Han Liu'], 'affiliations': ['Department of Computer Science Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18470.jpg', 'data': {'categories': ['#3d', '#training', '#rl', '#optimization', '#games', '#open_source', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'MetaSpatial: Улучшение 3D пространственного мышления ИИ с помощью обучения с подкреплением', 'desc': 'MetaSpatial - это первая система на основе обучения с подкреплением для улучшения 3D пространственного мышления в визуально-языковых моделях. Она позволяет генерировать реалистичные 3D сцены в реальном времени без жестко закодированных оптимизаций. MetaSpatial использует многоэтапный механизм оптимизации с учетом физических ограничений и оценкой отрендеренных изображений. Система демонстрирует значительное улучшение пространственной согласованности и стабильности форматирования для моделей различного масштаба.'}, 'en': {'title': 'Revolutionizing 3D Spatial Reasoning with Reinforcement Learning', 'desc': 'MetaSpatial is a novel framework that uses reinforcement learning (RL) to improve 3D spatial reasoning in vision-language models (VLMs). It tackles the challenges of inadequate internal 3D reasoning and the limitations of traditional supervised fine-tuning for layout generation. The framework employs a multi-turn RL optimization process that incorporates physics-aware constraints and evaluations of rendered images to create realistic and coherent 3D layouts. Through iterative refinement, MetaSpatial enhances the spatial consistency and functional coherence of object placements, making it valuable for applications in the metaverse, AR/VR, and game development.'}, 'zh': {'title': 'MetaSpatial：提升3D空间推理的强化学习框架', 'desc': 'MetaSpatial是第一个基于强化学习的框架，旨在增强视觉语言模型中的3D空间推理能力，实现实时3D场景生成，而无需硬编码优化。该框架解决了两个核心挑战：一是视觉语言模型缺乏内在的3D空间推理能力，限制了其生成逼真布局的能力；二是传统的监督微调在布局生成任务中效率低下，因为完美的真实标注不可用。MetaSpatial的创新在于采用多轮强化学习优化机制，结合物理约束和渲染图像评估，确保生成的3D布局一致、物理上合理且美观。通过适应性迭代推理过程，MetaSpatial使视觉语言模型能够在多个回合中逐步改进空间安排，从而提高场景的一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.17760', 'title': 'CODA: Repurposing Continuous VAEs for Discrete Tokenization', 'url': 'https://huggingface.co/papers/2503.17760', 'abstract': 'Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set of codes. Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce CODA(COntinuous-to-Discrete Adaptation), a framework that decouples compression and discretization. Instead of training discrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs -- already optimized for perceptual compression -- into discrete tokenizers via a carefully designed discretization process. By primarily focusing on discretization, CODA ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs. Empirically, with 6 times less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of 100% and notable reconstruction FID (rFID) of 0.43 and 1.34 for 8 times and 16 times compression on ImageNet 256times 256 benchmark.', 'score': 2, 'issue_id': 2887, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 марта', 'en': 'March 22', 'zh': '3月22日'}, 'hash': '5a93c7572e0fb46c', 'authors': ['Zeyu Liu', 'Zanlin Ni', 'Yeguo Hua', 'Xin Deng', 'Xiao Ma', 'Cheng Zhong', 'Gao Huang'], 'affiliations': ['Lenovo Research, AI Lab', 'Renmin University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.17760.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#cv', '#architecture', '#training'], 'emoji': '🖼️', 'ru': {'title': 'CODA: эффективная дискретизация изображений для генеративных моделей', 'desc': 'Статья представляет CODA - новый подход к дискретизации изображений для задач генерации. CODA адаптирует непрерывные вариационные автоэнкодеры в дискретные токенизаторы, разделяя процессы сжатия и дискретизации. Этот метод обеспечивает стабильное обучение, полное использование кодовой книги и высокое качество реконструкции. По сравнению со стандартным VQGAN, CODA достигает лучших результатов при меньших вычислительных затратах.'}, 'en': {'title': 'Decoupling Compression and Discretization for Better Visual Tokenization', 'desc': 'This paper presents CODA, a novel framework for transforming images into discrete tokens for visual generation. Unlike traditional methods that combine compression and discretization, CODA separates these tasks to enhance training stability and efficiency. By adapting existing continuous Variational Autoencoders (VAEs) for discretization, CODA achieves high-quality visual representations with optimal codebook usage. The results demonstrate that CODA requires significantly less training resources while achieving superior reconstruction quality compared to standard methods.'}, 'zh': {'title': 'CODA：高效的视觉标记离散化框架', 'desc': '本文介绍了一种新的框架CODA（连续到离散适应），用于将图像转换为离散的视觉标记序列。传统的离散标记器在压缩和离散化任务上通常会导致训练不稳定和重建质量低下。CODA通过将压缩和离散化过程解耦，利用现有的连续变分自编码器（VAE）进行适应，从而提高了训练的稳定性和效率。实验结果表明，CODA在训练预算上比标准VQGAN少6倍，同时实现了100%的代码本利用率和优异的重建FID值。'}}}, {'id': 'https://huggingface.co/papers/2503.17735', 'title': 'RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation', 'url': 'https://huggingface.co/papers/2503.17735', 'abstract': 'Recently, great progress has been made in video generation technology, attracting the widespread attention of scholars. To apply this technology to downstream applications under resource-constrained conditions, researchers usually fine-tune the pre-trained models based on parameter-efficient tuning methods such as Adapter or Lora. Although these methods can transfer the knowledge from the source domain to the target domain, fewer training parameters lead to poor fitting ability, and the knowledge from the source domain may lead to the inference process deviating from the target domain. In this paper, we argue that under constrained resources, training a smaller video generation model from scratch using only million-level samples can outperform parameter-efficient tuning on larger models in downstream applications: the core lies in the effective utilization of data and curriculum strategy. Take animated sticker generation (ASG) as a case study, we first construct a discrete frame generation network for stickers with low frame rates, ensuring that its parameters meet the requirements of model training under constrained resources. In order to provide data support for models trained from scratch, we come up with a dual-mask based data utilization strategy, which manages to improve the availability and expand the diversity of limited data. To facilitate convergence under dual-mask situation, we propose a difficulty-adaptive curriculum learning method, which decomposes the sample entropy into static and adaptive components so as to obtain samples from easy to difficult. The experiment demonstrates that our resource-efficient dual-mask training framework is quantitatively and qualitatively superior to efficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the feasibility of our method on downstream tasks under constrained resources. Code will be available.', 'score': 2, 'issue_id': 2876, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 марта', 'en': 'March 22', 'zh': '3月22日'}, 'hash': '186b92c438925eb6', 'authors': ['Zhiqiang Yuan', 'Ting Zhang', 'Ying Deng', 'Jiapei Zhang', 'Yeshuang Zhu', 'Zexi Jia', 'Jie Zhou', 'Jinchao Zhang'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2503.17735.jpg', 'data': {'categories': ['#video', '#dataset', '#optimization', '#training', '#transfer_learning'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео с нуля вместо тонкой настройки больших моделей', 'desc': 'Статья представляет новый подход к генерации видео в условиях ограниченных ресурсов. Авторы предлагают обучать с нуля небольшую модель на миллионах образцов вместо тонкой настройки больших предобученных моделей. Ключевые элементы подхода включают эффективное использование данных с помощью стратегии двойной маски и адаптивное обучение по учебной программе. Эксперименты на задаче генерации анимированных стикеров показывают превосходство предложенного метода над существующими подходами.'}, 'en': {'title': 'Train Small, Win Big: Efficient Video Generation Under Constraints', 'desc': 'This paper discusses advancements in video generation technology and its application in resource-limited environments. It challenges the effectiveness of parameter-efficient tuning methods like Adapter and Lora, suggesting that training smaller models from scratch can yield better results with limited data. The authors introduce a dual-mask data utilization strategy to enhance data diversity and a difficulty-adaptive curriculum learning method to improve model training. Their experiments show that this new approach outperforms existing tuning methods, demonstrating its potential for downstream applications.'}, 'zh': {'title': '资源受限下的视频生成新策略', 'desc': '最近，视频生成技术取得了显著进展，吸引了学者们的广泛关注。为了在资源受限的条件下应用这一技术，研究人员通常基于参数高效的调优方法对预训练模型进行微调。本文提出在资源受限的情况下，从头开始训练一个较小的视频生成模型，使用百万级样本，能够在下游应用中超越大型模型的参数高效调优。我们通过构建低帧率贴纸的离散帧生成网络和双掩码数据利用策略，结合难度自适应的课程学习方法，显著提高了模型的训练效果。'}}}, {'id': 'https://huggingface.co/papers/2503.16924', 'title': 'Optimized Minimal 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.16924', 'abstract': '3D Gaussian Splatting (3DGS) has emerged as a powerful representation for real-time, high-performance rendering, enabling a wide range of applications. However, representing 3D scenes with numerous explicit Gaussian primitives imposes significant storage and memory overhead. Recent studies have shown that high-quality rendering can be achieved with a substantially reduced number of Gaussians when represented with high-precision attributes. Nevertheless, existing 3DGS compression methods still rely on a relatively large number of Gaussians, focusing primarily on attribute compression. This is because a smaller set of Gaussians becomes increasingly sensitive to lossy attribute compression, leading to severe quality degradation. Since the number of Gaussians is directly tied to computational costs, it is essential to reduce the number of Gaussians effectively rather than only optimizing storage. In this paper, we propose Optimized Minimal Gaussians representation (OMG), which significantly reduces storage while using a minimal number of primitives. First, we determine the distinct Gaussian from the near ones, minimizing redundancy without sacrificing quality. Second, we propose a compact and precise attribute representation that efficiently captures both continuity and irregularity among primitives. Additionally, we propose a sub-vector quantization technique for improved irregularity representation, maintaining fast training with a negligible codebook size. Extensive experiments demonstrate that OMG reduces storage requirements by nearly 50% compared to the previous state-of-the-art and enables 600+ FPS rendering while maintaining high rendering quality. Our source code is available at https://maincold2.github.io/omg/.', 'score': 2, 'issue_id': 2877, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '280ac899f8c492d0', 'authors': ['Joo Chan Lee', 'Jong Hwan Ko', 'Eunbyung Park'], 'affiliations': ['Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16924.jpg', 'data': {'categories': ['#optimization', '#3d', '#inference', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'OMG: Оптимизация 3D Gaussian Splatting для эффективного рендеринга', 'desc': 'В этой статье представлен метод Optimized Minimal Gaussians (OMG) для оптимизации 3D Gaussian Splatting. OMG значительно сокращает требования к хранению данных и использует минимальное количество примитивов, сохраняя при этом высокое качество рендеринга. Метод включает определение различных гауссиан среди близких и компактное представление атрибутов, эффективно capturing непрерывность и нерегулярность примитивов. Авторы также предлагают технику субвекторной квантизации для улучшенного представления нерегулярности, сохраняя быстрое обучение с незначительным размером кодовой книги.'}, 'en': {'title': 'Streamlining 3D Rendering with Minimal Gaussians', 'desc': 'This paper introduces the Optimized Minimal Gaussians (OMG) representation, which aims to enhance 3D Gaussian Splatting (3DGS) by significantly reducing the number of Gaussian primitives needed for high-quality rendering. The authors focus on minimizing redundancy among similar Gaussians while ensuring that the quality of the rendered scenes is preserved. They also present a compact attribute representation that captures both smooth and irregular features of the 3D scene, along with a sub-vector quantization method to improve efficiency. The results show that OMG can cut storage requirements by nearly 50% and achieve over 600 frames per second in rendering without compromising quality.'}, 'zh': {'title': '优化最小高斯表示，提升渲染效率！', 'desc': '3D高斯点云表示（3DGS）是一种用于实时高性能渲染的强大方法，但使用大量显式高斯原语会导致存储和内存开销大。本文提出了一种优化的最小高斯表示（OMG），通过减少高斯数量来显著降低存储需求，同时保持高渲染质量。我们通过识别相似高斯来减少冗余，并提出了一种紧凑的属性表示方法，以有效捕捉原语之间的连续性和不规则性。此外，我们还引入了一种子向量量化技术，以提高不规则性的表示效果。'}}}, {'id': 'https://huggingface.co/papers/2503.18071', 'title': 'Mind with Eyes: from Language Reasoning to Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2503.18071', 'abstract': 'Language models have recently advanced into the realm of reasoning, yet it is through multimodal reasoning that we can fully unlock the potential to achieve more comprehensive, human-like cognitive capabilities. This survey provides a systematic overview of the recent multimodal reasoning approaches, categorizing them into two levels: language-centric multimodal reasoning and collaborative multimodal reasoning. The former encompasses one-pass visual perception and active visual perception, where vision primarily serves a supporting role in language reasoning. The latter involves action generation and state update within reasoning process, enabling a more dynamic interaction between modalities. Furthermore, we analyze the technical evolution of these methods, discuss their inherent challenges, and introduce key benchmark tasks and evaluation metrics for assessing multimodal reasoning performance. Finally, we provide insights into future research directions from the following two perspectives: (i) from visual-language reasoning to omnimodal reasoning and (ii) from multimodal reasoning to multimodal agents. This survey aims to provide a structured overview that will inspire further advancements in multimodal reasoning research.', 'score': 1, 'issue_id': 2888, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': 'bb57f84b4f49656e', 'authors': ['Zhiyu Lin', 'Yifei Gao', 'Xian Zhao', 'Yunfan Yang', 'Jitao Sang'], 'affiliations': ['Beijing Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18071.jpg', 'data': {'categories': ['#agents', '#benchmark', '#reasoning', '#multimodal', '#survey'], 'emoji': '🤖', 'ru': {'title': 'Мультимодальное рассуждение: путь к человекоподобному ИИ', 'desc': 'Эта статья представляет собой обзор современных подходов к мультимодальному рассуждению в области машинного обучения. Авторы классифицируют методы на два уровня: языково-центричное мультимодальное рассуждение и совместное мультимодальное рассуждение. В работе анализируется техническая эволюция этих методов, обсуждаются их проблемы и представляются ключевые задачи и метрики для оценки производительности мультимодального рассуждения. Статья также предлагает перспективы будущих исследований, включая переход от визуально-языкового рассуждения к омнимодальному и от мультимодального рассуждения к мультимодальным агентам.'}, 'en': {'title': 'Unlocking Human-like Reasoning through Multimodal Approaches', 'desc': 'This paper surveys recent advancements in multimodal reasoning, which combines different types of data, like text and images, to enhance cognitive abilities similar to humans. It categorizes multimodal reasoning into two main types: language-centric, where visual data supports language tasks, and collaborative, which involves generating actions and updating states for more interactive reasoning. The authors discuss the evolution of these methods, their challenges, and propose benchmarks for evaluating their performance. Additionally, they suggest future research directions, aiming to transition from visual-language reasoning to more comprehensive omnimodal reasoning and the development of multimodal agents.'}, 'zh': {'title': '解锁多模态推理的潜力', 'desc': '这篇论文探讨了多模态推理在机器学习中的重要性，强调了其在实现更全面的人类认知能力方面的潜力。论文将多模态推理方法分为两类：以语言为中心的多模态推理和协作多模态推理。前者主要关注视觉在语言推理中的辅助作用，而后者则涉及在推理过程中生成动作和更新状态，实现模态之间的动态交互。最后，论文分析了这些方法的技术演变、面临的挑战，并提出了未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2503.14774', 'title': 'Revisiting Image Fusion for Multi-Illuminant White-Balance Correction', 'url': 'https://huggingface.co/papers/2503.14774', 'abstract': 'White balance (WB) correction in scenes with multiple illuminants remains a persistent challenge in computer vision. Recent methods explored fusion-based approaches, where a neural network linearly blends multiple sRGB versions of an input image, each processed with predefined WB presets. However, we demonstrate that these methods are suboptimal for common multi-illuminant scenarios. Additionally, existing fusion-based methods rely on sRGB WB datasets lacking dedicated multi-illuminant images, limiting both training and evaluation. To address these challenges, we introduce two key contributions. First, we propose an efficient transformer-based model that effectively captures spatial dependencies across sRGB WB presets, substantially improving upon linear fusion techniques. Second, we introduce a large-scale multi-illuminant dataset comprising over 16,000 sRGB images rendered with five different WB settings, along with WB-corrected images. Our method achieves up to 100\\% improvement over existing techniques on our new multi-illuminant image fusion dataset.', 'score': 1, 'issue_id': 2887, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '654639b6ee1b8295', 'authors': ['David Serrano-Lozano', 'Aditya Arora', 'Luis Herranz', 'Konstantinos G. Derpanis', 'Michael S. Brown', 'Javier Vazquez-Corral'], 'affiliations': ['Computer Vision Center', 'Universidad Autonoma de Madrid', 'Universitat Aut`onoma de Barcelona', 'Vector Institute', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14774.jpg', 'data': {'categories': ['#cv', '#dataset', '#optimization'], 'emoji': '📸', 'ru': {'title': 'Трансформеры и большие данные для идеального баланса белого', 'desc': 'Статья представляет новый подход к коррекции баланса белого в сценах с несколькими источниками освещения. Авторы предлагают эффективную модель на основе трансформеров, которая улучшает существующие методы линейного слияния. Также они представляют новый крупномасштабный набор данных с более чем 16 000 sRGB изображений с различными настройками баланса белого. Их метод показывает улучшение до 100% по сравнению с существующими техниками на новом наборе данных для слияния изображений с несколькими источниками освещения.'}, 'en': {'title': 'Transforming White Balance Correction with a New Dataset and Model', 'desc': 'This paper addresses the challenge of white balance (WB) correction in images with multiple light sources. It critiques existing fusion-based methods that blend different sRGB images but finds them lacking for complex lighting scenarios. The authors propose a new transformer-based model that better captures the relationships between different WB settings, leading to significant performance improvements. Additionally, they introduce a comprehensive dataset with over 16,000 images to support training and evaluation of WB correction methods in multi-illuminant contexts.'}, 'zh': {'title': '提升多光源场景下的白平衡校正效果', 'desc': '在计算机视觉中，白平衡（WB）校正在多光源场景中仍然是一个持续的挑战。最近的方法探索了基于融合的技术，通过神经网络线性混合多个sRGB版本的输入图像，但这些方法在常见的多光源场景中效果不佳。我们提出了一种高效的基于变换器的模型，能够有效捕捉不同sRGB WB预设之间的空间依赖性，显著改善了线性融合技术。此外，我们还引入了一个大型多光源数据集，包含超过16,000张使用五种不同WB设置渲染的sRGB图像及其WB校正图像。'}}}, {'id': 'https://huggingface.co/papers/2503.13074', 'title': 'Rethinking Image Evaluation in Super-Resolution', 'url': 'https://huggingface.co/papers/2503.13074', 'abstract': "While recent advancing image super-resolution (SR) techniques are continually improving the perceptual quality of their outputs, they can usually fail in quantitative evaluations. This inconsistency leads to a growing distrust in existing image metrics for SR evaluations. Though image evaluation depends on both the metric and the reference ground truth (GT), researchers typically do not inspect the role of GTs, as they are generally accepted as `perfect' references. However, due to the data being collected in the early years and the ignorance of controlling other types of distortions, we point out that GTs in existing SR datasets can exhibit relatively poor quality, which leads to biased evaluations. Following this observation, in this paper, we are interested in the following questions: Are GT images in existing SR datasets 100% trustworthy for model evaluations? How does GT quality affect this evaluation? And how to make fair evaluations if there exist imperfect GTs? To answer these questions, this paper presents two main contributions. First, by systematically analyzing seven state-of-the-art SR models across three real-world SR datasets, we show that SR performances can be consistently affected across models by low-quality GTs, and models can perform quite differently when GT quality is controlled. Second, we propose a novel perceptual quality metric, Relative Quality Index (RQI), that measures the relative quality discrepancy of image pairs, thus issuing the biased evaluations caused by unreliable GTs. Our proposed model achieves significantly better consistency with human opinions. We expect our work to provide insights for the SR community on how future datasets, models, and metrics should be developed.", 'score': 1, 'issue_id': 2887, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '7026b065c090a9c7', 'authors': ['Shaolin Su', 'Josep M. Rocafort', 'Danna Xue', 'David Serrano-Lozano', 'Lei Sun', 'Javier Vazquez-Corral'], 'affiliations': ['Computer Vision Center', 'INSAIT, Sofia University St. Kliment Ohridski', 'Universitat Autonoma de Barcelona'], 'pdf_title_img': 'assets/pdf/title_img/2503.13074.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#cv', '#ethics'], 'emoji': '🔍', 'ru': {'title': 'Переосмысление эталонов в оценке алгоритмов сверхразрешения', 'desc': 'Статья исследует проблему оценки качества алгоритмов сверхразрешения изображений. Авторы обнаружили, что эталонные изображения в существующих наборах данных могут иметь низкое качество, что приводит к искаженным оценкам. Они проанализировали влияние качества эталонов на оценку современных моделей сверхразрешения. В результате был предложен новый метрический показатель Relative Quality Index (RQI), который измеряет относительную разницу качества пар изображений и лучше соответствует человеческому восприятию.'}, 'en': {'title': 'Rethinking Ground Truth: Enhancing Super-Resolution Evaluations', 'desc': 'This paper addresses the issue of trustworthiness in ground truth (GT) images used for evaluating image super-resolution (SR) models. It highlights that many existing GTs may not be perfect, leading to biased performance evaluations of SR techniques. The authors analyze various SR models and demonstrate that low-quality GTs can significantly impact model performance. They also introduce a new metric, the Relative Quality Index (RQI), which aims to provide a more reliable assessment of image quality by comparing pairs of images, aligning better with human judgment.'}, 'zh': {'title': '提升超分辨率评估的信任度', 'desc': '这篇论文探讨了图像超分辨率(SR)技术在定量评估中的不足，指出现有的图像评估指标可能不可靠。研究发现，现有SR数据集中作为参考的真实图像(GT)质量并不总是完美，可能导致评估结果的偏差。通过分析七种最先进的SR模型，论文表明低质量的GT会影响模型的表现，并提出了一种新的感知质量指标——相对质量指数(RQI)，用于衡量图像对之间的相对质量差异。该研究旨在为SR领域提供关于未来数据集、模型和评估指标开发的见解。'}}}, {'id': 'https://huggingface.co/papers/2503.18674', 'title': 'Human Motion Unlearning', 'url': 'https://huggingface.co/papers/2503.18674', 'abstract': 'We introduce the task of human motion unlearning to prevent the synthesis of toxic animations while preserving the general text-to-motion generative performance. Unlearning toxic motions is challenging as those can be generated from explicit text prompts and from implicit toxic combinations of safe motions (e.g., ``kicking" is ``loading and swinging a leg"). We propose the first motion unlearning benchmark by filtering toxic motions from the large and recent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines, by adapting state-of-the-art image unlearning techniques to process spatio-temporal signals. Finally, we propose a novel motion unlearning model based on Latent Code Replacement, which we dub LCR. LCR is training-free and suitable to the discrete latent spaces of state-of-the-art text-to-motion diffusion models. LCR is simple and consistently outperforms baselines qualitatively and quantitatively. Project page: https://www.pinlab.org/hmu{https://www.pinlab.org/hmu}.', 'score': 0, 'issue_id': 2888, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'c8c54c3b8e02cf7b', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#dataset', '#benchmark', '#ethics', '#diffusion', '#training'], 'emoji': '🕺', 'ru': {'title': 'Безопасная генерация движений: разобучение токсичной анимации', 'desc': 'Статья представляет задачу разобучения генерации токсичных движений человека в системах преобразования текста в анимацию. Авторы создают первый бенчмарк для разобучения движений, фильтруя токсичный контент из крупных датасетов. Они адаптируют современные методы разобучения изображений для работы с пространственно-временными сигналами. Предлагается новый метод LCR на основе замены латентных кодов, который превосходит базовые подходы.'}, 'en': {'title': 'Unlearning Toxic Motions for Safer Animation Generation', 'desc': 'This paper addresses the challenge of human motion unlearning, which aims to eliminate the generation of harmful animations while maintaining effective text-to-motion performance. The authors introduce a benchmark for motion unlearning by filtering out toxic motions from large datasets like HumanML3D and Motion-X. They adapt advanced image unlearning techniques to work with spatio-temporal data and propose a new model called Latent Code Replacement (LCR), which does not require training and is compatible with existing text-to-motion diffusion models. LCR demonstrates superior performance compared to existing methods in both qualitative and quantitative evaluations.'}, 'zh': {'title': '人类动作去学习：消除有害动画的创新方法', 'desc': '本文介绍了人类动作去学习的任务，旨在防止合成有害动画，同时保持文本到动作生成的整体性能。去学习有害动作的挑战在于，这些动作可以通过明确的文本提示生成，也可以通过安全动作的隐含有害组合生成。我们提出了第一个动作去学习基准，通过过滤HumanML3D和Motion-X这两个大型文本到动作数据集中的有害动作。我们还提出了一种基于潜在代码替换的新型动作去学习模型LCR，该模型无需训练，适用于最新文本到动作扩散模型的离散潜在空间。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.18494', 'title': 'Verbal Process Supervision Elicits Better Coding Agents', 'url': 'https://huggingface.co/papers/2503.18494', 'abstract': 'The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, a code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving a 3.65\\% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents a step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks.', 'score': 0, 'issue_id': 2881, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '97716e960b1a782d', 'authors': ['Hao-Yuan Chen', 'Cheng-Pong Huang', 'Jui-Ming Yao'], 'affiliations': ['Mindify AI, United States', 'National Taiwan University of Science and Technology, Taiwan', 'University of London, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2503.18494.jpg', 'data': {'categories': ['#training', '#benchmark', '#agents', '#agi', '#architecture', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'CURA: Агент с вербальным контролем для решения сложных задач программирования', 'desc': 'Статья представляет CURA - систему агента для понимания и рассуждения о коде, улучшенную с помощью вербального контроля процесса (VPS). CURA достигает улучшения на 3.65% по сравнению с базовыми моделями на сложных бенчмарках, таких как BigCodeBench. В сочетании с моделью o3-mini и техниками VPS, CURA достигает наилучших результатов в своей области. Это исследование представляет шаг вперед в интеграции архитектур, основанных на рассуждениях, с генерацией кода на основе больших языковых моделей.'}, 'en': {'title': 'CURA: Enhancing Code Generation with Reasoning and Supervision', 'desc': 'This paper presents CURA, a code understanding and reasoning agent that enhances large language models (LLMs) with verbal process supervision (VPS). CURA addresses the limitations of existing AI agents in tackling complex software engineering challenges by improving their reasoning capabilities. The system demonstrates a 3.65% performance boost on benchmarks like BigCodeBench compared to baseline models. By integrating reasoning-driven architectures with LLMs, CURA enables more effective code generation and problem-solving in software engineering tasks.'}, 'zh': {'title': 'CURA：提升代码理解与推理的智能代理系统', 'desc': '本论文介绍了一种名为CURA的代码理解与推理代理系统，旨在解决复杂的软件工程问题。CURA通过引入语言过程监督（VPS）技术，显著提高了在BigCodeBench等基准测试中的表现，提升幅度达到3.65%。此外，当CURA与o3-mini模型结合使用时，能够达到最先进的性能。该研究展示了将推理驱动架构与大型语言模型结合的潜力，为语言模型在复杂软件工程任务中的推理能力提供了新的方向。'}}}, {'id': 'https://huggingface.co/papers/2503.18406', 'title': 'Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated\n  Data Refinement Using Contrastive Learning', 'url': 'https://huggingface.co/papers/2503.18406', 'abstract': 'Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to challenges in creating large, high-quality training datasets. Previous work has typically relied on text-toimage (T2I) generative models to produce pairs of original and edited images that simulate the input/output of an instruction-guided image-editing model. However, these image pairs often fail to align with the specified edit instructions due to the limitations of T2I models, which negatively impacts models trained on such datasets. To address this, we present Instruct-CLIP, a self-supervised method that learns the semantic changes between original and edited images to refine and better align the instructions in existing datasets. Furthermore, we adapt Instruct-CLIP to handle noisy latent images and diffusion timesteps so that it can be used to train latent diffusion models (LDMs) [19] and efficiently enforce alignment between the edit instruction and the image changes in latent space at any step of the diffusion pipeline. We use Instruct-CLIP to correct the InstructPix2Pix dataset and get over 120K refined samples we then use to fine-tune their model, guided by our novel Instruct-CLIP-based loss function. The resulting model can produce edits that are more aligned with the given instructions. Our code and dataset are available at https://github.com/SherryXTChen/Instruct-CLIP.git.', 'score': 0, 'issue_id': 2893, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '1683f929d40d4b06', 'authors': ['Sherry X. Chen', 'Misha Sra', 'Pradeep Sen'], 'affiliations': ['University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2503.18406.jpg', 'data': {'categories': ['#diffusion', '#data', '#cv', '#optimization', '#dataset', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение редактирования изображений по инструкциям с помощью самообучения', 'desc': 'Статья представляет Instruct-CLIP - самообучаемый метод для улучшения наборов данных для обучения моделей редактирования изображений по текстовым инструкциям. Метод анализирует семантические изменения между оригинальными и отредактированными изображениями, чтобы уточнить инструкции и лучше согласовать их с фактическими изменениями. Instruct-CLIP адаптирован для работы с латентными изображениями и временными шагами диффузии, что позволяет использовать его для обучения латентных диффузионных моделей. С помощью Instruct-CLIP авторы улучшили набор данных InstructPix2Pix и обучили модель, способную создавать редактирования, более точно соответствующие заданным инструкциям.'}, 'en': {'title': 'Aligning Image Edits with Natural Language Instructions', 'desc': 'This paper introduces Instruct-CLIP, a self-supervised method designed to improve the alignment between image edits and natural language instructions. It addresses the limitations of traditional text-to-image models that often produce poorly aligned image pairs for training. By learning the semantic changes between original and edited images, Instruct-CLIP refines existing datasets and enhances the training of latent diffusion models. The method results in a more accurate model that generates image edits that closely match the specified instructions, significantly improving the quality of automated image editing.'}, 'zh': {'title': '提升图像编辑指令对齐的自监督方法', 'desc': '本文提出了一种名为Instruct-CLIP的自监督方法，旨在改善基于自然语言指令的图像编辑模型的性能。通过学习原始图像与编辑图像之间的语义变化，Instruct-CLIP能够更好地对齐现有数据集中的指令。该方法还适应了处理噪声潜在图像和扩散时间步，以便在潜在扩散模型中有效地执行指令与图像变化之间的对齐。最终，使用Instruct-CLIP修正后的数据集生成了超过12万条样本，从而提高了模型对指令的响应能力。'}}}, {'id': 'https://huggingface.co/papers/2503.16709', 'title': 'QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on\n  the Edge', 'url': 'https://huggingface.co/papers/2503.16709', 'abstract': 'Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer vision, supporting numerous real-world applications. However, deploying accurate depth estimation models on resource-limited edge devices, especially Application-Specific Integrated Circuits (ASICs), is challenging due to the high computational and memory demands. Recent advancements in foundational depth estimation deliver impressive results but further amplify the difficulty of deployment on ASICs. To address this, we propose QuartDepth which adopts post-training quantization to quantize MDE models with hardware accelerations for ASICs. Our approach involves quantizing both weights and activations to 4-bit precision, reducing the model size and computation cost. To mitigate the performance degradation, we introduce activation polishing and compensation algorithm applied before and after activation quantization, as well as a weight reconstruction method for minimizing errors in weight quantization. Furthermore, we design a flexible and programmable hardware accelerator by supporting kernel fusion and customized instruction programmability, enhancing throughput and efficiency. Experimental results demonstrate that our framework achieves competitive accuracy while enabling fast inference and higher energy efficiency on ASICs, bridging the gap between high-performance depth estimation and practical edge-device applicability. Code: https://github.com/shawnricecake/quart-depth', 'score': 0, 'issue_id': 2893, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '4d36736f3565bd06', 'authors': ['Xuan Shen', 'Weize Ma', 'Jing Liu', 'Changdi Yang', 'Rui Ding', 'Quanyi Wang', 'Henghui Ding', 'Wei Niu', 'Yanzhi Wang', 'Pu Zhao', 'Jun Lin', 'Jiuxiang Gu'], 'affiliations': ['Adobe Research', 'Fudan University', 'Monash University', 'Nanjing University', 'Nanjing University of Information Science and Technology', 'Northeastern University', 'University of Georgia'], 'pdf_title_img': 'assets/pdf/title_img/2503.16709.jpg', 'data': {'categories': ['#optimization', '#cv', '#inference'], 'emoji': '🔬', 'ru': {'title': 'Эффективная оценка глубины на краевых устройствах', 'desc': 'Статья представляет QuartDepth - подход к квантизации моделей монокулярной оценки глубины для применения на ASIC. Авторы предлагают квантизацию весов и активаций до 4 бит, а также методы полировки активаций и реконструкции весов для минимизации потери точности. Разработан гибкий аппаратный ускоритель с поддержкой слияния ядер и программируемости инструкций. Эксперименты показывают, что подход обеспечивает конкурентную точность при быстром выводе и высокой энергоэффективности на ASIC.'}, 'en': {'title': 'Optimizing Depth Estimation for Edge Devices with QuartDepth', 'desc': 'This paper presents QuartDepth, a novel approach for Monocular Depth Estimation (MDE) that focuses on optimizing models for resource-limited edge devices like ASICs. It utilizes post-training quantization to reduce the model size and computational requirements by quantizing weights and activations to 4-bit precision. To counteract potential performance loss from quantization, the authors introduce techniques such as activation polishing and a weight reconstruction method. The proposed framework not only maintains competitive accuracy but also enhances inference speed and energy efficiency, making high-performance depth estimation feasible on edge devices.'}, 'zh': {'title': '高效的单目深度估计解决方案', 'desc': '单目深度估计（MDE）在计算机视觉中扮演着重要角色，广泛应用于现实世界中。然而，在资源有限的边缘设备上，尤其是应用特定集成电路（ASICs）上，准确的深度估计模型的部署面临挑战，因为其计算和内存需求较高。为了解决这个问题，我们提出了QuartDepth，通过后训练量化技术对MDE模型进行量化，以适应ASICs的硬件加速。我们的方案通过将权重和激活量化到4位精度，减少模型大小和计算成本，同时引入激活抛光和补偿算法，以减轻性能下降。'}}}, {'id': 'https://huggingface.co/papers/2503.16426', 'title': 'DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding', 'url': 'https://huggingface.co/papers/2503.16426', 'abstract': "The advancement of remote sensing technology has improved the spatial resolution of satellite imagery, facilitating more detailed visual representations for diverse interpretations. However, existing methods exhibit limited generalization capabilities across varied applications. While some contemporary foundation models demonstrate potential, they are hindered by insufficient cross-task adaptability and primarily process low-resolution imagery of restricted sizes, thus failing to fully exploit high-resolution data or leverage comprehensive large-scene semantics. Crucially, remote sensing imagery differs fundamentally from natural images, as key foreground targets (eg., maritime objects, artificial structures) often occupy minimal spatial proportions (~1%) and exhibit sparse distributions. Efficiently modeling cross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a significant challenge yet remains critical for remote sensing image understanding. Motivated by the selective attention mechanisms inherent to the human visual system, we propose DynamicVis, a dynamic visual perception foundation model for remote sensing imagery. The framework integrates a novel dynamic region perception backbone based on the selective state space model, which strategically balances localized detail extraction with global contextual integration, enabling computationally efficient encoding of large-scale data while maintaining architectural scalability. To enhance cross-task knowledge transferring, we introduce a multi-instance learning paradigm utilizing meta-embedding representations, trained on million-scale region-level annotations. Evaluations across nine downstream tasks demonstrate the model's versatility. DynamicVis achieves multi-level feature modeling with exceptional efficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and 833 MB GPU memory (3% of ViT's).", 'score': 0, 'issue_id': 2885, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '973f386b0cfb485a', 'authors': ['Keyan Chen', 'Chenyang Liu', 'Bowen Chen', 'Wenyuan Li', 'Zhengxia Zou', 'Zhenwei Shi'], 'affiliations': ['Beihang University', 'the University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.16426.jpg', 'data': {'categories': ['#cv', '#optimization', '#transfer_learning', '#training', '#architecture'], 'emoji': '🛰️', 'ru': {'title': 'DynamicVis: эффективная модель машинного зрения для анализа спутниковых снимков', 'desc': 'Статья представляет DynamicVis - модель машинного зрения для анализа спутниковых снимков высокого разрешения. Модель использует селективный механизм внимания и пространственно-временную модель для эффективной обработки больших изображений. DynamicVis обучается на миллионах аннотаций на уровне регионов, что позволяет ей успешно решать различные задачи. Модель демонстрирует высокую эффективность, обрабатывая изображения 2048x2048 пикселей за 97 мс при использовании всего 833 МБ видеопамяти.'}, 'en': {'title': 'DynamicVis: Revolutionizing Remote Sensing with Efficient Cross-Task Learning', 'desc': 'This paper presents DynamicVis, a foundation model designed specifically for remote sensing imagery, which often contains sparse and small foreground targets. The model addresses the limitations of existing methods by enhancing cross-task adaptability and efficiently processing high-resolution satellite images. By employing a dynamic region perception backbone and a multi-instance learning paradigm, DynamicVis achieves effective feature modeling while maintaining low latency and memory usage. Evaluations show that it excels across various tasks, demonstrating its potential for improved understanding of remote sensing data.'}, 'zh': {'title': 'DynamicVis：遥感图像处理的新突破', 'desc': '本论文介绍了一种名为DynamicVis的动态视觉感知基础模型，专门用于遥感图像处理。该模型通过选择性状态空间模型，平衡局部细节提取与全局上下文整合，从而高效编码大规模数据。DynamicVis采用多实例学习范式，利用元嵌入表示，提升跨任务知识转移能力。实验结果表明，该模型在处理高分辨率图像时表现出色，具有极高的效率和灵活性。'}}}, {'id': 'https://huggingface.co/papers/2503.13358', 'title': 'One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation', 'url': 'https://huggingface.co/papers/2503.13358', 'abstract': 'Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present RSD, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce such images that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K.', 'score': 82, 'issue_id': 2830, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '7c30d8250da0c284', 'authors': ['Daniil Selikhanovych', 'David Li', 'Aleksei Leonov', 'Nikita Gushchin', 'Sergei Kushneriuk', 'Alexander Filippov', 'Evgeny Burnaev', 'Iaroslav Koshelev', 'Alexander Korotin'], 'affiliations': ['AI Foundation and Algorithm Lab', 'AIRI', 'HSE University', 'MIPT', 'Skoltech', 'Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.13358.jpg', 'data': {'categories': ['#synthetic', '#hallucinations', '#training', '#diffusion', '#cv', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'RSD: Эффективная дистилляция диффузионных моделей для сверхразрешения изображений', 'desc': 'Статья представляет новый метод дистилляции для диффузионных моделей сверхразрешения изображений под названием RSD. Этот подход обучает студенческую сеть генерировать такие изображения, чтобы новая фейковая модель ResShift, обученная на них, совпадала с учительской моделью. RSD достигает восстановления изображения за один шаг и значительно превосходит учительскую модель. По сравнению с другими методами, RSD обеспечивает лучшее согласование с исходными низкокачественными изображениями, требует меньше параметров и памяти GPU.'}, 'en': {'title': 'RSD: Revolutionizing Super-Resolution with Efficient Distillation', 'desc': 'This paper introduces RSD, a novel distillation method designed to enhance the performance of ResShift, a leading diffusion model for super-resolution (SR). RSD trains a student network to generate images that align closely with those produced by a teacher model, ensuring high-quality visual results while reducing computational costs. The method demonstrates significant improvements over existing techniques, such as SinSR, by maintaining realistic perceptual details and avoiding the hallucination of non-existent structures. Experimental results show that RSD not only matches but often exceeds the performance of state-of-the-art diffusion-based SR methods, while also being more efficient in terms of parameters and GPU memory usage.'}, 'zh': {'title': 'RSD：超分辨率的新突破', 'desc': '本文提出了一种新的蒸馏方法RSD，用于改进基于扩散模型的超分辨率(SR)技术。RSD通过训练学生网络生成图像，使得一个新的假ResShift模型在这些图像上与教师模型一致，从而实现单步恢复。与其他蒸馏方法相比，RSD在感知质量上表现出色，并且在参数和GPU内存需求上更具优势。实验结果表明，RSD在多个真实和合成数据集上均优于现有的超分辨率方法。'}}}, {'id': 'https://huggingface.co/papers/2503.16416', 'title': 'Survey on Evaluation of LLM-based Agents', 'url': 'https://huggingface.co/papers/2503.16416', 'abstract': 'The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research.', 'score': 58, 'issue_id': 2828, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '8d23d348a6ecdbce', 'authors': ['Asaf Yehudai', 'Lilach Eden', 'Alan Li', 'Guy Uziel', 'Yilun Zhao', 'Roy Bar-Haim', 'Arman Cohan', 'Michal Shmueli-Scheuer'], 'affiliations': ['IBM Research', 'The Hebrew University of Jerusalem', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16416.jpg', 'data': {'categories': ['#benchmark', '#survey', '#reasoning', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Комплексный анализ методов оценки AI-агентов нового поколения', 'desc': 'Статья представляет собой первый комплексный обзор методологий оценки агентов на основе больших языковых моделей (LLM). Авторы анализируют бенчмарки и фреймворки для оценки по четырем ключевым аспектам: фундаментальные способности агентов, специфические для приложений бенчмарки, бенчмарки для универсальных агентов и фреймворки оценки. Исследование выявляет тенденцию к более реалистичным и сложным методам оценки, а также определяет пробелы в оценке экономической эффективности, безопасности и надежности агентов. Статья предлагает направления для будущих исследований в области оценки агентов на основе LLM.'}, 'en': {'title': 'Evaluating the Future of LLM Agents: Trends and Gaps', 'desc': 'This paper surveys the evaluation methods for large language model (LLM)-based agents, which are AI systems capable of planning, reasoning, and interacting with their environments. It categorizes evaluation benchmarks into four key areas: fundamental capabilities, application-specific benchmarks, generalist agent benchmarks, and evaluation frameworks. The authors highlight a trend towards more realistic evaluations that adapt over time, while also pointing out significant gaps in current research, such as the need for better assessments of cost-efficiency and safety. The paper aims to provide a comprehensive overview of the evaluation landscape and suggest future research directions to enhance agent assessment.'}, 'zh': {'title': '智能体评估的新视野', 'desc': '本论文对基于大型语言模型（LLM）的智能体评估方法进行了全面的调查。我们分析了智能体能力、应用特定基准、通用智能体基准和评估框架等四个关键维度。研究发现，评估方法正朝着更真实和具有挑战性的方向发展，同时也指出了在成本效率、安全性和鲁棒性评估方面的研究空白。该调查为智能体评估的快速发展提供了地图，揭示了领域中的新兴趋势和未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2503.16419', 'title': 'Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models', 'url': 'https://huggingface.co/papers/2503.16419', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the "overthinking phenomenon". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.', 'score': 52, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'dc9b04a227f74af7', 'authors': ['Yang Sui', 'Yu-Neng Chuang', 'Guanchu Wang', 'Jiamu Zhang', 'Tianyi Zhang', 'Jiayi Yuan', 'Hongyi Liu', 'Andrew Wen', 'Shaochen', 'Zhong', 'Hanjie Chen', 'Xia Hu'], 'affiliations': ['Department of Computer Science, Rice University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16419.jpg', 'data': {'categories': ['#small_models', '#optimization', '#reasoning', '#rl', '#dataset', '#survey', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение в LLM: преодоление избыточности для оптимальной производительности', 'desc': 'Эта статья представляет собой обзор методов повышения эффективности рассуждений в больших языковых моделях (LLM). Авторы рассматривают три основных направления: оптимизация самих моделей, динамическое сокращение шагов рассуждения при выводе и улучшение входных промптов. Также обсуждается использование эффективных данных для обучения моделей рассуждения и возможности небольших языковых моделей. В работе отмечается, что хотя более длинные цепочки рассуждений улучшают производительность, они также увеличивают вычислительные затраты из-за многословности и избыточности.'}, 'en': {'title': 'Optimizing Reasoning Efficiency in Large Language Models', 'desc': 'This paper surveys the advancements in Large Reasoning Models (LRMs) that enhance reasoning capabilities in complex tasks like mathematics and programming. It highlights the use of supervised fine-tuning and reinforcement learning to improve Chain-of-Thought (CoT) reasoning, while addressing the computational challenges posed by longer reasoning sequences. The authors categorize existing approaches into three main strategies: optimizing reasoning models, reducing reasoning output length, and improving input prompts for efficiency. Additionally, the paper discusses the potential of small language models and introduces methods for evaluating reasoning performance.'}, 'zh': {'title': '高效推理：大型语言模型的新探索', 'desc': '大型语言模型（LLMs）在复杂任务中展现了卓越的能力。最近，大型推理模型（LRMs）的进展进一步提升了在数学和编程等系统-2推理领域的表现，利用监督微调（SFT）和强化学习（RL）技术来增强思维链（CoT）推理。然而，较长的CoT推理序列虽然提高了性能，却也带来了显著的计算开销，这种现象被称为“过度思考现象”。本文首次系统性地调查和探索了实现LLMs高效推理的当前进展，并将现有工作分类为多个关键方向。'}}}, {'id': 'https://huggingface.co/papers/2503.16302', 'title': 'Unleashing Vecset Diffusion Model for Fast Shape Generation', 'url': 'https://huggingface.co/papers/2503.16302', 'abstract': '3D shape generation has greatly flourished through the development of so-called "native" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation. Challenges exist because of difficulties not only in accelerating diffusion sampling but also VAE decoding in VDM, areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps and comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation. For VAE, we introduce a lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design. By exploiting the locality of the vecset and the sparsity of shape surface in the volume, our decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic evaluation, we show that our model significantly outperforms existing fast 3D generation methods, achieving comparable performance to the state-of-the-art while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are available at https://github.com/Tencent/FlashVDM.', 'score': 35, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'd91e862542d76892', 'authors': ['Zeqiang Lai', 'Yunfei Zhao', 'Zibo Zhao', 'Haolin Liu', 'Fuyun Wang', 'Huiwen Shi', 'Xianghui Yang', 'Qinxiang Lin', 'Jinwei Huang', 'Yuhong Liu', 'Jie Jiang', 'Chunchao Guo', 'Xiangyu Yue'], 'affiliations': ['MMLab, CUHK', 'ShanghaiTech', 'Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2503.16302.jpg', 'data': {'categories': ['#open_source', '#3d', '#diffusion', '#optimization', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Молниеносная генерация 3D-форм с FlashVDM', 'desc': 'Статья представляет FlashVDM - систему для ускорения генерации 3D-форм с использованием векторных диффузионных моделей (VDM). Авторы предлагают улучшения как для процесса диффузии, так и для декодирования VAE, включая метод Progressive Flow Distillation и оптимизированный декодер векторных множеств. FlashVDM позволяет генерировать 3D-формы высокого качества всего за 5 шагов вывода, значительно сокращая время inference. Результаты показывают превосходство FlashVDM над существующими методами быстрой 3D-генерации, уменьшая время вывода в 32-45 раз при сохранении качества на уровне state-of-the-art.'}, 'en': {'title': 'Accelerating 3D Shape Generation with FlashVDM', 'desc': 'This paper introduces FlashVDM, a new framework designed to enhance the speed of 3D shape generation using the Vecset Diffusion Model (VDM). It addresses the slow generation times associated with VDM by improving both the Variational Autoencoder (VAE) and the Diffusion Transformer (DiT) components. FlashVDM achieves faster diffusion sampling with fewer inference steps while maintaining high-quality outputs through a technique called Progressive Flow Distillation. Additionally, it optimizes the VAE with a new decoder that reduces computational load, resulting in significant improvements in inference speed without sacrificing performance.'}, 'zh': {'title': '加速3D形状生成的FlashVDM框架', 'desc': '本论文介绍了一种名为FlashVDM的框架，旨在加速3D形状生成中的VAE和DiT过程。通过引入渐进流蒸馏，FlashVDM实现了灵活的扩散采样，仅需5个推理步骤即可获得与现有方法相当的质量。我们还设计了一种闪电vecset解码器，利用自适应KV选择和分层体积解码，显著降低了计算复杂度。实验结果表明，FlashVDM在重建和生成方面的推理时间分别减少了超过45倍和32倍，显著优于现有的快速3D生成方法。'}}}, {'id': 'https://huggingface.co/papers/2503.15558', 'title': 'Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning', 'url': 'https://huggingface.co/papers/2503.15558', 'abstract': 'Physical AI systems need to perceive, understand, and perform complex actions in the physical world. In this paper, we present the Cosmos-Reason1 models that can understand the physical world and generate appropriate embodied decisions (e.g., next step action) in natural language through long chain-of-thought reasoning processes. We begin by defining key capabilities for Physical AI reasoning, with a focus on physical common sense and embodied reasoning. To represent physical common sense, we use a hierarchical ontology that captures fundamental knowledge about space, time, and physics. For embodied reasoning, we rely on a two-dimensional ontology that generalizes across different physical embodiments. Building on these capabilities, we develop two multimodal large language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data and train our models in four stages: vision pre-training, general supervised fine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL) as the post-training. To evaluate our models, we build comprehensive benchmarks for physical common sense and embodied reasoning according to our ontologies. Evaluation results show that Physical AI SFT and reinforcement learning bring significant improvements. To facilitate the development of Physical AI, we will make our code and pre-trained models available under the NVIDIA Open Model License at https://github.com/nvidia-cosmos/cosmos-reason1.', 'score': 29, 'issue_id': 2823, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '882bcaa2257d8251', 'authors': ['NVIDIA', ':', 'Alisson Azzolini', 'Hannah Brandon', 'Prithvijit Chattopadhyay', 'Huayu Chen', 'Jinju Chu', 'Yin Cui', 'Jenna Diamond', 'Yifan Ding', 'Francesco Ferroni', 'Rama Govindaraju', 'Jinwei Gu', 'Siddharth Gururani', 'Imad El Hanafi', 'Zekun Hao', 'Jacob Huffman', 'Jingyi Jin', 'Brendan Johnson', 'Rizwan Khan', 'George Kurian', 'Elena Lantz', 'Nayeon Lee', 'Zhaoshuo Li', 'Xuan Li', 'Tsung-Yi Lin', 'Yen-Chen Lin', 'Ming-Yu Liu', 'Andrew Mathau', 'Yun Ni', 'Lindsey Pavao', 'Wei Ping', 'David W. Romero', 'Misha Smelyanskiy', 'Shuran Song', 'Lyne Tchapmi', 'Andrew Z. Wang', 'Boxin Wang', 'Haoxiang Wang', 'Fangyin Wei', 'Jiashu Xu', 'Yao Xu', 'Xiaodong Yang', 'Zhuolin Yang', 'Xiaohui Zeng', 'Zhe Zhang'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.15558.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#open_source', '#agents', '#reasoning', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Физический ИИ: от понимания мира к воплощенным действиям', 'desc': 'В статье представлены модели Cosmos-Reason1, способные понимать физический мир и генерировать решения для воплощенных агентов с помощью цепочек рассуждений на естественном языке. Модели основаны на иерархической онтологии физического здравого смысла и двумерной онтологии для обобщения различных физических воплощений. Обучение проводилось в четыре этапа: предобучение зрения, общая контролируемая доводка, контролируемая доводка для физического ИИ и обучение с подкреплением. Результаты оценки показывают значительные улучшения после этапов доводки и обучения с подкреплением для физического ИИ.'}, 'en': {'title': 'Empowering Physical AI with Cosmos-Reason1 Models', 'desc': 'This paper introduces the Cosmos-Reason1 models designed for Physical AI systems, which need to understand and act in the physical world. The models utilize long chain-of-thought reasoning to generate actions in natural language, focusing on physical common sense and embodied reasoning. A hierarchical ontology is employed to represent physical common sense, while a two-dimensional ontology supports various physical embodiments. The authors train their models through a structured process, including vision pre-training and reinforcement learning, and demonstrate significant performance improvements through comprehensive evaluations.'}, 'zh': {'title': '物理人工智能的推理与决策新突破', 'desc': '本文介绍了Cosmos-Reason1模型，这是一种能够理解物理世界并通过长链推理过程生成适当决策的物理人工智能系统。我们定义了物理人工智能推理的关键能力，重点关注物理常识和具身推理。为了表示物理常识，我们使用了一个层次本体，捕捉空间、时间和物理学的基本知识。通过多模态大语言模型的开发，我们的研究展示了物理人工智能在推理和决策中的显著进步。'}}}, {'id': 'https://huggingface.co/papers/2503.16397', 'title': 'Scale-wise Distillation of Diffusion Models', 'url': 'https://huggingface.co/papers/2503.16397', 'abstract': 'We present SwD, a scale-wise distillation framework for diffusion models (DMs), which effectively employs next-scale prediction ideas for diffusion-based few-step generators. In more detail, SwD is inspired by the recent insights relating diffusion processes to the implicit spectral autoregression. We suppose that DMs can initiate generation at lower data resolutions and gradually upscale the samples at each denoising step without loss in performance while significantly reducing computational costs. SwD naturally integrates this idea into existing diffusion distillation methods based on distribution matching. Also, we enrich the family of distribution matching approaches by introducing a novel patch loss enforcing finer-grained similarity to the target distribution. When applied to state-of-the-art text-to-image diffusion models, SwD approaches the inference times of two full resolution steps and significantly outperforms the counterparts under the same computation budget, as evidenced by automated metrics and human preference studies.', 'score': 28, 'issue_id': 2830, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'b981ae73ba6549a8', 'authors': ['Nikita Starodubcev', 'Denis Kuznedelev', 'Artem Babenko', 'Dmitry Baranchuk'], 'affiliations': ['Yandex Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.16397.jpg', 'data': {'categories': ['#inference', '#training', '#diffusion', '#cv', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Эффективная дистилляция диффузионных моделей с масштабированием', 'desc': 'SwD - это новая структура для дистилляции диффузионных моделей, использующая идею предсказания следующего масштаба для генераторов с малым числом шагов. Метод основан на связи между диффузионными процессами и неявной спектральной авторегрессией, позволяя начинать генерацию с низкого разрешения и постепенно увеличивать его на каждом шаге. SwD интегрируется в существующие методы дистилляции диффузионных моделей и вводит новую функцию потерь на основе патчей для более точного соответствия целевому распределению. При применении к современным моделям генерации изображений по тексту, SwD значительно превосходит аналоги при том же вычислительном бюджете.'}, 'en': {'title': 'Efficient Image Generation with Scale-Wise Distillation', 'desc': 'The paper introduces SwD, a new framework for improving diffusion models (DMs) by using a scale-wise distillation approach. This method allows DMs to start generating images at lower resolutions and progressively enhance them at each denoising step, which reduces computational costs without sacrificing quality. SwD builds on existing diffusion distillation techniques by incorporating a novel patch loss that ensures closer alignment with the target distribution. When tested on advanced text-to-image models, SwD shows faster inference times and better performance compared to traditional methods within the same computational limits.'}, 'zh': {'title': 'SwD：高效的扩散模型蒸馏框架', 'desc': '我们提出了SwD，一个针对扩散模型的尺度级蒸馏框架，能够有效利用下一尺度预测的思想来生成少步扩散模型。SwD的灵感来源于将扩散过程与隐式谱自回归相关联的最新研究。我们假设扩散模型可以在较低的数据分辨率下开始生成，并在每个去噪步骤中逐步提升样本的分辨率，同时不损失性能并显著降低计算成本。通过引入一种新颖的补丁损失，SwD增强了分布匹配方法的细粒度相似性，应用于最先进的文本到图像扩散模型时，SwD在相同计算预算下显著提高了推理速度和性能。'}}}, {'id': 'https://huggingface.co/papers/2503.15299', 'title': 'Inside-Out: Hidden Factual Knowledge in LLMs', 'url': 'https://huggingface.co/papers/2503.15299', 'abstract': "This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model's observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) puts a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.", 'score': 27, 'issue_id': 2833, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '2ca2246fe2ad8d73', 'authors': ['Zorik Gekhman', 'Eyal Ben David', 'Hadas Orgad', 'Eran Ofek', 'Yonatan Belinkov', 'Idan Szpector', 'Jonathan Herzig', 'Roi Reichart'], 'affiliations': ['Google Research', 'Technion - Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.15299.jpg', 'data': {'categories': ['#hallucinations', '#multimodal', '#data', '#benchmark', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Скрытые знания в языковых моделях: больше, чем кажется на первый взгляд', 'desc': 'Статья представляет методологию оценки скрытых знаний в больших языковых моделях (LLM). Авторы вводят формальное определение знания и различают внешнее и внутреннее знание модели. Исследование показывает, что LLM содержат в среднем на 40% больше фактических знаний внутренне, чем выражают внешне. Обнаружено, что некоторые знания настолько скрыты, что модель может внутренне знать ответ идеально, но не генерировать его даже при многократном семплировании.'}, 'en': {'title': 'Unveiling Hidden Knowledge in Language Models', 'desc': 'This paper introduces a framework to evaluate the factual knowledge embedded in large language models (LLMs) compared to what they actually produce in their outputs. It defines knowledge in a quantifiable way, distinguishing between external knowledge (observable outputs) and internal knowledge (hidden computations). The study reveals that LLMs often possess more internal knowledge than they express, with a notable gap of 40% on average. Additionally, it highlights that some knowledge is so well-hidden that models can know the correct answer internally but fail to generate it, indicating limitations in their generation capabilities and affecting performance in closed-book question answering.'}, 'zh': {'title': '揭示大型语言模型的隐藏知识', 'desc': '本文提出了一个框架，用于评估大型语言模型（LLMs）在其参数中编码的事实知识是否超过其输出中表达的知识。我们首先提出了知识的正式定义，通过正确-错误答案对的比例来量化知识。研究表明，LLMs内部编码的事实知识通常比外部表达的多，平均差距达到40%。此外，某些知识深藏不露，模型可能内部完全知道答案，但在多次采样中却从未生成，这揭示了LLMs生成能力的基本局限性。'}}}, {'id': 'https://huggingface.co/papers/2503.16365', 'title': 'JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse', 'url': 'https://huggingface.co/papers/2503.16365', 'abstract': "Recently, action-based decision-making in open-world environments has gained significant attention. Visual Language Action (VLA) models, pretrained on large-scale web datasets, have shown promise in decision-making tasks. However, previous work has primarily focused on action post-training, often neglecting enhancements to the foundational model itself. In response, we introduce a novel approach, Act from Visual Language Post-Training, which refines Visual Language Models (VLMs) through visual and linguistic guidance in a self-supervised manner. This enhancement improves the models' capabilities in world knowledge, visual recognition, and spatial grounding in open-world environments. Following the above post-training paradigms, we obtain the first VLA models in Minecraft that can follow human instructions on over 1k different atomic tasks, including crafting, smelting, cooking, mining, and killing. Our experiments demonstrate that post-training on non-trajectory tasks leads to a significant 40% improvement over the best agent baseline on a diverse set of atomic tasks. Furthermore, we demonstrate that our approach surpasses traditional imitation learning-based policies in Minecraft, achieving state-of-the-art performance. We have open-sourced the code, models, and datasets to foster further research. The project page can be found in https://craftjarvis.github.io/JarvisVLA.", 'score': 25, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '764fc9201b406ec4', 'authors': ['Muyao Li', 'Zihao Wang', 'Kaichen He', 'Xiaojian Ma', 'Yitao Liang'], 'affiliations': ['BIGAI', 'Peking University', 'Team CraftJarvis'], 'pdf_title_img': 'assets/pdf/title_img/2503.16365.jpg', 'data': {'categories': ['#open_source', '#games', '#cv', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'Улучшение VLA моделей для принятия решений в открытых средах', 'desc': 'Статья представляет новый подход к обучению моделей визуального языка и действий (VLA) для принятия решений в открытых средах. Авторы предлагают метод Act from Visual Language Post-Training, который улучшает базовые модели VLM через визуальное и лингвистическое руководство. Этот подход значительно повышает способности моделей в области знаний о мире, визуального распознавания и пространственной привязки. Эксперименты в Minecraft показали 40% улучшение по сравнению с базовыми агентами на разнообразных атомарных задачах.'}, 'en': {'title': 'Enhancing Decision-Making in Open Worlds with Visual Language Models', 'desc': "This paper presents a new method called Act from Visual Language Post-Training, which enhances Visual Language Models (VLMs) for decision-making in open-world environments. The approach uses visual and linguistic guidance in a self-supervised way to improve the models' understanding of world knowledge, visual recognition, and spatial grounding. The authors demonstrate that their VLA models can effectively follow human instructions in Minecraft for over 1,000 different tasks, achieving a 40% performance boost compared to existing agents. Additionally, their method outperforms traditional imitation learning techniques, setting a new standard in the field."}, 'zh': {'title': '提升开放世界决策能力的视觉语言模型', 'desc': '最近，基于动作的决策在开放世界环境中引起了广泛关注。我们提出了一种新方法，称为视觉语言后训练（Act from Visual Language Post-Training），通过视觉和语言指导自我监督地改进视觉语言模型（VLMs）。这种增强提高了模型在世界知识、视觉识别和空间定位方面的能力。我们的实验表明，在Minecraft中，我们的模型能够执行超过1000个不同的原子任务，并在多样化的任务上实现了40%的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.16219', 'title': "Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't", 'url': 'https://huggingface.co/papers/2503.16219', 'abstract': 'Enhancing the reasoning capabilities of large language models (LLMs) typically relies on massive computational resources and extensive datasets, limiting accessibility for resource-constrained settings. Our study investigates the potential of reinforcement learning (RL) to improve reasoning in small LLMs, focusing on a 1.5-billion-parameter model, DeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA A40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy Optimization (GRPO) algorithm and curating a compact, high-quality mathematical reasoning dataset, we conducted three experiments to explore model behavior and performance. Our results demonstrate rapid reasoning gains - e.g., AMC23 accuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing o1-preview - using only 7,000 samples and a $42 training cost, compared to thousands of dollars for baseline models. However, challenges such as optimization instability and length constraints emerged with prolonged training. These findings highlight the efficacy of RL-based fine-tuning for small LLMs, offering a cost-effective alternative to large-scale approaches. We release our code and datasets as open-source resources, providing insights into trade-offs and laying a foundation for scalable, reasoning-capable LLMs in resource-limited environments. All are available at https://github.com/knoveleng/open-rs.', 'score': 25, 'issue_id': 2829, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'b9603e3e14ebf85d', 'authors': ['Quy-Anh Dang', 'Chris Ngo'], 'affiliations': ['Knovel Engineering Lab, Singapore', 'VNU University of Science, Vietnam'], 'pdf_title_img': 'assets/pdf/title_img/2503.16219.jpg', 'data': {'categories': ['#rl', '#dataset', '#small_models', '#reasoning', '#open_source', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное улучшение рассуждений в малых языковых моделях с помощью RL', 'desc': 'Исследование демонстрирует эффективность применения обучения с подкреплением для улучшения способностей рассуждения у небольших языковых моделей. Эксперименты проводились на модели DeepSeek-R1-Distill-Qwen-1.5B с ограниченными вычислительными ресурсами и временем обучения. Используя адаптированный алгоритм GRPO и специально подобранный набор данных, исследователи добились значительного улучшения производительности модели в задачах математических рассуждений. Результаты показывают, что этот подход может быть экономически эффективной альтернативой обучению крупномасштабных моделей.'}, 'en': {'title': 'Reinforcement Learning: A Cost-Effective Boost for Small Language Models!', 'desc': 'This paper explores how reinforcement learning (RL) can enhance the reasoning abilities of smaller language models, specifically a 1.5-billion-parameter model called DeepSeek-R1-Distill-Qwen-1.5B. The researchers used a modified Group Relative Policy Optimization (GRPO) algorithm and a carefully curated dataset to train the model efficiently on limited hardware. Their experiments showed significant improvements in reasoning accuracy, achieving notable performance gains with minimal training resources. The study emphasizes the potential of RL for fine-tuning smaller models, making advanced reasoning capabilities more accessible in resource-constrained settings.'}, 'zh': {'title': '小型LLMs的推理能力提升新方案', 'desc': '本研究探讨了如何通过强化学习（RL）提升小型大语言模型（LLMs）的推理能力。我们使用了一个包含15亿参数的模型，并在严格的资源限制下进行训练，使用4个NVIDIA A40 GPU，在24小时内完成。通过适应群体相对策略优化（GRPO）算法，并创建高质量的数学推理数据集，我们的实验显示模型的推理能力显著提高，训练成本也大幅降低。尽管在长时间训练中出现了优化不稳定和长度限制等挑战，但我们的研究为资源有限环境中的小型LLMs提供了有效的强化学习微调方案。'}}}, {'id': 'https://huggingface.co/papers/2503.14487', 'title': 'DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers', 'url': 'https://huggingface.co/papers/2503.14487', 'abstract': 'Diffusion models have demonstrated remarkable success in various image generation tasks, but their performance is often limited by the uniform processing of inputs across varying conditions and noise levels. To address this limitation, we propose a novel approach that leverages the inherent heterogeneity of the diffusion process. Our method, DiffMoE, introduces a batch-level global token pool that enables experts to access global token distributions during training, promoting specialized expert behavior. To unleash the full potential of the diffusion process, DiffMoE incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. Through comprehensive evaluation, DiffMoE achieves state-of-the-art performance among diffusion models on ImageNet benchmark, substantially outperforming both dense architectures with 3x activated parameters and existing MoE approaches while maintaining 1x activated parameters. The effectiveness of our approach extends beyond class-conditional generation to more challenging tasks such as text-to-image generation, demonstrating its broad applicability across different diffusion model applications. Project Page: https://shiml20.github.io/DiffMoE/', 'score': 25, 'issue_id': 2828, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'f530713a45ca1341', 'authors': ['Minglei Shi', 'Ziyang Yuan', 'Haotian Yang', 'Xintao Wang', 'Mingwu Zheng', 'Xin Tao', 'Wenliang Zhao', 'Wenzhao Zheng', 'Jie Zhou', 'Jiwen Lu', 'Pengfei Wan', 'Di Zhang', 'Kun Gai'], 'affiliations': ['Kuais', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14487.jpg', 'data': {'categories': ['#benchmark', '#cv', '#diffusion', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'DiffMoE: Умная специализация для улучшения генерации изображений', 'desc': 'DiffMoE - это новый подход к улучшению диффузионных моделей для генерации изображений. Он использует пул глобальных токенов на уровне пакета, что позволяет экспертам получать доступ к глобальным распределениям токенов во время обучения. DiffMoE также включает предиктор мощности, который динамически распределяет вычислительные ресурсы в зависимости от уровня шума и сложности образца. Метод достигает лучших результатов среди диффузионных моделей на бенчмарке ImageNet, значительно превосходя как плотные архитектуры, так и существующие подходы MoE.'}, 'en': {'title': 'Unlocking the Power of Diffusion with Expert Adaptation', 'desc': 'This paper introduces DiffMoE, a new method for improving diffusion models in image generation. It addresses the issue of uniform input processing by utilizing a batch-level global token pool, allowing for specialized expert behavior during training. Additionally, DiffMoE features a capacity predictor that adjusts computational resources based on the noise levels and complexity of the samples. The results show that DiffMoE achieves top performance on the ImageNet benchmark, outperforming existing models while using fewer activated parameters, and is effective in various tasks including text-to-image generation.'}, 'zh': {'title': '释放扩散模型的潜力，DiffMoE引领新潮流', 'desc': '扩散模型在图像生成任务中表现出色，但在不同条件和噪声水平下的输入处理上存在局限性。为了解决这个问题，我们提出了一种新方法，称为DiffMoE，利用扩散过程的内在异质性。该方法引入了一个批量级的全局令牌池，使专家在训练过程中能够访问全局令牌分布，从而促进专家的专业化行为。此外，DiffMoE还结合了一个容量预测器，根据噪声水平和样本复杂性动态分配计算资源，充分发挥扩散过程的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.16418', 'title': 'InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity', 'url': 'https://huggingface.co/papers/2503.16418', 'abstract': 'Achieving flexible and high-fidelity identity-preserved image generation remains formidable, particularly with advanced Diffusion Transformers (DiTs) like FLUX. We introduce InfiniteYou (InfU), one of the earliest robust frameworks leveraging DiTs for this task. InfU addresses significant issues of existing methods, such as insufficient identity similarity, poor text-image alignment, and low generation quality and aesthetics. Central to InfU is InfuseNet, a component that injects identity features into the DiT base model via residual connections, enhancing identity similarity while maintaining generation capabilities. A multi-stage training strategy, including pretraining and supervised fine-tuning (SFT) with synthetic single-person-multiple-sample (SPMS) data, further improves text-image alignment, ameliorates image quality, and alleviates face copy-pasting. Extensive experiments demonstrate that InfU achieves state-of-the-art performance, surpassing existing baselines. In addition, the plug-and-play design of InfU ensures compatibility with various existing methods, offering a valuable contribution to the broader community.', 'score': 24, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '71a484a2a785f8be', 'authors': ['Liming Jiang', 'Qing Yan', 'Yumin Jia', 'Zichuan Liu', 'Hao Kang', 'Xin Lu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.16418.jpg', 'data': {'categories': ['#cv', '#diffusion', '#architecture', '#synthetic', '#training'], 'emoji': '🖼️', 'ru': {'title': 'InfiniteYou: Новый уровень генерации изображений с сохранением идентичности', 'desc': 'InfiniteYou (InfU) - это новая система для генерации изображений с сохранением идентичности, использующая Диффузионные Трансформеры (DiT). Основной компонент InfU - InfuseNet, который внедряет признаки идентичности в базовую модель DiT через остаточные соединения. Система использует многоэтапную стратегию обучения, включая предобучение и контролируемую тонкую настройку на синтетических данных. Эксперименты показывают, что InfU превосходит существующие методы по качеству генерации и соответствию текста изображению.'}, 'en': {'title': 'InfiniteYou: Revolutionizing Identity-Preserved Image Generation with DiTs', 'desc': 'This paper presents InfiniteYou (InfU), a novel framework that enhances identity-preserved image generation using advanced Diffusion Transformers (DiTs) like FLUX. InfU tackles key challenges in the field, such as improving identity similarity, aligning text with images, and enhancing overall image quality. The core innovation is InfuseNet, which integrates identity features into the DiT model through residual connections, thereby boosting identity preservation. With a multi-stage training approach that includes pretraining and supervised fine-tuning on synthetic data, InfU achieves superior performance compared to existing methods, while also being adaptable to various frameworks.'}, 'zh': {'title': '无限可能的身份保留图像生成', 'desc': '本文介绍了一种名为InfiniteYou（InfU）的新框架，旨在实现高保真度的身份保留图像生成。InfU利用先进的扩散变换器（DiTs），解决了现有方法在身份相似性、文本与图像对齐以及生成质量等方面的不足。其核心组件InfuseNet通过残差连接将身份特征注入到DiT基础模型中，从而增强身份相似性，同时保持生成能力。通过多阶段训练策略，包括预训练和监督微调，InfU显著提高了文本与图像的对齐度和图像质量，实验结果表明其性能超越了现有基准。'}}}, {'id': 'https://huggingface.co/papers/2503.13657', 'title': 'Why Do Multi-Agent LLM Systems Fail?', 'url': 'https://huggingface.co/papers/2503.13657', 'abstract': "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM agents collaborate to accomplish tasks, their performance gains across popular benchmarks remain minimal compared to single-agent frameworks. This gap highlights the need to analyze the challenges hindering MAS effectiveness.   In this paper, we present the first comprehensive study of MAS challenges. We analyze five popular MAS frameworks across over 150 tasks, involving six expert human annotators. We identify 14 unique failure modes and propose a comprehensive taxonomy applicable to various MAS frameworks. This taxonomy emerges iteratively from agreements among three expert annotators per study, achieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are organized into 3 categories, (i) specification and system design failures, (ii) inter-agent misalignment, and (iii) task verification and termination. To support scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also explore if identified failures could be easily prevented by proposing two interventions: improved specification of agent roles and enhanced orchestration strategies. Our findings reveal that identified failures require more complex solutions, highlighting a clear roadmap for future research. We open-source our dataset and LLM annotator.", 'score': 24, 'issue_id': 2829, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'f3e2aec1e3948d46', 'authors': ['Mert Cemri', 'Melissa Z. Pan', 'Shuyi Yang', 'Lakshya A. Agrawal', 'Bhavya Chopra', 'Rishabh Tiwari', 'Kurt Keutzer', 'Aditya Parameswaran', 'Dan Klein', 'Kannan Ramchandran', 'Matei Zaharia', 'Joseph E. Gonzalez', 'Ion Stoica'], 'affiliations': ['Intesa Sanpaolo', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.13657.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#dataset', '#agi', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Раскрывая проблемы мультиагентных систем: путь к эффективному сотрудничеству ИИ', 'desc': 'Это исследование анализирует проблемы мультиагентных систем (МАС) с использованием больших языковых моделей. Авторы идентифицировали 14 уникальных режимов отказа и предложили таксономию, применимую к различным фреймворкам МАС. Исследование включало анализ пяти популярных фреймворков МАС на более чем 150 задачах с участием шести экспертов-аннотаторов. Результаты показывают, что выявленные проблемы требуют сложных решений, что открывает перспективы для будущих исследований.'}, 'en': {'title': 'Unlocking the Potential of Multi-Agent Systems: Identifying and Overcoming Challenges', 'desc': 'This paper investigates the challenges faced by Multi-Agent Systems (MAS) that involve multiple large language model (LLM) agents working together. The authors conducted a thorough analysis of five MAS frameworks across over 150 tasks, identifying 14 distinct failure modes that hinder performance. They categorized these failures into three main groups: specification and system design failures, inter-agent misalignment, and task verification and termination issues. Additionally, the paper proposes interventions to mitigate these failures and emphasizes the need for more sophisticated solutions to enhance MAS effectiveness.'}, 'zh': {'title': '揭示多智能体系统的挑战与解决方案', 'desc': '尽管多智能体系统（MAS）受到越来越多的关注，但与单智能体框架相比，其在流行基准上的性能提升仍然有限。本文首次全面研究了MAS面临的挑战，分析了五个流行的MAS框架及其在150多个任务中的表现。我们识别出14种独特的失败模式，并提出了适用于各种MAS框架的综合分类法。研究结果表明，解决这些失败模式需要更复杂的解决方案，为未来的研究指明了方向。'}}}, {'id': 'https://huggingface.co/papers/2503.16257', 'title': 'Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models', 'url': 'https://huggingface.co/papers/2503.16257', 'abstract': 'Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose a mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for a better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in a per-channel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts.', 'score': 20, 'issue_id': 2829, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '68c739951d6b3dce', 'authors': ['Keda Tao', 'Haoxuan You', 'Yang Sui', 'Can Qin', 'Huan Wang'], 'affiliations': ['Columbia University', 'Rice University', 'Salesforce AI Research', 'Westlake University', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16257.jpg', 'data': {'categories': ['#video', '#long_context', '#inference', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'VidKV: Эффективное сжатие кэша для видео-языковых моделей', 'desc': 'Эта статья представляет VidKV - новый метод квантизации кэша ключ-значение (KV) для видео-языковых моделей (VideoLLMs). Авторы предлагают смешанную стратегию квантизации для ключей и значений, позволяющую сжать кэш до менее чем 2 бит без существенной потери производительности. Метод включает 2-битную квантизацию для аномальных каналов ключей и 1-битную с FFT для нормальных, а также 1.58-битную квантизацию значений с сохранением семантически важных визуальных токенов. Эксперименты на шести бенчмарках показывают эффективность VidKV в сжатии кэша до 1.5-1.58 бит без значительного снижения качества работы модели.'}, 'en': {'title': 'Efficient KV Cache Compression for VideoLLMs with VidKV', 'desc': 'This paper introduces VidKV, a novel method for quantizing key-value (KV) caches in Video Large Language Models (VideoLLMs) to improve memory efficiency. The authors demonstrate that using 2-bit quantization does not significantly impact model performance, and they explore even lower quantization levels. VidKV employs a mixed-precision strategy for keys and a selective filtering approach for values, allowing for effective compression while maintaining accuracy. The results show that VidKV can reduce KV cache size to 1.5-bit and 1.58-bit with minimal performance loss, outperforming previous methods that used per-token quantization.'}, 'zh': {'title': '压缩KV缓存，提升视频模型性能', 'desc': '视频大型语言模型（VideoLLMs）能够处理更长的视频输入，并进行复杂的推理和分析。然而，由于视频帧中成千上万的视觉标记，键值（KV）缓存会显著增加内存需求，成为推理速度和内存使用的瓶颈。本文提出了一种名为VidKV的KV缓存量化方法，可以将KV缓存压缩到低于2位，同时保持模型性能几乎不变。我们的方法通过混合精度量化策略和选择性过滤语义显著的视觉标记，实现了更好的精度与模型性能之间的平衡。'}}}, {'id': 'https://huggingface.co/papers/2503.16252', 'title': 'Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.16252', 'abstract': 'Reasoning large language models are rapidly evolving across various domains. However, their capabilities in handling complex financial tasks still require in-depth exploration. In this paper, we introduce Fin-R1, a reasoning large language model specifically designed for the financial sector. Fin-R1 is built using a two-stage architecture, leveraging a financial reasoning dataset distilled and processed based on DeepSeek-R1. Through supervised fine-tuning (SFT) and reinforcement learning (RL) training, it demonstrates performance close to DeepSeek-R1 with a parameter size of 7 billion across a range of financial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA and ConvFinQA tasks between those LLMs in our evaluation, surpassing larger models in other tasks as well. Fin-R1 showcases strong reasoning and decision-making capabilities, providing solutions to various problems encountered in the financial domain. Our code is available at https://github.com/SUFE-AIFLM-Lab/Fin-R1.', 'score': 20, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'f2a1e8506f9711ee', 'authors': ['Zhaowei Liu', 'Xin Guo', 'Fangqi Lou', 'Lingfeng Zeng', 'Jinyi Niu', 'Zixuan Wang', 'Jiajie Xu', 'Weige Cai', 'Ziwei Yang', 'Xueqian Zhao', 'Chao Li', 'Sheng Xu', 'Dezhi Chen', 'Yun Chen', 'Zuo Bai', 'Liwen Zhang'], 'affiliations': ['FinStep', 'Fudan University', 'Shanghai University of Finance and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2503.16252.jpg', 'data': {'categories': ['#training', '#rl', '#architecture', '#reasoning', '#dataset', '#healthcare'], 'emoji': '💹', 'ru': {'title': 'Fin-R1: Мощная языковая модель для финансового анализа и рассуждений', 'desc': 'Исследователи представляют Fin-R1 - языковую модель, специализированную для финансового сектора. Модель использует двухэтапную архитектуру и обучена на финансовом наборе данных, основанном на DeepSeek-R1. Fin-R1 демонстрирует производительность, близкую к DeepSeek-R1, при размере в 7 миллиардов параметров. Модель достигает лучших результатов в задачах FinQA и ConvFinQA среди оцениваемых моделей, показывая сильные способности к рассуждению и принятию решений в финансовой сфере.'}, 'en': {'title': 'Fin-R1: Revolutionizing Financial Reasoning with AI', 'desc': 'This paper presents Fin-R1, a specialized reasoning large language model tailored for financial tasks. It employs a two-stage architecture and is trained on a unique financial reasoning dataset derived from DeepSeek-R1. Through techniques like supervised fine-tuning and reinforcement learning, Fin-R1 achieves competitive performance with 7 billion parameters, excelling in financial reasoning benchmarks such as FinQA and ConvFinQA. The model demonstrates advanced reasoning and decision-making skills, addressing various challenges in the financial sector.'}, 'zh': {'title': '金融领域的推理新星：Fin-R1', 'desc': '本文介绍了一种专为金融领域设计的推理大型语言模型Fin-R1。Fin-R1采用两阶段架构，利用基于DeepSeek-R1的金融推理数据集进行训练。通过监督微调（SFT）和强化学习（RL），它在多个金融推理任务中表现接近DeepSeek-R1，参数规模为70亿。Fin-R1在FinQA和ConvFinQA任务中达到了当前最先进的水平，展现出强大的推理和决策能力，为金融领域的各种问题提供了解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.16212', 'title': 'MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion', 'url': 'https://huggingface.co/papers/2503.16212', 'abstract': 'Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, MathFusionQA, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion.', 'score': 17, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'b5997ebd979f98f0', 'authors': ['Qizhi Pei', 'Lijun Wu', 'Zhuoshi Pan', 'Yu Li', 'Honglin Lin', 'Chenlin Ming', 'Xin Gao', 'Conghui He', 'Rui Yan'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Computer Science, Wuhan University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16212.jpg', 'data': {'categories': ['#training', '#synthetic', '#open_source', '#math', '#reasoning', '#dataset', '#benchmark', '#data'], 'emoji': '🧮', 'ru': {'title': 'Слияние задач для прокачки математических способностей ИИ', 'desc': 'MathFusion - это новый подход к улучшению математического мышления у больших языковых моделей (LLM) через синтез инструкций на основе связей между задачами. Метод использует три стратегии слияния задач: последовательное, параллельное и условное, что позволяет создавать более сложные и взаимосвязанные обучающие примеры. Авторы создали датасет MathFusionQA и провели эксперименты на нескольких LLM, показав значительное улучшение точности на 18 процентных пунктов при использовании всего 45 тысяч дополнительных синтетических инструкций. Этот подход демонстрирует существенное преимущество над традиционными методами обучения на отдельных задачах.'}, 'en': {'title': 'Enhancing Mathematical Reasoning through Conceptual Fusion', 'desc': 'This paper presents MathFusion, a new framework designed to improve the mathematical reasoning capabilities of Large Language Models (LLMs). Unlike traditional methods that only modify individual problems, MathFusion uses three innovative fusion strategies to connect related mathematical concepts and problems. These strategies include sequential, parallel, and conditional fusion, which help models learn from the relationships between problems rather than in isolation. The results show that MathFusion significantly enhances accuracy in mathematical reasoning tasks while being efficient in data usage, outperforming previous methods with fewer additional instructions.'}, 'zh': {'title': '通过MathFusion提升数学推理能力', 'desc': '大型语言模型在数学推理方面取得了显著进展。现有的数据增强方法主要集中在实例级别的修改，未能有效利用数学知识中固有的关系结构。我们提出了MathFusion框架，通过跨问题的指令合成来提升数学推理能力。该框架采用三种融合策略，显著提高了模型在数学推理任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.10625', 'title': 'LHM: Large Animatable Human Reconstruction Model from a Single Image in\n  Seconds', 'url': 'https://huggingface.co/papers/2503.10625', 'abstract': 'Animatable 3D human reconstruction from a single image is a challenging problem due to the ambiguity in decoupling geometry, appearance, and deformation. Recent advances in 3D human reconstruction mainly focus on static human modeling, and the reliance of using synthetic 3D scans for training limits their generalization ability. Conversely, optimization-based video methods achieve higher fidelity but demand controlled capture conditions and computationally intensive refinement processes. Motivated by the emergence of large reconstruction models for efficient static reconstruction, we propose LHM (Large Animatable Human Reconstruction Model) to infer high-fidelity avatars represented as 3D Gaussian splatting in a feed-forward pass. Our model leverages a multimodal transformer architecture to effectively encode the human body positional features and image features with attention mechanism, enabling detailed preservation of clothing geometry and texture. To further boost the face identity preservation and fine detail recovery, we propose a head feature pyramid encoding scheme to aggregate multi-scale features of the head regions. Extensive experiments demonstrate that our LHM generates plausible animatable human in seconds without post-processing for face and hands, outperforming existing methods in both reconstruction accuracy and generalization ability.', 'score': 17, 'issue_id': 2828, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '35de541953d40e20', 'authors': ['Lingteng Qiu', 'Xiaodong Gu', 'Peihao Li', 'Qi Zuo', 'Weichao Shen', 'Junfei Zhang', 'Kejie Qiu', 'Weihao Yuan', 'Guanying Chen', 'Zilong Dong', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.10625.jpg', 'data': {'categories': ['#multimodal', '#3d', '#architecture'], 'emoji': '🧑\u200d🦰', 'ru': {'title': 'Реконструкция анимируемых 3D-аватаров человека по одному изображению с помощью трансформеров', 'desc': 'Статья представляет LHM (Large Animatable Human Reconstruction Model) - новую модель для реконструкции анимируемых 3D-аватаров человека по одному изображению. Модель использует мультимодальную архитектуру трансформера для кодирования позиционных признаков тела и признаков изображения с механизмом внимания. Для улучшения сохранения идентичности лица и мелких деталей предложена схема кодирования пирамиды признаков головы. LHM генерирует правдоподобные анимируемые модели человека за секунды без постобработки, превосходя существующие методы по точности реконструкции и способности к обобщению.'}, 'en': {'title': 'Revolutionizing 3D Human Reconstruction with LHM', 'desc': 'This paper presents the Large Animatable Human Reconstruction Model (LHM), which addresses the challenge of creating 3D human avatars from a single image. Unlike previous methods that rely on static models or require extensive computational resources, LHM uses a multimodal transformer architecture to efficiently encode both body and image features. The model incorporates a head feature pyramid encoding scheme to enhance the detail and identity of facial features. Experimental results show that LHM can generate high-fidelity, animatable human models quickly and accurately, surpassing existing techniques in both performance and adaptability.'}, 'zh': {'title': '高效的3D可动画人类重建模型', 'desc': '本文提出了一种新的大型可动画人类重建模型（LHM），旨在从单张图像中重建高保真度的3D人类头像。该模型利用多模态变换器架构，有效编码人体位置特征和图像特征，能够详细保留衣物的几何形状和纹理。为了增强面部身份的保留和细节恢复，本文还提出了一种头部特征金字塔编码方案，聚合头部区域的多尺度特征。实验表明，LHM在重建准确性和泛化能力上优于现有方法，能够在几秒内生成可动画的人类模型，无需后处理。'}}}, {'id': 'https://huggingface.co/papers/2503.16420', 'title': 'SynCity: Training-Free Generation of 3D Worlds', 'url': 'https://huggingface.co/papers/2503.16420', 'abstract': 'We address the challenge of generating 3D worlds from textual descriptions. We propose SynCity, a training- and optimization-free approach, which leverages the geometric precision of pre-trained 3D generative models and the artistic versatility of 2D image generators to create large, high-quality 3D spaces. While most 3D generative models are object-centric and cannot generate large-scale worlds, we show how 3D and 2D generators can be combined to generate ever-expanding scenes. Through a tile-based approach, we allow fine-grained control over the layout and the appearance of scenes. The world is generated tile-by-tile, and each new tile is generated within its world-context and then fused with the scene. SynCity generates compelling and immersive scenes that are rich in detail and diversity.', 'score': 16, 'issue_id': 2836, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '2a35e5254e6b2f50', 'authors': ['Paul Engstler', 'Aleksandar Shtedritski', 'Iro Laina', 'Christian Rupprecht', 'Andrea Vedaldi'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.16420.jpg', 'data': {'categories': ['#synthetic', '#3d', '#games'], 'emoji': '🏙️', 'ru': {'title': 'SynCity: Создание бесконечных 3D-миров из текста', 'desc': 'Статья представляет SynCity - подход к генерации трехмерных миров на основе текстовых описаний. Метод объединяет геометрическую точность предобученных 3D-генеративных моделей с художественной гибкостью 2D-генераторов изображений. SynCity использует тайловый подход, позволяющий детально контролировать компоновку и внешний вид сцен. Мир генерируется плитка за плиткой, каждая новая плитка создается в контексте мира и затем сливается со сценой.'}, 'en': {'title': 'Building 3D Worlds from Words with SynCity', 'desc': 'This paper introduces SynCity, a novel method for creating 3D environments from text descriptions without the need for extensive training or optimization. It combines the strengths of pre-trained 3D generative models, which provide geometric accuracy, with the flexibility of 2D image generators to produce expansive and detailed 3D worlds. By using a tile-based approach, SynCity allows for precise control over the layout and aesthetics of the generated scenes, ensuring that each tile fits cohesively within the overall context of the world. The result is a system capable of generating immersive and richly detailed environments that enhance user experience.'}, 'zh': {'title': '从文本到3D世界的创新生成', 'desc': '我们解决了从文本描述生成3D世界的挑战。我们提出了SynCity，这是一种无需训练和优化的方法，利用预训练的3D生成模型的几何精度和2D图像生成器的艺术多样性来创建大型高质量的3D空间。通过基于瓦片的方法，我们可以对场景的布局和外观进行细致的控制。SynCity生成的场景引人入胜，细节丰富，具有多样性。'}}}, {'id': 'https://huggingface.co/papers/2503.16413', 'title': 'M3: 3D-Spatial MultiModal Memory', 'url': 'https://huggingface.co/papers/2503.16413', 'abstract': "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system designed to retain information about medium-sized static scenes through video sources for visual perception. By integrating 3D Gaussian Splatting techniques with foundation models, M3 builds a multimodal memory capable of rendering feature representations across granularities, encompassing a wide range of knowledge. In our exploration, we identify two key challenges in previous works on feature splatting: (1) computational constraints in storing high-dimensional features for each Gaussian primitive, and (2) misalignment or information loss between distilled features and foundation model features. To address these challenges, we propose M3 with key components of principal scene components and Gaussian memory attention, enabling efficient training and inference. To validate M3, we conduct comprehensive quantitative evaluations of feature similarity and downstream tasks, as well as qualitative visualizations to highlight the pixel trace of Gaussian memory attention. Our approach encompasses a diverse range of foundation models, including vision-language models (VLMs), perception models, and large multimodal and language models (LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy M3's feature field in indoor scenes on a quadruped robot. Notably, we claim that M3 is the first work to address the core compression challenges in 3D feature distillation.", 'score': 13, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '963439a9c78faf82', 'authors': ['Xueyan Zou', 'Yuchen Song', 'Ri-Zhao Qiu', 'Xuanbin Peng', 'Jianglong Ye', 'Sifei Liu', 'Xiaolong Wang'], 'affiliations': ['NVIDIA', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2503.16413.jpg', 'data': {'categories': ['#robotics', '#architecture', '#optimization', '#games', '#3d', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Эффективная 3D мультимодальная память для визуального восприятия', 'desc': 'Статья представляет 3D Пространственную МультиМодальную Память (M3) - систему, предназначенную для сохранения информации о статичных сценах среднего размера через видеоисточники для визуального восприятия. M3 объединяет методы 3D гауссовского сплаттинга с фундаментальными моделями для создания мультимодальной памяти, способной рендерить представления признаков разной гранулярности. Авторы предлагают решения для преодоления вычислительных ограничений при хранении высокоразмерных признаков и проблем несоответствия между дистиллированными признаками и признаками фундаментальных моделей. Система M3 демонстрирует эффективность в различных задачах и может применяться с разными типами фундаментальных моделей, включая мультимодальные модели и большие языковые модели.'}, 'en': {'title': 'Revolutionizing Scene Memory with M3: Efficient 3D Feature Distillation', 'desc': 'The paper introduces the 3D Spatial MultiModal Memory (M3), a novel memory system that captures and retains information about static scenes using video inputs for enhanced visual perception. M3 combines 3D Gaussian Splatting with foundation models to create a multimodal memory that effectively represents features at various levels of detail. It tackles two significant challenges in previous methods: the difficulty of storing high-dimensional features and the misalignment of distilled features with those from foundation models. The authors validate M3 through extensive evaluations and demonstrate its practical use in real-world scenarios, such as deploying it on a quadruped robot in indoor environments.'}, 'zh': {'title': '3D多模态记忆：解决特征压缩挑战的创新', 'desc': '我们提出了3D空间多模态记忆系统（M3），旨在通过视频源保留中等大小静态场景的信息，以增强视觉感知。M3结合了3D高斯点云技术和基础模型，构建了一个能够跨粒度渲染特征表示的多模态记忆系统，涵盖广泛的知识。我们识别了以往特征点云工作中的两个主要挑战：存储高维特征的计算限制，以及提取特征与基础模型特征之间的错位或信息丢失。为了解决这些问题，我们提出了M3，采用了主要场景组件和高斯记忆注意力的关键组件，实现了高效的训练和推理。'}}}, {'id': 'https://huggingface.co/papers/2503.16422', 'title': '1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering', 'url': 'https://huggingface.co/papers/2503.16422', 'abstract': '4D Gaussian Splatting (4DGS) has recently gained considerable attention as a method for reconstructing dynamic scenes. Despite achieving superior quality, 4DGS typically requires substantial storage and suffers from slow rendering speed. In this work, we delve into these issues and identify two key sources of temporal redundancy. (Q1) Short-Lifespan Gaussians: 4DGS uses a large portion of Gaussians with short temporal span to represent scene dynamics, leading to an excessive number of Gaussians. (Q2) Inactive Gaussians: When rendering, only a small subset of Gaussians contributes to each frame. Despite this, all Gaussians are processed during rasterization, resulting in redundant computation overhead. To address these redundancies, we present 4DGS-1K, which runs at over 1000 FPS on modern GPUs. For Q1, we introduce the Spatial-Temporal Variation Score, a new pruning criterion that effectively removes short-lifespan Gaussians while encouraging 4DGS to capture scene dynamics using Gaussians with longer temporal spans. For Q2, we store a mask for active Gaussians across consecutive frames, significantly reducing redundant computations in rendering. Compared to vanilla 4DGS, our method achieves a 41times reduction in storage and 9times faster rasterization speed on complex dynamic scenes, while maintaining comparable visual quality. Please see our project page at https://4DGS-1K.github.io.', 'score': 11, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'a00a6529a3f82f89', 'authors': ['Yuheng Yuan', 'Qiuhong Shen', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.16422.jpg', 'data': {'categories': ['#3d', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': '4DGS-1K: Сверхбыстрая реконструкция динамических сцен', 'desc': 'Статья представляет метод 4DGS-1K для оптимизации 4D гауссового сплаттинга в реконструкции динамических сцен. Авторы решают проблемы избыточного хранения и медленного рендеринга, вызванные короткоживущими гауссианами и неактивными гауссианами при визуализации. Предложенный подход использует новый критерий отсечения на основе пространственно-временной вариационной оценки и маскирование активных гауссианов. В результате достигается 41-кратное уменьшение объема хранения и 9-кратное ускорение растеризации по сравнению с обычным 4DGS при сохранении сопоставимого визуального качества.'}, 'en': {'title': 'Streamlining Dynamic Scene Reconstruction with 4DGS-1K', 'desc': 'This paper introduces 4DGS-1K, an improved version of 4D Gaussian Splatting that addresses issues of high storage requirements and slow rendering speeds in dynamic scene reconstruction. The authors identify two main sources of inefficiency: the use of short-lifespan Gaussians that clutter the representation and the processing of inactive Gaussians during rendering. To optimize performance, they propose a new pruning criterion called the Spatial-Temporal Variation Score, which helps retain only the most relevant Gaussians. As a result, 4DGS-1K achieves a significant reduction in storage and rendering time while preserving visual quality, making it a more efficient solution for real-time applications.'}, 'zh': {'title': '提升动态场景重建效率的4DGS-1K', 'desc': '4D高斯点云（4DGS）是一种用于重建动态场景的方法，但通常需要大量存储并且渲染速度较慢。本文探讨了导致这些问题的两个主要来源：短生命周期高斯和非活动高斯。我们提出了4DGS-1K，通过引入空间-时间变化评分来有效去除短生命周期高斯，并存储活跃高斯的掩码，从而显著减少冗余计算。与传统4DGS相比，我们的方法在复杂动态场景中实现了41倍的存储减少和9倍的渲染速度提升，同时保持了相似的视觉质量。'}}}, {'id': 'https://huggingface.co/papers/2503.16356', 'title': 'CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners', 'url': 'https://huggingface.co/papers/2503.16356', 'abstract': 'Knowledge Editing (KE) enables the modification of outdated or incorrect information in large language models (LLMs). While existing KE methods can update isolated facts, they struggle to generalize these updates to multi-hop reasoning tasks that depend on the modified knowledge. Through an analysis of reasoning circuits -- the neural pathways LLMs use for knowledge-based inference, we observe that current layer-localized KE approaches, such as MEMIT and WISE, which edit only single or a few model layers, struggle to effectively incorporate updated information into these reasoning pathways. To address this limitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method that enables more effective integration of updated knowledge in LLMs. CaKE leverages strategically curated data, guided by our circuits-based analysis, that enforces the model to utilize the modified knowledge, stimulating the model to develop appropriate reasoning circuits for newly integrated knowledge. Experimental results show that CaKE enables more accurate and consistent use of updated knowledge across related reasoning tasks, leading to an average of 20% improvement in multi-hop reasoning accuracy on MQuAKE dataset compared to existing KE methods. We release the code and data in https://github.com/zjunlp/CaKE.', 'score': 11, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '8b74fed43c99c37d', 'authors': ['Yunzhi Yao', 'Jizhan Fang', 'Jia-Chen Gu', 'Ningyu Zhang', 'Shumin Deng', 'Huajun Chen', 'Nanyun Peng'], 'affiliations': ['National University of Singapore', 'University of California, Los Angeles', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16356.jpg', 'data': {'categories': ['#training', '#open_source', '#reasoning', '#dataset', '#data'], 'emoji': '🧠', 'ru': {'title': 'CaKE: улучшение рассуждений в ИИ через умное редактирование знаний', 'desc': 'Статья представляет новый метод редактирования знаний в больших языковых моделях под названием CaKE. Этот метод позволяет более эффективно интегрировать обновленные знания в модели, улучшая их способность к многоступенчатым рассуждениям. CaKE использует стратегически подобранные данные, основанные на анализе нейронных путей, которые модели используют для логических выводов. Эксперименты показывают, что CaKE обеспечивает более точное и последовательное использование обновленных знаний в связанных задачах рассуждения, повышая точность многоступенчатых рассуждений в среднем на 20% по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing Knowledge Integration in Language Models with CaKE', 'desc': 'This paper introduces CaKE (Circuit-aware Knowledge Editing), a new method for updating knowledge in large language models (LLMs). Current methods struggle with multi-hop reasoning tasks that require the integration of modified information. CaKE improves upon these methods by analyzing the neural pathways used for reasoning and ensuring that updated knowledge is effectively incorporated. The results show a significant increase in accuracy for reasoning tasks, demonstrating the effectiveness of this approach.'}, 'zh': {'title': '电路感知知识编辑：提升多跳推理的准确性', 'desc': '知识编辑（KE）使得大型语言模型（LLMs）能够修改过时或不正确的信息。现有的KE方法虽然可以更新孤立的事实，但在多跳推理任务中却难以有效推广这些更新。通过对推理电路的分析，我们发现当前的层局部KE方法在有效整合更新信息方面存在困难。为了解决这个问题，我们提出了CaKE（电路感知知识编辑），它通过精心策划的数据，促进模型利用修改后的知识，从而提高多跳推理的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.16322', 'title': 'Ultra-Resolution Adaptation with Ease', 'url': 'https://huggingface.co/papers/2503.16322', 'abstract': 'Text-to-image diffusion models have achieved remarkable progress in recent years. However, training models for high-resolution image generation remains challenging, particularly when training data and computational resources are limited. In this paper, we explore this practical problem from two key perspectives: data and parameter efficiency, and propose a set of key guidelines for ultra-resolution adaptation termed URAE. For data efficiency, we theoretically and empirically demonstrate that synthetic data generated by some teacher models can significantly promote training convergence. For parameter efficiency, we find that tuning minor components of the weight matrices outperforms widely-used low-rank adapters when synthetic data are unavailable, offering substantial performance gains while maintaining efficiency. Additionally, for models leveraging guidance distillation, such as FLUX, we show that disabling classifier-free guidance, i.e., setting the guidance scale to 1 during adaptation, is crucial for satisfactory performance. Extensive experiments validate that URAE achieves comparable 2K-generation performance to state-of-the-art closed-source models like FLUX1.1 [Pro] Ultra with only 3K samples and 2K iterations, while setting new benchmarks for 4K-resolution generation. Codes are available https://github.com/Huage001/URAE{here}.', 'score': 11, 'issue_id': 2824, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '48ce70d2a4cf0cf7', 'authors': ['Ruonan Yu', 'Songhua Liu', 'Zhenxiong Tan', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.16322.jpg', 'data': {'categories': ['#optimization', '#data', '#diffusion', '#cv', '#synthetic', '#training', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная адаптация диффузионных моделей для сверхвысокого разрешения', 'desc': 'Статья представляет набор рекомендаций под названием URAE для адаптации диффузионных моделей к генерации изображений сверхвысокого разрешения. Авторы исследуют проблему с точки зрения эффективности данных и параметров, демонстрируя преимущества использования синтетических данных от учительских моделей. Они также обнаружили, что настройка небольших компонентов весовых матриц превосходит широко используемые низкоранговые адаптеры при отсутствии синтетических данных. Эксперименты подтверждают, что URAE достигает сопоставимой производительности при генерации 2K-изображений с использованием всего 3000 образцов и 2000 итераций, устанавливая новые ориентиры для генерации изображений с разрешением 4K.'}, 'en': {'title': 'Efficient High-Resolution Image Generation with URAE', 'desc': 'This paper addresses the challenges of training text-to-image diffusion models for high-resolution image generation, especially when data and computational resources are limited. It introduces URAE, a set of guidelines focused on improving data and parameter efficiency. The authors demonstrate that synthetic data from teacher models can enhance training convergence, and that tuning specific components of weight matrices can yield better results than traditional low-rank adapters. Their experiments show that URAE can achieve high-quality 2K and 4K image generation with significantly fewer training samples and iterations compared to existing models.'}, 'zh': {'title': '高效生成高分辨率图像的创新方法', 'desc': '本文探讨了高分辨率图像生成中的训练挑战，尤其是在数据和计算资源有限的情况下。我们提出了一套名为URAE的超分辨率适应指南，旨在提高数据和参数效率。研究表明，某些教师模型生成的合成数据可以显著促进训练收敛，而微调权重矩阵的少量组件在缺乏合成数据时表现优于常用的低秩适配器。通过大量实验验证，URAE在仅使用3000个样本和2000次迭代的情况下，达到了与FLUX1.1等最先进闭源模型相当的2K生成性能，并为4K分辨率生成设定了新基准。'}}}, {'id': 'https://huggingface.co/papers/2503.16057', 'title': 'Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts', 'url': 'https://huggingface.co/papers/2503.16057', 'abstract': 'Diffusion models have emerged as mainstream framework in visual generation. Building upon this success, the integration of Mixture of Experts (MoE) methods has shown promise in enhancing model scalability and performance. In this paper, we introduce Race-DiT, a novel MoE model for diffusion transformers with a flexible routing strategy, Expert Race. By allowing tokens and experts to compete together and select the top candidates, the model learns to dynamically assign experts to critical tokens. Additionally, we propose per-layer regularization to address challenges in shallow layer learning, and router similarity loss to prevent mode collapse, ensuring better expert utilization. Extensive experiments on ImageNet validate the effectiveness of our approach, showcasing significant performance gains while promising scaling properties.', 'score': 11, 'issue_id': 2827, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'bb5d00b3068e881c', 'authors': ['Yike Yuan', 'Ziyu Wang', 'Zihao Huang', 'Defa Zhu', 'Xun Zhou', 'Jingyi Yu', 'Qiyang Min'], 'affiliations': ['ByteDance Seed', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16057.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#cv', '#training', '#optimization'], 'emoji': '🏎️', 'ru': {'title': 'Race-DiT: Гонки экспертов для улучшения диффузионных моделей', 'desc': 'Статья представляет Race-DiT - новую модель Mixture of Experts (MoE) для диффузионных трансформеров с гибкой стратегией маршрутизации под названием Expert Race. Модель динамически назначает экспертов критически важным токенам, позволяя токенам и экспертам соревноваться и выбирать лучших кандидатов. Авторы предлагают регуляризацию для каждого слоя, чтобы решить проблемы обучения поверхностных слоев, и функцию потерь сходства маршрутизаторов для предотвращения коллапса мод. Эксперименты на ImageNet подтверждают эффективность подхода, демонстрируя значительный прирост производительности и перспективные свойства масштабирования.'}, 'en': {'title': 'Race-DiT: Enhancing Diffusion Transformers with Expert Competition', 'desc': 'This paper presents Race-DiT, a new model that combines diffusion transformers with Mixture of Experts (MoE) techniques to improve visual generation. The model features a flexible routing strategy called Expert Race, which allows tokens to compete and select the best experts for processing. To enhance learning in shallow layers, the authors introduce per-layer regularization and a router similarity loss to avoid mode collapse. Experimental results on ImageNet demonstrate that Race-DiT achieves significant performance improvements and scalability compared to previous models.'}, 'zh': {'title': 'Race-DiT：动态分配专家的扩散模型', 'desc': '扩散模型已成为视觉生成的主流框架。本文提出了一种新颖的混合专家模型Race-DiT，采用灵活的路由策略Expert Race，以提高模型的可扩展性和性能。该模型通过让令牌和专家竞争并选择最佳候选者，动态分配专家给关键令牌。此外，我们还提出了每层正则化和路由器相似性损失，以解决浅层学习中的挑战，确保更好的专家利用率。'}}}, {'id': 'https://huggingface.co/papers/2503.16428', 'title': 'XAttention: Block Sparse Attention with Antidiagonal Scoring', 'url': 'https://huggingface.co/papers/2503.16428', 'abstract': "Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements. In this paper, we introduce XAttention, a plug-and-play framework that dramatically accelerates long-context inference in Transformers models using sparse attention. XAttention's key innovation is the insight that the sum of antidiagonal values (i.e., from the lower-left to upper-right) in the attention matrix provides a powerful proxy for block importance. This allows for precise identification and pruning of non-essential blocks, resulting in high sparsity and dramatically accelerated inference. Across comprehensive evaluations on demanding long-context benchmarks-including RULER and LongBench for language, VideoMME for video understanding, and VBench for video generation. XAttention achieves accuracy comparable to full attention while delivering substantial computational gains. We demonstrate up to 13.5x acceleration in attention computation. These results underscore XAttention's ability to unlock the practical potential of block sparse attention, paving the way for scalable and efficient deployment of LCTMs in real-world applications. Code is available at https://github.com/mit-han-lab/x-attention.", 'score': 10, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'befbf726163c510a', 'authors': ['Ruyi Xu', 'Guangxuan Xiao', 'Haofeng Huang', 'Junxian Guo', 'Song Han'], 'affiliations': ['Massachusetts Institute of Technology', 'NVIDIA', 'SJTU', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16428.jpg', 'data': {'categories': ['#inference', '#architecture', '#long_context', '#optimization', '#video', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'XAttention: Ускорение трансформеров без потери точности', 'desc': 'XAttention - это новый фреймворк для ускорения обработки длинных контекстов в моделях трансформеров с использованием разреженного внимания. Основная инновация заключается в использовании суммы антидиагональных значений в матрице внимания для определения важности блоков. Это позволяет точно идентифицировать и отсекать несущественные блоки, что приводит к высокой разреженности и значительному ускорению вывода. XAttention показывает точность, сравнимую с полным вниманием, при существенном увеличении вычислительной эффективности на различных задачах обработки длинных контекстов.'}, 'en': {'title': 'Accelerating Long-Context Transformers with XAttention', 'desc': 'This paper presents XAttention, a new framework designed to improve the efficiency of long-context Transformer models by utilizing block-sparse attention. Traditional attention mechanisms are computationally expensive, but XAttention reduces this cost by focusing on important regions of the attention matrix. The key innovation is using the sum of antidiagonal values to identify and prune less important blocks, which maintains accuracy while enhancing speed. Evaluations show that XAttention can achieve up to 13.5 times faster attention computation without sacrificing performance, making it a significant advancement for deploying LCTMs in practical applications.'}, 'zh': {'title': 'XAttention：加速长上下文推理的创新框架', 'desc': '长上下文变换器模型（LCTMs）在实际应用中非常重要，但由于注意力机制的平方复杂度，计算成本很高。块稀疏注意力通过集中计算在关键区域来缓解这一问题，但现有方法在平衡准确性和效率方面面临挑战。本文介绍了XAttention，这是一种即插即用的框架，通过稀疏注意力显著加速变换器模型的长上下文推理。XAttention的关键创新在于利用注意力矩阵中反对角线值的总和作为块重要性的强大代理，从而实现高稀疏性和显著加速推理。'}}}, {'id': 'https://huggingface.co/papers/2503.16425', 'title': 'Tokenize Image as a Set', 'url': 'https://huggingface.co/papers/2503.16425', 'abstract': "This paper proposes a fundamentally new paradigm for image generation through set-based tokenization and distribution modeling. Unlike conventional methods that serialize images into fixed-position latent codes with a uniform compression ratio, we introduce an unordered token set representation to dynamically allocate coding capacity based on regional semantic complexity. This TokenSet enhances global context aggregation and improves robustness against local perturbations. To address the critical challenge of modeling discrete sets, we devise a dual transformation mechanism that bijectively converts sets into fixed-length integer sequences with summation constraints. Further, we propose Fixed-Sum Discrete Diffusion--the first framework to simultaneously handle discrete values, fixed sequence length, and summation invariance--enabling effective set distribution modeling. Experiments demonstrate our method's superiority in semantic-aware representation and generation quality. Our innovations, spanning novel representation and modeling strategies, advance visual generation beyond traditional sequential token paradigms. Our code and models are publicly available at https://github.com/Gengzigang/TokenSet.", 'score': 10, 'issue_id': 2831, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'c960fe86078f8a7b', 'authors': ['Zigang Geng', 'Mengde Xu', 'Han Hu', 'Shuyang Gu'], 'affiliations': ['Tencent Hunyuan Research', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16425.jpg', 'data': {'categories': ['#cv', '#open_source', '#diffusion', '#video'], 'emoji': '🧩', 'ru': {'title': 'Революция в генерации изображений: от последовательностей к множествам', 'desc': 'Эта статья представляет новую парадигму генерации изображений, используя токенизацию на основе множеств и моделирование распределений. Авторы вводят представление в виде неупорядоченного набора токенов, что позволяет динамически распределять емкость кодирования в зависимости от семантической сложности регионов изображения. Для моделирования дискретных множеств предлагается механизм двойного преобразования, который биективно преобразует множества в целочисленные последовательности фиксированной длины с ограничениями на сумму. Эксперименты показывают превосходство этого метода в семантически-ориентированном представлении и качестве генерации изображений.'}, 'en': {'title': 'Revolutionizing Image Generation with Dynamic Token Sets', 'desc': 'This paper introduces a new way to generate images using a method called set-based tokenization and distribution modeling. Instead of using fixed-position codes, it uses an unordered set of tokens that can adapt based on the complexity of different image areas. This approach improves how the model understands the overall context of the image and makes it more resilient to small changes. The authors also present a new framework called Fixed-Sum Discrete Diffusion, which effectively manages discrete values and maintains certain constraints, leading to better image generation results.'}, 'zh': {'title': '图像生成的新范式：集合标记化与分布建模', 'desc': '本文提出了一种全新的图像生成范式，通过基于集合的标记化和分布建模来实现。与传统方法将图像序列化为固定位置的潜在编码不同，我们引入了无序的标记集合表示，能够根据区域语义复杂性动态分配编码能力。该TokenSet增强了全局上下文聚合能力，并提高了对局部扰动的鲁棒性。我们还提出了固定和离散扩散框架，首次同时处理离散值、固定序列长度和和不变性，从而有效地建模集合分布。'}}}, {'id': 'https://huggingface.co/papers/2503.16421', 'title': 'MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance', 'url': 'https://huggingface.co/papers/2503.16421', 'abstract': 'Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site.', 'score': 8, 'issue_id': 2823, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'ae5c7f7abb796973', 'authors': ['Quanhao Li', 'Zhen Xing', 'Rui Wang', 'Hui Zhang', 'Qi Dai', 'Zuxuan Wu'], 'affiliations': ['Fudan University', 'Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2503.16421.jpg', 'data': {'categories': ['#open_source', '#optimization', '#video', '#games', '#dataset', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'MagicMotion: точное управление движением объектов в генерируемом видео', 'desc': 'MagicMotion - новая система генерации видео из изображений с контролем траектории движения объектов. Она поддерживает три уровня управления траекторией: маски, ограничивающие рамки и разреженные рамки. Авторы также представили MagicData - крупномасштабный датасет для обучения и MagicBench - бенчмарк для оценки качества видео и точности контроля траектории. Эксперименты показывают, что MagicMotion превосходит существующие методы по различным метрикам.'}, 'en': {'title': 'MagicMotion: Mastering Object Motion in Video Generation', 'desc': 'This paper presents MagicMotion, a new framework for generating videos from images while allowing precise control over object movements along defined paths. It addresses the limitations of existing methods that struggle with complex and multi-object trajectories, which often lead to poor visual quality and object consistency. MagicMotion introduces a tiered approach to trajectory control using masks, bounding boxes, and sparse boxes, enhancing flexibility and applicability. Additionally, the authors provide a new dataset, MagicData, and a benchmark, MagicBench, to facilitate better training and evaluation of trajectory-controllable video generation.'}, 'zh': {'title': 'MagicMotion：精准轨迹控制的视频生成新框架', 'desc': '最近视频生成技术取得了显著进展，提升了视觉质量和时间一致性。针对复杂物体运动和多物体运动控制的问题，本文提出了MagicMotion框架，支持通过不同层次的条件（如掩码、边界框和稀疏框）进行轨迹控制。MagicMotion能够在保持物体一致性和视觉质量的同时，沿着定义的轨迹无缝动画化物体。此外，我们还推出了MagicData数据集和MagicBench基准，促进了轨迹控制视频生成的研究和评估。'}}}, {'id': 'https://huggingface.co/papers/2503.16375', 'title': 'NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes', 'url': 'https://huggingface.co/papers/2503.16375', 'abstract': 'In this paper, we explore the task of generating expansive outdoor scenes, ranging from castles to high-rises. Unlike indoor scene generation, which has been a primary focus of prior work, outdoor scene generation presents unique challenges, including wide variations in scene heights and the need for a method capable of rapidly producing large landscapes. To address this, we propose an efficient approach that encodes scene chunks as uniform vector sets, offering better compression and performance than the spatially structured latents used in prior methods. Furthermore, we train an explicit outpainting model for unbounded generation, which improves coherence compared to prior resampling-based inpainting schemes while also speeding up generation by eliminating extra diffusion steps. To facilitate this task, we curate NuiScene43, a small but high-quality set of scenes, preprocessed for joint training. Notably, when trained on scenes of varying styles, our model can blend different environments, such as rural houses and city skyscrapers, within the same scene, highlighting the potential of our curation process to leverage heterogeneous scenes for joint training.', 'score': 8, 'issue_id': 2829, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'bd1b53cbfef36cbc', 'authors': ['Han-Hung Lee', 'Qinghong Han', 'Angel X. Chang'], 'affiliations': ['Canada CIFAR AI Chair, Amii', 'Simon Fraser University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16375.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#synthetic', '#3d'], 'emoji': '🏙️', 'ru': {'title': 'Генерация масштабных открытых сцен: от замков до небоскребов', 'desc': 'Статья исследует генерацию обширных открытых сцен, включая замки и небоскребы. Авторы предлагают эффективный подход, кодирующий фрагменты сцены как наборы векторов, что улучшает сжатие и производительность. Разработана модель явного расширения изображения для неограниченной генерации, повышающая согласованность и скорость. Для обучения создан набор данных NuiScene43, позволяющий модели комбинировать различные стили в одной сцене.'}, 'en': {'title': 'Efficient Outdoor Scene Generation with Unified Vector Encoding', 'desc': 'This paper presents a novel approach to generating expansive outdoor scenes, addressing the unique challenges posed by varying heights and large landscapes. The authors introduce a method that encodes scene chunks as uniform vector sets, which enhances compression and performance compared to previous spatially structured latents. They also develop an explicit outpainting model that allows for unbounded scene generation, improving coherence and speeding up the process by reducing diffusion steps. Additionally, the creation of the NuiScene43 dataset enables the model to effectively blend different environmental styles, showcasing the benefits of joint training on diverse scenes.'}, 'zh': {'title': '高效生成广阔户外场景的创新方法', 'desc': '本文探讨了生成广阔户外场景的任务，包括城堡和高楼等。与以往主要关注的室内场景生成不同，户外场景生成面临独特的挑战，如场景高度的广泛变化和快速生成大规模景观的方法需求。为了解决这些问题，我们提出了一种高效的方法，将场景块编码为统一的向量集，这比以往方法中使用的空间结构潜变量提供了更好的压缩和性能。此外，我们训练了一个显式的外扩模型，以实现无界生成，改善了与以往基于重采样的修补方案相比的一致性，同时通过消除额外的扩散步骤加快了生成速度。'}}}, {'id': 'https://huggingface.co/papers/2503.16055', 'title': 'SALT: Singular Value Adaptation with Low-Rank Transformation', 'url': 'https://huggingface.co/papers/2503.16055', 'abstract': 'The complex nature of medical image segmentation calls for models that are specifically designed to capture detailed, domain-specific features. Large foundation models offer considerable flexibility, yet the cost of fine-tuning these models remains a significant barrier. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model weights with low-rank matrices but may suffer from underfitting when the chosen rank is insufficient to capture domain-specific nuances. Conversely, full-rank Singular Value Decomposition (SVD) based methods provide comprehensive updates by modifying all singular values, yet they often lack flexibility and exhibit variable performance across datasets. We propose SALT (Singular Value Adaptation with Low-Rank Transformation), a method that selectively adapts the most influential singular values using trainable scale and shift parameters while complementing this with a low-rank update for the remaining subspace. This hybrid approach harnesses the advantages of both LoRA and SVD, enabling effective adaptation without relying on increasing model size or depth. Evaluated on 5 challenging medical datasets, ranging from as few as 20 samples to 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in Dice with only 3.9% trainable parameters, demonstrating robust adaptation even in low-resource settings. The code for SALT is available at: https://github.com/BioMedIA-MBZUAI/SALT', 'score': 8, 'issue_id': 2827, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '1b4a367b51eed08e', 'authors': ['Abdelrahman Elsayed', 'Sarim Hashmi', 'Mohammed Elseiagy', 'Hu Wang', 'Mohammad Yaqub', 'Ibrahim Almakky'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE'], 'pdf_title_img': 'assets/pdf/title_img/2503.16055.jpg', 'data': {'categories': ['#open_source', '#low_resource', '#healthcare', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'SALT: Эффективная настройка моделей для сегментации медицинских изображений', 'desc': 'Статья представляет новый метод SALT для эффективной настройки моделей машинного обучения в задачах сегментации медицинских изображений. SALT сочетает преимущества методов LoRA и SVD, адаптируя наиболее влиятельные сингулярные значения и применяя низкоранговое обновление для оставшегося подпространства. Метод показал превосходство над современными подходами параметрически-эффективной тонкой настройки на 5 сложных медицинских наборах данных. SALT продемонстрировал улучшение показателя Dice на 2-5% при использовании всего 3.9% обучаемых параметров.'}, 'en': {'title': 'SALT: Smart Adaptation for Medical Image Segmentation', 'desc': 'This paper introduces SALT, a novel method for fine-tuning large foundation models specifically for medical image segmentation. SALT combines the strengths of Low-Rank Adaptation (LoRA) and full-rank Singular Value Decomposition (SVD) by selectively adapting the most important singular values while applying low-rank updates to the rest. This approach allows for effective model adaptation without significantly increasing the number of trainable parameters. The results show that SALT outperforms existing Parameter-Efficient Fine-Tuning methods on various medical datasets, demonstrating its effectiveness even with limited training data.'}, 'zh': {'title': 'SALT：高效医学图像分割的新方法', 'desc': '医学图像分割的复杂性需要专门设计的模型来捕捉详细的领域特征。虽然大型基础模型提供了灵活性，但微调这些模型的成本仍然是一个重要障碍。我们提出了一种名为SALT的方法，它通过可训练的缩放和偏移参数选择性地适应最具影响力的奇异值，同时对其余子空间进行低秩更新。这种混合方法结合了LoRA和SVD的优点，在低资源环境中也能实现有效的适应。'}}}, {'id': 'https://huggingface.co/papers/2503.15851', 'title': 'Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video\n  Diffusion', 'url': 'https://huggingface.co/papers/2503.15851', 'abstract': 'Animatable head avatar generation typically requires extensive data for training. To reduce the data requirements, a natural solution is to leverage existing data-free static avatar generation methods, such as pre-trained diffusion models with score distillation sampling (SDS), which align avatars with pseudo ground-truth outputs from the diffusion model. However, directly distilling 4D avatars from video diffusion often leads to over-smooth results due to spatial and temporal inconsistencies in the generated video. To address this issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial and temporal consistency dataset for 4D avatar reconstruction using the video diffusion model. Specifically, Zero-1-to-A iteratively constructs video datasets and optimizes animatable avatars in a progressive manner, ensuring that avatar quality increases smoothly and consistently throughout the learning process. This progressive learning involves two stages: (1) Spatial Consistency Learning fixes expressions and learns from front-to-side views, and (2) Temporal Consistency Learning fixes views and learns from relaxed to exaggerated expressions, generating 4D avatars in a simple-to-complex manner. Extensive experiments demonstrate that Zero-1-to-A improves fidelity, animation quality, and rendering speed compared to existing diffusion-based methods, providing a solution for lifelike avatar creation. Code is publicly available at: https://github.com/ZhenglinZhou/Zero-1-to-A.', 'score': 8, 'issue_id': 2826, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '470193a07d663d04', 'authors': ['Zhou Zhenglin', 'Ma Fan', 'Fan Hehe', 'Chua Tat-Seng'], 'affiliations': ['National University of Singapore', 'ReLER, CCAI, Zhejiang University', 'State Key Laboratory of Brain-machine Intelligence, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15851.jpg', 'data': {'categories': ['#data', '#diffusion', '#dataset', '#synthetic', '#video', '#open_source'], 'emoji': '🎭', 'ru': {'title': 'Zero-1-to-A: Создание реалистичных 3D-аватаров без реальных данных', 'desc': 'Статья представляет метод Zero-1-to-A для генерации анимируемых 3D-аватаров без использования реальных данных. Авторы предлагают использовать предобученные диффузионные модели для создания псевдо-истинных данных, но с учетом пространственной и временной согласованности. Метод включает два этапа: обучение пространственной согласованности (фиксированные выражения, разные ракурсы) и обучение временной согласованности (фиксированные ракурсы, разные выражения). Эксперименты показывают, что Zero-1-to-A улучшает качество аватаров и скорость рендеринга по сравнению с существующими методами на основе диффузии.'}, 'en': {'title': 'Revolutionizing 4D Avatar Generation with Zero-1-to-A', 'desc': 'This paper presents Zero-1-to-A, a novel approach for generating animatable 4D avatars using video diffusion models while minimizing data requirements. The method addresses the challenge of over-smoothing in avatar generation by creating a dataset that ensures both spatial and temporal consistency. It employs a two-stage progressive learning process: first, it focuses on spatial consistency by learning from different views, and then it enhances temporal consistency by varying expressions. The results show that Zero-1-to-A significantly improves the quality and speed of avatar rendering compared to traditional methods.'}, 'zh': {'title': '高效生成真实感4D头像的创新方法', 'desc': '本论文提出了一种名为Zero-1-to-A的方法，用于生成可动画的4D头像，旨在减少对大量训练数据的需求。该方法利用视频扩散模型，通过构建空间和时间一致性的数据集，逐步优化头像的质量。Zero-1-to-A的学习过程分为两个阶段：空间一致性学习和时间一致性学习，确保头像在学习过程中质量平滑提升。实验结果表明，该方法在头像的真实感、动画质量和渲染速度上优于现有的扩散模型方法。'}}}, {'id': 'https://huggingface.co/papers/2503.15242', 'title': 'BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?', 'url': 'https://huggingface.co/papers/2503.15242', 'abstract': 'We introduce BigO(Bench), a novel coding benchmark designed to evaluate the capabilities of generative language models in understanding and generating code with specified time and space complexities. This benchmark addresses the gap in current evaluations that often overlook the ability of models to comprehend and produce code constrained by computational complexity. BigO(Bench) includes tooling to infer the algorithmic complexity of any Python function from profiling measurements, including human- or LLM-generated solutions. BigO(Bench) also includes of set of 3,105 coding problems and 1,190,250 solutions from Code Contests annotated with inferred (synthetic) time and space complexity labels from the complexity framework, as well as corresponding runtime and memory footprint values for a large set of input sizes. We present results from evaluating multiple state-of-the-art language models on this benchmark, highlighting their strengths and weaknesses in handling complexity requirements. In particular, token-space reasoning models are unrivaled in code generation but not in complexity understanding, hinting that they may not generalize well to tasks for which no reward was given at training time.', 'score': 7, 'issue_id': 2831, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': 'bac5c66c1f05b167', 'authors': ['Pierre Chambon', 'Baptiste Roziere', 'Benoit Sagot', 'Gabriel Synnaeve'], 'affiliations': ['FAIR at Meta', 'Inria', 'Mistral AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.15242.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#dataset', '#plp', '#reasoning'], 'emoji': '⏱️', 'ru': {'title': 'BigO(Bench): Оценка понимания алгоритмической сложности языковыми моделями', 'desc': 'BigO(Bench) - это новый benchmark для оценки способности генеративных языковых моделей понимать и создавать код с заданной временной и пространственной сложностью. Он включает инструменты для определения алгоритмической сложности Python-функций на основе профилирования, а также набор из 3,105 задач по программированию с 1,190,250 решениями, аннотированными метками сложности. Результаты оценки современных языковых моделей на этом бенчмарке показывают их сильные и слабые стороны в работе с требованиями к сложности. Модели, рассуждающие в пространстве токенов, лидируют в генерации кода, но не в понимании сложности.'}, 'en': {'title': 'Evaluating Code Complexity with BigO(Bench)', 'desc': "BigO(Bench) is a new benchmark created to test how well generative language models can understand and generate code while considering time and space complexities. It fills a gap in existing evaluations by focusing on the models' ability to handle computational constraints. The benchmark includes tools to analyze the algorithmic complexity of Python functions and features a large dataset of coding problems and solutions with annotated complexity labels. Results show that while some models excel at generating code, they struggle with understanding complexity, indicating potential limitations in their generalization capabilities."}, 'zh': {'title': '评估生成模型的复杂度理解能力', 'desc': '我们介绍了BigO(Bench)，这是一个新颖的编码基准，旨在评估生成语言模型在理解和生成具有特定时间和空间复杂度的代码方面的能力。该基准填补了当前评估中常常忽视模型在计算复杂度限制下理解和生成代码的能力的空白。BigO(Bench)包括工具，可以从性能测量中推断任何Python函数的算法复杂度，并包含来自代码竞赛的3,105个编码问题和1,190,250个解决方案，标注了推断的时间和空间复杂度标签。我们展示了对多个最先进语言模型在该基准上的评估结果，突出了它们在处理复杂性要求方面的优缺点。'}}}, {'id': 'https://huggingface.co/papers/2503.13356', 'title': 'Agents Play Thousands of 3D Video Games', 'url': 'https://huggingface.co/papers/2503.13356', 'abstract': "We present PORTAL, a novel framework for developing artificial intelligence agents capable of playing thousands of 3D video games through language-guided policy generation. By transforming decision-making problems into language modeling tasks, our approach leverages large language models (LLMs) to generate behavior trees represented in domain-specific language (DSL). This method eliminates the computational burden associated with traditional reinforcement learning approaches while preserving strategic depth and rapid adaptability. Our framework introduces a hybrid policy structure that combines rule-based nodes with neural network components, enabling both high-level strategic reasoning and precise low-level control. A dual-feedback mechanism incorporating quantitative game metrics and vision-language model analysis facilitates iterative policy improvement at both tactical and strategic levels. The resulting policies are instantaneously deployable, human-interpretable, and capable of generalizing across diverse gaming environments. Experimental results demonstrate PORTAL's effectiveness across thousands of first-person shooter (FPS) games, showcasing significant improvements in development efficiency, policy generalization, and behavior diversity compared to traditional approaches. PORTAL represents a significant advancement in game AI development, offering a practical solution for creating sophisticated agents that can operate across thousands of commercial video games with minimal development overhead. Experiment results on the 3D video games are best viewed on https://zhongwen.one/projects/portal .", 'score': 7, 'issue_id': 2835, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '950044773aa39963', 'authors': ['Zhongwen Xu', 'Xianliang Wang', 'Siyi Li', 'Tao Yu', 'Liang Wang', 'Qiang Fu', 'Wei Yang'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2503.13356.jpg', 'data': {'categories': ['#rl', '#interpretability', '#video', '#3d', '#games', '#reasoning', '#agi', '#agents'], 'emoji': '🎮', 'ru': {'title': 'Языковые модели покоряют мир 3D-игр', 'desc': 'PORTAL - это новая система для создания ИИ-агентов, способных играть в тысячи 3D-видеоигр с помощью генерации политик на основе языка. Она использует большие языковые модели для создания деревьев поведения, представленных на предметно-ориентированном языке. Система вводит гибридную структуру политики, сочетающую узлы на основе правил с компонентами нейронных сетей. PORTAL демонстрирует значительные улучшения в эффективности разработки, обобщении политик и разнообразии поведения по сравнению с традиционными подходами.'}, 'en': {'title': 'PORTAL: Revolutionizing Game AI with Language-Guided Policies', 'desc': 'PORTAL is a new framework that enables AI agents to play many 3D video games by using language to guide their decision-making. It turns complex decision problems into tasks that large language models can handle, generating behavior trees in a specific language. This approach reduces the heavy computational needs of traditional reinforcement learning while maintaining strategic depth and adaptability. By combining rule-based and neural network elements, PORTAL allows for both high-level strategy and detailed control, making it easier to create versatile game-playing agents.'}, 'zh': {'title': 'PORTAL：游戏AI的新纪元', 'desc': 'PORTAL是一个新颖的框架，旨在开发能够通过语言引导策略生成来玩数千款3D视频游戏的人工智能代理。该方法将决策问题转化为语言建模任务，利用大型语言模型（LLMs）生成以领域特定语言（DSL）表示的行为树。通过结合基于规则的节点和神经网络组件，PORTAL实现了高层次的战略推理和精确的低层次控制。实验结果表明，PORTAL在数千款第一人称射击游戏中表现出显著的开发效率提升、策略泛化和行为多样性。'}}}, {'id': 'https://huggingface.co/papers/2503.16188', 'title': 'CLS-RL: Image Classification with Rule-Based Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.16188', 'abstract': "Classification is a core task in machine learning. Recent research has shown that although Multimodal Large Language Models (MLLMs) are initially poor at image classification, fine-tuning them with an adequate amount of data can significantly enhance their performance, making them comparable to SOTA classification models. However, acquiring large-scale labeled data is expensive. In this paper, we explore few-shot MLLM classification fine-tuning. We found that SFT can cause severe overfitting issues and may even degrade performance over the zero-shot approach. To address this challenge, inspired by the recent successes in rule-based reinforcement learning, we propose CLS-RL, which uses verifiable signals as reward to fine-tune MLLMs. We discovered that CLS-RL outperforms SFT in most datasets and has a much higher average accuracy on both base-to-new and few-shot learning setting. Moreover, we observed a free-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular dataset, their performance on other distinct datasets may also improve over zero-shot models, even if those datasets differ in distribution and class names. This suggests that RL-based methods effectively teach models the fundamentals of classification. Lastly, inspired by recent works in inference time thinking, we re-examine the `thinking process' during fine-tuning, a critical aspect of RL-based methods, in the context of visual classification. We question whether such tasks require extensive thinking process during fine-tuning, proposing that this may actually detract from performance. Based on this premise, we introduce the No-Thinking-CLS-RL method, which minimizes thinking processes during training by setting an equality accuracy reward. Our findings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL method achieves superior in-domain performance and generalization capabilities than CLS-RL.", 'score': 6, 'issue_id': 2826, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '904362034cdf6d16', 'authors': ['Ming Li', 'Shitian Zhao', 'Jike Zhong', 'Yuxiang Lai', 'Kaipeng Zhang'], 'affiliations': ['Emory University', 'Shanghai AI Laboratory', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2503.16188.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#optimization', '#cv', '#multimodal', '#training'], 'emoji': '🤖', 'ru': {'title': 'Обучение с подкреплением открывает новые горизонты в классификации изображений для мультимодальных ИИ', 'desc': "Статья исследует применение обучения с подкреплением для улучшения классификации изображений мультимодальными большими языковыми моделями (MLLM) в условиях малого количества обучающих данных. Авторы предлагают метод CLS-RL, использующий проверяемые сигналы в качестве награды для дообучения MLLM. Результаты показывают, что CLS-RL превосходит стандартное дообучение и демонстрирует улучшение точности на различных наборах данных. Исследователи также вводят метод No-Thinking-CLS-RL, минимизирующий 'процесс мышления' во время обучения, что приводит к лучшим результатам при меньшем времени дообучения."}, 'en': {'title': 'Reinforcement Learning Revolutionizes Few-Shot Image Classification', 'desc': 'This paper investigates the fine-tuning of Multimodal Large Language Models (MLLMs) for image classification, particularly in few-shot scenarios. It highlights the limitations of standard supervised fine-tuning (SFT), which can lead to overfitting and reduced performance compared to zero-shot methods. To overcome these challenges, the authors propose a novel approach called CLS-RL, which utilizes reinforcement learning with verifiable signals as rewards, resulting in improved accuracy across various datasets. Additionally, they introduce the No-Thinking-CLS-RL method, which minimizes unnecessary cognitive processes during training, further enhancing performance and generalization with less fine-tuning time.'}, 'zh': {'title': '少样本微调，提升分类性能的创新方法', 'desc': '本文探讨了多模态大型语言模型（MLLMs）在图像分类任务中的应用。尽管这些模型在初始阶段的分类性能较差，但通过少量样本的微调，可以显著提升其表现。我们提出了一种新的方法CLS-RL，利用可验证信号作为奖励来微调MLLMs，结果显示其在大多数数据集上优于传统的微调方法。最后，我们还提出了No-Thinking-CLS-RL方法，通过减少微调过程中的思考时间，进一步提高了模型的性能和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.15567', 'title': 'Towards Unified Latent Space for 3D Molecular Latent Diffusion Modeling', 'url': 'https://huggingface.co/papers/2503.15567', 'abstract': '3D molecule generation is crucial for drug discovery and material science, requiring models to process complex multi-modalities, including atom types, chemical bonds, and 3D coordinates. A key challenge is integrating these modalities of different shapes while maintaining SE(3) equivariance for 3D coordinates. To achieve this, existing approaches typically maintain separate latent spaces for invariant and equivariant modalities, reducing efficiency in both training and sampling. In this work, we propose Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D), a multi-modal VAE that compresses 3D molecules into latent sequences from a unified latent space, while maintaining near-zero reconstruction error. This unified latent space eliminates the complexities of handling multi-modality and equivariance when performing latent diffusion modeling. We demonstrate this by employing the Diffusion Transformer--a general-purpose diffusion model without any molecular inductive bias--for latent generation. Extensive experiments on GEOM-Drugs and QM9 datasets demonstrate that our method significantly establishes new benchmarks in both de novo and conditional 3D molecule generation, achieving leading efficiency and quality.', 'score': 6, 'issue_id': 2822, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': 'd1787a1a75f723a2', 'authors': ['Yanchen Luo', 'Zhiyuan Liu', 'Yi Zhao', 'Sihang Li', 'Kenji Kawaguchi', 'Tat-Seng Chua', 'Xiang Wang'], 'affiliations': ['National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.15567.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#benchmark', '#3d', '#optimization', '#dataset'], 'emoji': '🧪', 'ru': {'title': 'Единое латентное пространство для эффективной генерации 3D молекул', 'desc': 'Статья представляет новый подход к генерации трехмерных молекул для разработки лекарств и материаловедения. Авторы предлагают унифицированный вариационный автоэнкодер (UAE-3D) для сжатия 3D молекул в латентные последовательности из единого латентного пространства. Этот метод устраняет сложности обработки мультимодальности и эквивариантности при моделировании латентной диффузии. Эксперименты на наборах данных GEOM-Drugs и QM9 показывают, что предложенный метод значительно улучшает результаты в задачах de novo и условной генерации 3D молекул.'}, 'en': {'title': 'Unified Latent Space for Efficient 3D Molecule Generation', 'desc': 'This paper presents a new approach called Unified Variational Auto-Encoder for 3D Molecular Latent Diffusion Modeling (UAE-3D) aimed at generating 3D molecules for drug discovery and material science. The method addresses the challenge of integrating different types of data, such as atom types and chemical bonds, while ensuring that the 3D coordinates maintain SE(3) equivariance. By using a unified latent space, UAE-3D simplifies the process of handling multi-modalities and improves the efficiency of training and sampling. The results show that this approach outperforms existing methods in generating high-quality 3D molecules, setting new benchmarks in the field.'}, 'zh': {'title': '统一潜在空间，提升3D分子生成效率', 'desc': '3D分子生成对药物发现和材料科学至关重要，需要模型处理复杂的多模态信息，包括原子类型、化学键和3D坐标。一个主要挑战是如何在保持SE(3)等变性的同时，整合不同形状的模态。我们提出了一种统一的变分自编码器（UAE-3D），它将3D分子压缩为来自统一潜在空间的潜在序列，同时保持近乎零的重建误差。通过使用Diffusion Transformer，我们的方法在GEOM-Drugs和QM9数据集上显著提高了新分子生成的效率和质量，建立了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2503.16429', 'title': 'Sonata: Self-Supervised Learning of Reliable Point Representations', 'url': 'https://huggingface.co/papers/2503.16429', 'abstract': 'In this paper, we question whether we have a reliable self-supervised point cloud model that can be used for diverse 3D tasks via simple linear probing, even with limited data and minimal computation. We find that existing 3D self-supervised learning approaches fall short when evaluated on representation quality through linear probing. We hypothesize that this is due to what we term the "geometric shortcut", which causes representations to collapse to low-level spatial features. This challenge is unique to 3D and arises from the sparse nature of point cloud data. We address it through two key strategies: obscuring spatial information and enhancing the reliance on input features, ultimately composing a Sonata of 140k point clouds through self-distillation. Sonata is simple and intuitive, yet its learned representations are strong and reliable: zero-shot visualizations demonstrate semantic grouping, alongside strong spatial reasoning through nearest-neighbor relationships. Sonata demonstrates exceptional parameter and data efficiency, tripling linear probing accuracy (from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1% of the data compared to previous approaches. Full fine-tuning further advances SOTA across both 3D indoor and outdoor perception tasks.', 'score': 5, 'issue_id': 2833, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': 'cab3efeaf1688094', 'authors': ['Xiaoyang Wu', 'Daniel DeTone', 'Duncan Frost', 'Tianwei Shen', 'Chris Xie', 'Nan Yang', 'Jakob Engel', 'Richard Newcombe', 'Hengshuang Zhao', 'Julian Straub'], 'affiliations': ['Meta Reality Labs Research', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.16429.jpg', 'data': {'categories': ['#3d', '#optimization', '#transfer_learning'], 'emoji': '🌐', 'ru': {'title': 'Sonata: новая эра самообучения для 3D облаков точек', 'desc': 'В статье рассматривается проблема создания надежной модели для работы с облаками точек в 3D, которая может использоваться для различных задач с минимальными вычислениями. Авторы обнаружили, что существующие подходы к самообучению в 3D не обеспечивают качественных представлений из-за так называемого "геометрического сокращения". Это явление связано с тем, что представления сводятся к низкоуровневым пространственным признакам из-за разреженности данных облаков точек. Для решения этой проблемы предложена модель Sonata, которая улучшает качество представлений и значительно повышает точность линейного зондирования.'}, 'en': {'title': "Unlocking 3D Potential: Sonata's Self-Supervised Breakthrough", 'desc': "This paper investigates the effectiveness of self-supervised learning models for point clouds in 3D tasks, particularly focusing on their performance with limited data. The authors identify a problem called the 'geometric shortcut', which leads to poor representation quality due to reliance on low-level spatial features. To overcome this, they propose a method called Sonata, which obscures spatial information and emphasizes input features, resulting in improved representation learning. Sonata achieves significant gains in linear probing accuracy and demonstrates strong performance in both zero-shot visualizations and full fine-tuning across various 3D perception tasks."}, 'zh': {'title': 'Sonata：高效的自监督3D点云学习', 'desc': '本文质疑现有的自监督点云模型在有限数据和最小计算下是否可靠，能否用于多样的3D任务。我们发现现有的3D自监督学习方法在通过线性探测评估表示质量时表现不佳，原因在于所谓的“几何捷径”，导致表示退化为低级空间特征。我们提出了两种关键策略来应对这一挑战：遮蔽空间信息和增强对输入特征的依赖，最终通过自蒸馏构建了一个包含14万个点云的Sonata模型。Sonata在参数和数据效率上表现出色，在线性探测准确率上提升了三倍，并在全微调中进一步推动了3D感知任务的最新进展。'}}}, {'id': 'https://huggingface.co/papers/2503.16278', 'title': 'Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens', 'url': 'https://huggingface.co/papers/2503.16278', 'abstract': 'Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\\% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at https://github.com/dptech-corp/Uni-3DAR.', 'score': 5, 'issue_id': 2822, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '7012b17f03aeb180', 'authors': ['Shuqi Lu', 'Haowei Lin', 'Lin Yao', 'Zhifeng Gao', 'Xiaohong Ji', 'Weinan E', 'Linfeng Zhang', 'Guolin Ke'], 'affiliations': ['AI for Science Institute, Beijing 100080, China', 'Center for Machine Learning Research, Peking University, Beijing 100084, China', 'DP Technology, Beijing, 100080, China', 'Institute for Artificial Intelligence, Peking University, Beijing 100871, China', 'School of Mathematical Sciences, Peking University, Beijing, 100871, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.16278.jpg', 'data': {'categories': ['#multimodal', '#science', '#training', '#architecture', '#3d', '#optimization', '#inference'], 'emoji': '🧬', 'ru': {'title': 'Uni-3DAR: Единая авторегрессионная модель для 3D генерации и понимания', 'desc': 'Статья представляет Uni-3DAR - унифицированную систему для задач 3D генерации и понимания на основе авторегрессионного предсказания. Ключевой особенностью является иерархическая токенизация с использованием октодерева для сжатия 3D пространства. Предложены оптимизации: двухуровневое сжатие поддеревьев и механизм маскированного предсказания следующего токена. Эксперименты показывают значительное превосходство Uni-3DAR над существующими диффузионными моделями по точности и скорости для различных микроскопических 3D задач.'}, 'en': {'title': 'Unifying 3D Generation and Understanding with Uni-3DAR', 'desc': 'This paper presents Uni-3DAR, a novel framework that integrates 3D structural generation and understanding tasks using autoregressive next-token prediction. It introduces a hierarchical tokenization method that efficiently compresses 3D space with an octree, capturing both the overall structure and fine details like atom types and spatial coordinates. The framework includes optimizations such as a two-level subtree compression strategy and a masked next-token prediction mechanism, enhancing both efficiency and model performance. Experimental results show that Uni-3DAR significantly outperforms existing models, achieving faster inference speeds and improved accuracy across various microscopic 3D tasks.'}, 'zh': {'title': '统一3D结构生成与理解的创新框架', 'desc': '本文介绍了一种名为Uni-3DAR的统一框架，旨在通过自回归预测整合3D结构生成与理解（3D GU）任务。该框架采用新颖的分层标记化方法，利用八叉树压缩3D空间，并为微观3D结构捕捉关键属性如原子类型和精确坐标。Uni-3DAR还提出了两项优化策略，以提高效率和效果，包括两级子树压缩和动态变化标记位置的掩码下一个标记预测机制。通过广泛的实验，Uni-3DAR在多种微观3D GU任务中表现出色，显著超越了之前的扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2503.15451', 'title': 'MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space', 'url': 'https://huggingface.co/papers/2503.15451', 'abstract': 'This paper addresses the challenge of text-conditioned streaming motion generation, which requires us to predict the next-step human pose based on variable-length historical motions and incoming texts. Existing methods struggle to achieve streaming motion generation, e.g., diffusion models are constrained by pre-defined motion lengths, while GPT-based methods suffer from delayed response and error accumulation problem due to discretized non-causal tokenization. To solve these problems, we propose MotionStreamer, a novel framework that incorporates a continuous causal latent space into a probabilistic autoregressive model. The continuous latents mitigate information loss caused by discretization and effectively reduce error accumulation during long-term autoregressive generation. In addition, by establishing temporal causal dependencies between current and historical motion latents, our model fully utilizes the available information to achieve accurate online motion decoding. Experiments show that our method outperforms existing approaches while offering more applications, including multi-round generation, long-term generation, and dynamic motion composition. Project Page: https://zju3dv.github.io/MotionStreamer/', 'score': 5, 'issue_id': 2827, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '014f7568fd194ca1', 'authors': ['Lixing Xiao', 'Shunlin Lu', 'Huaijin Pi', 'Ke Fan', 'Liang Pan', 'Yueer Zhou', 'Ziyong Feng', 'Xiaowei Zhou', 'Sida Peng', 'Jingbo Wang'], 'affiliations': ['DeepGlint', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15451.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#games', '#video', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'Плавная генерация движений по тексту в реальном времени', 'desc': 'Статья представляет MotionStreamer - новый фреймворк для генерации движений человека на основе текстовых описаний в режиме реального времени. В отличие от существующих методов, MotionStreamer использует непрерывное причинно-следственное латентное пространство в вероятностной авторегрессионной модели. Это позволяет снизить потерю информации из-за дискретизации и уменьшить накопление ошибок при долгосрочной генерации. Модель устанавливает временные причинно-следственные зависимости между текущими и историческими латентными представлениями движений для точного онлайн-декодирования.'}, 'en': {'title': 'Real-Time Motion Generation with Text Input', 'desc': 'This paper introduces MotionStreamer, a new framework for generating human motion in real-time based on text inputs. It addresses limitations of previous methods, such as diffusion models and GPT-based approaches, which struggle with fixed motion lengths and error accumulation. By using a continuous causal latent space, MotionStreamer reduces information loss and improves the accuracy of long-term motion generation. The framework also leverages temporal dependencies to enhance online motion decoding, demonstrating superior performance in various applications.'}, 'zh': {'title': '流式运动生成的新突破：MotionStreamer', 'desc': '本文解决了基于文本的流式运动生成问题，旨在根据可变长度的历史动作和输入文本预测下一步的人体姿态。现有方法在流式运动生成方面存在困难，例如扩散模型受到预定义运动长度的限制，而基于GPT的方法由于离散化的非因果标记化而面临响应延迟和错误累积的问题。为了解决这些问题，我们提出了MotionStreamer，这是一种新颖的框架，将连续因果潜在空间融入概率自回归模型中。通过建立当前和历史运动潜在之间的时间因果依赖关系，我们的模型充分利用可用信息，实现准确的在线运动解码。'}}}, {'id': 'https://huggingface.co/papers/2503.14237', 'title': 'Make Your Training Flexible: Towards Deployment-Efficient Video Models', 'url': 'https://huggingface.co/papers/2503.14237', 'abstract': 'Popular video training methods mainly operate on a fixed number of tokens sampled from a predetermined spatiotemporal grid, resulting in sub-optimal accuracy-computation trade-offs due to inherent video redundancy. They also lack adaptability to varying computational budgets for downstream tasks, hindering applications of the most competitive model in real-world scenes. We thus propose a new test setting, Token Optimization, for maximized input information across budgets, which optimizes the size-limited set of input tokens through token selection from more suitably sampled videos. To this end, we propose a novel augmentation tool termed Flux. By making the sampling grid flexible and leveraging token selection, it is easily adopted in most popular video training frameworks, boosting model robustness with nearly no additional cost. We integrate Flux in large-scale video pre-training, and the resulting FluxViT establishes new state-of-the-art results across extensive tasks at standard costs. Notably, with 1/4 tokens only, it can still match the performance of previous state-of-the-art models with Token Optimization, yielding nearly 90\\% savings. All models and data are available at https://github.com/OpenGVLab/FluxViT.', 'score': 5, 'issue_id': 2829, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'bae6f023e68eb914', 'authors': ['Chenting Wang', 'Kunchang Li', 'Tianxiang Jiang', 'Xiangyu Zeng', 'Yi Wang', 'Limin Wang'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'State Key Laboratory for Novel Software Technology, Nanjing University', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14237.jpg', 'data': {'categories': ['#benchmark', '#video', '#open_source', '#training', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Flux: Эффективное обучение видеомоделей с оптимизацией токенов', 'desc': 'Статья представляет новый метод обучения видеомоделей под названием Flux. Он оптимизирует выборку токенов из видео, делая ее более гибкой и эффективной. Flux можно легко интегрировать в существующие архитектуры для предобучения видеомоделей. Результирующая модель FluxViT достигает нового уровня производительности на различных задачах, используя всего 1/4 токенов по сравнению с предыдущими подходами.'}, 'en': {'title': 'Optimizing Video Training with Token Selection for Efficiency', 'desc': 'This paper introduces a new approach to video training methods that addresses the inefficiencies of fixed token sampling from a spatiotemporal grid. The authors propose a test setting called Token Optimization, which allows for better input information management by selecting tokens from more appropriately sampled videos. They introduce a tool named Flux that enhances the flexibility of the sampling grid and can be integrated into existing video training frameworks with minimal cost. The results show that their method, FluxViT, achieves state-of-the-art performance while significantly reducing the number of tokens needed, demonstrating substantial computational savings.'}, 'zh': {'title': '优化视频训练，提升模型效率', 'desc': '本论文提出了一种新的视频训练方法，旨在优化输入信息的使用。通过引入一种名为Flux的增强工具，研究者们实现了灵活的采样网格和有效的token选择，从而提高了模型的鲁棒性。该方法在大规模视频预训练中应用，取得了新的最先进结果，且在使用仅1/4的token时，仍能与之前的最佳模型相媲美。此研究为视频处理任务提供了更高效的计算预算适应性，显著降低了资源消耗。'}}}, {'id': 'https://huggingface.co/papers/2503.13834', 'title': 'See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language\n  Balance to Mitigate Dominant Modality Bias', 'url': 'https://huggingface.co/papers/2503.13834', 'abstract': 'Vision-language (VL) models have demonstrated strong performance across various tasks. However, these models often rely on a specific modality for predictions, leading to "dominant modality bias.\'\' This bias significantly hurts performance, especially when one modality is impaired. In this study, we analyze model behavior under dominant modality bias and theoretically show that unaligned gradients or differences in gradient magnitudes prevent balanced convergence of the loss. Based on these findings, we propose a novel framework, BalGrad to mitigate dominant modality bias. Our approach includes inter-modality gradient reweighting, adjusting the gradient of KL divergence based on each modality\'s contribution, and inter-task gradient projection to align task directions in a non-conflicting manner. Experiments on UPMC Food-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively alleviates over-reliance on specific modalities when making predictions.', 'score': 4, 'issue_id': 2826, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'd17ffa2ed0a9ff25', 'authors': ['JuneHyoung Kwon', 'MiHyeon Kim', 'Eunju Lee', 'Juhwan Choi', 'YoungBin Kim'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University', 'Graduate School of Advanced Imaging Sciences, Multimedia and Film, Chung-Ang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13834.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#optimization', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Балансировка модальностей для улучшения мультимодальных моделей', 'desc': 'Исследование посвящено проблеме доминирующей модальности в мультимодальных моделях машинного обучения. Авторы анализируют поведение моделей и теоретически показывают, что несогласованные градиенты препятствуют сбалансированной сходимости функции потерь. Для решения этой проблемы предлагается новый фреймворк BalGrad, включающий перевзвешивание градиентов между модальностями и проекцию градиентов между задачами. Эксперименты на нескольких наборах данных подтверждают эффективность BalGrad в снижении чрезмерной зависимости от конкретных модальностей при прогнозировании.'}, 'en': {'title': 'Balancing Modalities for Better Predictions', 'desc': "This paper addresses the issue of dominant modality bias in vision-language (VL) models, which can negatively impact their performance when one modality is compromised. The authors analyze how unaligned gradients and varying gradient magnitudes hinder the model's ability to converge effectively. To counter this bias, they introduce a new framework called BalGrad, which employs techniques like inter-modality gradient reweighting and inter-task gradient projection. Experimental results on multiple datasets demonstrate that BalGrad successfully reduces the dependency on any single modality, leading to improved prediction accuracy."}, 'zh': {'title': '平衡模态偏差，提升模型性能', 'desc': '视觉语言（VL）模型在多种任务中表现出色，但它们通常依赖于特定的模态进行预测，这导致了“主导模态偏差”。这种偏差会显著影响模型性能，尤其是在某一模态受损时。我们分析了主导模态偏差下模型的行为，并理论上证明了未对齐的梯度或梯度幅度差异会阻碍损失的平衡收敛。基于这些发现，我们提出了一种新框架BalGrad，通过模态间梯度重加权和任务间梯度投影来减轻主导模态偏差。'}}}, {'id': 'https://huggingface.co/papers/2503.12689', 'title': 'MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization', 'url': 'https://huggingface.co/papers/2503.12689', 'abstract': "Video identity customization seeks to produce high-fidelity videos that maintain consistent identity and exhibit significant dynamics based on users' reference images. However, existing approaches face two key challenges: identity degradation over extended video length and reduced dynamics during training, primarily due to their reliance on traditional self-reconstruction training with static images. To address these issues, we introduce MagicID, a novel framework designed to directly promote the generation of identity-consistent and dynamically rich videos tailored to user preferences. Specifically, we propose constructing pairwise preference video data with explicit identity and dynamic rewards for preference learning, instead of sticking to the traditional self-reconstruction. To address the constraints of customized preference data, we introduce a hybrid sampling strategy. This approach first prioritizes identity preservation by leveraging static videos derived from reference images, then enhances dynamic motion quality in the generated videos using a Frontier-based sampling method. By utilizing these hybrid preference pairs, we optimize the model to align with the reward differences between pairs of customized preferences. Extensive experiments show that MagicID successfully achieves consistent identity and natural dynamics, surpassing existing methods across various metrics.", 'score': 4, 'issue_id': 2824, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '2285de85beb9dbfb', 'authors': ['Hengjia Li', 'Lifan Jiang', 'Xi Xiao', 'Tianyang Wang', 'Hongwei Yi', 'Boxi Wu', 'Deng Cai'], 'affiliations': ['Hedra AI', 'University of Alabama at Birmingham', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12689.jpg', 'data': {'categories': ['#video', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'MagicID: Персонализированные видео с сохранением идентичности и естественной динамикой', 'desc': 'MagicID - это новая система для создания видео с сохранением идентичности и динамики на основе пользовательских изображений. Она решает проблемы деградации идентичности и снижения динамики, характерные для существующих подходов. MagicID использует попарное обучение с предпочтениями вместо традиционной самореконструкции. Система применяет гибридную стратегию сэмплирования для сохранения идентичности и улучшения динамики движений.'}, 'en': {'title': 'MagicID: Dynamic Video Identity Customization Made Easy', 'desc': 'This paper presents MagicID, a new framework for generating high-quality videos that maintain a consistent identity while exhibiting dynamic movements based on user preferences. The authors identify two main challenges in existing methods: the loss of identity over longer videos and the lack of dynamics during training due to reliance on static images. To overcome these issues, MagicID uses pairwise preference video data that incorporates explicit rewards for identity and dynamics, moving away from traditional self-reconstruction techniques. The framework employs a hybrid sampling strategy to prioritize identity preservation and enhance motion quality, leading to improved performance in generating videos that meet user expectations.'}, 'zh': {'title': '魔法身份：定制动态视频的新方法', 'desc': '视频身份定制旨在生成高保真度的视频，这些视频能够保持一致的身份并根据用户的参考图像展现显著的动态性。然而，现有方法面临两个主要挑战：在较长视频长度下身份的退化和训练过程中动态性的减少，这主要是由于它们依赖于传统的自我重建训练与静态图像。为了解决这些问题，我们提出了MagicID，一个新颖的框架，旨在直接促进生成符合用户偏好的身份一致且动态丰富的视频。我们通过构建具有明确身份和动态奖励的成对偏好视频数据来进行偏好学习，而不是坚持传统的自我重建方法。'}}}, {'id': 'https://huggingface.co/papers/2503.09949', 'title': 'UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?', 'url': 'https://huggingface.co/papers/2503.09949', 'abstract': 'With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE.', 'score': 4, 'issue_id': 2835, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '9533dd54603f19de', 'authors': ['Yuanxin Liu', 'Rui Zhu', 'Shuhuai Ren', 'Jiacong Wang', 'Haoyuan Guo', 'Xu Sun', 'Lu Jiang'], 'affiliations': ['ByteDance Seed', 'Peking University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.09949.jpg', 'data': {'categories': ['#interpretability', '#video', '#games', '#optimization', '#benchmark', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'MLLM как универсальный оценщик AI-генерированных видео', 'desc': 'Статья представляет новый подход к оценке видео, сгенерированных искусственным интеллектом (AIGV), используя мультимодальные большие языковые модели (MLLM). Авторы создали бенчмарк UVE-Bench для оценки эффективности автоматических метрик в унифицированной оценке AIGV. Исследование показало, что продвинутые MLLM, хотя и уступают человеческим оценщикам, демонстрируют многообещающие способности в унифицированной оценке AIGV, значительно превосходя существующие специализированные методы оценки. Авторы также проанализировали ключевые факторы, влияющие на производительность оценщиков на основе MLLM.'}, 'en': {'title': 'Harnessing MLLMs for Comprehensive AI-Generated Video Evaluation', 'desc': 'This paper addresses the need for effective evaluation metrics for AI-generated videos (AIGVs) as video generative models (VGMs) become more prevalent. It critiques existing methods that either depend on models designed for different tasks or require human assessments, which limits their scalability and comprehensiveness. The authors propose using multimodal large language models (MLLMs) as a unified evaluator, capitalizing on their capabilities in visual perception and language understanding. They introduce UVE-Bench, a benchmark for evaluating AIGVs, and find that while MLLMs do not yet match human evaluators, they outperform traditional evaluation methods, providing insights for future improvements in AIGV assessment.'}, 'zh': {'title': '利用多模态语言模型提升AI生成视频评估的统一性', 'desc': '随着视频生成模型（VGM）的快速发展，开发可靠的自动评估指标对于AI生成视频（AIGV）变得至关重要。现有方法通常依赖于为其他任务优化的现成模型或人类评估数据来训练专门的评估器，这限制了评估的全面性和可扩展性。为了解决这个问题，本文探讨了使用多模态大型语言模型（MLLM）作为AIGV的统一评估器的可行性，并引入了一个名为UVE-Bench的基准，收集了由最先进的VGM生成的视频及其人类偏好注释。我们的实证结果表明，尽管先进的MLLM仍然落后于人类评估者，但在统一AIGV评估中表现出色，显著超越了现有的专门评估方法。'}}}, {'id': 'https://huggingface.co/papers/2503.16194', 'title': 'Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction', 'url': 'https://huggingface.co/papers/2503.16194', 'abstract': "Autoregressive models have shown remarkable success in image generation by adapting sequential prediction techniques from language modeling. However, applying these approaches to images requires discretizing continuous pixel data through vector quantization methods like VQ-VAE. To alleviate the quantization errors that existed in VQ-VAE, recent works tend to use larger codebooks. However, this will accordingly expand vocabulary size, complicating the autoregressive modeling task. This paper aims to find a way to enjoy the benefits of large codebooks without making autoregressive modeling more difficult. Through empirical investigation, we discover that tokens with similar codeword representations produce similar effects on the final generated image, revealing significant redundancy in large codebooks. Based on this insight, we propose to predict tokens from coarse to fine (CTF), realized by assigning the same coarse label for similar tokens. Our framework consists of two stages: (1) an autoregressive model that sequentially predicts coarse labels for each token in the sequence, and (2) an auxiliary model that simultaneously predicts fine-grained labels for all tokens conditioned on their coarse labels. Experiments on ImageNet demonstrate our method's superior performance, achieving an average improvement of 59 points in Inception Score compared to baselines. Notably, despite adding an inference step, our approach achieves faster sampling speeds.", 'score': 3, 'issue_id': 2826, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '0e990c414e8ba81c', 'authors': ['Ziyao Guo', 'Kaipeng Zhang', 'Michael Qizhe Shieh'], 'affiliations': ['National University of Singapore', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.16194.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'От грубого к точному: новый подход к генерации изображений', 'desc': 'Статья представляет новый подход к генерации изображений с использованием авторегрессионных моделей. Авторы предлагают метод предсказания токенов от грубого к точному (CTF), чтобы использовать преимущества больших кодовых книг без усложнения задачи авторегрессионного моделирования. Метод состоит из двух этапов: авторегрессионная модель для последовательного предсказания грубых меток и вспомогательная модель для одновременного предсказания точных меток. Эксперименты на ImageNet показали значительное улучшение Inception Score и более быструю генерацию по сравнению с базовыми методами.'}, 'en': {'title': 'Enhancing Image Generation with Coarse-to-Fine Token Prediction', 'desc': 'This paper explores how autoregressive models can be improved for image generation by addressing the challenges of using large codebooks in vector quantization. It identifies that many tokens in these large codebooks are redundant, meaning they have similar effects on the generated images. To tackle this, the authors propose a coarse-to-fine (CTF) prediction method that first predicts broad categories for tokens and then refines these predictions with more detailed labels. Their experiments show that this approach not only enhances image quality, as indicated by a significant increase in Inception Score, but also speeds up the sampling process despite the added complexity.'}, 'zh': {'title': '从粗到细：优化自回归图像生成', 'desc': '自回归模型在图像生成中取得了显著成功，借鉴了语言建模中的序列预测技术。然而，将这些方法应用于图像时，需要通过向量量化方法（如VQ-VAE）将连续像素数据离散化。为了减少VQ-VAE中存在的量化误差，最近的研究倾向于使用更大的代码本，但这会增加词汇量，复杂化自回归建模任务。本文提出了一种从粗到细（CTF）预测标记的方法，利用相似代码词表示的标记对最终生成图像的相似影响，从而在不增加建模难度的情况下享受大代码本的优势。'}}}, {'id': 'https://huggingface.co/papers/2503.16031', 'title': 'Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content', 'url': 'https://huggingface.co/papers/2503.16031', 'abstract': 'This paper presents the Deceptive Humor Dataset (DHD), a novel resource for studying humor derived from fabricated claims and misinformation. In an era of rampant misinformation, understanding how humor intertwines with deception is essential. DHD consists of humor-infused comments generated from false narratives, incorporating fabricated claims and manipulated information using the ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging from 1 for subtle satire to 3 for high-level satire and classified into five distinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and Absurdity. The dataset spans multiple languages including English, Telugu, Hindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En, Ta-En), making it a valuable multilingual benchmark. By introducing DHD, we establish a structured foundation for analyzing humor in deceptive contexts, paving the way for a new research direction that explores how humor not only interacts with misinformation but also influences its perception and spread. We establish strong baselines for the proposed dataset, providing a foundation for future research to benchmark and advance deceptive humor detection models.', 'score': 3, 'issue_id': 2822, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '80726382feca8f20', 'authors': ['Sai Kartheek Reddy Kasu', 'Shankar Biradar', 'Sunil Saumya'], 'affiliations': ['IIIT Dharwad', 'MIT Manipal'], 'pdf_title_img': 'assets/pdf/title_img/2503.16031.jpg', 'data': {'categories': ['#ethics', '#low_resource', '#benchmark', '#dataset', '#multilingual'], 'emoji': '🤡', 'ru': {'title': 'Смех сквозь ложь: новый датасет для анализа обманчивого юмора', 'desc': 'Эта статья представляет новый набор данных DHD (Deceptive Humor Dataset) для изучения юмора, основанного на выдуманных утверждениях и дезинформации. DHD состоит из юмористических комментариев, сгенерированных на основе ложных нарративов с использованием модели ChatGPT-4o. Каждый экземпляр помечен уровнем сатиры и классифицирован по пяти категориям юмора. Датасет охватывает несколько языков, включая английский и индийские языки, что делает его ценным многоязычным ресурсом для исследований.'}, 'en': {'title': 'Unraveling Humor in the Age of Misinformation', 'desc': 'The Deceptive Humor Dataset (DHD) is a new resource designed to explore the relationship between humor and misinformation. It includes humor-filled comments based on false narratives, generated using the ChatGPT-4o model, and is labeled with a Satire Level and categorized into five types of humor. This dataset supports multiple languages, making it a useful tool for multilingual research in humor and deception. By providing a structured dataset, DHD aims to enhance the understanding of how humor can affect the perception and dissemination of misinformation.'}, 'zh': {'title': '揭示幽默与欺骗的交织', 'desc': '本文介绍了一个新的资源——欺骗幽默数据集（DHD），用于研究源自虚假声明和错误信息的幽默。在信息泛滥的时代，理解幽默与欺骗之间的关系至关重要。DHD包含从虚假叙述中生成的幽默评论，并使用ChatGPT-4o模型生成虚假声明和操控信息。每个实例都标注了讽刺水平，并分为五种幽默类别，为分析欺骗背景下的幽默提供了结构化基础。'}}}, {'id': 'https://huggingface.co/papers/2503.15855', 'title': 'VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting\n  Generation with Flexible Pose and Multi-View Joint Modeling', 'url': 'https://huggingface.co/papers/2503.15855', 'abstract': 'We propose VideoRFSplat, a direct text-to-3D model leveraging a video generation model to generate realistic 3D Gaussian Splatting (3DGS) for unbounded real-world scenes. To generate diverse camera poses and unbounded spatial extent of real-world scenes, while ensuring generalization to arbitrary text prompts, previous methods fine-tune 2D generative models to jointly model camera poses and multi-view images. However, these methods suffer from instability when extending 2D generative models to joint modeling due to the modality gap, which necessitates additional models to stabilize training and inference. In this work, we propose an architecture and a sampling strategy to jointly model multi-view images and camera poses when fine-tuning a video generation model. Our core idea is a dual-stream architecture that attaches a dedicated pose generation model alongside a pre-trained video generation model via communication blocks, generating multi-view images and camera poses through separate streams. This design reduces interference between the pose and image modalities. Additionally, we propose an asynchronous sampling strategy that denoises camera poses faster than multi-view images, allowing rapidly denoised poses to condition multi-view generation, reducing mutual ambiguity and enhancing cross-modal consistency. Trained on multiple large-scale real-world datasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms existing text-to-3D direct generation methods that heavily depend on post-hoc refinement via score distillation sampling, achieving superior results without such refinement.', 'score': 3, 'issue_id': 2829, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '66858187cf22b842', 'authors': ['Hyojun Go', 'Byeongjun Park', 'Hyelin Nam', 'Byung-Hoon Kim', 'Hyungjin Chung', 'Changick Kim'], 'affiliations': ['EverEx', 'KAIST', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15855.jpg', 'data': {'categories': ['#architecture', '#video', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Реалистичные 3D-сцены из текста без дополнительной оптимизации', 'desc': 'VideoRFSplat - это модель для генерации реалистичных 3D-сцен на основе текстовых описаний. Она использует модель генерации видео для создания многоракурсных изображений и положений камеры. Ключевая идея заключается в двухпоточной архитектуре, где отдельные потоки генерируют изображения и позы камеры. Модель применяет асинхронную стратегию сэмплирования, что улучшает согласованность между модальностями и качество результатов.'}, 'en': {'title': 'Revolutionizing 3D Scene Generation with VideoRFSplat', 'desc': 'VideoRFSplat is a novel text-to-3D model that utilizes a video generation framework to create realistic 3D Gaussian Splatting for expansive real-world scenes. Unlike previous methods that struggle with stability when combining 2D generative models for camera poses and images, our approach employs a dual-stream architecture that separates the generation of poses and images. This design minimizes interference between the two modalities and incorporates an asynchronous sampling strategy that accelerates the denoising of camera poses, enhancing the overall consistency of the generated outputs. Trained on extensive datasets, VideoRFSplat demonstrates superior performance compared to existing methods that rely on additional refinement techniques.'}, 'zh': {'title': 'VideoRFSplat：文本到3D的创新之路', 'desc': '我们提出了VideoRFSplat，这是一种直接的文本到3D模型，利用视频生成模型生成逼真的3D高斯点云（3DGS），适用于无限的真实场景。与之前的方法不同，我们的模型通过双流架构同时建模多视图图像和相机姿态，减少了姿态和图像模态之间的干扰。我们还提出了一种异步采样策略，使得相机姿态的去噪速度快于多视图图像，从而提高了跨模态的一致性。经过在多个大规模真实数据集上的训练，VideoRFSplat在不依赖后期精炼的情况下，超越了现有的文本到3D直接生成方法。'}}}, {'id': 'https://huggingface.co/papers/2503.13891', 'title': 'Where do Large Vision-Language Models Look at when Answering Questions?', 'url': 'https://huggingface.co/papers/2503.13891', 'abstract': 'Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. A fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contribute to their responses? It is non-trivial to interpret the free-form generation of LVLMs due to their complicated visual architecture (e.g., multiple encoders and multi-resolution) and variable-length outputs. In this paper, we extend existing heatmap visualization methods (e.g., iGOS++) to support LVLMs for open-ended visual question answering. We propose a method to select visually relevant tokens that reflect the relevance between generated answers and input image. Furthermore, we conduct a comprehensive analysis of state-of-the-art LVLMs on benchmarks designed to require visual information to answer. Our findings offer several insights into LVLM behavior, including the relationship between focus region and answer correctness, differences in visual attention across architectures, and the impact of LLM scale on visual understanding. The code and data are available at https://github.com/bytedance/LVLM_Interpretation.', 'score': 2, 'issue_id': 2841, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '561c77b5d6495701', 'authors': ['Xiaoying Xing', 'Chia-Wen Kuo', 'Li Fuxin', 'Yulei Niu', 'Fan Chen', 'Ming Li', 'Ying Wu', 'Longyin Wen', 'Sijie Zhu'], 'affiliations': ['Bytedance Intelligent Creation', 'Northwestern University', 'Oregon State University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13891.jpg', 'data': {'categories': ['#cv', '#benchmark', '#multimodal', '#interpretability', '#games'], 'emoji': '🔍', 'ru': {'title': 'Заглядывая в глаза искусственному интеллекту: понимание визуального восприятия LVLM', 'desc': 'Эта статья исследует поведение больших визуально-языковых моделей (LVLM) в задачах понимания и рассуждения на основе изображений и текста. Авторы разработали метод визуализации тепловых карт для интерпретации свободной генерации LVLM в задачах открытого визуального вопросно-ответного анализа. Они провели комплексный анализ современных LVLM на специальных тестовых наборах, требующих визуальной информации для ответа. Результаты исследования дают представление о связи между фокусными областями изображения и правильностью ответов, различиях в визуальном внимании между архитектурами и влиянии масштаба языковой модели на визуальное понимание.'}, 'en': {'title': 'Unlocking the Visual Understanding of LVLMs', 'desc': 'This paper investigates how Large Vision-Language Models (LVLMs) understand and utilize visual information when answering questions about images. It addresses the challenge of interpreting the complex visual architectures of LVLMs, which include multiple encoders and produce variable-length outputs. The authors enhance existing heatmap visualization techniques to identify which parts of an image are most relevant to the answers generated by the models. Their analysis reveals important insights into how different LVLM architectures focus on visual regions and how the scale of the language model affects its visual comprehension capabilities.'}, 'zh': {'title': '揭示大型视觉语言模型的视觉理解', 'desc': '大型视觉语言模型（LVLMs）在视觉语言理解和推理任务中表现出色，但它们的视觉理解行为仍然不够明确。本文探讨了LVLMs对视觉输入的依赖程度以及哪些图像区域对其响应有贡献。我们扩展了现有的热图可视化方法，以支持LVLMs在开放式视觉问答中的应用，并提出了一种选择与生成答案相关的视觉标记的方法。通过对最先进的LVLMs进行全面分析，我们揭示了焦点区域与答案正确性之间的关系，以及不同架构在视觉注意力上的差异。'}}}, {'id': 'https://huggingface.co/papers/2503.07906', 'title': 'Painting with Words: Elevating Detailed Image Captioning with Benchmark\n  and Alignment Learning', 'url': 'https://huggingface.co/papers/2503.07906', 'abstract': 'Image captioning has long been a pivotal task in visual understanding, with recent advancements in vision-language models (VLMs) significantly enhancing the ability to generate detailed image captions. However, the evaluation of detailed image captioning remains underexplored due to outdated evaluation metrics and coarse annotations. In this paper, we introduce DeCapBench along with a novel metric, DCScore, specifically designed for detailed captioning tasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by deconstructing responses into the smallest self-sufficient units, termed primitive information units, and assessing them individually. Our evaluation shows that DCScore aligns more closely with human judgment than other rule-based or model-based metrics. Concurrently, DeCapBench exhibits a high correlation with VLM arena results on descriptive tasks, surpassing existing benchmarks for vision-language models. Additionally, we present an automatic fine-grained feedback collection method, FeedQuill, for preference optimization based on our advanced metric, showing robust generalization capabilities across auto-generated preference data. Extensive experiments on multiple VLMs demonstrate that our method not only significantly reduces hallucinations but also enhances performance across various benchmarks, achieving superior detail captioning performance while surpassing GPT-4o.', 'score': 2, 'issue_id': 2843, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'df60b4319f196868', 'authors': ['Qinghao Ye', 'Xianhan Zeng', 'Fu Li', 'Chunyuan Li', 'Haoqi Fan'], 'affiliations': ['ByteDance Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.07906.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#cv', '#benchmark', '#multimodal', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Новый подход к оценке детального описания изображений', 'desc': 'Статья представляет новый метод оценки детального описания изображений под названием DCScore. Этот метрик разбивает описания на элементарные информационные единицы и оценивает их индивидуально, что позволяет лучше выявлять галлюцинации и полноту описания. Авторы также предлагают набор данных DeCapBench для оценки моделей машинного зрения и языка в задаче детального описания изображений. Дополнительно представлен метод FeedQuill для автоматического сбора обратной связи, который позволяет улучшить производительность моделей на различных бенчмарках.'}, 'en': {'title': 'Revolutionizing Image Caption Evaluation with DeCapBench and DCScore', 'desc': 'This paper addresses the challenge of evaluating detailed image captioning, which has been hindered by outdated metrics and coarse annotations. The authors introduce DeCapBench and a new metric called DCScore, which focuses on assessing image captions by breaking them down into primitive information units. This approach allows for a more accurate evaluation of hallucinations and the comprehensiveness of captions, aligning better with human judgment. Additionally, the paper presents FeedQuill, a method for collecting fine-grained feedback to optimize preferences, demonstrating improved performance in detailed captioning tasks across various vision-language models.'}, 'zh': {'title': '提升图像描述的评估与生成能力', 'desc': '图像描述一直是视觉理解中的一个重要任务，最近视觉语言模型（VLMs）的进步显著提升了生成详细图像描述的能力。然而，详细图像描述的评估仍然不够深入，主要是因为评估指标过时和注释粗糙。本文介绍了DeCapBench和一种新颖的评估指标DCScore，专门用于详细描述任务，能够更好地评估描述的准确性和细致程度。我们的评估结果表明，DCScore与人类判断的吻合度高于其他基于规则或模型的指标，同时DeCapBench在描述性任务上与VLM领域的结果高度相关，超越了现有的基准。'}}}, {'id': 'https://huggingface.co/papers/2503.16091', 'title': 'AIMI: Leveraging Future Knowledge and Personalization in Sparse Event\n  Forecasting for Treatment Adherence', 'url': 'https://huggingface.co/papers/2503.16091', 'abstract': 'Adherence to prescribed treatments is crucial for individuals with chronic conditions to avoid costly or adverse health outcomes. For certain patient groups, intensive lifestyle interventions are vital for enhancing medication adherence. Accurate forecasting of treatment adherence can open pathways to developing an on-demand intervention tool, enabling timely and personalized support. With the increasing popularity of smartphones and wearables, it is now easier than ever to develop and deploy smart activity monitoring systems. However, effective forecasting systems for treatment adherence based on wearable sensors are still not widely available. We close this gap by proposing Adherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI is a knowledge-guided adherence forecasting system that leverages smartphone sensors and previous medication history to estimate the likelihood of forgetting to take a prescribed medication. A user study was conducted with 27 participants who took daily medications to manage their cardiovascular diseases. We designed and developed CNN and LSTM-based forecasting models with various combinations of input features and found that LSTM models can forecast medication adherence with an accuracy of 0.932 and an F-1 score of 0.936. Moreover, through a series of ablation studies involving convolutional and recurrent neural network architectures, we demonstrate that leveraging known knowledge about future and personalized training enhances the accuracy of medication adherence forecasting. Code available: https://github.com/ab9mamun/AIMI.', 'score': 1, 'issue_id': 2832, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '905c53a7c6bb7214', 'authors': ['Abdullah Mamun', 'Diane J. Cook', 'Hassan Ghasemzadeh'], 'affiliations': ['College of Health Solutions, Arizona State University, Phoenix, AZ 85054, USA', 'School of Computing and Augmented Intelligence, Arizona State University, Phoenix, AZ 85054, USA', 'School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA 99164, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.16091.jpg', 'data': {'categories': ['#science', '#architecture', '#healthcare', '#optimization', '#open_source', '#training'], 'emoji': '💊', 'ru': {'title': 'ИИ на страже вашего здоровья: точное прогнозирование приема лекарств', 'desc': 'Данная статья представляет систему AIMI (Adherence Forecasting and Intervention with Machine Intelligence) для прогнозирования приверженности лечению на основе данных с носимых устройств и смартфонов. Авторы разработали модели на базе сверточных нейронных сетей (CNN) и LSTM для оценки вероятности пропуска приема лекарств пациентами с сердечно-сосудистыми заболеваниями. Исследование показало, что модели LSTM достигают точности 0,932 и F1-меры 0,936 в задаче прогнозирования. Эксперименты также продемонстрировали, что использование априорных знаний о будущем и персонализированное обучение повышают точность прогнозов.'}, 'en': {'title': 'Smart Forecasting for Better Medication Adherence', 'desc': 'This paper presents AIMI, a machine learning system designed to forecast medication adherence using smartphone sensors and patient medication history. By employing advanced models like CNN and LSTM, AIMI achieves high accuracy in predicting when patients might forget to take their medications. The study involved 27 participants with cardiovascular diseases, demonstrating that personalized training and knowledge integration significantly improve forecasting performance. The findings suggest that AIMI can serve as a valuable tool for timely interventions, ultimately enhancing treatment adherence for chronic condition management.'}, 'zh': {'title': '智能预测，助力药物依从性', 'desc': '本论文提出了一种名为AIMI的药物依从性预测系统，利用智能手机传感器和患者的用药历史来预测患者忘记服药的可能性。通过对27名心血管疾病患者的用户研究，使用基于卷积神经网络（CNN）和长短期记忆网络（LSTM）的模型进行预测。研究结果表明，LSTM模型在药物依从性预测中达到了93.2%的准确率和93.6的F-1分数。通过一系列消融实验，证明了利用已知知识和个性化训练可以提高药物依从性预测的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.11509', 'title': 'TikZero: Zero-Shot Text-Guided Graphics Program Synthesis', 'url': 'https://huggingface.co/papers/2503.11509', 'abstract': 'With the rise of generative AI, synthesizing figures from text captions becomes a compelling application. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available.', 'score': 1, 'issue_id': 2837, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '2335a2440c406122', 'authors': ['Jonas Belouadi', 'Eddy Ilg', 'Margret Keuper', 'Hideki Tanaka', 'Masao Utiyama', 'Raj Dabre', 'Steffen Eger', 'Simone Paolo Ponzetto'], 'affiliations': ['National Institute of Information and Communications Technology, Japan', 'University of Mannheim, Germany', 'University of Technology Nuremberg, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.11509.jpg', 'data': {'categories': ['#training', '#synthetic', '#open_source', '#multimodal', '#data', '#dataset', '#inference'], 'emoji': '🎨', 'ru': {'title': 'TikZero: мост между текстом и векторной графикой через промежуточные изображения', 'desc': 'TikZero - это новый подход к генерации графических программ на основе текстовых описаний. Метод использует изображения в качестве промежуточного звена между текстом и кодом, что позволяет обучаться на несогласованных данных. TikZero превосходит базовые модели, работающие только с выровненными данными, и достигает результатов на уровне крупных коммерческих систем. Метод позволяет создавать точные и редактируемые изображения в формате векторной графики.'}, 'en': {'title': 'Bridging Text and Graphics with TikZero', 'desc': 'This paper introduces TikZero, a novel approach for generating graphics programs from text captions using generative AI. It addresses the challenge of limited aligned training data by utilizing unaligned graphics programs and captioned images as separate training sources. TikZero employs image representations as a bridge, allowing for independent training and enabling zero-shot synthesis of graphics programs guided by text. The results demonstrate that TikZero significantly outperforms existing methods that rely solely on aligned data, achieving competitive performance with larger models when combined with aligned graphics programs.'}, 'zh': {'title': 'TikZero：解耦文本与图形生成的创新方法', 'desc': '随着生成性人工智能的兴起，从文本描述合成图形成为一个引人注目的应用。然而，实现高几何精度和可编辑性需要将图形表示为像TikZ这样的图形程序，而对齐的训练数据（即带有描述的图形程序）仍然稀缺。我们提出了TikZero，通过使用图像表示作为中介桥梁，将图形程序生成与文本理解解耦。这使得我们能够独立训练图形程序和带描述的图像，并在推理过程中实现零-shot文本引导的图形程序合成。'}}}, {'id': 'https://huggingface.co/papers/2503.15672', 'title': 'GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving', 'url': 'https://huggingface.co/papers/2503.15672', 'abstract': 'Self-supervised pre-training based on next-token prediction has enabled large language models to capture the underlying structure of text, and has led to unprecedented performance on a large array of tasks when applied at scale. Similarly, autonomous driving generates vast amounts of spatiotemporal data, alluding to the possibility of harnessing scale to learn the underlying geometric and semantic structure of the environment and its evolution over time. In this direction, we propose a geometric and semantic self-supervised pre-training method, GASP, that learns a unified representation by predicting, at any queried future point in spacetime, (1) general occupancy, capturing the evolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle path through the environment; and (3) distilled high-level features from a vision foundation model. By modeling geometric and semantic 4D occupancy fields instead of raw sensor measurements, the model learns a structured, generalizable representation of the environment and its evolution through time. We validate GASP on multiple autonomous driving benchmarks, demonstrating significant improvements in semantic occupancy forecasting, online mapping, and ego trajectory prediction. Our results demonstrate that continuous 4D geometric and semantic occupancy prediction provides a scalable and effective pre-training paradigm for autonomous driving. For code and additional visualizations, see \\href{https://research.zenseact.com/publications/gasp/.', 'score': 0, 'issue_id': 2838, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '94ae46d5f2ffc089', 'authors': ['William Ljungbergh', 'Adam Lilja', 'Adam Tonderski. Arvid Laveno Ling', 'Carl Lindström', 'Willem Verbeke', 'Junsheng Fu', 'Christoffer Petersson', 'Lars Hammarstrand', 'Michael Felsberg'], 'affiliations': ['Chalmers University of Technology', 'Linkoping University', 'Lund University', 'Zenseact'], 'pdf_title_img': 'assets/pdf/title_img/2503.15672.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#3d', '#architecture', '#survey', '#multimodal'], 'emoji': '🚗', 'ru': {'title': 'Предсказание 4D-занятости для эффективного предобучения в автономном вождении', 'desc': 'Статья представляет метод GASP - геометрическое и семантическое самоконтролируемое предобучение для автономного вождения. GASP обучает единое представление, предсказывая будущую общую и эго-занятость, а также высокоуровневые визуальные признаки. Модель учится структурированному представлению окружающей среды и ее эволюции во времени, моделируя геометрические и семантические 4D-поля занятости. Эксперименты показывают значительные улучшения в прогнозировании семантической занятости, онлайн-картографировании и предсказании траектории движения.'}, 'en': {'title': 'GASP: Revolutionizing Autonomous Driving with 4D Occupancy Prediction', 'desc': 'This paper introduces GASP, a self-supervised pre-training method designed for autonomous driving applications. GASP predicts future occupancy in a 4D space, focusing on general occupancy, ego occupancy, and high-level features from a vision model. By utilizing geometric and semantic representations instead of raw sensor data, GASP creates a more structured understanding of the environment. The method shows significant improvements in tasks like semantic occupancy forecasting and ego trajectory prediction across various benchmarks.'}, 'zh': {'title': 'GASP：自动驾驶的自监督预训练新方法', 'desc': '本文提出了一种几何和语义自监督预训练方法GASP，旨在通过预测未来时空点的占用情况来学习环境的统一表示。该方法包括对3D场景的动态结构、车辆在环境中的路径以及从视觉基础模型提取的高级特征进行建模。通过建模几何和语义的4D占用场，而不是原始传感器数据，模型能够学习到结构化且具有可推广性的环境表示。我们在多个自动驾驶基准测试中验证了GASP，显示出在语义占用预测、在线地图构建和自我轨迹预测方面的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2503.14201', 'title': 'Why Personalizing Deep Learning-Based Code Completion Tools Matters', 'url': 'https://huggingface.co/papers/2503.14201', 'abstract': "Deep learning (DL)-based code completion tools have transformed software development by enabling advanced code generation. These tools leverage models trained on vast amounts of code from numerous repositories, capturing general coding patterns. However, the impact of fine-tuning these models for specific organizations or developers to boost their performance on such subjects remains unexplored. In this work, we fill this gap by presenting solid empirical evidence answering this question. More specifically, we consider 136 developers from two organizations (Apache and Spring), two model architectures (T5 and Code Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5 models (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source projects, excluding the subject organizations' data, and compared against versions fine-tuned on organization- and developer-specific datasets. For the Code Llama model (7B), we compared the performance of the already pre-trained model publicly available online with the same model fine-tuned via parameter-efficient fine-tuning on organization- and developer-specific datasets. Our results show that there is a boost in prediction capabilities provided by both an organization-specific and a developer-specific additional fine-tuning, with the former being particularly performant. Such a finding generalizes across (i) the two subject organizations (i.e., Apache and Spring) and (ii) models of completely different magnitude (from 60M to 7B trainable parameters). Finally, we show that DL models fine-tuned on an organization-specific dataset achieve the same completion performance of pre-trained code models used out of the box and being sim10times larger, with consequent savings in terms of deployment and inference cost (e.g., smaller GPUs needed).", 'score': 0, 'issue_id': 2840, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'b244dc3db7a6fb65', 'authors': ['Alessandro Giagnorio', 'Alberto Martin-Lopez', 'Gabriele Bavota'], 'affiliations': ['Università della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2503.14201.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#open_source', '#small_models', '#training', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Дообучение моделей кода: меньше параметров, выше точность', 'desc': 'Исследование показывает, что дообучение моделей для автодополнения кода на данных конкретных организаций или разработчиков повышает их эффективность. Эксперименты проводились с использованием моделей T5 и Code Llama различных размеров на данных проектов Apache и Spring. Результаты демонстрируют, что дообучение на специфических данных организации дает наибольший прирост производительности. Модели, дообученные на данных организации, показывают такую же эффективность, как и в 10 раз большие предобученные модели, что позволяет экономить ресурсы.'}, 'en': {'title': 'Boosting Code Completion with Tailored Fine-Tuning', 'desc': 'This paper investigates the effectiveness of fine-tuning deep learning models for code completion tailored to specific organizations and developers. It analyzes the performance of two model architectures, T5 and Code Llama, across different sizes and datasets. The study finds that fine-tuning on organization-specific and developer-specific datasets significantly enhances prediction capabilities, with organization-specific fine-tuning yielding the best results. Notably, models fine-tuned on smaller datasets can match the performance of larger pre-trained models, leading to cost savings in deployment and inference.'}, 'zh': {'title': '微调模型，提升代码补全性能！', 'desc': '基于深度学习的代码补全工具通过先进的代码生成技术改变了软件开发。这些工具利用从大量代码库中训练的模型，捕捉一般的编码模式。然而，针对特定组织或开发者对这些模型进行微调以提高性能的影响尚未被深入研究。我们的研究表明，针对组织和开发者的微调可以显著提升模型的预测能力，尤其是组织特定的微调效果更为显著。'}}}, {'id': 'https://huggingface.co/papers/2503.14456', 'title': 'RWKV-7 "Goose" with Expressive Dynamic State Evolution', 'url': 'https://huggingface.co/papers/2503.14456', 'abstract': 'We present RWKV-7 "Goose", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7\'s language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.', 'score': 92, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '0cd796cef6fa6475', 'authors': ['Bo Peng', 'Ruichong Zhang', 'Daniel Goldstein', 'Eric Alcaide', 'Haowen Hou', 'Janna Lu', 'William Merrill', 'Guangyu Song', 'Kaifeng Tan', 'Saiteja Utpala', 'Nathan Wilce', 'Johan S. Wind', 'Tianyi Wu', 'Daniel Wuttke', 'Christian Zhou-Zheng'], 'affiliations': ['Beijing Normal University', 'Dalle Molle Institute for Artificial Intelligence USI-SUPSI', 'Denigma', 'EleutherAI', 'George Mason University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'New York University', 'RWKV Project (under Linux Foundation AI & Data)', 'Recursal AI', 'Shenzhen University', 'Tano Labs', 'Tsinghua University', 'University of Oslo'], 'pdf_title_img': 'assets/pdf/title_img/2503.14456.jpg', 'data': {'categories': ['#architecture', '#training', '#open_source', '#dataset', '#multilingual'], 'emoji': '🦢', 'ru': {'title': 'RWKV-7: Эффективная архитектура для многоязычного моделирования последовательностей', 'desc': "RWKV-7 'Goose' - это новая архитектура моделирования последовательностей, которая устанавливает новый state-of-the-art в производительности при 3 миллиардах параметров на многоязычных задачах. Модель требует постоянного использования памяти и времени вывода на токен, вводит обобщенную формулировку правила дельты с векторным гейтингом и обучением в контексте. RWKV-7 способна отслеживать состояния и распознавать все регулярные языки, превосходя возможности трансформеров. Авторы также представляют многоязычный корпус из 3,1 триллиона токенов и обучают на нем четыре модели RWKV-7 размером от 0,19 до 2,9 миллиардов параметров."}, 'en': {'title': 'RWKV-7: Efficient Multilingual Mastery with Fewer Parameters', 'desc': 'RWKV-7 "Goose" is a novel sequence modeling architecture that achieves state-of-the-art performance on multilingual tasks with only 3 billion parameters. It utilizes a unique formulation of the delta rule and vector-valued gating, allowing for efficient memory usage and constant inference time per token. The model demonstrates advanced capabilities such as state tracking and recognition of regular languages, surpassing traditional Transformers in complexity. Additionally, RWKV-7 is trained on a large multilingual corpus and is made openly available for further research and development.'}, 'zh': {'title': 'RWKV-7：多语言任务的新突破', 'desc': 'RWKV-7 "Goose" 是一种新的序列建模架构，具有3亿参数的预训练语言模型，在多语言任务中达到了新的最先进水平。与其他顶级3B模型相比，RWKV-7在训练时使用的标记数量显著减少，但仍能与当前的英语语言性能相匹配。该模型采用了新的广义增量规则，结合了向量值门控和上下文学习率，同时保持了训练的并行性。RWKV-7能够进行状态跟踪并识别所有正规语言，超越了标准复杂性猜想下的Transformer的能力。'}}}, {'id': 'https://huggingface.co/papers/2503.14378', 'title': 'Impossible Videos', 'url': 'https://huggingface.co/papers/2503.14378', 'abstract': "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.", 'score': 47, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '3334aa74b743ac8d', 'authors': ['Zechen Bai', 'Hai Ci', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.14378.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#synthetic', '#video'], 'emoji': '🎬', 'ru': {'title': 'IPV-Bench: Новый рубеж в понимании и генерации невозможных видео', 'desc': 'Эта статья представляет IPV-Bench - новый бенчмарк для оценки и развития понимания и генерации видео. Он основан на таксономии, охватывающей 4 домена и 14 категорий, и включает разнообразные сцены, нарушающие физические, биологические, географические или социальные законы. Бенчмарк содержит набор промптов для оценки моделей генерации видео и видеоданные для оценки способности Video-LLM понимать невозможные видео. Комплексная оценка выявляет ограничения и предоставляет идеи для будущих направлений развития видеомоделей.'}, 'en': {'title': 'Exploring the Impossible: Advancing Video Generation and Understanding', 'desc': 'This paper addresses the limitations of current synthetic video datasets, which mainly focus on replicating real-world scenarios. It introduces IPV-Bench, a new benchmark designed to evaluate video generation and understanding models on their ability to create and comprehend impossible video content. The benchmark includes a detailed taxonomy with various categories that challenge models to generate videos that defy physical and social laws. The findings highlight the current capabilities and shortcomings of video models, providing insights for future advancements in the field.'}, 'zh': {'title': '探索不可能视频的生成与理解', 'desc': '本论文探讨了合成视频在数据稀缺和多样性方面的应用。当前的合成数据集主要复制现实场景，而对不可能、反事实和反现实的视频概念研究不足。我们提出了IPV-Bench，这是一个新颖的基准，旨在评估和促进视频理解与生成的进展。该基准涵盖了多种违反物理、生物、地理或社会法则的场景，并通过构建提示套件来挑战视频生成模型的创造力和提示跟随能力。'}}}, {'id': 'https://huggingface.co/papers/2503.14476', 'title': 'DAPO: An Open-Source LLM Reinforcement Learning System at Scale', 'url': 'https://huggingface.co/papers/2503.14476', 'abstract': 'Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.', 'score': 42, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '5b4841d2845817e8', 'authors': ['Qiying Yu', 'Zheng Zhang', 'Ruofei Zhu', 'Yufeng Yuan', 'Xiaochen Zuo', 'Yu Yue', 'Tiantian Fan', 'Gaohong Liu', 'Lingjun Liu', 'Xin Liu', 'Haibin Lin', 'Zhiqi Lin', 'Bole Ma', 'Guangming Sheng', 'Yuxuan Tong', 'Chi Zhang', 'Mofan Zhang', 'Wang Zhang', 'Hang Zhu', 'Jinhua Zhu', 'Jiaze Chen', 'Jiangjie Chen', 'Chengyi Wang', 'Hongli Yu', 'Weinan Dai', 'Yuxuan Song', 'Xiangpeng Wei', 'Hao Zhou', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Lin Yan', 'Mu Qiao', 'Yonghui Wu', 'Mingxuan Wang'], 'affiliations': ['ByteDance', 'Institute for AI Industry Research (AIR), Tsinghua University', 'SIA-Lab of Tsinghua AIR and ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.14476.jpg', 'data': {'categories': ['#rl', '#dataset', '#reasoning', '#optimization', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Открытая система RL для улучшения рассуждений больших языковых моделей', 'desc': 'Статья представляет алгоритм DAPO для обучения с подкреплением крупномасштабных языковых моделей. Авторы открыто публикуют систему, достигающую 50 баллов на AIME 2024 с использованием базовой модели Qwen2.5-32B. В работе раскрываются четыре ключевые техники, делающие успешным RL для больших ЯМ. Исследователи также предоставляют код обучения и подготовленный набор данных для воспроизводимости результатов.'}, 'en': {'title': 'Unlocking LLM Potential with Open-Source Reinforcement Learning', 'desc': 'This paper introduces the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, which enhances the reasoning capabilities of large language models (LLMs) through reinforcement learning (RL). The authors address the lack of transparency in existing state-of-the-art LLMs by providing detailed insights into their training processes and techniques. They report achieving a significant performance score of 50 points on the AIME 2024 benchmark using the Qwen2.5-32B model. By open-sourcing their training code and dataset, they aim to improve reproducibility and foster further advancements in large-scale LLM reinforcement learning research.'}, 'zh': {'title': '解锁大型语言模型的推理潜力', 'desc': '本论文提出了一种新的算法，称为解耦剪辑和动态采样策略优化（DAPO），旨在提升大型语言模型（LLM）的推理能力。我们通过强化学习技术，成功实现了在AIME 2024上获得50分的成绩，使用的是Qwen2.5-32B基础模型。与以往研究不同，我们公开了四个关键技术细节，帮助社区更好地理解和复现我们的训练结果。此外，我们还开源了训练代码和经过精心处理的数据集，以促进大型LLM强化学习的可重复性和未来研究。'}}}, {'id': 'https://huggingface.co/papers/2503.14478', 'title': 'Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM', 'url': 'https://huggingface.co/papers/2503.14478', 'abstract': "Creativity is a fundamental aspect of intelligence, involving the ability to generate novel and appropriate solutions across diverse contexts. While Large Language Models (LLMs) have been extensively evaluated for their creative capabilities, the assessment of Multimodal Large Language Models (MLLMs) in this domain remains largely unexplored. To address this gap, we introduce Creation-MMBench, a multimodal benchmark specifically designed to evaluate the creative capabilities of MLLMs in real-world, image-based tasks. The benchmark comprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous evaluation, we define instance-specific evaluation criteria for each test case, guiding the assessment of both general response quality and factual consistency with visual inputs. Experimental results reveal that current open-source MLLMs significantly underperform compared to proprietary models in creative tasks. Furthermore, our analysis demonstrates that visual fine-tuning can negatively impact the base LLM's creative abilities. Creation-MMBench provides valuable insights for advancing MLLM creativity and establishes a foundation for future improvements in multimodal generative intelligence. Full data and evaluation code is released on https://github.com/open-compass/Creation-MMBench.", 'score': 37, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '120f1d8ec2eb88a8', 'authors': ['Xinyu Fang', 'Zhijian Chen', 'Kai Lan', 'Shengyuan Ding', 'Yingji Liang', 'Xiangyu Zhao', 'Farong Wen', 'Zicheng Zhang', 'Guofeng Zhang', 'Haodong Duan', 'Kai Chen', 'Dahua Lin'], 'affiliations': ['East China Normal University', 'Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'Tongji University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14478.jpg', 'data': {'categories': ['#creativity', '#open_source', '#benchmark', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Измеряя творчество искусственного интеллекта: новый бенчмарк для мультимодальных моделей', 'desc': 'Статья представляет Creation-MMBench - новый мультимодальный бенчмарк для оценки творческих способностей мультимодальных больших языковых моделей (MLLM) в задачах, основанных на изображениях. Бенчмарк содержит 765 тестовых примеров, охватывающих 51 детализированную задачу, с критериями оценки для каждого случая. Эксперименты показали, что открытые MLLM значительно уступают проприетарным моделям в творческих задачах. Исследование также выявило, что визуальная дообучение может негативно влиять на творческие способности базовой языковой модели.'}, 'en': {'title': 'Unlocking Creativity in Multimodal AI with Creation-MMBench', 'desc': 'This paper introduces Creation-MMBench, a new benchmark designed to evaluate the creative capabilities of Multimodal Large Language Models (MLLMs) in tasks that involve both text and images. The benchmark includes 765 test cases across 51 specific tasks, each with tailored evaluation criteria to assess the quality of responses and their consistency with visual inputs. Experimental findings indicate that current open-source MLLMs perform poorly in creative tasks compared to proprietary models, and that visual fine-tuning may hinder the creative performance of base LLMs. Creation-MMBench aims to enhance understanding and development of MLLM creativity, providing a resource for future research in multimodal generative intelligence.'}, 'zh': {'title': '评估多模态大型语言模型的创造力', 'desc': '创造力是智能的一个基本方面，涉及在不同情境中生成新颖且适当的解决方案。虽然大型语言模型（LLMs）在创造能力方面得到了广泛评估，但多模态大型语言模型（MLLMs）的评估仍然相对缺乏。为了解决这一问题，我们推出了Creation-MMBench，这是一个专门设计用于评估MLLMs在基于图像的实际任务中创造能力的多模态基准。实验结果表明，当前的开源MLLMs在创造性任务中显著低于专有模型，而视觉微调可能会对基础LLM的创造能力产生负面影响。'}}}, {'id': 'https://huggingface.co/papers/2503.12797', 'title': 'DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding', 'url': 'https://huggingface.co/papers/2503.12797', 'abstract': 'Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requires both fine-grained perception and domain-specific knowledge integration. To address the challenges of KVG, we propose DeepPerception, an MLLM enhanced with cognitive visual perception capabilities. Our approach consists of (1) an automated data synthesis pipeline that generates high-quality, knowledge-aligned training samples, and (2) a two-stage training framework combining supervised fine-tuning for cognitive reasoning scaffolding and reinforcement learning to optimize perception-cognition synergy. To benchmark performance, we introduce KVG-Bench a comprehensive dataset spanning 10 domains with 1.3K manually curated test cases. Experimental results demonstrate that DeepPerception significantly outperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on KVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over baseline approaches. Our findings highlight the importance of integrating cognitive processes into MLLMs for human-like visual perception and open new directions for multimodal reasoning research. The data, codes, and models are released at https://github.com/thunlp/DeepPerception.', 'score': 24, 'issue_id': 2777, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c682a086aaac0fa1', 'authors': ['Xinyu Ma', 'Ziyang Ding', 'Zhicong Luo', 'Chi Chen', 'Zonghao Guo', 'Derek F. Wong', 'Xiaoyi Feng', 'Maosong Sun'], 'affiliations': ['Northwestern Polytechnical University', 'Shandong University', 'Tsinghua University', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2503.12797.jpg', 'data': {'categories': ['#multimodal', '#data', '#dataset', '#reasoning', '#synthetic', '#benchmark', '#transfer_learning', '#training'], 'emoji': '🔬', 'ru': {'title': 'DeepPerception: Улучшение визуального восприятия ИИ через интеграцию знаний', 'desc': 'Эта статья представляет новый подход к улучшению визуального восприятия мультимодальных языковых моделей (MLLM). Авторы вводят задачу knowledge-intensive visual grounding (KVG), требующую тонкого визуального различения и интеграции специализированных знаний. Предлагается модель DeepPerception, использующая автоматическую генерацию обучающих данных и двухэтапное обучение для улучшения когнитивных способностей MLLM. Эксперименты на созданном датасете KVG-Bench показывают значительное улучшение точности и обобщающей способности модели.'}, 'en': {'title': 'Enhancing Visual Perception in MLLMs with DeepPerception', 'desc': 'This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in visual perception and reasoning. It introduces a new task called knowledge-intensive visual grounding (KVG), which combines fine-grained visual discrimination with domain-specific knowledge. The authors propose DeepPerception, an enhanced MLLM that utilizes a data synthesis pipeline and a two-stage training framework to improve cognitive reasoning and perception integration. Experimental results show that DeepPerception outperforms existing methods, demonstrating the potential for MLLMs to achieve human-like visual understanding.'}, 'zh': {'title': '提升视觉感知的认知整合能力', 'desc': '人类专家在细粒度视觉辨别方面表现出色，能够利用领域知识来优化感知特征，而当前的多模态大型语言模型（MLLMs）在这方面仍显不足。尽管拥有丰富的专家级知识，MLLMs在视觉感知中整合推理的能力较弱，常常直接生成响应而缺乏深入分析。为了解决这一问题，我们提出了知识密集型视觉定位（KVG），这是一项新颖的视觉定位任务，要求同时具备细粒度感知和领域特定知识的整合。我们提出的DeepPerception模型增强了认知视觉感知能力，通过自动化数据合成和两阶段训练框架，显著提高了在KVG-Bench数据集上的准确性和跨领域泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.12329', 'title': 'CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era', 'url': 'https://huggingface.co/papers/2503.12329', 'abstract': 'Image captioning has been a longstanding challenge in vision-language research. With the rise of LLMs, modern Vision-Language Models (VLMs) generate detailed and comprehensive image descriptions. However, benchmarking the quality of such captions remains unresolved. This paper addresses two key questions: (1) How well do current VLMs actually perform on image captioning, particularly compared to humans? We built CapArena, a platform with over 6000 pairwise caption battles and high-quality human preference votes. Our arena-style evaluation marks a milestone, showing that leading models like GPT-4o achieve or even surpass human performance, while most open-source models lag behind. (2) Can automated metrics reliably assess detailed caption quality? Using human annotations from CapArena, we evaluate traditional and recent captioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while some metrics (e.g., METEOR) show decent caption-level agreement with humans, their systematic biases lead to inconsistencies in model ranking. In contrast, VLM-as-a-Judge demonstrates robust discernment at both the caption and model levels. Building on these insights, we release CapArena-Auto, an accurate and efficient automated benchmark for detailed captioning, achieving 94.3% correlation with human rankings at just $4 per test. Data and resources will be open-sourced at https://caparena.github.io.', 'score': 19, 'issue_id': 2778, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': 'c97a8c730bfcbfa8', 'authors': ['Kanzhi Cheng', 'Wenpo Song', 'Jiaxin Fan', 'Zheng Ma', 'Qiushi Sun', 'Fangzhi Xu', 'Chenyang Yan', 'Nuo Chen', 'Jianbing Zhang', 'Jiajun Chen'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.12329.jpg', 'data': {'categories': ['#benchmark', '#games', '#open_source', '#cv'], 'emoji': '📸', 'ru': {'title': 'Новый рубеж в генерации подписей к изображениям: ИИ догоняет человека', 'desc': 'Статья посвящена проблеме оценки качества генерации подписей к изображениям с помощью современных моделей компьютерного зрения и обработки естественного языка (VLM). Авторы создали платформу CapArena для сравнения подписей, сгенерированных моделями и людьми, показав, что некоторые модели (например, GPT-4o) достигают или превосходят человеческий уровень. Исследование также оценивает эффективность автоматических метрик для оценки качества подписей, выявляя их ограничения. На основе полученных результатов авторы разработали CapArena-Auto - автоматизированный бенчмарк для оценки детальных подписей к изображениям.'}, 'en': {'title': 'Elevating Image Captioning: Human-Level Performance and Reliable Metrics', 'desc': 'This paper tackles the challenge of evaluating image captioning performance in Vision-Language Models (VLMs). It introduces CapArena, a platform that conducts over 6000 pairwise caption battles to compare VLM outputs with human-generated captions. The findings indicate that advanced models like GPT-4o can match or exceed human performance, while many open-source models do not. Additionally, the study assesses various automated metrics for caption quality, revealing that while some metrics align with human preferences, VLM-as-a-Judge provides a more reliable evaluation method, leading to the development of CapArena-Auto for efficient benchmarking.'}, 'zh': {'title': '图像描述的新标准：VLM的崛起与评估', 'desc': '图像描述一直是视觉与语言研究中的一个重要挑战。随着大型语言模型（LLMs）的发展，现代视觉-语言模型（VLMs）能够生成详细且全面的图像描述。本文通过建立CapArena平台，评估当前VLM在图像描述任务上的表现，发现领先模型如GPT-4o的表现甚至超过了人类。我们还分析了自动评估指标的可靠性，结果表明VLM作为评判者在描述质量评估中表现出色，提供了一种新的高效基准方法。'}}}, {'id': 'https://huggingface.co/papers/2503.13424', 'title': 'Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation', 'url': 'https://huggingface.co/papers/2503.13424', 'abstract': 'Large-scale articulated objects with high quality are desperately needed for multiple tasks related to embodied AI. Most existing methods for creating articulated objects are either data-driven or simulation based, which are limited by the scale and quality of the training data or the fidelity and heavy labour of the simulation. In this paper, we propose Infinite Mobility, a novel method for synthesizing high-fidelity articulated objects through procedural generation. User study and quantitative evaluation demonstrate that our method can produce results that excel current state-of-the-art methods and are comparable to human-annotated datasets in both physics property and mesh quality. Furthermore, we show that our synthetic data can be used as training data for generative models, enabling next-step scaling up. Code is available at https://github.com/Intern-Nexus/Infinite-Mobility', 'score': 18, 'issue_id': 2776, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '20793013b58dba36', 'authors': ['Xinyu Lian', 'Zichao Yu', 'Ruiming Liang', 'Yitong Wang', 'Li Ray Luo', 'Kaixu Chen', 'Yuanzhen Zhou', 'Qihong Tang', 'Xudong Xu', 'Zhaoyang Lyu', 'Bo Dai', 'Jiangmiao Pang'], 'affiliations': ['Fudan University', 'Harbin Institute of Technology, Shenzhen', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai Artificial Intelligence Laboratory', 'South China University of Technology', 'The University of Hong Kong', 'Tongji University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.13424.jpg', 'data': {'categories': ['#3d', '#open_source', '#dataset', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Процедурная генерация сочлененных объектов для воплощенного ИИ', 'desc': 'Статья представляет новый метод под названием Infinite Mobility для синтеза высококачественных сочлененных объектов с помощью процедурной генерации. Авторы утверждают, что их подход превосходит современные методы и сравним с датасетами, размеченными вручную, по физическим свойствам и качеству полигональных сеток. Метод решает проблему ограниченности существующих подходов, основанных на данных или симуляции. Синтетические данные, полученные с помощью Infinite Mobility, могут использоваться для обучения генеративных моделей, что открывает возможности для масштабирования.'}, 'en': {'title': 'Revolutionizing Articulated Object Creation with Infinite Mobility', 'desc': 'This paper introduces Infinite Mobility, a new approach for creating high-quality articulated objects using procedural generation techniques. Unlike traditional methods that rely on limited data or labor-intensive simulations, Infinite Mobility generates objects that maintain high fidelity in both physical properties and mesh quality. The authors conducted user studies and quantitative evaluations, showing that their method outperforms existing state-of-the-art techniques and rivals human-annotated datasets. Additionally, the synthetic data produced can be utilized for training generative models, facilitating further advancements in the field of embodied AI.'}, 'zh': {'title': '无限移动：合成高保真关节物体的新方法', 'desc': '在这篇论文中，我们提出了一种名为无限移动（Infinite Mobility）的方法，用于通过程序生成合成高保真度的关节物体。这种方法克服了现有数据驱动或模拟方法在规模和质量上的限制。用户研究和定量评估表明，我们的方法在物理属性和网格质量上超越了当前最先进的方法，并且与人工标注的数据集相当。此外，我们的合成数据可以作为生成模型的训练数据，支持后续的扩展。'}}}, {'id': 'https://huggingface.co/papers/2503.14125', 'title': 'Frac-Connections: Fractional Extension of Hyper-Connections', 'url': 'https://huggingface.co/papers/2503.14125', 'abstract': 'Residual connections are central to modern deep learning architectures, enabling the training of very deep networks by mitigating gradient vanishing. Hyper-Connections recently generalized residual connections by introducing multiple connection strengths at different depths, thereby addressing the seesaw effect between gradient vanishing and representation collapse. However, Hyper-Connections increase memory access costs by expanding the width of hidden states. In this paper, we propose Frac-Connections, a novel approach that divides hidden states into multiple parts rather than expanding their width. Frac-Connections retain partial benefits of Hyper-Connections while reducing memory consumption. To validate their effectiveness, we conduct large-scale experiments on language tasks, with the largest being a 7B MoE model trained on up to 3T tokens, demonstrating that Frac-Connections significantly outperform residual connections.', 'score': 13, 'issue_id': 2776, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '936ea0aa4972c382', 'authors': ['Defa Zhu', 'Hongzhi Huang', 'Jundong Zhou', 'Zihao Huang', 'Yutao Zeng', 'Banggu Wu', 'Qiyang Min', 'Xun Zhou'], 'affiliations': ['ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2503.14125.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '🧩', 'ru': {'title': 'Frac-Connections: оптимизация глубоких нейросетей без расширения скрытых состояний', 'desc': 'Статья представляет новый подход к архитектуре глубоких нейронных сетей под названием Frac-Connections. Этот метод развивает идею остаточных соединений (residual connections), разделяя скрытые состояния на несколько частей вместо расширения их ширины. Frac-Connections сохраняют преимущества Hyper-Connections, при этом снижая потребление памяти. Эффективность подхода подтверждена масштабными экспериментами на языковых задачах, включая обучение модели MoE размером 7 миллиардов параметров на 3 триллионах токенов.'}, 'en': {'title': 'Frac-Connections: Efficient Memory for Deep Learning', 'desc': 'This paper introduces Frac-Connections, a new method that improves upon Hyper-Connections in deep learning. While Hyper-Connections allow for multiple connection strengths to combat issues like gradient vanishing, they also increase memory usage due to wider hidden states. Frac-Connections address this by splitting hidden states into smaller parts, maintaining some advantages of Hyper-Connections while reducing memory costs. The authors demonstrate the effectiveness of Frac-Connections through extensive experiments on language tasks, showing superior performance compared to traditional residual connections.'}, 'zh': {'title': 'Frac-Connections：优化深度学习的内存使用', 'desc': '残差连接是现代深度学习架构的核心，能够通过减轻梯度消失问题来训练非常深的网络。超连接最近通过在不同深度引入多个连接强度来推广残差连接，从而解决了梯度消失与表示崩溃之间的摇摆效应。然而，超连接通过扩展隐藏状态的宽度增加了内存访问成本。我们提出了Frac-Connections，这是一种新方法，通过将隐藏状态划分为多个部分而不是扩展其宽度，保留了超连接的部分优势，同时减少了内存消耗。'}}}, {'id': 'https://huggingface.co/papers/2503.14492', 'title': 'Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control', 'url': 'https://huggingface.co/papers/2503.14492', 'abstract': 'We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.', 'score': 12, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '5098c043161888fa', 'authors': ['NVIDIA', ':', 'Hassan Abu Alhaija', 'Jose Alvarez', 'Maciej Bala', 'Tiffany Cai', 'Tianshi Cao', 'Liz Cha', 'Joshua Chen', 'Mike Chen', 'Francesco Ferroni', 'Sanja Fidler', 'Dieter Fox', 'Yunhao Ge', 'Jinwei Gu', 'Ali Hassani', 'Michael Isaev', 'Pooya Jannaty', 'Shiyi Lan', 'Tobias Lasser', 'Huan Ling', 'Ming-Yu Liu', 'Xian Liu', 'Yifan Lu', 'Alice Luo', 'Qianli Ma', 'Hanzi Mao', 'Fabio Ramos', 'Xuanchi Ren', 'Tianchang Shen', 'Shitao Tang', 'Ting-Chun Wang', 'Jay Wu', 'Jiashu Xu', 'Stella Xu', 'Kevin Xie', 'Yuchong Ye', 'Xiaodong Yang', 'Xiaohui Zeng', 'Yu Zeng'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.14492.jpg', 'data': {'categories': ['#agents', '#robotics', '#transfer_learning', '#open_source', '#inference', '#3d'], 'emoji': '🌌', 'ru': {'title': 'Адаптивная генерация миров с пространственным контролем', 'desc': 'Cosmos-Transfer - это условная модель генерации миров, способная создавать симуляции на основе нескольких пространственных входных данных разных модальностей. Модель позволяет гибко настраивать веса различных условных входов в разных пространственных локациях. Это обеспечивает высоко контролируемую генерацию миров и находит применение в различных сценариях переноса между мирами, включая Sim2Real. Авторы провели обширные оценки модели и продемонстрировали ее применение для физического ИИ, в том числе для робототехники Sim2Real и обогащения данных для беспилотных автомобилей.'}, 'en': {'title': 'Empowering World Generation with Adaptive Control', 'desc': 'Cosmos-Transfer is a novel model designed for generating world simulations using various spatial control inputs like segmentation, depth, and edge maps. It features an adaptive and customizable spatial conditional scheme that allows for different weights to be assigned to inputs based on their spatial locations. This flexibility enhances the controllability of world generation, making it suitable for applications such as Sim2Real in robotics and autonomous vehicles. The model has been extensively evaluated, and its code is open-sourced to foster further research in the field.'}, 'zh': {'title': '可控的世界生成，助力物理人工智能', 'desc': '我们介绍了Cosmos-Transfer，这是一种条件世界生成模型，可以根据多种空间控制输入（如分割、深度和边缘）生成世界模拟。在设计上，空间条件方案是自适应和可定制的，允许在不同空间位置对不同条件输入进行加权。这使得世界生成具有高度可控性，并在多种世界到世界的转移应用中找到用途，包括Sim2Real。我们进行了广泛的评估，分析了所提出的模型，并展示了其在物理人工智能中的应用，包括机器人Sim2Real和自动驾驶车辆数据增强。'}}}, {'id': 'https://huggingface.co/papers/2503.14504', 'title': 'Aligning Multimodal LLM with Human Preference: A Survey', 'url': 'https://huggingface.co/papers/2503.14504', 'abstract': 'Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.', 'score': 10, 'issue_id': 2778, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'b33bcba515cfa942', 'authors': ['Tao Yu', 'Yi-Fan Zhang', 'Chaoyou Fu', 'Junkang Wu', 'Jinda Lu', 'Kun Wang', 'Xingyu Lu', 'Yunhang Shen', 'Guibin Zhang', 'Dingjie Song', 'Yibo Yan', 'Tianlong Xu', 'Qingsong Wen', 'Zhang Zhang', 'Yan Huang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['Institute of automation, Chinese academy of science', 'Lehigh University', 'Nanjing University', 'Nanyang Technological University', 'National University of Singapore', 'Shenzhen International Graduate School, Tsinghua University', 'Squirrel Ai Learning', 'Tencent Youtu Lab', 'The Hong Kong University of Science and Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14504.jpg', 'data': {'categories': ['#survey', '#multimodal', '#benchmark', '#dataset', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Выравнивание мультимодальных ИИ: путь к безопасности и эффективности', 'desc': 'Эта статья представляет собой всесторонний обзор алгоритмов выравнивания для мультимодальных больших языковых моделей (MLLM). Авторы рассматривают различные сценарии применения этих алгоритмов, включая понимание изображений, видео и аудио. Они анализируют ключевые факторы в создании наборов данных для выравнивания и обсуждают бенчмарки для оценки эффективности алгоритмов. Статья также предлагает потенциальные направления для будущих исследований в этой области.'}, 'en': {'title': 'Aligning MLLMs: Bridging Gaps for Better Understanding', 'desc': 'This paper reviews alignment algorithms for Multimodal Large Language Models (MLLMs), which integrate visual, auditory, and textual data. It highlights the challenges of truthfulness, safety, and reasoning in MLLMs, emphasizing the need for effective alignment strategies. The authors categorize alignment algorithms based on application scenarios, dataset construction, and evaluation benchmarks. The goal is to provide a structured overview that aids researchers in advancing alignment techniques for MLLMs.'}, 'zh': {'title': '对齐算法助力多模态语言模型的未来', 'desc': '大型语言模型（LLMs）能够通过简单的提示处理各种通用任务，而无需特定任务的训练。多模态大型语言模型（MLLMs）在处理涉及视觉、听觉和文本数据的复杂任务方面展现了令人印象深刻的潜力。然而，关于真实性、安全性、类o1推理和与人类偏好的对齐等关键问题仍未得到充分解决。本文旨在系统性地回顾MLLMs的对齐算法，探讨其应用场景、数据集构建核心因素、评估基准以及未来发展方向。'}}}, {'id': 'https://huggingface.co/papers/2503.14499', 'title': 'Measuring AI Ability to Complete Long Tasks', 'url': 'https://huggingface.co/papers/2503.14499', 'abstract': "Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.", 'score': 9, 'issue_id': 2777, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'c31aeeed7f6139af', 'authors': ['Thomas Kwa', 'Ben West', 'Joel Becker', 'Amy Deng', 'Katharyn Garcia', 'Max Hasin', 'Sami Jawhar', 'Megan Kinniment', 'Nate Rush', 'Sydney Von Arx', 'Ryan Bloom', 'Thomas Broadley', 'Haoxing Du', 'Brian Goodrich', 'Nikola Jurkovic', 'Luke Harold Miles', 'Seraphina Nix', 'Tao Lin', 'Neev Parikh', 'David Rein', 'Lucas Jun Koba Sato', 'Hjalmar Wijk', 'Daniel M. Ziegler', 'Elizabeth Barnes', 'Lawrence Chan'], 'affiliations': ['Anthropic', 'Model Evaluation & Threat Research (METR)'], 'pdf_title_img': 'assets/pdf/title_img/2503.14499.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#reasoning'], 'emoji': '⏳', 'ru': {'title': 'Время на вашей стороне: ИИ догоняет человека в скорости выполнения задач', 'desc': 'Статья предлагает новую метрику для оценки возможностей ИИ-систем - горизонт времени 50%-ного выполнения задач. Этот показатель измеряет время, которое обычно требуется людям для выполнения задач, которые модели ИИ могут выполнить с 50%-ной вероятностью успеха. Исследование показало, что современные передовые модели ИИ, такие как Claude 3.7 Sonnet, имеют горизонт времени около 50 минут. Авторы отмечают, что горизонт времени ИИ удваивается примерно каждые семь месяцев с 2019 года, и обсуждают потенциальные последствия повышения автономности ИИ-систем.'}, 'en': {'title': 'Measuring AI Progress: The 50%-Task-Completion Time Horizon', 'desc': 'This paper introduces a new metric called the 50%-task-completion time horizon to evaluate AI systems based on human performance. It measures the time it takes for humans to complete tasks that AI can accomplish with a 50% success rate. The study found that leading AI models, like Claude 3.7 Sonnet, have a time horizon of about 50 minutes, and this capability has been improving rapidly, doubling approximately every seven months. The authors highlight the implications of this trend, suggesting that AI could soon automate complex software tasks that currently require significant human effort.'}, 'zh': {'title': 'AI能力的新度量：50%任务完成时间', 'desc': '尽管人工智能在基准测试上取得了快速进展，但其实际表现的意义仍不明确。我们提出了一种新的度量标准：50%任务完成时间范围，旨在量化AI系统与人类能力的对比。通过对人类专家在多个任务上的完成时间进行测量，我们发现当前的前沿AI模型在这些任务上的50%时间范围约为50分钟。我们的研究表明，AI模型的时间范围自2019年以来大约每七个月翻倍，未来五年内，AI系统可能能够自动化许多目前需要人类一个月才能完成的软件任务。'}}}, {'id': 'https://huggingface.co/papers/2503.12355', 'title': 'Atlas: Multi-Scale Attention Improves Long Context Image Modeling', 'url': 'https://huggingface.co/papers/2503.12355', 'abstract': 'Efficiently modeling massive images is a long-standing challenge in machine learning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on two key ideas, (i) multi-scale representations (ii) bi-directional cross-scale communication. MSA creates O(log N) scales to represent the image across progressively coarser features and leverages cross-attention to propagate information across scales. We then introduce Atlas, a novel neural network architecture based on MSA. We demonstrate that Atlas significantly improves the compute-performance tradeoff of long-context image modeling in a high-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves 91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster. Atlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96% better than LongViT. In comparisons against MambaVision-S, we find Atlas-S achieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px respectively, while obtaining similar runtimes. Code for reproducing our experiments and pretrained models is available at https://github.com/yalalab/atlas.', 'score': 9, 'issue_id': 2791, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '0b78d0c7af506f20', 'authors': ['Kumar Krishna Agrawal', 'Long Lian', 'Longchao Liu', 'Natalia Harguindeguy', 'Boyi Li', 'Alexander Bick', 'Maggie Chung', 'Trevor Darrell', 'Adam Yala'], 'affiliations': ['University of California San Francisco', 'University of California, Berkeley', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12355.jpg', 'data': {'categories': ['#cv', '#open_source', '#architecture', '#optimization', '#long_context'], 'emoji': '🔍', 'ru': {'title': 'Масштабируемое внимание для эффективной обработки гигантских изображений', 'desc': 'Статья представляет новый подход к эффективному моделированию больших изображений - Multi-Scale Attention (MSA). MSA использует многомасштабные представления и двунаправленную коммуникацию между масштабами, создавая O(log N) уровней для представления изображения. На основе MSA авторы разработали нейросетевую архитектуру Atlas, которая значительно улучшает соотношение производительности и вычислительных затрат при моделировании изображений с большим контекстом. Atlas демонстрирует высокую точность и скорость работы по сравнению с современными моделями на задаче классификации изображений высокого разрешения.'}, 'en': {'title': 'Revolutionizing Image Modeling with Multi-Scale Attention', 'desc': 'This paper presents Multi-Scale Attention (MSA), a method designed to efficiently model large images in machine learning. MSA utilizes multi-scale representations and bi-directional cross-scale communication to enhance image feature extraction. The authors introduce Atlas, a neural network architecture that implements MSA, achieving significant improvements in speed and accuracy for high-resolution image tasks. Atlas outperforms existing models like ConvNext and FasterViT, demonstrating a superior compute-performance tradeoff while maintaining high accuracy on the ImageNet dataset.'}, 'zh': {'title': '高效图像建模的新突破：多尺度注意力', 'desc': '在机器学习中，高效建模大规模图像一直是一个挑战。为此，我们提出了多尺度注意力（MSA），它依赖于多尺度表示和双向跨尺度通信两个关键思想。MSA通过创建O(log N)的尺度来表示图像，并利用交叉注意力在不同尺度之间传播信息。我们还介绍了一种基于MSA的新型神经网络架构Atlas，实验表明Atlas在高分辨率图像建模中显著提高了计算性能的平衡。'}}}, {'id': 'https://huggingface.co/papers/2503.10522', 'title': 'AudioX: Diffusion Transformer for Anything-to-Audio Generation', 'url': 'https://huggingface.co/papers/2503.10522', 'abstract': 'Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/', 'score': 9, 'issue_id': 2791, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'c3590459e5737c75', 'authors': ['Zeyue Tian', 'Yizhu Jin', 'Zhaoyang Liu', 'Ruibin Yuan', 'Xu Tan', 'Qifeng Chen', 'Wei Xue', 'Yike Guo'], 'affiliations': ['Hong Kong University of Science and Technology', 'Moonshot AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.10522.jpg', 'data': {'categories': ['#synthetic', '#architecture', '#diffusion', '#audio', '#multimodal', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'AudioX: универсальная модель для генерации аудио из чего угодно', 'desc': 'AudioX - это унифицированная модель Diffusion Transformer для генерации аудио и музыки на основе различных входных данных. Модель использует стратегию мультимодального маскированного обучения для создания надежных кросс-модальных представлений. Для обучения были созданы два больших набора данных: vggsound-caps и V2M-caps. Эксперименты показывают, что AudioX не уступает специализированным моделям и обладает высокой универсальностью в обработке различных модальностей входных данных.'}, 'en': {'title': 'AudioX: Unifying Audio and Music Generation Across Modalities', 'desc': 'This paper introduces AudioX, a novel Diffusion Transformer model designed for generating audio and music from various input types. Unlike traditional models that focus on specific tasks, AudioX integrates multiple modalities, allowing it to process text, video, images, and audio seamlessly. The model employs a unique multi-modal masked training strategy, which enhances its ability to learn robust representations by masking inputs across different modalities. Additionally, the authors address the challenge of limited high-quality training data by creating two extensive datasets, enabling AudioX to outperform existing specialized models in versatility and performance.'}, 'zh': {'title': '统一音频与音乐生成的创新模型', 'desc': '音频和音乐生成在许多应用中变得越来越重要，但现有方法存在显著局限性。我们提出了AudioX，这是一种统一的扩散变换器模型，能够实现多种输入到音频和音乐的生成。AudioX的创新之处在于其多模态掩蔽训练策略，使模型能够从不同模态的掩蔽输入中学习，从而生成高质量的音频和音乐。通过构建两个全面的数据集，AudioX在处理多样化输入模态和生成任务方面展现了卓越的灵活性和性能。'}}}, {'id': 'https://huggingface.co/papers/2503.12505', 'title': 'MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process\n  Errors Identification', 'url': 'https://huggingface.co/papers/2503.12505', 'abstract': 'Reasoning is an essential capacity for large language models (LLMs) to address complex tasks, where the identification of process errors is vital for improving this ability. Recently, process-level reward models (PRMs) were proposed to provide step-wise rewards that facilitate reinforcement learning and data production during training and guide LLMs toward correct steps during inference, thereby improving reasoning accuracy. However, existing benchmarks of PRMs are text-based and focus on error detection, neglecting other scenarios like reasoning search. To address this gap, we introduce MPBench, a comprehensive, multi-task, multimodal benchmark designed to systematically assess the effectiveness of PRMs in diverse scenarios. MPBench employs three evaluation paradigms, each targeting a specific role of PRMs in the reasoning process: (1) Step Correctness, which assesses the correctness of each intermediate reasoning step; (2) Answer Aggregation, which aggregates multiple solutions and selects the best one; and (3) Reasoning Process Search, which guides the search for optimal reasoning steps during inference. Through these paradigms, MPBench makes comprehensive evaluations and provides insights into the development of multimodal PRMs.', 'score': 8, 'issue_id': 2776, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '0ac4fdd3411855ac', 'authors': ['Zhaopan Xu', 'Pengfei Zhou', 'Jiaxin Ai', 'Wangbo Zhao', 'Kai Wang', 'Xiaojiang Peng', 'Wenqi Shao', 'Hongxun Yao', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'NUS', 'SZTU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.12505.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#rl'], 'emoji': '🧠', 'ru': {'title': 'MPBench: Комплексная оценка рассуждений языковых моделей', 'desc': 'Статья представляет новый бенчмарк MPBench для оценки эффективности моделей вознаграждения на уровне процесса (PRM) в различных сценариях рассуждений. MPBench включает три парадигмы оценки: корректность шагов, агрегацию ответов и поиск процесса рассуждений. Бенчмарк охватывает мультимодальные задачи и позволяет всесторонне оценить PRM в контексте рассуждений языковых моделей. Это помогает улучшить способности больших языковых моделей (LLM) к рассуждениям и выявлению ошибок в процессе.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with MPBench', 'desc': "This paper discusses the importance of reasoning in large language models (LLMs) and introduces process-level reward models (PRMs) to enhance their reasoning capabilities. PRMs provide step-wise rewards that help LLMs learn from their mistakes during training and improve their decision-making during inference. The authors identify a gap in existing benchmarks, which primarily focus on error detection, and propose MPBench, a new benchmark that evaluates PRMs across various reasoning scenarios. MPBench includes three evaluation paradigms: assessing step correctness, aggregating answers, and guiding the reasoning process search, offering a comprehensive framework for understanding PRMs' effectiveness."}, 'zh': {'title': '全面评估推理过程的多模态基准', 'desc': '推理是大型语言模型（LLMs）处理复杂任务的重要能力，而识别过程错误对于提升这一能力至关重要。最近提出的过程级奖励模型（PRMs）通过提供逐步奖励，促进了强化学习和数据生成，从而在推理过程中引导LLMs走向正确的步骤，提高推理准确性。然而，现有的PRMs基准主要基于文本，专注于错误检测，忽视了推理搜索等其他场景。为了解决这一问题，我们引入了MPBench，这是一个全面的多任务多模态基准，旨在系统评估PRMs在不同场景中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.13265', 'title': 'FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View\n  Synthesis', 'url': 'https://huggingface.co/papers/2503.13265', 'abstract': 'Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from a coarse scene, and (2) a progressive expansion process to construct a complete 3D scene. In particular, leveraging an advanced pre-trained video model and accurate depth-estimated training pairs, our V2V model can generate novel views under large camera pose variations. Building upon it, FlexWorld progressively generates new 3D content and integrates it into the global scene through geometry-aware scene fusion. Extensive experiments demonstrate the effectiveness of FlexWorld in generating high-quality novel view videos and flexible-view 3D scenes from single images, achieving superior visual quality under multiple popular metrics and datasets compared to existing state-of-the-art methods. Qualitatively, we highlight that FlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg} rotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.', 'score': 7, 'issue_id': 2785, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '8b68e5c333f5fe62', 'authors': ['Luxi Chen', 'Zihan Zhou', 'Min Zhao', 'Yikai Wang', 'Ge Zhang', 'Wenhao Huang', 'Hao Sun', 'Ji-Rong Wen', 'Chongxuan Li'], 'affiliations': ['Beijing Key Laboratory of Big Data Management and Analysis Methods', 'ByteDance', 'Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch MLCenter, Tsinghua University', 'Gaoling School of AI, Renmin University of China', 'School of Artificial Intelligence, Beijing Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13265.jpg', 'data': {'categories': ['#diffusion', '#3d', '#video'], 'emoji': '🌐', 'ru': {'title': 'Создание гибких 3D-миров из одного изображения', 'desc': 'FlexWorld - это новая система для создания гибких 3D-сцен из одиночных изображений, включая вращение на 360° и масштабирование. Она состоит из мощной модели диффузии видео-в-видео для генерации высококачественных изображений с новых ракурсов и процесса прогрессивного расширения для построения полной 3D-сцены. FlexWorld использует предобученную видеомодель и точные пары обучающих данных с оценкой глубины для генерации видов при значительных изменениях положения камеры. Эксперименты показывают превосходство FlexWorld над существующими методами в генерации высококачественных видео с новых ракурсов и гибких 3D-сцен из одиночных изображений.'}, 'en': {'title': 'Transforming Single Images into Dynamic 3D Worlds', 'desc': 'FlexWorld is a new framework designed to create flexible 3D scenes from single images, addressing the challenge of limited 3D data. It features a video-to-video (V2V) diffusion model that generates high-quality images from incomplete scenes, allowing for significant camera pose variations. Additionally, it employs a progressive expansion process to build a complete 3D environment, integrating new content through geometry-aware scene fusion. Experiments show that FlexWorld outperforms existing methods in generating high-fidelity scenes with flexible viewing options like 360-degree rotations and zooming.'}, 'zh': {'title': 'FlexWorld：从单图像生成灵活视角3D场景的创新框架', 'desc': 'FlexWorld是一个新颖的框架，旨在从单张图像生成灵活视角的3D场景，包括360度旋转和缩放。该框架包含两个关键组件：一个强大的视频到视频（V2V）扩散模型，用于从粗糙场景渲染的缺失输入中生成高质量的新视图图像，以及一个渐进扩展过程，用于构建完整的3D场景。通过利用先进的预训练视频模型和准确的深度估计训练对，V2V模型能够在大相机姿态变化下生成新视图。实验结果表明，FlexWorld在生成高质量新视图视频和灵活视角3D场景方面表现优越，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.13111', 'title': 'MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs', 'url': 'https://huggingface.co/papers/2503.13111', 'abstract': 'Multimodal large language models (MLLMs) excel at 2D visual understanding but remain limited in their ability to reason about 3D space. In this work, we leverage large-scale high-quality 3D scene data with open-set annotations to introduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation benchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data covers diverse spatial tasks including spatial relationship prediction, metric size and distance estimation, and 3D grounding. We show that CA-VQA enables us to train MM-Spatial, a strong generalist MLLM that also achieves state-of-the-art performance on 3D spatial understanding benchmarks, including our own. We show how incorporating metric depth and multi-view inputs (provided in CA-VQA) can further improve 3D understanding, and demonstrate that data alone allows our model to achieve depth perception capabilities comparable to dedicated monocular depth estimation models. We will publish our SFT dataset and benchmark.', 'score': 6, 'issue_id': 2785, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'ce045e2d94eafd19', 'authors': ['Erik Daxberger', 'Nina Wenzel', 'David Griffiths', 'Haiming Gang', 'Justin Lazarow', 'Gefen Kohavi', 'Kai Kang', 'Marcin Eichner', 'Yinfei Yang', 'Afshin Dehghan', 'Peter Grasch'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.13111.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#3d', '#multimodal', '#benchmark', '#dataset'], 'emoji': '🏠', 'ru': {'title': 'Новый шаг к пониманию 3D-пространства искусственным интеллектом', 'desc': 'Статья представляет новый подход к обучению мультимодальных языковых моделей пониманию трехмерного пространства. Авторы создали набор данных CA-VQA для обучения и оценки моделей на задачах пространственного анализа в интерьерах. На основе этих данных была обучена модель MM-Spatial, показавшая высокие результаты в понимании 3D-пространства. Исследование демонстрирует, что включение метрической глубины и многоракурсных изображений улучшает пространственное понимание моделей.'}, 'en': {'title': 'Enhancing 3D Understanding in MLLMs with CA-VQA', 'desc': 'This paper addresses the limitations of multimodal large language models (MLLMs) in understanding 3D spaces. The authors introduce a new supervised fine-tuning dataset called Cubify Anything VQA (CA-VQA), which includes high-quality 3D scene data with open-set annotations. CA-VQA focuses on various spatial tasks such as predicting spatial relationships and estimating sizes and distances in indoor environments. The study demonstrates that their model, MM-Spatial, achieves state-of-the-art performance in 3D spatial understanding by leveraging the rich data from CA-VQA, enhancing depth perception capabilities.'}, 'zh': {'title': '提升三维空间理解的多模态语言模型', 'desc': '这篇论文介绍了一种新的多模态大型语言模型（MLLM），它在理解二维视觉方面表现出色，但在三维空间推理上仍然有限。研究者利用大规模高质量的三维场景数据，创建了一个新的监督微调数据集和评估基准，专注于室内场景。新数据集Cubify Anything VQA（CA-VQA）涵盖了多种空间任务，包括空间关系预测、度量大小和距离估计以及三维定位。研究表明，通过结合度量深度和多视角输入，模型在三维理解方面的表现得到了显著提升，甚至达到了专用单目深度估计模型的能力。'}}}, {'id': 'https://huggingface.co/papers/2503.12271', 'title': 'Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion\n  Transformers via In-Context Reflection', 'url': 'https://huggingface.co/papers/2503.12271', 'abstract': 'The predominant approach to advancing text-to-image generation has been training-time scaling, where larger models are trained on more data using greater computational resources. While effective, this approach is computationally expensive, leading to growing interest in inference-time scaling to improve performance. Currently, inference-time scaling for text-to-image diffusion models is largely limited to best-of-N sampling, where multiple images are generated per prompt and a selection model chooses the best output. Inspired by the recent success of reasoning models like DeepSeek-R1 in the language domain, we introduce an alternative to naive best-of-N sampling by equipping text-to-image Diffusion Transformers with in-context reflection capabilities. We propose Reflect-DiT, a method that enables Diffusion Transformers to refine their generations using in-context examples of previously generated images alongside textual feedback describing necessary improvements. Instead of passively relying on random sampling and hoping for a better result in a future generation, Reflect-DiT explicitly tailors its generations to address specific aspects requiring enhancement. Experimental results demonstrate that Reflect-DiT improves performance on the GenEval benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it achieves a new state-of-the-art score of 0.81 on GenEval while generating only 20 samples per prompt, surpassing the previous best score of 0.80, which was obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples under the best-of-N approach.', 'score': 6, 'issue_id': 2778, 'pub_date': '2025-03-15', 'pub_date_card': {'ru': '15 марта', 'en': 'March 15', 'zh': '3月15日'}, 'hash': '2f560e2ec0839955', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Akash Gokul', 'Arsh Koneru', 'Yusuke Kato', 'Kazuki Kozuka', 'Aditya Grover'], 'affiliations': ['Panasonic AI Research', 'Salesforce AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2503.12271.jpg', 'data': {'categories': ['#optimization', '#cv', '#benchmark', '#inference', '#diffusion', '#reasoning'], 'emoji': '🖼️', 'ru': {'title': 'Reflect-DiT: Умное улучшение генерации изображений через анализ и рефлексию', 'desc': 'Эта статья представляет новый метод Reflect-DiT для улучшения генерации изображений по текстовому описанию. В отличие от традиционного подхода best-of-N, Reflect-DiT позволяет моделям Diffusion Transformer анализировать ранее сгенерированные изображения и текстовую обратную связь для улучшения результатов. Метод показывает улучшение производительности на бенчмарке GenEval, достигая нового рекордного результата 0.81. Reflect-DiT демонстрирует эффективность подхода inference-time scaling для повышения качества генерации изображений.'}, 'en': {'title': 'Refining Image Generation with Reflective Learning', 'desc': 'This paper presents Reflect-DiT, a novel method for enhancing text-to-image generation by incorporating in-context reflection capabilities into Diffusion Transformers. Unlike traditional best-of-N sampling, which generates multiple images and selects the best, Reflect-DiT refines its outputs based on previous generations and textual feedback. This approach allows the model to focus on specific areas for improvement, leading to more tailored and effective image generation. Experimental results show that Reflect-DiT achieves a new state-of-the-art performance on the GenEval benchmark with fewer samples, demonstrating its efficiency compared to larger models.'}, 'zh': {'title': '反思生成，提升图像质量！', 'desc': '本文提出了一种新的文本到图像生成方法，称为Reflect-DiT。该方法通过在生成过程中引入上下文反思能力，帮助Diffusion Transformers根据之前生成的图像和文本反馈进行改进。与传统的最佳N采样方法不同，Reflect-DiT能够针对特定的改进需求进行调整，从而提高生成质量。实验结果表明，Reflect-DiT在GenEval基准测试中表现优异，达到了新的最先进分数。'}}}, {'id': 'https://huggingface.co/papers/2503.14495', 'title': 'Temporal Consistency for LLM Reasoning Process Error Identification', 'url': 'https://huggingface.co/papers/2503.14495', 'abstract': 'Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency', 'score': 5, 'issue_id': 2779, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '590d8cdf2ae26151', 'authors': ['Jiacheng Guo', 'Yue Wu', 'Jiahao Qiu', 'Kaixuan Huang', 'Xinzhe Juan', 'Ling Yang', 'Mengdi Wang'], 'affiliations': ['AI Lab, Princeton University', 'Department of Computer Science & Engineering, University of Michigan', 'Department of Electrical & Computer Engineering, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14495.jpg', 'data': {'categories': ['#math', '#benchmark', '#optimization', '#reasoning', '#training', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'Повышение точности верификации математических рассуждений через временную согласованность', 'desc': 'Статья представляет новый метод временной согласованности для верификации математических рассуждений. В отличие от одноразовой проверки или подходов с участием нескольких моделей, этот метод использует последовательность действий самоанализа для повышения точности верификации. Эмпирические оценки на различных эталонных тестах по выявлению ошибок в математических процессах показали стабильное улучшение производительности по сравнению с базовыми методами. При применении к недавно дистиллированным моделям DeepSeek R1 метод продемонстрировал высокую эффективность, позволив моделям 7B/8B превзойти все модели 70B/72B и GPT-4o на тесте ProcessBench.'}, 'en': {'title': 'Enhancing Verification Accuracy through Temporal Consistency', 'desc': 'This paper introduces a novel temporal consistency method for enhancing verification in mathematical reasoning. The approach allows verifiers to iteratively refine their judgments based on previous assessments, improving accuracy over traditional one-round verification methods. By utilizing a sequence of self-reflection actions, the method shows significant performance gains on various benchmarks for identifying mathematical process errors. Notably, it enables smaller distilled models to outperform larger models, demonstrating its effectiveness in practical applications.'}, 'zh': {'title': '提升数学验证的时间一致性方法', 'desc': '本文提出了一种新的时间一致性验证方法，旨在提高数学推理的有效性。该方法通过迭代地根据之前的评估来细化判断，克服了单轮验证和多模型辩论的局限性。通过在多个数学过程错误识别基准（如Mathcheck、ProcessBench和PRM800K）上的实证评估，显示出相较于基线方法的一致性性能提升。应用于最新的DeepSeek R1蒸馏模型时，我们的方法使得7B/8B蒸馏模型在ProcessBench上超越了所有70B/72B模型和GPT-4o。'}}}, {'id': 'https://huggingface.co/papers/2503.14151', 'title': 'Concat-ID: Towards Universal Identity-Preserving Video Synthesis', 'url': 'https://huggingface.co/papers/2503.14151', 'abstract': "We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs Variational Autoencoders to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms without the need for additional modules. A novel cross-video pairing strategy and a multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-ID's superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes a new benchmark for identity-preserving video synthesis, providing a versatile and scalable solution for a wide range of applications.", 'score': 5, 'issue_id': 2783, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'ba38580b0d116018', 'authors': ['Yong Zhong', 'Zhuoyi Yang', 'Jiayan Teng', 'Xiaotao Gu', 'Chongxuan Li'], 'affiliations': ['Gaoling School of AI, Renmin University of China, Beijing, China', 'Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.14151.jpg', 'data': {'categories': ['#benchmark', '#3d', '#video'], 'emoji': '🎬', 'ru': {'title': 'Concat-ID: Новый стандарт в генерации видео с сохранением идентичности', 'desc': 'Concat-ID - это унифицированная система для генерации видео с сохранением идентичности. Она использует вариационные автоэнкодеры для извлечения признаков изображений и 3D механизмы самовнимания для обработки видеопоследовательностей. Предложена новая стратегия сопоставления видео и многоэтапное обучение для баланса между сохранением идентичности и редактируемостью лиц. Эксперименты показывают превосходство Concat-ID над существующими методами в генерации видео с одним и несколькими персонажами.'}, 'en': {'title': 'Identity-Preserving Video Generation Made Easy with Concat-ID', 'desc': 'Concat-ID is a new framework designed for generating videos that maintain the identity of subjects. It uses Variational Autoencoders to capture important image features and combines them with video data using 3D self-attention, simplifying the process without extra components. The framework introduces a unique method for pairing videos and a multi-stage training approach to ensure that the generated videos are both consistent in identity and editable, while also looking natural. Through extensive testing, Concat-ID has shown to outperform existing techniques in generating videos with single and multiple identities, making it adaptable for various applications like virtual try-ons and customizable backgrounds.'}, 'zh': {'title': 'Concat-ID：身份一致性视频生成的新标杆', 'desc': 'Concat-ID 是一个统一的框架，用于生成保持身份一致性的视频。它使用变分自编码器提取图像特征，并将这些特征与视频潜在变量在序列维度上进行连接，完全依赖于 3D 自注意力机制，而无需额外模块。该方法引入了一种新颖的跨视频配对策略和多阶段训练方案，以平衡身份一致性和面部可编辑性，同时提高视频的自然性。实验结果表明，Concat-ID 在单一和多身份生成方面优于现有方法，并且能够无缝扩展到多主体场景，包括虚拟试穿和背景可控生成。'}}}, {'id': 'https://huggingface.co/papers/2503.12545', 'title': 'PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for\n  Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2503.12545', 'abstract': 'In recent years, Multimodal Large Language Models (MLLMs) have demonstrated remarkable advancements in tasks such as visual question answering, visual understanding, and reasoning. However, this impressive progress relies on vast amounts of data collected from the internet, raising significant concerns about privacy and security. To address these issues, machine unlearning (MU) has emerged as a promising solution, enabling the removal of specific knowledge from an already trained model without requiring retraining from scratch. Although MU for MLLMs has gained attention, current evaluations of its efficacy remain incomplete, and the underlying problem is often poorly defined, which hinders the development of strategies for creating more secure and trustworthy systems. To bridge this gap, we introduce a benchmark, named PEBench, which includes a dataset of personal entities and corresponding general event scenes, designed to comprehensively assess the performance of MU for MLLMs. Through PEBench, we aim to provide a standardized and robust framework to advance research in secure and privacy-preserving multimodal models. We benchmarked 6 MU methods, revealing their strengths and limitations, and shedding light on key challenges and opportunities for MU in MLLMs.', 'score': 4, 'issue_id': 2776, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '8a908fbc8ce24853', 'authors': ['Zhaopan Xu', 'Pengfei Zhou', 'Weidong Tang', 'Jiaxin Ai', 'Wangbo Zhao', 'Xiaojiang Peng', 'Kai Wang', 'Yang You', 'Wenqi Shao', 'Hongxun Yao', 'Kaipeng Zhang'], 'affiliations': ['HIT', 'NUS', 'SZTU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'XDU'], 'pdf_title_img': 'assets/pdf/title_img/2503.12545.jpg', 'data': {'categories': ['#multimodal', '#ethics', '#security', '#benchmark', '#dataset'], 'emoji': '🔒', 'ru': {'title': 'PEBench: Новый стандарт для оценки безопасности мультимодальных ИИ-моделей', 'desc': 'Статья представляет новый бенчмарк PEBench для оценки эффективности методов машинного разобучения (MU) в мультимодальных больших языковых моделях (MLLM). PEBench включает набор данных с личными сущностями и соответствующими общими сценами событий. Авторы протестировали 6 методов MU, выявив их сильные и слабые стороны. Исследование направлено на продвижение разработки безопасных и конфиденциальных мультимодальных моделей.'}, 'en': {'title': 'Enhancing Privacy in Multimodal Models with Machine Unlearning', 'desc': 'This paper discusses the advancements of Multimodal Large Language Models (MLLMs) in tasks like visual question answering and reasoning, while highlighting the privacy concerns associated with their data usage. It introduces machine unlearning (MU) as a method to selectively remove knowledge from these models without complete retraining. The authors present PEBench, a benchmark designed to evaluate MU methods specifically for MLLMs, using a dataset of personal entities and general event scenes. By benchmarking six MU techniques, the study identifies their strengths and weaknesses, aiming to enhance the security and trustworthiness of multimodal systems.'}, 'zh': {'title': '推动多模态模型的安全与隐私保护', 'desc': '近年来，多模态大型语言模型（MLLMs）在视觉问答、视觉理解和推理等任务上取得了显著进展。然而，这些进展依赖于从互联网收集的大量数据，这引发了隐私和安全方面的重大担忧。为了解决这些问题，机器遗忘（MU）作为一种有前景的解决方案应运而生，能够在不需要从头开始重新训练的情况下，从已训练的模型中删除特定知识。我们引入了一个基准测试PEBench，旨在全面评估MU在MLLMs中的表现，推动安全和隐私保护的多模态模型研究。'}}}, {'id': 'https://huggingface.co/papers/2503.12303', 'title': 'Towards Self-Improving Systematic Cognition for Next-Generation\n  Foundation MLLMs', 'url': 'https://huggingface.co/papers/2503.12303', 'abstract': "Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) face challenges with fine-grained perception and complex reasoning. Prevalent multimodal pre-training approaches focus on enhancing perception by training on high-quality image captions due to the extremely high cost of collecting chain-of-thought (CoT) reasoning data for improving reasoning. While leveraging advanced MLLMs for caption generation enhances scalability, the outputs often lack comprehensiveness and accuracy. In this paper, we introduce Self-Improving cognition (SIcog), a self-learning framework designed to construct next-generation foundation MLLMs by enhancing their systematic cognitive capabilities through multimodal pre-training with self-generated data. Specifically, we propose Chain-of-Description, an approach that improves an MLLM's systematic perception by enabling step-by-step visual understanding, ensuring greater comprehensiveness and accuracy. Additionally, we adopt a structured CoT reasoning technique to enable MLLMs to integrate in-depth multimodal reasoning. To construct a next-generation foundation MLLM with self-improved cognition, SIcog first equips an MLLM with systematic perception and reasoning abilities using minimal external annotations. The enhanced models then generate detailed captions and CoT reasoning data, which are further curated through self-consistency. This curated data is ultimately used for multimodal pre-training to develop next-generation foundation models. Extensive experiments on both low- and high-resolution MLLMs across diverse benchmarks demonstrate that, with merely 213K self-generated pre-training samples, SIcog produces next-generation foundation MLLMs with significantly improved cognition, achieving benchmark-leading performance compared to prevalent pre-training approaches.", 'score': 4, 'issue_id': 2787, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': 'd0f6251740b9c15c', 'authors': ['Xiaoying Zhang', 'Da Peng', 'Yipeng Zhang', 'Zonghao Guo', 'Chengyue Wu', 'Chi Chen', 'Wei Ke', 'Helen Meng', 'Maosong Sun'], 'affiliations': ['The Chinese University of Hong Kong', 'The University of Hong Kong', 'Tsinghua University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12303.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#transfer_learning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствующийся ИИ: новый подход к обучению мультимодальных языковых моделей', 'desc': 'Статья представляет SIcog - фреймворк для улучшения когнитивных способностей мультимодальных больших языковых моделей (MLLM). SIcog использует метод Chain-of-Description для пошагового понимания визуальной информации и структурированное рассуждение по цепочке для интеграции мультимодальных рассуждений. Фреймворк сначала обучает MLLM систематическому восприятию и рассуждению с минимальными внешними аннотациями, затем генерирует детальные подписи и данные для рассуждений. Эксперименты показывают, что SIcog значительно улучшает когнитивные способности MLLM, достигая ведущих результатов на различных бенчмарках.'}, 'en': {'title': 'Enhancing MLLMs with Self-Improving Cognition for Better Perception and Reasoning', 'desc': 'This paper addresses the limitations of Multimodal Large Language Models (MLLMs) in fine-grained perception and complex reasoning. It introduces Self-Improving cognition (SIcog), a framework that enhances MLLMs by using self-generated data for multimodal pre-training. The proposed Chain-of-Description method allows MLLMs to develop systematic visual understanding, improving the quality of generated captions and reasoning. Through minimal external annotations, SIcog enables MLLMs to achieve superior performance on various benchmarks with significantly fewer training samples.'}, 'zh': {'title': '自我改进认知，提升多模态模型的智能', 'desc': '尽管多模态大型语言模型（MLLMs）具有强大的能力，但在细粒度感知和复杂推理方面仍面临挑战。现有的多模态预训练方法主要通过高质量的图像描述来增强感知，但收集链式思维（CoT）推理数据的成本极高。本文提出了一种自我改进认知框架（SIcog），旨在通过自生成数据的多模态预训练来提升MLLM的系统认知能力。我们提出的描述链方法能够逐步增强视觉理解，确保生成的描述更全面、更准确，同时结合结构化的CoT推理技术，实现深入的多模态推理。'}}}, {'id': 'https://huggingface.co/papers/2503.09443', 'title': 'Florenz: Scaling Laws for Systematic Generalization in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2503.09443', 'abstract': 'Cross-lingual transfer enables vision-language models (VLMs) to perform vision tasks in various languages with training data only in one language. Current approaches rely on large pre-trained multilingual language models. However, they face the curse of multilinguality, sacrificing downstream task performance for multilingual capabilities, struggling with lexical ambiguities, and falling behind recent advances. In this work, we study the scaling laws of systematic generalization with monolingual VLMs for multilingual tasks, focusing on the impact of model size and seen training samples. We propose Florenz, a monolingual encoder-decoder VLM with 0.4B to 11.2B parameters combining the pre-trained VLM Florence-2 and the large language model Gemma-2. Florenz is trained with varying compute budgets on a synthetic dataset that features intentionally incomplete language coverage for image captioning, thus, testing generalization from the fully covered translation task. We show that not only does indirectly learning unseen task-language pairs adhere to a scaling law, but also that with our data generation pipeline and the proposed Florenz model family, image captioning abilities can emerge in a specific language even when only data for the translation task is available. Fine-tuning on a mix of downstream datasets yields competitive performance and demonstrates promising scaling trends in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy).', 'score': 4, 'issue_id': 2784, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'c895c390f49ed5c1', 'authors': ['Julian Spravil', 'Sebastian Houben', 'Sven Behnke'], 'affiliations': ['Fraunhofer IAIS, Germany', 'Lamarr Institute for Machine Learning and Artificial Intelligence, Germany', 'University of Applied Sciences Bonn-Rhein-Sieg, Germany', 'University of Bonn, Computer Science Institute VI, Center for Robotics, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.09443.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#multimodal', '#training', '#synthetic', '#machine_translation', '#long_context', '#transfer_learning'], 'emoji': '🌐', 'ru': {'title': 'Монолингвальные модели преодолевают языковые барьеры в мультимодальных задачах', 'desc': 'Статья исследует масштабируемость монолингвальных мультимодальных моделей для решения мультиязычных задач. Авторы предлагают модель Florenz, объединяющую предобученную мультимодальную модель Florence-2 и языковую модель Gemma-2. Модель обучается на синтетическом датасете с неполным языковым покрытием для генерации описаний изображений. Результаты показывают, что способность к описанию изображений на конкретном языке может появиться даже при наличии только данных для задачи перевода.'}, 'en': {'title': 'Florenz: Bridging Languages with Monolingual Vision-Language Models', 'desc': 'This paper introduces Florenz, a monolingual vision-language model (VLM) designed to enhance cross-lingual transfer for vision tasks. Unlike existing multilingual models that struggle with performance due to the complexities of multiple languages, Florenz leverages a single language to achieve systematic generalization across various languages. The model is built on a combination of the pre-trained VLM Florence-2 and the large language model Gemma-2, with a focus on scaling laws related to model size and training data. Results show that Florenz can effectively learn to perform image captioning in new languages, even when trained only on translation tasks, demonstrating strong performance in several downstream applications.'}, 'zh': {'title': '单语模型的跨语言能力新突破', 'desc': '这篇论文研究了跨语言迁移如何使视觉语言模型（VLMs）在仅用一种语言的训练数据下执行视觉任务。当前的方法依赖于大型预训练的多语言模型，但在多语言能力与下游任务性能之间存在权衡。我们提出了Florenz，一个单语编码器-解码器VLM，结合了预训练的Florence-2和大型语言模型Gemma-2，具有0.4B到11.2B的参数。通过在合成数据集上训练，Florenz展示了即使只有翻译任务的数据，也能在特定语言中产生图像描述能力。'}}}, {'id': 'https://huggingface.co/papers/2503.10905', 'title': 'Learning to Inference Adaptively for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2503.10905', 'abstract': 'Multimodal Large Language Models (MLLMs) have shown impressive capabilities in reasoning, yet come with substantial computational cost, limiting their deployment in resource-constrained settings. Despite recent efforts on improving the efficiency of MLLMs, prior solutions fall short in responding to varying runtime conditions, in particular changing resource availability (e.g., contention due to the execution of other programs on the device). To bridge this gap, we introduce AdaLLaVA, an adaptive inference framework that learns to dynamically reconfigure operations in an MLLM during inference, accounting for the input data and a latency budget. We conduct extensive experiments across benchmarks involving question-answering, reasoning, and hallucination. Our results show that AdaLLaVA effectively adheres to input latency budget, achieving varying accuracy and latency tradeoffs at runtime. Further, we demonstrate that AdaLLaVA adapts to both input latency and content, can be integrated with token selection for enhanced efficiency, and generalizes across MLLMs. Our project webpage with code release is at https://zhuoyan-xu.github.io/ada-llava/.', 'score': 2, 'issue_id': 2791, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'c1a89325eab3a9af', 'authors': ['Zhuoyan Xu', 'Khoi Duc Nguyen', 'Preeti Mukherjee', 'Saurabh Bagchi', 'Somali Chaterji', 'Yingyu Liang', 'Yin Li'], 'affiliations': ['Purdue University', 'The University of Hong Kong', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.10905.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#reasoning', '#optimization', '#hallucinations', '#multimodal', '#inference'], 'emoji': '⚡', 'ru': {'title': 'AdaLLaVA: Адаптивный вывод для эффективных мультимодальных языковых моделей', 'desc': 'AdaLLaVA - это адаптивная система вывода для мультимодальных больших языковых моделей (MLLM), которая динамически реконфигурирует операции во время выполнения с учетом входных данных и ограничений по задержке. Эта система позволяет эффективно использовать MLLM в условиях ограниченных ресурсов, адаптируясь к меняющимся условиям выполнения. Эксперименты показали, что AdaLLaVA успешно соблюдает заданные ограничения по задержке, достигая различных компромиссов между точностью и скоростью работы. Система также демонстрирует способность адаптироваться к входным данным и обобщаться на различные MLLM.'}, 'en': {'title': 'Adaptive Inference for Efficient Multimodal Language Models', 'desc': 'This paper presents AdaLLaVA, an adaptive inference framework designed for Multimodal Large Language Models (MLLMs) to optimize their performance under varying computational resources. It addresses the challenge of maintaining efficiency while responding to different runtime conditions, such as resource contention from other applications. AdaLLaVA dynamically adjusts the operations of MLLMs during inference based on the input data and a specified latency budget, allowing for flexible accuracy and latency trade-offs. The framework has been tested across various benchmarks, demonstrating its ability to adapt to both input characteristics and latency requirements, while also integrating with token selection for improved efficiency.'}, 'zh': {'title': '自适应推理，提升多模态模型效率', 'desc': '多模态大型语言模型（MLLMs）在推理方面表现出色，但其计算成本高，限制了在资源受限环境中的应用。尽管近期对提高MLLMs效率的努力有所增加，但现有解决方案在应对不同运行时条件方面仍显不足，特别是在资源可用性变化时。为了解决这个问题，我们提出了AdaLLaVA，这是一种自适应推理框架，能够在推理过程中动态重新配置MLLM的操作，考虑输入数据和延迟预算。我们的实验表明，AdaLLaVA能够有效遵循输入延迟预算，实现运行时的准确性和延迟之间的权衡。'}}}, {'id': 'https://huggingface.co/papers/2503.10546', 'title': 'KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for\n  Open-Vocabulary Robotic Manipulation', 'url': 'https://huggingface.co/papers/2503.10546', 'abstract': 'With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.', 'score': 2, 'issue_id': 2776, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'f856affbe8bcf064', 'authors': ['Zixian Liu', 'Mingtong Zhang', 'Yunzhu Li'], 'affiliations': ['Columbia University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10546.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#robotics', '#open_source', '#agents'], 'emoji': '🤖', 'ru': {'title': 'KUDA: Динамическое планирование для роботов с открытым словарем', 'desc': 'KUDA - это система манипуляции с открытым словарем, которая объединяет обучение динамике и визуальное управление через ключевые точки. Она использует модели видео-языкового взаимодействия (VLM) и нейронные модели динамики для планирования траекторий робота. KUDA сначала назначает ключевые точки на RGB-изображении и запрашивает VLM для генерации целевых спецификаций. Затем эти абстрактные представления на основе ключевых точек преобразуются в функции стоимости, которые оптимизируются с помощью обученной модели динамики.'}, 'en': {'title': 'KUDA: Bridging Language and Dynamics for Robotic Manipulation', 'desc': 'This paper presents KUDA, an innovative open-vocabulary robotic manipulation system that incorporates dynamics learning and visual prompting. Unlike previous methods, KUDA utilizes keypoints to create target specifications that are understandable by vision-language models (VLMs) and can be transformed into cost functions for planning. The system processes language instructions and visual data to assign keypoints to images, which are then optimized using a learned dynamics model to generate robotic movements. The effectiveness of KUDA is validated through various manipulation tasks, showcasing its ability to handle complex interactions with different object types.'}, 'zh': {'title': 'KUDA：动态学习与视觉提示的开放词汇操作系统', 'desc': '随着大语言模型（LLMs）和视觉语言模型（VLMs）的快速发展，开放词汇的机器人操作系统取得了显著进展。然而，许多现有方法忽视了物体动态的重要性，限制了它们在更复杂动态任务中的适用性。我们提出了KUDA，一个集成了动态学习和通过关键点进行视觉提示的开放词汇操作系统，利用了VLMs和基于学习的神经动态模型。KUDA通过将关键点分配给RGB图像，并查询VLM生成目标规范，将抽象的关键点表示转换为成本函数，从而优化机器人轨迹。'}}}, {'id': 'https://huggingface.co/papers/2503.10410', 'title': 'RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground\n  Simulation', 'url': 'https://huggingface.co/papers/2503.10410', 'abstract': 'Roadside Collaborative Perception refers to a system where multiple roadside units collaborate to pool their perceptual data, assisting vehicles in enhancing their environmental awareness. Existing roadside perception methods concentrate on model design but overlook data issues like calibration errors, sparse information, and multi-view consistency, leading to poor performance on recent published datasets. To significantly enhance roadside collaborative perception and address critical data issues, we present the first simulation framework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable of generating diverse, multi-view consistent simulated roadside data through dynamic foreground editing and full-scene style transfer of a single image. RoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures accurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View Occlusion-Aware Sampler (MOAS) determines the placement of diverse digital assets within 3D space; (3) DepthSAM innovatively models foreground-background relationships from single-frame fixed-view images, ensuring multi-view consistency of foreground; and (4) Scalable Post-Processing Toolkit generates more realistic and enriched scenes through style transfer and other enhancements. RoCo-Sim significantly improves roadside 3D object detection, outperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on TUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception simulation. Code and pre-trained models will be released soon: https://github.com/duyuwen-duen/RoCo-Sim', 'score': 2, 'issue_id': 2776, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '44150f611040e79d', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#3d', '#optimization', '#data', '#dataset', '#cv', '#open_source'], 'emoji': '🚗', 'ru': {'title': 'RoCo-Sim: прорыв в симуляции дорожного восприятия', 'desc': 'RoCo-Sim - это первая система симуляции для совместного восприятия дорожной обстановки. Она решает проблемы калибровки, разреженности данных и мультиракурсной согласованности путем генерации синтетических данных. RoCo-Sim включает оптимизацию внешних параметров камер, многоракурсный выборщик с учетом окклюзий, моделирование отношений передний план-фон и инструменты постобработки. Система значительно улучшает 3D-детектирование объектов, превосходя современные методы на популярных наборах данных.'}, 'en': {'title': 'Enhancing Roadside Awareness with Collaborative Perception', 'desc': 'This paper introduces Roadside Collaborative Perception, a system where multiple roadside units work together to improve vehicle awareness of their surroundings. It highlights the limitations of current methods that focus mainly on model design while neglecting important data issues such as calibration errors and sparse information. To address these challenges, the authors present RoCo-Sim, a simulation framework that generates diverse and consistent roadside data using advanced techniques like dynamic foreground editing. RoCo-Sim significantly enhances roadside 3D object detection performance, surpassing state-of-the-art methods on benchmark datasets.'}, 'zh': {'title': '提升路边感知的协同力量', 'desc': '路边协同感知是一种系统，多个路边单元协作汇聚感知数据，帮助车辆提高环境意识。现有的路边感知方法主要关注模型设计，但忽视了数据问题，如校准误差、信息稀疏和多视图一致性，导致在最新数据集上的表现不佳。为显著提升路边协同感知并解决关键数据问题，我们提出了首个路边协同感知模拟框架RoCo-Sim。RoCo-Sim能够通过动态前景编辑和单图像的全场景风格迁移生成多样化的、多视图一致的模拟路边数据。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.08893', 'title': 'EvalTree: Profiling Language Model Weaknesses via Hierarchical\n  Capability Trees', 'url': 'https://huggingface.co/papers/2503.08893', 'abstract': "An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for Language Model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, given an LM's performance on every individual instance in a benchmark. We introduce a suite of quantitative assessments to compare different weakness profiling methods. We also propose a weakness profiling method EvalTree. It constructs a capability tree where each node represents a capability described in natural language and is linked to a subset of benchmark instances that specifically evaluate this capability; it then extracts nodes where the LM performs poorly to generate a weakness profile. On the MATH and WildChat benchmarks, we show that EvalTree outperforms baseline weakness profiling methods by identifying weaknesses more precisely and comprehensively. Weakness profiling further enables weakness-guided data collection, and training data collection guided by EvalTree-identified weaknesses improves LM performance more than other data collection strategies. We also show how EvalTree exposes flaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate future work, we release our code and an interface that allows practitioners to interactively explore the capability trees built by EvalTree.", 'score': 2, 'issue_id': 2790, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '8645a737473c8209', 'authors': ['Zhiyuan Zeng', 'Yizhong Wang', 'Hannaneh Hajishirzi', 'Pang Wei Koh'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Paul G. Allen School of Computer Science & Engineering, University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.08893.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#data', '#interpretability', '#training', '#open_source'], 'emoji': '🌳', 'ru': {'title': 'EvalTree: точное профилирование слабостей языковых моделей', 'desc': 'Статья представляет метод EvalTree для создания профиля слабостей языковых моделей. EvalTree строит дерево возможностей, где каждый узел связан с подмножеством тестовых примеров, оценивающих конкретную способность модели. Метод превосходит базовые подходы в точности и полноте выявления слабостей на бенчмарках MATH и WildChat. Профилирование слабостей позволяет целенаправленно собирать обучающие данные, что улучшает производительность модели эффективнее других стратегий.'}, 'en': {'title': 'Uncovering Weaknesses for Stronger Language Models', 'desc': 'This paper focuses on improving the evaluation of Language Models (LMs) by creating a weakness profile that identifies specific areas where the model underperforms. The authors introduce EvalTree, a method that organizes model capabilities into a tree structure, linking each capability to benchmark instances that test it. By analyzing these capabilities, EvalTree generates a detailed profile of weaknesses, allowing for targeted improvements in model training. The results demonstrate that using EvalTree leads to better performance in LMs by guiding data collection based on identified weaknesses, surpassing traditional evaluation methods.'}, 'zh': {'title': '识别模型弱点，提升语言模型性能', 'desc': '本文提出了一种新的语言模型评估方法，旨在识别模型的弱点并提供改进建议。我们引入了弱点分析的概念，通过生成弱点档案来描述模型在基准测试中的表现。我们提出的EvalTree方法构建了一个能力树，能够更准确地识别模型的不足之处。通过在MATH和WildChat基准测试上的实验，EvalTree显示出比传统方法更优越的性能，并且能够指导数据收集以提升模型表现。'}}}, {'id': 'https://huggingface.co/papers/2503.14002', 'title': 'MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling', 'url': 'https://huggingface.co/papers/2503.14002', 'abstract': 'Generative models have recently made remarkable progress in the field of 3D objects. However, their practical application in fields like engineering remains limited since they fail to deliver the accuracy, quality, and controllability needed for domain-specific tasks. Fine-tuning large generative models is a promising perspective for making these models available in these fields. Creating high-quality, domain-specific 3D datasets is crucial for fine-tuning large generative models, yet the data filtering and annotation process remains a significant bottleneck. We present MeshFleet, a filtered and annotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive publicly available collection of 3D objects. Our approach proposes a pipeline for automated data filtering based on a quality classifier. This classifier is trained on a manually labeled subset of Objaverse, incorporating DINOv2 and SigLIP embeddings, refined through caption-based analysis and uncertainty estimation. We demonstrate the efficacy of our filtering method through a comparative analysis against caption and image aesthetic score-based techniques and fine-tuning experiments with SV3D, highlighting the importance of targeted data selection for domain-specific 3D generative modeling.', 'score': 1, 'issue_id': 2789, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '8affb04cd78c70c9', 'authors': ['Damian Boborzi', 'Phillip Mueller', 'Jonas Emrich', 'Dominik Schmid', 'Sebastian Mueller', 'Lars Mikelsons'], 'affiliations': ['BMW Group', 'University of Augsburg'], 'pdf_title_img': 'assets/pdf/title_img/2503.14002.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#optimization', '#data', '#3d'], 'emoji': '🚗', 'ru': {'title': 'MeshFleet: Автоматизированная фильтрация данных для 3D генеративных моделей транспортных средств', 'desc': 'Статья представляет MeshFleet - отфильтрованный и аннотированный набор данных 3D моделей транспортных средств, извлеченный из Objaverse-XL. Авторы предлагают автоматизированный пайплайн фильтрации данных на основе классификатора качества, обученного на вручную размеченном подмножестве Objaverse с использованием эмбеддингов DINOv2 и SigLIP. Эффективность метода фильтрации демонстрируется путем сравнительного анализа с методами, основанными на подписях и оценках эстетичности изображений. Исследование подчеркивает важность целенаправленного отбора данных для предметно-ориентированного 3D генеративного моделирования.'}, 'en': {'title': 'Enhancing 3D Generative Models with Targeted Data Selection', 'desc': "This paper discusses advancements in generative models for creating 3D objects, particularly focusing on their application in engineering. It highlights the challenges of achieving the necessary accuracy and quality for specific tasks, which can be addressed by fine-tuning these models with high-quality datasets. The authors introduce MeshFleet, a curated 3D vehicle dataset derived from Objaverse-XL, aimed at improving the training process of generative models. They also present a novel automated data filtering method using a quality classifier, which enhances the dataset's relevance and effectiveness for domain-specific applications."}, 'zh': {'title': 'MeshFleet：提升3D生成模型的关键数据集', 'desc': '生成模型在3D物体领域取得了显著进展，但在工程等领域的实际应用仍然有限，因为它们无法提供所需的准确性、质量和可控性。对大型生成模型进行微调是使这些模型在特定领域可用的有前景的方法。创建高质量、特定领域的3D数据集对于微调大型生成模型至关重要，但数据过滤和标注过程仍然是一个重大瓶颈。我们提出了MeshFleet，这是一个从Objaverse-XL提取的经过过滤和标注的3D车辆数据集，展示了基于质量分类器的自动化数据过滤方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.13661', 'title': 'Pensez: Less Data, Better Reasoning -- Rethinking French LLM', 'url': 'https://huggingface.co/papers/2503.13661', 'abstract': 'Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, achieving strong performance in specialized domains like mathematical reasoning and non-English languages often requires extensive training on massive datasets. This paper investigates a contrasting approach: strategic fine-tuning on a small, high-quality, bilingual (English-French) dataset to enhance both the reasoning capabilities and French language proficiency of a large language model. Rather than relying on scale, we explore the hypothesis that targeted data curation and optimized training can achieve competitive, or even superior, performance. We demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000 carefully selected samples, significant improvements in mathematical reasoning. Specifically, Pensez 7B exhibits an increase in accuracy of the base model up to 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark. These results challenge the prevailing assumption that massive datasets are aprerequisite for strong reasoning performance in LLMs, highlighting the potential of strategic data curation and optimized fine-tuning for enhancing both specialized skills and multilingual capabilities. Our findings have implications for the efficient development of high-performing, multilingual LLMs, especially in resource-constrained scenarios.', 'score': 1, 'issue_id': 2784, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '248eff76119a7839', 'authors': ['Huy Hoang Ha'], 'affiliations': ['Menlo Research', 'Université Grenoble Alpes'], 'pdf_title_img': 'assets/pdf/title_img/2503.13661.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multilingual', '#data', '#training', '#transfer_learning', '#low_resource'], 'emoji': '🧠', 'ru': {'title': 'Малые данные, большой результат: оптимизация языковых моделей без масштабирования', 'desc': 'Данная статья исследует стратегический подход к улучшению языковых моделей (LLM) путем тонкой настройки на небольшом, но качественном двуязычном наборе данных. Авторы демонстрируют значительное улучшение способностей модели к математическим рассуждениям и владению французским языком после обучения на всего 2000 тщательно отобранных примерах. Модель Pensez 7B показала увеличение точности до 20% на тесте AIME25 и на 12% на французском эквиваленте теста MATH уровня 5. Результаты исследования ставят под сомнение необходимость огромных наборов данных для достижения высокой производительности LLM в специализированных задачах.'}, 'en': {'title': 'Strategic Fine-Tuning: Small Data, Big Gains!', 'desc': 'This paper explores how to improve large language models (LLMs) in specialized areas like math and French without needing huge datasets. It focuses on strategic fine-tuning using a small, high-quality bilingual dataset of only 2,000 samples. The results show that this targeted approach can significantly boost reasoning accuracy, achieving up to a 20% improvement in mathematical tasks. This challenges the idea that only large datasets can lead to strong performance, suggesting that careful data selection and training can be more effective.'}, 'zh': {'title': '小数据集也能提升大型语言模型的推理能力', 'desc': '大型语言模型（LLMs）在自然语言处理任务中表现出色，但在数学推理和非英语语言等专业领域的表现通常需要大量数据的训练。本文探讨了一种不同的方法：通过在小规模高质量的双语（英语-法语）数据集上进行战略性微调，以提高大型语言模型的推理能力和法语水平。我们通过对仅2000个精心挑选的样本进行有针对性的监督微调（SFT），在数学推理上取得了显著的提升，Pensez 7B在AIME25上的准确率提高了20%，在法语MATH 5级基准上提高了12%。这些结果挑战了大规模数据集是LLMs强大推理性能的先决条件的普遍假设，突显了战略数据策划和优化微调在提升专业技能和多语言能力方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.12127', 'title': 'Hyperbolic Safety-Aware Vision-Language Models', 'url': 'https://huggingface.co/papers/2503.12127', 'abstract': "Addressing the retrieval of unsafe content from vision-language models such as CLIP is an important step towards real-world integration. Current efforts have relied on unlearning techniques that try to erase the model's knowledge of unsafe concepts. While effective in reducing unwanted outputs, unlearning limits the model's capacity to discern between safe and unsafe content. In this work, we introduce a novel approach that shifts from unlearning to an awareness paradigm by leveraging the inherent hierarchical properties of the hyperbolic space. We propose to encode safe and unsafe content as an entailment hierarchy, where both are placed in different regions of hyperbolic space. Our HySAC, Hyperbolic Safety-Aware CLIP, employs entailment loss functions to model the hierarchical and asymmetrical relations between safe and unsafe image-text pairs. This modelling, ineffective in standard vision-language models due to their reliance on Euclidean embeddings, endows the model with awareness of unsafe content, enabling it to serve as both a multimodal unsafe classifier and a flexible content retriever, with the option to dynamically redirect unsafe queries toward safer alternatives or retain the original output. Extensive experiments show that our approach not only enhances safety recognition but also establishes a more adaptable and interpretable framework for content moderation in vision-language models. Our source code is available at https://github.com/aimagelab/HySAC.", 'score': 1, 'issue_id': 2784, 'pub_date': '2025-03-15', 'pub_date_card': {'ru': '15 марта', 'en': 'March 15', 'zh': '3月15日'}, 'hash': 'fe42b5bcd1d5d4be', 'authors': ['Tobia Poppi', 'Tejaswi Kasarla', 'Pascal Mettes', 'Lorenzo Baraldi', 'Rita Cucchiara'], 'affiliations': ['IIT-CNR, Italy', 'University of Amsterdam, Netherlands', 'University of Modena and Reggio Emilia, Italy', 'University of Pisa, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.12127.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#open_source', '#cv', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Гиперболическая иерархия для осознанной безопасности в мультимодальных моделях', 'desc': 'Статья представляет новый подход к обработке небезопасного контента в мультимодальных моделях, таких как CLIP. Вместо удаления знаний о небезопасных концепциях, авторы предлагают использовать гиперболическое пространство для создания иерархии безопасного и небезопасного контента. Модель HySAC (Hyperbolic Safety-Aware CLIP) использует функции потерь для моделирования иерархических отношений между безопасными и небезопасными парами изображение-текст. Эксперименты показывают, что этот подход улучшает распознавание безопасности и создает более гибкую систему модерации контента.'}, 'en': {'title': 'Enhancing Safety Awareness in Vision-Language Models with Hyperbolic Space', 'desc': 'This paper addresses the challenge of retrieving unsafe content from vision-language models like CLIP by introducing a new approach called Hyperbolic Safety-Aware CLIP (HySAC). Instead of using unlearning techniques that erase knowledge of unsafe concepts, HySAC utilizes the hierarchical properties of hyperbolic space to encode safe and unsafe content as an entailment hierarchy. By employing entailment loss functions, the model can better understand the relationships between safe and unsafe image-text pairs, enhancing its ability to classify and moderate content. The results demonstrate that HySAC improves safety recognition and offers a more flexible framework for content moderation in multimodal applications.'}, 'zh': {'title': '提升视觉-语言模型的安全性意识', 'desc': '本论文探讨了如何从视觉-语言模型（如CLIP）中检索不安全内容。当前的方法主要依赖于消除学习技术，但这种方法限制了模型区分安全和不安全内容的能力。我们提出了一种新方法，利用双曲空间的层次特性，将安全和不安全内容编码为蕴含层次结构。我们的HySAC模型通过蕴含损失函数建模安全与不安全图像-文本对之间的层次和不对称关系，从而增强了模型对不安全内容的意识。'}}}, {'id': 'https://huggingface.co/papers/2503.08683', 'title': 'CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving', 'url': 'https://huggingface.co/papers/2503.08683', 'abstract': 'Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise for improving safety by addressing the perception and prediction uncertainties inherent in single-agent systems. However, traditional cooperative methods are constrained by rigid collaboration protocols and limited generalization to unseen interactive scenarios. While LLM-based approaches offer generalized reasoning capabilities, their challenges in spatial planning and unstable inference latency hinder their direct application in cooperative driving. To address these limitations, we propose CoLMDriver, the first full-pipeline LLM-based cooperative driving system, enabling effective language-based negotiation and real-time driving control. CoLMDriver features a parallel driving pipeline with two key components: (i) an LLM-based negotiation module under an actor-critic paradigm, which continuously refines cooperation policies through feedback from previous decisions of all vehicles; and (ii) an intention-guided waypoint generator, which translates negotiation outcomes into executable waypoints. Additionally, we introduce InterDrive, a CARLA-based simulation benchmark comprising 10 challenging interactive driving scenarios for evaluating V2V cooperation. Experimental results demonstrate that CoLMDriver significantly outperforms existing approaches, achieving an 11% higher success rate across diverse highly interactive V2V driving scenarios. Code will be released on https://github.com/cxliu0314/CoLMDriver.', 'score': 1, 'issue_id': 2790, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'ef46cd2c17c64880', 'authors': ['Changxing Liu', 'Genjia Liu', 'Zijun Wang', 'Jinchang Yang', 'Siheng Chen'], 'affiliations': ['Multi-Agent Governance & Intelligence Crew (MAGIC)', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08683.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#rl', '#agents'], 'emoji': '🚗', 'ru': {'title': 'Языковые модели открывают новую эру в кооперативном автономном вождении', 'desc': 'CoLMDriver - это первая система кооперативного вождения на основе языковых моделей (LLM), обеспечивающая эффективные языковые переговоры и управление в реальном времени. Система включает модуль переговоров на основе LLM и генератор путевых точек, управляемый намерениями. Авторы также представили InterDrive - симуляционный бенчмарк на базе CARLA для оценки кооперации между транспортными средствами. Эксперименты показали, что CoLMDriver значительно превосходит существующие подходы, достигая на 11% более высокого уровня успешности в различных сценариях взаимодействия автомобилей.'}, 'en': {'title': 'Revolutionizing V2V Cooperation with CoLMDriver!', 'desc': 'This paper introduces CoLMDriver, a novel system for vehicle-to-vehicle (V2V) cooperative autonomous driving that leverages large language models (LLMs) for improved safety and decision-making. CoLMDriver addresses the limitations of traditional methods by implementing a full-pipeline approach that includes a negotiation module and a waypoint generator, allowing vehicles to communicate and plan collaboratively in real-time. The negotiation module uses an actor-critic framework to adaptively refine cooperation strategies based on feedback from previous interactions. Experimental results show that CoLMDriver outperforms existing systems, achieving a higher success rate in complex driving scenarios, demonstrating its effectiveness in enhancing V2V cooperation.'}, 'zh': {'title': 'CoLMDriver：智能协作驾驶的新突破', 'desc': '本文提出了一种名为CoLMDriver的全流程基于大语言模型（LLM）的合作驾驶系统，旨在提高车辆间的安全性。该系统通过语言基础的协商和实时驾驶控制，克服了传统合作方法的局限性。CoLMDriver包含两个关键组件：基于演员-评论家范式的协商模块和意图引导的路径点生成器，能够有效地将协商结果转化为可执行的路径点。实验结果表明，CoLMDriver在多种高度互动的车辆间驾驶场景中，成功率比现有方法提高了11%。'}}}, {'id': 'https://huggingface.co/papers/2503.21776', 'title': 'Video-R1: Reinforcing Video Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2503.21776', 'abstract': "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released.", 'score': 66, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'f88fc5679e0ee30c', 'authors': ['Kaituo Feng', 'Kaixiong Gong', 'Bohao Li', 'Zonghao Guo', 'Yibing Wang', 'Tianshuo Peng', 'Benyou Wang', 'Xiangyu Yue'], 'affiliations': ['CUHK (SZ)', 'CUHK MMLab', 'Tsinghua University', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2503.21776.jpg', 'data': {'categories': ['#reasoning', '#rl', '#open_source', '#benchmark', '#dataset', '#multimodal', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Video-R1: Прорыв в видео-рассуждениях для мультимодальных языковых моделей', 'desc': 'Статья представляет Video-R1 - первую попытку применить парадигму R1 для развития способностей видео-рассуждений в мультимодальных больших языковых моделях (MLLM). Авторы предлагают алгоритм T-GRPO для улучшения временного моделирования при видео-рассуждениях. Для обучения используются как видео-, так и изображения-данные высокого качества. Результаты показывают значительное улучшение производительности Video-R1 на различных бенчмарках видео-рассуждений.'}, 'en': {'title': 'Enhancing Video Reasoning with Temporal Insights and Image Data', 'desc': 'This paper presents Video-R1, a novel approach to enhance video reasoning capabilities in multimodal large language models (MLLMs) using rule-based reinforcement learning (RL). The authors identify two main challenges: the need for better temporal modeling in video reasoning and the limited availability of high-quality video-reasoning data. To overcome these challenges, they introduce the T-GRPO algorithm, which leverages temporal information, and augment the training process with high-quality image-reasoning data. The results show that Video-R1 significantly outperforms existing benchmarks, achieving notable accuracy improvements in video reasoning tasks.'}, 'zh': {'title': 'Video-R1：视频推理的新突破', 'desc': '本文介绍了Video-R1，这是首次系统探索在多模态大语言模型中通过规则基础的强化学习（RL）来引发视频推理能力。我们提出了T-GRPO算法，以解决视频推理中的时间建模不足和高质量视频推理数据稀缺的问题。通过结合高质量的图像推理数据，我们构建了两个数据集，分别用于冷启动和强化学习训练。实验结果表明，Video-R1在多个视频推理基准测试中取得了显著的改进，尤其在视频空间推理基准VSI-bench上达到了35.8%的准确率，超越了商业模型GPT-4o。'}}}, {'id': 'https://huggingface.co/papers/2503.21620', 'title': 'UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.21620', 'abstract': 'The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain.', 'score': 45, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '81580448c4650ed8', 'authors': ['Zhengxi Lu', 'Yuxiang Chai', 'Yaxuan Guo', 'Xi Yin', 'Liang Liu', 'Hao Wang', 'Guanjing Xiong', 'Hongsheng Li'], 'affiliations': ['MMLab @ CUHK', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.21620.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#dataset', '#multimodal', '#optimization'], 'emoji': '🖥️', 'ru': {'title': 'Улучшение понимания GUI с помощью обучения с подкреплением', 'desc': 'Исследователи изучают, как обучение с подкреплением на основе правил может улучшить способности мультимодальных больших языковых моделей (MLLM) в задачах прогнозирования действий графического пользовательского интерфейса (GUI). Они создали небольшой, но качественный набор данных из 136 сложных задач и разработали унифицированную систему вознаграждений за действия на основе правил. Их модель UI-R1-3B показала значительные улучшения как на внутридоменных, так и на внедоменных задачах по сравнению с базовой моделью. Результаты подчеркивают потенциал обучения с подкреплением на основе правил для улучшения понимания и управления GUI.'}, 'en': {'title': 'Enhancing GUI Action Prediction with Rule-Based Reinforcement Learning', 'desc': 'This paper introduces a novel approach to enhance the reasoning capabilities of multimodal large language models (MLLMs) using rule-based reinforcement learning (RL) for predicting actions in graphic user interfaces (GUIs). The authors curate a high-quality dataset of 136 challenging tasks that involve common actions on mobile devices, allowing for effective model training. They propose a unified rule-based action reward system that optimizes the model through policy-based algorithms like Group Relative Policy Optimization (GRPO). Experimental results show that their model, UI-R1-3B, significantly outperforms the base model in both in-domain and out-of-domain tasks, highlighting the effectiveness of rule-based RL in improving GUI action prediction.'}, 'zh': {'title': '基于规则的强化学习提升GUI动作预测能力', 'desc': '最近的DeepSeek-R1展示了大型语言模型（LLMs）通过基于规则的奖励进行强化学习（RL）而展现出的推理能力。我们首次探索了基于规则的强化学习如何增强多模态大型语言模型（MLLMs）在图形用户界面（GUI）动作预测任务中的推理能力。为此，我们整理了一个小而高质量的数据集，包含136个具有挑战性的任务，涵盖五种常见的移动设备动作类型。实验结果表明，我们提出的数据高效模型UI-R1-3B在领域内和领域外任务上均取得了显著的改进，展示了基于规则的强化学习在提升GUI理解和控制方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.21460', 'title': 'Large Language Model Agent: A Survey on Methodology, Applications and\n  Challenges', 'url': 'https://huggingface.co/papers/2503.21460', 'abstract': 'The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.', 'score': 41, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '8295a3726d0cc1b8', 'authors': ['Junyu Luo', 'Weizhi Zhang', 'Ye Yuan', 'Yusheng Zhao', 'Junwei Yang', 'Yiyang Gu', 'Bohan Wu', 'Binqi Chen', 'Ziyue Qiao', 'Qingqing Long', 'Rongcheng Tu', 'Xiao Luo', 'Wei Ju', 'Zhiping Xiao', 'Yifan Wang', 'Meng Xiao', 'Chenwu Liu', 'Jingyang Yuan', 'Shichang Zhang', 'Yiqiao Jin', 'Fan Zhang', 'Xian Wu', 'Hanqing Zhao', 'Dacheng Tao', 'Philip S. Yu', 'Ming Zhang'], 'affiliations': ['Computer Network Information Center, Chinese Academy of Sciences, Beijing, China', 'Department of Computer Science, University of California, Los Angeles, USA', 'Department of Computer Science, University of Illinois at Chicago, Chicago, USA', 'Georgia Institute of Technology, Atlanta, USA', 'Harvard University, Cambridge, USA', 'Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China', 'Nanyang Technological University, Singapore', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, USA', 'School of Computer Science and PKU-Anker LLM Lab, Peking University, Beijing, China', 'School of Computing and Information Technology, Great Bay University, Guangdong, China', 'School of Information Technology & Management, University of International Business and Economics, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.21460.jpg', 'data': {'categories': ['#agi', '#benchmark', '#agents', '#architecture', '#survey'], 'emoji': '🤖', 'ru': {'title': 'Агенты LLM: архитектура, сотрудничество и эволюция искусственного интеллекта', 'desc': 'Эта статья представляет собой обзор систем агентов на основе больших языковых моделей (LLM). Авторы предлагают методологически-ориентированную таксономию, связывающую архитектурные основы, механизмы сотрудничества и эволюционные пути агентов LLM. В работе рассматриваются принципы проектирования агентов, их поведение в сложных средах, а также методологии оценки и практические применения. Статья предоставляет исследователям структурированный подход к пониманию агентов LLM и определяет перспективные направления для будущих исследований.'}, 'en': {'title': 'Unifying the Future of Intelligent LLM Agents', 'desc': 'This paper explores the advancements in Large Language Model (LLM) agents, which are intelligent systems capable of adapting and achieving specific goals. It presents a structured taxonomy that categorizes LLM agent systems based on their architecture, collaboration methods, and evolutionary processes. The authors aim to connect various research areas by highlighting the relationship between design principles and the behaviors of these agents in complex environments. Additionally, the paper discusses evaluation methods, practical challenges, and potential applications, providing a comprehensive overview for future research in this field.'}, 'zh': {'title': '大型语言模型代理：通向人工通用智能的关键', 'desc': '本文探讨了大型语言模型（LLM）代理的系统，强调其在实现人工通用智能方面的重要性。通过建立一个以方法论为中心的分类法，文章将代理的架构基础、协作机制和演化路径进行了系统性分析。我们揭示了代理设计原则与其在复杂环境中涌现行为之间的基本联系，从而统一了分散的研究线索。最后，本文为研究人员提供了一个结构化的分类体系，以理解LLM代理，并指出未来研究的有前景方向。'}}}, {'id': 'https://huggingface.co/papers/2503.21380', 'title': 'Challenging the Boundaries of Reasoning: An Olympiad-Level Math\n  Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2503.21380', 'abstract': "In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.", 'score': 35, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '923e6e5f48ca95e4', 'authors': ['Haoxiang Sun', 'Yingqian Min', 'Zhipeng Chen', 'Wayne Xin Zhao', 'Zheng Liu', 'Zhongyuan Wang', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'DataCanvas Alaya NeW', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'School of Information, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.21380.jpg', 'data': {'categories': ['#reasoning', '#multilingual', '#benchmark', '#low_resource', '#math'], 'emoji': '🧮', 'ru': {'title': 'OlymMATH: Новая высота в оценке математических способностей ИИ', 'desc': 'OlymMATH - это новый бенчмарк для оценки математических рассуждений моделей машинного обучения на уровне олимпиад. Он содержит 200 тщательно отобранных задач двух уровней сложности, охватывающих четыре основные области математики. Бенчмарк доступен на английском и китайском языках, что позволяет проводить двуязычную оценку. Результаты тестирования показали, что даже современные языковые модели (LLM) демонстрируют ограниченную точность на сложном подмножестве задач.'}, 'en': {'title': 'OlymMATH: Raising the Bar for Mathematical Reasoning Evaluation', 'desc': "The paper introduces OlymMATH, a new benchmark for evaluating the mathematical reasoning abilities of large language models (LLMs). It consists of 200 carefully curated problems, verified for accuracy, and presented in both English and Chinese. The problems are categorized into two difficulty levels: AIME-level (easy) and more challenging problems (hard) that test the limits of current models. Empirical results show that even advanced models struggle with the hard problems, highlighting the benchmark's effectiveness in assessing complex reasoning skills."}, 'zh': {'title': 'OlymMATH：挑战数学推理的新基准', 'desc': '近年来，大型推理模型的快速发展使得现有的数学推理评估基准趋于饱和，迫切需要更具挑战性和严格性的评估框架。为此，我们推出了OlymMATH，这是一个新颖的奥林匹克级数学基准，旨在严格测试大型语言模型的复杂推理能力。OlymMATH包含200个经过精心挑选的问题，分为AIME级（简单）和更具挑战性的（困难）两种难度，涵盖四个核心数学领域，并提供可验证的数值解。实证结果表明，OlymMATH对当前最先进的模型提出了显著挑战，尤其是在困难子集上，模型的准确性明显有限。'}}}, {'id': 'https://huggingface.co/papers/2503.21755', 'title': 'VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic\n  Faithfulness', 'url': 'https://huggingface.co/papers/2503.21755', 'abstract': 'Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real "world models" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored for individual dimensions, our evaluation framework integrates generalists such as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive annotations to ensure alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.', 'score': 28, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '4bd65083c265f9a4', 'authors': ['Dian Zheng', 'Ziqi Huang', 'Hongbo Liu', 'Kai Zou', 'Yinan He', 'Fan Zhang', 'Yuanhan Zhang', 'Jingwen He', 'Wei-Shi Zheng', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Sun Yat-Sen University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.21755.jpg', 'data': {'categories': ['#video', '#alignment', '#benchmark', '#games'], 'emoji': '🎬', 'ru': {'title': 'От визуальной правдоподобности к реалистичности: новый стандарт оценки генеративных видеомоделей', 'desc': 'Статья представляет VBench-2.0 - новый эталонный тест для оценки генеративных видеомоделей. В отличие от предыдущих версий, фокусирующихся на внешней достоверности, VBench-2.0 оценивает внутреннюю достоверность видео. Тест включает оценку пяти ключевых аспектов: точность отображения людей, управляемость, креативность, физическую корректность и здравый смысл. VBench-2.0 использует как модели общего назначения (VLM, LLM), так и специализированные методы для комплексной оценки генеративных видеомоделей.'}, 'en': {'title': 'Towards Realism: VBench-2.0 for Intrinsic Faithfulness in Video Generation', 'desc': 'This paper discusses the evolution of video generation models from producing unrealistic outputs to creating visually convincing and temporally coherent videos. It highlights the limitations of current evaluation benchmarks like VBench, which focus on superficial aspects of faithfulness rather than adherence to real-world principles. The authors introduce VBench-2.0, a new benchmark that evaluates video generative models based on intrinsic faithfulness, which includes factors like physics, commonsense reasoning, and anatomical correctness. By emphasizing these deeper aspects of realism, VBench-2.0 aims to improve the quality of video generation for applications such as AI-assisted filmmaking and simulated world modeling.'}, 'zh': {'title': '追求内在可信度的下一代视频生成标准', 'desc': '视频生成技术已经取得了显著进展，从最初生成不真实的输出到现在能够生成视觉上令人信服且时间上连贯的视频。为了评估这些视频生成模型，开发了VBench等基准，主要测量每帧的美观性、时间一致性和基本提示遵循等因素。然而，这些评估主要关注表面上的可信度，而不是视频是否遵循现实世界的原则。为实现真正的“世界模型”，我们引入了VBench-2.0，旨在自动评估视频生成模型的内在可信度，确保生成的视频符合物理法则和常识推理。'}}}, {'id': 'https://huggingface.co/papers/2503.21749', 'title': 'LeX-Art: Rethinking Text Generation via Scalable High-Quality Data\n  Synthesis', 'url': 'https://huggingface.co/papers/2503.21749', 'abstract': 'We introduce LeX-Art, a comprehensive suite for high-quality text-image synthesis that systematically bridges the gap between prompt expressiveness and text rendering fidelity. Our approach follows a data-centric paradigm, constructing a high-quality data synthesis pipeline based on Deepseek-R1 to curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined 1024times1024 images. Beyond dataset construction, we develop LeX-Enhancer, a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX and LeX-Lumina, achieving state-of-the-art text rendering performance. To systematically evaluate visual text generation, we introduce LeX-Bench, a benchmark that assesses fidelity, aesthetics, and alignment, complemented by Pairwise Normalized Edit Distance (PNED), a novel metric for robust text accuracy evaluation. Experiments demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX outperforming baselines in color (+3.18%), positional (+4.45%), and font accuracy (+3.81%). Our codes, models, datasets, and demo are publicly available.', 'score': 21, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'fe7d17315ae060c8', 'authors': ['Shitian Zhao', 'Qilong Wu', 'Xinyue Li', 'Bo Zhang', 'Ming Li', 'Qi Qin', 'Dongyang Liu', 'Kaipeng Zhang', 'Hongsheng Li', 'Yu Qiao', 'Peng Gao', 'Bin Fu', 'Zhen Li'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.21749.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#dataset', '#data'], 'emoji': '🖼️', 'ru': {'title': 'LeX-Art: Революция в синтезе текста и изображений', 'desc': 'LeX-Art представляет собой комплексный набор инструментов для высококачественного синтеза текста и изображений, который систематически устраняет разрыв между выразительностью промпта и точностью рендеринга текста. Подход основан на парадигме, ориентированной на данные, и включает создание высококачественного конвейера синтеза данных на основе Deepseek-R1 для курирования набора данных LeX-10K. Разработаны LeX-Enhancer для обогащения промптов и две модели text-to-image: LeX-FLUX и LeX-Lumina, достигающие передового уровня производительности в рендеринге текста. Для оценки введен бенчмарк LeX-Bench и новая метрика PNED для надежной оценки точности текста.'}, 'en': {'title': "Bridging Text and Image: LeX-Art's High-Quality Synthesis Revolution", 'desc': 'LeX-Art is a new system designed to create high-quality images from text prompts, focusing on improving how well the text is rendered in the images. It builds a large dataset called LeX-10K, which contains 10,000 high-resolution images that are visually appealing. The system includes a model called LeX-Enhancer that improves the prompts used for generating images, and two advanced text-to-image models, LeX-FLUX and LeX-Lumina, which achieve top performance in rendering text. Additionally, LeX-Art introduces a benchmark called LeX-Bench to evaluate the quality of the generated images, using a new metric called Pairwise Normalized Edit Distance (PNED) to measure text accuracy.'}, 'zh': {'title': '高质量文本-图像合成的新突破', 'desc': 'LeX-Art 是一个全面的文本-图像合成工具，旨在提高提示表达能力和文本渲染的准确性。我们构建了 LeX-10K 数据集，包含 10,000 张高分辨率、经过美学优化的图像，并开发了 LeX-Enhancer 模型来增强提示效果。我们训练了两个文本到图像模型 LeX-FLUX 和 LeX-Lumina，达到了最先进的文本渲染性能。通过 LeX-Bench 基准测试，我们评估了视觉文本生成的保真度、美学和一致性，实验结果显示显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.21729', 'title': 'ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large\n  Reasoning Models with Iterative Retrieval Augmented Generation', 'url': 'https://huggingface.co/papers/2503.21729', 'abstract': "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).", 'score': 18, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '83959049f8af99fe', 'authors': ['Zhicheng Lee', 'Shulin Cao', 'Jinxin Liu', 'Jiajie Zhang', 'Weichuan Liu', 'Xiaoyin Che', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Siemens AG', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21729.jpg', 'data': {'categories': ['#reasoning', '#rag', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'ReaRAG: Улучшение фактической точности моделей рассуждений с помощью контролируемого поиска', 'desc': 'Статья представляет ReaRAG - модель рассуждений с улучшенной фактической точностью для задач вопросно-ответных систем. Авторы предлагают новый подход к построению данных с ограничением длины цепочки рассуждений. ReaRAG использует большую модель рассуждений для генерации обдуманных шагов и выбора действий из предопределенного пространства. Модель демонстрирует улучшенные результаты на задачах многошаговых вопросно-ответных систем по сравнению с существующими методами.'}, 'en': {'title': 'Enhancing Factuality in Reasoning with ReaRAG', 'desc': 'This paper introduces ReaRAG, a new reasoning model designed to improve the factual accuracy of Large Reasoning Models (LRMs) in question answering tasks. Unlike traditional RL-based LRMs that often overthink and lack robustness, ReaRAG employs a structured approach to reasoning by limiting the length of reasoning chains and allowing for diverse query exploration. The model uses a two-step action process, where it can either search for information or finish the reasoning process, enhancing its ability to refine answers based on retrieved data. Overall, ReaRAG demonstrates superior performance in multi-hop question answering by effectively combining reasoning capabilities with retrieval mechanisms.'}, 'zh': {'title': '增强事实性的推理模型ReaRAG', 'desc': '大型推理模型（LRMs）展现了卓越的推理能力，但主要依赖参数知识，限制了事实准确性。虽然最近的研究为基于强化学习的LRMs增加了检索能力，但它们在推理时容易过度思考，缺乏稳健性，从而降低了在问答任务中的有效性。为了解决这个问题，我们提出了ReaRAG，这是一种增强事实性的推理模型，能够在不进行过多迭代的情况下探索多样化的查询。我们的解决方案包括一个新颖的数据构建框架，并对推理链的长度设定上限，从而提高了LRMs的事实性和推理的稳健性。'}}}, {'id': 'https://huggingface.co/papers/2503.21696', 'title': 'Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for\n  Embodied Interactive Tasks', 'url': 'https://huggingface.co/papers/2503.21696', 'abstract': "Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.", 'score': 17, 'issue_id': 2946, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'a52516705bc7a122', 'authors': ['Wenqi Zhang', 'Mengna Wang', 'Gangao Liu', 'Xu Huixin', 'Yiwei Jiang', 'Yongliang Shen', 'Guiyang Hou', 'Zhe Zheng', 'Hang Zhang', 'Xin Li', 'Weiming Lu', 'Peng Li', 'Yueting Zhuang'], 'affiliations': ['Alibaba Group', 'College of Computer Science and Technology, Zhejiang University', 'DAMO Academy, Alibaba Group', 'Hohai University', 'Institute of Software, Chinese Academy of Sciences', 'Nanjing Institute of Software Technology', 'Nanjing University of Posts and Telecommunications', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.21696.jpg', 'data': {'categories': ['#cv', '#optimization', '#reasoning', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'Воплощённый разум: новый подход к рассуждениям в физическом мире', 'desc': 'Статья представляет модель Embodied Reasoner, которая расширяет возможности рассуждений в стиле GPT-4 для интерактивных задач поиска в физическом мире. Модель обучается на синтезированном наборе данных, содержащем траектории наблюдений, мыслей и действий. Авторы разработали трехэтапный процесс обучения, включающий имитационное обучение, самоисследование и самокоррекцию. Результаты показывают, что Embodied Reasoner превосходит современные модели визуального рассуждения, особенно в сложных долгосрочных задачах.'}, 'en': {'title': 'Empowering Reasoning in Interactive Environments', 'desc': 'This paper introduces the Embodied Reasoner, a model designed to enhance reasoning in interactive environments that require continuous engagement through visual and action-based tasks. Unlike traditional mathematical reasoning, this model focuses on spatial understanding and temporal reasoning, which are crucial for navigating real-world scenarios. The authors created a large dataset of 9.3k Observation-Thought-Action trajectories to train the model, employing a three-stage training process that includes imitation learning and self-correction. The results demonstrate that the Embodied Reasoner outperforms existing visual reasoning models, showing improved efficiency and fewer logical errors in complex tasks.'}, 'zh': {'title': '提升交互式推理能力的全新模型', 'desc': '最近深度学习模型在数学和编程任务上展现了出色的推理能力，但在需要与环境持续互动的实际应用领域中，其有效性仍未得到充分探索。我们提出了"Embodied Reasoner"模型，旨在将推理能力扩展到交互式的实际搜索任务中。与主要依赖逻辑推理的数学推理不同，实际场景需要空间理解、时间推理以及基于互动历史的自我反思。通过合成9.3千条连贯的观察-思考-行动轨迹，我们开发了一个三阶段的训练流程，显著提升了模型在复杂任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.21144', 'title': 'ChatAnyone: Stylized Real-time Portrait Video Generation with\n  Hierarchical Motion Diffusion Model', 'url': 'https://huggingface.co/papers/2503.21144', 'abstract': 'Real-time interactive video-chat portraits have been increasingly recognized as the future trend, particularly due to the remarkable progress made in text and voice chat technologies. However, existing methods primarily focus on real-time generation of head movements, but struggle to produce synchronized body motions that match these head actions. Additionally, achieving fine-grained control over the speaking style and nuances of facial expressions remains a challenge. To address these limitations, we introduce a novel framework for stylized real-time portrait video generation, enabling expressive and flexible video chat that extends from talking head to upper-body interaction. Our approach consists of the following two stages. The first stage involves efficient hierarchical motion diffusion models, that take both explicit and implicit motion representations into account based on audio inputs, which can generate a diverse range of facial expressions with stylistic control and synchronization between head and body movements. The second stage aims to generate portrait video featuring upper-body movements, including hand gestures. We inject explicit hand control signals into the generator to produce more detailed hand movements, and further perform face refinement to enhance the overall realism and expressiveness of the portrait video. Additionally, our approach supports efficient and continuous generation of upper-body portrait video in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting interactive video-chat in real-time. Experimental results demonstrate the capability of our approach to produce portrait videos with rich expressiveness and natural upper-body movements.', 'score': 17, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '12e50e9826751c62', 'authors': ['Jinwei Qi', 'Chaonan Ji', 'Sheng Xu', 'Peng Zhang', 'Bang Zhang', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.21144.jpg', 'data': {'categories': ['#cv', '#multimodal', '#video', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Реалистичные видеопортреты с движениями тела для видеочатов в реальном времени', 'desc': 'Эта статья представляет новую систему для генерации стилизованных видеопортретов в реальном времени для видеочатов. Система использует иерархические модели диффузии движения, учитывающие как явные, так и неявные представления движения на основе аудиовхода. Она позволяет генерировать разнообразные выражения лица с контролем стиля и синхронизацией движений головы и тела. Система также включает генерацию движений верхней части тела, включая жесты рук, и дополнительное улучшение лица для повышения реалистичности.'}, 'en': {'title': 'Expressive Real-Time Video Chats with Synchronized Body Movements', 'desc': 'This paper presents a new framework for creating real-time interactive video chat portraits that include both head and upper-body movements. It utilizes hierarchical motion diffusion models to synchronize facial expressions and body motions based on audio inputs, allowing for expressive and stylistically controlled video generation. The framework also incorporates explicit hand control signals to enhance the realism of hand gestures and facial refinements. Overall, the approach enables high-quality, interactive video chats at a resolution of 512 * 768 and up to 30 frames per second.'}, 'zh': {'title': '实时互动视频聊天的未来趋势', 'desc': '本论文提出了一种新颖的实时肖像视频生成框架，旨在解决现有方法在头部动作与身体动作同步方面的不足。该框架通过高效的层次运动扩散模型，结合音频输入生成多样化的面部表情，并实现头部与身体动作的协调。第二阶段则专注于生成包含上半身动作的肖像视频，通过注入手部控制信号来增强手部动作的细节，并进行面部细化以提升视频的真实感和表现力。实验结果表明，该方法能够以高达30fps的速度生成丰富表现力和自然上半身动作的肖像视频，适用于实时互动视频聊天。'}}}, {'id': 'https://huggingface.co/papers/2503.20990', 'title': 'FinAudio: A Benchmark for Audio Large Language Models in Financial\n  Applications', 'url': 'https://huggingface.co/papers/2503.20990', 'abstract': 'Audio Large Language Models (AudioLLMs) have received widespread attention and have significantly improved performance on audio tasks such as conversation, audio understanding, and automatic speech recognition (ASR). Despite these advancements, there is an absence of a benchmark for assessing AudioLLMs in financial scenarios, where audio data, such as earnings conference calls and CEO speeches, are crucial resources for financial analysis and investment decisions. In this paper, we introduce FinAudio, the first benchmark designed to evaluate the capacity of AudioLLMs in the financial domain. We first define three tasks based on the unique characteristics of the financial domain: 1) ASR for short financial audio, 2) ASR for long financial audio, and 3) summarization of long financial audio. Then, we curate two short and two long audio datasets, respectively, and develop a novel dataset for financial audio summarization, comprising the FinAudio benchmark. Then, we evaluate seven prevalent AudioLLMs on FinAudio. Our evaluation reveals the limitations of existing AudioLLMs in the financial domain and offers insights for improving AudioLLMs. All datasets and codes will be released.', 'score': 17, 'issue_id': 2943, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '55f780e7347209e5', 'authors': ['Yupeng Cao', 'Haohang Li', 'Yangyang Yu', 'Shashidhar Reddy Javaji', 'Yueru He', 'Jimin Huang', 'Zining Zhu', 'Qianqian Xie', 'Xiao-yang Liu', 'Koduvayur Subbalakshmi', 'Meikang Qiu', 'Sophia Ananiadou', 'Jian-Yun Nie'], 'affiliations': ['Augusta University', 'Columbia University', 'Stevens Institute of Technology', 'The Fin AI', 'The University of Manchester', 'University of Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.20990.jpg', 'data': {'categories': ['#audio', '#benchmark', '#dataset'], 'emoji': '🎙️', 'ru': {'title': 'FinAudio: Первый бенчмарк для оценки аудио-ИИ в финансах', 'desc': 'Статья представляет FinAudio - первый бенчмарк для оценки аудио-языковых моделей (AudioLLMs) в финансовой сфере. Авторы определяют три задачи: распознавание коротких и длинных финансовых аудиозаписей, а также их суммаризация. Для бенчмарка созданы наборы данных коротких и длинных аудио, а также уникальный датасет для суммаризации финансового аудио. Оценка семи популярных AudioLLMs на FinAudio выявила ограничения существующих моделей в финансовой области.'}, 'en': {'title': 'Benchmarking AudioLLMs for Financial Insights', 'desc': 'This paper introduces FinAudio, a benchmark specifically designed to evaluate Audio Large Language Models (AudioLLMs) in financial contexts. It identifies three key tasks: automatic speech recognition (ASR) for both short and long financial audio, and summarization of long financial audio. The authors curate datasets tailored to these tasks, highlighting the unique characteristics of financial audio data. The evaluation of seven existing AudioLLMs on this benchmark reveals their limitations and suggests areas for enhancement in their performance within the financial domain.'}, 'zh': {'title': '金融领域音频模型评估新基准', 'desc': '音频大型语言模型（AudioLLMs）在对话、音频理解和自动语音识别等音频任务上取得了显著进展。然而，目前缺乏一个专门用于评估AudioLLMs在金融场景中的基准。本文提出了FinAudio，这是第一个旨在评估AudioLLMs在金融领域能力的基准，定义了三个基于金融领域特征的任务，并创建了相应的数据集。通过对七个流行的AudioLLMs进行评估，我们揭示了现有模型在金融领域的局限性，并为改进AudioLLMs提供了见解。'}}}, {'id': 'https://huggingface.co/papers/2503.21248', 'title': 'ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition', 'url': 'https://huggingface.co/papers/2503.21248', 'abstract': 'Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.', 'score': 16, 'issue_id': 2944, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'c58f620a0f532dc0', 'authors': ['Yujie Liu', 'Zonglin Yang', 'Tong Xie', 'Jinjie Ni', 'Ben Gao', 'Yuqiang Li', 'Shixiang Tang', 'Wanli Ouyang', 'Erik Cambria', 'Dongzhan Zhou'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'University of New South Wales', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21248.jpg', 'data': {'categories': ['#benchmark', '#science', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'LLM как генератор научных гипотез: новый бенчмарк открывает потенциал ИИ в исследованиях', 'desc': 'Эта статья представляет первый масштабный бенчмарк для оценки способности больших языковых моделей (LLM) генерировать качественные научные гипотезы. Авторы разработали автоматизированную систему для извлечения ключевых компонентов из научных статей в 12 дисциплинах, включая исследовательские вопросы, обзоры литературы, источники вдохновения и гипотезы. Оценка показала, что LLM хорошо справляются с поиском вдохновения, что говорит об их способности находить новые ассоциации знаний. Это позиционирует LLM как перспективный инструмент для автоматизированного научного открытия, способный генерировать инновационные гипотезы в больших масштабах.'}, 'en': {'title': 'Unlocking Scientific Discovery with LLMs', 'desc': 'This paper introduces a new benchmark to evaluate large language models (LLMs) in the context of scientific research. It focuses on three key tasks: retrieving inspirations, composing hypotheses, and ranking them based on quality. The authors developed an automated framework that accurately extracts essential elements from scientific papers, validated by experts. The findings indicate that LLMs excel at retrieving inspirations, highlighting their potential to generate innovative research hypotheses efficiently.'}, 'zh': {'title': '大型语言模型助力科学发现的潜力', 'desc': '大型语言模型（LLMs）在科学研究中显示出潜力，但它们发现高质量研究假设的能力尚未得到验证。为了解决这个问题，我们引入了第一个大规模基准，用于评估LLMs在科学发现中的表现，包括灵感检索、假设构建和假设排序等子任务。我们开发了一个自动化框架，从12个学科的科学论文中提取关键组件，并通过专家验证确认其准确性。评估结果表明，LLMs在检索灵感方面表现良好，表明它们能够发现新的知识关联，从而推动自动化科学发现。'}}}, {'id': 'https://huggingface.co/papers/2503.21758', 'title': 'Lumina-Image 2.0: A Unified and Efficient Image Generative Framework', 'url': 'https://huggingface.co/papers/2503.21758', 'abstract': 'We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-Image-2.0.', 'score': 15, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '976a9523e7e6ba13', 'authors': ['Qi Qin', 'Le Zhuo', 'Yi Xin', 'Ruoyi Du', 'Zhen Li', 'Bin Fu', 'Yiting Lu', 'Jiakang Yuan', 'Xinyue Li', 'Dongyang Liu', 'Xiangyang Zhu', 'Manyuan Zhang', 'Will Beddow', 'Erwann Millon', 'Victor Perez', 'Wenhai Wang', 'Conghui He', 'Bo Zhang', 'Xiaohong Liu', 'Hongsheng Li', 'Yu Qiao', 'Chang Xu', 'Peng Gao'], 'affiliations': ['Krea AI', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.21758.jpg', 'data': {'categories': ['#inference', '#cv', '#open_source', '#training', '#multimodal', '#small_models', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Единство и эффективность в генерации изображений по тексту', 'desc': 'Lumina-Image 2.0 - это усовершенствованная система генерации изображений по текстовому описанию. Она основана на унифицированной архитектуре, объединяющей обработку текста и изображений, и включает специальную систему генерации подписей UniCap. Разработчики применили стратегии многоэтапного прогрессивного обучения и методы ускорения вывода для повышения эффективности модели. Несмотря на относительно небольшое количество параметров (2.6 млрд), Lumina-Image 2.0 демонстрирует высокую производительность на академических бенчмарках и в публичных соревнованиях по генерации изображений.'}, 'en': {'title': 'Revolutionizing Text-to-Image Generation with Lumina-Image 2.0', 'desc': "Lumina-Image 2.0 is a cutting-edge framework for generating images from text, significantly improving upon its predecessor, Lumina-Next. It utilizes a unified architecture that allows text and image data to interact seamlessly, enhancing the model's ability to handle various tasks. The introduction of the Unified Captioner (UniCap) enables the generation of high-quality captions that align well with images, improving training efficiency and output accuracy. Additionally, the model incorporates advanced training and inference techniques to maintain high image quality while being resource-efficient, demonstrating strong performance with a relatively small number of parameters."}, 'zh': {'title': 'Lumina-Image 2.0：高效的文本到图像生成新纪元', 'desc': 'Lumina-Image 2.0 是一个先进的文本到图像生成框架，相比于之前的 Lumina-Next 取得了显著进展。该框架基于两个关键原则：统一性和效率。统一性通过采用统一架构（Unified Next-DiT）来实现文本和图像标记的联合处理，促进了跨模态的自然交互。效率方面，我们开发了多阶段渐进训练策略和推理加速技术，确保在不降低图像质量的情况下提高模型效率。'}}}, {'id': 'https://huggingface.co/papers/2503.21774', 'title': 'Optimal Stepsize for Diffusion Sampling', 'url': 'https://huggingface.co/papers/2503.21774', 'abstract': 'Diffusion models achieve remarkable generation quality but suffer from computational intensive sampling due to suboptimal step discretization. While existing works focus on optimizing denoising directions, we address the principled design of stepsize schedules. This paper proposes Optimal Stepsize Distillation, a dynamic programming framework that extracts theoretically optimal schedules by distilling knowledge from reference trajectories. By reformulating stepsize optimization as recursive error minimization, our method guarantees global discretization bounds through optimal substructure exploitation. Crucially, the distilled schedules demonstrate strong robustness across architectures, ODE solvers, and noise schedules. Experiments show 10x accelerated text-to-image generation while preserving 99.4% performance on GenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.', 'score': 11, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '709832ee6e0a6f3f', 'authors': ['Jianning Pei', 'Han Hu', 'Shuyang Gu'], 'affiliations': ['Tencent Hunyuan Research', 'University Chinese Academic of Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.21774.jpg', 'data': {'categories': ['#training', '#architecture', '#data', '#diffusion', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных моделей без потери качества', 'desc': 'Это исследование предлагает новый метод оптимизации процесса генерации изображений с помощью диффузионных моделей. Авторы разработали фреймворк Optimal Stepsize Distillation, который использует динамическое программирование для извлечения оптимальных расписаний шагов из эталонных траекторий. Метод гарантирует глобальные границы дискретизации и демонстрирует устойчивость к различным архитектурам и шумовым расписаниям. Эксперименты показывают 10-кратное ускорение генерации изображений по тексту при сохранении 99.4% производительности на GenEval.'}, 'en': {'title': 'Accelerating Diffusion Models with Optimal Stepsize Distillation', 'desc': 'This paper introduces a new method called Optimal Stepsize Distillation to improve the efficiency of diffusion models in generating images. It focuses on optimizing the stepsize schedules used during the sampling process, which is often computationally intensive. By using dynamic programming, the method distills optimal schedules from reference trajectories, ensuring that the stepsize is effectively minimized. The results show that this approach can speed up text-to-image generation by 10 times while maintaining high performance levels.'}, 'zh': {'title': '最优步长蒸馏：加速扩散模型生成的关键', 'desc': '扩散模型在生成质量上表现出色，但由于步骤离散化不理想，导致计算开销大。本文提出了一种名为最优步长蒸馏的动态规划框架，通过从参考轨迹中提取理论最优的步长调度来解决这一问题。我们将步长优化重新表述为递归误差最小化，从而保证了全局离散化界限。实验结果表明，该方法在保持99.4%性能的同时，实现了文本到图像生成的10倍加速。'}}}, {'id': 'https://huggingface.co/papers/2503.21765', 'title': 'Exploring the Evolution of Physics Cognition in Video Generation: A\n  Survey', 'url': 'https://huggingface.co/papers/2503.21765', 'abstract': 'Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models. Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of \'\'visual realism but physical absurdity". Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios. Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap. Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks. Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry. Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of \'\'visual mimicry\'\' towards a new phase of \'\'human-like physical comprehension\'\'.', 'score': 9, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '87e887fdf8f612cf', 'authors': ['Minghui Lin', 'Xiang Wang', 'Yishan Wang', 'Shu Wang', 'Fengqi Dai', 'Pengxiang Ding', 'Cunxiang Wang', 'Zhengrong Zuo', 'Nong Sang', 'Siteng Huang', 'Donglin Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'Shandong University', 'Tsinghua University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21765.jpg', 'data': {'categories': ['#benchmark', '#survey', '#video', '#architecture', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'От визуального мимикрии к человекоподобному пониманию физики в генерации видео', 'desc': 'Этот обзор посвящен интеграции физического познания в генерацию видео с использованием генеративных моделей. Авторы предлагают трехуровневую таксономию, охватывающую восприятие базовых схем, пассивное познание физических законов и активное моделирование мира. В статье обсуждаются современные методы, классические парадигмы и эталонные тесты в этой области. Исследование направлено на развитие интерпретируемых, управляемых и физически согласованных парадигм генерации видео.'}, 'en': {'title': 'From Visual Mimicry to Human-like Physical Comprehension in Video Generation', 'desc': 'This paper discusses the recent improvements in video generation using diffusion models, highlighting their limitations in understanding physical laws. It points out that while these models can create visually appealing content, they often produce results that do not adhere to the principles of physics, leading to unrealistic scenarios. The authors propose a structured overview of how physical cognition can be integrated into video generation, categorizing it into three levels: basic perception, passive knowledge, and active simulation. The survey aims to guide future research towards creating video generation systems that are not only visually realistic but also physically coherent, moving beyond mere visual mimicry to a deeper understanding of the physical world.'}, 'zh': {'title': '推动视频生成向人类物理理解的新阶段', 'desc': '近年来，视频生成技术取得了显著进展，尤其是扩散模型的快速发展。然而，这些模型在物理认知方面的不足逐渐引起了广泛关注，生成的内容常常违反基本的物理法则，陷入了“视觉真实但物理荒谬”的困境。研究人员开始认识到物理真实感在视频生成中的重要性，并尝试将启发式的物理认知融入生成系统，以模拟现实世界的动态场景。本文综述了物理认知在视频生成中的演变过程，提出了三层次的分类法，并强调了该领域的关键挑战和未来研究的潜在方向。'}}}, {'id': 'https://huggingface.co/papers/2503.20822', 'title': 'Synthetic Video Enhances Physical Fidelity in Video Synthesis', 'url': 'https://huggingface.co/papers/2503.20822', 'abstract': 'We investigate how to enhance the physical fidelity of video generation models by leveraging synthetic videos derived from computer graphics pipelines. These rendered videos respect real-world physics, such as maintaining 3D consistency, and serve as a valuable resource that can potentially improve video generation models. To harness this potential, we propose a solution that curates and integrates synthetic data while introducing a method to transfer its physical realism to the model, significantly reducing unwanted artifacts. Through experiments on three representative tasks emphasizing physical consistency, we demonstrate its efficacy in enhancing physical fidelity. While our model still lacks a deep understanding of physics, our work offers one of the first empirical demonstrations that synthetic video enhances physical fidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/', 'score': 8, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '8ad7de4de496fce6', 'authors': ['Qi Zhao', 'Xingyu Ni', 'Ziyu Wang', 'Feng Cheng', 'Ziyan Yang', 'Lu Jiang', 'Bohan Wang'], 'affiliations': ['ByteDance Seed', 'National University of Singapore', 'Peking University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20822.jpg', 'data': {'categories': ['#dataset', '#video', '#data', '#synthetic', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Синтетические видео для реалистичной генерации: шаг к физической достоверности', 'desc': 'Исследователи изучают, как улучшить физическую достоверность моделей генерации видео, используя синтетические видео из компьютерной графики. Эти рендеренные видео соблюдают законы физики реального мира и могут улучшить модели генерации видео. Авторы предлагают метод интеграции синтетических данных и передачи их физического реализма модели, значительно уменьшая нежелательные артефакты. Эксперименты на трех задачах, подчеркивающих физическую согласованность, демонстрируют эффективность метода в повышении физической достоверности.'}, 'en': {'title': 'Enhancing Video Realism with Synthetic Physics', 'desc': 'This paper explores how to improve the realism of video generation models by using synthetic videos created through computer graphics. These synthetic videos adhere to real-world physics, ensuring 3D consistency, which can enhance the training of video generation models. The authors propose a method to curate and integrate this synthetic data, allowing the model to adopt the physical realism of the videos and reduce visual artifacts. Their experiments show that this approach effectively boosts the physical fidelity of generated videos, marking a significant step in the field of video synthesis.'}, 'zh': {'title': '合成视频提升视频生成的物理真实性', 'desc': '我们研究了如何通过利用计算机图形学生成的合成视频来增强视频生成模型的物理真实性。这些渲染视频遵循现实世界的物理规律，保持三维一致性，成为改善视频生成模型的重要资源。我们提出了一种解决方案，策划和整合合成数据，同时引入了一种将物理真实感转移到模型的方法，显著减少了不必要的伪影。通过在三个强调物理一致性的代表性任务上的实验，我们证明了这种方法在提高物理真实性方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.21088', 'title': 'ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging', 'url': 'https://huggingface.co/papers/2503.21088', 'abstract': "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.", 'score': 6, 'issue_id': 2941, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '494379ac783f4297', 'authors': ['Haoming Xu', 'Shuxun Wang', 'Yanqiu Zhao', 'Yi Zhong', 'Ziyan Jiang', 'Ningyuan Zhao', 'Shumin Deng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.21088.jpg', 'data': {'categories': ['#benchmark', '#training', '#data', '#leakage', '#ethics', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Избирательное забывание: новый подход к конфиденциальности в LLM', 'desc': "Статья представляет систему для избирательного удаления конфиденциальных знаний из больших языковых моделей (LLM). Авторы используют метод слияния моделей (Model Merging), в частности TIES-Merging, для создания сбалансированной модели с 'забытыми' данными. Система показала высокие результаты на соревновании SemEval-2025, заняв второе место среди 26 команд. В работе также проводится анализ процесса разобучения (unlearning) и обсуждаются ограничения существующих методов оценки эффективности удаления знаний из моделей."}, 'en': {'title': 'Mastering Unlearning: Balancing Sensitivity in Language Models', 'desc': "This paper discusses the ZJUKLAB team's approach to the SemEval-2025 Task 4, which focuses on unlearning sensitive information from large language models. The proposed method utilizes Model Merging, specifically TIES-Merging, to create a balanced model that effectively manages the challenges of over-forgetting and under-forgetting. The team achieved impressive results, ranking second out of 26 participants, and conducted thorough experiments to analyze the unlearning process, including performance metrics and loss dynamics. The authors also highlight the limitations of current evaluation methods and advocate for improved metrics to better assess unlearning effectiveness in future studies."}, 'zh': {'title': '选择性删除，重塑语言模型的未来', 'desc': '本文介绍了ZJUKLAB团队在SemEval-2025任务4中的提交，旨在从大型语言模型中选择性地删除敏感内容。我们提出了一种利用模型合并（特别是TIES-Merging）的方法，将两个专门模型结合成一个更平衡的未学习模型。我们的系统在26个团队中排名第二，任务聚合的在线得分为0.944，总体聚合得分为0.487。我们还进行了局部实验，全面分析了未学习过程的表现轨迹、损失动态和权重视角，并强调了现有评估指标的不足，呼吁未来研究需要更全面的评估方法。'}}}, {'id': 'https://huggingface.co/papers/2503.20776', 'title': 'Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields', 'url': 'https://huggingface.co/papers/2503.20776', 'abstract': 'Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The "X" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction.', 'score': 6, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'd066b6a18982ec5f', 'authors': ['Shijie Zhou', 'Hui Ren', 'Yijia Weng', 'Shuwang Zhang', 'Zhen Wang', 'Dejia Xu', 'Zhiwen Fan', 'Suya You', 'Zhangyang Wang', 'Leonidas Guibas', 'Achuta Kadambi'], 'affiliations': ['DEVCOM ARL', 'MIT', 'Stanford', 'UCLA', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2503.20776.jpg', 'data': {'categories': ['#3d', '#cv', '#agi', '#agents', '#dataset', '#multimodal', '#optimization'], 'emoji': '🌐', 'ru': {'title': 'Feature4X: Универсальный мост между 2D и 4D компьютерным зрением', 'desc': 'Статья представляет Feature4X - универсальную систему для расширения функциональности 2D моделей компьютерного зрения на 4D пространство, используя только монокулярное видео. Feature4X позволяет выполнять сегментацию, редактирование и ответы на вопросы в 4D сценах с помощью языковых подсказок. В основе подхода лежит оптимизация, объединяющая возможности нескольких моделей в единое представление. Это первый метод, который преобразует признаки видео-моделей в явное 4D поле признаков с помощью Gaussian Splatting.'}, 'en': {'title': 'Feature4X: Bridging 2D Vision to 4D Interaction', 'desc': 'This paper presents Feature4X, a novel framework that enhances 2D vision models to operate in 4D environments using only monocular video inputs. It addresses the challenge of limited annotated datasets for 3D/4D tasks by enabling versatile interactions and semantic operations in complex scenes. The framework employs a dynamic optimization strategy to unify various model capabilities into a single representation, allowing for adaptable feature extraction. Feature4X is the first to distill video foundation model features into a 4D feature field, facilitating advanced tasks like segmentation, scene editing, and visual question answering with improved context awareness.'}, 'zh': {'title': 'Feature4X：将2D视觉扩展到4D的通用框架', 'desc': '本文介绍了一种名为Feature4X的通用框架，旨在将2D视觉基础模型的功能扩展到4D领域。该框架仅使用单目视频输入，解决了3D/4D数据集稀缺的问题。Feature4X通过动态优化策略，将多种模型能力统一为单一表示，支持开放词汇和基于提示的分割、语言引导编辑和视觉问答等任务。实验结果表明，该方法能够实现新视角的分割、几何和外观场景编辑，以及跨时间步的自由形式视觉问答，推动了智能代理AI应用的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.21780', 'title': 'Semantic Library Adaptation: LoRA Retrieval and Fusion for\n  Open-Vocabulary Semantic Segmentation', 'url': 'https://huggingface.co/papers/2503.21780', 'abstract': "Open-vocabulary semantic segmentation models associate vision and text to label pixels from an undefined set of classes using textual queries, providing versatile performance on novel datasets. However, large shifts between training and test domains degrade their performance, requiring fine-tuning for effective real-world applications. We introduce Semantic Library Adaptation (SemLA), a novel framework for training-free, test-time domain adaptation. SemLA leverages a library of LoRA-based adapters indexed with CLIP embeddings, dynamically merging the most relevant adapters based on proximity to the target domain in the embedding space. This approach constructs an ad-hoc model tailored to each specific input without additional training. Our method scales efficiently, enhances explainability by tracking adapter contributions, and inherently protects data privacy, making it ideal for sensitive applications. Comprehensive experiments on a 20-domain benchmark built over 10 standard datasets demonstrate SemLA's superior adaptability and performance across diverse settings, establishing a new standard in domain adaptation for open-vocabulary semantic segmentation.", 'score': 5, 'issue_id': 2954, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': 'f676e21908b87897', 'authors': ['Reza Qorbani', 'Gianluca Villani', 'Theodoros Panagiotakopoulos', 'Marc Botet Colomer', 'Linus Härenstam-Nielsen', 'Mattia Segu', 'Pier Luigi Dovesi', 'Jussi Karlgren', 'Daniel Cremers', 'Federico Tombari', 'Matteo Poggi'], 'affiliations': ['AMD', 'ETH Zurich', 'Google', 'KTH', 'King', 'Munich Center for Machine Learning', 'Silo AI', 'Technical University of Munich', 'The Good AI Lab', 'University of Bologna', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.21780.jpg', 'data': {'categories': ['#training', '#architecture', '#transfer_learning', '#dataset', '#interpretability', '#security', '#benchmark'], 'emoji': '🔄', 'ru': {'title': 'Адаптация моделей сегментации на лету без переобучения', 'desc': 'Статья представляет новый метод адаптации моделей сегментации изображений к новым доменам без дополнительного обучения. Метод SemLA использует библиотеку LoRA-адаптеров, индексированных с помощью CLIP-эмбеддингов. Для каждого входного изображения динамически выбираются и объединяются наиболее релевантные адаптеры. Эксперименты на 20 доменах показали превосходство SemLA в адаптивности и производительности.'}, 'en': {'title': 'Adapt and Conquer: Dynamic Domain Adaptation for Semantic Segmentation', 'desc': 'This paper presents a new method called Semantic Library Adaptation (SemLA) for improving open-vocabulary semantic segmentation models. These models can label pixels using text queries but struggle when the training and testing data are very different. SemLA allows the model to adapt to new domains at test time without needing additional training by using a library of pre-trained adapters. The method is efficient, enhances model explainability, and protects data privacy, making it suitable for sensitive applications.'}, 'zh': {'title': '无训练的领域适应，提升语义分割性能', 'desc': '本文提出了一种新的框架，称为语义库适应（SemLA），用于无训练的测试时领域适应，旨在提高开放词汇语义分割模型在不同数据集上的表现。SemLA利用基于LoRA的适配器库，并通过CLIP嵌入索引，动态合并与目标领域最相关的适配器，从而构建针对特定输入的模型。该方法无需额外训练，能够高效扩展，并通过跟踪适配器的贡献来增强可解释性，同时保护数据隐私，适合敏感应用。通过在20个领域基准上的全面实验，SemLA展示了其在多种设置下的优越适应性和性能，确立了开放词汇语义分割领域适应的新标准。'}}}, {'id': 'https://huggingface.co/papers/2503.20853', 'title': 'Unified Multimodal Discrete Diffusion', 'url': 'https://huggingface.co/papers/2503.20853', 'abstract': 'Multimodal generative models that can understand and generate across multiple modalities are dominated by autoregressive (AR) approaches, which process tokens sequentially from left to right, or top to bottom. These models jointly handle images, text, video, and audio for various tasks such as image captioning, question answering, and image generation. In this work, we explore discrete diffusion models as a unified generative formulation in the joint text and image domain, building upon their recent success in text generation. Discrete diffusion models offer several advantages over AR models, including improved control over quality versus diversity of generated samples, the ability to perform joint multimodal inpainting (across both text and image domains), and greater controllability in generation through guidance. Leveraging these benefits, we present the first Unified Multimodal Discrete Diffusion (UniDisc) model which is capable of jointly understanding and generating text and images for a variety of downstream tasks. We compare UniDisc to multimodal AR models, performing a scaling analysis and demonstrating that UniDisc outperforms them in terms of both performance and inference-time compute, enhanced controllability, editability, inpainting, and flexible trade-off between inference time and generation quality. Code and additional visualizations are available at https://unidisc.github.io.', 'score': 5, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '9650b4dd1188fcc0', 'authors': ['Alexander Swerdlow', 'Mihir Prabhudesai', 'Siddharth Gandhi', 'Deepak Pathak', 'Katerina Fragkiadaki'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.20853.jpg', 'data': {'categories': ['#cv', '#training', '#audio', '#multimodal', '#video', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'UniDisc: Новый подход к мультимодальному машинному обучению', 'desc': 'В статье представлена модель UniDisc, первая унифицированная мультимодальная модель дискретной диффузии, способная совместно понимать и генерировать текст и изображения для различных задач. Модель UniDisc предлагает ряд преимуществ по сравнению с авторегрессионными моделями, включая улучшенный контроль над качеством и разнообразием генерируемых образцов, возможность совместного мультимодального инпейнтинга и большую управляемость при генерации. Авторы проводят сравнительный анализ с мультимодальными авторегрессионными моделями, демонстрируя превосходство UniDisc по производительности и вычислительным затратам при выводе. Модель также обеспечивает гибкий компромисс между временем вывода и качеством генерации.'}, 'en': {'title': 'Revolutionizing Multimodal Generation with UniDisc!', 'desc': 'This paper introduces the Unified Multimodal Discrete Diffusion (UniDisc) model, which is designed to generate and understand both text and images simultaneously. Unlike traditional autoregressive models that process data sequentially, UniDisc utilizes discrete diffusion techniques to enhance the quality and diversity of generated outputs. The model excels in tasks such as multimodal inpainting and offers improved controllability and editability during generation. Through extensive comparisons, UniDisc demonstrates superior performance and efficiency over existing multimodal autoregressive approaches.'}, 'zh': {'title': '统一多模态生成，超越自回归模型！', 'desc': '这篇论文探讨了一种新的多模态生成模型，称为统一多模态离散扩散模型（UniDisc）。与传统的自回归模型不同，UniDisc能够同时理解和生成文本与图像，适用于多种任务。该模型在生成样本的质量与多样性之间提供了更好的控制，并且能够进行跨文本和图像的联合修复。通过与自回归模型的比较，UniDisc在性能、计算效率和可控性等方面表现更优。'}}}, {'id': 'https://huggingface.co/papers/2503.20578', 'title': 'LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation', 'url': 'https://huggingface.co/papers/2503.20578', 'abstract': 'Failure-inducing inputs play a crucial role in diagnosing and analyzing software bugs. Bug reports typically contain these inputs, which developers extract to facilitate debugging. Since bug reports are written in natural language, prior research has leveraged various Natural Language Processing (NLP) techniques for automated input extraction. With the advent of Large Language Models (LLMs), an important research question arises: how effectively can generative LLMs extract failure-inducing inputs from bug reports? In this paper, we propose LLPut, a technique to empirically evaluate the performance of three open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in extracting relevant inputs from bug reports. We conduct an experimental evaluation on a dataset of 206 bug reports to assess the accuracy and effectiveness of these models. Our findings provide insights into the capabilities and limitations of generative LLMs in automated bug diagnosis.', 'score': 4, 'issue_id': 2941, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '5ee433dad4dd5d00', 'authors': ['Alif Al Hasan', 'Subarna Saha', 'Mia Mohammad Imran', 'Tarannum Shaila Zaman'], 'affiliations': ['Jahangirnagar University Dhaka, Bangladesh', 'Missouri University of Science and Technology Rolla, Missouri, USA', 'University of Maryland Baltimore County Bsltimore, Maryland, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.20578.jpg', 'data': {'categories': ['#open_source', '#science', '#dataset', '#multimodal', '#data'], 'emoji': '🐛', 'ru': {'title': 'Генеративные языковые модели на страже отладки: извлечение данных об ошибках из отчетов', 'desc': 'Это исследование посвящено оценке эффективности генеративных языковых моделей (LLM) в извлечении входных данных, вызывающих ошибки, из отчетов о багах. Авторы предлагают методику LLPut для оценки производительности трех открытых LLM: LLaMA, Qwen и Qwen-Coder. Эксперимент проводился на наборе из 206 отчетов о багах для оценки точности и эффективности этих моделей. Результаты дают представление о возможностях и ограничениях генеративных LLM в автоматизированной диагностике ошибок.'}, 'en': {'title': 'Harnessing LLMs for Smart Bug Diagnosis', 'desc': 'This paper investigates how well generative Large Language Models (LLMs) can extract failure-inducing inputs from bug reports, which are essential for diagnosing software issues. The authors introduce a technique called LLPut to evaluate the performance of three open-source LLMs: LLaMA, Qwen, and Qwen-Coder. They conduct experiments on a dataset of 206 bug reports to measure the accuracy and effectiveness of these models in identifying relevant inputs. The results offer valuable insights into the strengths and weaknesses of using generative LLMs for automated bug diagnosis.'}, 'zh': {'title': '利用LLM提升缺陷报告分析的效率', 'desc': '本文探讨了如何利用大型语言模型（LLMs）从软件缺陷报告中提取导致故障的输入。我们提出了一种名为LLPut的技术，旨在评估三种开源生成性LLM（LLaMA、Qwen和Qwen-Coder）在这一任务中的表现。通过对206个缺陷报告的数据集进行实验评估，我们分析了这些模型的准确性和有效性。研究结果揭示了生成性LLM在自动化缺陷诊断中的能力和局限性。'}}}, {'id': 'https://huggingface.co/papers/2503.21541', 'title': 'LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized\n  Text-Guided Image Editing', 'url': 'https://huggingface.co/papers/2503.21541', 'abstract': 'Text-guided image editing aims to modify specific regions of an image according to natural language instructions while maintaining the general structure and the background fidelity. Existing methods utilize masks derived from cross-attention maps generated from diffusion models to identify the target regions for modification. However, since cross-attention mechanisms focus on semantic relevance, they struggle to maintain the image integrity. As a result, these methods often lack spatial consistency, leading to editing artifacts and distortions. In this work, we address these limitations and introduce LOCATEdit, which enhances cross-attention maps through a graph-based approach utilizing self-attention-derived patch relationships to maintain smooth, coherent attention across image regions, ensuring that alterations are limited to the designated items while retaining the surrounding structure. \\method consistently and substantially outperforms existing baselines on PIE-Bench, demonstrating its state-of-the-art performance and effectiveness on various editing tasks. Code can be found on https://github.com/LOCATEdit/LOCATEdit/', 'score': 1, 'issue_id': 2950, 'pub_date': '2025-03-27', 'pub_date_card': {'ru': '27 марта', 'en': 'March 27', 'zh': '3月27日'}, 'hash': '38e7c3d2a3738793', 'authors': ['Achint Soni', 'Meet Soni', 'Sirisha Rambhatla'], 'affiliations': ['Stony Brook University', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.21541.jpg', 'data': {'categories': ['#cv', '#multimodal', '#architecture', '#open_source', '#games', '#optimization', '#graphs', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Точное редактирование изображений с сохранением структуры', 'desc': 'LOCATEdit - это новый метод редактирования изображений на основе текстовых инструкций. Он использует графовый подход для улучшения карт внимания, полученных из диффузионных моделей. Это позволяет сохранять целостность изображения и избегать артефактов при редактировании. LOCATEdit превосходит существующие методы на бенчмарке PIE-Bench в различных задачах редактирования.'}, 'en': {'title': 'Enhancing Text-Guided Image Editing with LOCATEdit', 'desc': 'This paper presents LOCATEdit, a novel approach for text-guided image editing that improves upon existing methods by addressing issues of spatial consistency. Traditional techniques rely on cross-attention maps from diffusion models, which can lead to artifacts due to their focus on semantic relevance rather than spatial integrity. LOCATEdit enhances these maps using a graph-based method that leverages self-attention to maintain coherent attention across different regions of the image. The results show that LOCATEdit significantly outperforms current baselines on the PIE-Bench dataset, proving its effectiveness in preserving the overall structure while allowing precise modifications.'}, 'zh': {'title': '精确图像编辑，保持结构完整性', 'desc': '本文介绍了一种名为LOCATEdit的图像编辑方法，旨在根据自然语言指令修改图像的特定区域，同时保持整体结构和背景的完整性。现有方法使用来自扩散模型的交叉注意力图生成的掩码来识别目标区域，但由于交叉注意力机制关注语义相关性，导致图像完整性难以保持。LOCATEdit通过基于图的自注意力方法增强交叉注意力图，确保图像区域之间的平滑一致性，从而限制修改仅在指定项目上，同时保留周围结构。实验结果表明，LOCATEdit在PIE-Bench上显著优于现有基线，展示了其在各种编辑任务中的先进性能和有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.19904', 'title': 'Tracktention: Leveraging Point Tracking to Attend Videos Faster and\n  Better', 'url': 'https://huggingface.co/papers/2503.19904', 'abstract': 'Temporal consistency is critical in video prediction to ensure that outputs are coherent and free of artifacts. Traditional methods, such as temporal attention and 3D convolution, may struggle with significant object motion and may not capture long-range temporal dependencies in dynamic scenes. To address this gap, we propose the Tracktention Layer, a novel architectural component that explicitly integrates motion information using point tracks, i.e., sequences of corresponding points across frames. By incorporating these motion cues, the Tracktention Layer enhances temporal alignment and effectively handles complex object motions, maintaining consistent feature representations over time. Our approach is computationally efficient and can be seamlessly integrated into existing models, such as Vision Transformers, with minimal modification. It can be used to upgrade image-only models to state-of-the-art video ones, sometimes outperforming models natively designed for video prediction. We demonstrate this on video depth prediction and video colorization, where models augmented with the Tracktention Layer exhibit significantly improved temporal consistency compared to baselines.', 'score': 1, 'issue_id': 2957, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '14b58ae496ab3e3b', 'authors': ['Zihang Lai', 'Andrea Vedaldi'], 'affiliations': ['Visual Geometry Group (VGG), University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.19904.jpg', 'data': {'categories': ['#architecture', '#optimization', '#video'], 'emoji': '🎞️', 'ru': {'title': 'Tracktention Layer: новый подход к временной согласованности в видеопредсказании', 'desc': 'Статья представляет новый архитектурный компонент под названием Tracktention Layer для улучшения временной согласованности в задаче предсказания видео. Этот слой интегрирует информацию о движении, используя траектории точек между кадрами, что позволяет лучше обрабатывать сложные движения объектов. Tracktention Layer можно легко интегрировать в существующие модели, такие как Vision Transformers, с минимальными изменениями. Авторы демонстрируют эффективность подхода на задачах предсказания глубины и колоризации видео, где модели с Tracktention Layer показывают значительно улучшенную временную согласованность по сравнению с базовыми моделями.'}, 'en': {'title': 'Enhancing Video Prediction with Tracktention for Temporal Consistency', 'desc': 'This paper introduces the Tracktention Layer, a new component designed to improve video prediction by enhancing temporal consistency. Traditional methods often fail to manage significant object motion and long-range dependencies, leading to artifacts in the output. The Tracktention Layer utilizes point tracks to integrate motion information, allowing for better alignment and representation of features over time. This approach is efficient and can be easily added to existing models, showing superior performance in tasks like video depth prediction and colorization.'}, 'zh': {'title': '提升视频预测的时间一致性', 'desc': '在视频预测中，时间一致性非常重要，以确保输出结果连贯且没有伪影。传统方法如时间注意力和3D卷积在处理显著物体运动时可能会遇到困难，无法捕捉动态场景中的长距离时间依赖关系。为了解决这个问题，我们提出了Tracktention层，这是一种新颖的架构组件，明确整合了运动信息，通过点轨迹来实现。通过引入这些运动线索，Tracktention层增强了时间对齐能力，有效处理复杂的物体运动，保持了一致的特征表示。'}}}, {'id': 'https://huggingface.co/papers/2503.20215', 'title': 'Qwen2.5-Omni Technical Report', 'url': 'https://huggingface.co/papers/2503.20215', 'abstract': "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.", 'score': 55, 'issue_id': 2924, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'dd7a3c8e8564b973', 'authors': ['Jin Xu', 'Zhifang Guo', 'Jinzheng He', 'Hangrui Hu', 'Ting He', 'Shuai Bai', 'Keqin Chen', 'Jialin Wang', 'Yang Fan', 'Kai Dang', 'Bin Zhang', 'Xiong Wang', 'Yunfei Chu', 'Junyang Lin'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.20215.jpg', 'data': {'categories': ['#architecture', '#agi', '#benchmark', '#multimodal', '#video', '#games', '#audio'], 'emoji': '🤖', 'ru': {'title': 'Qwen2.5-Omni: Мультимодальный ИИ нового поколения', 'desc': 'Qwen2.5-Omni - это мультимодальная модель, способная воспринимать текст, изображения, аудио и видео, генерируя при этом текстовые и речевые ответы в потоковом режиме. Модель использует блочную обработку для аудио и видео энкодеров, а также новый подход к позиционному кодированию TMRoPE для синхронизации временных меток. Архитектура Thinker-Talker позволяет одновременно генерировать текст и речь без взаимных помех. Qwen2.5-Omni демонстрирует высокую производительность на различных мультимодальных бенчмарках, включая Omni-Bench и MMLU.'}, 'en': {'title': 'Streamlining Multimodal Interaction with Qwen2.5-Omni', 'desc': 'Qwen2.5-Omni is a cutting-edge multimodal model that can process and generate responses across various formats, including text, images, audio, and video. It employs a unique block-wise processing method for audio and visual data to facilitate real-time streaming. The model features a dual architecture, where the Thinker generates text and the Talker produces audio, ensuring smooth interaction between the two. With innovative techniques like TMRoPE for timestamp alignment and a sliding-window DiT for audio decoding, Qwen2.5-Omni sets new benchmarks in multimodal performance and speech generation.'}, 'zh': {'title': '多模态流式生成的未来', 'desc': '本文介绍了Qwen2.5-Omni，这是一个端到端的多模态模型，能够处理文本、图像、音频和视频等多种输入，同时生成文本和自然语音响应。为了实现多模态信息的流式处理，音频和视觉编码器采用了块处理的方法，并通过一种新颖的位置嵌入方法TMRoPE来同步音频和视频的时间戳。该模型的Thinker-Talker架构使得文本生成和语音生成可以并行进行，避免了两者之间的干扰。Qwen2.5-Omni在多模态基准测试中表现出色，尤其在流式语音生成方面，其性能优于大多数现有的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2503.19757', 'title': 'Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy', 'url': 'https://huggingface.co/papers/2503.19757', 'abstract': "While recent vision-language-action models trained on diverse robot datasets exhibit promising generalization capabilities with limited in-domain data, their reliance on compact action heads to predict discretized or continuous actions constrains adaptability to heterogeneous action spaces. We present Dita, a scalable framework that leverages Transformer architectures to directly denoise continuous action sequences through a unified multimodal diffusion process. Departing from prior methods that condition denoising on fused embeddings via shallow networks, Dita employs in-context conditioning -- enabling fine-grained alignment between denoised actions and raw visual tokens from historical observations. This design explicitly models action deltas and environmental nuances. By scaling the diffusion action denoiser alongside the Transformer's scalability, Dita effectively integrates cross-embodiment datasets across diverse camera perspectives, observation scenes, tasks, and action spaces. Such synergy enhances robustness against various variances and facilitates the successful execution of long-horizon tasks. Evaluations across extensive benchmarks demonstrate state-of-the-art or comparative performance in simulation. Notably, Dita achieves robust real-world adaptation to environmental variances and complex long-horizon tasks through 10-shot finetuning, using only third-person camera inputs. The architecture establishes a versatile, lightweight and open-source baseline for generalist robot policy learning. Project Page: https://robodita.github.io.", 'score': 39, 'issue_id': 2920, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'f481410d892c371a', 'authors': ['Zhi Hou', 'Tianyi Zhang', 'Yuwen Xiong', 'Haonan Duan', 'Hengjun Pu', 'Ronglei Tong', 'Chengyang Zhao', 'Xizhou Zhu', 'Yu Qiao', 'Jifeng Dai', 'Yuntao Chen'], 'affiliations': ['Center for Artificial Intelligence and Robotics, HKISI, CAS', 'College of Computer Science and Technology, Zhejiang University', 'MMLab, The Chinese University of Hong Kong', 'Peking University', 'SenseTime Research', 'Shanghai AI Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19757.jpg', 'data': {'categories': ['#cv', '#open_source', '#multimodal', '#architecture', '#diffusion', '#agents', '#training'], 'emoji': '🤖', 'ru': {'title': 'Универсальное обучение роботов с помощью диффузионных трансформеров', 'desc': 'Статья представляет Dita - масштабируемую модель для обучения роботов, использующую архитектуру трансформеров для денойзинга непрерывных последовательностей действий. В отличие от предыдущих подходов, Dita применяет контекстное обусловливание, что позволяет точнее согласовывать действия с визуальными данными. Модель эффективно интегрирует разнородные наборы данных, повышая устойчивость к различным вариациям среды. Эксперименты показывают высокую производительность Dita как в симуляции, так и в реальном мире при адаптации к сложным долгосрочным задачам.'}, 'en': {'title': 'Dita: Transforming Robot Action Learning with Multimodal Diffusion', 'desc': 'The paper introduces Dita, a new framework that improves how robots learn to perform actions by using advanced Transformer models. Unlike previous methods that used simple networks to predict actions, Dita employs a sophisticated approach called in-context conditioning, which helps align actions more closely with visual information from past experiences. This allows Dita to effectively handle a wide range of actions and environments, making it adaptable to different tasks and camera views. The results show that Dita not only performs well in simulations but also adapts successfully to real-world scenarios with minimal additional training.'}, 'zh': {'title': 'Dita：提升机器人适应能力的创新框架', 'desc': '本文介绍了一种名为Dita的框架，旨在提高机器人在多样化动作空间中的适应能力。Dita利用Transformer架构，通过统一的多模态扩散过程直接去噪连续动作序列，克服了传统方法的局限。该框架通过上下文条件化实现了去噪动作与历史观察的原始视觉标记之间的精细对齐，从而更好地建模动作变化和环境细节。Dita在多种基准测试中表现出色，能够有效适应真实世界的环境变化，并成功执行复杂的长期任务。'}}}, {'id': 'https://huggingface.co/papers/2503.20314', 'title': 'Wan: Open and Advanced Large-Scale Video Generative Models', 'url': 'https://huggingface.co/papers/2503.20314', 'abstract': "This report presents Wan, a comprehensive and open suite of video foundation models designed to push the boundaries of video generation. Built upon the mainstream diffusion transformer paradigm, Wan achieves significant advancements in generative capabilities through a series of innovations, including our novel VAE, scalable pre-training strategies, large-scale data curation, and automated evaluation metrics. These contributions collectively enhance the model's performance and versatility. Specifically, Wan is characterized by four key features: Leading Performance: The 14B model of Wan, trained on a vast dataset comprising billions of images and videos, demonstrates the scaling laws of video generation with respect to both data and model size. It consistently outperforms the existing open-source models as well as state-of-the-art commercial solutions across multiple internal and external benchmarks, demonstrating a clear and significant performance superiority. Comprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B parameters, for efficiency and effectiveness respectively. It also covers multiple downstream applications, including image-to-video, instruction-guided video editing, and personal video generation, encompassing up to eight tasks. Consumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource efficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range of consumer-grade GPUs. Openness: We open-source the entire series of Wan, including source code and all models, with the goal of fostering the growth of the video generation community. This openness seeks to significantly expand the creative possibilities of video production in the industry and provide academia with high-quality video foundation models. All the code and models are available at https://github.com/Wan-Video/Wan2.1.", 'score': 29, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'e9770d8e9d313979', 'authors': ['WanTeam', ':', 'Ang Wang', 'Baole Ai', 'Bin Wen', 'Chaojie Mao', 'Chen-Wei Xie', 'Di Chen', 'Feiwu Yu', 'Haiming Zhao', 'Jianxiao Yang', 'Jianyuan Zeng', 'Jiayu Wang', 'Jingfeng Zhang', 'Jingren Zhou', 'Jinkai Wang', 'Jixuan Chen', 'Kai Zhu', 'Kang Zhao', 'Keyu Yan', 'Lianghua Huang', 'Mengyang Feng', 'Ningyi Zhang', 'Pandeng Li', 'Pingyu Wu', 'Ruihang Chu', 'Ruili Feng', 'Shiwei Zhang', 'Siyang Sun', 'Tao Fang', 'Tianxing Wang', 'Tianyi Gui', 'Tingyu Weng', 'Tong Shen', 'Wei Lin', 'Wei Wang', 'Wei Wang', 'Wenmeng Zhou', 'Wente Wang', 'Wenting Shen', 'Wenyuan Yu', 'Xianzhong Shi', 'Xiaoming Huang', 'Xin Xu', 'Yan Kou', 'Yangyu Lv', 'Yifei Li', 'Yijing Liu', 'Yiming Wang', 'Yingya Zhang', 'Yitong Huang', 'Yong Li', 'You Wu', 'Yu Liu', 'Yulin Pan', 'Yun Zheng', 'Yuntao Hong', 'Yupeng Shi', 'Yutong Feng', 'Zeyinzi Jiang', 'Zhen Han', 'Zhi-Fan Wu', 'Ziyu Liu'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.20314.jpg', 'data': {'categories': ['#video', '#open_source', '#multimodal', '#architecture', '#diffusion', '#benchmark', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Wan: Открытый набор передовых видео-моделей для революции в генерации видео', 'desc': 'Статья представляет Wan - комплексный набор видео-моделей, основанных на архитектуре диффузионного трансформера. Wan достигает значительных улучшений в генеративных возможностях благодаря инновациям, включая новый VAE, масштабируемые стратегии предобучения и автоматизированные метрики оценки. Модель Wan с 14 миллиардами параметров, обученная на огромном наборе данных, демонстрирует превосходную производительность по сравнению с существующими открытыми и коммерческими решениями. Wan предлагает эффективные модели для различных задач генерации видео и открыт для сообщества, что способствует развитию технологий в этой области.'}, 'en': {'title': 'Wan: Revolutionizing Video Generation with Open-Source Models', 'desc': 'This paper introduces Wan, a suite of video foundation models that enhances video generation using a diffusion transformer approach. Wan features a novel Variational Autoencoder (VAE) and scalable pre-training strategies, which improve its generative capabilities. The 14B model, trained on a massive dataset, showcases superior performance compared to existing models, while the 1.3B model offers efficiency for consumer-grade hardware. By open-sourcing the models and code, Wan aims to support the video generation community and expand creative opportunities in video production.'}, 'zh': {'title': '推动视频生成的开放模型——Wan', 'desc': '本报告介绍了Wan，这是一个全面且开放的视频基础模型套件，旨在推动视频生成的边界。Wan基于主流的扩散变换器范式，通过创新的变分自编码器（VAE）、可扩展的预训练策略、大规模数据整理和自动评估指标，显著提升了生成能力。Wan的14B模型在数十亿图像和视频的数据集上训练，展示了视频生成在数据和模型规模方面的扩展规律，超越了现有的开源模型和商业解决方案。该模型不仅高效且多功能，支持多种下游应用，且所有代码和模型均已开源，旨在促进视频生成社区的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.19990', 'title': 'LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?', 'url': 'https://huggingface.co/papers/2503.19990', 'abstract': "Multi-step spatial reasoning entails understanding and reasoning about spatial relationships across multiple sequential steps, which is crucial for tackling complex real-world applications, such as robotic manipulation, autonomous navigation, and automated assembly. To assess how well current Multimodal Large Language Models (MLLMs) have acquired this fundamental capability, we introduce LEGO-Puzzles, a scalable benchmark designed to evaluate both spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks. LEGO-Puzzles consists of 1,100 carefully curated visual question-answering (VQA) samples spanning 11 distinct tasks, ranging from basic spatial understanding to complex multi-step reasoning. Based on LEGO-Puzzles, we conduct a comprehensive evaluation of state-of-the-art MLLMs and uncover significant limitations in their spatial reasoning capabilities: even the most powerful MLLMs can answer only about half of the test cases, whereas human participants achieve over 90\\% accuracy. In addition to VQA tasks, we evaluate MLLMs' abilities to generate LEGO images following assembly illustrations. Our experiments show that only Gemini-2.0-Flash and GPT-4o exhibit a limited ability to follow these instructions, while other MLLMs either replicate the input image or generate completely irrelevant outputs. Overall, LEGO-Puzzles exposes critical deficiencies in existing MLLMs' spatial understanding and sequential reasoning capabilities, and underscores the need for further advancements in multimodal spatial reasoning.", 'score': 26, 'issue_id': 2922, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'ec85f1936ae0edd9', 'authors': ['Kexian Tang', 'Junyao Gao', 'Yanhong Zeng', 'Haodong Duan', 'Yanan Sun', 'Zhening Xing', 'Wenran Liu', 'Kaifeng Lyu', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Simons Institute, UC Berkeley', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19990.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#multimodal', '#reasoning'], 'emoji': '🧩', 'ru': {'title': 'LEGO-Puzzles: выявление пробелов в пространственном мышлении ИИ', 'desc': 'Статья представляет LEGO-Puzzles - новый бенчмарк для оценки пространственного мышления и последовательного рассуждения у мультимодальных больших языковых моделей (MLLM). Бенчмарк состоит из 1100 задач визуальных вопросов и ответов на основе LEGO, охватывающих различные аспекты пространственного мышления. Результаты показывают, что даже самые мощные MLLM справляются лишь с половиной тестов, в то время как люди достигают более 90% точности. Исследование также выявило ограниченные возможности MLLM в генерации изображений LEGO по инструкциям сборки.'}, 'en': {'title': 'LEGO-Puzzles: Unveiling Spatial Reasoning Gaps in MLLMs', 'desc': "This paper introduces LEGO-Puzzles, a benchmark designed to evaluate the spatial reasoning and sequential understanding capabilities of Multimodal Large Language Models (MLLMs). The benchmark consists of 1,100 visual question-answering samples across 11 tasks, ranging from basic to complex reasoning. The evaluation reveals that current MLLMs struggle with spatial reasoning, achieving only about 50% accuracy compared to over 90% for humans. Additionally, the study assesses MLLMs' ability to generate images based on assembly instructions, finding that only a couple of models perform adequately, highlighting significant gaps in MLLMs' spatial understanding."}, 'zh': {'title': 'LEGO-Puzzles：评估多模态语言模型的空间推理能力', 'desc': '多步骤空间推理是理解和推理空间关系的重要能力，尤其在复杂的现实应用中，如机器人操作和自动导航。为评估当前多模态大型语言模型（MLLMs）在这一能力上的表现，我们引入了LEGO-Puzzles，这是一个可扩展的基准，旨在通过基于LEGO的任务评估空间理解和顺序推理。LEGO-Puzzles包含1100个精心策划的视觉问答样本，涵盖从基本空间理解到复杂多步骤推理的11个不同任务。我们的评估显示，现有的MLLMs在空间推理能力上存在显著不足，最强的模型仅能回答约一半的测试案例，而人类参与者的准确率超过90%。'}}}, {'id': 'https://huggingface.co/papers/2503.20201', 'title': 'Open Deep Search: Democratizing Search with Open-source Reasoning Agents', 'url': 'https://huggingface.co/papers/2503.20201', 'abstract': "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES.", 'score': 22, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'a9b1bed8d26f5055', 'authors': ['Salaheddin Alzubi', 'Creston Brooks', 'Purva Chiniya', 'Edoardo Contente', 'Chiara von Gerlach', 'Lucas Irwin', 'Yihan Jiang', 'Arda Kaz', 'Windsor Nguyen', 'Sewoong Oh', 'Himanshu Tyagi', 'Pramod Viswanath'], 'affiliations': ['Princeton University', 'Sentient', 'UC Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.20201.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#agents', '#benchmark', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'ODS: открытый ИИ-поиск на уровне проприетарных решений', 'desc': 'Open Deep Search (ODS) - это новая система, объединяющая возможности открытых языковых моделей с инструментами веб-поиска для ответов на запросы. ODS состоит из двух компонентов: Open Search Tool (инструмент поиска) и Open Reasoning Agent (агент рассуждений), которые работают с базовой языковой моделью по выбору пользователя. Система демонстрирует высокую эффективность на бенчмарках SimpleQA и FRAMES, превосходя существующие решения. ODS позволяет улучшить возможности любых языковых моделей, добавляя им функции поиска и рассуждений.'}, 'en': {'title': 'Empowering Open-Source LLMs with Advanced Search and Reasoning', 'desc': 'Open Deep Search (ODS) is a new framework designed to enhance the reasoning abilities of open-source large language models (LLMs) by integrating them with advanced web search tools. It features two main components: the Open Search Tool, which performs web searches, and the Open Reasoning Agent, which interprets tasks and coordinates actions, including using the search tool. ODS has shown to significantly improve performance on benchmarks like SimpleQA and FRAMES, even surpassing proprietary models like GPT-4o Search Preview in accuracy. This innovation allows users to leverage powerful open-source LLMs while achieving state-of-the-art results in query answering tasks.'}, 'zh': {'title': '开放深度搜索：提升开源LLM的推理与搜索能力', 'desc': '我们介绍了开放深度搜索（ODS），旨在缩小专有搜索人工智能解决方案与开源解决方案之间的差距。ODS的主要创新是通过推理代理增强最新开源大语言模型（LLM）的推理能力，使其能够明智地使用网络搜索工具来回答查询。ODS由两个组件组成：开放搜索工具和开放推理代理，后者负责解释任务并协调一系列操作，包括调用工具。通过与强大的开源推理LLM（如DeepSeek-R1）结合，ODS在SimpleQA和FRAMES两个基准测试中几乎达到了现有的最先进水平，甚至在FRAMES基准上提高了9.7%的准确率。'}}}, {'id': 'https://huggingface.co/papers/2503.20240', 'title': 'Unconditional Priors Matter! Improving Conditional Generation of\n  Fine-Tuned Diffusion Models', 'url': 'https://huggingface.co/papers/2503.20240', 'abstract': 'Classifier-Free Guidance (CFG) is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation. Inspired by the fact that most CFG-based conditional models are trained by fine-tuning a base model with better unconditional generation, we first show that simply replacing the unconditional noise in CFG with that predicted by the base model can significantly improve conditional generation. Furthermore, we show that a diffusion model other than the one the fine-tuned model was trained on can be used for unconditional noise replacement. We experimentally verify our claim with a range of CFG-based conditional models for both image and video generation, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and InstructPix2Pix.', 'score': 19, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '4add99d8d7510263', 'authors': ['Prin Phunyaphibarn', 'Phillip Y. Lee', 'Jaihoon Kim', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.20240.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#video'], 'emoji': '🔀', 'ru': {'title': 'Улучшение CFG: замена безусловного шума для качественной генерации', 'desc': 'Статья посвящена улучшению метода Classifier-Free Guidance (CFG) в обучении условных диффузионных моделей. Авторы обнаружили, что совместное обучение условного и безусловного предсказания шума приводит к ухудшению качества генерации. Они предлагают заменять безусловный шум в CFG на предсказания базовой модели, что значительно улучшает условную генерацию. Эксперименты подтверждают эффективность этого подхода для различных моделей генерации изображений и видео.'}, 'en': {'title': 'Enhancing Conditional Generation with Improved Unconditional Noise', 'desc': 'This paper discusses Classifier-Free Guidance (CFG), a technique used in training conditional diffusion models. The authors identify that using a single network for both conditional and unconditional noise prediction leads to poor performance in generating unconditional noise, which negatively impacts the quality of conditional outputs. They propose a solution where the unconditional noise predictions are replaced with those from a better-performing base model, resulting in improved conditional generation. The findings are validated through experiments on various CFG-based models for generating images and videos, demonstrating the effectiveness of their approach.'}, 'zh': {'title': '提升条件生成质量的无条件噪声替代', 'desc': '无分类器引导（CFG）是一种在训练条件扩散模型中使用的基本技术。传统的CFG训练方法是使用单一网络同时学习条件和无条件噪声预测，但这种联合学习会导致无条件噪声的先验质量较差。研究表明，使用更好的无条件生成模型的噪声替代可以显著提高条件生成的质量。我们通过实验验证了这一点，适用于多种基于CFG的条件模型，包括图像和视频生成。'}}}, {'id': 'https://huggingface.co/papers/2503.19480', 'title': 'GenHancer: Imperfect Generative Models are Secretly Strong\n  Vision-Centric Enhancers', 'url': 'https://huggingface.co/papers/2503.19480', 'abstract': "The synergy between generative and discriminative models receives growing attention. While discriminative Contrastive Language-Image Pre-Training (CLIP) excels in high-level semantics, it struggles with perceiving fine-grained visual details. Generally, to enhance representations, generative models take CLIP's visual features as conditions for reconstruction. However, the underlying principle remains underexplored. In this work, we empirically found that visually perfect generations are not always optimal for representation enhancement. The essence lies in effectively extracting fine-grained knowledge from generative models while mitigating irrelevant information. To explore critical factors, we delve into three aspects: (1) Conditioning mechanisms: We found that even a small number of local tokens can drastically reduce the difficulty of reconstruction, leading to collapsed training. We thus conclude that utilizing only global visual tokens as conditions is the most effective strategy. (2) Denoising configurations: We observed that end-to-end training introduces extraneous information. To address this, we propose a two-stage training strategy to prioritize learning useful visual knowledge. Additionally, we demonstrate that lightweight denoisers can yield remarkable improvements. (3) Generation paradigms: We explore both continuous and discrete denoisers with desirable outcomes, validating the versatility of our method. Through our in-depth explorations, we have finally arrived at an effective method, namely GenHancer, which consistently outperforms prior arts on the MMVP-VLM benchmark, e.g., 6.0% on OpenAICLIP. The enhanced CLIP can be further plugged into multimodal large language models for better vision-centric performance. All the models and codes are made publicly available.", 'score': 14, 'issue_id': 2920, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '0dc97844010f5fb0', 'authors': ['Shijie Ma', 'Yuying Ge', 'Teng Wang', 'Yuxin Guo', 'Yixiao Ge', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Institute of Automation, CAS'], 'pdf_title_img': 'assets/pdf/title_img/2503.19480.jpg', 'data': {'categories': ['#cv', '#open_source', '#multimodal', '#architecture', '#optimization', '#benchmark', '#training'], 'emoji': '🔬', 'ru': {'title': 'GenHancer: Улучшение визуальных репрезентаций через синергию генеративных и дискриминативных моделей', 'desc': 'Статья исследует синергию между генеративными и дискриминативными моделями в контексте улучшения визуальных представлений. Авторы обнаружили, что визуально идеальные генерации не всегда оптимальны для улучшения репрезентаций и предложили метод GenHancer. Этот метод фокусируется на эффективном извлечении детальных знаний из генеративных моделей, используя глобальные визуальные токены и двухэтапную стратегию обучения. GenHancer превосходит предыдущие методы на бенчмарке MMVP-VLM, демонстрируя улучшение на 6.0% для OpenAICLIP.'}, 'en': {'title': 'Enhancing CLIP with GenHancer: Bridging Generative and Discriminative Models', 'desc': 'This paper investigates the relationship between generative and discriminative models in machine learning, specifically focusing on enhancing the performance of the Contrastive Language-Image Pre-Training (CLIP) model. The authors found that while generative models can improve representations, perfect visual generations do not always lead to optimal results. They propose a method called GenHancer, which effectively extracts fine-grained knowledge while reducing irrelevant information through careful conditioning and denoising strategies. Their approach outperforms existing methods on the MMVP-VLM benchmark, demonstrating its potential for improving vision-centric tasks in multimodal large language models.'}, 'zh': {'title': '生成与判别模型的完美结合', 'desc': '本文探讨了生成模型与判别模型之间的协同作用，特别是如何利用生成模型来增强判别模型CLIP的表示能力。研究发现，视觉上完美的生成并不总是最优的表示增强方式，关键在于有效提取细粒度知识并减少无关信息。我们提出了GenHancer方法，通过优化条件机制、去噪配置和生成范式，显著提升了CLIP在多模态任务中的表现。最终，GenHancer在MMVP-VLM基准测试中超越了之前的研究成果，展示了其在视觉中心性能上的优势。'}}}, {'id': 'https://huggingface.co/papers/2503.20020', 'title': 'Gemini Robotics: Bringing AI into the Physical World', 'url': 'https://huggingface.co/papers/2503.20020', 'abstract': "Recent advancements in large multimodal models have led to the emergence of remarkable generalist capabilities in digital domains, yet their translation to physical agents such as robots remains a significant challenge. This report introduces a new family of AI models purposefully designed for robotics and built upon the foundation of Gemini 2.0. We present Gemini Robotics, an advanced Vision-Language-Action (VLA) generalist model capable of directly controlling robots. Gemini Robotics executes smooth and reactive movements to tackle a wide range of complex manipulation tasks while also being robust to variations in object types and positions, handling unseen environments as well as following diverse, open vocabulary instructions. We show that with additional fine-tuning, Gemini Robotics can be specialized to new capabilities including solving long-horizon, highly dexterous tasks, learning new short-horizon tasks from as few as 100 demonstrations and adapting to completely novel robot embodiments. This is made possible because Gemini Robotics builds on top of the Gemini Robotics-ER model, the second model we introduce in this work. Gemini Robotics-ER (Embodied Reasoning) extends Gemini's multimodal reasoning capabilities into the physical world, with enhanced spatial and temporal understanding. This enables capabilities relevant to robotics including object detection, pointing, trajectory and grasp prediction, as well as multi-view correspondence and 3D bounding box predictions. We show how this novel combination can support a variety of robotics applications. We also discuss and address important safety considerations related to this new class of robotics foundation models. The Gemini Robotics family marks a substantial step towards developing general-purpose robots that realizes AI's potential in the physical world.", 'score': 13, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '5edeeaed81b90426', 'authors': ['Gemini Robotics Team', 'Saminda Abeyruwan', 'Joshua Ainslie', 'Jean-Baptiste Alayrac', 'Montserrat Gonzalez Arenas', 'Travis Armstrong', 'Ashwin Balakrishna', 'Robert Baruch', 'Maria Bauza', 'Michiel Blokzijl', 'Steven Bohez', 'Konstantinos Bousmalis', 'Anthony Brohan', 'Thomas Buschmann', 'Arunkumar Byravan', 'Serkan Cabi', 'Ken Caluwaerts', 'Federico Casarini', 'Oscar Chang', 'Jose Enrique Chen', 'Xi Chen', 'Hao-Tien Lewis Chiang', 'Krzysztof Choromanski', "David D'Ambrosio", 'Sudeep Dasari', 'Todor Davchev', 'Coline Devin', 'Norman Di Palo', 'Tianli Ding', 'Adil Dostmohamed', 'Danny Driess', 'Yilun Du', 'Debidatta Dwibedi', 'Michael Elabd', 'Claudio Fantacci', 'Cody Fong', 'Erik Frey', 'Chuyuan Fu', 'Marissa Giustina', 'Keerthana Gopalakrishnan', 'Laura Graesser', 'Leonard Hasenclever', 'Nicolas Heess', 'Brandon Hernaez', 'Alexander Herzog', 'R. Alex Hofer', 'Jan Humplik', 'Atil Iscen', 'Mithun George Jacob', 'Deepali Jain', 'Ryan Julian', 'Dmitry Kalashnikov', 'M. Emre Karagozler', 'Stefani Karp', 'Chase Kew', 'Jerad Kirkland', 'Sean Kirmani', 'Yuheng Kuang', 'Thomas Lampe', 'Antoine Laurens', 'Isabel Leal', 'Alex X. Lee', 'Tsang-Wei Edward Lee', 'Jacky Liang', 'Yixin Lin', 'Sharath Maddineni', 'Anirudha Majumdar', 'Assaf Hurwitz Michaely', 'Robert Moreno', 'Michael Neunert', 'Francesco Nori', 'Carolina Parada', 'Emilio Parisotto', 'Peter Pastor', 'Acorn Pooley', 'Kanishka Rao', 'Krista Reymann', 'Dorsa Sadigh', 'Stefano Saliceti', 'Pannag Sanketi', 'Pierre Sermanet', 'Dhruv Shah', 'Mohit Sharma', 'Kathryn Shea', 'Charles Shu', 'Vikas Sindhwani', 'Sumeet Singh', 'Radu Soricut', 'Jost Tobias Springenberg', 'Rachel Sterneck', 'Razvan Surdulescu', 'Jie Tan', 'Jonathan Tompson', 'Vincent Vanhoucke', 'Jake Varley', 'Grace Vesom', 'Giulia Vezzani', 'Oriol Vinyals', 'Ayzaan Wahid', 'Stefan Welker', 'Paul Wohlhart', 'Fei Xia', 'Ted Xiao', 'Annie Xie', 'Jinyu Xie', 'Peng Xu', 'Sichun Xu', 'Ying Xu', 'Zhuo Xu', 'Yuxiang Yang', 'Rui Yao', 'Sergey Yaroshenko', 'Wenhao Yu', 'Wentao Yuan', 'Jingwei Zhang', 'Tingnan Zhang', 'Allan Zhou', 'Yuxiang Zhou'], 'affiliations': ['Gemini Robotics Team, Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.20020.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#agents', '#agi', '#ethics', '#games', '#reasoning', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Gemini Robotics: ИИ выходит в реальный мир', 'desc': 'Статья представляет семейство моделей Gemini Robotics, основанных на Gemini 2.0 и предназначенных для управления роботами. Gemini Robotics - это усовершенствованная мультимодальная модель типа Vision-Language-Action, способная напрямую контролировать роботов и выполнять сложные манипуляции. Модель Gemini Robotics-ER расширяет возможности рассуждений Gemini на физический мир, улучшая пространственное и временное понимание. Исследователи демонстрируют, как эти модели могут быть применены в различных робототехнических задачах, включая обнаружение объектов, прогнозирование траекторий и адаптацию к новым воплощениям роботов.'}, 'en': {'title': 'Empowering Robots with Gemini Robotics: A Leap in Multimodal AI', 'desc': "This paper presents Gemini Robotics, a new family of AI models designed specifically for robotic applications, building on the Gemini 2.0 framework. The model integrates Vision-Language-Action (VLA) capabilities, allowing robots to perform complex manipulation tasks with smooth and adaptive movements. It can learn from limited demonstrations and adapt to new tasks and robot designs, showcasing its versatility in various environments. Additionally, the paper introduces Gemini Robotics-ER, which enhances the model's reasoning abilities in physical contexts, addressing safety and practical applications in robotics."}, 'zh': {'title': 'Gemini Robotics：通用机器人的新纪元', 'desc': '最近大型多模态模型的进展使得数字领域的通用能力显著提升，但将其应用于机器人等物理代理仍然面临挑战。本文介绍了一种新型的人工智能模型，专为机器人设计，基于Gemini 2.0构建。Gemini Robotics是一个先进的视觉-语言-动作（VLA）通用模型，能够直接控制机器人，执行复杂的操作任务，并对物体类型和位置的变化具有鲁棒性。通过额外的微调，Gemini Robotics可以专门化为新的能力，包括解决长时间跨度的高灵巧任务，以及从少量示例中学习新任务。'}}}, {'id': 'https://huggingface.co/papers/2503.19786', 'title': 'Gemma 3 Technical Report', 'url': 'https://huggingface.co/papers/2503.19786', 'abstract': 'We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.', 'score': 12, 'issue_id': 2934, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'df574ff057c95baa', 'authors': ['Gemma Team', 'Aishwarya Kamath', 'Johan Ferret', 'Shreya Pathak', 'Nino Vieillard', 'Ramona Merhej', 'Sarah Perrin', 'Tatiana Matejovicova', 'Alexandre Ramé', 'Morgane Rivière', 'Louis Rouillard', 'Thomas Mesnard', 'Geoffrey Cideron', 'Jean-bastien Grill', 'Sabela Ramos', 'Edouard Yvinec', 'Michelle Casbon', 'Etienne Pot', 'Ivo Penchev', 'Gaël Liu', 'Francesco Visin', 'Kathleen Kenealy', 'Lucas Beyer', 'Xiaohai Zhai', 'Anton Tsitsulin', 'Robert Busa-Fekete', 'Alex Feng', 'Noveen Sachdeva', 'Benjamin Coleman', 'Yi Gao', 'Basil Mustafa', 'Iain Barr', 'Emilio Parisotto', 'David Tian', 'Matan Eyal', 'Colin Cherry', 'Jan-Thorsten Peter', 'Danila Sinopalnikov', 'Surya Bhupatiraju', 'Rishabh Agarwal', 'Mehran Kazemi', 'Dan Malkin', 'Ravin Kumar', 'David Vilar', 'Idan Brusilovsky', 'Jiaming Luo', 'Andreas Steiner', 'Abe Friesen', 'Abhanshu Sharma', 'Abheesht Sharma', 'Adi Mayrav Gilady', 'Adrian Goedeckemeyer', 'Alaa Saade', 'Alex Feng', 'Alexander Kolesnikov', 'Alexei Bendebury', 'Alvin Abdagic', 'Amit Vadi', 'András György', 'André Susano Pinto', 'Anil Das', 'Ankur Bapna', 'Antoine Miech', 'Antoine Yang', 'Antonia Paterson', 'Ashish Shenoy', 'Ayan Chakrabarti', 'Bilal Piot', 'Bo Wu', 'Bobak Shahriari', 'Bryce Petrini', 'Charlie Chen', 'Charline Le Lan', 'Christopher A. Choquette-Choo', 'CJ Carey', 'Cormac Brick', 'Daniel Deutsch', 'Danielle Eisenbud', 'Dee Cattle', 'Derek Cheng', 'Dimitris Paparas', 'Divyashree Shivakumar Sreepathihalli', 'Doug Reid', 'Dustin Tran', 'Dustin Zelle', 'Eric Noland', 'Erwin Huizenga', 'Eugene Kharitonov', 'Frederick Liu', 'Gagik Amirkhanyan', 'Glenn Cameron', 'Hadi Hashemi', 'Hanna Klimczak-Plucińska', 'Harman Singh', 'Harsh Mehta', 'Harshal Tushar Lehri', 'Hussein Hazimeh', 'Ian Ballantyne', 'Idan Szpektor', 'Ivan Nardini', 'Jean Pouget-Abadie', 'Jetha Chan', 'Joe Stanton', 'John Wieting', 'Jonathan Lai', 'Jordi Orbay', 'Joseph Fernandez', 'Josh Newlan', 'Ju-yeong Ji', 'Jyotinder Singh', 'Kat Black', 'Kathy Yu', 'Kevin Hui', 'Kiran Vodrahalli', 'Klaus Greff', 'Linhai Qiu', 'Marcella Valentine', 'Marina Coelho', 'Marvin Ritter', 'Matt Hoffman', 'Matthew Watson', 'Mayank Chaturvedi', 'Michael Moynihan', 'Min Ma', 'Nabila Babar', 'Natasha Noy', 'Nathan Byrd', 'Nick Roy', 'Nikola Momchev', 'Nilay Chauhan', 'Noveen Sachdeva', 'Oskar Bunyan', 'Pankil Botarda', 'Paul Caron', 'Paul Kishan Rubenstein', 'Phil Culliton', 'Philipp Schmid', 'Pier Giuseppe Sessa', 'Pingmei Xu', 'Piotr Stanczyk', 'Pouya Tafti', 'Rakesh Shivanna', 'Renjie Wu', 'Renke Pan', 'Reza Rokni', 'Rob Willoughby', 'Rohith Vallu', 'Ryan Mullins', 'Sammy Jerome', 'Sara Smoot', 'Sertan Girgin', 'Shariq Iqbal', 'Shashir Reddy', 'Shruti Sheth', 'Siim Põder', 'Sijal Bhatnagar', 'Sindhu Raghuram Panyam', 'Sivan Eiger', 'Susan Zhang', 'Tianqi Liu', 'Trevor Yacovone', 'Tyler Liechty', 'Uday Kalra', 'Utku Evci', 'Vedant Misra', 'Vincent Roseberry', 'Vlad Feinberg', 'Vlad Kolesnikov', 'Woohyun Han', 'Woosuk Kwon', 'Xi Chen', 'Yinlam Chow', 'Yuvein Zhu', 'Zichuan Wei', 'Zoltan Egyed', 'Victor Cotruta', 'Minh Giang', 'Phoebe Kirk', 'Anand Rao', 'Kat Black', 'Nabila Babar', 'Jessica Lo', 'Erica Moreira', 'Luiz Gustavo Martins', 'Omar Sanseviero', 'Lucas Gonzalez', 'Zach Gleicher', 'Tris Warkentin', 'Vahab Mirrokni', 'Evan Senter', 'Eli Collins', 'Joelle Barral', 'Zoubin Ghahramani', 'Raia Hadsell', 'Yossi Matias', 'D. Sculley', 'Slav Petrov', 'Noah Fiedel', 'Noam Shazeer', 'Oriol Vinyals', 'Jeff Dean', 'Demis Hassabis', 'Koray Kavukcuoglu', 'Clement Farabet', 'Elena Buchatskaya', 'Jean-Baptiste Alayrac', 'Rohan Anil', 'Dmitry', 'Lepikhin', 'Sebastian Borgeaud', 'Olivier Bachem', 'Armand Joulin', 'Alek Andreev', 'Cassidy Hardin', 'Robert Dadashi', 'Léonard Hussenot'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.19786.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#open_source', '#multilingual', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Gemma 3: Мультимодальный ИИ с улучшенной эффективностью и расширенными возможностями', 'desc': 'Представлена Gemma 3 - мультимодальная версия семейства легковесных открытых моделей Gemma с диапазоном от 1 до 27 миллиардов параметров. Модель получила возможности понимания изображений, расширенную поддержку языков и увеличенный контекст до 128К токенов. Архитектура была изменена для уменьшения KV-кэша памяти путем увеличения соотношения локальных и глобальных слоев внимания. Модели Gemma 3 обучены с использованием дистилляции и показывают превосходную производительность по сравнению с Gemma 2 как в предобученных, так и в инструктированных версиях.'}, 'en': {'title': 'Gemma 3: Multimodal Mastery with Extended Context!', 'desc': 'Gemma 3 is a new version of the Gemma model family that enhances multimodal capabilities, allowing it to understand both text and images. It features a larger scale, with models ranging from 1 to 27 billion parameters, and supports longer context lengths of at least 128K tokens. The architecture has been optimized to manage memory usage better during long contexts by adjusting the balance of local and global attention layers. With improved training techniques, Gemma 3 outperforms its predecessor, Gemma 2, in various tasks including math, chat, and multilingual processing, making it a strong competitor in the field.'}, 'zh': {'title': 'Gemma 3：多模态轻量级模型的突破', 'desc': 'Gemma 3 是 Gemma 系列轻量级开放模型的多模态版本，参数规模从 1 到 270 亿不等。该版本引入了视觉理解能力，支持更多语言，并且能够处理更长的上下文，至少达到 128K 个标记。我们通过调整模型架构，增加局部注意力层与全局注意力层的比例，来减少长上下文下 KV-cache 内存的爆炸。Gemma 3 模型经过蒸馏训练，表现优于 Gemma 2，特别是在数学、对话、指令跟随和多语言能力方面有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2503.20672', 'title': 'BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation', 'url': 'https://huggingface.co/papers/2503.20672', 'abstract': 'Recently, state-of-the-art text-to-image generation models, such as Flux and Ideogram 2.0, have made significant progress in sentence-level visual text rendering. In this paper, we focus on the more challenging scenarios of article-level visual text rendering and address a novel task of generating high-quality business content, including infographics and slides, based on user provided article-level descriptive prompts and ultra-dense layouts. The fundamental challenges are twofold: significantly longer context lengths and the scarcity of high-quality business content data.   In contrast to most previous works that focus on a limited number of sub-regions and sentence-level prompts, ensuring precise adherence to ultra-dense layouts with tens or even hundreds of sub-regions in business content is far more challenging. We make two key technical contributions: (i) the construction of scalable, high-quality business content dataset, i.e., Infographics-650K, equipped with ultra-dense layouts and prompts by implementing a layer-wise retrieval-augmented infographic generation scheme; and (ii) a layout-guided cross attention scheme, which injects tens of region-wise prompts into a set of cropped region latent space according to the ultra-dense layouts, and refine each sub-regions flexibly during inference using a layout conditional CFG.   We demonstrate the strong results of our system compared to previous SOTA systems such as Flux and SD3 on our BizEval prompt set. Additionally, we conduct thorough ablation experiments to verify the effectiveness of each component. We hope our constructed Infographics-650K and BizEval can encourage the broader community to advance the progress of business content generation.', 'score': 11, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'b04cbfb976ce4e45', 'authors': ['Yuyang Peng', 'Shishi Xiao', 'Keming Wu', 'Qisheng Liao', 'Bohan Chen', 'Kevin Lin', 'Danqing Huang', 'Ji Li', 'Yuhui Yuan'], 'affiliations': ['Brown University', 'Microsoft', 'Microsoft Research Asia', 'Tsinghua University', 'University of Liverpool'], 'pdf_title_img': 'assets/pdf/title_img/2503.20672.jpg', 'data': {'categories': ['#long_context', '#cv', '#synthetic', '#dataset', '#rag'], 'emoji': '📊', 'ru': {'title': 'Революция в генерации бизнес-контента: от текста к инфографике', 'desc': 'Статья представляет новый подход к генерации бизнес-контента, включая инфографику и слайды, на основе пользовательских запросов уровня статьи и сверхплотных макетов. Авторы создали масштабируемый набор данных Infographics-650K с высококачественным бизнес-контентом, используя послойную схему генерации инфографики с помощью извлечения информации. Они также разработали схему кросс-внимания с учетом макета, которая внедряет десятки регион-специфичных подсказок в латентное пространство обрезанных регионов согласно сверхплотным макетам. Результаты показывают превосходство предложенной системы над современными аналогами, такими как Flux и SD3, на наборе запросов BizEval.'}, 'en': {'title': 'Revolutionizing Business Content Generation with Ultra-Dense Layouts', 'desc': 'This paper presents advancements in text-to-image generation, specifically targeting the creation of business content like infographics and slides from article-level prompts. The authors introduce a new dataset, Infographics-650K, which includes ultra-dense layouts and is designed to address the challenges of longer context lengths and limited high-quality data. They propose a layout-guided cross attention mechanism that allows for precise generation across multiple sub-regions, enhancing the fidelity of the output. The results show significant improvements over existing models, encouraging further research in business content generation.'}, 'zh': {'title': '推动商业内容生成的新突破', 'desc': '本文关注于文章级视觉文本渲染的挑战，特别是在生成高质量商业内容方面，如信息图和幻灯片。我们提出了一种新的任务，旨在根据用户提供的描述性提示和超密集布局生成这些内容。我们构建了一个可扩展的高质量商业内容数据集Infographics-650K，并实现了一种基于布局的交叉注意力机制，以处理复杂的区域提示。我们的系统在与现有最先进系统的比较中表现出色，并通过消融实验验证了各个组件的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.20757', 'title': 'MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search', 'url': 'https://huggingface.co/papers/2503.20757', 'abstract': 'We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting a new standard for reasoning in small-scale models.', 'score': 7, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'd4c3b116518a2b0f', 'authors': ['Yunhai Hu', 'Yilun Zhao', 'Chen Zhao', 'Arman Cohan'], 'affiliations': ['New York University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20757.jpg', 'data': {'categories': ['#hallucinations', '#reasoning', '#rag', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'MCTS-RAG: Усиление рассуждений малых языковых моделей', 'desc': 'MCTS-RAG - это новый подход, объединяющий retrieval-augmented generation (RAG) и Monte Carlo Tree Search (MCTS) для улучшения рассуждений малых языковых моделей в задачах, требующих обширных знаний. Метод динамически интегрирует поиск информации и рассуждение через итеративный процесс принятия решений. MCTS-RAG сочетает структурированное рассуждение с адаптивным поиском, что улучшает принятие решений, снижает галлюцинации и повышает фактическую точность. Эксперименты показывают, что этот метод позволяет малым языковым моделям достичь производительности, сравнимой с крупными моделями типа GPT-4, эффективно масштабируя вычисления во время вывода.'}, 'en': {'title': 'Enhancing Small Models with Smart Retrieval and Reasoning', 'desc': 'MCTS-RAG is a new method that improves how small language models handle complex tasks that require knowledge. It combines retrieval-augmented generation (RAG) with Monte Carlo Tree Search (MCTS) to enhance reasoning by providing relevant information during the decision-making process. This approach allows the model to access external facts while reasoning, leading to better accuracy and consistency in responses. Experiments show that MCTS-RAG enables smaller models to perform as well as larger models like GPT-4o on challenging datasets.'}, 'zh': {'title': 'MCTS-RAG：小型模型推理的新标准', 'desc': '我们介绍了一种新方法MCTS-RAG，它通过结合检索增强生成（RAG）和蒙特卡洛树搜索（MCTS），提升小型语言模型在知识密集型任务上的推理能力。MCTS-RAG通过迭代决策过程动态整合检索和推理，克服了传统RAG方法和MCTS推理的局限性。与标准RAG方法不同，MCTS-RAG能够更好地结合结构化推理和自适应检索，从而提高决策质量，减少幻觉现象，并确保更高的事实准确性和响应一致性。实验结果表明，该方法使小型语言模型的性能可与前沿大型语言模型（如GPT-4o）相媲美，树立了小型模型推理的新标准。'}}}, {'id': 'https://huggingface.co/papers/2503.19950', 'title': 'LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation', 'url': 'https://huggingface.co/papers/2503.19950', 'abstract': "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions.   LogQuant takes a different approach. By applying a log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25% and boosts batch size by 60% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40% to 200% at the same compression ratio, outperforming comparable techniques.LogQuant integrates effortlessly with popular inference frameworks like Python's transformers library. Implementation can be available in https://github.com/Concyclics/LogQuantKV.", 'score': 7, 'issue_id': 2921, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'b75e0acc68cc5153', 'authors': ['Han Chen', 'Zicong Jiang', 'Zining Zhang', 'Bingsheng He', 'Pingyi Luo', 'Mian Lu', 'Yuqiang Chen'], 'affiliations': ['4Paradigm', 'School of Computing National University of Singapore', 'School of Electronic and Information Engineering South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.19950.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие памяти для ускорения работы языковых моделей', 'desc': 'LogQuant - это новая техника 2-битной квантизации для KV-кэша при инференсе больших языковых моделей. Она применяет логарифмический механизм фильтрации для выборочного сжатия KV-кэша по всему контексту. В тестах LogQuant повышает пропускную способность на 25% и увеличивает размер батча на 60% без роста потребления памяти. Для сложных задач, таких как математика и завершение кода, LogQuant улучшает точность на 40-200% при том же коэффициенте сжатия.'}, 'en': {'title': 'LogQuant: Efficient 2-Bit Quantization for Enhanced LLM Performance', 'desc': 'LogQuant is a novel 2-bit quantization method designed for efficiently managing the KV Cache in large language model inference. Unlike previous techniques that prioritize later tokens or rely on early attention patterns, LogQuant employs a log-based filtering mechanism to compress the KV Cache more effectively. This approach not only reduces memory usage but also enhances performance, achieving a 25% increase in throughput and a 60% increase in batch size without additional memory costs. In challenging tasks like Math and Code Completion, LogQuant significantly boosts accuracy by 40% to 200% while maintaining the same compression ratio, making it a superior choice for LLM applications.'}, 'zh': {'title': 'LogQuant：高效的KV缓存量化技术', 'desc': 'LogQuant是一种创新的2位量化技术，专为大型语言模型（LLM）推理中的KV缓存设计。它通过应用基于对数的过滤机制，选择性地压缩KV缓存，从而在保持优越性能的同时显著节省内存。与以往方法不同，LogQuant避免了性能瓶颈和频繁的错误预测，提升了25%的吞吐量和60%的批处理大小。对于数学和代码补全等复杂任务，LogQuant在相同压缩比下提高了40%到200%的准确性，超越了类似技术。'}}}, {'id': 'https://huggingface.co/papers/2503.20271', 'title': 'ViLBench: A Suite for Vision-Language Process Reward Modeling', 'url': 'https://huggingface.co/papers/2503.20271', 'abstract': "Process-supervised reward models serve as a fine-grained function that provides detailed step-wise feedback to model responses, facilitating effective selection of reasoning trajectories for complex tasks. Despite its advantages, evaluation on PRMs remains less explored, especially in the multimodal domain. To address this gap, this paper first benchmarks current vision large language models (VLLMs) as two types of reward models: output reward models (ORMs) and process reward models (PRMs) on multiple vision-language benchmarks, which reveal that neither ORM nor PRM consistently outperforms across all tasks, and superior VLLMs do not necessarily yield better rewarding performance. To further advance evaluation, we introduce ViLBench, a vision-language benchmark designed to require intensive process reward signals. Notably, OpenAI's GPT-4o with Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the benchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a promising pathway towards bridging the gap between general VLLMs and reward models -- by collecting 73.6K vision-language process reward data using an enhanced tree-search algorithm, our 3B model is able to achieve an average improvement of 3.3% over standard CoT and up to 2.5% compared to its untrained counterpart on ViLBench by selecting OpenAI o1's generations. We release the implementations at https://ucsc-vlaa.github.io/ViLBench with our code, model, and data.", 'score': 6, 'issue_id': 2925, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '53462508b8990597', 'authors': ['Haoqin Tu', 'Weitao Feng', 'Hardy Chen', 'Hui Liu', 'Xianfeng Tang', 'Cihang Xie'], 'affiliations': ['Amazon Research', 'UC Santa Cruz', 'UT Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2503.20271.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#optimization', '#multimodal', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Прогресс в оценке и улучшении мультимодальных моделей вознаграждения', 'desc': 'Статья исследует эффективность моделей вознаграждения, контролируемых процессом (PRM), в области мультимодального машинного обучения. Авторы проводят сравнительный анализ визуально-языковых моделей (VLLM) в качестве моделей вознаграждения на различных бенчмарках. Они представляют новый бенчмарк ViLBench, специально разработанный для оценки интенсивных сигналов вознаграждения процесса. Исследование также демонстрирует перспективный подход к улучшению производительности VLLM с помощью сбора данных о вознаграждениях процесса и применения улучшенного алгоритма поиска по дереву.'}, 'en': {'title': 'Enhancing Multimodal Learning with Process-Supervised Rewards', 'desc': 'This paper explores the use of process-supervised reward models (PRMs) to provide detailed feedback for complex reasoning tasks in the multimodal domain. It benchmarks vision large language models (VLLMs) as output reward models (ORMs) and PRMs across various vision-language tasks, finding that neither consistently outperforms the other. The authors introduce ViLBench, a challenging benchmark that emphasizes the need for process reward signals, revealing that even advanced models like GPT-4o struggle with its complexity. Additionally, they demonstrate a method to enhance reward model performance by collecting a large dataset of vision-language process rewards, leading to measurable improvements in model accuracy.'}, 'zh': {'title': '提升视觉-语言模型的奖励评估', 'desc': '本文探讨了过程监督奖励模型（PRMs）在复杂任务中的应用，提供了详细的逐步反馈以帮助选择推理路径。尽管PRMs具有优势，但在多模态领域的评估仍然较少。我们首次对当前的视觉大语言模型（VLLMs）进行基准测试，发现输出奖励模型（ORMs）和过程奖励模型（PRMs）在不同任务中的表现并不一致。为此，我们引入了ViLBench，一个需要强烈过程奖励信号的视觉-语言基准，展示了当前VLLMs在此基准上的挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.19462', 'title': 'AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset', 'url': 'https://huggingface.co/papers/2503.19462', 'abstract': 'Diffusion models have achieved remarkable progress in the field of video generation. However, their iterative denoising nature requires a large number of inference steps to generate a video, which is slow and computationally expensive. In this paper, we begin with a detailed analysis of the challenges present in existing diffusion distillation methods and propose a novel efficient method, namely AccVideo, to reduce the inference steps for accelerating video diffusion models with synthetic dataset. We leverage the pretrained video diffusion model to generate multiple valid denoising trajectories as our synthetic dataset, which eliminates the use of useless data points during distillation. Based on the synthetic dataset, we design a trajectory-based few-step guidance that utilizes key data points from the denoising trajectories to learn the noise-to-video mapping, enabling video generation in fewer steps. Furthermore, since the synthetic dataset captures the data distribution at each diffusion timestep, we introduce an adversarial training strategy to align the output distribution of the student model with that of our synthetic dataset, thereby enhancing the video quality. Extensive experiments demonstrate that our model achieves 8.5x improvements in generation speed compared to the teacher model while maintaining comparable performance. Compared to previous accelerating methods, our approach is capable of generating videos with higher quality and resolution, i.e., 5-seconds, 720x1280, 24fps.', 'score': 5, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '721d2bb59c963434', 'authors': ['Haiyu Zhang', 'Xinyuan Chen', 'Yaohui Wang', 'Xihui Liu', 'Yunhong Wang', 'Yu Qiao'], 'affiliations': ['Beihang University', 'Shanghai AI Laboratory', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19462.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#inference', '#video', '#dataset', '#synthetic'], 'emoji': '🎬', 'ru': {'title': 'Ускорение генерации видео с помощью синтетических данных и дистилляции диффузионных моделей', 'desc': 'Статья представляет новый метод AccVideo для ускорения генерации видео с помощью диффузионных моделей. Авторы используют предобученную модель для создания синтетического набора данных с множественными траекториями денойзинга. Предложенный подход включает траекторно-ориентированное руководство по малошаговой генерации и состязательное обучение для улучшения качества видео. Эксперименты показывают 8.5-кратное ускорение генерации по сравнению с исходной моделью при сохранении сопоставимого качества.'}, 'en': {'title': 'Accelerating Video Generation with AccVideo', 'desc': 'This paper addresses the slow and resource-intensive process of video generation using diffusion models, which require many steps for denoising. The authors introduce AccVideo, a new method that reduces the number of inference steps needed by utilizing a synthetic dataset generated from a pretrained video diffusion model. By focusing on key data points from denoising trajectories, the model learns to map noise to video more efficiently. Additionally, an adversarial training strategy is employed to improve the quality of the generated videos, resulting in significant speed improvements while maintaining high resolution and quality.'}, 'zh': {'title': '加速视频生成，提升质量与效率', 'desc': '扩散模型在视频生成领域取得了显著进展，但其迭代去噪的特性导致生成视频需要大量推理步骤，速度慢且计算成本高。本文分析了现有扩散蒸馏方法中的挑战，并提出了一种新颖的高效方法AccVideo，以减少推理步骤，加速视频扩散模型。我们利用预训练的视频扩散模型生成多个有效的去噪轨迹作为合成数据集，从而在蒸馏过程中消除无用数据点。通过设计基于轨迹的少步引导，我们能够在更少的步骤中实现视频生成，同时引入对抗训练策略以提高视频质量。'}}}, {'id': 'https://huggingface.co/papers/2503.19846', 'title': 'Attention IoU: Examining Biases in CelebA using Attention Maps', 'url': 'https://huggingface.co/papers/2503.19846', 'abstract': "Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We introduce the Attention-IoU (Attention Intersection over Union) metric and related scores, which use attention maps to reveal biases within a model's internal representations and identify image features potentially causing the biases. First, we validate Attention-IoU on the synthetic Waterbirds dataset, showing that the metric accurately measures model bias. We then analyze the CelebA dataset, finding that Attention-IoU uncovers correlations beyond accuracy disparities. Through an investigation of individual attributes through the protected attribute of Male, we examine the distinct ways biases are represented in CelebA. Lastly, by subsampling the training set to change attribute correlations, we demonstrate that Attention-IoU reveals potential confounding variables not present in dataset labels.", 'score': 4, 'issue_id': 2921, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'a4039050131ab9d6', 'authors': ['Aaron Serianni', 'Tyler Zhu', 'Olga Russakovsky', 'Vikram V. Ramaswamy'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19846.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#cv', '#dataset', '#interpretability'], 'emoji': '👁️', 'ru': {'title': 'Новый взгляд на предвзятость нейросетей через призму внимания', 'desc': 'Статья представляет новый метод оценки предвзятости в моделях компьютерного зрения - Attention-IoU. В отличие от существующих методов, Attention-IoU анализирует внутренние представления модели, используя карты внимания. Авторы валидируют метрику на синтетическом наборе данных Waterbirds и применяют ее к набору CelebA, обнаруживая корреляции, выходящие за рамки различий в точности. Исследование показывает, что Attention-IoU может выявлять потенциальные искажающие переменные, не присутствующие в метках набора данных.'}, 'en': {'title': 'Unveiling Biases with Attention-IoU in Computer Vision Models', 'desc': "This paper addresses the issue of bias in computer vision models, which can be amplified by the datasets they are trained on. The authors introduce a new metric called Attention-IoU, which utilizes attention maps to uncover biases in a model's internal representations rather than just focusing on dataset distribution or performance metrics. They validate this metric using the Waterbirds dataset and further analyze the CelebA dataset to reveal hidden correlations that go beyond mere accuracy differences. By manipulating the training set, they demonstrate that Attention-IoU can identify confounding variables that are not explicitly labeled in the dataset, providing deeper insights into model biases."}, 'zh': {'title': '揭示模型内部偏见的注意力交并比', 'desc': '本文探讨了计算机视觉模型在不同数据集和任务中表现出的偏见。现有的量化分类模型偏见的方法主要关注数据集分布和模型在子群体上的表现，而忽视了模型内部的工作机制。我们提出了注意力交并比（Attention-IoU）指标，通过注意力图揭示模型内部表示中的偏见，并识别可能导致偏见的图像特征。通过对Waterbirds和CelebA数据集的分析，我们验证了Attention-IoU的有效性，并发现其能够揭示超出准确性差异的相关性。'}}}, {'id': 'https://huggingface.co/papers/2503.20756', 'title': 'ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving\n  Systems', 'url': 'https://huggingface.co/papers/2503.20756', 'abstract': "Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.", 'score': 3, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '6b8affbfbdd5a426', 'authors': ['Chenxi Wang', 'Jizhan Fang', 'Xiang Chen', 'Bozhong Tian', 'Ziwen Xu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20756.jpg', 'data': {'categories': ['#multimodal', '#agents', '#dataset'], 'emoji': '🚗', 'ru': {'title': 'Редактирование знаний для улучшения автономного вождения', 'desc': 'Статья описывает применение больших мультимодальных моделей (LMM) в системах автономного вождения (ADS). Авторы предлагают использовать редактирование знаний для улучшения работы моделей без полного переобучения. Они также представляют набор данных ADS-Edit для оценки редактирования знаний в контексте автономного вождения. Проведенные эксперименты показывают перспективность этого подхода для развития автономных транспортных средств.'}, 'en': {'title': 'Enhancing Autonomous Driving with Targeted Knowledge Editing', 'desc': "This paper discusses the use of Large Multimodal Models (LMMs) in Autonomous Driving Systems (ADS) and the challenges they face, such as traffic knowledge misunderstanding and complex road conditions. To overcome these issues, the authors propose a method called Knowledge Editing, which allows for specific adjustments to a model's behavior without needing to retrain it entirely. They also introduce ADS-Edit, a specialized dataset for multimodal knowledge editing in ADS, which includes diverse real-world scenarios and various data types. The paper presents experimental results that highlight the potential of knowledge editing to enhance the performance of autonomous driving systems."}, 'zh': {'title': '知识编辑助力自动驾驶系统的进步', 'desc': '本文探讨了大型多模态模型（LMMs）在自动驾驶系统（ADS）中的应用潜力。尽管LMMs有前景，但在交通知识理解、复杂路况和车辆多样性等方面面临挑战。为了解决这些问题，我们提出了知识编辑的方法，可以在不完全重训练的情况下，针对性地修改模型的行为。同时，我们还引入了ADS-Edit，这是一个专为自动驾驶设计的多模态知识编辑数据集，包含多种真实场景和数据类型。'}}}, {'id': 'https://huggingface.co/papers/2503.20220', 'title': 'DINeMo: Learning Neural Mesh Models with no 3D Annotations', 'url': 'https://huggingface.co/papers/2503.20220', 'abstract': 'Category-level 3D/6D pose estimation is a crucial step towards comprehensive 3D scene understanding, which would enable a broad range of applications in robotics and embodied AI. Recent works explored neural mesh models that approach a range of 2D and 3D tasks from an analysis-by-synthesis perspective. Despite the largely enhanced robustness to partial occlusion and domain shifts, these methods depended heavily on 3D annotations for part-contrastive learning, which confines them to a narrow set of categories and hinders efficient scaling. In this work, we present DINeMo, a novel neural mesh model that is trained with no 3D annotations by leveraging pseudo-correspondence obtained from large visual foundation models. We adopt a bidirectional pseudo-correspondence generation method, which produce pseudo correspondence utilize both local appearance features and global context information. Experimental results on car datasets demonstrate that our DINeMo outperforms previous zero- and few-shot 3D pose estimation by a wide margin, narrowing the gap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively and efficiently when incorporating more unlabeled images during training, which demonstrate the advantages over supervised learning methods that rely on 3D annotations. Our project page is available at https://analysis-by-synthesis.github.io/DINeMo/.', 'score': 3, 'issue_id': 2919, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'f65c515bbb6af49b', 'authors': ['Weijie Guo', 'Guofeng Zhang', 'Wufei Ma', 'Alan Yuille'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20220.jpg', 'data': {'categories': ['#transfer_learning', '#3d', '#robotics', '#optimization'], 'emoji': '🚗', 'ru': {'title': 'Точная 3D-поза без 3D-разметки', 'desc': 'DINeMo - это новая нейронная сетевая модель для оценки 3D/6D позы объектов на уровне категорий без использования 3D-разметки. Модель использует псевдо-соответствия, полученные из крупных визуальных фундаментальных моделей, применяя двунаправленный метод генерации. DINeMo значительно превосходит предыдущие методы оценки 3D-позы с нулевым и малым количеством примеров, сокращая разрыв с полностью контролируемыми методами на 67.3%. Модель эффективно масштабируется при добавлении неразмеченных изображений в процессе обучения.'}, 'en': {'title': 'Revolutionizing 3D Pose Estimation with Unlabeled Data', 'desc': 'This paper introduces DINeMo, a new neural mesh model designed for category-level 3D/6D pose estimation without relying on 3D annotations. It utilizes pseudo-correspondence generated from large visual foundation models, allowing it to learn from unlabeled data effectively. The model employs a bidirectional approach to generate pseudo correspondences, combining local appearance features with global context. Experimental results show that DINeMo significantly improves performance in zero- and few-shot 3D pose estimation, achieving results close to fully-supervised methods while being more scalable and efficient.'}, 'zh': {'title': '无标注3D姿态估计的新突破', 'desc': '本论文提出了一种新的神经网格模型DINeMo，用于类别级的3D/6D姿态估计，旨在提高3D场景理解的能力。与以往依赖3D标注的学习方法不同，DINeMo通过利用大型视觉基础模型获得的伪对应关系进行训练，从而避免了对3D标注的依赖。我们采用双向伪对应生成方法，结合局部外观特征和全局上下文信息，显著提升了模型的鲁棒性。实验结果表明，DINeMo在车类数据集上的表现超越了之前的零样本和少样本3D姿态估计方法，缩小了与完全监督方法的差距。'}}}, {'id': 'https://huggingface.co/papers/2503.20198', 'title': 'Beyond Words: Advancing Long-Text Image Generation via Multimodal\n  Autoregressive Models', 'url': 'https://huggingface.co/papers/2503.20198', 'abstract': 'Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \\ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \\ModelName~significantly outperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E 3~dalle3 in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \\ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.', 'score': 3, 'issue_id': 2920, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'b2b966b253011624', 'authors': ['Alex Jinpeng Wang', 'Linjie Li', 'Zhengyuan Yang', 'Lijuan Wang', 'Min Li'], 'affiliations': ['Central South University', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.20198.jpg', 'data': {'categories': ['#long_context', '#cv', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Новая эра генерации изображений с длинными текстами', 'desc': 'Современные autoregressive и diffusion модели хорошо справляются с генерацией изображений с короткими текстами, но испытывают трудности с длинными текстами, такими как абзацы в документах. В этой работе представлена первая модель, специально разработанная для генерации изображений с длинными текстами, что закрывает важный пробел в существующих системах text-to-image. Основной проблемой является image tokenizer, который ограничивает качество генерации текста. Для решения этой проблемы был разработан новый бинарный tokenizer, оптимизированный для детального захвата текстовых особенностей, что позволило создать модель, превосходящую существующие решения в точности и гибкости генерации длинных текстов.'}, 'en': {'title': 'Revolutionizing Long-Text Image Generation with \\ModelName', 'desc': 'This paper introduces a new approach to generating long-form text in images, which has been a challenge for existing generative models. The authors identify that the image tokenizer is a key limitation in the quality of text generation. To overcome this, they propose a novel binary tokenizer that focuses on capturing detailed features of scene text. Their multimodal autoregressive model, named \\ModelName, demonstrates superior performance in generating high-quality long-text images, allowing for customizable text properties and paving the way for new applications in document and presentation generation.'}, 'zh': {'title': '长文本图像生成的新突破', 'desc': '最近，自回归和扩散模型的进展使得短文本图像生成表现出色。然而，生成连贯的长文本图像（如幻灯片或文档中的段落）仍然是当前生成模型面临的主要挑战。我们首次专注于长文本图像生成，填补了现有文本到图像系统的关键空白。通过分析最先进的自回归生成模型，我们发现图像分词器是影响文本生成质量的关键瓶颈，因此我们提出了一种新的文本专注的二进制分词器，以优化细节场景文本特征的捕捉。'}}}, {'id': 'https://huggingface.co/papers/2503.17358', 'title': 'Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred\n  Image', 'url': 'https://huggingface.co/papers/2503.17358', 'abstract': 'In many robotics and VR/AR applications, fast camera motions cause a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.', 'score': 3, 'issue_id': 2927, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '491a350f070233ec', 'authors': ['Jerred Chen', 'Ronald Clark'], 'affiliations': ['University of Oxford Department of Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.17358.jpg', 'data': {'categories': ['#dataset', '#robotics', '#training', '#cv', '#benchmark'], 'emoji': '📷', 'ru': {'title': 'Размытие в движении как ключ к точному позиционированию камеры', 'desc': 'Статья представляет новый подход к оценке положения камеры в условиях сильного размытия изображения при быстром движении. Метод предсказывает поле плотного потока движения и карту глубины по одному размытому изображению. Затем восстанавливается мгновенная скорость камеры путем решения задачи наименьших квадратов. Модель обучается на синтетических данных и дообучается на реальных изображениях с помощью полностью дифференцируемого конвейера.'}, 'en': {'title': 'Harnessing Motion Blur for Enhanced Camera Motion Estimation', 'desc': "This paper introduces a new method for estimating camera motion in situations where fast movements cause motion blur, which typically hinders existing techniques. Instead of viewing motion blur as a problem, the authors utilize it to predict a dense motion flow field and a depth map from a single blurred image. They calculate the camera's instantaneous velocity by solving a linear least squares problem, effectively treating the motion blur as valuable information. The proposed framework is trained on a large dataset and shows superior performance in estimating camera velocities compared to traditional methods."}, 'zh': {'title': '利用运动模糊提升相机运动估计的创新方法', 'desc': '在许多机器人和虚拟现实/增强现实应用中，快速的相机运动会导致严重的运动模糊，现有的相机姿态估计方法因此失效。我们提出了一种新颖的框架，将运动模糊视为运动估计的丰富线索，而不是不必要的伪影。该方法通过从单一的运动模糊图像中直接预测密集的运动流场和单目深度图来工作。我们的模型经过大规模合成运动模糊数据集训练，并在真实数据上进行端到端的微调，最终在真实世界基准测试中表现出色，超越了现有的MASt3R和COLMAP等方法。'}}}, {'id': 'https://huggingface.co/papers/2503.20641', 'title': 'Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging', 'url': 'https://huggingface.co/papers/2503.20641', 'abstract': "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.", 'score': 2, 'issue_id': 2932, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': '52b4dbdb179d7229', 'authors': ['Han Wu', 'Yuxuan Yao', 'Shuqi Liu', 'Zehua Liu', 'Xiaojin Fu', 'Xiongwei Han', 'Xing Li', 'Hui-Ling Zhen', 'Tao Zhong', 'Mingxuan Yuan'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.20641.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение в LLM: от длинного к короткому через объединение моделей', 'desc': 'Статья посвящена проблеме избыточного мышления в больших языковых моделях (LLM) при переходе от быстрого интуитивного мышления (System 1) к медленному аналитическому (System 2). Авторы предлагают метод объединения моделей (model merging) для достижения баланса между глубиной рассуждений и эффективностью. Эксперименты показывают, что этот подход может сократить среднюю длину ответа на 55% при сохранении или улучшении производительности. Исследование также выявляет сильную корреляцию между масштабом модели и эффективностью объединения.'}, 'en': {'title': 'Efficient Reasoning through Model Merging', 'desc': 'This paper discusses the transition from quick, intuitive reasoning (System 1) to more deliberate, analytical reasoning (System 2) in large language models (LLMs). It highlights the inefficiencies that arise when models overthink, leading to unnecessary complexity without significant gains in output quality. The authors propose a method called Long-to-Short (L2S) reasoning, which aims to optimize the balance between deep reasoning and efficiency. They introduce model merging as a solution, which combines the strengths of both reasoning systems, demonstrating that this approach can significantly reduce response length while maintaining or enhancing performance.'}, 'zh': {'title': '模型合并：高效的长到短推理解决方案', 'desc': '本文探讨了大型语言模型（LLMs）在复杂任务中从系统1推理到系统2推理的转变。尽管这种进步提高了推理的深度，但往往导致效率下降，模型可能会产生冗余的推理步骤。为了解决这个问题，长到短（L2S）推理提出了一种平衡推理深度与效率的方案。通过模型合并，我们能够将系统1模型的快速思维与系统2模型的系统性推理结合，从而在保持性能的同时显著减少响应长度。'}}}, {'id': 'https://huggingface.co/papers/2503.19953', 'title': 'Self-Supervised Learning of Motion Concepts by Optimizing\n  Counterfactuals', 'url': 'https://huggingface.co/papers/2503.19953', 'abstract': "Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these models' capabilities in real-world contexts. Despite recent developments in large-scale self-supervised learning from videos, leveraging such representations for motion estimation remains relatively underexplored. In this work, we develop Opt-CWM, a self-supervised technique for flow and occlusion estimation from a pre-trained next-frame prediction model. Opt-CWM works by learning to optimize counterfactual probes that extract motion information from a base video model, avoiding the need for fixed heuristics while training on unrestricted video inputs. We achieve state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data.", 'score': 2, 'issue_id': 2919, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '01093e8b98f32607', 'authors': ['Stefan Stojanov', 'David Wendt', 'Seungwoo Kim', 'Rahul Venkatesh', 'Kevin Feigelis', 'Jiajun Wu', 'Daniel LK Yamins'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19953.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': '🎥', 'ru': {'title': 'Самообучаемая оценка движения в видео без размеченных данных', 'desc': 'Opt-CWM - это самообучаемый метод для оценки потока и окклюзии на основе предобученной модели предсказания следующего кадра. Он работает путем оптимизации контрфактических проб, извлекающих информацию о движении из базовой видеомодели. Этот подход позволяет избежать использования фиксированных эвристик и обучаться на неограниченных видеовходах. Opt-CWM достигает наилучших результатов в оценке движения на реальных видео без использования размеченных данных.'}, 'en': {'title': 'Revolutionizing Motion Estimation with Self-Supervised Learning', 'desc': 'This paper presents Opt-CWM, a self-supervised method for estimating motion in videos, which is crucial for applications like video generation and robotics. Unlike traditional approaches that rely on synthetic data or specific heuristics, Opt-CWM utilizes a pre-trained next-frame prediction model to learn motion information directly from real-world video inputs. The technique optimizes counterfactual probes, allowing it to adaptively extract flow and occlusion data without needing fixed rules. As a result, Opt-CWM achieves state-of-the-art performance in motion estimation while eliminating the requirement for labeled datasets.'}, 'zh': {'title': '自监督运动估计的新突破', 'desc': '本论文探讨了视频中的运动估计问题，这是计算机视觉中的一个重要课题，广泛应用于可控视频生成和机器人技术。现有的方法主要依赖合成数据训练或特定情境的启发式调整，这限制了模型在真实场景中的表现。我们提出了一种名为Opt-CWM的自监督技术，通过预训练的下一帧预测模型进行流动和遮挡估计。Opt-CWM通过优化反事实探针来提取运动信息，避免了固定启发式的需求，并在无需标注数据的情况下，在真实视频上实现了最先进的运动估计性能。'}}}, {'id': 'https://huggingface.co/papers/2503.16870', 'title': 'Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs', 'url': 'https://huggingface.co/papers/2503.16870', 'abstract': "Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.", 'score': 2, 'issue_id': 2921, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'e8b397bd8ee5118a', 'authors': ['Anshumann', 'Mohd Abbas Zaidi', 'Akhil Kedia', 'Jinwoo Ahn', 'Taehwak Kwon', 'Kangwook Lee', 'Haejun Lee', 'Joohyung Lee'], 'affiliations': ['Samsung Research, Seoul'], 'pdf_title_img': 'assets/pdf/title_img/2503.16870.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективная дистилляция знаний для больших языковых моделей', 'desc': "Статья представляет новый метод дистилляции знаний для обучения больших языковых моделей. Авторы показывают, что наивные подходы к разреженной дистилляции знаний, такие как кэширование Top-K вероятностей, дают смещенные оценки распределения вероятностей учителя. Предлагается метод 'Random Sampling Knowledge Distillation', основанный на выборке по важности, который обеспечивает несмещенные оценки и сохраняет градиент в ожидании. Этот метод позволяет быстрее обучать модели-ученики с минимальными накладными расходами по сравнению с обучением на основе кросс-энтропии, сохраняя при этом конкурентоспособную производительность."}, 'en': {'title': 'Unbiased Knowledge Distillation for Efficient Model Training', 'desc': "This paper discusses a new method for knowledge distillation in Large Language Models, focusing on the challenges of pre-training. The authors highlight that traditional methods, like caching Top-K probabilities, can lead to biased teacher probability distributions, which negatively affect the student's performance. They introduce 'Random Sampling Knowledge Distillation', an importance-sampling approach that provides unbiased estimates and maintains gradient preservation. This method allows for faster training with minimal overhead while achieving competitive results compared to full distillation across various model sizes."}, 'zh': {'title': '高效的知识蒸馏方法提升学生模型训练', 'desc': '知识蒸馏是一种有效的技术，可以从大型语言模型中提取知识。本文探讨了在预训练阶段应用知识蒸馏的挑战，特别是简单的稀疏知识蒸馏方法可能导致学生模型获得偏差的教师概率分布。我们提出了一种基于重要性采样的随机采样知识蒸馏方法，能够提供无偏估计，并在期望中保留梯度。该方法在存储稀疏logits的同时，能加快学生模型的训练速度，并在不同模型规模下保持竞争力的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.18929', 'title': 'Trajectory Balance with Asynchrony: Decoupling Exploration and Learning\n  for Fast, Scalable LLM Post-Training', 'url': 'https://huggingface.co/papers/2503.18929', 'abstract': 'Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, existing on-policy algorithms used for post-training are inherently incompatible with the use of experience replay buffers, which can be populated scalably by distributed off-policy actors to enhance exploration as compute increases. We propose efficiently obtaining this benefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a massively scalable LLM RL system. In contrast to existing approaches, TBA uses a larger fraction of compute on search, constantly generating off-policy data for a central replay buffer. A training node simultaneously samples data from this buffer based on reward or recency to update the policy using Trajectory Balance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA offers three key advantages: (1) decoupled training and search, speeding up training wall-clock time by 4x or more; (2) improved diversity through large-scale off-policy sampling; and (3) scalable search for sparse reward settings. On mathematical reasoning, preference-tuning, and automated red-teaming (diverse and representative post-training tasks), TBA produces speed and performance improvements over strong baselines.', 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'ae8d68c1a89d88a1', 'authors': ['Brian R. Bartoldson', 'Siddarth Venkatraman', 'James Diffenderfer', 'Moksh Jain', 'Tal Ben-Nun', 'Seanie Lee', 'Minsu Kim', 'Johan Obando-Ceron', 'Yoshua Bengio', 'Bhavya Kailkhura'], 'affiliations': ['CIFAR Fellow', 'KAIST', 'Lawrence Livermore National Laboratory', 'Mila Quebec AI Institute', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.18929.jpg', 'data': {'categories': ['#optimization', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'TBA: Масштабируемое обучение с подкреплением для крупных языковых моделей', 'desc': 'Эта статья представляет новый метод обучения с подкреплением для крупных языковых моделей под названием Trajectory Balance with Asynchrony (TBA). TBA использует буфер воспроизведения опыта для улучшения исследования и разнообразия данных. Метод предлагает асинхронный подход, разделяющий процессы поиска и обучения, что значительно ускоряет общее время обучения. TBA демонстрирует улучшения производительности в задачах математических рассуждений, настройки предпочтений и автоматизированного тестирования безопасности по сравнению с существующими методами.'}, 'en': {'title': 'Boosting LLM Training with Efficient Replay and Exploration', 'desc': 'This paper introduces a new method called Trajectory Balance with Asynchrony (TBA) for improving reinforcement learning in large language models (LLMs). TBA allows the use of experience replay buffers, which help in better exploration by storing past experiences and using them for training. The method separates the training and search processes, leading to faster training times and enhanced diversity in the data sampled. Overall, TBA shows significant improvements in performance and efficiency on various post-training tasks compared to existing methods.'}, 'zh': {'title': '提升强化学习效率的轨迹平衡与异步方法', 'desc': '强化学习（RL）是大型语言模型（LLM）后训练的重要组成部分。现有的在线算法与经验重放缓冲区不兼容，而我们提出的轨迹平衡与异步（TBA）方法可以有效利用重放缓冲区的优势。TBA通过将计算资源更多地用于搜索，持续生成离线数据来更新策略，从而加速训练过程。该方法在数学推理、偏好调优和自动红队等任务上表现出显著的速度和性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.15893', 'title': 'UniHDSA: A Unified Relation Prediction Approach for Hierarchical\n  Document Structure Analysis', 'url': 'https://huggingface.co/papers/2503.15893', 'abstract': "Document structure analysis, aka document layout analysis, is crucial for understanding both the physical layout and logical structure of documents, serving information retrieval, document summarization, knowledge extraction, etc. Hierarchical Document Structure Analysis (HDSA) specifically aims to restore the hierarchical structure of documents created using authoring software with hierarchical schemas. Previous research has primarily followed two approaches: one focuses on tackling specific subtasks of HDSA in isolation, such as table detection or reading order prediction, while the other adopts a unified framework that uses multiple branches or modules, each designed to address a distinct task. In this work, we propose a unified relation prediction approach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as relation prediction problems and consolidates relation prediction labels into a unified label space. This allows a single relation prediction module to handle multiple tasks simultaneously, whether at a page-level or document-level structure analysis. To validate the effectiveness of UniHDSA, we develop a multimodal end-to-end system based on Transformer architectures. Extensive experimental results demonstrate that our approach achieves state-of-the-art performance on a hierarchical document structure analysis benchmark, Comp-HRDoc, and competitive results on a large-scale document layout analysis dataset, DocLayNet, effectively illustrating the superiority of our method across all sub-tasks. The Comp-HRDoc benchmark and UniHDSA's configurations are publicly available at https://github.com/microsoft/CompHRDoc.", 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '459fe3a2bedb5ed6', 'authors': ['Jiawei Wang', 'Kai Hu', 'Qiang Huo'], 'affiliations': ['Department of EEIS, University of Science and Technology of China, Hefei, 230026, China', 'Microsoft Research Asia, Beijing, 100080, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.15893.jpg', 'data': {'categories': ['#dataset', '#architecture', '#multimodal', '#benchmark'], 'emoji': '📄', 'ru': {'title': 'Унифицированный подход к анализу иерархической структуры документов', 'desc': 'Статья представляет новый подход к иерархическому анализу структуры документов (HDSA) под названием UniHDSA. Этот метод рассматривает различные подзадачи HDSA как проблемы предсказания отношений и объединяет метки предсказания отношений в единое пространство меток. UniHDSA использует единый модуль предсказания отношений для одновременной обработки нескольких задач на уровне страницы и документа. Авторы разработали мультимодальную систему на основе архитектуры Transformer для валидации эффективности UniHDSA.'}, 'en': {'title': 'Unified Relation Prediction for Enhanced Document Structure Analysis', 'desc': 'This paper introduces a new method called UniHDSA for Hierarchical Document Structure Analysis (HDSA), which focuses on understanding the layout and structure of documents. Unlike previous methods that tackled individual tasks separately, UniHDSA treats various HDSA tasks as relation prediction problems, allowing for a more integrated approach. The method uses a single relation prediction module to analyze both page-level and document-level structures simultaneously. Experimental results show that UniHDSA outperforms existing methods on benchmark datasets, demonstrating its effectiveness in document layout analysis.'}, 'zh': {'title': '统一关系预测，提升文档结构分析', 'desc': '文档结构分析对于理解文档的物理布局和逻辑结构至关重要，涉及信息检索、文档摘要和知识提取等应用。本文提出了一种统一的关系预测方法UniHDSA，旨在将文档结构分析的各个子任务视为关系预测问题，并将关系预测标签整合到一个统一的标签空间中。通过这种方法，单一的关系预测模块能够同时处理多个任务，无论是在页面级别还是文档级别的结构分析。实验结果表明，UniHDSA在层次文档结构分析基准Comp-HRDoc上达到了最先进的性能，并在大规模文档布局分析数据集DocLayNet上也取得了竞争力的结果，展示了该方法在各个子任务上的优越性。'}}}, {'id': 'https://huggingface.co/papers/2503.10997', 'title': 'RONA: Pragmatically Diverse Image Captioning with Coherence Relations', 'url': 'https://huggingface.co/papers/2503.10997', 'abstract': 'Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally generate diverse image captions by employing syntactic and semantic variations to describe image components. However, human-written captions prioritize conveying a central message alongside visual descriptions using pragmatic cues. To enhance pragmatic diversity, it is essential to explore alternative ways of communicating these messages in conjunction with visual content. To address this challenge, we propose RONA, a novel prompting strategy for Multi-modal Large Language Models (MLLM) that leverages Coherence Relations as an axis for variation. We demonstrate that RONA generates captions with better overall diversity and ground-truth alignment, compared to MLLM baselines across multiple domains. Our code is available at: https://github.com/aashish2000/RONA', 'score': 1, 'issue_id': 2932, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'd3b2354bbfc88139', 'authors': ['Aashish Anantha Ramakrishnan', 'Aadarsh Anantha Ramakrishnan', 'Dongwon Lee'], 'affiliations': ['National Institute of Technology, Tiruchirappalli', 'The Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10997.jpg', 'data': {'categories': ['#open_source', '#story_generation', '#multimodal', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'RONA: прагматическое разнообразие в генерации подписей к изображениям', 'desc': 'Статья представляет новый метод RONA для генерации разнообразных подписей к изображениям с помощью мультимодальных языковых моделей (MLLM). В отличие от традиционных подходов, RONA использует отношения согласованности для создания прагматически разнообразных подписей. Эксперименты показывают, что RONA превосходит базовые MLLM по общему разнообразию и соответствию эталонным подписям в различных доменах. Авторы предоставляют открытый исходный код своего метода.'}, 'en': {'title': 'RONA: Enhancing Caption Diversity with Pragmatic Cues', 'desc': 'This paper introduces RONA, a new prompting strategy designed for Multi-modal Large Language Models (MLLM) to improve the diversity of image captions. Unlike traditional methods that focus on syntactic and semantic variations, RONA emphasizes pragmatic cues to convey a central message alongside visual descriptions. By utilizing Coherence Relations, RONA enhances the way messages are communicated in relation to images. The results show that RONA outperforms existing MLLM baselines in generating captions that are more diverse and closely aligned with ground-truth descriptions across various domains.'}, 'zh': {'title': 'RONA：提升图像标题多样性的创新策略', 'desc': '本文提出了一种新的提示策略RONA，用于多模态大语言模型（MLLM），旨在提高图像标题的多样性。传统的写作助手生成的标题往往侧重于语法和语义的变化，而人类撰写的标题则更注重传达中心信息。RONA通过利用一致性关系作为变化的轴心，探索了与视觉内容结合的替代沟通方式。实验结果表明，RONA生成的标题在多样性和真实对齐度上优于现有的MLLM基线。'}}}, {'id': 'https://huggingface.co/papers/2503.20731', 'title': 'RecTable: Fast Modeling Tabular Data with Rectified Flow', 'url': 'https://huggingface.co/papers/2503.20731', 'abstract': 'Score-based or diffusion models generate high-quality tabular data, surpassing GAN-based and VAE-based models. However, these methods require substantial training time. In this paper, we introduce RecTable, which uses the rectified flow modeling, applied in such as text-to-image generation and text-to-video generation. RecTable features a simple architecture consisting of a few stacked gated linear unit blocks. Additionally, our training strategies are also simple, incorporating a mixed-type noise distribution and a logit-normal timestep distribution. Our experiments demonstrate that RecTable achieves competitive performance compared to the several state-of-the-art diffusion and score-based models while reducing the required training time. Our code is available at https://github.com/fmp453/rectable.', 'score': 0, 'issue_id': 2935, 'pub_date': '2025-03-26', 'pub_date_card': {'ru': '26 марта', 'en': 'March 26', 'zh': '3月26日'}, 'hash': 'dfe7dcfbd0067c32', 'authors': ['Masane Fuchi', 'Tomohiro Takagi'], 'affiliations': ['Meiji University'], 'pdf_title_img': 'assets/pdf/title_img/2503.20731.jpg', 'data': {'categories': ['#open_source', '#architecture', '#dataset', '#optimization', '#diffusion', '#training'], 'emoji': '📊', 'ru': {'title': 'RecTable: быстрая генерация качественных табличных данных', 'desc': 'Авторы представляют RecTable - новый метод генерации табличных данных, основанный на технологии выпрямленного потока (rectified flow). RecTable использует простую архитектуру из нескольких блоков с управляемыми линейными юнитами. Метод применяет смешанное распределение шума и логит-нормальное распределение временных шагов. Эксперименты показывают, что RecTable достигает результатов, сопоставимых с современными диффузионными моделями, при меньшем времени обучения.'}, 'en': {'title': 'RecTable: Fast and Efficient Tabular Data Generation', 'desc': 'This paper presents RecTable, a novel approach for generating high-quality tabular data using rectified flow modeling. Unlike traditional methods like GANs and VAEs, RecTable significantly reduces training time while maintaining competitive performance. The architecture is straightforward, utilizing stacked gated linear unit blocks, which simplifies the model design. Additionally, the training strategies involve a mixed-type noise distribution and a logit-normal timestep distribution, enhancing efficiency and effectiveness in data generation.'}, 'zh': {'title': 'RecTable：高效生成表格数据的新方法', 'desc': '本论文介绍了一种新的模型RecTable，用于生成高质量的表格数据，超越了基于GAN和VAE的模型。RecTable采用了修正流建模，具有简单的架构，由几个堆叠的门控线性单元块组成。我们的方法在训练策略上也很简单，结合了混合类型的噪声分布和对数正态时间步分布。实验结果表明，RecTable在性能上与多种最先进的扩散和评分模型相当，同时减少了训练时间。'}}}, {'id': 'https://huggingface.co/papers/2503.17970', 'title': 'PathoHR: Breast Cancer Survival Prediction on High-Resolution\n  Pathological Images', 'url': 'https://huggingface.co/papers/2503.17970', 'abstract': "Breast cancer survival prediction in computational pathology presents a remarkable challenge due to tumor heterogeneity. For instance, different regions of the same tumor in the pathology image can show distinct morphological and molecular characteristics. This makes it difficult to extract representative features from whole slide images (WSIs) that truly reflect the tumor's aggressive potential and likely survival outcomes. In this paper, we present PathoHR, a novel pipeline for accurate breast cancer survival prediction that enhances any size of pathological images to enable more effective feature learning. Our approach entails (1) the incorporation of a plug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise WSI representation, enabling more detailed and comprehensive feature extraction, (2) the systematic evaluation of multiple advanced similarity metrics for comparing WSI-extracted features, optimizing the representation learning process to better capture tumor characteristics, (3) the demonstration that smaller image patches enhanced follow the proposed pipeline can achieve equivalent or superior prediction accuracy compared to raw larger patches, while significantly reducing computational overhead. Experimental findings valid that PathoHR provides the potential way of integrating enhanced image resolution with optimized feature learning to advance computational pathology, offering a promising direction for more accurate and efficient breast cancer survival prediction. Code will be available at https://github.com/AIGeeksGroup/PathoHR.", 'score': 0, 'issue_id': 2928, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '7982f77e78076ec7', 'authors': ['Yang Luo', 'Shiru Wang', 'Jun Liu', 'Jiaxuan Xiao', 'Rundong Xue', 'Zeyu Zhang', 'Hao Zhang', 'Yu Lu', 'Yang Zhao', 'Yutong Xie'], 'affiliations': ['ANU', 'DLMU', 'Dartmouth', 'La Trobe', 'MBZUAI', 'NUP', 'SZTU', 'UCAS', 'XJLTU', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2503.17970.jpg', 'data': {'categories': ['#cv', '#healthcare', '#training'], 'emoji': '🔬', 'ru': {'title': 'Повышение точности прогноза рака груди с помощью улучшения изображений и глубокого обучения', 'desc': 'PathoHR - это новый метод для точного прогнозирования выживаемости при раке молочной железы, который улучшает качество патологических изображений любого размера для более эффективного извлечения признаков. Он использует высокоразрешающий Vision Transformer для улучшения представления патч-изображений, оценивает различные метрики сходства для сравнения извлеченных признаков и демонстрирует, что улучшенные маленькие патчи могут достичь точности прогнозирования сравнимой или превосходящей большие необработанные патчи. PathoHR предлагает перспективный подход к интеграции улучшенного разрешения изображений с оптимизированным извлечением признаков для развития вычислительной патологии.'}, 'en': {'title': 'Enhancing Breast Cancer Survival Prediction with PathoHR', 'desc': 'This paper addresses the challenge of predicting breast cancer survival by focusing on the variability within tumors, which can complicate feature extraction from whole slide images (WSIs). The authors introduce PathoHR, a new pipeline that utilizes a high-resolution Vision Transformer (ViT) to improve the representation of tumor features at a patch level. They evaluate various similarity metrics to optimize the learning process, allowing for better capture of tumor characteristics. The results show that smaller, enhanced image patches can achieve similar or better prediction accuracy compared to larger patches, while also being more computationally efficient.'}, 'zh': {'title': '提升乳腺癌生存预测的PathoHR管道', 'desc': '本论文提出了一种名为PathoHR的新型管道，用于提高乳腺癌生存预测的准确性。由于肿瘤的异质性，病理图像中的不同区域可能表现出不同的形态和分子特征，这使得从全切片图像中提取代表性特征变得困难。PathoHR通过引入高分辨率的视觉变换器（ViT）来增强图像补丁的表示，从而实现更有效的特征学习。此外，实验结果表明，经过该管道增强的小图像补丁在预测准确性上可以与原始的大图像补丁相媲美，同时显著降低计算开销。'}}}, {'id': 'https://huggingface.co/papers/2503.10622', 'title': 'Transformers without Normalization', 'url': 'https://huggingface.co/papers/2503.10622', 'abstract': 'Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x) = tanh(alpha x), as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.', 'score': 84, 'issue_id': 2702, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'd790a15c7d75d15e', 'authors': ['Jiachen Zhu', 'Xinlei Chen', 'Kaiming He', 'Yann LeCun', 'Zhuang Liu'], 'affiliations': ['FAIR, Meta', 'MIT', 'New York University', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10622.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#multimodal', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Прощай, нормализация: Dynamic Tanh меняет правила игры в архитектуре трансформеров', 'desc': 'Исследователи представили альтернативу нормализационным слоям в трансформерах - Dynamic Tanh (DyT). DyT - это простая поэлементная операция, которая может заменить нормализацию без потери производительности. Эксперименты показали, что трансформеры с DyT работают не хуже или лучше стандартных моделей в различных задачах компьютерного зрения и обработки естественного языка. Это открытие ставит под сомнение необходимость нормализационных слоев в современных нейронных сетях.'}, 'en': {'title': 'Transformers Thrive Without Normalization: Introducing Dynamic Tanh', 'desc': 'This paper explores the role of normalization layers in Transformers, showing that they may not be as essential as previously thought. The authors introduce Dynamic Tanh (DyT), a simple element-wise operation that can replace normalization layers while maintaining or improving performance. DyT is based on the observation that layer normalization often results in S-shaped mappings similar to the tanh function. The study demonstrates that Transformers using DyT can perform well across various tasks, suggesting a reevaluation of the necessity of normalization in deep learning architectures.'}, 'zh': {'title': '动态双曲正切：超越归一化的变换器', 'desc': '本研究展示了在现代神经网络中，归一化层并非必不可少。我们提出了一种名为动态双曲正切（Dynamic Tanh, DyT）的简单技术，可以替代变换器中的归一化层。DyT通过观察变换器中的层归一化通常产生类似tanh的S形输入输出映射而得出。通过引入DyT，未使用归一化的变换器可以在多种任务中达到或超过使用归一化的变换器的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.10613', 'title': 'CoSTAast: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing', 'url': 'https://huggingface.co/papers/2503.10613', 'abstract': 'Text-to-image models like stable diffusion and DALLE-3 still struggle with multi-turn image editing. We decompose such a task as an agentic workflow (path) of tool use that addresses a sequence of subtasks by AI tools of varying costs. Conventional search algorithms require expensive exploration to find tool paths. While large language models (LLMs) possess prior knowledge of subtask planning, they may lack accurate estimations of capabilities and costs of tools to determine which to apply in each subtask. Can we combine the strengths of both LLMs and graph search to find cost-efficient tool paths? We propose a three-stage approach "CoSTA*" that leverages LLMs to create a subtask tree, which helps prune a graph of AI tools for the given task, and then conducts A* search on the small subgraph to find a tool path. To better balance the total cost and quality, CoSTA* combines both metrics of each tool on every subtask to guide the A* search. Each subtask\'s output is then evaluated by a vision-language model (VLM), where a failure will trigger an update of the tool\'s cost and quality on the subtask. Hence, the A* search can recover from failures quickly to explore other paths. Moreover, CoSTA* can automatically switch between modalities across subtasks for a better cost-quality trade-off. We build a novel benchmark of challenging multi-turn image editing, on which CoSTA* outperforms state-of-the-art image-editing models or agents in terms of both cost and quality, and performs versatile trade-offs upon user preference.', 'score': 60, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '7f2d9ee971af97a8', 'authors': ['Advait Gupta', 'NandaKiran Velaga', 'Dang Nguyen', 'Tianyi Zhou'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.10613.jpg', 'data': {'categories': ['#games', '#benchmark', '#multimodal', '#agents', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'CoSTA*: Умное редактирование изображений с помощью ИИ и поиска', 'desc': 'Статья представляет новый подход CoSTA* для многоэтапного редактирования изображений. Метод объединяет большие языковые модели (LLM) для создания дерева подзадач и алгоритм поиска A* для нахождения оптимального пути использования инструментов ИИ. CoSTA* использует мультимодальную модель для оценки результатов каждого этапа и может адаптироваться к неудачам. Эксперименты показывают превосходство CoSTA* над существующими моделями и агентами в соотношении стоимости и качества редактирования изображений.'}, 'en': {'title': 'Optimizing Multi-Turn Image Editing with CoSTA*', 'desc': 'This paper addresses the challenges faced by text-to-image models in multi-turn image editing by introducing a new approach called CoSTA*. It combines large language models (LLMs) with graph search techniques to efficiently plan and execute a sequence of subtasks using AI tools. CoSTA* creates a subtask tree to streamline the selection of tools based on their costs and capabilities, and employs A* search to find optimal tool paths. The method also adapts to failures by updating tool metrics, allowing for quick recovery and improved cost-quality trade-offs in image editing tasks.'}, 'zh': {'title': '高效的多轮图像编辑工具路径优化', 'desc': '本文提出了一种名为CoSTA*的三阶段方法，用于解决文本到图像模型在多轮图像编辑中的挑战。该方法结合了大型语言模型（LLMs）和图搜索的优点，通过创建子任务树来优化AI工具的使用路径。CoSTA*在每个子任务中综合考虑工具的成本和质量，以指导A*搜索，从而找到高效的工具路径。实验结果表明，CoSTA*在多轮图像编辑任务中超越了现有的最先进模型，能够根据用户偏好进行灵活的成本与质量权衡。'}}}, {'id': 'https://huggingface.co/papers/2503.10633', 'title': "Charting and Navigating Hugging Face's Model Atlas", 'url': 'https://huggingface.co/papers/2503.10633', 'abstract': 'As there are now millions of publicly available neural networks, searching and analyzing large model repositories becomes increasingly important. Navigating so many models requires an atlas, but as most models are poorly documented charting such an atlas is challenging. To explore the hidden potential of model repositories, we chart a preliminary atlas representing the documented fraction of Hugging Face. It provides stunning visualizations of the model landscape and evolution. We demonstrate several applications of this atlas including predicting model attributes (e.g., accuracy), and analyzing trends in computer vision models. However, as the current atlas remains incomplete, we propose a method for charting undocumented regions. Specifically, we identify high-confidence structural priors based on dominant real-world model training practices. Leveraging these priors, our approach enables accurate mapping of previously undocumented areas of the atlas. We publicly release our datasets, code, and interactive atlas.', 'score': 50, 'issue_id': 2703, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '128e9e97a54b8404', 'authors': ['Eliahu Horwitz', 'Nitzan Kurer', 'Jonathan Kahana', 'Liel Amar', 'Yedid Hoshen'], 'affiliations': ['School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.10633.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#cv', '#data', '#survey', '#dataset'], 'emoji': '🗺️', 'ru': {'title': 'Создание атласа нейросетей: навигация в океане моделей машинного обучения', 'desc': 'Статья описывает создание атласа для навигации по миллионам доступных нейронных сетей. Авторы визуализируют ландшафт моделей на основе документированной части репозитория Hugging Face. Они демонстрируют применение атласа для предсказания атрибутов моделей и анализа трендов в компьютерном зрении. Предлагается метод для картирования недокументированных областей атласа с использованием структурных априорных знаний.'}, 'en': {'title': 'Mapping the Neural Network Landscape: An Interactive Atlas', 'desc': 'This paper addresses the challenge of navigating the vast number of publicly available neural networks by creating a visual atlas of documented models from Hugging Face. The atlas not only visualizes the model landscape but also tracks the evolution of these models over time. It includes applications such as predicting model attributes like accuracy and analyzing trends in computer vision. To improve the atlas, the authors propose a method to chart undocumented regions by using high-confidence structural priors based on common training practices.'}, 'zh': {'title': '探索神经网络模型的地图', 'desc': '随着公开神经网络数量的激增，搜索和分析大型模型库变得越来越重要。由于大多数模型文档不全，绘制模型的地图变得具有挑战性。我们绘制了一个初步的地图，展示了Hugging Face上已记录模型的分布和演变，并展示了多种应用，包括预测模型属性和分析计算机视觉模型的趋势。为了填补当前地图的空白，我们提出了一种方法，通过识别基于主流训练实践的高置信度结构先验，准确绘制未记录区域。'}}}, {'id': 'https://huggingface.co/papers/2503.10480', 'title': 'World Modeling Makes a Better Planner: Dual Preference Optimization for\n  Embodied Task Planning', 'url': 'https://huggingface.co/papers/2503.10480', 'abstract': 'Recent advances in large vision-language models (LVLMs) have shown promise for embodied task planning, yet they struggle with fundamental challenges like dependency constraints and efficiency. Existing approaches either solely optimize action selection or leverage world models during inference, overlooking the benefits of learning to model the world as a way to enhance planning capabilities. We propose Dual Preference Optimization (D^2PO), a new learning framework that jointly optimizes state prediction and action selection through preference learning, enabling LVLMs to understand environment dynamics for better planning. To automatically collect trajectories and stepwise preference data without human annotation, we introduce a tree search mechanism for extensive exploration via trial-and-error. Extensive experiments on VoTa-Bench demonstrate that our D^2PO-based method significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B), achieving superior task success rates with more efficient execution paths.', 'score': 39, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '2d3e6b8c84c51e69', 'authors': ['Siyin Wang', 'Zhaoye Fei', 'Qinyuan Cheng', 'Shiduo Zhang', 'Panpan Cai', 'Jinlan Fu', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10480.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#agents', '#optimization', '#rl', '#training'], 'emoji': '🤖', 'ru': {'title': 'Улучшение планирования действий у LVLM через совместную оптимизацию предсказаний и выбора', 'desc': 'Статья представляет новый подход к обучению больших визуально-языковых моделей (LVLM) для планирования действий в воплощенных задачах. Предложенный метод Dual Preference Optimization (D^2PO) совместно оптимизирует предсказание состояний и выбор действий через обучение предпочтениям. Для сбора данных без участия человека используется механизм поиска по дереву. Эксперименты показывают, что D^2PO значительно превосходит существующие методы по успешности выполнения задач и эффективности траекторий.'}, 'en': {'title': 'Enhancing Planning in LVLMs with Dual Preference Optimization', 'desc': 'This paper introduces Dual Preference Optimization (D^2PO), a novel framework designed to improve the planning capabilities of large vision-language models (LVLMs) by jointly optimizing state prediction and action selection. The approach addresses key challenges in embodied task planning, such as dependency constraints and efficiency, which have been inadequately handled by existing methods. By employing a tree search mechanism, D^2PO enables the automatic collection of trajectories and preference data, facilitating extensive exploration through trial-and-error without the need for human annotation. Experimental results on VoTa-Bench show that D^2PO significantly enhances task success rates and execution efficiency compared to previous methods and GPT-4o across various LVLMs.'}, 'zh': {'title': '双重偏好优化：提升任务规划能力的关键', 'desc': '本文提出了一种新的学习框架，称为双重偏好优化（D^2PO），旨在提高大型视觉语言模型（LVLMs）在任务规划中的能力。该框架通过偏好学习同时优化状态预测和动作选择，使模型能够更好地理解环境动态。为了自动收集轨迹和逐步偏好数据，本文引入了一种树搜索机制，以便通过试错进行广泛探索。实验结果表明，基于D^2PO的方法在多个基准测试中显著优于现有方法和GPT-4o，达到了更高的任务成功率和更高效的执行路径。'}}}, {'id': 'https://huggingface.co/papers/2503.10639', 'title': 'GoT: Unleashing Reasoning Capability of Multimodal Large Language Model\n  for Visual Generation and Editing', 'url': 'https://huggingface.co/papers/2503.10639', 'abstract': 'Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT.', 'score': 37, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '70c9d741eedd181c', 'authors': ['Rongyao Fang', 'Chengqi Duan', 'Kun Wang', 'Linjiang Huang', 'Hao Li', 'Shilin Yan', 'Hao Tian', 'Xingyu Zeng', 'Rui Zhao', 'Jifeng Dai', 'Xihui Liu', 'Hongsheng Li'], 'affiliations': ['BUAA', 'CUHK MMLab', 'HKU', 'SenseTime', 'Shanghai AI Laboratory', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2503.10639.jpg', 'data': {'categories': ['#multimodal', '#cv', '#dataset', '#reasoning', '#diffusion', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Разумная генерация изображений: от текста к визуальному мышлению', 'desc': 'Статья представляет новую парадигму Generation Chain-of-Thought (GoT) для генерации и редактирования изображений. GoT использует явный процесс языкового рассуждения перед выводом изображений, анализируя семантические отношения и пространственное расположение. Авторы создали масштабные наборы данных GoT и реализовали унифицированную архитектуру, объединяющую Qwen2.5-VL для генерации цепочек рассуждений с диффузионной моделью. Эксперименты показывают значительное улучшение производительности по сравнению с базовыми методами и возможность интерактивной визуальной генерации.'}, 'en': {'title': 'Revolutionizing Image Generation with Reasoning!', 'desc': 'The paper introduces Generation Chain-of-Thought (GoT), a new method for image generation and editing that incorporates reasoning about visual composition. Instead of simply processing text prompts, GoT uses a reasoning-guided framework to analyze semantic relationships and spatial arrangements before generating images. It includes a large dataset with over 9 million samples that capture detailed reasoning chains, enhancing the understanding of how different elements relate visually. The results show that GoT significantly improves the quality of generated and edited images, allowing for interactive adjustments based on user-defined reasoning steps.'}, 'zh': {'title': '推理驱动的图像生成与编辑新方向', 'desc': '当前的图像生成和编辑方法主要将文本提示作为直接输入，而没有考虑视觉构图和明确的操作。我们提出了一种新颖的生成思维链（GoT）范式，通过在输出图像之前进行明确的语言推理过程来实现生成和编辑。该方法将传统的文本到图像生成和编辑转变为一个基于推理的框架，分析语义关系和空间排列。我们的GoT框架在生成和编辑任务上表现出色，显著优于基线，并允许用户通过明确修改推理步骤来进行交互式视觉生成。'}}}, {'id': 'https://huggingface.co/papers/2503.09669', 'title': 'Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2503.09669', 'abstract': 'Text-to-image diffusion models have achieved remarkable success in generating high-quality contents from text prompts. However, their reliance on publicly available data and the growing trend of data sharing for fine-tuning make these models particularly vulnerable to data poisoning attacks. In this work, we introduce the Silent Branding Attack, a novel data poisoning method that manipulates text-to-image diffusion models to generate images containing specific brand logos or symbols without any text triggers. We find that when certain visual patterns are repeatedly in the training data, the model learns to reproduce them naturally in its outputs, even without prompt mentions. Leveraging this, we develop an automated data poisoning algorithm that unobtrusively injects logos into original images, ensuring they blend naturally and remain undetected. Models trained on this poisoned dataset generate images containing logos without degrading image quality or text alignment. We experimentally validate our silent branding attack across two realistic settings on large-scale high-quality image datasets and style personalization datasets, achieving high success rates even without a specific text trigger. Human evaluation and quantitative metrics including logo detection show that our method can stealthily embed logos.', 'score': 31, 'issue_id': 2701, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'f54f510a4cf48d22', 'authors': ['Sangwon Jang', 'June Suk Choi', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.09669.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#security', '#diffusion', '#data'], 'emoji': '🕵️', 'ru': {'title': 'Невидимые логотипы: скрытое внедрение брендов в генеративные модели', 'desc': "Это исследование представляет новый метод атаки на модели диффузии текст-изображение путем отравления данных. Метод, названный 'Тихая брендинговая атака', заставляет модели генерировать изображения с определенными логотипами без явных текстовых триггеров. Авторы разработали алгоритм, который незаметно внедряет логотипы в оригинальные изображения для обучения модели. Эксперименты показали высокую эффективность метода на крупномасштабных наборах данных, что подтверждено как человеческой оценкой, так и количественными метриками."}, 'en': {'title': 'Stealthy Logo Injection in Image Generation', 'desc': "This paper presents a new method called the Silent Branding Attack, which targets text-to-image diffusion models by subtly injecting brand logos into training data. The attack exploits the model's tendency to reproduce visual patterns it has seen, allowing logos to appear in generated images without any explicit text prompts. The authors developed an automated algorithm that seamlessly integrates these logos into original images, making them difficult to detect. Their experiments demonstrate that models trained on this manipulated data can produce high-quality images containing logos, achieving significant success rates in various settings."}, 'zh': {'title': '静默品牌攻击：隐秘植入品牌标志的创新方法', 'desc': '文本到图像的扩散模型在根据文本提示生成高质量内容方面取得了显著成功。然而，这些模型依赖于公开数据，并且数据共享的趋势使其特别容易受到数据中毒攻击。本文提出了一种新的数据中毒方法——静默品牌攻击，能够操控文本到图像的扩散模型生成包含特定品牌标志或符号的图像，而无需任何文本触发。我们开发了一种自动化的数据中毒算法，可以在原始图像中悄无声息地注入标志，确保它们自然融合且不被检测。'}}}, {'id': 'https://huggingface.co/papers/2503.10291', 'title': 'VisualPRM: An Effective Process Reward Model for Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2503.10291', 'abstract': 'We introduce VisualPRM, an advanced multimodal Process Reward Model (PRM) with 8B parameters, which improves the reasoning abilities of existing Multimodal Large Language Models (MLLMs) across different model scales and families with Best-of-N (BoN) evaluation strategies. Specifically, our model improves the reasoning performance of three types of MLLMs and four different model scales. Even when applied to the highly capable InternVL2.5-78B, it achieves a 5.9-point improvement across seven multimodal reasoning benchmarks. Experimental results show that our model exhibits superior performance compared to Outcome Reward Models and Self-Consistency during BoN evaluation. To facilitate the training of multimodal PRMs, we construct a multimodal process supervision dataset VisualPRM400K using an automated data pipeline. For the evaluation of multimodal PRMs, we propose VisualProcessBench, a benchmark with human-annotated step-wise correctness labels, to measure the abilities of PRMs to detect erroneous steps in multimodal reasoning tasks. We hope that our work can inspire more future research and contribute to the development of MLLMs. Our model, data, and benchmark are released in https://internvl.github.io/blog/2025-03-13-VisualPRM/.', 'score': 28, 'issue_id': 2705, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'f4734440c38ca048', 'authors': ['Weiyun Wang', 'Zhangwei Gao', 'Lianjie Chen', 'Zhe Chen', 'Jinguo Zhu', 'Xiangyu Zhao', 'Yangzhou Liu', 'Yue Cao', 'Shenglong Ye', 'Xizhou Zhu', 'Lewei Lu', 'Haodong Duan', 'Yu Qiao', 'Jifeng Dai', 'Wenhai Wang'], 'affiliations': ['Fudan University', 'Nanjing University', 'SenseTime Research', 'Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10291.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#reasoning', '#multimodal', '#training'], 'emoji': '🧠', 'ru': {'title': 'VisualPRM: улучшение мультимодальных рассуждений с помощью продвинутой модели вознаграждения процесса', 'desc': 'VisualPRM - это усовершенствованная мультимодальная модель вознаграждения процесса с 8 миллиардами параметров. Она улучшает способности рассуждения существующих мультимодальных больших языковых моделей (MLLM) разных масштабов и семейств с использованием стратегии оценки Best-of-N. Модель демонстрирует превосходную производительность по сравнению с моделями вознаграждения результатов и самосогласованностью при оценке BoN. Для обучения и оценки мультимодальных PRM авторы создали набор данных VisualPRM400K и бенчмарк VisualProcessBench.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with VisualPRM', 'desc': 'VisualPRM is a multimodal Process Reward Model designed to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) with 8 billion parameters. It demonstrates significant improvements in reasoning performance across various model scales and types, achieving a notable 5.9-point increase on seven multimodal reasoning benchmarks, even with high-capacity models like InternVL2.5-78B. The model outperforms traditional Outcome Reward Models and Self-Consistency methods during Best-of-N evaluations. Additionally, VisualPRM introduces a new dataset, VisualPRM400K, and a benchmark, VisualProcessBench, to support the training and evaluation of multimodal PRMs.'}, 'zh': {'title': '提升多模态推理能力的VisualPRM模型', 'desc': '我们介绍了一种先进的多模态过程奖励模型（PRM）VisualPRM，具有80亿个参数，能够提升现有多模态大型语言模型（MLLMs）的推理能力。该模型在三种类型的MLLMs和四种不同的模型规模上均表现出色，尤其是在强大的InternVL2.5-78B模型上，推理性能提高了5.9分。实验结果表明，VisualPRM在最佳选择（BoN）评估中优于结果奖励模型和自一致性方法。为了支持多模态PRMs的训练，我们构建了一个名为VisualPRM400K的多模态过程监督数据集，并提出了VisualProcessBench基准，以评估PRMs在多模态推理任务中检测错误步骤的能力。'}}}, {'id': 'https://huggingface.co/papers/2503.09662', 'title': 'CoRe^2: Collect, Reflect and Refine to Generate Better and Faster', 'url': 'https://huggingface.co/papers/2503.09662', 'abstract': "Making text-to-image (T2I) generative model sample both fast and well represents a promising research direction. Previous studies have typically focused on either enhancing the visual quality of synthesized images at the expense of sampling efficiency or dramatically accelerating sampling without improving the base model's generative capacity. Moreover, nearly all inference methods have not been able to ensure stable performance simultaneously on both diffusion models (DMs) and visual autoregressive models (ARMs). In this paper, we introduce a novel plug-and-play inference paradigm, CoRe^2, which comprises three subprocesses: Collect, Reflect, and Refine. CoRe^2 first collects classifier-free guidance (CFG) trajectories, and then use collected data to train a weak model that reflects the easy-to-learn contents while reducing number of function evaluations during inference by half. Subsequently, CoRe^2 employs weak-to-strong guidance to refine the conditional output, thereby improving the model's capacity to generate high-frequency and realistic content, which is difficult for the base model to capture. To the best of our knowledge, CoRe^2 is the first to demonstrate both efficiency and effectiveness across a wide range of DMs, including SDXL, SD3.5, and FLUX, as well as ARMs like LlamaGen. It has exhibited significant performance improvements on HPD v2, Pick-of-Pic, Drawbench, GenEval, and T2I-Compbench. Furthermore, CoRe^2 can be seamlessly integrated with the state-of-the-art Z-Sampling, outperforming it by 0.3 and 0.16 on PickScore and AES, while achieving 5.64s time saving using SD3.5.Code is released at https://github.com/xie-lab-ml/CoRe/tree/main.", 'score': 28, 'issue_id': 2705, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'a6fdb00ac6a8ebca', 'authors': ['Shitong Shao', 'Zikai Zhou', 'Dian Xie', 'Yuetong Fang', 'Tian Ye', 'Lichen Bai', 'Zeke Xie'], 'affiliations': ['The Hong Kong University of Science and Technology (GuangZhou)'], 'pdf_title_img': 'assets/pdf/title_img/2503.09662.jpg', 'data': {'categories': ['#cv', '#benchmark', '#inference', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'CoRe^2: Революционный метод для быстрой и качественной генерации изображений по тексту', 'desc': 'Статья представляет новый метод CoRe^2 для улучшения генеративных моделей текст-в-изображение. Метод состоит из трех этапов: сбор траекторий безклассификационного руководства, обучение слабой модели и уточнение результатов с помощью руководства от слабой к сильной модели. CoRe^2 показывает высокую эффективность и качество результатов для различных диффузионных и авторегрессионных моделей. Метод превосходит современные подходы по метрикам качества изображений и скорости генерации.'}, 'en': {'title': 'CoRe^2: Fast and High-Quality Text-to-Image Generation', 'desc': 'This paper presents CoRe^2, a new method for improving the speed and quality of text-to-image (T2I) generative models. It introduces a three-step process: Collect, Reflect, and Refine, which enhances sampling efficiency while maintaining high visual fidelity. By using classifier-free guidance trajectories, CoRe^2 trains a weak model that simplifies the learning process and reduces function evaluations during inference. The method shows significant performance gains across various diffusion models and autoregressive models, making it a notable advancement in the field of generative modeling.'}, 'zh': {'title': '高效生成，真实图像！', 'desc': '本文提出了一种新的文本到图像生成模型推理范式，称为CoRe^2。该方法通过三个子过程：收集、反射和精炼，来提高生成效率和效果。CoRe^2首先收集无分类器引导轨迹，然后训练一个弱模型以减少推理过程中的函数评估次数。最后，通过弱到强的引导来精炼条件输出，从而生成更高频率和更真实的内容，显著提升了多种扩散模型和自回归模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.10437', 'title': '4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.10437', 'abstract': 'Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries.', 'score': 22, 'issue_id': 2702, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '212017b2c934863c', 'authors': ['Wanhua Li', 'Renping Zhou', 'Jiawei Zhou', 'Yingwei Song', 'Johannes Herter', 'Minghan Qin', 'Gao Huang', 'Hanspeter Pfister'], 'affiliations': ['Brown University', 'ETH Zurich', 'Harvard University', 'Stony Brook University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10437.jpg', 'data': {'categories': ['#3d', '#video', '#optimization', '#multimodal', '#interpretability', '#reasoning', '#games', '#agi', '#transfer_learning'], 'emoji': '🌐', 'ru': {'title': '4D языковые поля для динамических сцен: от статики к временной динамике', 'desc': '4D LangSplat - это метод для создания 4D языковых полей, позволяющий выполнять открытые языковые запросы в динамических сценах с учетом времени. Он использует мультимодальные большие языковые модели (MLLM) для генерации подробных описаний объектов в видео, которые затем кодируются в высококачественные векторные представления с помощью LLM. Метод включает сеть деформации состояний для моделирования плавных изменений объектов во времени. 4D LangSplat демонстрирует точные и эффективные результаты для открытых запросов как с учетом времени, так и без него.'}, 'en': {'title': 'Empowering Dynamic Scene Understanding with 4D Language Fields', 'desc': 'This paper introduces 4D LangSplat, a novel approach for creating 4D language fields that can handle dynamic scenes in videos. Unlike previous models that focus on static images, 4D LangSplat learns directly from text generated by Multimodal Large Language Models (MLLMs) using object-wise video captions. The method employs a multimodal prompting technique to produce high-quality, temporally consistent captions, which are then transformed into sentence embeddings for effective querying. Additionally, a status deformable network is proposed to accurately model the continuous changes of objects over time, resulting in improved performance for both time-sensitive and time-agnostic queries.'}, 'zh': {'title': '动态场景中的4D语言场学习', 'desc': '本论文提出了一种新的方法，称为4D LangSplat，旨在处理动态场景中的时间敏感和开放词汇查询。与现有的静态图像-文本模型不同，4D LangSplat能够学习4D语言场，直接从对象视频字幕生成的文本中获取信息。该方法利用多模态大型语言模型（MLLMs）生成高质量的时间一致性字幕，并将其编码为句子嵌入，以支持像素对齐的对象特征监督。通过引入状态可变形网络，4D LangSplat有效建模了对象在时间上的连续变化，展示了在多个基准测试中取得的精确和高效的查询结果。'}}}, {'id': 'https://huggingface.co/papers/2503.08677', 'title': 'OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting', 'url': 'https://huggingface.co/papers/2503.08677', 'abstract': 'Diffusion-based generative models have revolutionized object-oriented image editing, yet their deployment in realistic object removal and insertion remains hampered by challenges such as the intricate interplay of physical effects and insufficient paired training data. In this work, we introduce OmniPaint, a unified framework that re-conceptualizes object removal and insertion as interdependent processes rather than isolated tasks. Leveraging a pre-trained diffusion prior along with a progressive training pipeline comprising initial paired sample optimization and subsequent large-scale unpaired refinement via CycleFlow, OmniPaint achieves precise foreground elimination and seamless object insertion while faithfully preserving scene geometry and intrinsic properties. Furthermore, our novel CFD metric offers a robust, reference-free evaluation of context consistency and object hallucination, establishing a new benchmark for high-fidelity image editing. Project page: https://yeates.github.io/OmniPaint-Page/', 'score': 22, 'issue_id': 2712, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '6110f983d46b536a', 'authors': ['Yongsheng Yu', 'Ziyun Zeng', 'Haitian Zheng', 'Jiebo Luo'], 'affiliations': ['Adobe Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2503.08677.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#hallucinations', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'OmniPaint: революция в редактировании изображений с помощью ИИ', 'desc': 'OmniPaint - это унифицированная система для редактирования изображений, основанная на диффузионных генеративных моделях. Она рассматривает удаление и вставку объектов как взаимосвязанные процессы, используя предобученную диффузионную модель и прогрессивный конвейер обучения. OmniPaint достигает точного удаления переднего плана и бесшовной вставки объектов, сохраняя геометрию сцены и внутренние свойства. Система также предлагает новую метрику CFD для оценки согласованности контекста и галлюцинаций объектов.'}, 'en': {'title': 'OmniPaint: Seamless Object Editing through Interconnected Processes', 'desc': "This paper presents OmniPaint, a new framework for object removal and insertion in images, treating these tasks as interconnected rather than separate. It utilizes a pre-trained diffusion model and a two-step training process that starts with paired samples and then refines the model using unpaired data. OmniPaint effectively removes unwanted objects and adds new ones while maintaining the original scene's geometry and characteristics. Additionally, the authors introduce a new evaluation metric called CFD, which measures the consistency and realism of the edited images without needing reference images."}, 'zh': {'title': 'OmniPaint：物体编辑的新纪元', 'desc': '本论文介绍了一种名为OmniPaint的统一框架，旨在解决物体移除和插入的复杂问题。该框架将物体移除和插入视为相互依赖的过程，而非孤立的任务。通过利用预训练的扩散模型和逐步训练流程，OmniPaint能够实现精确的前景消除和无缝的物体插入，同时保持场景的几何形状和内在特性。此外，我们提出的CFD指标为上下文一致性和物体幻觉提供了一种强健的无参考评估方法，建立了高保真图像编辑的新基准。'}}}, {'id': 'https://huggingface.co/papers/2503.10596', 'title': 'GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding', 'url': 'https://huggingface.co/papers/2503.10596', 'abstract': 'Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., 4.5 times faster than the GLaMM.', 'score': 18, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '1b9ea337d198c741', 'authors': ['Rui Hu', 'Lianghui Zhu', 'Yuxuan Zhang', 'Tianheng Cheng', 'Lei Liu', 'Heng Liu', 'Longjin Ran', 'Xiaoxin Chen', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['School of EIC, Huazhong University of Science & Technology', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.10596.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'GroundingSuite: революция в пиксельной локализации объектов', 'desc': 'Статья представляет GroundingSuite - новый набор инструментов для задачи пиксельной локализации объектов по текстовому описанию. Он включает автоматизированную систему аннотации данных на основе мультиагентного подхода с использованием нескольких моделей машинного обучения для обработки естественного языка и компьютерного зрения. GroundingSuite также содержит крупномасштабный датасет для обучения с 9,56 миллионами разнообразных языковых выражений и соответствующих им сегментаций, а также тщательно подготовленный набор данных для оценки из 3800 изображений. Модели, обученные на этом датасете, достигают наилучших результатов в задачах сегментации по текстовому описанию.'}, 'en': {'title': 'Revolutionizing Pixel Grounding with GroundingSuite', 'desc': 'This paper introduces GroundingSuite, a new framework designed to enhance pixel grounding tasks like Referring Expression Segmentation (RES) by addressing the limitations of existing datasets. GroundingSuite features an automated data annotation system that utilizes multiple Vision-Language Model (VLM) agents, resulting in a large-scale dataset with 9.56 million diverse referring expressions and their segmentations. The framework not only improves the quality and diversity of training data but also provides a curated evaluation benchmark with 3,800 images. Models trained on this dataset achieve state-of-the-art performance, significantly outperforming previous methods in both efficiency and accuracy.'}, 'zh': {'title': 'GroundingSuite：提升视觉与语言的桥梁', 'desc': '本文介绍了一个名为GroundingSuite的框架，旨在解决现有数据集在像素定位任务中的局限性。该框架包括一个自动化数据注释系统，利用多个视觉-语言模型（VLM）代理生成数据。GroundingSuite提供了一个包含956万种多样化指称表达及其对应分割的大规模训练数据集，并建立了一个包含3800张图像的评估基准。通过使用GroundingSuite训练的数据集，模型的性能显著提升，达到了最新的研究成果。'}}}, {'id': 'https://huggingface.co/papers/2503.10351', 'title': 'New Trends for Modern Machine Translation with Large Reasoning Models', 'url': 'https://huggingface.co/papers/2503.10351', 'abstract': 'Recent advances in Large Reasoning Models (LRMs), particularly those leveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility for Machine Translation (MT). This position paper argues that LRMs substantially transformed traditional neural MT as well as LLMs-based MT paradigms by reframing translation as a dynamic reasoning task that requires contextual, cultural, and linguistic understanding and reasoning. We identify three foundational shifts: 1) contextual coherence, where LRMs resolve ambiguities and preserve discourse structure through explicit reasoning over cross-sentence and complex context or even lack of context; 2) cultural intentionality, enabling models to adapt outputs by inferring speaker intent, audience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can perform self-reflection during the inference time to correct the potential errors in translation especially extremely noisy cases, showing better robustness compared to simply mapping X->Y translation. We explore various scenarios in translation including stylized translation, document-level translation and multimodal translation by showcasing empirical examples that demonstrate the superiority of LRMs in translation. We also identify several interesting phenomenons for LRMs for MT including auto-pivot translation as well as the critical challenges such as over-localisation in translation and inference efficiency. In conclusion, we think that LRMs redefine translation systems not merely as text converters but as multilingual cognitive agents capable of reasoning about meaning beyond the text. This paradigm shift reminds us to think of problems in translation beyond traditional translation scenarios in a much broader context with LRMs - what we can achieve on top of it.', 'score': 18, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'c0a1b109c05698cc', 'authors': ['Sinuo Liu', 'Chenyang Lyu', 'Minghao Wu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.10351.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multilingual', '#machine_translation'], 'emoji': '🧠', 'ru': {'title': 'LRM: От простого перевода к многоязычным когнитивным агентам', 'desc': 'Эта статья рассматривает влияние больших моделей рассуждений (LRM) на машинный перевод. Авторы утверждают, что LRM трансформируют традиционные подходы, представляя перевод как динамическую задачу рассуждения, требующую контекстуального, культурного и лингвистического понимания. Выделяются три ключевых изменения: контекстуальная согласованность, культурная интенциональность и саморефлексия. Статья также исследует различные сценарии применения LRM в переводе и обсуждает возникающие феномены и проблемы.'}, 'en': {'title': 'Transforming Translation: LRMs as Cognitive Agents', 'desc': 'This paper discusses how Large Reasoning Models (LRMs) are changing the way we think about Machine Translation (MT). It highlights three key shifts: first, LRMs improve contextual coherence by understanding complex contexts and resolving ambiguities; second, they incorporate cultural intentionality, allowing translations to reflect speaker intent and social norms; and third, LRMs can self-reflect during translation to correct errors, making them more robust. The authors provide examples of how LRMs excel in various translation scenarios, suggesting that these models should be seen as cognitive agents that reason about meaning rather than just text converters.'}, 'zh': {'title': '大型推理模型重塑机器翻译的未来', 'desc': '大型推理模型（LRMs）在机器翻译（MT）领域带来了新的可能性，特别是通过链式思维推理（CoT）。这篇论文指出，LRMs将传统神经机器翻译转变为一种动态推理任务，强调上下文、文化和语言理解的重要性。我们识别出三个基础转变：上下文连贯性、文化意图性和自我反思能力，使得LRMs在翻译中表现出更好的鲁棒性。最终，我们认为LRMs重新定义了翻译系统，使其不仅仅是文本转换器，而是能够超越文本进行意义推理的多语言认知代理。'}}}, {'id': 'https://huggingface.co/papers/2503.04723', 'title': 'Shifting Long-Context LLMs Research from Input to Output', 'url': 'https://huggingface.co/papers/2503.04723', 'abstract': 'Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications.', 'score': 18, 'issue_id': 2700, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '009f3e654e927dc3', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqing Hu', 'Shangqing Tu', 'Ming Shan Hee', 'Juanzi Li', 'Roy Ka-Wei Lee'], 'affiliations': ['Singapore University', 'Singapore University of Technology and Design', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04723.jpg', 'data': {'categories': ['#data', '#long_context', '#multimodal'], 'emoji': '📝', 'ru': {'title': 'Новый вызов для ИИ: создание длинных текстов', 'desc': 'Статья обсуждает необходимость развития моделей обработки естественного языка для генерации длинных текстов. Авторы отмечают, что современные исследования в основном сосредоточены на обработке длинных входных контекстов, но не на создании объемных выходных данных. Подчеркивается важность разработки языковых моделей, способных генерировать связные и логически последовательные длинные тексты. Такие модели необходимы для решения задач написания романов, долгосрочного планирования и сложных рассуждений.'}, 'en': {'title': 'Bridging the Gap: Enhancing Long-Form Output in LLMs', 'desc': 'This paper discusses the need for improvements in long-form output generation by Large Language Models (LLMs). While recent advancements have focused on understanding long input contexts, generating coherent and contextually rich long outputs remains underexplored. The authors emphasize that tasks like novel writing and complex reasoning require models to produce logically consistent extended text. They call for more research efforts to develop LLMs that can effectively handle these long-output challenges, which are crucial for various real-world applications.'}, 'zh': {'title': '推动长文本生成的研究转型', 'desc': '最近，长上下文的大型语言模型（LLMs）在处理扩展输入上下文方面取得了显著进展，但生成长文本输出的研究相对较少。本文提倡自然语言处理（NLP）研究向解决长输出生成的挑战转变。长篇小说写作、长期规划和复杂推理等任务需要模型理解广泛的上下文，并生成连贯、丰富且逻辑一致的扩展文本。我们强调这一未被充分探索领域的重要性，并呼吁集中力量开发专门用于生成高质量长文本输出的基础LLM，以满足现实应用的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.10460', 'title': 'Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond', 'url': 'https://huggingface.co/papers/2503.10460', 'abstract': 'This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL.', 'score': 17, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '503c1d89e949bb41', 'authors': ['Liang Wen', 'Yunke Cai', 'Fenrui Xiao', 'Xin He', 'Qi An', 'Zhenyu Duan', 'Yimin Du', 'Junchen Liu', 'Lifu Tang', 'Xiaowei Lv', 'Haosheng Zou', 'Yongchao Deng', 'Shousheng Jia', 'Xiangzheng Zhang'], 'affiliations': ['Qiyuan Tech', 'Renmin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10460.jpg', 'data': {'categories': ['#rlhf', '#dataset', '#rl', '#reasoning', '#training', '#optimization', '#long_context', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в обучении ИИ длинным цепочкам рассуждений', 'desc': 'Статья представляет серию моделей Light-R1, обученных выполнять длинные цепочки рассуждений (COT) с нуля. Используя двухэтапное обучение с учителем (SFT) и полу-онлайновое обучение с предпочтениями (DPO), авторы получили модель Light-R1-32B, превосходящую аналоги в математических задачах. Дальнейшее применение обучения с подкреплением (RL) позволило создать модель Light-R1-14B-DS, достигшую наилучших результатов среди 14B моделей в математике. Исследование подтверждает эффективность обучения длинным COT с нуля и демонстрирует важность качественных данных для SFT.'}, 'en': {'title': 'Revolutionizing Long COT Models with Light-R1 Series', 'desc': "This paper introduces the Light-R1 series, focusing on training long Chain of Thought (COT) models from scratch. The authors employ a two-stage Supervised Fine-Tuning (SFT) and semi-on-policy Direct Preference Optimization (DPO) to enhance the model's math capabilities, resulting in the Light-R1-32B model that outperforms existing models. They also demonstrate that a specially constructed 3k dataset significantly boosts the performance of other models, achieving state-of-the-art (SOTA) results in various parameter sizes. Additionally, the application of reinforcement learning (RL) further improves reasoning performance, with the Light-R1-14B-DS model achieving impressive scores in math tasks, surpassing even larger models."}, 'zh': {'title': '从零开始训练长链推理模型的成功之路', 'desc': '本文介绍了Light-R1系列的研究工作，发布了模型、数据和代码。我们专注于从头开始训练长链推理（COT）模型，采用两阶段的监督微调（SFT）和半在线策略优化（DPO）进行课程训练。Light-R1-32B模型在数学性能上优于DeepSeek-R1-Distill-Qwen-32B，尽管仅在数学数据上训练，但在其他领域也表现出强大的泛化能力。此外，通过强化学习（RL）进一步提升推理性能，我们成功训练了Light-R1-14B-DS，达到了14B参数模型中的最新技术水平。'}}}, {'id': 'https://huggingface.co/papers/2503.10618', 'title': 'DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture\n  Design in Text to Image Generation', 'url': 'https://huggingface.co/papers/2503.10618', 'abstract': 'In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size.', 'score': 16, 'issue_id': 2704, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '3e0a293a0bdb9c8c', 'authors': ['Chen Chen', 'Rui Qian', 'Wenze Hu', 'Tsu-Jui Fu', 'Lezhi Li', 'Bowen Zhang', 'Alex Schwing', 'Wei Liu', 'Yinfei Yang'], 'affiliations': ['Apple Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.10618.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#dataset', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Эффективные Диффузионные Трансформеры для генерации изображений', 'desc': 'В этой работе исследуются Диффузионные Трансформеры (DiT) для генерации изображений по тексту. Авторы изучают различные архитектуры DiT, включая варианты PixArt и MMDiT, и сравнивают их со стандартным DiT. Результаты показывают, что стандартный DiT сопоставим по производительности со специализированными моделями, но более эффективен по параметрам. Используя стратегию послойного разделения параметров, исследователи добиваются дальнейшего уменьшения размера модели на 66% по сравнению с MMDiT.'}, 'en': {'title': 'Efficient Image Generation with Diffusion Transformers', 'desc': 'This paper investigates Diffusion Transformers (DiTs) for generating images from text, examining different architectural designs and training methods. The authors compare various DiT models, including specialized versions and a standard model that combines text and noise inputs. They find that the standard DiT performs similarly to more complex models while being more efficient in terms of parameters, especially when scaled. Additionally, they introduce new models, DiT-Air and DiT-Air-Lite, which achieve top performance on benchmark tests while maintaining a smaller size.'}, 'zh': {'title': '扩散变换器：高效的文本到图像生成', 'desc': '本文研究了扩散变换器（DiTs）在文本到图像生成中的应用，重点关注架构选择、文本条件策略和训练协议。我们评估了多种基于DiT的架构，包括PixArt风格和MMDiT变体，并与直接处理文本和噪声输入的标准DiT变体进行了比较。研究结果表明，标准DiT的性能与这些专业模型相当，同时在参数效率上表现更佳，尤其是在模型规模增大时。通过层级参数共享策略，我们将模型大小进一步减少了66%，对性能影响极小。'}}}, {'id': 'https://huggingface.co/papers/2503.10582', 'title': 'VisualWebInstruct: Scaling up Multimodal Instruction Data through Web\n  Search', 'url': 'https://huggingface.co/papers/2503.10582', 'abstract': "Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct - a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks.", 'score': 16, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'ff1e79c1589f8602', 'authors': ['Yiming Jia', 'Jiachen Li', 'Xiang Yue', 'Bo Li', 'Ping Nie', 'Kai Zou', 'Wenhu Chen'], 'affiliations': ['CMU', 'Independent', 'NUS', 'Netmind.ai', 'UC Santa Barbara', 'University of Toronto', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2503.10582.jpg', 'data': {'categories': ['#dataset', '#data', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'VisualWebInstruct: прорыв в обучении мультимодальных моделей рассуждению', 'desc': 'Исследователи представили VisualWebInstruct - новый подход к созданию разнообразного и качественного набора данных для обучения мультимодальных моделей машинного обучения. Используя поисковую систему и обработку HTML-страниц, они собрали около 900 тысяч пар вопрос-ответ по различным дисциплинам. Модели, дообученные на этом наборе данных, показали значительный прирост производительности на нескольких бенчмарках. Лучшая модель MAmmoTH-VL2 продемонстрировала state-of-the-art результаты в своем классе на задачах рассуждения и решения задач.'}, 'en': {'title': 'Enhancing Reasoning in Vision-Language Models with VisualWebInstruct', 'desc': 'This paper introduces VisualWebInstruct, a new dataset aimed at improving the reasoning abilities of Vision-Language Models (VLMs). The dataset is created by leveraging search engines to gather diverse and high-quality multimodal data, specifically focusing on reasoning tasks across various disciplines. By processing over 700,000 unique web sources, the authors compile approximately 900,000 question-answer pairs, enhancing the training data available for VLMs. The results show that models trained on this dataset achieve significant performance improvements on reasoning benchmarks, demonstrating its effectiveness in advancing VLM capabilities.'}, 'zh': {'title': '提升视觉语言模型推理能力的新数据集', 'desc': '本文提出了一种新的方法VisualWebInstruct，旨在解决推理导向的多模态数据集稀缺问题。我们利用搜索引擎创建了一个多学科的高质量数据集，包括数学、物理、金融和化学等领域。通过从30,000个种子图像开始，收集和处理超过70万个独特网址的HTML内容，我们构建了约90万个问答对的数据集。经过在VisualWebInstruct上微调的模型在多个基准测试中表现出显著的性能提升，证明了该数据集在增强视觉语言模型推理能力方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.09642', 'title': 'Open-Sora 2.0: Training a Commercial-Level Video Generation Model in\n  $200k', 'url': 'https://huggingface.co/papers/2503.09642', 'abstract': 'Video generation models have achieved remarkable progress in the past year. The quality of AI video continues to improve, but at the cost of larger model size, increased data quantity, and greater demand for training compute. In this report, we present Open-Sora 2.0, a commercial-level video generation model trained for only $200k. With this model, we demonstrate that the cost of training a top-performing video generation model is highly controllable. We detail all techniques that contribute to this efficiency breakthrough, including data curation, model architecture, training strategy, and system optimization. According to human evaluation results and VBench scores, Open-Sora 2.0 is comparable to global leading video generation models including the open-source HunyuanVideo and the closed-source Runway Gen-3 Alpha. By making Open-Sora 2.0 fully open-source, we aim to democratize access to advanced video generation technology, fostering broader innovation and creativity in content creation. All resources are publicly available at: https://github.com/hpcaitech/Open-Sora.', 'score': 14, 'issue_id': 2701, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '8987bc0bbdde3b5a', 'authors': ['Xiangyu Peng', 'Zangwei Zheng', 'Chenhui Shen', 'Tom Young', 'Xinying Guo', 'Binluo Wang', 'Hang Xu', 'Hongxin Liu', 'Mingyan Jiang', 'Wenjun Li', 'Yuhui Wang', 'Anbang Ye', 'Gang Ren', 'Qianran Ma', 'Wanying Liang', 'Xiang Lian', 'Xiwen Wu', 'Yuting Zhong', 'Zhuangyan Li', 'Chaoyu Gong', 'Guojun Lei', 'Leijun Cheng', 'Limin Zhang', 'Minghao Li', 'Ruijie Zhang', 'Silan Hu', 'Shijie Huang', 'Xiaokang Wang', 'Yuanheng Zhao', 'Yuqi Wang', 'Ziang Wei', 'Yang You'], 'affiliations': ['HPC-AI Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.09642.jpg', 'data': {'categories': ['#training', '#optimization', '#open_source', '#architecture', '#video', '#data'], 'emoji': '🎬', 'ru': {'title': 'Доступная генерация видео: качество мирового уровня по разумной цене', 'desc': 'Open-Sora 2.0 - это модель генерации видео, обученная всего за $200 тыс. Она демонстрирует, что создание высокопроизводительной модели генерации видео может быть экономически эффективным. Исследователи применили ряд техник для оптимизации, включая курирование данных, архитектуру модели и стратегию обучения. По результатам человеческой оценки и показателям VBench, Open-Sora 2.0 сопоставима с ведущими моделями генерации видео.'}, 'en': {'title': 'Affordable Excellence in Video Generation', 'desc': 'Open-Sora 2.0 is a new video generation model that has been developed to produce high-quality videos while keeping training costs low. It was trained for only $200,000, showcasing that effective video generation can be achieved without massive resources. The model incorporates various techniques such as optimized data curation, innovative model architecture, and efficient training strategies to enhance performance. By making Open-Sora 2.0 open-source, the authors aim to provide wider access to advanced video generation tools, encouraging creativity and innovation in the field.'}, 'zh': {'title': 'Open-Sora 2.0：高效视频生成的未来', 'desc': '视频生成模型在过去一年取得了显著进展。虽然AI视频的质量不断提高，但这也导致了模型规模增大、数据量增加和训练计算需求加大。我们介绍了Open-Sora 2.0，这是一个商业级的视频生成模型，仅用20万美元进行训练。通过优化数据管理、模型架构、训练策略和系统性能，我们展示了高效训练顶级视频生成模型的可控性，并希望通过开源这一模型来促进内容创作的创新与发展。'}}}, {'id': 'https://huggingface.co/papers/2503.09641', 'title': 'SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency\n  Distillation', 'url': 'https://huggingface.co/papers/2503.09641', 'abstract': 'This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step - outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10x faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024 x 1024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.', 'score': 14, 'issue_id': 2702, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '4079b7fe9f67a246', 'authors': ['Junsong Chen', 'Shuchen Xue', 'Yuyang Zhao', 'Jincheng Yu', 'Sayak Paul', 'Junyu Chen', 'Han Cai', 'Enze Xie', 'Song Han'], 'affiliations': ['Huggingface', 'Independent Researcher', 'MIT', 'NVIDIA', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09641.jpg', 'data': {'categories': ['#diffusion', '#training', '#inference', '#cv', '#open_source'], 'emoji': '🚀', 'ru': {'title': 'Молниеносная генерация изображений с SANA-Sprint', 'desc': 'SANA-Sprint - это эффективная диффузионная модель для сверхбыстрой генерации изображений по тексту. Она использует предобученную базовую модель и гибридную дистилляцию, сокращая количество шагов вывода с 20 до 1-4. Ключевые инновации включают подход без обучения для непрерывной консистентной дистилляции, унифицированную адаптивную модель и интеграцию с ControlNet для интерактивной генерации. SANA-Sprint достигает state-of-the-art результатов по метрикам FID и GenEval за 1 шаг, превосходя существующие решения по скорости и качеству.'}, 'en': {'title': 'SANA-Sprint: Ultra-Fast Text-to-Image Generation Revolutionized!', 'desc': 'This paper introduces SANA-Sprint, a novel diffusion model designed for rapid text-to-image (T2I) generation. By leveraging a pre-trained foundation model and employing hybrid distillation techniques, SANA-Sprint significantly reduces the number of inference steps required, achieving high-quality image generation in just 1-4 steps. The model integrates a training-free approach and combines continuous-time consistency distillation with latent adversarial distillation to enhance generation fidelity. Additionally, SANA-Sprint incorporates ControlNet for real-time interactive image generation, setting a new standard in the speed-quality tradeoff for T2I models.'}, 'zh': {'title': 'SANA-Sprint：超快速文本到图像生成的革命性模型', 'desc': '本文介绍了SANA-Sprint，一种高效的扩散模型，用于超快速的文本到图像生成。SANA-Sprint基于预训练的基础模型，并通过混合蒸馏技术显著减少推理步骤，从20步减少到1-4步。该模型的三项关键创新包括无训练的流匹配模型转换、统一的步适应模型以及与ControlNet的集成，实现实时交互式图像生成。SANA-Sprint在速度与质量的权衡中建立了新的Pareto前沿，以1步生成达到7.59 FID和0.74 GenEval的最佳性能，且速度比FLUX-schnell快10倍。'}}}, {'id': 'https://huggingface.co/papers/2503.10615', 'title': 'R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization', 'url': 'https://huggingface.co/papers/2503.10615', 'abstract': 'Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.', 'score': 13, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '857ef8e4c110da7f', 'authors': ['Yi Yang', 'Xiaoxuan He', 'Hongkun Pan', 'Xiyan Jiang', 'Yan Deng', 'Xingtao Yang', 'Haoyu Lu', 'Dacheng Yin', 'Fengyun Rao', 'Minfeng Zhu', 'Bo Zhang', 'Wei Chen'], 'affiliations': ['Renmin University of China', 'WeChat Vision, Tencent Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10615.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Преодолевая барьер между зрением и рассуждением в ИИ', 'desc': 'R1-Onevision - это мультимодальная модель рассуждений, преодолевающая разрыв между визуальным восприятием и глубоким анализом. Модель использует конвейер кросс-модальных рассуждений, преобразующий изображения в формальные текстовые представления для точного языкового анализа. Авторы создали датасет R1-Onevision с подробными пошаговыми аннотациями мультимодальных рассуждений из разных областей. Модель R1-Onevision, обученная с помощью обучения с учителем и обучения с подкреплением, превосходит существующие модели на различных сложных мультимодальных тестах.'}, 'en': {'title': 'Bridging Visual and Textual Reasoning with R1-Onevision', 'desc': 'This paper presents R1-Onevision, a new model for multimodal reasoning that combines visual and textual information. It addresses the limitations of existing visual-language models by introducing a cross-modal reasoning pipeline that converts images into structured text representations. The authors also create the R1-Onevision dataset, which includes detailed annotations for multimodal reasoning tasks across various domains. Additionally, they establish R1-Onevision-Bench, a benchmark for evaluating multimodal reasoning abilities at different educational levels, demonstrating that their model outperforms others like GPT-4o and Qwen2.5-VL.'}, 'zh': {'title': 'R1-Onevision：视觉与语言的完美结合', 'desc': '本论文介绍了一种名为R1-Onevision的多模态推理模型，旨在解决视觉和文本信息整合的挑战。该模型通过跨模态推理管道，将图像转换为正式的文本表示，从而实现精确的基于语言的推理。我们还构建了R1-Onevision数据集，提供了详细的多模态推理注释，以支持不同领域的研究。实验结果表明，R1-Onevision在多个复杂的多模态推理基准测试中表现优异，超越了现有的先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.10589', 'title': 'Long Context Tuning for Video Generation', 'url': 'https://huggingface.co/papers/2503.10589', 'abstract': 'Recent advances in video generation can produce realistic, minute-long single-shot videos with scalable diffusion transformers. However, real-world narrative videos require multi-shot scenes with visual and dynamic consistency across shots. In this work, we introduce Long Context Tuning (LCT), a training paradigm that expands the context window of pre-trained single-shot video diffusion models to learn scene-level consistency directly from data. Our method expands full attention mechanisms from individual shots to encompass all shots within a scene, incorporating interleaved 3D position embedding and an asynchronous noise strategy, enabling both joint and auto-regressive shot generation without additional parameters. Models with bidirectional attention after LCT can further be fine-tuned with context-causal attention, facilitating auto-regressive generation with efficient KV-cache. Experiments demonstrate single-shot models after LCT can produce coherent multi-shot scenes and exhibit emerging capabilities, including compositional generation and interactive shot extension, paving the way for more practical visual content creation. See https://guoyww.github.io/projects/long-context-video/ for more details.', 'score': 13, 'issue_id': 2702, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '3850b6066f2b7160', 'authors': ['Yuwei Guo', 'Ceyuan Yang', 'Ziyan Yang', 'Zhibei Ma', 'Zhijie Lin', 'Zhenheng Yang', 'Dahua Lin', 'Lu Jiang'], 'affiliations': ['ByteDance', 'ByteDance Seed', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.10589.jpg', 'data': {'categories': ['#long_context', '#3d', '#video', '#diffusion', '#training'], 'emoji': '🎬', 'ru': {'title': 'Генерация согласованных многокадровых видео с помощью Long Context Tuning', 'desc': 'Статья представляет новый метод обучения под названием Long Context Tuning (LCT) для генерации многокадровых видео с сохранением согласованности сцен. LCT расширяет контекстное окно предобученных моделей диффузии для одиночных кадров, позволяя им учиться непосредственно из данных. Метод использует механизмы полного внимания и асинхронную стратегию шума для совместной и авторегрессивной генерации кадров. Эксперименты показывают, что модели после LCT способны создавать согласованные многокадровые сцены и демонстрируют новые возможности, такие как композиционная генерация и интерактивное расширение кадров.'}, 'en': {'title': 'Enhancing Video Generation with Long Context Tuning', 'desc': 'This paper presents Long Context Tuning (LCT), a new training approach for video generation models that enhances their ability to create multi-shot scenes with visual and dynamic consistency. By expanding the context window of pre-trained single-shot video diffusion models, LCT allows the model to learn from the entire scene rather than just individual shots. The method utilizes full attention mechanisms and incorporates advanced techniques like interleaved 3D position embedding and an asynchronous noise strategy. As a result, models trained with LCT can generate coherent multi-shot videos and demonstrate new capabilities such as compositional generation and interactive shot extension.'}, 'zh': {'title': '长上下文调优：提升视频生成的一致性与连贯性', 'desc': '本研究提出了一种新的训练范式，称为长上下文调优（LCT），旨在提高视频生成模型在多镜头场景中的一致性。通过扩展预训练单镜头视频扩散模型的上下文窗口，LCT能够直接从数据中学习场景级别的一致性。该方法利用全注意力机制，将注意力从单个镜头扩展到整个场景，结合了3D位置嵌入和异步噪声策略，实现了无额外参数的联合和自回归镜头生成。实验表明，经过LCT处理的单镜头模型能够生成连贯的多镜头场景，并展现出组合生成和交互式镜头扩展等新能力。'}}}, {'id': 'https://huggingface.co/papers/2503.10637', 'title': 'Distilling Diversity and Control in Diffusion Models', 'url': 'https://huggingface.co/papers/2503.10637', 'abstract': 'Distilled diffusion models suffer from a critical limitation: reduced sample diversity compared to their base counterparts. In this work, we uncover that despite this diversity loss, distilled models retain the fundamental concept representations of base models. We demonstrate control distillation - where control mechanisms like Concept Sliders and LoRAs trained on base models can be seamlessly transferred to distilled models and vice-versa, effectively distilling control without any retraining. This preservation of representational structure prompted our investigation into the mechanisms of diversity collapse during distillation. To understand how distillation affects diversity, we introduce Diffusion Target (DT) Visualization, an analysis and debugging tool that reveals how models predict final outputs at intermediate steps. Through DT-Visualization, we identify generation artifacts, inconsistencies, and demonstrate that initial diffusion timesteps disproportionately determine output diversity, while later steps primarily refine details. Based on these insights, we introduce diversity distillation - a hybrid inference approach that strategically employs the base model for only the first critical timestep before transitioning to the efficient distilled model. Our experiments demonstrate that this simple modification not only restores the diversity capabilities from base to distilled models but surprisingly exceeds it, while maintaining nearly the computational efficiency of distilled inference, all without requiring additional training or model modifications. Our code and data are available at https://distillation.baulab.info', 'score': 12, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '5313c06d699a1a7f', 'authors': ['Rohit Gandikota', 'David Bau'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.10637.jpg', 'data': {'categories': ['#dataset', '#inference', '#training', '#optimization', '#diffusion', '#data'], 'emoji': '🔬', 'ru': {'title': 'Восстановление разнообразия в дистиллированных диффузионных моделях', 'desc': 'Исследование посвящено проблеме уменьшения разнообразия сэмплов в дистиллированных диффузионных моделях по сравнению с базовыми. Авторы обнаружили, что дистиллированные модели сохраняют фундаментальные концептуальные представления базовых моделей, что позволяет переносить механизмы контроля между ними без переобучения. Для анализа причин потери разнообразия была разработана техника визуализации Diffusion Target, которая показала, что начальные шаги диффузии непропорционально влияют на разнообразие выходных данных. На основе этих наблюдений предложен метод diversity distillation, использующий базовую модель только для первого критического шага, что восстанавливает и даже превосходит разнообразие базовой модели при сохранении эффективности дистиллированного вывода.'}, 'en': {'title': 'Restoring Diversity in Distilled Diffusion Models', 'desc': 'This paper addresses the issue of reduced sample diversity in distilled diffusion models compared to their original versions. The authors reveal that distilled models still maintain the essential concept representations of their base models. They introduce a method called control distillation, which allows for the transfer of control mechanisms between models without retraining. Additionally, they present Diffusion Target (DT) Visualization to analyze how diversity is affected during the distillation process, leading to a new approach called diversity distillation that enhances output diversity while keeping computational efficiency intact.'}, 'zh': {'title': '恢复蒸馏模型的多样性', 'desc': '本文探讨了蒸馏扩散模型的一个重要限制，即与基础模型相比，样本多样性降低。尽管存在这种多样性损失，蒸馏模型仍然保留了基础模型的基本概念表示。我们提出了控制蒸馏的方法，可以将控制机制无缝转移到蒸馏模型，并且不需要重新训练。通过引入扩散目标可视化工具，我们分析了蒸馏对多样性的影响，并提出了一种新的混合推理方法，能够在保持计算效率的同时恢复多样性。'}}}, {'id': 'https://huggingface.co/papers/2503.10357', 'title': 'Do I look like a `cat.n.01` to you? A Taxonomy Image Generation\n  Benchmark', 'url': 'https://huggingface.co/papers/2503.10357', 'abstract': "This paper explores the feasibility of using text-to-image models in a zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored. To address this, we propose a comprehensive benchmark for Taxonomy Image Generation that assesses models' abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources.", 'score': 11, 'issue_id': 2705, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'dad8c63e36f6d8c2', 'authors': ['Viktor Moskvoretskii', 'Alina Lobanova', 'Ekaterina Neminova', 'Chris Biemann', 'Alexander Panchenko', 'Irina Nikishina'], 'affiliations': ['AIRI', 'HSE University', 'Skoltech', 'University of Hamburg'], 'pdf_title_img': 'assets/pdf/title_img/2503.10357.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark'], 'emoji': '🌳', 'ru': {'title': 'Визуализация таксономий: новый рубеж в генерации изображений', 'desc': 'Статья исследует возможность использования моделей преобразования текста в изображение для генерации визуального контента для таксономических концепций. Авторы предлагают комплексный бенчмарк для оценки способности моделей понимать таксономические концепции и генерировать релевантные, качественные изображения. Исследование включает оценку 12 моделей по 9 новым метрикам, связанным с таксономией, а также обратную связь от людей и GPT-4. Результаты показывают, что модели Playground-v2 и FLUX превосходят другие в этой задаче, демонстрируя потенциал для автоматизации курирования структурированных данных.'}, 'en': {'title': 'Unlocking Visual Understanding in Taxonomy with Zero-Shot Image Generation', 'desc': 'This paper investigates how well text-to-image models can create images based on taxonomy concepts without prior training on those specific concepts, known as a zero-shot setup. It introduces a new benchmark for evaluating these models, focusing on their ability to generate relevant and high-quality images that align with taxonomy terms. The study employs innovative metrics and human feedback to assess the performance of 12 different models, revealing that their effectiveness varies significantly from traditional text-to-image tasks. The results suggest that certain models, like Playground-v2 and FLUX, excel in this context, indicating a promising avenue for automating the enhancement of structured data resources.'}, 'zh': {'title': '探索零样本下的分类图像生成潜力', 'desc': '本文探讨了在零样本设置下使用文本到图像模型生成分类概念图像的可行性。虽然基于文本的方法在分类丰富方面已经相当成熟，但视觉维度的潜力尚未被充分挖掘。为此，我们提出了一个全面的分类图像生成基准，评估模型理解分类概念和生成相关高质量图像的能力。实验结果表明，模型的排名与标准的文本到图像任务有显著不同，强调了自动化结构化数据资源整理的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.09799', 'title': 'Communication-Efficient Language Model Training Scales Reliably and\n  Robustly: Scaling Laws for DiLoCo', 'url': 'https://huggingface.co/papers/2503.09799', 'abstract': "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.", 'score': 10, 'issue_id': 2714, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '78f500ddd43c6e65', 'authors': ['Zachary Charles', 'Gabriel Teston', 'Lucio Dery', 'Keith Rush', 'Nova Fallen', 'Zachary Garrett', 'Arthur Szlam', 'Arthur Douillard'], 'affiliations': ['Google DeepMind', 'Google Research', 'Google Search'], 'pdf_title_img': 'assets/pdf/title_img/2503.09799.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '📈', 'ru': {'title': 'DiLoCo: Эффективное масштабирование обучения больших языковых моделей', 'desc': 'Эта статья исследует масштабируемость метода DiLoCo при обучении больших языковых моделей (LLM) с фиксированным бюджетом вычислений. Авторы анализируют, как алгоритмические факторы, включая количество реплик модели, гиперпараметры и бюджет токенов, влияют на обучение, и как это можно предсказать с помощью законов масштабирования. Результаты показывают, что DiLoCo масштабируется предсказуемо и надежно с увеличением размера модели, превосходя параллельное обучение по данным даже для небольших моделей. Исследование демонстрирует более широкий спектр преимуществ DiLoCo, включая увеличение оптимальных размеров батча и улучшение обобщения на downstream задачах.'}, 'en': {'title': 'DiLoCo: Scaling Up Without the Sync Slowdown!', 'desc': 'This paper investigates the performance of the DiLoCo approach for training large language models (LLMs) while minimizing synchronization delays. It analyzes how various factors, such as the number of model replicas and hyperparameters, influence the training process under a fixed compute budget. The authors demonstrate that DiLoCo not only scales effectively with model size but also outperforms traditional data-parallel training methods, even for smaller models. Their findings highlight the advantages of DiLoCo, including better batch sizes and enhanced generalization capabilities as model size increases.'}, 'zh': {'title': 'DiLoCo：超越数据并行的训练新方法', 'desc': '随着机器学习模型规模的扩大，数据并行方法中频繁的同步需求导致显著的性能下降，成为进一步扩展的关键挑战。本文提出了一种名为DiLoCo的方法，它在不影响模型质量的情况下，放宽了同步需求。我们研究了在固定计算预算下，DiLoCo在训练大型语言模型时的扩展规律，分析了模型副本数量、超参数和令牌预算等算法因素对训练的影响。研究结果表明，DiLoCo在模型规模上具有可预测和稳健的扩展性，经过良好调优后，其扩展性能优于数据并行训练，甚至在小规模模型中也能超越数据并行训练。'}}}, {'id': 'https://huggingface.co/papers/2503.10391', 'title': 'CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance', 'url': 'https://huggingface.co/papers/2503.10391', 'abstract': 'Video generation has witnessed remarkable progress with the advent of deep generative models, particularly diffusion models. While existing methods excel in generating high-quality videos from text prompts or single images, personalized multi-subject video generation remains a largely unexplored challenge. This task involves synthesizing videos that incorporate multiple distinct subjects, each defined by separate reference images, while ensuring temporal and spatial consistency. Current approaches primarily rely on mapping subject images to keywords in text prompts, which introduces ambiguity and limits their ability to model subject relationships effectively. In this paper, we propose CINEMA, a novel framework for coherent multi-subject video generation by leveraging Multimodal Large Language Model (MLLM). Our approach eliminates the need for explicit correspondences between subject images and text entities, mitigating ambiguity and reducing annotation effort. By leveraging MLLM to interpret subject relationships, our method facilitates scalability, enabling the use of large and diverse datasets for training. Furthermore, our framework can be conditioned on varying numbers of subjects, offering greater flexibility in personalized content creation. Through extensive evaluations, we demonstrate that our approach significantly improves subject consistency, and overall video coherence, paving the way for advanced applications in storytelling, interactive media, and personalized video generation.', 'score': 9, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'd7f2c49b29951ace', 'authors': ['Yufan Deng', 'Xun Guo', 'Yizhi Wang', 'Jacob Zhiyuan Fang', 'Angtian Wang', 'Shenghai Yuan', 'Yiding Yang', 'Bo Liu', 'Haibin Huang', 'Chongyang Ma'], 'affiliations': ['ByteDance Intelligent Creation', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10391.jpg', 'data': {'categories': ['#synthetic', '#multimodal', '#story_generation', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'CINEMA: персонализированная генерация видео с множеством субъектов при помощи MLLM', 'desc': 'Статья представляет CINEMA - новую систему для генерации видео с несколькими персонажами, использующую мультимодальную большую языковую модель (MLLM). CINEMA устраняет необходимость в явном соответствии между изображениями субъектов и текстовыми сущностями, что уменьшает неоднозначность и упрощает аннотирование. Система позволяет масштабировать процесс, используя большие и разнообразные наборы данных для обучения. CINEMA демонстрирует значительное улучшение согласованности персонажей и общей связности видео по сравнению с существующими методами.'}, 'en': {'title': 'CINEMA: Coherent Multi-Subject Video Generation Made Easy', 'desc': 'This paper introduces CINEMA, a new framework for generating videos that feature multiple distinct subjects from separate reference images. Unlike traditional methods that rely on mapping images to text prompts, CINEMA uses a Multimodal Large Language Model (MLLM) to understand and interpret relationships between subjects, which reduces ambiguity. The framework allows for flexible conditioning on different numbers of subjects, making it easier to create personalized content. Extensive evaluations show that CINEMA enhances subject consistency and overall video coherence, opening up new possibilities for applications in storytelling and interactive media.'}, 'zh': {'title': '个性化多主体视频生成的新突破', 'desc': '视频生成在深度生成模型，特别是扩散模型的推动下取得了显著进展。现有方法在从文本提示或单张图像生成高质量视频方面表现出色，但个性化的多主体视频生成仍然是一个未被充分探索的挑战。我们提出了CINEMA框架，通过利用多模态大语言模型（MLLM），实现了一致的多主体视频生成，消除了主体图像与文本实体之间的明确对应关系，从而减少了歧义和标注工作。我们的框架能够根据不同数量的主体进行条件生成，提供了更大的个性化内容创作灵活性。'}}}, {'id': 'https://huggingface.co/papers/2503.10614', 'title': 'ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style\n  Transfer', 'url': 'https://huggingface.co/papers/2503.10614', 'abstract': 'Style transfer involves transferring the style from a reference image to the content of a target image. Recent advancements in LoRA-based (Low-Rank Adaptation) methods have shown promise in effectively capturing the style of a single image. However, these approaches still face significant challenges such as content inconsistency, style misalignment, and content leakage. In this paper, we comprehensively analyze the limitations of the standard diffusion parameterization, which learns to predict noise, in the context of style transfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method that enhances both content and style consistency by optimizing the LoRA weights to predict the original image rather than noise. We also propose a two-step training strategy that decouples the learning of content and style from the reference image. To effectively capture both the global structure and local details of the content image, we introduce a stepwise loss transition strategy. Additionally, we present an inference guidance method that enables continuous control over content and style strengths during inference. Through both qualitative and quantitative evaluations, our method demonstrates significant improvements in content and style consistency while effectively reducing content leakage.', 'score': 7, 'issue_id': 2712, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '12aacf8b56fdac9c', 'authors': ['Bolin Chen', 'Baoquan Zhao', 'Haoran Xie', 'Yi Cai', 'Qing Li', 'Xudong Mao'], 'affiliations': ['Lingnan University', 'South China University of Technology', 'Sun Yat-sen University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10614.jpg', 'data': {'categories': ['#synthetic', '#cv', '#inference', '#training', '#diffusion', '#leakage', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'ConsisLoRA: Новый уровень согласованности в переносе стиля изображений', 'desc': 'Эта статья представляет новый метод переноса стиля изображений под названием ConsisLoRA. Метод основан на оптимизации весов LoRA для предсказания оригинального изображения вместо шума, что улучшает согласованность содержания и стиля. Авторы предлагают двухэтапную стратегию обучения и пошаговый переход функции потерь для эффективного захвата глобальной структуры и локальных деталей. Также представлен метод управления силой содержания и стиля при генерации изображений.'}, 'en': {'title': 'Enhancing Style Transfer with ConsisLoRA for Better Consistency', 'desc': 'This paper focuses on improving style transfer techniques, which blend the style of one image with the content of another. It identifies key challenges in existing methods, such as inconsistencies in content and style alignment. The authors introduce ConsisLoRA, a new approach that optimizes LoRA weights to enhance both content and style consistency by predicting the original image instead of noise. They also propose a two-step training strategy and a stepwise loss transition to better capture the details of the images, resulting in improved performance in style transfer tasks.'}, 'zh': {'title': '提升风格迁移的一致性与控制力', 'desc': '风格迁移是将参考图像的风格转移到目标图像内容上的技术。最近，基于LoRA（低秩适应）的方法在有效捕捉单一图像风格方面显示出良好前景，但仍面临内容不一致、风格错位和内容泄漏等挑战。本文分析了标准扩散参数化在风格迁移中的局限性，并提出了ConsisLoRA方法，通过优化LoRA权重来增强内容和风格的一致性。我们还提出了两步训练策略和逐步损失过渡策略，以有效捕捉内容图像的全局结构和局部细节。'}}}, {'id': 'https://huggingface.co/papers/2503.10630', 'title': 'UniGoal: Towards Universal Zero-shot Goal-oriented Navigation', 'url': 'https://huggingface.co/papers/2503.10630', 'abstract': 'In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.', 'score': 6, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '9e7667a37c699f4c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#reasoning', '#long_context', '#games', '#graphs', '#benchmark', '#agents', '#rl'], 'emoji': '🧭', 'ru': {'title': 'Универсальная навигация с нулевым обучением на основе графов и языковых моделей', 'desc': 'Статья представляет универсальную систему для навигации с нулевым обучением к различным целям. Авторы предлагают единое графовое представление для унификации разных типов целей и наблюдений агента. Используя большие языковые модели для рассуждений на основе графов, система проводит сопоставление графа сцены и графа цели. Предложенный метод UniGoal достигает лучших результатов в задачах навигации с нулевым обучением по сравнению с существующими подходами.'}, 'en': {'title': 'Navigating Goals Universally with Graphs!', 'desc': 'This paper introduces a new framework for universal zero-shot goal-oriented navigation, which allows an agent to navigate towards various goals without prior training on specific tasks. Unlike existing methods that rely heavily on large language models (LLMs) tailored for individual tasks, this approach uses a uniform graph representation to integrate different types of goals, such as object categories and text descriptions. The agent maintains an online scene graph that captures the environment, enabling it to perform graph-based reasoning and match its observations with the goals. The proposed method demonstrates superior performance in navigation tasks, surpassing both task-specific and supervised approaches, by effectively managing goal exploration and verification through innovative strategies.'}, 'zh': {'title': '通用零-shot导航的创新框架', 'desc': '本文提出了一种通用的零-shot目标导向导航框架。现有的零-shot方法通常依赖于大型语言模型（LLM）进行特定任务的推理，导致在不同目标之间的泛化能力不足。我们提出了一种统一的图表示方法，将不同的目标（如物体类别、实例图像和文本描述）整合在一起，并将代理的观察结果转换为在线维护的场景图。通过这种一致的场景和目标表示，我们能够利用LLM进行基于图的推理，并在多个基准测试中展示了我们的UniGoal在零-shot导航任务上的优越性能。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2503.10568', 'title': 'Autoregressive Image Generation with Randomized Parallel Decoding', 'url': 'https://huggingface.co/papers/2503.10568', 'abstract': 'We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over a 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at a similar scale.', 'score': 6, 'issue_id': 2701, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '44a1e551455870ac', 'authors': ['Haopeng Li', 'Jinyue Yang', 'Guoqi Li', 'Huan Wang'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10568.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#cv', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'ARPG: Революция в генерации изображений с случайным порядком и параллельным выводом', 'desc': 'ARPG - это новая визуальная авторегрессионная модель, позволяющая осуществлять рандомизированную параллельную генерацию изображений. Она решает проблемы традиционных подходов с фиксированным порядком генерации токенов, предлагая механизм управляемого декодирования, который разделяет позиционное руководство и представление контента. ARPG обучается и генерирует в полностью случайном порядке, что позволяет ей легко адаптироваться к задачам без дополнительного обучения, таким как заполнение и расширение изображений. Модель демонстрирует высокую производительность и эффективность по сравнению с аналогами, достигая FID 1.94 на ImageNet-1K 256 всего за 64 шага сэмплирования.'}, 'en': {'title': 'Revolutionizing Image Generation with Randomized Parallel Processing', 'desc': 'The paper presents ARPG, a new visual autoregressive model designed for efficient and flexible image generation. Unlike traditional methods that generate tokens in a fixed order, ARPG allows for randomized parallel generation, improving inference speed and enabling zero-shot generalization. It introduces a guided decoding framework that separates positional guidance from content representation, enhancing the causal attention mechanism. As a result, ARPG excels in tasks like image inpainting and outpainting, achieving significant performance improvements on benchmarks while using less memory.'}, 'zh': {'title': 'ARPG：实现随机并行生成的自回归模型', 'desc': '我们提出了一种新颖的视觉自回归模型ARPG，能够实现随机并行生成，解决了传统光栅顺序方法在推理效率和零-shot泛化方面的固有限制。我们的关键见解是，有效的随机顺序建模需要明确的指导来确定下一个预测标记的位置。为此，我们提出了一种新颖的引导解码框架，将位置指导与内容表示分开编码，分别作为查询和键值对。通过将这种指导直接融入因果注意力机制，我们的方法实现了完全随机顺序的训练和生成，消除了对双向注意力的需求。'}}}, {'id': 'https://huggingface.co/papers/2503.10365', 'title': 'Piece it Together: Part-Based Concepting with IP-Priors', 'url': 'https://huggingface.co/papers/2503.10365', 'abstract': 'Advanced generative models excel at synthesizing images but often rely on text-based conditioning. Visual designers, however, often work beyond language, directly drawing inspiration from existing visual elements. In many cases, these elements represent only fragments of a potential concept-such as an uniquely structured wing, or a specific hairstyle-serving as inspiration for the artist to explore how they can come together creatively into a coherent whole. Recognizing this need, we introduce a generative framework that seamlessly integrates a partial set of user-provided visual components into a coherent composition while simultaneously sampling the missing parts needed to generate a plausible and complete concept. Our approach builds on a strong and underexplored representation space, extracted from IP-Adapter+, on which we train IP-Prior, a lightweight flow-matching model that synthesizes coherent compositions based on domain-specific priors, enabling diverse and context-aware generations. Additionally, we present a LoRA-based fine-tuning strategy that significantly improves prompt adherence in IP-Adapter+ for a given task, addressing its common trade-off between reconstruction quality and prompt adherence.', 'score': 6, 'issue_id': 2713, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '80bcd7657c63561e', 'authors': ['Elad Richardson', 'Kfir Goldberg', 'Yuval Alaluf', 'Daniel Cohen-Or'], 'affiliations': ['Bria AI', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10365.jpg', 'data': {'categories': ['#training', '#cv', '#multimodal', '#synthetic'], 'emoji': '🎨', 'ru': {'title': 'Генерация изображений на основе визуальных компонентов: новый подход к творческому синтезу', 'desc': 'Статья представляет новый генеративный фреймворк для создания изображений, который интегрирует визуальные компоненты, предоставленные пользователем, в целостную композицию. Авторы разработали модель IP-Prior, основанную на пространстве представлений IP-Adapter+, которая синтезирует согласованные композиции на основе доменно-специфичных приоров. Предложенный подход позволяет генерировать разнообразные и контекстно-зависимые изображения. Кроме того, авторы представили стратегию тонкой настройки на основе LoRA, которая значительно улучшает соответствие промпту в IP-Adapter+.'}, 'en': {'title': 'Bridging Visual Inspiration with Generative Design', 'desc': "This paper presents a new generative framework that allows visual designers to create coherent compositions by integrating partial visual elements provided by users. Unlike traditional models that rely heavily on text descriptions, this approach focuses on visual components, enabling artists to explore creative combinations. The framework utilizes a lightweight flow-matching model called IP-Prior, which is trained on a specialized representation space to ensure context-aware and diverse outputs. Additionally, the authors introduce a fine-tuning strategy that enhances the model's ability to adhere to user prompts while maintaining high reconstruction quality."}, 'zh': {'title': '无缝整合视觉元素，创造连贯概念', 'desc': '本论文介绍了一种生成框架，能够将用户提供的部分视觉元素无缝整合成一个连贯的整体，同时生成所需的缺失部分，以创造出合理且完整的概念。该方法基于一个强大且未被充分探索的表示空间，利用IP-Adapter+提取的特征，训练出轻量级的流匹配模型IP-Prior。此模型能够基于特定领域的先验知识合成连贯的作品，实现多样化和上下文感知的生成。此外，我们还提出了一种基于LoRA的微调策略，显著提高了IP-Adapter+在特定任务中的提示遵循性，解决了重建质量与提示遵循性之间的常见权衡问题。'}}}, {'id': 'https://huggingface.co/papers/2503.09905', 'title': "Quantization for OpenAI's Whisper Models: A Comparative Analysis", 'url': 'https://huggingface.co/papers/2503.09905', 'abstract': 'Automated speech recognition (ASR) models have gained prominence for applications such as captioning, speech translation, and live transcription. This paper studies Whisper and two model variants: one optimized for live speech streaming and another for offline transcription. Notably, these models have been found to generate hallucinated content, reducing transcription reliability. Furthermore, larger model variants exhibit increased latency and pose challenges for deployment on resource-constrained devices. This study analyzes the similarities and differences between three Whisper models, qualitatively examining their distinct capabilities. Next, this study quantifies the impact of model quantization on latency and evaluates its viability for edge deployment. Using the open source LibriSpeech dataset, this paper evaluates the word error rate (WER) along with latency analysis of whispercpp using 3 quantization methods (INT4, INT5, INT8). Results show that quantization reduces latency by 19\\% and model size by 45\\%, while preserving transcription accuracy. These findings provide insights into the optimal use cases of different Whisper models and edge device deployment possibilities. All code, datasets, and implementation details are available in a public GitHub repository: https://github.com/allisonandreyev/WhisperQuantization.git', 'score': 6, 'issue_id': 2700, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '8d7b17a97a4cbb6e', 'authors': ['Allison Andreyev'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.09905.jpg', 'data': {'categories': ['#inference', '#audio', '#open_source', '#optimization', '#hallucinations', '#dataset'], 'emoji': '🎙️', 'ru': {'title': 'Оптимизация моделей Whisper для эффективного распознавания речи на периферийных устройствах', 'desc': 'Это исследование анализирует модели автоматического распознавания речи Whisper и их варианты для потоковой и офлайн-транскрипции. Авторы изучают проблему галлюцинаций в выводе моделей и трудности развертывания больших моделей на устройствах с ограниченными ресурсами. Проводится оценка влияния квантования модели на задержку и точность транскрипции с использованием набора данных LibriSpeech. Результаты показывают, что квантование уменьшает задержку на 19% и размер модели на 45%, сохраняя при этом точность транскрипции.'}, 'en': {'title': 'Optimizing Whisper Models for Efficient Speech Recognition', 'desc': 'This paper investigates the performance of Whisper, an automated speech recognition (ASR) model, along with its two variants tailored for live streaming and offline transcription. It highlights the issue of hallucinated content in the transcriptions, which can compromise reliability, especially in larger models that also face latency challenges. The study further explores the effects of model quantization on latency and size, demonstrating a significant reduction in both while maintaining transcription accuracy. By analyzing the word error rate (WER) using the LibriSpeech dataset, the research provides valuable insights for deploying Whisper models on resource-constrained devices.'}, 'zh': {'title': '优化Whisper模型，提升语音识别效率', 'desc': '这篇论文研究了自动语音识别（ASR）模型Whisper及其两个变体，一个针对实时语音流优化，另一个用于离线转录。研究发现，这些模型可能会生成虚假内容，从而降低转录的可靠性。此外，较大的模型变体在延迟方面表现较差，给资源受限的设备部署带来了挑战。通过对比分析，论文量化了模型量化对延迟的影响，并评估了其在边缘设备上的可行性。'}}}, {'id': 'https://huggingface.co/papers/2503.09837', 'title': 'On the Limitations of Vision-Language Models in Understanding Image\n  Transforms', 'url': 'https://huggingface.co/papers/2503.09837', 'abstract': 'Vision Language Models (VLMs) have demonstrated significant potential in various downstream tasks, including Image/Video Generation, Visual Question Answering, Multimodal Chatbots, and Video Understanding. However, these models often struggle with basic image transformations. This paper investigates the image-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by Google. Our findings reveal that these models lack comprehension of multiple image-level augmentations. To facilitate this study, we created an augmented version of the Flickr8k dataset, pairing each image with a detailed description of the applied transformation. We further explore how this deficiency impacts downstream tasks, particularly in image editing, and evaluate the performance of state-of-the-art Image2Image models on simple transformations.', 'score': 6, 'issue_id': 2709, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'cc8e8180cfeef80d', 'authors': ['Ahmad Mustafa Anis', 'Hasnain Ali', 'Saquib Sarfraz'], 'affiliations': ['Arbisoft', 'Cohere for AI Community', 'Karlsruhe Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.09837.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#games', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Ограниченное понимание преобразований изображений в современных VLM', 'desc': 'Это исследование посвящено анализу способности моделей визуального языка (VLM) понимать базовые преобразования изображений. Авторы обнаружили, что популярные модели CLIP и SigLIP не справляются с распознаванием многих аугментаций на уровне изображений. Для проведения исследования был создан расширенный набор данных на основе Flickr8k с подробными описаниями применённых трансформаций. Также изучалось влияние этого недостатка на задачи редактирования изображений и оценивалась эффективность современных моделей Image2Image при выполнении простых преобразований.'}, 'en': {'title': 'Unlocking Image Transformations in Vision Language Models', 'desc': 'This paper examines the limitations of Vision Language Models (VLMs) like CLIP and SigLIP in understanding basic image transformations. Despite their success in tasks such as image generation and visual question answering, these models struggle with comprehending various image-level augmentations. The authors created an augmented Flickr8k dataset to analyze how well these models recognize and describe transformations applied to images. The study also assesses the impact of these deficiencies on downstream tasks, particularly in image editing, and evaluates the performance of advanced Image2Image models in handling simple transformations.'}, 'zh': {'title': '提升视觉语言模型的图像理解能力', 'desc': '本文研究了视觉语言模型（VLMs）在图像理解方面的不足，特别是OpenAI的CLIP和Google的SigLIP。尽管这些模型在多种下游任务中表现出色，但它们在基本图像变换方面存在困难。我们创建了一个增强版的Flickr8k数据集，为每张图像配上详细的变换描述，以便进行研究。研究结果表明，这种理解缺陷会影响下游任务的表现，尤其是在图像编辑方面。'}}}, {'id': 'https://huggingface.co/papers/2503.09046', 'title': 'Discovering Influential Neuron Path in Vision Transformers', 'url': 'https://huggingface.co/papers/2503.09046', 'abstract': "Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there's been a notable gap in considering layer-level information and the holistic path of information flow across layers. In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly. We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome. And we further provide a layer-progressive neuron locating approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model. Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions. Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. We further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning. The project website including implementation code is available at https://foundation-model-research.github.io/NeuronPath/.", 'score': 6, 'issue_id': 2712, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'c4d7c64b526c6486', 'authors': ['Yifan Wang', 'Yifei Liu', 'Yingdong Shi', 'Changming Li', 'Anqi Pang', 'Sibei Yang', 'Jingyi Yu', 'Kan Ren'], 'affiliations': ['ShanghaiTech University', 'Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2503.09046.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#cv', '#training', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Раскрытие тайн Vision Transformer: исследование влиятельных нейронных путей', 'desc': 'Статья исследует значимые пути нейронов в моделях Vision Transformer, предлагая метод оценки влияния групп нейронов на результат модели. Авторы разработали послойный подход для выбора наиболее влиятельных нейронов, позволяющий обнаружить ключевой путь прохождения информации от входа к выходу. Эксперименты показали превосходство предложенного метода над существующими решениями в поиске наиболее значимых нейронных путей. Анализ выявленных путей нейронов продемонстрировал специфические механизмы обработки визуальной информации в Vision Transformer и потенциал для практических приложений, таких как pruning моделей.'}, 'en': {'title': 'Unveiling the Neuron Paths in Vision Transformers', 'desc': "This paper explores how to better understand Vision Transformer models by focusing on the paths of influential neurons that contribute significantly to model predictions. It introduces a joint influence measure to evaluate the impact of neuron sets on the output and proposes a method to identify the most important neurons layer by layer. The findings reveal that these neuron paths not only clarify the model's decision-making process but also maintain the model's performance on tasks like image classification. This research could lead to practical applications such as model pruning, enhancing the efficiency of Vision Transformers in real-world scenarios."}, 'zh': {'title': '揭示视觉变换器中的关键神经元路径', 'desc': '本文研究了视觉变换器模型中影响力神经元路径的重要性，这些路径从模型输入到输出，对模型推理有显著影响。我们提出了一种联合影响度量方法，评估一组神经元对模型结果的贡献，并提供了一种逐层神经元定位方法，以高效选择每层中最具影响力的神经元。实验结果表明，我们的方法在寻找信息流动的最具影响力神经元路径方面优于现有基线解决方案。此外，这些神经元路径揭示了视觉变换器在处理同一图像类别的视觉信息时的特定内部工作机制。'}}}, {'id': 'https://huggingface.co/papers/2503.10242', 'title': 'MinorBench: A hand-built benchmark for content-based risks for children', 'url': 'https://huggingface.co/papers/2503.10242', 'abstract': "Large Language Models (LLMs) are rapidly entering children's lives - through parent-driven adoption, schools, and peer networks - yet current AI ethics and safety research do not adequately address content-related risks specific to minors. In this paper, we highlight these gaps with a real-world case study of an LLM-based chatbot deployed in a middle school setting, revealing how students used and sometimes misused the system. Building on these findings, we propose a new taxonomy of content-based risks for minors and introduce MinorBench, an open-source benchmark designed to evaluate LLMs on their ability to refuse unsafe or inappropriate queries from children. We evaluate six prominent LLMs under different system prompts, demonstrating substantial variability in their child-safety compliance. Our results inform practical steps for more robust, child-focused safety mechanisms and underscore the urgency of tailoring AI systems to safeguard young users.", 'score': 4, 'issue_id': 2712, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '28234b0e69863cbe', 'authors': ['Shaun Khoo', 'Gabriel Chua', 'Rachel Shong'], 'affiliations': ['Government Technology Agency Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.10242.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#benchmark', '#ethics'], 'emoji': '🧒', 'ru': {'title': 'Защита детей от рисков больших языковых моделей', 'desc': 'Данная статья исследует риски использования больших языковых моделей (LLM) детьми. Авторы проводят анализ реального случая применения чат-бота на основе LLM в средней школе. На основе полученных результатов предлагается новая таксономия контентных рисков для несовершеннолетних и представляется MinorBench - открытый бенчмарк для оценки способности LLM отклонять небезопасные или неуместные запросы от детей. Исследование подчеркивает необходимость разработки более надежных механизмов безопасности, ориентированных на детей.'}, 'en': {'title': 'Ensuring Child Safety in AI: A Call for Better LLM Standards', 'desc': 'This paper discusses the increasing presence of Large Language Models (LLMs) in the lives of children and the associated risks that are not adequately addressed by current AI ethics. It presents a case study of a chatbot used in a middle school, highlighting both the positive and negative ways students interacted with the system. The authors propose a new classification system for content-related risks specific to minors and introduce MinorBench, a benchmark to assess LLMs on their ability to handle inappropriate queries from children. The evaluation of six LLMs shows significant differences in their compliance with child safety standards, emphasizing the need for improved safety measures tailored for young users.'}, 'zh': {'title': '保护儿童，安全使用大型语言模型', 'desc': '本文探讨了大型语言模型（LLMs）在儿童生活中的应用，尤其是在学校环境中的使用情况。通过案例研究，我们发现学生在使用聊天机器人时存在内容相关的风险，尤其是对未成年人而言。为了解决这些问题，我们提出了一种新的未成年人内容风险分类法，并引入了MinorBench，一个开源基准，用于评估LLMs拒绝不安全或不当查询的能力。我们的研究结果显示，不同的LLMs在儿童安全合规性方面存在显著差异，强调了为保护年轻用户而定制AI系统的紧迫性。'}}}, {'id': 'https://huggingface.co/papers/2503.10072', 'title': '"Silent Is Not Actually Silent": An Investigation of Toxicity on Bug\n  Report Discussion', 'url': 'https://huggingface.co/papers/2503.10072', 'abstract': 'Toxicity in bug report discussions poses significant challenges to the collaborative dynamics of open-source software development. Bug reports are crucial for identifying and resolving defects, yet their inherently problem-focused nature and emotionally charged context make them susceptible to toxic interactions. This study explores toxicity in GitHub bug reports through a qualitative analysis of 203 bug threads, including 81 toxic ones. Our findings reveal that toxicity frequently arises from misaligned perceptions of bug severity and priority, unresolved frustrations with tools, and lapses in professional communication. These toxic interactions not only derail productive discussions but also reduce the likelihood of actionable outcomes, such as linking issues with pull requests. Our preliminary findings offer actionable recommendations to improve bug resolution by mitigating toxicity.', 'score': 4, 'issue_id': 2699, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'a91b6bc8232847a3', 'authors': ['Mia Mohammad Imran', 'Jaydeb Sarker'], 'affiliations': ['Missouri University of Science and Technology, Rolla, Missouri, USA', 'University of Nebraska Omaha, Omaha, Nebraska, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10072.jpg', 'data': {'categories': ['#ethics', '#open_source'], 'emoji': '🐛', 'ru': {'title': 'Токсичность в отчетах об ошибках: препятствие для продуктивного разрешения проблем', 'desc': 'Исследование посвящено проблеме токсичности в обсуждениях отчетов об ошибках в проектах с открытым исходным кодом. Авторы провели качественный анализ 203 веток обсуждения ошибок на GitHub, включая 81 токсичную. Результаты показывают, что токсичность часто возникает из-за расхождений в восприятии серьезности и приоритетности ошибок, нерешенных проблем с инструментами и нарушений профессиональной коммуникации. Токсичные взаимодействия не только нарушают продуктивные обсуждения, но и снижают вероятность достижения конкретных результатов, таких как привязка проблем к запросам на слияние.'}, 'en': {'title': 'Mitigating Toxicity for Better Bug Resolution in Open Source', 'desc': 'This paper investigates the issue of toxicity in bug report discussions within open-source software development on GitHub. It analyzes 203 bug threads, identifying 81 as toxic, and highlights that toxicity often stems from differing views on bug severity, frustrations with tools, and poor communication. The study emphasizes that such toxic interactions hinder productive collaboration and decrease the chances of resolving issues effectively. The authors provide recommendations aimed at reducing toxicity to enhance the bug resolution process.'}, 'zh': {'title': '减少毒性，提升开源协作效率', 'desc': '本研究探讨了开源软件开发中，GitHub bug 报告讨论中的毒性问题。通过对203个bug线程的定性分析，发现81个线程存在毒性互动。研究表明，毒性往往源于对bug严重性和优先级的误解、对工具的不满以及专业沟通的缺失。这些毒性互动不仅影响了有效讨论，还降低了将问题与拉取请求关联的可能性，因此提出了减少毒性以改善bug解决的建议。'}}}, {'id': 'https://huggingface.co/papers/2503.10636', 'title': 'The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation', 'url': 'https://huggingface.co/papers/2503.10636', 'abstract': 'Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport C^2OT that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at https://hkchengrex.github.io/C2OT', 'score': 3, 'issue_id': 2700, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '93d6f410c35246be', 'authors': ['Ho Kei Cheng', 'Alexander Schwing'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10636.jpg', 'data': {'categories': ['#cv', '#inference', '#optimization', '#training'], 'emoji': '🔀', 'ru': {'title': 'Условный оптимальный транспорт для улучшения потоковых моделей', 'desc': 'Статья предлагает метод условного оптимального транспорта (C^2OT) для улучшения обучения потоковых моделей в условных задачах. Авторы выявили проблему несоответствия между тренировочным и тестовым распределением при использовании мини-батч оптимального транспорта в условных моделях. C^2OT решает эту проблему путем добавления условного весового коэффициента в матрицу стоимости при вычислении оптимального транспорта. Эксперименты показывают, что предложенный метод работает лучше существующих базовых линий на различных наборах данных и при разных бюджетах вычислений.'}, 'en': {'title': 'Bridging the Gap in Conditional Flow Matching with C^2OT', 'desc': 'This paper introduces a method called conditional optimal transport (C^2OT) to improve the performance of flow matching in machine learning. The authors highlight that while minibatch optimal transport simplifies computations during inference, it fails in conditional settings due to a mismatch between training and testing distributions. By incorporating a conditional weighting term in the cost matrix, C^2OT effectively aligns the training and testing conditions. Experiments show that this approach outperforms existing methods across various datasets, demonstrating its effectiveness in both discrete and continuous scenarios.'}, 'zh': {'title': '条件最优传输：缩小训练与测试的性能差距', 'desc': '本文提出了一种条件最优传输方法C^2OT，以解决在条件设置下小批量最优传输的不足。传统的最优传输映射在训练时忽略了条件，导致训练期间的先验分布偏斜，而测试时却无法访问这种偏斜的先验。通过在成本矩阵中添加条件加权项，C^2OT能够更好地计算最优传输分配，从而缩小训练与测试之间的性能差距。实验结果表明，该方法在多个数据集上表现优于现有基线。'}}}, {'id': 'https://huggingface.co/papers/2503.10602', 'title': 'TruthPrInt: Mitigating LVLM Object Hallucination Via Latent\n  Truthful-Guided Pre-Intervention', 'url': 'https://huggingface.co/papers/2503.10602', 'abstract': 'Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the "overall truthfulness" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as "per-token" hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist "generic truthful directions" shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at https://github.com/jinhaoduan/TruthPrInt.', 'score': 3, 'issue_id': 2712, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '9d4ccef966377a11', 'authors': ['Jinhao Duan', 'Fei Kong', 'Hao Cheng', 'James Diffenderfer', 'Bhavya Kailkhura', 'Lichao Sun', 'Xiaofeng Zhu', 'Xiaoshuang Shi', 'Kaidi Xu'], 'affiliations': ['Drexel University', 'Hong Kong University of Science and Technology (Guangzhou)', 'LLNL', 'Lehigh University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.10602.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#transfer_learning', '#cv', '#training', '#hallucinations', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Борьба с галлюцинациями в больших моделях', 'desc': 'В статье рассматривается проблема галлюцинации объектов в больших моделях, объединяющих зрение и язык (LVLMs). Исследователи обнаружили, что внутренние состояния этих моделей могут служить индикаторами галлюцинаций на уровне каждого токена. Они также выявили, что разные LVLMs имеют общие паттерны галлюцинаций в скрытых подпространствах. На основе этих открытий предложен метод TruthPrInt, который улучшает точность моделей за счёт направленного вмешательства в процесс декодирования.'}, 'en': {'title': 'Enhancing Truthfulness in Vision-Language Models', 'desc': "This paper addresses the problem of Object Hallucination (OH) in Large Vision-Language Models (LVLMs), which affects the reliability of their outputs. It investigates how internal states of LVLMs can act as indicators of hallucination on a per-token basis, revealing that these states can signal specific hallucination behaviors. The authors introduce a method called Truthful-Guided Pre-Intervention (TruthPrInt) that learns the 'truthful direction' during decoding to improve the accuracy of generated responses. Additionally, they propose ComnHallu to enhance the detection of hallucinations across different LVLMs and datasets by aligning their latent subspaces, demonstrating significant improvements over existing methods in their experiments."}, 'zh': {'title': '揭示LVLM中的真实方向，减少对象幻觉', 'desc': '本文探讨了大型视觉语言模型（LVLMs）中的对象幻觉（OH）问题，认为内部状态可以作为幻觉行为的指示器。研究发现，LVLM的内部状态能够高特异性地指示每个token的幻觉行为，并且不同的LVLM在潜在子空间中编码了通用的幻觉模式。基于这些发现，提出了一种名为Truthful-Guided Pre-Intervention（TruthPrInt）的方法，通过学习LVLM解码的真实方向来进行干预。实验结果表明，TruthPrInt在多个流行的LVLM和OH基准测试中显著优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.10638', 'title': 'Studying Classifier(-Free) Guidance From a Classifier-Centric\n  Perspective', 'url': 'https://huggingface.co/papers/2503.10638', 'abstract': 'Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance. Concretely, instead of solely focusing on classifier-free guidance, we trace back to the root, i.e., classifier guidance, pinpoint the key assumption for the derivation, and conduct a systematic study to understand the role of the classifier. We find that both classifier guidance and classifier-free guidance achieve conditional generation by pushing the denoising diffusion trajectories away from decision boundaries, i.e., areas where conditional information is usually entangled and is hard to learn. Based on this classifier-centric understanding, we propose a generic postprocessing step built upon flow-matching to shrink the gap between the learned distribution for a pre-trained denoising diffusion model and the real data distribution, majorly around the decision boundaries. Experiments on various datasets verify the effectiveness of the proposed approach.', 'score': 2, 'issue_id': 2715, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '67e97d098f101b16', 'authors': ['Xiaoming Zhao', 'Alexander G. Schwing'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.10638.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#diffusion', '#benchmark'], 'emoji': '🧭', 'ru': {'title': 'Новый взгляд на управление генерацией в моделях диффузии', 'desc': 'Это исследование посвящено анализу методов управления генерацией в моделях диффузии, в частности, classifier-free guidance и classifier guidance. Авторы обнаружили, что оба метода работают, отталкивая траектории денойзинга от границ принятия решений. На основе этого понимания они предложили новый метод постобработки с использованием flow-matching для улучшения распределения предобученной модели диффузии. Эксперименты подтвердили эффективность предложенного подхода на различных наборах данных.'}, 'en': {'title': 'Bridging the Gap in Conditional Generation with Classifier Insights', 'desc': 'This paper explores classifier-free guidance in denoising diffusion models, which is important for generating data based on specific conditions. The authors investigate the foundational concept of classifier guidance to better understand how both methods work. They discover that both approaches help in generating data by moving away from decision boundaries, where information is complex and difficult to learn. To improve the performance of pre-trained models, they introduce a new postprocessing technique that aligns the learned data distribution with the actual data distribution, particularly near these decision boundaries.'}, 'zh': {'title': '深入理解无分类器引导的条件生成', 'desc': '无分类器引导已成为去噪扩散模型条件生成的常用方法。然而，目前对无分类器引导的全面理解仍然缺乏。本文通过实证研究提供了对无分类器引导的新视角，追溯到分类器引导的根源，明确了推导的关键假设，并系统研究了分类器的作用。我们的研究发现，无论是分类器引导还是无分类器引导，都是通过将去噪扩散轨迹推离决策边界来实现条件生成，这些边界通常是条件信息交织且难以学习的区域。'}}}, {'id': 'https://huggingface.co/papers/2503.09368', 'title': 'PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with\n  Implicit Hierarchical Masked Image Modeling', 'url': 'https://huggingface.co/papers/2503.09368', 'abstract': 'We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct a comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features a hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at https://github.com/Nikolai10/PerCoV2.', 'score': 2, 'issue_id': 2707, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '7aa613c0b5e4220f', 'authors': ['Nikolai Körber', 'Eduard Kromer', 'Andreas Siebert', 'Sascha Hauke', 'Daniel Mueller-Gritschneder', 'Björn Schuller'], 'affiliations': ['TU Wien', 'Technical University of Munich', 'University of Applied Sciences Landshut'], 'pdf_title_img': 'assets/pdf/title_img/2503.09368.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset', '#architecture'], 'emoji': '🗜️', 'ru': {'title': 'Сжимай эффективнее, сохраняй качество: PerCoV2 на страже битрейта', 'desc': 'PerCoV2 - это новая система сжатия изображений с ультранизким битрейтом, основанная на перцептивном подходе. Она использует экосистему Stable Diffusion 3 и улучшает эффективность энтропийного кодирования путем моделирования дискретного гиперлатентного распределения изображений. Авторы сравнивают авторегрессионные методы VAR и MaskGIT для энтропийного моделирования и оценивают свой подход на датасете MSCOCO-30k. PerCoV2 достигает более высокой точности изображений при еще более низких битрейтах, сохраняя при этом конкурентоспособное перцептивное качество.'}, 'en': {'title': 'Ultra-Low Bit-Rate Image Compression Redefined', 'desc': 'PerCoV2 is a new image compression system that uses very few bits to represent images, making it ideal for situations where bandwidth and storage are limited. It improves on earlier methods by better modeling how images are represented in a compressed form, specifically within the Stable Diffusion 3 framework. The system has been tested against other advanced methods and shows better image quality at lower bit rates, while also offering a way to save even more space. Additionally, all components of PerCoV2 are publicly available, allowing others to use and build upon this technology.'}, 'zh': {'title': 'PerCoV2：超低比特率图像压缩的新突破', 'desc': 'PerCoV2是一种新型的超低比特率感知图像压缩系统，专为带宽和存储受限的应用而设计。该系统在Careil等人的先前工作基础上进行了扩展，增强了熵编码效率，通过明确建模离散超潜图像分布来实现。我们对最近的自回归方法（VAR和MaskGIT）进行了全面比较，并在大规模的MSCOCO-30k基准上评估了我们的方法。与之前的工作相比，PerCoV2在更低比特率下实现了更高的图像保真度，同时保持了竞争性的感知质量。'}}}, {'id': 'https://huggingface.co/papers/2503.10635', 'title': 'A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90%\n  Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1', 'url': 'https://huggingface.co/papers/2503.10635', 'abstract': 'Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution: at each optimization step, the adversarial image is cropped randomly by a controlled aspect ratio and scale, resized, and then aligned with the target image in the embedding space. Experimental results confirm our hypothesis. Our adversarial examples crafted with local-aggregated perturbations focused on crucial regions exhibit surprisingly good transferability to commercial LVLMs, including GPT-4.5, GPT-4o, Gemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning models like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly outperforming all prior state-of-the-art attack methods. Our optimized adversarial examples under different configurations and training code are available at https://github.com/VILA-Lab/M-Attack.', 'score': 1, 'issue_id': 2718, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'aba878bb95fd4fb4', 'authors': ['Zhaoyi Li', 'Xiaohan Zhao', 'Dong-Dong Wu', 'Jiacheng Cui', 'Zhiqiang Shen'], 'affiliations': ['VILA Lab, MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.10635.jpg', 'data': {'categories': ['#training', '#benchmark', '#security', '#optimization', '#dataset', '#cv'], 'emoji': '🎯', 'ru': {'title': 'Точечная атака: семантическое оружие против черных ящиков ИИ', 'desc': 'Статья представляет новый метод атаки на коммерческие мультимодальные модели, такие как GPT-4 и Gemini. Авторы обнаружили, что существующие методы часто терпят неудачу из-за отсутствия семантической информации в создаваемых возмущениях. Предложенный подход фокусируется на семантически важных областях изображения и использует локальное агрегирование возмущений. Экспериментальные результаты показывают высокую эффективность метода, достигая успешности атаки более 90% на ряде коммерческих моделей.'}, 'en': {'title': 'Enhancing Adversarial Attacks with Semantic Precision', 'desc': 'This paper addresses the challenges of transferring targeted adversarial attacks to black-box commercial large vision-language models (LVLMs). It identifies that previous attacks often fail due to the lack of semantic detail in the perturbations, which leads to ineffective responses from the models. The authors propose a novel method that enhances semantic clarity by focusing on important regions of the input, allowing for more effective adversarial modifications. Their experimental results demonstrate that this approach significantly improves the success rate of attacks on various commercial LVLMs, achieving over 90% success on several models.'}, 'zh': {'title': '聚焦语义细节，提升对抗攻击成功率', 'desc': '尽管开源的大型视觉语言模型（LVLMs）表现良好，但针对黑箱商业LVLMs的转移攻击往往失败。分析失败的对抗扰动发现，学习到的扰动通常来自均匀分布，缺乏明确的语义细节，导致意外的响应。为了克服这些问题，我们提出了一种方法，通过在局部区域内编码明确的语义细节，确保模型能够捕捉到更细粒度的特征，并集中修改在语义丰富的区域。实验结果表明，我们的方法在多个商业LVLMs上取得了超过90%的成功率，显著优于以往的攻击方法。'}}}, {'id': 'https://huggingface.co/papers/2503.07111', 'title': 'PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping\n  with VLM', 'url': 'https://huggingface.co/papers/2503.07111', 'abstract': 'This paper introduces PoseLess, a novel framework for robot hand control that eliminates the need for explicit pose estimation by directly mapping 2D images to joint angles using projected representations. Our approach leverages synthetic training data generated through randomized joint configurations, enabling zero-shot generalization to real-world scenarios and cross-morphology transfer from robotic to human hands. By projecting visual inputs and employing a transformer-based decoder, PoseLess achieves robust, low-latency control while addressing challenges such as depth ambiguity and data scarcity. Experimental results demonstrate competitive performance in joint angle prediction accuracy without relying on any human-labelled dataset.', 'score': 1, 'issue_id': 2721, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'edc49497be1a7cb7', 'authors': ['Alan Dao', 'Dinh Bach Vu', 'Tuan Le Duc Anh', 'Bui Quang Huy'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.07111.jpg', 'data': {'categories': ['#dataset', '#robotics', '#transfer_learning', '#training', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Управление роботизированной рукой без явной оценки позы', 'desc': 'PoseLess - это новая система управления роботизированной рукой, которая устраняет необходимость в явной оценке позы, напрямую сопоставляя 2D-изображения с углами суставов с помощью проецируемых представлений. Подход использует синтетические обучающие данные, сгенерированные путем рандомизации конфигураций суставов, что позволяет осуществлять обобщение с нулевого выстрела на реальные сценарии и межморфологический перенос с роботизированных на человеческие руки. PoseLess применяет проецирование визуальных входных данных и декодер на основе трансформера для достижения надежного управления с низкой задержкой. Экспериментальные результаты демонстрируют конкурентоспособную точность прогнозирования углов суставов без использования каких-либо наборов данных с человеческой разметкой.'}, 'en': {'title': 'Direct Control of Robot Hands Without Pose Estimation', 'desc': 'PoseLess is a new framework designed for controlling robot hands without needing to estimate their positions explicitly. It uses 2D images and directly maps them to joint angles, which simplifies the control process. The framework is trained on synthetic data created from various joint configurations, allowing it to adapt to real-world situations and transfer knowledge between different hand types. By utilizing a transformer-based decoder, PoseLess effectively manages challenges like depth ambiguity and limited data, achieving high accuracy in predicting joint angles without any human-labeled data.'}, 'zh': {'title': 'PoseLess：无须姿态估计的机器人手控制新框架', 'desc': '本文介绍了一种名为PoseLess的新框架，用于机器人手部控制，省去了显式姿态估计的需求，直接将2D图像映射到关节角度。我们的方法利用通过随机关节配置生成的合成训练数据，实现了对真实场景的零样本泛化和从机器人手到人类手的跨形态迁移。通过对视觉输入进行投影并采用基于变换器的解码器，PoseLess实现了稳健、低延迟的控制，同时解决了深度模糊和数据稀缺等挑战。实验结果表明，在关节角度预测准确性方面，PoseLess的表现具有竞争力，且不依赖于任何人工标注的数据集。'}}}, {'id': 'https://huggingface.co/papers/2503.02682', 'title': 'MPO: Boosting LLM Agents with Meta Plan Optimization', 'url': 'https://huggingface.co/papers/2503.02682', 'abstract': "Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.", 'score': 15, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'e5fa3c849c1eee72', 'authors': ['Weimin Xiong', 'Yifan Song', 'Qingxiu Dong', 'Bingchan Zhao', 'Feifan Song', 'Xun Wang', 'Sujian Li'], 'affiliations': ['National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University', 'Peking University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.02682.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#hallucinations', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Метапланы для умных агентов: эффективнее, универсальнее, без галлюцинаций', 'desc': 'Статья представляет новый подход к улучшению планирования задач агентами на основе больших языковых моделей (LLM). Метод под названием Meta Plan Optimization (MPO) использует высокоуровневые метапланы для руководства агентом и оптимизирует их на основе обратной связи. MPO превосходит существующие методы в эффективности выполнения задач и способности к обобщению. Этот подход не требует переобучения для каждого нового агента и решает проблему галлюцинаций при планировании.'}, 'en': {'title': 'Enhancing Agent Planning with Meta Plans', 'desc': "This paper introduces the Meta Plan Optimization (MPO) framework to improve the planning abilities of large language model (LLM)-based agents. MPO addresses issues like planning hallucinations and the need for retraining by using high-level meta plans for guidance. This approach allows for continuous optimization based on feedback from the agent's performance in tasks. Experimental results show that MPO not only outperforms existing methods but also enhances efficiency and adaptability in new situations."}, 'zh': {'title': '元规划优化：提升智能体规划能力的创新框架', 'desc': '最近，大型语言模型（LLMs）的进步使得基于LLM的智能体能够成功处理交互式规划任务。然而，现有方法常常面临规划幻觉的问题，并且每个新智能体都需要重新训练。为了解决这些挑战，我们提出了元规划优化（MPO）框架，通过直接引入明确的指导来增强智能体的规划能力。我们的实验表明，MPO在任务完成效率和在未见场景中的泛化能力上显著优于现有基线。'}}}, {'id': 'https://huggingface.co/papers/2503.02846', 'title': 'Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs', 'url': 'https://huggingface.co/papers/2503.02846', 'abstract': 'Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preference learning inevitably introduced noises during training. Therefore, this paper proposes a fine-grained factuality alignment method based on Direct Preference Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as mask signals, Mask-DPO only learns from factually correct sentences in the preferred samples and prevents the penalty on factual contents in the not preferred samples, which resolves the ambiguity in the preference learning. Extensive experimental results demonstrate that Mask-DPO can significantly improve the factuality of LLMs responses to questions from both in-domain and out-of-domain datasets, although these questions and their corresponding topics are unseen during training. Only trained on the ANAH train set, the score of Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%, even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its FactScore on the out-of-domain Biography dataset is also improved from 30.29% to 39.39%. We further study the generalization property of Mask-DPO using different training sample scaling strategies and find that scaling the number of topics in the dataset is more effective than the number of questions. We provide a hypothesis of what factual alignment is doing with LLMs, on the implication of this phenomenon, and conduct proof-of-concept experiments to verify it. We hope the method and the findings pave the way for future research on scaling factuality alignment.', 'score': 14, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '7e9277cf8ca5bb98', 'authors': ['Yuzhe Gu', 'Wenwei Zhang', 'Chengqi Lyu', 'Dahua Lin', 'Kai Chen'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02846.jpg', 'data': {'categories': ['#hallucinations', '#training', '#rlhf', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Mask-DPO: точная настройка фактов в ответах языковых моделей', 'desc': 'Эта статья представляет метод Mask-DPO для улучшения фактической точности больших языковых моделей (LLM). Метод основан на оптимизации прямых предпочтений (DPO) и использует маскирование на уровне предложений для обучения только на фактически верных частях ответов. Экспериментальные результаты показывают значительное улучшение фактической точности как на знакомых, так и на новых данных. Исследование также выявило, что масштабирование количества тем в обучающем наборе более эффективно, чем увеличение числа вопросов.'}, 'en': {'title': 'Enhancing Factuality in LLMs with Mask-DPO', 'desc': 'This paper addresses the issue of hallucinations in large language models (LLMs), where they generate incorrect or nonsensical information despite including some accurate content. The authors introduce a new method called Mask-DPO, which focuses on fine-grained factuality alignment by using sentence-level factuality as mask signals. This approach allows the model to learn only from factually correct sentences in preferred samples, reducing noise during training and improving the overall factual accuracy of LLM responses. Experimental results show that Mask-DPO significantly enhances the factuality of LLMs, even on unseen questions and topics, outperforming larger models in certain tests.'}, 'zh': {'title': '提升大型语言模型的事实性对齐', 'desc': '大型语言模型（LLMs）在作为AI助手时常常会出现幻觉现象，即提供不真实或无意义的信息。以往的事实对齐方法在训练过程中引入了噪声，因为它们在响应级别进行偏好学习。本文提出了一种基于直接偏好优化（DPO）的细粒度事实对齐方法，称为Mask-DPO，该方法通过句子级别的事实作为掩码信号，仅从事实正确的句子中学习，从而提高了LLMs的响应准确性。实验结果表明，Mask-DPO显著提升了LLMs在未见问题上的事实性，尤其是在不同领域的数据集上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2503.02879', 'title': 'Wikipedia in the Era of LLMs: Evolution and Risks', 'url': 'https://huggingface.co/papers/2503.02879', 'abstract': "In this paper, we present a thorough analysis of the impact of Large Language Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through existing data and using simulations to explore potential risks. We begin by analyzing page views and article content to study Wikipedia's recent changes and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect various Natural Language Processing (NLP) tasks related to Wikipedia, including machine translation and retrieval-augmented generation (RAG). Our findings and simulation results reveal that Wikipedia articles have been influenced by LLMs, with an impact of approximately 1%-2% in certain categories. If the machine translation benchmark based on Wikipedia is influenced by LLMs, the scores of the models may become inflated, and the comparative results among models might shift as well. Moreover, the effectiveness of RAG might decrease if the knowledge base becomes polluted by LLM-generated content. While LLMs have not yet fully changed Wikipedia's language and knowledge structures, we believe that our empirical findings signal the need for careful consideration of potential future risks.", 'score': 13, 'issue_id': 2535, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'd8fedc8bf5ffc308', 'authors': ['Siming Huang', 'Yuliang Xu', 'Mingmeng Geng', 'Yao Wan', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'International School for Advanced Studies (SISSA)'], 'pdf_title_img': 'assets/pdf/title_img/2503.02879.jpg', 'data': {'categories': ['#rag', '#benchmark', '#machine_translation', '#dataset', '#multimodal', '#science', '#data'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели меняют лицо Википедии: анализ влияния и потенциальных рисков', 'desc': 'Эта статья представляет анализ влияния больших языковых моделей (LLM) на Википедию. Исследователи изучили изменения в просмотрах страниц и содержании статей, а также провели симуляции для оценки потенциальных рисков. Результаты показывают, что LLM повлияли на 1-2% статей в некоторых категориях, что может привести к искажению результатов в задачах машинного перевода и генерации с использованием внешних знаний (RAG). Авторы призывают к осторожности в отношении будущих рисков, связанных с влиянием LLM на структуру знаний в Википедии.'}, 'en': {'title': "Navigating the Impact of LLMs on Wikipedia's Evolution", 'desc': 'This paper analyzes how Large Language Models (LLMs) are affecting Wikipedia by examining changes in page views and article content. It assesses the influence of LLMs on various Natural Language Processing (NLP) tasks, such as machine translation and retrieval-augmented generation (RAG). The study finds that LLMs have caused a 1%-2% impact on certain Wikipedia categories, which could inflate machine translation benchmarks and alter model comparisons. The authors emphasize the importance of monitoring these changes to mitigate potential risks associated with LLM-generated content.'}, 'zh': {'title': '大型语言模型对维基百科的影响分析', 'desc': '本文深入分析了大型语言模型（LLMs）对维基百科的影响，研究了维基百科的演变。我们通过分析页面浏览量和文章内容，评估LLMs对维基百科的影响，发现某些类别的影响约为1%-2%。此外，我们还评估了LLMs对与维基百科相关的自然语言处理（NLP）任务的影响，包括机器翻译和检索增强生成（RAG）。我们的研究结果表明，LLMs可能会导致机器翻译基准的分数膨胀，并可能影响模型之间的比较结果，因此需要对未来的潜在风险进行仔细考虑。'}}}, {'id': 'https://huggingface.co/papers/2503.01935', 'title': 'MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents', 'url': 'https://huggingface.co/papers/2503.01935', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.', 'score': 10, 'issue_id': 2532, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4353f27d396c322a', 'authors': ['Kunlun Zhu', 'Hongyi Du', 'Zhaochen Hong', 'Xiaocheng Yang', 'Shuyi Guo', 'Zhe Wang', 'Zhenhailong Wang', 'Cheng Qian', 'Xiangru Tang', 'Heng Ji', 'Jiaxuan You'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.01935.jpg', 'data': {'categories': ['#games', '#optimization', '#open_source', '#benchmark', '#agents'], 'emoji': '🤖', 'ru': {'title': 'MultiAgentBench: новый стандарт для оценки мультиагентных LLM-систем', 'desc': 'Статья представляет MultiAgentBench - комплексный бенчмарк для оценки мультиагентных систем на основе больших языковых моделей (LLM) в различных интерактивных сценариях. Фреймворк измеряет не только выполнение задач, но и качество сотрудничества и конкуренции с помощью новых ключевых показателей эффективности. Авторы оценивают различные протоколы координации и инновационные стратегии, такие как групповое обсуждение и когнитивное планирование. Результаты показывают, что gpt-4o-mini достигает наивысшего среднего балла за выполнение задач, а графовая структура лучше всего работает среди протоколов координации в исследовательском сценарии.'}, 'en': {'title': 'Evaluating LLMs in Multi-Agent Dynamics', 'desc': 'This paper presents MultiAgentBench, a new benchmark for assessing the performance of Large Language Models (LLMs) in multi-agent environments. Unlike previous benchmarks that focus on single-agent tasks, MultiAgentBench evaluates how well LLMs can collaborate and compete in various interactive scenarios. The framework introduces key performance indicators that measure both task completion and the quality of interactions among agents. The study also explores different coordination protocols and innovative strategies, revealing that certain configurations, like graph structures and cognitive planning, significantly enhance performance.'}, 'zh': {'title': '多智能体系统的全面评估基准', 'desc': '本文介绍了MultiAgentBench，这是一个全面的基准测试，旨在评估基于大型语言模型（LLM）的多智能体系统在多样化互动场景中的表现。该框架不仅测量任务完成情况，还评估协作和竞争的质量，使用新颖的里程碑式关键绩效指标。我们还评估了多种协调协议（如星形、链形、树形和图形拓扑）以及创新策略，如小组讨论和认知规划。研究表明，gpt-4o-mini在任务得分上表现最佳，图形结构在协调协议中表现最佳，而认知规划提高了里程碑达成率3%。'}}}, {'id': 'https://huggingface.co/papers/2503.01328', 'title': 'PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization', 'url': 'https://huggingface.co/papers/2503.01328', 'abstract': 'Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging the under-explored memory offload strategy in PP. With empirical study, we discover that in the majority of standard configurations, at least half, and potentially all, of the activations can be offloaded with negligible overhead. In the cases where full overload is not possible, we introduce a novel selective offload strategy that decreases peak activation memory in a better-than-linear manner. Furthermore, we integrate memory offload with other techniques to jointly consider overall throughput and memory limitation. Our experiments proves that the per-device activation memory effectively reduces with the total number of stages, making PP a stronger alternative than TP, offering up to a 19\\% acceleration with even lower memory consumption. The implementation is open-sourced at https://github.com/sail-sg/zero-bubble-pipeline-parallelism{this url}.', 'score': 10, 'issue_id': 2532, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '75e25b312e4cef8a', 'authors': ['Xinyi Wan', 'Penghui Qi', 'Guangxing Huang', 'Jialin Li', 'Min Lin'], 'affiliations': ['National University of', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.01328.jpg', 'data': {'categories': ['#open_source', '#inference', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Оптимизация памяти для масштабирования больших языковых моделей', 'desc': 'Статья посвящена улучшению масштабируемости конвейерного параллелизма при обучении больших языковых моделей. Авторы предлагают стратегию выгрузки активаций из памяти, которая позволяет значительно снизить пиковое потребление памяти. Они также представляют метод выборочной выгрузки для случаев, когда полная выгрузка невозможна. Эксперименты показывают, что данный подход делает конвейерный параллелизм более эффективной альтернативой тензорному параллелизму, ускоряя обучение на 19% при меньшем потреблении памяти.'}, 'en': {'title': 'Optimizing Memory Usage in Pipeline Parallelism for Faster Training', 'desc': 'This paper addresses the challenge of high activation memory consumption in pipeline parallelism (PP) when training large language models (LLMs). The authors explore a memory offload strategy that allows for significant reductions in memory usage, showing that up to half of the activations can be offloaded with minimal impact on performance. They also propose a selective offload strategy that further decreases peak activation memory in a more efficient way. The results demonstrate that using memory offload in conjunction with other techniques can enhance throughput while reducing memory requirements, making PP a more viable option compared to tensor parallelism (TP).'}, 'zh': {'title': '优化管道并行性，降低内存消耗！', 'desc': '本文探讨了在训练大型语言模型时，管道并行性（PP）面临的高激活内存消耗问题。我们提出了一种内存卸载策略，可以在大多数标准配置中卸载至少一半的激活，几乎没有额外开销。对于无法完全卸载的情况，我们引入了一种新的选择性卸载策略，显著降低了峰值激活内存。实验结果表明，随着阶段数量的增加，每个设备的激活内存有效减少，使得PP成为比张量并行性（TP）更强的选择，提供高达19%的加速，同时降低内存消耗。'}}}, {'id': 'https://huggingface.co/papers/2503.00735', 'title': 'LADDER: Self-Improving LLMs Through Recursive Problem Decomposition', 'url': 'https://huggingface.co/papers/2503.00735', 'abstract': "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of complex problems. Unlike prior approaches that require curated datasets or human feedback, LADDER leverages a model's own capabilities to generate easier question variants. We demonstrate LADDER's effectiveness in the subject of mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve 73% on the MIT Integration Bee qualifying examination. We also introduce TTRL (Test-Time Reinforcement Learning), where we perform reinforcement learning on variants of test problems at inference time. TTRL enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of 90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's performance. These results show how self-directed strategic learning can achieve significant capability improvements without relying on architectural scaling or human supervision.", 'score': 9, 'issue_id': 2540, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'c9ce66a728a6df87', 'authors': ['Toby Simonds', 'Akira Yoshiyama'], 'affiliations': ['Tufa Labs'], 'pdf_title_img': 'assets/pdf/title_img/2503.00735.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#training', '#rl'], 'emoji': '🧮', 'ru': {'title': 'Самообучение языковых моделей через генерацию упрощенных задач', 'desc': 'Представлен метод LADDER, позволяющий языковым моделям самостоятельно улучшать навыки решения задач путем генерации и решения упрощенных вариантов сложных проблем. В отличие от предыдущих подходов, LADDER не требует подготовленных датасетов или обратной связи от людей. Эффективность метода продемонстрирована на задачах математического интегрирования, где точность модели Llama 3.2 3B выросла с 1% до 82% на задачах университетского уровня. Также представлен метод TTRL, использующий обучение с подкреплением во время вывода, что позволило модели Qwen2.5 7B достичь рекордных 90% на отборочном экзамене MIT Integration Bee.'}, 'en': {'title': 'Empowering Models to Learn and Solve Problems Autonomously', 'desc': 'LADDER is a new framework that helps Large Language Models (LLMs) improve their problem-solving skills by creating and solving simpler versions of complex problems on their own. This method does not need pre-made datasets or human input, as it allows the model to generate easier questions based on its own understanding. The framework has shown remarkable success in mathematical integration, significantly boosting the accuracy of models like Llama 3.2 and Qwen2.5 on challenging exams. Additionally, the introduction of Test-Time Reinforcement Learning (TTRL) further enhances performance by applying reinforcement learning during the testing phase, leading to state-of-the-art results.'}, 'zh': {'title': '自主学习，提升能力的全新框架', 'desc': '本文介绍了LADDER（通过自主难度驱动的示例递归学习）框架，该框架使大型语言模型能够通过自我引导学习，逐步生成和解决复杂问题的简化变体，从而提高其解决问题的能力。与以往需要人工反馈或精心策划数据集的方法不同，LADDER利用模型自身的能力生成更简单的问题变体。我们展示了LADDER在数学积分领域的有效性，使Llama 3.2 3B在本科水平问题上的准确率从1%提高到82%，并使Qwen2.5 7B Deepseek-R1 Distilled在MIT积分竞赛资格考试中达到73%。此外，我们还引入了TTRL（测试时强化学习），在推理时对测试问题的变体进行强化学习，使Qwen2.5 7B Deepseek-R1 Distilled在MIT积分竞赛资格考试中获得90%的领先成绩，超越了OpenAI o1的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.02368', 'title': 'Iterative Value Function Optimization for Guided Decoding', 'url': 'https://huggingface.co/papers/2503.02368', 'abstract': 'While Reinforcement Learning from Human Feedback (RLHF) has become the predominant method for controlling language model outputs, it suffers from high computational costs and training instability. Guided decoding, especially value-guided methods, offers a cost-effective alternative by controlling outputs without re-training models. However, the accuracy of the value function is crucial for value-guided decoding, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with accurately estimating the optimal value function, leading to less effective control. We propose Iterative Value Function Optimization, a novel framework that addresses these limitations through two key components: Monte Carlo Value Estimation, which reduces estimation variance by exploring diverse trajectories, and Iterative On-Policy Optimization, which progressively improves value estimation through collecting trajectories from value-guided policies. Extensive experiments on text summarization, multi-turn dialogue, and instruction following demonstrate the effectiveness of value-guided decoding approaches in aligning language models. These approaches not only achieve alignment but also significantly reduce computational costs by leveraging principled value function optimization for efficient and effective control.', 'score': 9, 'issue_id': 2538, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '6a7f0679f238bd4d', 'authors': ['Zhenhua Liu', 'Lijun Li', 'Ruizhe Chen', 'Yuxian Jiang', 'Tong Zhu', 'Wenliang Chen', 'Jing Shao'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Soochow University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02368.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Эффективное управление языковыми моделями без переобучения', 'desc': "Статья представляет новый метод управления выходными данными языковых моделей, называемый 'Итеративная оптимизация функции ценности'. Этот подход предлагает альтернативу традиционному обучению с подкреплением на основе обратной связи от человека (RLHF), снижая вычислительные затраты и повышая стабильность обучения. Метод использует управляемое декодирование на основе функции ценности, улучшая её точность с помощью оценки Монте-Карло и итеративной оптимизации. Эксперименты показали эффективность предложенного подхода в задачах суммаризации текста, многоэтапного диалога и выполнения инструкций."}, 'en': {'title': 'Optimizing Value Functions for Efficient Language Model Control', 'desc': 'This paper addresses the challenges of Reinforcement Learning from Human Feedback (RLHF) in controlling language models, particularly its high computational costs and instability. It introduces a new framework called Iterative Value Function Optimization, which enhances value-guided decoding methods by improving the accuracy of the value function. The framework employs Monte Carlo Value Estimation to minimize estimation variance and Iterative On-Policy Optimization to refine value estimation through trajectory collection. Experimental results show that this approach not only aligns language models effectively but also reduces computational expenses significantly.'}, 'zh': {'title': '高效控制语言模型的新方法', 'desc': '本论文探讨了人类反馈强化学习（RLHF）在控制语言模型输出时的高计算成本和训练不稳定性问题。我们提出了一种新的框架——迭代值函数优化，通过蒙特卡洛值估计和迭代在线优化来提高值函数的准确性。该方法通过探索多样化的轨迹来减少估计方差，并逐步改进值估计。实验结果表明，基于值引导的解码方法在文本摘要、多轮对话和指令跟随任务中表现出色，显著降低了计算成本。'}}}, {'id': 'https://huggingface.co/papers/2503.00955', 'title': 'SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking', 'url': 'https://huggingface.co/papers/2503.00955', 'abstract': 'The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01 and 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.', 'score': 7, 'issue_id': 2535, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '651aacbd378465bd', 'authors': ['Nam V. Nguyen', 'Dien X. Tran', 'Thanh T. Tran', 'Anh T. Hoang', 'Tai V. Duong', 'Di T. Le', 'Phuc-Lu Le'], 'affiliations': ['FPT Software AI Center, Viet Nam', 'FPT Telecom, Viet Nam', 'Faculty of Information Technology, Industrial University of Ho Chi Minh City, Viet Nam', 'Faculty of Information Technology, University of Science, VNU-HCM, Viet Nam'], 'pdf_title_img': 'assets/pdf/title_img/2503.00955.jpg', 'data': {'categories': ['#multilingual', '#inference', '#benchmark', '#low_resource', '#dataset', '#science', '#data'], 'emoji': '🕵️', 'ru': {'title': 'SemViQA: Передовая система проверки фактов для борьбы с дезинформацией на вьетнамском языке', 'desc': 'SemViQA - это новая система проверки фактов на вьетнамском языке, разработанная для борьбы с дезинформацией, усугубляемой крупными языковыми моделями. Она объединяет семантический поиск доказательств и двухэтапную классификацию вердиктов, обеспечивая баланс между точностью и скоростью. SemViQA достигла наилучших результатов на датасетах ISE-DSC01 и ViWikiFC, установив новый стандарт для проверки фактов на вьетнамском языке. Быстрая версия SemViQA Faster увеличивает скорость вывода в 7 раз при сохранении конкурентоспособной точности.'}, 'en': {'title': 'SemViQA: Revolutionizing Vietnamese Fact-Checking Against Misinformation', 'desc': 'This paper addresses the challenge of misinformation in low-resource languages, specifically Vietnamese, by introducing SemViQA, a new fact-checking framework. SemViQA combines Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC) to enhance both accuracy and efficiency in verifying facts. The framework achieves impressive results, with a strict accuracy of 78.97% on the ISE-DSC01 dataset and 80.82% on ViWikiFC, outperforming existing methods. Additionally, SemViQA Faster significantly boosts inference speed by 7 times while maintaining competitive accuracy, setting a new standard for fact verification in Vietnamese.'}, 'zh': {'title': 'SemViQA：越南语事实核查的新标杆', 'desc': '随着大型语言模型（LLMs）如GPT和Gemini的兴起，虚假信息问题日益严重，尤其是在资源匮乏的语言如越南语中，迫切需要强有力的事实核查解决方案。现有的方法在语义模糊、同义词和复杂语言结构方面存在困难，往往在准确性和效率之间做出妥协。我们提出了SemViQA，这是一种新颖的越南语事实核查框架，结合了基于语义的证据检索（SER）和两步裁决分类（TVC）。我们的方案在保持竞争性准确度的同时，实现了精度与速度的平衡，在ISE-DSC01上达到了78.97%的严格准确率，在ViWikiFC上达到了80.82%，并在UIT数据科学挑战赛中获得第一名。'}}}, {'id': 'https://huggingface.co/papers/2502.14856', 'title': 'FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling', 'url': 'https://huggingface.co/papers/2502.14856', 'abstract': 'Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12times speedup over the state-of-the-art speculative sampling method EAGLE-2.', 'score': 6, 'issue_id': 2535, 'pub_date': '2025-02-20', 'pub_date_card': {'ru': '20 февраля', 'en': 'February 20', 'zh': '2月20日'}, 'hash': '45e128ccea542dad', 'authors': ['Weilin Zhao', 'Tengyu Pan', 'Xu Han', 'Yudi Zhang', 'Ao Sun', 'Yuxiang Huang', 'Kaihuo Zhang', 'Weilun Zhao', 'Yuxuan Li', 'Jianyong Wang', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Beijing University of Posts and Telecommunications, Beijing, China', 'Harbin Institute of Technology, Harbin, China', 'OpenBMB', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.14856.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Быстрее и эффективнее: оптимизация спекулятивной выборки для LLM', 'desc': 'Статья представляет FR-Spec - новый метод спекулятивной выборки для ускорения работы больших языковых моделей (LLM). Метод оптимизирует выбор кандидатов путем сжатия пространства словаря, отдавая приоритет наиболее частотным токенам. Это позволяет снизить вычислительные затраты на 75% по сравнению с существующими методами, сохраняя эквивалентность итогового распределения. Эксперименты показывают ускорение в 1.12 раза по сравнению с современным методом EAGLE-2.'}, 'en': {'title': 'Accelerating Token Generation with Frequency-Ranked Speculative Sampling', 'desc': 'This paper introduces FR-Spec, a new framework for speculative sampling in large language models (LLMs) that enhances the efficiency of token generation. By using a draft-then-verify approach, FR-Spec optimizes the selection of draft candidates by focusing on a frequency-ranked subset of tokens, which reduces computational overhead significantly. The method achieves a 75% reduction in LM Head computation while maintaining the same output distribution as traditional methods. Experimental results show that FR-Spec provides an average speedup of 1.12 times compared to the leading speculative sampling technique, EAGLE-2.'}, 'zh': {'title': '频率优先，推测采样加速！', 'desc': '本文提出了一种名为FR-Spec的频率排名推测采样框架，旨在加速大型语言模型的自回归生成过程。该方法通过压缩词汇空间，优化草稿候选选择，从而减少计算开销。FR-Spec将草稿搜索限制在一个优先考虑频率的词汇子集上，使得语言模型头的计算开销降低了75%。实验结果表明，FR-Spec在多个数据集上相较于最先进的推测采样方法EAGLE-2实现了平均1.12倍的加速。'}}}, {'id': 'https://huggingface.co/papers/2503.02537', 'title': 'RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification', 'url': 'https://huggingface.co/papers/2503.02537', 'abstract': "Diffusion models have achieved remarkable advances in various image generation tasks. However, their performance notably declines when generating images at resolutions higher than those used during the training period. Despite the existence of numerous methods for producing high-resolution images, they either suffer from inefficiency or are hindered by complex operations. In this paper, we propose RectifiedHR, an efficient and straightforward solution for training-free high-resolution image generation. Specifically, we introduce the noise refresh strategy, which theoretically only requires a few lines of code to unlock the model's high-resolution generation ability and improve efficiency. Additionally, we first observe the phenomenon of energy decay that may cause image blurriness during the high-resolution image generation process. To address this issue, we propose an Energy Rectification strategy, where modifying the hyperparameters of the classifier-free guidance effectively improves the generation performance. Our method is entirely training-free and boasts a simple implementation logic. Through extensive comparisons with numerous baseline methods, our RectifiedHR demonstrates superior effectiveness and efficiency.", 'score': 5, 'issue_id': 2540, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '764b3070cbcdd839', 'authors': ['Zhen Yang', 'Guibao Shen', 'Liang Hou', 'Mushui Liu', 'Luozhou Wang', 'Xin Tao', 'Pengfei Wan', 'Di Zhang', 'Ying-Cong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Kuaishou Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02537.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений высокого разрешения без дополнительного обучения', 'desc': 'Статья представляет новый метод RectifiedHR для генерации изображений высокого разрешения с помощью диффузионных моделей. Авторы предлагают стратегию обновления шума, которая позволяет улучшить способность модели генерировать изображения высокого разрешения без дополнительного обучения. Они также вводят стратегию энергетической ректификации для решения проблемы размытости изображений. Метод RectifiedHR показывает превосходную эффективность по сравнению с существующими подходами.'}, 'en': {'title': 'Unlocking High-Resolution Image Generation with RectifiedHR', 'desc': 'This paper presents RectifiedHR, a novel approach for generating high-resolution images using diffusion models without the need for additional training. The authors introduce a noise refresh strategy that simplifies the process, allowing for efficient high-resolution image generation with minimal code changes. They also identify and address the issue of energy decay, which can lead to blurry images, by proposing an Energy Rectification strategy that optimizes hyperparameters for better performance. Overall, RectifiedHR stands out for its simplicity and effectiveness compared to existing methods.'}, 'zh': {'title': '高效无训练的高分辨率图像生成', 'desc': '扩散模型在图像生成任务中取得了显著进展，但在生成高于训练分辨率的图像时性能明显下降。尽管已有多种方法可以生成高分辨率图像，但它们往往效率低下或操作复杂。本文提出了一种名为RectifiedHR的高效且简单的无训练高分辨率图像生成解决方案。我们引入了噪声刷新策略和能量修正策略，显著提高了生成性能，且实现逻辑简单。'}}}, {'id': 'https://huggingface.co/papers/2503.02197', 'title': 'ATLaS: Agent Tuning via Learning Critical Steps', 'url': 'https://huggingface.co/papers/2503.02197', 'abstract': "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training's focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.", 'score': 5, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a06c17fd97b92f03', 'authors': ['Zhixun Chen', 'Ming Li', 'Yuxuan Huang', 'Yali Du', 'Meng Fang', 'Tianyi Zhou'], 'affiliations': ['Kings College London', 'University of Liverpool', 'University of Maryland', 'University of Technology Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.02197.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#agents', '#agi'], 'emoji': '🎯', 'ru': {'title': 'ATLaS: точечная настройка LLM-агентов для улучшения обобщения', 'desc': 'ATLaS - это новый метод настройки агентов на основе больших языковых моделей (LLM). Он фокусируется на обучении только критически важным шагам в экспертных траекториях, что улучшает обобщение и снижает риск переобучения. Эксперименты показывают, что LLM, настроенная на 30% критических шагов, выбранных ATLaS, превосходит модели, обученные на всех шагах. Этот подход позволяет сохранить и улучшить базовые навыки LLM как универсальных агентов для взаимодействия с различными средами.'}, 'en': {'title': 'Focus on Critical Steps for Better LLM Performance', 'desc': 'This paper introduces ATLaS, a novel approach for tuning Large Language Model (LLM) agents by focusing on critical steps in expert trajectories rather than using full trajectory behavior-cloning. By finetuning LLMs on only 30% of these essential steps, ATLaS reduces the risk of expert bias and enhances generalization to unseen states. The method emphasizes the importance of planning, reasoning, and decision-making in agent tasks, which are crucial for improving performance. Experimental results show that LLMs trained with ATLaS outperform those trained on complete trajectories and other recent models, while also maintaining their generalist capabilities across various tasks.'}, 'zh': {'title': '聚焦关键步骤，提升LLM代理的泛化能力', 'desc': '大型语言模型（LLM）代理在多领域任务中展现了出色的泛化能力。现有的代理调优方法通常在整个专家轨迹上进行监督微调，但这种行为克隆可能引入专家偏见，削弱对未覆盖状态的泛化能力。本文提出的ATLaS方法通过识别专家轨迹中的关键步骤，仅在这些步骤上进行微调，从而降低成本并提高效率。实验表明，基于ATLaS选择的30%关键步骤微调的LLM在性能上优于在所有步骤上微调的LLM和最近的开源LLM代理。'}}}, {'id': 'https://huggingface.co/papers/2503.02878', 'title': 'Language Models can Self-Improve at State-Value Estimation for Better Search', 'url': 'https://huggingface.co/papers/2503.02878', 'abstract': 'Collecting ground truth task completion rewards or human demonstrations for multi-step reasoning tasks is often cost-prohibitive and time-consuming, especially in interactive domains like web tasks. To address this bottleneck, we present self-taught lookahead, a self-supervised method that leverages state-transition dynamics to train a value model capable of effectively guiding language model-controlled search. We find that moderately sized (8 billion parameters) open-weight value models improved with self-taught lookahead can match the performance of using a frontier LLM such as gpt-4o as the value model. Furthermore, we find that self-taught lookahead improves performance by 20% while reducing costs 37x compared to previous LLM-based tree search, without relying on ground truth rewards.', 'score': 5, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '04d05c7118ce4a93', 'authors': ['Ethan Mendes', 'Alan Ritter'], 'affiliations': ['Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.02878.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самообучение для эффективного поиска без дорогостоящих демонстраций', 'desc': "Статья представляет метод 'self-taught lookahead' для обучения модели оценки, способной эффективно направлять поиск, управляемый языковой моделью. Этот самоконтролируемый метод использует динамику переходов состояний, что позволяет избежать дорогостоящего и трудоемкого сбора наград за выполнение задач или демонстраций от людей. Исследование показывает, что относительно небольшие (8 миллиардов параметров) модели оценки, улучшенные с помощью 'self-taught lookahead', могут сравниться по производительности с использованием передовых языковых моделей, таких как GPT-4, в качестве модели оценки. Более того, метод улучшает производительность на 20% при одновременном снижении затрат в 37 раз по сравнению с предыдущими методами поиска на основе больших языковых моделей."}, 'en': {'title': 'Self-Taught Lookahead: Cost-Effective Multi-Step Reasoning', 'desc': 'This paper introduces a method called self-taught lookahead, which is designed to improve the efficiency of training value models for multi-step reasoning tasks without needing expensive human input. By utilizing state-transition dynamics, this self-supervised approach allows the model to learn how to guide searches effectively. The authors demonstrate that a moderately sized value model can achieve performance comparable to larger models like GPT-4o, while also significantly reducing costs. Overall, self-taught lookahead enhances performance by 20% and cuts costs by 37 times compared to traditional LLM-based methods.'}, 'zh': {'title': '自我教导前瞻：高效的多步骤推理解决方案', 'desc': '本论文提出了一种自我学习的前瞻性方法，称为自我教导前瞻（self-taught lookahead），旨在解决多步骤推理任务中收集真实奖励或人类示范的高成本和时间消耗问题。该方法利用状态转移动态来训练一个价值模型，从而有效指导语言模型控制的搜索。研究表明，使用自我教导前瞻的中等规模（80亿参数）开放权重价值模型，其性能可以与前沿的语言模型（如gpt-4o）相媲美。更重要的是，自我教导前瞻在不依赖真实奖励的情况下，能够将性能提升20%，同时将成本降低37倍。'}}}, {'id': 'https://huggingface.co/papers/2503.01342', 'title': 'UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface', 'url': 'https://huggingface.co/papers/2503.01342', 'abstract': 'Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present \\ours, a framework that Unifies Fine-grained visual perception tasks through an Open-ended language interface. By transforming all perception targets into the language space, \\ours unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, \\ours outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby enabling more challenging tasks such as reasoning segmentation. Code and models will be publicly available.', 'score': 4, 'issue_id': 2535, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8f87e24c90eade92', 'authors': ['Hao Tang', 'Chenwei Xie', 'Haiyang Wang', 'Xiaoyi Bao', 'Tingyu Weng', 'Pandeng Li', 'Yun Zheng', 'Liwei Wang'], 'affiliations': ['Alibaba Group', 'Center for Data Science, Peking University', 'Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.01342.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#agi', '#open_source', '#multimodal', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Унификация задач компьютерного зрения через языковой интерфейс', 'desc': 'Статья представляет новый фреймворк для унификации задач тонкой визуальной перцепции через языковой интерфейс. Авторы трансформируют все цели восприятия в языковое пространство, объединяя обнаружение объектов, сегментацию на уровне пикселей и задачи зрения-языка в единую модель. Предложен новый подход с извлечением эмбеддингов, использующий только языковой интерфейс для поддержки задач сегментации. Фреймворк превосходит предыдущие модели-генералисты на нескольких стандартных наборах данных и легко интегрируется с существующими мультимодальными языковыми моделями.'}, 'en': {'title': 'Unifying Fine-Grained Perception with Language for Enhanced Model Performance', 'desc': 'This paper introduces a framework called \textit{ours} that aims to unify fine-grained visual perception tasks, such as detection and segmentation, with language-based tasks. By converting all perception targets into a language format, the framework simplifies the integration of various tasks into a single model. The authors also propose a new embedding retrieval method that utilizes the language interface to enhance segmentation capabilities. The results show that \textit{ours} outperforms existing generalist models on multiple datasets, demonstrating its effectiveness in bridging fine-grained perception and vision-language tasks.'}, 'zh': {'title': '统一细粒度视觉感知与语言任务的创新框架', 'desc': '本论文提出了一种新的框架\textit{ours}，旨在通过开放式语言接口统一细粒度视觉感知任务，如目标检测和像素分割。该框架将所有感知目标转化为语言空间，从而将对象级检测、像素级分割和图像级视觉语言任务整合到一个模型中。我们还引入了一种新颖的嵌入检索方法，仅依赖语言接口来支持分割任务。经过在五个标准视觉感知数据集上的多任务训练，\textit{ours}在COCO实例分割上比之前的最先进通用模型提高了12.3 mAP，在ADE20K语义分割上提高了3.3 mIoU。'}}}, {'id': 'https://huggingface.co/papers/2503.02876', 'title': 'SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models', 'url': 'https://huggingface.co/papers/2503.02876', 'abstract': 'Advancing AI in computational pathology requires large, high-quality, and diverse datasets, yet existing public datasets are often limited in organ diversity, class coverage, or annotation quality. To bridge this gap, we introduce SPIDER (Supervised Pathology Image-DEscription Repository), the largest publicly available patch-level dataset covering multiple organ types, including Skin, Colorectal, and Thorax, with comprehensive class coverage for each organ. SPIDER provides high-quality annotations verified by expert pathologists and includes surrounding context patches, which enhance classification performance by providing spatial context.   Alongside the dataset, we present baseline models trained on SPIDER using the Hibou-L foundation model as a feature extractor combined with an attention-based classification head. The models achieve state-of-the-art performance across multiple tissue categories and serve as strong benchmarks for future digital pathology research. Beyond patch classification, the model enables rapid identification of significant areas, quantitative tissue metrics, and establishes a foundation for multimodal approaches.   Both the dataset and trained models are publicly available to advance research, reproducibility, and AI-driven pathology development. Access them at: https://github.com/HistAI/SPIDER', 'score': 4, 'issue_id': 2532, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a85b69f61f2f377f', 'authors': ['Dmitry Nechaev', 'Alexey Pchelnikov', 'Ekaterina Ivanova'], 'affiliations': ['HistAI'], 'pdf_title_img': 'assets/pdf/title_img/2503.02876.jpg', 'data': {'categories': ['#open_source', '#dataset', '#multimodal', '#science', '#benchmark'], 'emoji': '🕷️', 'ru': {'title': 'SPIDER: Новый стандарт данных для ИИ в патологии', 'desc': 'Статья представляет SPIDER - крупнейший общедоступный набор данных для вычислительной патологии, охватывающий множество типов органов. SPIDER предоставляет высококачественные аннотации, проверенные экспертами-патологами, и включает контекстные патчи для улучшения классификации. Авторы также представляют базовые модели, обученные на SPIDER с использованием модели Hibou-L в качестве экстрактора признаков и классификационной головки на основе механизма внимания. Модели достигают современного уровня производительности в нескольких категориях тканей и служат эталоном для будущих исследований в области цифровой патологии.'}, 'en': {'title': 'SPIDER: Bridging the Gap in Pathology Datasets for AI Advancement', 'desc': 'This paper introduces SPIDER, a large and diverse dataset designed for computational pathology, addressing the limitations of existing public datasets. SPIDER includes high-quality, expert-verified annotations for various organ types, enhancing the classification of pathology images. The authors also present baseline models that utilize the Hibou-L foundation model and an attention-based classification head, achieving state-of-the-art results in tissue classification. The dataset and models are publicly available, promoting further research and development in AI-driven pathology.'}, 'zh': {'title': '推动病理学的AI进步', 'desc': '本论文介绍了SPIDER（监督病理图像描述库），这是一个涵盖多种器官类型的最大公开补丁级数据集，包括皮肤、结直肠和胸部。该数据集提供了高质量的注释，由专家病理学家验证，并包含周围上下文补丁，以提高分类性能。我们还展示了基于SPIDER训练的基线模型，使用Hibou-L基础模型作为特征提取器，并结合基于注意力的分类头，达到了多种组织类别的最先进性能。该数据集和训练模型均可公开获取，以推动研究、可重复性和基于AI的病理发展。'}}}, {'id': 'https://huggingface.co/papers/2503.00876', 'title': 'Improve Representation for Imbalanced Regression through Geometric Constraints', 'url': 'https://huggingface.co/papers/2503.00876', 'abstract': 'In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classification-based methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In a geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of a hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via a Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometry-based loss functions.', 'score': 3, 'issue_id': 2543, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': '018d350ccd3dd3f5', 'authors': ['Zijian Dong', 'Yilei Wu', 'Chongyao Chen', 'Yingtian Zou', 'Yichi Zhang', 'Juan Helen Zhou'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.00876.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': '🌐', 'ru': {'title': 'Геометрический подход к равномерному представлению в несбалансированной регрессии', 'desc': 'Эта статья исследует проблему равномерности представления данных в задачах регрессии с несбалансированными выборками. Авторы предлагают новый подход, основанный на геометрических принципах, для улучшения равномерности в латентном пространстве. Они вводят две ключевые функции потерь: обволакивающую и гомогенности, которые способствуют равномерному распределению признаков на гиперсфере. Эксперименты на реальных данных подтверждают эффективность предложенного метода для задач регрессии и обучения операторов.'}, 'en': {'title': 'Achieving Uniformity in Imbalanced Regression through Geometric Losses', 'desc': "This paper addresses the challenge of representation learning in imbalanced regression tasks, where the distribution of data points is uneven across different classes. It introduces a novel approach that focuses on achieving uniformity in the latent space by employing two specific loss functions: enveloping and homogeneity. The enveloping loss promotes a uniform distribution of features on the hypersphere's surface, while the homogeneity loss ensures that these features are evenly spaced. The proposed Surrogate-driven Representation Learning (SRL) framework demonstrates the effectiveness of these geometric principles in improving performance on real-world regression tasks."}, 'zh': {'title': '提升不平衡回归的均匀性', 'desc': '在表示学习中，均匀性指的是潜在空间中特征的均匀分布。以往的研究表明，提高均匀性有助于学习代表性不足的类别，但大多数研究集中在分类任务上，而不平衡回归的表示空间尚未被探索。我们的方法通过两个关键损失函数：包络损失和均匀性损失，确保不平衡回归中的潜在空间均匀性。实验结果表明，我们的几何基础损失函数在不平衡回归中具有重要意义，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.02357', 'title': 'Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content', 'url': 'https://huggingface.co/papers/2503.02357', 'abstract': 'Evaluating text-to-vision content hinges on two crucial aspects: visual quality and alignment. While significant progress has been made in developing objective models to assess these dimensions, the performance of such models heavily relies on the scale and quality of human annotations. According to Scaling Law, increasing the number of human-labeled instances follows a predictable pattern that enhances the performance of evaluation models. Therefore, we introduce a comprehensive dataset designed to Evaluate Visual quality and Alignment Level for text-to-vision content (Q-EVAL-100K), featuring the largest collection of human-labeled Mean Opinion Scores (MOS) for the mentioned two aspects. The Q-EVAL-100K dataset encompasses both text-to-image and text-to-video models, with 960K human annotations specifically focused on visual quality and alignment for 100K instances (60K images and 40K videos). Leveraging this dataset with context prompt, we propose Q-Eval-Score, a unified model capable of evaluating both visual quality and alignment with special improvements for handling long-text prompt alignment. Experimental results indicate that the proposed Q-Eval-Score achieves superior performance on both visual quality and alignment, with strong generalization capabilities across other benchmarks. These findings highlight the significant value of the Q-EVAL-100K dataset. Data and codes will be available at https://github.com/zzc-1998/Q-Eval.', 'score': 3, 'issue_id': 2541, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a9bc65d32288d8f5', 'authors': ['Zicheng Zhang', 'Tengchuan Kou', 'Shushi Wang', 'Chunyi Li', 'Wei Sun', 'Wei Wang', 'Xiaoyu Li', 'Zongyu Wang', 'Xuezhi Cao', 'Xiongkuo Min', 'Xiaohong Liu', 'Guangtao Zhai'], 'affiliations': ['Meituan', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02357.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#long_context', '#dataset', '#multimodal', '#cv', '#video'], 'emoji': '🔍', 'ru': {'title': 'Большие данные для лучшей оценки генеративных моделей', 'desc': 'Статья описывает создание большого набора данных Q-EVAL-100K для оценки качества и соответствия контента, сгенерированного моделями text-to-image и text-to-video. На основе этого набора данных авторы разработали модель Q-Eval-Score для автоматической оценки визуального качества и соответствия запросу. Модель показала высокую эффективность и обобщающую способность на различных бенчмарках. Исследование подчеркивает важность масштабных размеченных человеком данных для улучшения оценочных моделей в области генерации изображений и видео по текстовому описанию.'}, 'en': {'title': 'Enhancing Text-to-Vision Evaluation with Q-EVAL-100K', 'desc': 'This paper discusses the importance of evaluating text-to-vision content based on visual quality and alignment. It highlights the role of human annotations in improving the performance of evaluation models, following the Scaling Law principle. The authors introduce a new dataset called Q-EVAL-100K, which contains a large number of human-labeled Mean Opinion Scores for both text-to-image and text-to-video models. They also present Q-Eval-Score, a model that effectively assesses visual quality and alignment, demonstrating strong performance and generalization across various benchmarks.'}, 'zh': {'title': '提升文本到视觉内容评估的质量与对齐度', 'desc': '本文探讨了文本到视觉内容评估的两个关键方面：视觉质量和对齐度。尽管已有客观模型在这两个维度上取得了显著进展，但这些模型的性能依赖于人类标注的规模和质量。我们引入了一个全面的数据集Q-EVAL-100K，包含960K个人类标注的平均意见分数（MOS），用于评估文本到图像和文本到视频模型的视觉质量和对齐度。通过利用该数据集，我们提出了Q-Eval-Score模型，能够有效评估视觉质量和对齐度，并在处理长文本提示对齐方面进行了特别改进。'}}}, {'id': 'https://huggingface.co/papers/2503.02783', 'title': 'IterPref: Focal Preference Learning for Code Generation via Iterative Debugging', 'url': 'https://huggingface.co/papers/2503.02783', 'abstract': 'Preference learning enhances Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons. Existing methods construct preference pairs from   candidates based on test case success, treating the higher pass rate sample as positive and the lower as negative. However, this approach does not pinpoint specific errors in the code, which prevents the model from learning more informative error correction patterns, as aligning failing code as a whole lacks the granularity needed to capture meaningful error-resolution relationships. To address these issues, we propose IterPref, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. IterPref explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To generate informative pairs, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with IterPref achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our code and data will be made publicaly available.', 'score': 3, 'issue_id': 2540, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '3d7ced41646a5f95', 'authors': ['Jie Wu', 'Haoling Li', 'Xin Zhang', 'Jianwen Luo', 'Yangyu Huang', 'Ruihang Chu', 'Yujiu Yang', 'Scarlett Li'], 'affiliations': ['CASIA', 'Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02783.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#open_source', '#dataset', '#optimization', '#training'], 'emoji': '🔧', 'ru': {'title': 'Итеративное обучение предпочтениям для усовершенствования языковых моделей кода', 'desc': 'Статья представляет новый метод IterPref для улучшения языковых моделей кода (Code LLMs) с помощью обучения предпочтениям. В отличие от существующих подходов, IterPref точно определяет области ошибок в коде и выравнивает соответствующие токены с использованием специального алгоритма DPO. Для генерации информативных пар авторы создали набор данных CodeFlow, где образцы кода итеративно улучшаются до прохождения тестов. Эксперименты показывают, что IterPref значительно повышает производительность Code LLMs в генерации кода и улучшает результаты на сложных задачах.'}, 'en': {'title': 'Iterative Preference Learning for Enhanced Code LLMs', 'desc': 'This paper introduces IterPref, a novel framework for enhancing Code LLMs (Language Models) through preference learning. Unlike traditional methods that simply compare pass rates of code samples, IterPref focuses on identifying specific error regions in code, allowing for more precise learning of error correction patterns. By utilizing a tailored DPO (Direct Preference Optimization) algorithm and the CodeFlow dataset, which iteratively refines code samples, the framework generates informative preference pairs that improve model training. Experimental results demonstrate that Code LLMs using IterPref show significant performance improvements in code generation tasks and exhibit fewer errors overall.'}, 'zh': {'title': '迭代调试，提升代码生成能力！', 'desc': '本论文提出了一种新的偏好对齐框架IterPref，旨在通过模拟人类的迭代调试过程来提升代码大语言模型（Code LLMs）的性能。现有方法通过测试用例的成功率构建偏好对，但未能准确定位代码中的具体错误，限制了模型学习有效的错误修正模式。IterPref通过定制的DPO算法明确定位错误区域，并对相应的标记进行对齐，从而生成更具信息量的偏好对。实验结果表明，使用IterPref的多样化代码大语言模型在代码生成和复杂任务上均取得了显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.02268', 'title': 'AppAgentX: Evolving GUI Agents as Proficient Smartphone Users', 'url': 'https://huggingface.co/papers/2503.02268', 'abstract': "Recent advancements in Large Language Models (LLMs) have led to the development of intelligent LLM-based agents capable of interacting with graphical user interfaces (GUIs). These agents demonstrate strong reasoning and adaptability, enabling them to perform complex tasks that traditionally required predefined rules. However, the reliance on step-by-step reasoning in LLM-based agents often results in inefficiencies, particularly for routine tasks. In contrast, traditional rule-based systems excel in efficiency but lack the intelligence and flexibility to adapt to novel scenarios. To address this challenge, we propose a novel evolutionary framework for GUI agents that enhances operational efficiency while retaining intelligence and flexibility. Our approach incorporates a memory mechanism that records the agent's task execution history. By analyzing this history, the agent identifies repetitive action sequences and evolves high-level actions that act as shortcuts, replacing these low-level operations and improving efficiency. This allows the agent to focus on tasks requiring more complex reasoning, while simplifying routine actions. Experimental results on multiple benchmark tasks demonstrate that our approach significantly outperforms existing methods in both efficiency and accuracy. The code will be open-sourced to support further research.", 'score': 3, 'issue_id': 2536, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '03199cb797dc8d88', 'authors': ['Wenjia Jiang', 'Yangyang Zhuang', 'Chenxi Song', 'Xu Yang', 'Chi Zhang'], 'affiliations': ['Henan University', 'Southeast University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.02268.jpg', 'data': {'categories': ['#optimization', '#agents', '#benchmark', '#reasoning', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эволюционное обучение агентов GUI: баланс эффективности и гибкости', 'desc': 'Эта статья представляет новый эволюционный подход для агентов, работающих с графическим интерфейсом на основе больших языковых моделей (LLM). Авторы предлагают механизм памяти, который записывает историю выполнения задач агентом и анализирует ее для выявления повторяющихся последовательностей действий. На основе этого анализа создаются высокоуровневые действия, которые заменяют низкоуровневые операции, повышая эффективность работы. Экспериментальные результаты показывают, что данный метод значительно превосходит существующие подходы по эффективности и точности.'}, 'en': {'title': 'Evolving Efficiency: Smart Shortcuts for GUI Agents', 'desc': 'This paper presents a new framework for Large Language Model (LLM)-based agents that interact with graphical user interfaces (GUIs). The proposed method enhances the efficiency of these agents by incorporating a memory mechanism that tracks their task execution history. By analyzing this history, the agents can identify repetitive actions and evolve high-level shortcuts, allowing them to streamline routine tasks. This approach maintains the intelligence and adaptability of LLMs while improving their operational efficiency, as demonstrated by experimental results on benchmark tasks.'}, 'zh': {'title': '智能代理的进化：提升效率与灵活性', 'desc': '最近，大型语言模型（LLMs）的进步使得基于LLM的智能代理能够与图形用户界面（GUIs）进行交互。这些代理展现出强大的推理能力和适应性，能够执行传统上需要预定义规则的复杂任务。然而，基于LLM的代理在依赖逐步推理时，往往在处理常规任务时效率较低。为了解决这个问题，我们提出了一种新颖的进化框架，通过记录代理的任务执行历史，识别重复的操作序列，从而演化出高层次的快捷操作，提升了操作效率，同时保留了智能和灵活性。'}}}, {'id': 'https://huggingface.co/papers/2503.02812', 'title': 'Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression', 'url': 'https://huggingface.co/papers/2503.02812', 'abstract': 'Autoregressive language models rely on a Key-Value (KV) Cache, which avoids re-computing past hidden states during generation, making it faster. As model sizes and context lengths grow, the KV Cache becomes a significant memory bottleneck, which calls for compression methods that limit its size during generation. In this paper, we discover surprising properties of Query (Q) and Key (K) vectors that allow us to efficiently approximate attention scores without computing the attention maps. We propose Q-Filters, a training-free KV Cache compression method that filters out less crucial Key-Value pairs based on a single context-agnostic projection. Contrarily to many alternatives, Q-Filters is compatible with FlashAttention, as it does not require direct access to attention weights. Experimental results in long-context settings demonstrate that Q-Filters is competitive with attention-based compression methods such as SnapKV in retrieval tasks while consistently outperforming efficient compression schemes such as Streaming-LLM in generation setups. Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task with a x32 compression level while reducing the generation perplexity drop by up to 65% in text generation compared to Streaming-LLM.', 'score': 2, 'issue_id': 2546, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '9dc95b6ce0d4b6ca', 'authors': ['Nathan Godey', 'Alessio Devoto', 'Yu Zhao', 'Simone Scardapane', 'Pasquale Minervini', 'Éric de la Clergerie', 'Benoît Sagot'], 'affiliations': ['Miniml.AI', 'Sapienza University of Rome', 'Sorbonne Université, Paris, France', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.02812.jpg', 'data': {'categories': ['#inference', '#long_context', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Q-Filters: Эффективное сжатие KV-кэша без компромиссов в производительности', 'desc': 'Статья представляет новый метод сжатия KV-кэша для авторегрессивных языковых моделей под названием Q-Filters. Этот метод основан на обнаруженных свойствах векторов запросов (Q) и ключей (K), позволяющих эффективно аппроксимировать оценки внимания без вычисления карт внимания. Q-Filters фильтрует менее важные пары ключ-значение на основе одной контекстно-независимой проекции, что делает его совместимым с FlashAttention. Экспериментальные результаты показывают, что Q-Filters превосходит другие методы сжатия в задачах генерации текста и извлечения информации при длинных контекстах.'}, 'en': {'title': 'Efficient KV Cache Compression with Q-Filters', 'desc': 'This paper addresses the memory limitations of autoregressive language models caused by the Key-Value (KV) Cache during text generation. It introduces Q-Filters, a novel method that compresses the KV Cache by filtering out less important Key-Value pairs without needing to compute attention maps. The method is efficient and compatible with existing techniques like FlashAttention, making it a practical solution for large models. Experimental results show that Q-Filters not only competes well with other compression methods but also significantly improves performance in text generation tasks.'}, 'zh': {'title': '高效压缩：Q-Filters的创新之路', 'desc': '自回归语言模型依赖于键值缓存（KV Cache），以避免在生成过程中重新计算过去的隐藏状态，从而提高速度。随着模型规模和上下文长度的增加，KV Cache 成为一个重要的内存瓶颈，因此需要压缩方法来限制其大小。本文发现了查询（Q）和键（K）向量的意外特性，使我们能够在不计算注意力图的情况下有效地近似注意力分数。我们提出了 Q-Filters，这是一种无训练的 KV Cache 压缩方法，通过单一的上下文无关投影过滤掉不太重要的键值对。'}}}, {'id': 'https://huggingface.co/papers/2503.02823', 'title': 'A Multimodal Symphony: Integrating Taste and Sound through Generative AI', 'url': 'https://huggingface.co/papers/2503.02823', 'abstract': "In recent decades, neuroscientific and psychological research has traced direct relationships between taste and auditory perceptions. This article explores multimodal generative models capable of converting taste information into music, building on this foundational research. We provide a brief review of the state of the art in this field, highlighting key findings and methodologies. We present an experiment in which a fine-tuned version of a generative music model (MusicGEN) is used to generate music based on detailed taste descriptions provided for each musical piece. The results are promising: according the participants' (n=111) evaluation, the fine-tuned model produces music that more coherently reflects the input taste descriptions compared to the non-fine-tuned model. This study represents a significant step towards understanding and developing embodied interactions between AI, sound, and taste, opening new possibilities in the field of generative AI. We release our dataset, code and pre-trained model at: https://osf.io/xs5jy/.", 'score': 2, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '841602ca85525c24', 'authors': ['Matteo Spanio', 'Massimiliano Zampini', 'Antonio Rodà', 'Franco Pierucci'], 'affiliations': ['Center for Mind/Brain Sciences (CIMeC) University of Trento Rovereto, Italy', 'Centro di Sonologia Computazionale (CSC) Department of Information Engineering University of Padova Padova, Italy', 'SoundFood s.r.l. Terni, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.02823.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#games', '#dataset'], 'emoji': '🎵', 'ru': {'title': 'От вкуса к мелодии: ИИ преобразует гастрономические ощущения в музыку', 'desc': 'Статья исследует генеративные модели, способные преобразовывать информацию о вкусе в музыку. Авторы провели эксперимент с использованием дообученной версии модели MusicGEN для генерации музыки на основе описаний вкусов. Результаты показали, что дообученная модель создает музыку, более согласованную с входными описаниями вкусов по сравнению с базовой моделью. Это исследование открывает новые возможности в области генеративного искусственного интеллекта и мультимодального восприятия.'}, 'en': {'title': 'Transforming Taste into Sound: A New Frontier in Generative AI', 'desc': 'This paper investigates the connection between taste and sound by using multimodal generative models. It focuses on a specific model called MusicGEN, which has been fine-tuned to create music that corresponds to taste descriptions. An experiment with 111 participants showed that the fine-tuned model produced music that better matched the taste inputs than the original model. This research advances the understanding of how AI can create embodied experiences that link different sensory modalities, particularly taste and sound.'}, 'zh': {'title': '味觉与音乐的奇妙结合', 'desc': '近年来，神经科学和心理学研究发现味觉与听觉之间存在直接关系。本文探讨了多模态生成模型，能够将味觉信息转换为音乐，基于这一基础研究进行深入分析。我们回顾了该领域的最新进展，强调了关键发现和方法论。实验结果表明，经过微调的生成音乐模型（MusicGEN）能够更好地反映输入的味觉描述，展示了AI、声音与味觉之间的互动新可能。'}}}, {'id': 'https://huggingface.co/papers/2503.02152', 'title': 'Tabby: Tabular Data Synthesis with Language Models', 'url': 'https://huggingface.co/papers/2503.02152', 'abstract': 'While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received relatively less attention. We address this disparity with Tabby, a simple but powerful post-training modification to the standard Transformer language model architecture, enabling its use for tabular dataset synthesis. Tabby enables the representation of differences across columns using Gated Mixture-of-Experts, with column-specific sets of parameters. Empirically, Tabby results in data quality near or equal to that of real data. By pairing our novel LLM table training technique, Plain, with Tabby, we observe up to a 44% improvement in quality over previous methods. We also show that Tabby extends beyond tables to more general structured data, reaching parity with real data on a nested JSON dataset as well.', 'score': 1, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d33263eb28e0be2', 'authors': ['Sonia Cromp', 'Satya Sai Srinath Namburi GNVV', 'Mohammed Alkhudhayri', 'Catherine Cao', 'Samuel Guo', 'Nicholas Roberts', 'Frederic Sala'], 'affiliations': ['GE HealthCare', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.02152.jpg', 'data': {'categories': ['#dataset', '#synthetic', '#architecture', '#data'], 'emoji': '📊', 'ru': {'title': 'Tabby: прорыв в синтезе табличных данных с помощью языковых моделей', 'desc': 'Исследователи представили Tabby - модификацию архитектуры трансформера для синтеза табличных данных. Tabby использует Gated Mixture-of-Experts для представления различий между столбцами с помощью специфичных для каждого столбца параметров. В сочетании с новой техникой обучения Plain, Tabby показывает улучшение качества синтетических данных до 44% по сравнению с предыдущими методами. Модель также эффективна для работы с вложенными JSON-данными.'}, 'en': {'title': 'Tabby: Transforming Tabular Data Synthesis with LLMs', 'desc': 'This paper introduces Tabby, a new method for synthesizing tabular data using a modified Transformer language model. It employs Gated Mixture-of-Experts to effectively capture the unique characteristics of different columns in a dataset. The results show that Tabby can generate synthetic data that is comparable in quality to real data, achieving significant improvements over existing techniques. Additionally, Tabby is versatile enough to be applied to other structured data formats, such as nested JSON, demonstrating its broad applicability in data synthesis tasks.'}, 'zh': {'title': 'Tabby：表格数据合成的新突破', 'desc': '本文介绍了一种名为Tabby的方法，用于合成表格数据。Tabby是对标准Transformer语言模型架构的后训练修改，能够有效表示列之间的差异。通过使用门控混合专家模型，Tabby为每一列提供特定的参数集，从而提高数据质量。实验表明，Tabby在合成数据的质量上接近或等同于真实数据，并且在处理嵌套JSON数据集时也表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.02304', 'title': 'A Token-level Text Image Foundation Model for Document Understanding', 'url': 'https://huggingface.co/papers/2503.02304', 'abstract': 'In recent years, general visual foundation models (VFMs) have witnessed increasing adoption, particularly as image encoders for popular multi-modal large language models (MLLMs). However, without semantically fine-grained supervision, these models still encounter fundamental prediction errors in the context of downstream text-image-related tasks, i.e., perception, understanding and reasoning with images containing small and dense texts. To bridge this gap, we develop TokenOCR, the first token-level visual foundation model specifically tailored for text-image-related tasks, designed to support a variety of traditional downstream applications. To facilitate the pretraining of TokenOCR, we also devise a high-quality data production pipeline that constructs the first token-level image text dataset, TokenIT, comprising 20 million images and 1.8 billion token-mask pairs. Furthermore, leveraging this foundation with exceptional image-as-text capability, we seamlessly replace previous VFMs with TokenOCR to construct a document-level MLLM, TokenVL, for VQA-based document understanding tasks. Finally, extensive experiments demonstrate the effectiveness of TokenOCR and TokenVL. Code, datasets, and weights will be available at https://token-family.github.io/TokenOCR_project.', 'score': 1, 'issue_id': 2545, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'b275d8ba26cca4b5', 'authors': ['Tongkun Guan', 'Zining Wang', 'Pei Fu', 'Zhengtao Guo', 'Wei Shen', 'Kai Zhou', 'Tiezhu Yue', 'Chen Duan', 'Hao Sun', 'Qianyi Jiang', 'Junfeng Luo', 'Xiaokang Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.02304.jpg', 'data': {'categories': ['#dataset', '#cv', '#agi', '#reasoning', '#optimization', '#multimodal', '#games', '#data', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'TokenOCR: Новый уровень распознавания текста на изображениях', 'desc': 'Исследователи разработали TokenOCR - первую визуальную фундаментальную модель на уровне токенов для задач, связанных с текстом на изображениях. Для предобучения модели создан набор данных TokenIT, содержащий 20 миллионов изображений и 1,8 миллиарда пар токен-маска. На основе TokenOCR построена мультимодальная языковая модель TokenVL для задач понимания документов. Эксперименты подтверждают эффективность предложенных моделей TokenOCR и TokenVL.'}, 'en': {'title': 'TokenOCR: Bridging Text and Image Understanding', 'desc': 'This paper introduces TokenOCR, a novel visual foundation model designed specifically for tasks that involve understanding and reasoning about images with dense text. Traditional visual foundation models struggle with these tasks due to a lack of detailed supervision, leading to prediction errors. TokenOCR addresses this issue by utilizing a new dataset, TokenIT, which contains 20 million images and 1.8 billion token-mask pairs for effective pretraining. The model is integrated into a document-level multi-modal large language model, TokenVL, enhancing its capabilities for visual question answering and document understanding tasks.'}, 'zh': {'title': 'TokenOCR：文本图像任务的视觉基础模型新突破', 'desc': '近年来，通用视觉基础模型（VFM）在多模态大语言模型（MLLM）中得到了广泛应用，尤其作为图像编码器。然而，在没有细粒度语义监督的情况下，这些模型在处理与文本相关的图像任务时仍然会出现基本的预测错误。为了解决这个问题，我们开发了TokenOCR，这是第一个专门针对文本-图像相关任务的标记级视觉基础模型，并设计了一个高质量的数据生产管道，构建了包含2000万张图像和18亿个标记-掩码对的TokenIT数据集。通过这一基础，我们成功地用TokenOCR替换了之前的VFM，构建了用于文档理解任务的TokenVL文档级MLLM，实验结果证明了TokenOCR和TokenVL的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.00200', 'title': 'Unified Video Action Model', 'url': 'https://huggingface.co/papers/2503.00200', 'abstract': 'A unified video and action model holds significant promise for robotics, where videos provide rich scene information for action prediction, and actions provide dynamics information for video prediction. However, effectively combining video generation and action prediction remains challenging, and current video generation-based methods struggle to match the performance of direct policy learning in action accuracy and inference speed. To bridge this gap, we introduce the Unified Video Action model (UVA), which jointly optimizes video and action predictions to achieve both high accuracy and efficient action inference. The key lies in learning a joint video-action latent representation and decoupling video-action decoding. The joint latent representation bridges the visual and action domains, effectively modeling the relationship between video and action sequences. Meanwhile, the decoupled decoding, powered by two lightweight diffusion heads, enables high-speed action inference by bypassing video generation during inference. Such a unified framework further enables versatile functionality through masked input training. By selectively masking actions or videos, a single model can tackle diverse tasks beyond policy learning, such as forward and inverse dynamics modeling and video generation. Via an extensive set of experiments, we demonstrate that UVA can serve as a general-purpose solution for a wide range of robotics tasks, such as policy learning, forward/inverse dynamics and video observation prediction, without compromising performance compared to methods tailored for specific applications. Results are best viewed on https://unified-video-action-model.github.io/.', 'score': 0, 'issue_id': 2551, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '286c6abc07fdf95e', 'authors': ['Shuang Li', 'Yihuai Gao', 'Dorsa Sadigh', 'Shuran Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.00200.jpg', 'data': {'categories': ['#robotics', '#video', '#games', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Единая модель для видео и действий: новый подход к робототехнике', 'desc': 'Модель UVA (Unified Video Action) представляет собой единую систему для видео и действий в робототехнике. Она объединяет генерацию видео и предсказание действий, используя совместное латентное представление и раздельное декодирование. UVA обеспечивает высокую точность и эффективность вывода действий, избегая генерации видео во время вывода. Модель применима к различным задачам робототехники, включая обучение политик, прямую и обратную динамику, и предсказание видеонаблюдений.'}, 'en': {'title': 'Bridging Video and Action for Smarter Robotics', 'desc': "The Unified Video Action model (UVA) integrates video generation and action prediction to enhance robotics applications. It achieves this by creating a joint latent representation that connects visual data with action sequences, allowing for better modeling of their interrelationship. The model employs decoupled decoding with lightweight diffusion heads, which speeds up action inference by avoiding the need for video generation during this process. UVA's versatility is further demonstrated through masked input training, enabling it to perform various tasks like policy learning and dynamics modeling efficiently."}, 'zh': {'title': '统一视频与动作模型：提升机器人智能的关键', 'desc': '本文提出了一种统一的视频与动作模型（UVA），旨在提高机器人领域中的动作预测和视频生成的性能。UVA通过联合优化视频和动作预测，学习视频与动作的联合潜在表示，从而有效建模视频与动作序列之间的关系。该模型采用解耦解码的方法，利用轻量级的扩散头实现快速的动作推理，避免了在推理过程中生成视频的需求。通过掩蔽输入训练，UVA能够处理多种任务，如策略学习、前向/逆向动力学建模和视频生成，展现出广泛的适用性。'}}}, {'id': 'https://huggingface.co/papers/2503.01842', 'title': 'Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets Skateboarding', 'url': 'https://huggingface.co/papers/2503.01842', 'abstract': 'This paper introduces Discrete-time Hybrid Automata Learning (DHAL), a framework using on-policy Reinforcement Learning to identify and execute mode-switching without trajectory segmentation or event function learning. Hybrid dynamical systems, which include continuous flow and discrete mode switching, can model robotics tasks like legged robot locomotion. Model-based methods usually depend on predefined gaits, while model-free approaches lack explicit mode-switching knowledge. Current methods identify discrete modes via segmentation before regressing continuous flow, but learning high-dimensional complex rigid body dynamics without trajectory labels or segmentation is a challenging open problem. Our approach incorporates a beta policy distribution and a multi-critic architecture to model contact-guided motions, exemplified by a challenging quadrupedal robot skateboard task. We validate our method through simulations and real-world tests, demonstrating robust performance in hybrid dynamical systems.', 'score': 0, 'issue_id': 2545, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '07eab4aa91f88137', 'authors': ['Hang Liu', 'Sangli Teng', 'Ben Liu', 'Wei Zhang', 'Maani Ghaffari'], 'affiliations': ['Southern University of Science and Technology', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2503.01842.jpg', 'data': {'categories': ['#games', '#rl', '#robotics', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Обучение гибридным системам без сегментации траекторий', 'desc': 'Статья представляет DHAL - фреймворк для обучения гибридным автоматам с дискретным временем, использующий обучение с подкреплением для идентификации и выполнения переключения режимов. Метод применяет бета-распределение политики и архитектуру с несколькими критиками для моделирования движений, управляемых контактами. DHAL не требует сегментации траекторий или предварительного обучения функций событий. Эффективность подхода продемонстрирована на сложной задаче управления четвероногим роботом на скейтборде в симуляции и реальном мире.'}, 'en': {'title': 'Learning Mode-Switching in Robotics Without Segmentation', 'desc': 'This paper presents Discrete-time Hybrid Automata Learning (DHAL), a novel framework that leverages on-policy Reinforcement Learning to effectively manage mode-switching in hybrid dynamical systems. Unlike traditional methods that require trajectory segmentation or predefined gaits, DHAL learns to identify and execute mode transitions directly from the data. The approach utilizes a beta policy distribution and a multi-critic architecture to handle complex contact-guided motions, particularly in tasks like quadrupedal robot locomotion. The results show that DHAL can robustly learn high-dimensional dynamics without the need for explicit trajectory labels, making it a significant advancement in the field.'}, 'zh': {'title': '无轨迹分割的智能模式切换学习', 'desc': '本文介绍了一种离散时间混合自动机学习（DHAL）框架，利用基于策略的强化学习来识别和执行模式切换，而无需进行轨迹分割或事件函数学习。混合动态系统能够模拟机器人任务，例如四足机器人行走。传统的模型方法通常依赖于预定义的步态，而无模型的方法则缺乏明确的模式切换知识。我们的方法通过引入贝塔策略分布和多重评论家架构，成功地建模了接触引导的运动，并在仿真和实际测试中验证了其在混合动态系统中的强大性能。'}}}, {'id': 'https://huggingface.co/papers/2503.06053', 'title': 'DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal\n  Consistent Video Generation', 'url': 'https://huggingface.co/papers/2503.06053', 'abstract': 'Spatio-temporal consistency is a critical research topic in video generation. A qualified generated video segment must ensure plot plausibility and coherence while maintaining visual consistency of objects and scenes across varying viewpoints. Prior research, especially in open-source projects, primarily focuses on either temporal or spatial consistency, or their basic combination, such as appending a description of a camera movement after a prompt without constraining the outcomes of this movement. However, camera movement may introduce new objects to the scene or eliminate existing ones, thereby overlaying and affecting the preceding narrative. Especially in videos with numerous camera movements, the interplay between multiple plots becomes increasingly complex. This paper introduces and examines integral spatio-temporal consistency, considering the synergy between plot progression and camera techniques, and the long-term impact of prior content on subsequent generation. Our research encompasses dataset construction through to the development of the model. Initially, we constructed a DropletVideo-10M dataset, which comprises 10 million videos featuring dynamic camera motion and object actions. Each video is annotated with an average caption of 206 words, detailing various camera movements and plot developments. Following this, we developed and trained the DropletVideo model, which excels in preserving spatio-temporal coherence during video generation. The DropletVideo dataset and model are accessible at https://dropletx.github.io.', 'score': 72, 'issue_id': 2758, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': 'c6a544d3dc36bfbf', 'authors': ['Runze Zhang', 'Guoguang Du', 'Xiaochuan Li', 'Qi Jia', 'Liang Jin', 'Lu Liu', 'Jingjing Wang', 'Cong Xu', 'Zhenhua Guo', 'Yaqian Zhao', 'Xiaoli Gong', 'Rengang Li', 'Baoyu Fan'], 'affiliations': ['IEIT System Co., Ltd.', 'Nankai University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06053.jpg', 'data': {'categories': ['#open_source', '#dataset', '#video', '#training'], 'emoji': '🎥', 'ru': {'title': 'Интегральная пространственно-временная согласованность в генерации видео', 'desc': 'Статья посвящена проблеме пространственно-временной согласованности в генерации видео. Авторы представляют новый подход, учитывающий взаимосвязь между развитием сюжета и движениями камеры, а также долгосрочное влияние предыдущего контента на последующую генерацию. Для исследования был создан датасет DropletVideo-10M, содержащий 10 миллионов видео с динамическим движением камеры и действиями объектов. На основе этих данных была разработана и обучена модель DropletVideo, способная сохранять пространственно-временную согласованность при генерации видео.'}, 'en': {'title': 'Achieving Seamless Video Generation with Spatio-Temporal Consistency', 'desc': 'This paper addresses the challenge of spatio-temporal consistency in video generation, which is essential for creating coherent and visually consistent narratives. It highlights the limitations of previous research that often focuses on either temporal or spatial aspects without integrating them effectively. The authors introduce a new dataset, DropletVideo-10M, containing 10 million videos with dynamic camera movements and detailed annotations, which aids in training their model. The proposed DropletVideo model demonstrates improved performance in maintaining coherence across both plot progression and camera techniques, ensuring a more seamless video generation experience.'}, 'zh': {'title': '提升视频生成的时空一致性', 'desc': '本论文研究了视频生成中的时空一致性问题。生成的视频片段需要在情节合理性和视觉一致性之间取得平衡，尤其是在不同视角下的物体和场景。以往的研究主要关注时间或空间一致性，缺乏对两者的综合考虑。我们提出了整体时空一致性的方法，构建了包含1000万段动态镜头和物体动作的视频数据集，并开发了DropletVideo模型，以提高视频生成的时空连贯性。'}}}, {'id': 'https://huggingface.co/papers/2503.12533', 'title': 'Being-0: A Humanoid Robotic Agent with Vision-Language Models and\n  Modular Skills', 'url': 'https://huggingface.co/papers/2503.12533', 'abstract': "Building autonomous robotic agents capable of achieving human-level performance in real-world embodied tasks is an ultimate goal in humanoid robot research. Recent advances have made significant progress in high-level cognition with Foundation Models (FMs) and low-level skill development for humanoid robots. However, directly combining these components often results in poor robustness and efficiency due to compounding errors in long-horizon tasks and the varied latency of different modules. We introduce Being-0, a hierarchical agent framework that integrates an FM with a modular skill library. The FM handles high-level cognitive tasks such as instruction understanding, task planning, and reasoning, while the skill library provides stable locomotion and dexterous manipulation for low-level control. To bridge the gap between these levels, we propose a novel Connector module, powered by a lightweight vision-language model (VLM). The Connector enhances the FM's embodied capabilities by translating language-based plans into actionable skill commands and dynamically coordinating locomotion and manipulation to improve task success. With all components, except the FM, deployable on low-cost onboard computation devices, Being-0 achieves efficient, real-time performance on a full-sized humanoid robot equipped with dexterous hands and active vision. Extensive experiments in large indoor environments demonstrate Being-0's effectiveness in solving complex, long-horizon tasks that require challenging navigation and manipulation subtasks. For further details and videos, visit https://beingbeyond.github.io/being-0.", 'score': 49, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '18b42d1274f6b260', 'authors': ['Haoqi Yuan', 'Yu Bai', 'Yuhui Fu', 'Bohan Zhou', 'Yicheng Feng', 'Xinrun Xu', 'Yi Zhan', 'Börje F. Karlsson', 'Zongqing Lu'], 'affiliations': ['BAAI', 'BeingBeyond', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12533.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#robotics', '#agi', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Being-0: Мост между AI и роботом для решения сложных задач в реальном мире', 'desc': 'Статья представляет Being-0 - иерархическую систему для автономных роботов-гуманоидов, объединяющую фундаментальную модель (ФМ) для высокоуровневых когнитивных задач с библиотекой низкоуровневых навыков. Ключевым элементом является модуль Connector на основе легковесной мультимодальной модели, связывающий ФМ с исполнительными механизмами робота. Система Being-0 способна эффективно решать сложные долгосрочные задачи, требующие навигации и манипуляций, работая в режиме реального времени на недорогом бортовом оборудовании. Эксперименты показали эффективность Being-0 в решении комплексных задач в больших помещениях.'}, 'en': {'title': 'Bridging High-Level Cognition and Low-Level Skills in Humanoid Robots', 'desc': "This paper presents Being-0, a hierarchical framework designed to enhance humanoid robots' performance in complex tasks. It combines a Foundation Model (FM) for high-level cognitive functions with a modular skill library for low-level control, addressing issues of robustness and efficiency. A novel Connector module, utilizing a lightweight vision-language model, translates language-based plans into actionable commands, improving coordination between locomotion and manipulation. The system demonstrates effective real-time performance on humanoid robots in challenging environments, showcasing its ability to tackle long-horizon tasks successfully."}, 'zh': {'title': '提升类人机器人智能的层次化框架', 'desc': '本文介绍了一种名为Being-0的层次化智能体框架，旨在提升类人机器人在现实世界任务中的表现。该框架将基础模型（FM）与模块化技能库相结合，FM负责高层次的认知任务，如指令理解和任务规划，而技能库则提供稳定的运动和灵巧操作。为了连接这两个层次，本文提出了一种新颖的连接模块，利用轻量级的视觉-语言模型（VLM）将语言计划转化为可执行的技能命令。通过在低成本的计算设备上部署大部分组件，Being-0在复杂的长时间任务中展现出高效的实时性能。'}}}, {'id': 'https://huggingface.co/papers/2503.12885', 'title': 'DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale\n  Text-to-Image Models', 'url': 'https://huggingface.co/papers/2503.12885', 'abstract': 'Image-conditioned generation methods, such as depth- and canny-conditioned approaches, have demonstrated remarkable abilities for precise image synthesis. However, existing models still struggle to accurately control the content of multiple instances (or regions). Even state-of-the-art models like FLUX and 3DIS face challenges, such as attribute leakage between instances, which limits user control. To address these issues, we introduce DreamRenderer, a training-free approach built upon the FLUX model. DreamRenderer enables users to control the content of each instance via bounding boxes or masks, while ensuring overall visual harmony. We propose two key innovations: 1) Bridge Image Tokens for Hard Text Attribute Binding, which uses replicated image tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely on text data, bind the correct visual attributes for each instance during Joint Attention; 2) Hard Image Attribute Binding applied only to vital layers. Through our analysis of FLUX, we identify the critical layers responsible for instance attribute rendering and apply Hard Image Attribute Binding only in these layers, using soft binding in the others. This approach ensures precise control while preserving image quality. Evaluations on the COCO-POS and COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models like GLIGEN and 3DIS by up to 26.8%. Project Page: https://limuloo.github.io/DreamRenderer/.', 'score': 34, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'e6332652493dc1ab', 'authors': ['Dewei Zhou', 'Mingwei Li', 'Zongxin Yang', 'Yi Yang'], 'affiliations': ['DBMI, HMS, Harvard University', 'RELER, CCAI, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12885.jpg', 'data': {'categories': ['#optimization', '#cv', '#training', '#games', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Точный контроль над множественными объектами при генерации изображений', 'desc': 'DreamRenderer - это новый подход к генерации изображений по условиям, построенный на основе модели FLUX. Он позволяет точно контролировать содержимое нескольких объектов на изображении с помощью ограничивающих рамок или масок. Ключевые инновации включают использование мостовых токенов изображения для жесткой привязки текстовых атрибутов и применение жесткой привязки атрибутов изображения только в критически важных слоях. Эксперименты показали значительное улучшение качества генерации по сравнению с существующими методами.'}, 'en': {'title': 'DreamRenderer: Precise Control in Image Generation', 'desc': 'This paper presents DreamRenderer, a novel approach for image generation that allows precise control over multiple instances in an image. Unlike existing models, DreamRenderer uses a training-free method that leverages the FLUX model to bind visual attributes to specific instances through bounding boxes or masks. The key innovations include Bridge Image Tokens for ensuring accurate text-to-image attribute mapping and Hard Image Attribute Binding focused on critical layers for instance rendering. Evaluations show that DreamRenderer significantly outperforms FLUX and enhances other layout-to-image models, demonstrating its effectiveness in generating high-quality images with user-defined content.'}, 'zh': {'title': 'DreamRenderer：精确控制图像生成的创新方法', 'desc': '本文介绍了一种名为DreamRenderer的图像生成方法，旨在解决现有模型在多实例内容控制方面的不足。DreamRenderer基于FLUX模型，允许用户通过边界框或掩码精确控制每个实例的内容，同时保持整体视觉和谐。我们提出了两项关键创新：一是使用桥接图像标记来确保文本属性的准确绑定，二是在关键层中应用硬图像属性绑定，以提高实例属性渲染的精度。实验结果表明，DreamRenderer在图像成功率上比FLUX提高了17.7%，并且在布局到图像模型的性能上提升了多达26.8%。'}}}, {'id': 'https://huggingface.co/papers/2503.12590', 'title': 'Personalize Anything for Free with Diffusion Transformer', 'url': 'https://huggingface.co/papers/2503.12590', 'abstract': 'Personalized image generation aims to produce images of user-specified concepts while enabling flexible editing. Recent training-free approaches, while exhibit higher computational efficiency than training-based methods, struggle with identity preservation, applicability, and compatibility with diffusion transformers (DiTs). In this paper, we uncover the untapped potential of DiT, where simply replacing denoising tokens with those of a reference subject achieves zero-shot subject reconstruction. This simple yet effective feature injection technique unlocks diverse scenarios, from personalization to image editing. Building upon this observation, we propose Personalize Anything, a training-free framework that achieves personalized image generation in DiT through: 1) timestep-adaptive token replacement that enforces subject consistency via early-stage injection and enhances flexibility through late-stage regularization, and 2) patch perturbation strategies to boost structural diversity. Our method seamlessly supports layout-guided generation, multi-subject personalization, and mask-controlled editing. Evaluations demonstrate state-of-the-art performance in identity preservation and versatility. Our work establishes new insights into DiTs while delivering a practical paradigm for efficient personalization.', 'score': 22, 'issue_id': 2754, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '25e3ee07c4a11ed2', 'authors': ['Haoran Feng', 'Zehuan Huang', 'Lin Li', 'Hairong Lv', 'Lu Sheng'], 'affiliations': ['Beihang University', 'Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12590.jpg', 'data': {'categories': ['#cv', '#multimodal', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Персонализация изображений без обучения: новый взгляд на возможности диффузионных трансформеров', 'desc': 'Статья представляет новый подход к персонализированной генерации изображений с использованием диффузионных трансформеров (DiT). Авторы предлагают метод Personalize Anything, который позволяет достичь высокого качества персонализации без дополнительного обучения модели. Ключевые элементы метода включают адаптивную замену токенов и стратегии возмущения патчей для улучшения структурного разнообразия. Метод демонстрирует высокую эффективность в сохранении идентичности и универсальность применения.'}, 'en': {'title': 'Personalize Anything: Efficient Image Generation with Diffusion Transformers', 'desc': 'This paper presents a novel approach to personalized image generation using diffusion transformers (DiTs) without the need for extensive training. The authors introduce a technique that replaces denoising tokens with those from a reference subject, enabling effective zero-shot subject reconstruction. They propose a framework called Personalize Anything, which incorporates adaptive token replacement and patch perturbation strategies to enhance identity preservation and structural diversity. The results show that this method achieves state-of-the-art performance in generating personalized images while allowing for flexible editing options.'}, 'zh': {'title': '个性化图像生成的新视角', 'desc': '个性化图像生成旨在根据用户指定的概念生成图像，并允许灵活编辑。最近的无训练方法在计算效率上优于基于训练的方法，但在身份保持、适用性和与扩散变换器的兼容性方面存在挑战。本文揭示了扩散变换器的潜力，通过简单地用参考对象的去噪令牌替换实现零-shot的对象重建。我们提出的“个性化任何事物”框架，通过时间步自适应令牌替换和补丁扰动策略，实现了高效的个性化图像生成。'}}}, {'id': 'https://huggingface.co/papers/2503.12349', 'title': 'SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?', 'url': 'https://huggingface.co/papers/2503.12349', 'abstract': 'Reasoning and strategic behavior in social interactions is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multi-hop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human--AI teaming.', 'score': 22, 'issue_id': 2765, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '5f862770d0575514', 'authors': ['Jianzhu Yao', 'Kevin Wang', 'Ryan Hsieh', 'Haisu Zhou', 'Tianqing Zou', 'Zerui Cheng', 'Zhangyang Wang', 'Pramod Viswanath'], 'affiliations': ['Department of Electrical and Computer Engineering, Princeton University, New Jersey, US', 'Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, US'], 'pdf_title_img': 'assets/pdf/title_img/2503.12349.jpg', 'data': {'categories': ['#games', '#agents', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'SPIN-Bench: Оценка стратегического и социального интеллекта ИИ', 'desc': 'SPIN-Bench - это новый многодоменный фреймворк для оценки стратегического планирования и социального мышления искусственного интеллекта. Он объединяет классические задачи PDDL, конкурентные настольные игры, кооперативные карточные игры и сценарии многоагентных переговоров. SPIN-Bench систематически варьирует пространства действий, сложность состояний и количество взаимодействующих агентов для симуляции различных социальных ситуаций. Эксперименты показывают, что современные языковые модели справляются с базовым извлечением фактов и краткосрочным планированием, но испытывают трудности с задачами, требующими глубокого многоступенчатого рассуждения и социально адаптивной координации в условиях неопределенности.'}, 'en': {'title': "SPIN-Bench: Evaluating AI's Strategic and Social Intelligence", 'desc': "This paper introduces SPIN-Bench, a new evaluation framework designed to assess AI's ability in strategic planning and social reasoning across various domains. Unlike existing benchmarks that focus on single-agent tasks, SPIN-Bench integrates multiple scenarios, including competitive games and cooperative interactions, to provide a comprehensive testing ground. The framework systematically varies factors like action spaces and the number of agents to simulate complex social environments where success relies on both strategic decision-making and understanding others' behaviors. The findings indicate that while current large language models perform well in basic tasks, they struggle with complex reasoning and coordination in uncertain social contexts."}, 'zh': {'title': '智能推理与战略行为的新基准', 'desc': '本文介绍了一种新的多领域评估工具，称为战略规划、互动与谈判基准（SPIN-Bench），旨在测量战略规划和社会推理的智能水平。与现有的基准不同，SPIN-Bench结合了经典的PDDL任务、竞争性棋盘游戏、合作性纸牌游戏和多智能体谈判场景，提供了一个统一的框架。该框架不仅包括基准测试，还提供了一个模拟和评估各种社会环境的场所，以测试人工智能代理的推理和战略行为。实验结果表明，尽管当前的大型语言模型在基本事实检索和短期规划方面表现良好，但在需要深度多跳推理和不确定性下的社会协调任务中却遇到了显著的性能瓶颈。'}}}, {'id': 'https://huggingface.co/papers/2503.13327', 'title': 'Edit Transfer: Learning Image Editing via Vision In-Context Relations', 'url': 'https://huggingface.co/papers/2503.13327', 'abstract': 'We introduce a new setting, Edit Transfer, where a model learns a transformation from just a single source-target example and applies it to a new query image. While text-based methods excel at semantic manipulations through textual prompts, they often struggle with precise geometric details (e.g., poses and viewpoint changes). Reference-based editing, on the other hand, typically focuses on style or appearance and fails at non-rigid transformations. By explicitly learning the editing transformation from a source-target pair, Edit Transfer mitigates the limitations of both text-only and appearance-centric references. Drawing inspiration from in-context learning in large language models, we propose a visual relation in-context learning paradigm, building upon a DiT-based text-to-image model. We arrange the edited example and the query image into a unified four-panel composite, then apply lightweight LoRA fine-tuning to capture complex spatial transformations from minimal examples. Despite using only 42 training samples, Edit Transfer substantially outperforms state-of-the-art TIE and RIE methods on diverse non-rigid scenarios, demonstrating the effectiveness of few-shot visual relation learning.', 'score': 21, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '24a5e5d1d86949d5', 'authors': ['Lan Chen', 'Qi Mao', 'Yuchao Gu', 'Mike Zheng Shou'], 'affiliations': ['MIPG, Communication University of China', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.13327.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Передача редактирования: обучение визуальным преобразованиям по одному примеру', 'desc': "Статья представляет новый подход в машинном обучении под названием 'Edit Transfer'. Этот метод позволяет модели изучать преобразование на основе всего одного примера исходного и целевого изображения, а затем применять его к новому запросу. Авторы предлагают парадигму обучения визуальным отношениям в контексте, основанную на модели преобразования текста в изображение DiT. Несмотря на использование всего 42 обучающих образцов, Edit Transfer значительно превосходит современные методы TIE и RIE в различных сценариях нежесткого преобразования."}, 'en': {'title': 'Transforming Images with Just One Example!', 'desc': 'This paper presents a novel approach called Edit Transfer, which enables a model to learn how to transform images using just one example of a source and target image. Unlike traditional text-based methods that struggle with geometric details and reference-based methods that focus on style, Edit Transfer effectively captures complex spatial transformations. The method utilizes a visual relation in-context learning paradigm, inspired by large language models, and employs a DiT-based text-to-image model. Remarkably, it achieves superior performance in non-rigid scenarios with only 42 training samples, showcasing the power of few-shot learning in visual transformations.'}, 'zh': {'title': '编辑转移：少样本学习的突破', 'desc': '我们提出了一种新的设置，称为编辑转移（Edit Transfer），模型通过单一的源-目标示例学习变换，并将其应用于新的查询图像。与文本方法在语义操作上表现优异但在几何细节上存在困难不同，编辑转移通过明确学习源-目标对的编辑变换，克服了文本和外观参考的局限性。我们借鉴大型语言模型中的上下文学习，提出了一种视觉关系上下文学习范式，并在DiT基础的文本到图像模型上进行构建。尽管仅使用42个训练样本，编辑转移在多样的非刚性场景中显著超越了现有的最先进方法，展示了少样本视觉关系学习的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.13434', 'title': 'BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing', 'url': 'https://huggingface.co/papers/2503.13434', 'abstract': 'Element-level visual manipulation is essential in digital content creation, but current diffusion-based methods lack the precision and flexibility of traditional tools. In this work, we introduce BlobCtrl, a framework that unifies element-level generation and editing using a probabilistic blob-based representation. By employing blobs as visual primitives, our approach effectively decouples and represents spatial location, semantic content, and identity information, enabling precise element-level manipulation. Our key contributions include: 1) a dual-branch diffusion architecture with hierarchical feature fusion for seamless foreground-background integration; 2) a self-supervised training paradigm with tailored data augmentation and score functions; and 3) controllable dropout strategies to balance fidelity and diversity. To support further research, we introduce BlobData for large-scale training and BlobBench for systematic evaluation. Experiments show that BlobCtrl excels in various element-level manipulation tasks while maintaining computational efficiency, offering a practical solution for precise and flexible visual content creation. Project page: https://liyaowei-stu.github.io/project/BlobCtrl/', 'score': 19, 'issue_id': 2758, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c8ae2d6baee8bf12', 'authors': ['Yaowei Li', 'Lingen Li', 'Zhaoyang Zhang', 'Xiaoyu Li', 'Guangzhi Wang', 'Hongxiang Li', 'Xiaodong Cun', 'Ying Shan', 'Yuexian Zou'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.13434.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#cv', '#open_source', '#dataset', '#training'], 'emoji': '🎨', 'ru': {'title': 'Точное управление элементами изображения с помощью блобов', 'desc': "BlobCtrl - это новая система для точного управления элементами изображения в цифровом контенте. Она использует вероятностное представление на основе 'блобов', что позволяет отделить пространственное положение, семантическое содержание и информацию об идентичности объектов. Архитектура включает двухветвевую диффузионную модель с иерархическим слиянием признаков и самоконтролируемое обучение. BlobCtrl превосходит существующие методы в задачах манипуляции элементами изображения, сохраняя при этом вычислительную эффективность."}, 'en': {'title': 'Precision and Flexibility in Visual Manipulation with BlobCtrl', 'desc': 'BlobCtrl is a new framework designed for element-level visual manipulation in digital content creation, addressing the limitations of current diffusion-based methods. It uses a probabilistic blob-based representation to separate and manage spatial location, semantic content, and identity, allowing for precise editing of visual elements. The framework features a dual-branch diffusion architecture that integrates foreground and background seamlessly, along with a self-supervised training approach that enhances model performance through tailored data augmentation. Additionally, BlobCtrl introduces BlobData for extensive training and BlobBench for evaluation, demonstrating superior efficiency and effectiveness in various manipulation tasks.'}, 'zh': {'title': 'BlobCtrl：精确灵活的视觉内容创作新方法', 'desc': '本文介绍了一种名为BlobCtrl的框架，旨在提高数字内容创作中元素级视觉操作的精确性和灵活性。该框架采用基于概率的blob表示，能够有效解耦空间位置、语义内容和身份信息，从而实现精确的元素级操作。我们提出了双分支扩散架构和自监督训练范式，以增强前景和背景的无缝集成，并引入可控的dropout策略来平衡保真度和多样性。实验结果表明，BlobCtrl在多种元素级操作任务中表现优异，同时保持计算效率，为精确和灵活的视觉内容创作提供了实用解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.13435', 'title': 'WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes', 'url': 'https://huggingface.co/papers/2503.13435', 'abstract': 'With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D, which generates stable and high-quality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. Project: https://github.com/Gen-Verse/WideRange4D', 'score': 16, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c17d4be710ed24e0', 'authors': ['Ling Yang', 'Kaixin Zhu', 'Juanxi Tian', 'Bohan Zeng', 'Mingbao Lin', 'Hongjuan Pei', 'Wentao Zhang', 'Shuicheng Yan'], 'affiliations': ['National University of Singapore', 'Peking University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.13435.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#3d'], 'emoji': '🌀', 'ru': {'title': 'Прорыв в 4D-реконструкции: от статики к динамике', 'desc': 'Статья представляет новый бенчмарк WideRange4D для 4D-реконструкции сцен с широким диапазоном пространственных движений. Авторы также предлагают новый метод 4D-реконструкции под названием Progress4D, который генерирует стабильные и высококачественные 4D-результаты для различных сложных задач реконструкции 4D-сцен. Метод Progress4D превосходит существующие передовые методы 4D-реконструкции в количественных и качественных сравнительных экспериментах на WideRange4D. Работа направлена на преодоление ограничений существующих методов 4D-реконструкции, которые плохо справляются с широкомасштабными пространственными движениями.'}, 'en': {'title': 'Advancing 4D Reconstruction with Wide Spatial Movements', 'desc': 'This paper addresses the limitations of current 4D reconstruction methods, which primarily focus on actions performed in place and struggle with wide-range spatial movements. The authors introduce a new benchmark called WideRange4D, which includes diverse 4D scene data featuring significant spatial variations, enabling better evaluation of 4D generation techniques. They also propose a novel reconstruction method named Progress4D, designed to produce stable and high-quality 4D results across complex scenarios. Experimental results demonstrate that Progress4D surpasses existing state-of-the-art methods in both quantitative and qualitative assessments.'}, 'zh': {'title': '突破空间限制，实现高质量4D重建', 'desc': '随着3D重建技术的快速发展，4D重建研究也在不断进步。现有的4D重建方法能够生成高质量的4D场景，但在获取多视角视频数据方面面临挑战，导致现有基准主要展示有限场景中的动作。本文提出了一个新的4D重建基准WideRange4D，包含丰富的4D场景数据，允许对4D生成方法的能力进行更全面的评估。同时，我们引入了一种新的4D重建方法Progress4D，在各种复杂的4D场景重建任务中生成稳定且高质量的结果。'}}}, {'id': 'https://huggingface.co/papers/2503.13399', 'title': 'MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research', 'url': 'https://huggingface.co/papers/2503.13399', 'abstract': "Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53\\%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at https://huggingface.co/datasets/jmhb/microvqa, and project page at https://jmhb0.github.io/microvqa.", 'score': 16, 'issue_id': 2754, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '50d4f1f510eff333', 'authors': ['James Burgess', 'Jeffrey J Nirschl', 'Laura Bravo-Sánchez', 'Alejandro Lozano', 'Sanket Rajan Gupte', 'Jesus G. Galaz-Montoya', 'Yuhui Zhang', 'Yuchang Su', 'Disha Bhowmik', 'Zachary Coman', 'Sarina M. Hasan', 'Alexandra Johannesson', 'William D. Leineweber', 'Malvika G Nair', 'Ridhi Yarlagadda', 'Connor Zuraski', 'Wah Chiu', 'Sarah Cohen', 'Jan N. Hansen', 'Manuel D Leonetti', 'Chad Liu', 'Emma Lundberg', 'Serena Yeung-Levy'], 'affiliations': ['Chan Zuckerberg Biohub Network', 'KTH Royal Institute of Technology', 'Princeton University', 'Stanford University', 'Tsinghua University', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.13399.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#multimodal', '#benchmark', '#science'], 'emoji': '🔬', 'ru': {'title': 'MicroVQA: Новый рубеж в оценке ИИ для научных исследований в биологии', 'desc': 'Статья представляет MicroVQA - новый бенчмарк для оценки мультимодальных языковых моделей в контексте научных исследований в биологии. Бенчмарк состоит из 1042 вопросов с множественным выбором, охватывающих различные модальности микроскопии и оценивающих понимание изображений, генерацию гипотез и предложение экспериментов. Авторы разработали двухэтапный процесс создания вопросов, чтобы избежать языковых упрощений. Тестирование современных мультимодальных моделей показало максимальную точность 53%, выявив сложности в мультимодальных рассуждениях для научных задач.'}, 'en': {'title': 'MicroVQA: Advancing Multimodal Reasoning in Scientific Discovery', 'desc': 'This paper introduces MicroVQA, a new benchmark for visual-question answering (VQA) that focuses on complex reasoning needed in scientific research, particularly in biology. It assesses three key capabilities: understanding images, generating hypotheses, and proposing experiments, using questions curated by biology experts. The benchmark consists of 1,042 multiple-choice questions designed to reflect real scientific practices, addressing the limitations of existing multimodal reasoning benchmarks. The study reveals that while current models perform at a peak of 53%, the challenges in multimodal reasoning are significant, emphasizing the need for improved AI tools in biomedical research.'}, 'zh': {'title': 'MicroVQA：推动生物医学研究的多模态推理基准', 'desc': '本论文介绍了MicroVQA，这是一个针对生物学研究的视觉问答基准，旨在评估科学研究中所需的三种推理能力：专家图像理解、假设生成和实验提案。现有的多模态大语言模型（MLLMs）在处理复杂的多模态推理时存在不足，MicroVQA通过1,042个由生物学专家策划的多项选择题来填补这一空白。研究发现，标准的多项选择题生成方法容易产生语言捷径，因此提出了一种新的两阶段流程来优化问题和答案的结构。通过对最先进的MLLMs进行基准测试，结果显示，尽管小型LLMs的表现略逊于顶级模型，但语言推理的难度低于多模态推理，这突显了在科学推理中的挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.12605', 'title': 'Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2503.12605', 'abstract': 'By extending the advantage of chain-of-thought (CoT) reasoning in human-like step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning has recently garnered significant research attention, especially in the integration with multimodal large language models (MLLMs). Existing MCoT studies design various methodologies and innovative reasoning paradigms to address the unique challenges of image, video, speech, audio, 3D, and structured data across different modalities, achieving extensive success in applications such as robotics, healthcare, autonomous driving, and multimodal generation. However, MCoT still presents distinct challenges and opportunities that require further focus to ensure consistent thriving in this field, where, unfortunately, an up-to-date review of this domain is lacking. To bridge this gap, we present the first systematic survey of MCoT reasoning, elucidating the relevant foundational concepts and definitions. We offer a comprehensive taxonomy and an in-depth analysis of current methodologies from diverse perspectives across various application scenarios. Furthermore, we provide insights into existing challenges and future research directions, aiming to foster innovation toward multimodal AGI.', 'score': 14, 'issue_id': 2762, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '4b0cc0276cb6afed', 'authors': ['Yaoting Wang', 'Shengqiong Wu', 'Yuecheng Zhang', 'William Wang', 'Ziwei Liu', 'Jiebo Luo', 'Hao Fei'], 'affiliations': ['CUHK', 'NTU', 'NUS', 'UCSB', 'UR'], 'pdf_title_img': 'assets/pdf/title_img/2503.12605.jpg', 'data': {'categories': ['#multimodal', '#robotics', '#healthcare', '#3d', '#video', '#agi', '#reasoning', '#survey', '#audio'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальное рассуждение: шаг к AGI', 'desc': 'Статья посвящена мультимодальному рассуждению по цепочке мыслей (MCoT) в контексте мультимодальных больших языковых моделей (MLLM). Авторы представляют первый систематический обзор MCoT, объясняя основные концепции и определения. Они предлагают всестороннюю таксономию и глубокий анализ текущих методологий в различных сценариях применения. Статья также рассматривает существующие проблемы и направления будущих исследований в области мультимодального искусственного интеллекта общего назначения.'}, 'en': {'title': 'Unlocking Multimodal Reasoning for Future AI Innovations', 'desc': 'This paper introduces multimodal chain-of-thought (MCoT) reasoning, which enhances human-like reasoning processes across different types of data such as images, videos, and audio. It reviews various methodologies and innovative reasoning paradigms that have been developed to tackle the unique challenges posed by these multimodal contexts. The authors present a systematic survey that includes foundational concepts, a comprehensive taxonomy, and an analysis of current approaches in MCoT applications. Additionally, the paper highlights existing challenges and suggests future research directions to advance the field towards multimodal artificial general intelligence (AGI).'}, 'zh': {'title': '多模态推理的未来之路', 'desc': '多模态链式思维（MCoT）推理将人类的逐步推理优势扩展到多种数据类型，近年来受到广泛关注，尤其是在与多模态大型语言模型（MLLMs）的结合方面。现有的MCoT研究设计了多种方法和创新的推理范式，以应对图像、视频、语音、音频、3D和结构化数据等不同模态的独特挑战，并在机器人技术、医疗保健、自动驾驶和多模态生成等应用中取得了显著成功。尽管如此，MCoT仍面临独特的挑战和机遇，需要进一步关注，以确保该领域的持续发展。为此，我们提供了MCoT推理的首次系统性综述，阐明相关的基础概念和定义，并对当前方法进行了全面的分类和深入分析。'}}}, {'id': 'https://huggingface.co/papers/2503.11751', 'title': 'reWordBench: Benchmarking and Improving the Robustness of Reward Models\n  with Transformed Inputs', 'url': 'https://huggingface.co/papers/2503.11751', 'abstract': 'Reward models have become a staple in modern NLP, serving as not only a scalable text evaluator, but also an indispensable component in many alignment recipes and inference-time algorithms. However, while recent reward models increase performance on standard benchmarks, this may partly be due to overfitting effects, which would confound an understanding of their true capability. In this work, we scrutinize the robustness of reward models and the extent of such overfitting. We build **reWordBench**, which systematically transforms reward model inputs in meaning- or ranking-preserving ways. We show that state-of-the-art reward models suffer from substantial performance degradation even with minor input transformations, sometimes dropping to significantly below-random accuracy, suggesting brittleness. To improve reward model robustness, we propose to explicitly train them to assign similar scores to paraphrases, and find that this approach also improves robustness to other distinct kinds of transformations. For example, our robust reward model reduces such degradation by roughly half for the Chat Hard subset in RewardBench. Furthermore, when used in alignment, our robust reward models demonstrate better utility and lead to higher-quality outputs, winning in up to 59% of instances against a standardly trained RM.', 'score': 14, 'issue_id': 2755, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '0adbcb6b7ba858f8', 'authors': ['Zhaofeng Wu', 'Michihiro Yasunaga', 'Andrew Cohen', 'Yoon Kim', 'Asli Celikyilmaz', 'Marjan Ghazvininejad'], 'affiliations': ['FAIR at Meta', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2503.11751.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#training', '#alignment', '#rlhf'], 'emoji': '🔬', 'ru': {'title': 'Повышение устойчивости моделей вознаграждения в NLP', 'desc': 'Эта статья исследует устойчивость моделей вознаграждения (reward models) в обработке естественного языка. Авторы создали набор данных reWordBench для систематического тестирования этих моделей путем трансформации входных данных. Результаты показывают, что современные модели вознаграждения существенно теряют в производительности даже при незначительных изменениях входных данных. Для повышения устойчивости авторы предлагают обучать модели присваивать схожие оценки парафразам, что также улучшает устойчивость к другим видам трансформаций.'}, 'en': {'title': 'Enhancing Robustness in Reward Models for NLP', 'desc': 'This paper investigates the reliability of reward models in natural language processing (NLP), which are crucial for evaluating text and enhancing model alignment. The authors introduce **reWordBench**, a tool that modifies inputs to test the robustness of these models against overfitting. Their findings reveal that many state-of-the-art reward models perform poorly when faced with slight input changes, indicating a lack of robustness. To address this issue, they propose training reward models to provide consistent scores for paraphrased inputs, resulting in improved performance and higher-quality outputs during alignment tasks.'}, 'zh': {'title': '提升奖励模型鲁棒性，减少过拟合影响', 'desc': '奖励模型在现代自然语言处理（NLP）中扮演着重要角色，既是可扩展的文本评估工具，也是许多对齐算法和推理时算法的关键组成部分。尽管最近的奖励模型在标准基准测试中表现出色，但这可能部分是由于过拟合现象，影响了对其真实能力的理解。我们构建了reWordBench，系统地对奖励模型输入进行意义或排名保持的转换，发现即使是微小的输入变化，最先进的奖励模型也会出现显著的性能下降，显示出其脆弱性。为提高奖励模型的鲁棒性，我们提出显式训练模型对同义句赋予相似分数，这种方法也改善了模型对其他不同类型转换的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.12937', 'title': 'R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization', 'url': 'https://huggingface.co/papers/2503.12937', 'abstract': "Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine-tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong reasoning paths are. In this work, we aim to enhance the MLLMs' reasoning ability beyond passively imitating positive reasoning paths. To this end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new online reinforcement learning framework that enables MLLMs to self-improve reasoning ability via simple, effective and dense step-wise rewarding. Specifically, StepGRPO introduces two novel rule-based reasoning rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary intermediate reasoning steps via a soft key-step matching technique, while StepRAR rewards reasoning paths that follow a well-structured and logically consistent reasoning process through a reasoning completeness and logic evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive experiments over 8 benchmarks demonstrate the superiority of our methods.", 'score': 13, 'issue_id': 2755, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'dbdf6970963a4489', 'authors': ['Jingyi Zhang', 'Jiaxing Huang', 'Huanjin Yao', 'Shunyu Liu', 'Xikun Zhang', 'Shijian Lu', 'Dacheng Tao'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12937.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самосовершенствование языковых моделей через пошаговое обучение с подкреплением', 'desc': 'Это исследование представляет новый подход к улучшению способностей мультимодальных языковых моделей (MLLM) к рассуждению. Авторы разработали метод Step-wise Group Relative Policy Optimization (StepGRPO), который использует онлайн-обучение с подкреплением для самосовершенствования моделей. StepGRPO вводит два новых вознаграждения на основе правил: Step-wise Reasoning Accuracy Reward (StepRAR) и Step-wise Reasoning Validity Reward (StepRVR). Эксперименты на 8 бенчмарках показали превосходство предложенного метода.'}, 'en': {'title': 'Empowering MLLMs with Step-wise Reasoning Rewards', 'desc': 'This paper presents a new approach to improve the reasoning abilities of machine learning language models (MLLMs) by using reinforcement learning. The proposed method, called Step-wise Group Relative Policy Optimization (StepGRPO), focuses on rewarding MLLMs for their reasoning steps rather than just imitating correct paths. It introduces two innovative rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR), which encourage models to follow logical reasoning processes. The results show that MLLMs trained with StepGRPO, referred to as R1-VL, perform significantly better in step-by-step reasoning tasks across multiple benchmarks.'}, 'zh': {'title': '提升推理能力的创新方法', 'desc': '最近的研究通常通过在高质量的推理数据上进行监督微调来增强多语言大模型（MLLMs）的推理能力，这往往导致模型仅仅模仿成功的推理路径，而不理解错误的推理路径。在这项工作中，我们旨在超越被动模仿积极推理路径，提升MLLMs的推理能力。为此，我们设计了一种新的在线强化学习框架——逐步组相对策略优化（StepGRPO），使MLLMs能够通过简单、有效和密集的逐步奖励自我提升推理能力。具体而言，StepGRPO引入了两种新颖的基于规则的推理奖励：逐步推理准确性奖励（StepRAR）和逐步推理有效性奖励（StepRVR）。'}}}, {'id': 'https://huggingface.co/papers/2503.11495', 'title': 'V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning', 'url': 'https://huggingface.co/papers/2503.11495', 'abstract': 'Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames ("when") and then analyse the spatial relationships ("where") between key objects, and finally leverage these relationships to draw inferences ("what"). However, can Video Large Language Models (Video-LLMs) also "reason through a sequential spatio-temporal logic" in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained "memory" of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.', 'score': 10, 'issue_id': 2754, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '93b4a63d45a11f3d', 'authors': ['Zixu Cheng', 'Jian Hu', 'Ziquan Liu', 'Chenyang Si', 'Wei Li', 'Shaogang Gong'], 'affiliations': ['Nanjing University', 'Nanyang Technological University', 'Queen Mary University of London'], 'pdf_title_img': 'assets/pdf/title_img/2503.11495.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#video'], 'emoji': '🎥', 'ru': {'title': 'Новый взгляд на оценку пространственно-временного мышления видео-LLM', 'desc': 'Статья представляет новый бенчмарк V-STaR для оценки пространственно-временного рассуждения в видео-LLM моделях. Авторы предлагают декомпозировать понимание видео на задачу обратного пространственно-временного рассуждения (RSTR), оценивающую присутствие объектов, время событий и их расположение. Для этого создан датасет с вопросами, сгенерированными с помощью GPT-4, имитирующими цепочку рассуждений человека. Эксперименты на 14 видео-LLM моделях выявили значительные пробелы в их способности к надежному и последовательному пространственно-временному рассуждению.'}, 'en': {'title': 'Enhancing Video Understanding with Spatio-Temporal Reasoning', 'desc': "This paper explores how Video Large Language Models (Video-LLMs) can understand videos using a method similar to human reasoning, which involves identifying when events happen, where objects are located, and how they interact. The authors highlight that current benchmarks for Video-LLMs mainly check for object presence but fail to assess the models' ability to reason about relationships and interactions between objects. To address this, they introduce the Video Spatio-Temporal Reasoning (V-STaR) benchmark, which breaks down video understanding into a task that evaluates object presence, timing of events, and spatial relationships while capturing the reasoning process. Their findings show that there are significant gaps in the reasoning capabilities of existing Video-LLMs, indicating a need for improved models that can perform robust spatio-temporal reasoning."}, 'zh': {'title': '提升视频理解的时空推理能力', 'desc': '本论文探讨了视频大型语言模型（Video-LLMs）在视频理解中的时空推理能力。我们提出了一个新的基准，称为视频时空推理（V-STaR），旨在评估模型在识别对象、事件发生时间和空间位置方面的能力。通过构建一个包含细致推理链的问题数据集，我们模拟人类的认知过程，以便更好地评估模型的推理能力。实验结果显示，现有的Video-LLMs在时空推理方面存在显著不足，无法满足实际应用的需求。'}}}, {'id': 'https://huggingface.co/papers/2503.13082', 'title': 'Free-form language-based robotic reasoning and grasping', 'url': 'https://huggingface.co/papers/2503.13082', 'abstract': "Performing robotic grasping from a cluttered bin based on human instructions is a challenging task, as it requires understanding both the nuances of free-form language and the spatial relationships between objects. Vision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have demonstrated remarkable reasoning capabilities across both text and images. But can they truly be used for this task in a zero-shot setting? And what are their limitations? In this paper, we explore these research questions via the free-form language-based robotic grasping task, and propose a novel method, FreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about human instructions and object spatial arrangements. Our method detects all objects as keypoints and uses these keypoints to annotate marks on images, aiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our method to determine whether a requested object is directly graspable or if other objects must be grasped and removed first. Since no existing dataset is specifically designed for this task, we introduce a synthetic dataset FreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated instructions and ground-truth grasping sequences. We conduct extensive analyses with both FreeGraspData and real-world validation with a gripper-equipped robotic arm, demonstrating state-of-the-art performance in grasp reasoning and execution. Project website: https://tev-fbk.github.io/FreeGrasp/.", 'score': 9, 'issue_id': 2762, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'c29bd4c3e364d62c', 'authors': ['Runyu Jiao', 'Alice Fasoli', 'Francesco Giuliari', 'Matteo Bortolon', 'Sergio Povoli', 'Guofeng Mei', 'Yiming Wang', 'Fabio Poiesi'], 'affiliations': ['Fondazione Bruno Kessler', 'Istituto Italiano di Tecnologia', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.13082.jpg', 'data': {'categories': ['#dataset', '#robotics', '#agents', '#reasoning', '#cv', '#synthetic'], 'emoji': '🦾', 'ru': {'title': 'Роботы учатся хватать по-человечески', 'desc': 'Статья представляет новый метод FreeGrasp для роботизированного захвата объектов на основе инструкций на естественном языке. Метод использует предобученные мультимодальные языковые модели (VLM) для понимания инструкций и пространственных отношений между объектами. FreeGrasp определяет объекты как ключевые точки и использует их для аннотации изображений, чтобы улучшить рассуждения модели о пространственных отношениях. Авторы также создали синтетический датасет FreeGraspData для оценки производительности метода.'}, 'en': {'title': 'Empowering Robots with Language: Grasping Made Easy!', 'desc': 'This paper investigates the use of Vision-Language Models (VLMs) for robotic grasping tasks based on human instructions, particularly in cluttered environments. The authors introduce a method called FreeGrasp, which utilizes pre-trained VLMs to enhance spatial reasoning by detecting objects as keypoints and annotating them on images. They also create a new synthetic dataset, FreeGraspData, to support their research, as no existing dataset fits their needs. The results show that FreeGrasp achieves state-of-the-art performance in understanding and executing grasping tasks, demonstrating the potential of VLMs in robotics.'}, 'zh': {'title': '基于人类指令的智能抓取新方法', 'desc': '本论文探讨了基于人类指令的机器人抓取任务，尤其是在杂乱的环境中进行抓取的挑战。我们提出了一种新方法FreeGrasp，利用预训练的视觉-语言模型（VLM）来理解人类指令和物体之间的空间关系。该方法通过将所有物体检测为关键点，并在图像上标注这些关键点，来增强模型的空间推理能力。我们还创建了一个合成数据集FreeGraspData，以支持这一任务的研究，并在真实世界中验证了我们方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.13444', 'title': 'VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning', 'url': 'https://huggingface.co/papers/2503.13444', 'abstract': 'Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.', 'score': 8, 'issue_id': 2755, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '4845903cb3dc6761', 'authors': ['Ye Liu', 'Kevin Qinghong Lin', 'Chang Wen Chen', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13444.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#agents', '#optimization', '#video', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'VideoMind: Темпоральное рассуждение по видео с помощью ролевых агентов', 'desc': 'Статья представляет VideoMind - новый видео-языковой агент для темпорально-обоснованного понимания видео. Авторы разработали ролевой подход с планировщиком, локализатором, верификатором и ответчиком для временного рассуждения по видео. Предложена стратегия Chain-of-LoRA для эффективного переключения между ролями с помощью легких LoRA-адаптеров. Эксперименты на 14 бенчмарках показали, что VideoMind достигает state-of-the-art результатов в различных задачах понимания видео.'}, 'en': {'title': 'VideoMind: Advancing Video Understanding with Temporal Reasoning', 'desc': 'This paper presents VideoMind, a new video-language agent that enhances understanding of videos by linking answers to visual evidence. It introduces a role-based workflow that includes a planner, grounder, verifier, and answerer to facilitate effective temporal reasoning in videos. The authors also propose a Chain-of-LoRA strategy, which allows for efficient role-switching without the need for multiple models, thus improving both efficiency and flexibility. The results show that VideoMind outperforms existing methods on various benchmarks, highlighting its capability in grounded video question-answering and temporal reasoning tasks.'}, 'zh': {'title': 'VideoMind：视频理解的新突破', 'desc': '本论文介绍了一种新的视频语言智能体，名为VideoMind，旨在实现视频的时间基础理解。该智能体通过角色驱动的工作流程，整合了规划者、定位者、验证者和回答者等关键角色，以提高视频的时间推理能力。为了高效整合这些角色，提出了一种新颖的Chain-of-LoRA策略，允许通过轻量级的LoRA适配器实现角色之间的无缝切换。实验结果表明，VideoMind在多项视频理解任务上表现出色，推动了视频智能体和长时序推理的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.11412', 'title': 'MTV-Inpaint: Multi-Task Long Video Inpainting', 'url': 'https://huggingface.co/papers/2503.11412', 'abstract': 'Video inpainting involves modifying local regions within a video, ensuring spatial and temporal consistency. Most existing methods focus primarily on scene completion (i.e., filling missing regions) and lack the capability to insert new objects into a scene in a controllable manner. Fortunately, recent advancements in text-to-video (T2V) diffusion models pave the way for text-guided video inpainting. However, directly adapting T2V models for inpainting remains limited in unifying completion and insertion tasks, lacks input controllability, and struggles with long videos, thereby restricting their applicability and flexibility. To address these challenges, we propose MTV-Inpaint, a unified multi-task video inpainting framework capable of handling both traditional scene completion and novel object insertion tasks. To unify these distinct tasks, we design a dual-branch spatial attention mechanism in the T2V diffusion U-Net, enabling seamless integration of scene completion and object insertion within a single framework. In addition to textual guidance, MTV-Inpaint supports multimodal control by integrating various image inpainting models through our proposed image-to-video (I2V) inpainting mode. Additionally, we propose a two-stage pipeline that combines keyframe inpainting with in-between frame propagation, enabling MTV-Inpaint to effectively handle long videos with hundreds of frames. Extensive experiments demonstrate that MTV-Inpaint achieves state-of-the-art performance in both scene completion and object insertion tasks. Furthermore, it demonstrates versatility in derived applications such as multi-modal inpainting, object editing, removal, image object brush, and the ability to handle long videos. Project page: https://mtv-inpaint.github.io/.', 'score': 7, 'issue_id': 2756, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'df14c3a2c0dfe3fd', 'authors': ['Shiyuan Yang', 'Zheng Gu', 'Liang Hou', 'Xin Tao', 'Pengfei Wan', 'Xiaodong Chen', 'Jing Liao'], 'affiliations': ['City University of Hong Kong', 'Kuaishou Technology', 'Shenzhen University', 'Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11412.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'Универсальный видеоинпейнтинг: от заполнения пропусков до вставки объектов', 'desc': 'MTV-Inpaint - это новая система для видеоинпейнтинга, объединяющая задачи заполнения пропусков и вставки новых объектов. Она использует диффузионную модель text-to-video с двухветвевым механизмом пространственного внимания в U-Net архитектуре. MTV-Inpaint поддерживает мультимодальное управление, интегрируя различные модели инпейнтинга изображений. Система применяет двухэтапный подход с инпейнтингом ключевых кадров и распространением на промежуточные, что позволяет обрабатывать длинные видео.'}, 'en': {'title': 'Unified Video Inpainting: Complete and Insert with Control!', 'desc': 'This paper presents MTV-Inpaint, a novel framework for video inpainting that integrates scene completion and object insertion tasks. It utilizes a dual-branch spatial attention mechanism within a text-to-video diffusion U-Net to achieve seamless control over both tasks. The framework also introduces a two-stage pipeline that effectively manages long videos by combining keyframe inpainting with in-between frame propagation. Extensive experiments show that MTV-Inpaint outperforms existing methods and offers versatility for various applications, including multi-modal inpainting and object editing.'}, 'zh': {'title': '统一视频修复，场景补全与物体插入的完美结合', 'desc': '视频修复是指在视频中修改局部区域，以确保空间和时间的一致性。现有的方法主要集中在场景补全上，缺乏可控地插入新物体的能力。为了解决这些问题，我们提出了MTV-Inpaint，一个统一的多任务视频修复框架，能够同时处理传统的场景补全和新物体插入任务。通过设计双分支空间注意力机制，MTV-Inpaint实现了场景补全和物体插入的无缝集成，并支持多模态控制，适用于长视频的处理。'}}}, {'id': 'https://huggingface.co/papers/2503.13070', 'title': 'Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation', 'url': 'https://huggingface.co/papers/2503.13070', 'abstract': 'Aligning generated images to complicated text prompts and human preferences is a central challenge in Artificial Intelligence-Generated Content (AIGC). With reward-enhanced diffusion distillation emerging as a promising approach that boosts controllability and fidelity of text-to-image models, we identify a fundamental paradigm shift: as conditions become more specific and reward signals stronger, the rewards themselves become the dominant force in generation. In contrast, the diffusion losses serve as an overly expensive form of regularization. To thoroughly validate our hypothesis, we introduce R0, a novel conditional generation approach via regularized reward maximization. Instead of relying on tricky diffusion distillation losses, R0 proposes a new perspective that treats image generations as an optimization problem in data space which aims to search for valid images that have high compositional rewards. By innovative designs of the generator parameterization and proper regularization techniques, we train state-of-the-art few-step text-to-image generative models with R0 at scales. Our results challenge the conventional wisdom of diffusion post-training and conditional generation by demonstrating that rewards play a dominant role in scenarios with complex conditions. We hope our findings can contribute to further research into human-centric and reward-centric generation paradigms across the broader field of AIGC. Code is available at https://github.com/Luo-Yihong/R0.', 'score': 6, 'issue_id': 2756, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '3d6b6c6e117e1abd', 'authors': ['Yihong Luo', 'Tianyang Hu', 'Weijian Luo', 'Kenji Kawaguchi', 'Jing Tang'], 'affiliations': ['HKUST', 'HKUST (GZ)', 'NUS', 'Xiaohongshu Inc'], 'pdf_title_img': 'assets/pdf/title_img/2503.13070.jpg', 'data': {'categories': ['#rag', '#diffusion', '#alignment', '#training', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'R0: Революция в генерации изображений через максимизацию вознаграждений', 'desc': 'Статья представляет новый подход к генерации изображений по текстовым запросам, называемый R0. Вместо использования сложных методов дистилляции диффузионных моделей, R0 рассматривает генерацию как оптимизационную задачу в пространстве данных. Авторы утверждают, что при более конкретных условиях и сильных сигналах вознаграждения, сами вознаграждения становятся доминирующей силой в генерации. Результаты исследования бросают вызов традиционным представлениям о постобучении диффузионных моделей и условной генерации.'}, 'en': {'title': 'Revolutionizing Image Generation: Prioritizing Rewards Over Diffusion', 'desc': 'This paper addresses the challenge of aligning generated images with complex text prompts and human preferences in AI-generated content. It introduces R0, a new approach that emphasizes reward maximization over traditional diffusion distillation methods, which are seen as inefficient. The authors argue that as conditions for image generation become more specific, the influence of reward signals becomes more significant than diffusion losses. Their findings suggest a shift in focus towards reward-centric generation strategies, which could enhance the effectiveness of text-to-image models.'}, 'zh': {'title': '奖励驱动的图像生成新视角', 'desc': '本论文探讨了在人工智能生成内容（AIGC）中，将生成图像与复杂文本提示和人类偏好对齐的挑战。我们提出了一种新的条件生成方法R0，通过正则化奖励最大化来优化图像生成过程。研究表明，在条件变得更加具体且奖励信号更强时，奖励在生成过程中起着主导作用，而扩散损失则成为一种过于昂贵的正则化形式。我们的结果挑战了传统的扩散后训练和条件生成的观念，强调了在复杂条件下奖励的重要性。'}}}, {'id': 'https://huggingface.co/papers/2503.10719', 'title': 'Long-Video Audio Synthesis with Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2503.10719', 'abstract': 'Video-to-audio synthesis, which generates synchronized audio for visual content, critically enhances viewer immersion and narrative coherence in film and interactive media. However, video-to-audio dubbing for long-form content remains an unsolved challenge due to dynamic semantic shifts, temporal misalignment, and the absence of dedicated datasets. While existing methods excel in short videos, they falter in long scenarios (e.g., movies) due to fragmented synthesis and inadequate cross-scene consistency. We propose LVAS-Agent, a novel multi-agent framework that emulates professional dubbing workflows through collaborative role specialization. Our approach decomposes long-video synthesis into four steps including scene segmentation, script generation, sound design and audio synthesis. Central innovations include a discussion-correction mechanism for scene/script refinement and a generation-retrieval loop for temporal-semantic alignment. To enable systematic evaluation, we introduce LVAS-Bench, the first benchmark with 207 professionally curated long videos spanning diverse scenarios. Experiments demonstrate superior audio-visual alignment over baseline methods. Project page: https://lvas-agent.github.io', 'score': 6, 'issue_id': 2760, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '121476792dd15d11', 'authors': ['Yehang Zhang', 'Xinli Xu', 'Xiaojie Xu', 'Li Liu', 'Yingcong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2503.10719.jpg', 'data': {'categories': ['#long_context', '#agents', '#benchmark', '#video', '#games'], 'emoji': '🎬', 'ru': {'title': 'Профессиональный дубляж длинных видео с помощью ИИ', 'desc': 'LVAS-Agent - это новая мультиагентная система для синтеза аудио к длинным видео, имитирующая профессиональный процесс дубляжа. Она разбивает задачу на четыре этапа: сегментацию сцен, генерацию сценария, звуковой дизайн и синтез аудио. Ключевые инновации включают механизм обсуждения и коррекции для улучшения сцен/сценария и цикл генерации-поиска для временно-семантического выравнивания. Авторы также представили LVAS-Bench - первый бенчмарк из 207 длинных видео для оценки таких систем.'}, 'en': {'title': 'Revolutionizing Long-Form Video Dubbing with LVAS-Agent', 'desc': 'This paper presents LVAS-Agent, a new framework for generating audio that matches long videos, like movies, to improve viewer experience. The framework breaks down the audio synthesis process into four key steps: segmenting scenes, generating scripts, designing sounds, and synthesizing audio. It introduces innovative techniques such as a discussion-correction mechanism to refine scripts and a generation-retrieval loop to ensure that audio aligns well with the video over time. Additionally, the authors provide LVAS-Bench, a benchmark dataset of 207 long videos to evaluate the effectiveness of their approach against existing methods.'}, 'zh': {'title': '长视频配音的新突破：LVAS-Agent', 'desc': '视频到音频合成是为视觉内容生成同步音频的技术，能够显著提升观众的沉浸感和叙事连贯性。然而，对于长篇内容的视频到音频配音仍然是一个未解决的挑战，主要由于动态语义变化、时间错位和缺乏专门的数据集。现有方法在短视频中表现良好，但在长视频（如电影）中由于合成碎片化和跨场景一致性不足而表现不佳。我们提出了LVAS-Agent，一个新颖的多代理框架，通过协作角色专业化模拟专业配音工作流程，分解长视频合成为场景分割、脚本生成、声音设计和音频合成四个步骤。'}}}, {'id': 'https://huggingface.co/papers/2503.10704', 'title': 'Error Analyses of Auto-Regressive Video Diffusion Models: A Unified\n  Framework', 'url': 'https://huggingface.co/papers/2503.10704', 'abstract': 'A variety of Auto-Regressive Video Diffusion Models (ARVDM) have achieved remarkable successes in generating realistic long-form videos. However, theoretical analyses of these models remain scant. In this work, we develop theoretical underpinnings for these models and use our insights to improve the performance of existing models. We first develop Meta-ARVDM, a unified framework of ARVDMs that subsumes most existing methods. Using Meta-ARVDM, we analyze the KL-divergence between the videos generated by Meta-ARVDM and the true videos. Our analysis uncovers two important phenomena inherent to ARVDM -- error accumulation and memory bottleneck. By deriving an information-theoretic impossibility result, we show that the memory bottleneck phenomenon cannot be avoided. To mitigate the memory bottleneck, we design various network structures to explicitly use more past frames. We also achieve a significantly improved trade-off between the mitigation of the memory bottleneck and the inference efficiency by compressing the frames. Experimental results on DMLab and Minecraft validate the efficacy of our methods. Our experiments also demonstrate a Pareto-frontier between the error accumulation and memory bottleneck across different methods.', 'score': 5, 'issue_id': 2756, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'e5f88bd433320c2f', 'authors': ['Jing Wang', 'Fengzhuo Zhang', 'Xiaoli Li', 'Vincent Y. F. Tan', 'Tianyu Pang', 'Chao Du', 'Aixin Sun', 'Zhuoran Yang'], 'affiliations': ['A*STAR', 'Nanyang Technological University', 'National University of Singapore', 'Sea AI Lab', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10704.jpg', 'data': {'categories': ['#diffusion', '#video', '#inference', '#architecture', '#math', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Теоретические основы и улучшение авторегрессионных моделей видеодиффузии', 'desc': 'Статья представляет теоретический анализ авторегрессионных моделей видеодиффузии (ARVDM) и предлагает способы их улучшения. Авторы разработали Meta-ARVDM - унифицированную структуру, объединяющую большинство существующих методов ARVDM. Анализ выявил два ключевых явления в ARVDM: накопление ошибок и ограничение памяти, причем последнее неизбежно согласно информационно-теоретическому результату. Для смягчения эффекта ограничения памяти предложены новые архитектуры нейронных сетей и методы сжатия кадров, эффективность которых подтверждена экспериментами.'}, 'en': {'title': 'Enhancing Video Generation with Meta-ARVDM: Tackling Memory Bottlenecks!', 'desc': 'This paper focuses on improving Auto-Regressive Video Diffusion Models (ARVDM) for generating realistic long videos. The authors introduce Meta-ARVDM, a comprehensive framework that encompasses existing ARVDM methods and provides a theoretical analysis of their performance. They identify two key issues: error accumulation and memory bottleneck, the latter of which is shown to be unavoidable through an information-theoretic approach. To address these challenges, the authors propose new network architectures that utilize more past frames and optimize the balance between memory usage and inference efficiency, demonstrating their findings through experiments on DMLab and Minecraft.'}, 'zh': {'title': '提升视频生成效率，破解内存瓶颈', 'desc': '本文探讨了自回归视频扩散模型（ARVDM）的理论基础，并提出了Meta-ARVDM这一统一框架，涵盖了大多数现有方法。通过分析Meta-ARVDM生成的视频与真实视频之间的KL散度，揭示了ARVDM固有的两个重要现象：误差累积和内存瓶颈。我们通过信息论的不可能性结果证明了内存瓶颈现象是无法避免的。为了解决内存瓶颈问题，本文设计了多种网络结构，以显式利用更多的过去帧，并通过压缩帧实现了内存瓶颈缓解与推理效率之间的显著改进。'}}}, {'id': 'https://huggingface.co/papers/2503.13369', 'title': 'Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions', 'url': 'https://huggingface.co/papers/2503.13369', 'abstract': 'Often, the needs and visual abilities differ between the annotator group and the end user group. Generating detailed diagram descriptions for blind and low-vision (BLV) users is one such challenging domain. Sighted annotators could describe visuals with ease, but existing studies have shown that direct generations by them are costly, bias-prone, and somewhat lacking by BLV standards. In this study, we ask sighted individuals to assess -- rather than produce -- diagram descriptions generated by vision-language models (VLM) that have been guided with latent supervision via a multi-pass inference. The sighted assessments prove effective and useful to professional educators who are themselves BLV and teach visually impaired learners. We release Sightation, a collection of diagram description datasets spanning 5k diagrams and 137k samples for completion, preference, retrieval, question answering, and reasoning training purposes and demonstrate their fine-tuning potential in various downstream tasks.', 'score': 3, 'issue_id': 2760, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '30b312815a3aaed9', 'authors': ['Wan Ju Kang', 'Eunki Kim', 'Na Min An', 'Sangryul Kim', 'Haemin Choi', 'Ki Hoon Kwak', 'James Thorne'], 'affiliations': ['KAIST AI', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13369.jpg', 'data': {'categories': ['#training', '#alignment', '#data', '#low_resource', '#dataset', '#multimodal'], 'emoji': '👁️', 'ru': {'title': 'Искусственный интеллект на службе незрячих: новый подход к описанию визуальной информации', 'desc': 'Статья описывает метод создания детальных описаний диаграмм для слепых и слабовидящих пользователей с помощью моделей компьютерного зрения и обработки естественного языка. Авторы предлагают использовать зрячих аннотаторов для оценки описаний, сгенерированных моделями, вместо того чтобы создавать их самостоятельно. Исследование показало эффективность этого подхода для преподавателей с нарушениями зрения. В результате работы был создан набор данных Sightation, содержащий описания 5000 диаграмм для различных задач машинного обучения.'}, 'en': {'title': 'Empowering BLV Education with Vision-Language Models', 'desc': "This paper addresses the challenge of creating effective diagram descriptions for blind and low-vision (BLV) users, highlighting the differences in needs between annotators and end users. It proposes a novel approach where sighted annotators assess diagram descriptions generated by vision-language models (VLM) instead of creating them directly, reducing bias and improving quality. The study introduces 'Sightation', a comprehensive dataset containing 5,000 diagrams and 137,000 samples designed for various machine learning tasks such as preference and reasoning. The findings suggest that using sighted assessments can enhance the educational resources available for BLV learners, making them more accessible and relevant."}, 'zh': {'title': '为视觉障碍用户生成精准图表描述', 'desc': '本研究探讨了视觉障碍用户（BLV）对图表描述的需求与视觉标注者之间的差异。我们提出了一种新方法，通过视觉语言模型（VLM）生成图表描述，并让视觉正常的评估者对这些描述进行评估，而不是直接生成。研究表明，这种评估方式对专业的视觉障碍教育者非常有效，能够帮助他们更好地服务于视觉障碍学习者。我们还发布了名为Sightation的数据集，包含5000个图表和137000个样本，旨在支持多种下游任务的训练。'}}}, {'id': 'https://huggingface.co/papers/2503.12530', 'title': 'Basic Category Usage in Vision Language Models', 'url': 'https://huggingface.co/papers/2503.12530', 'abstract': "The field of psychology has long recognized a basic level of categorization that humans use when labeling visual stimuli, a term coined by Rosch in 1976. This level of categorization has been found to be used most frequently, to have higher information density, and to aid in visual language tasks with priming in humans. Here, we investigate basic level categorization in two recently released, open-source vision-language models (VLMs). This paper demonstrates that Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level categorization consistent with human behavior. Moreover, the models' preferences are consistent with nuanced human behaviors like the biological versus non-biological basic level effects and the well established expert basic level shift, further suggesting that VLMs acquire cognitive categorization behaviors from the human data on which they are trained.", 'score': 2, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': 'eca67c7a2633554a', 'authors': ['Hunter Sawyer', 'Jesse Roberts', 'Kyle Moore'], 'affiliations': ['Tennessee Tech University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12530.jpg', 'data': {'categories': ['#open_source', '#interpretability', '#multimodal', '#cv'], 'emoji': '🧠', 'ru': {'title': 'ИИ мыслит как человек: базовая категоризация в моделях компьютерного зрения', 'desc': 'Это исследование рассматривает применение базового уровня категоризации, концепции из психологии, в современных моделях компьютерного зрения и обработки естественного языка (VLM). Авторы обнаружили, что модели Llama 3.2 Vision Instruct и Molmo 7B-D предпочитают использовать базовый уровень категоризации, что соответствует поведению человека. Более того, модели демонстрируют нюансы, характерные для людей, такие как различия в категоризации биологических и небиологических объектов, а также сдвиг базового уровня у экспертов. Это указывает на то, что VLM приобретают когнитивные паттерны категоризации из человеческих данных, на которых они обучаются.'}, 'en': {'title': 'Bridging Human and Machine: Basic Level Categorization in Vision-Language Models', 'desc': 'This paper explores how vision-language models (VLMs) categorize visual stimuli at a basic level, similar to human categorization as identified by Rosch. The study shows that Llama 3.2 Vision Instruct and Molmo 7B-D models exhibit a preference for basic level categorization, which aligns with human behavior. Additionally, the models reflect complex human categorization nuances, such as distinguishing between biological and non-biological entities. This suggests that VLMs learn cognitive categorization patterns from the human data they are trained on, highlighting the influence of human cognitive processes on machine learning models.'}, 'zh': {'title': '探索视觉语言模型中的基本分类行为', 'desc': '本论文研究了视觉语言模型（VLMs）中的基本分类水平，这一概念最早由Rosch在1976年提出。研究发现，Llama 3.2 Vision Instruct和Molmo 7B-D这两个模型在分类时更倾向于使用与人类行为一致的基本分类水平。模型的偏好还与人类的细微行为相符，例如生物与非生物的基本分类效应，以及专家的基本分类转变。这表明，视觉语言模型在训练过程中从人类数据中学习了认知分类行为。'}}}, {'id': 'https://huggingface.co/papers/2503.12528', 'title': 'Investigating Human-Aligned Large Language Model Uncertainty', 'url': 'https://huggingface.co/papers/2503.12528', 'abstract': 'Recent work has sought to quantify large language model uncertainty to facilitate model control and modulate user trust. Previous works focus on measures of uncertainty that are theoretically grounded or reflect the average overt behavior of the model. In this work, we investigate a variety of uncertainty measures, in order to identify measures that correlate with human group-level uncertainty. We find that Bayesian measures and a variation on entropy measures, top-k entropy, tend to agree with human behavior as a function of model size. We find that some strong measures decrease in human-similarity with model size, but, by multiple linear regression, we find that combining multiple uncertainty measures provide comparable human-alignment with reduced size-dependency.', 'score': 2, 'issue_id': 2755, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '7c140046351fe245', 'authors': ['Kyle Moore', 'Jesse Roberts', 'Daryl Watson', 'Pamela Wisniewski'], 'affiliations': ['Tennessee Tech University', 'Vanderbilt University'], 'pdf_title_img': 'assets/pdf/title_img/2503.12528.jpg', 'data': {'categories': ['#interpretability', '#hallucinations', '#rlhf', '#training'], 'emoji': '🤔', 'ru': {'title': 'Измерение неопределенности LLM: как приблизиться к человеку', 'desc': 'Статья исследует различные методы оценки неопределенности в больших языковых моделях (LLM) с целью найти те, которые лучше всего соответствуют групповой неопределенности людей. Авторы обнаружили, что байесовские методы и вариация энтропийных мер (энтропия top-k) хорошо согласуются с человеческим поведением в зависимости от размера модели. Некоторые сильные меры показывают снижение сходства с человеком при увеличении размера модели. Комбинирование нескольких мер неопределенности с помощью множественной линейной регрессии обеспечивает сопоставимое соответствие человеческому поведению с меньшей зависимостью от размера модели.'}, 'en': {'title': 'Aligning Model Uncertainty with Human Perception', 'desc': 'This paper explores how to measure uncertainty in large language models to improve their control and enhance user trust. It examines various uncertainty measures, particularly focusing on Bayesian methods and a new approach called top-k entropy, to see how well they align with human perceptions of uncertainty. The study reveals that while some measures lose their effectiveness as model size increases, combining different measures can maintain a strong correlation with human behavior regardless of model size. Ultimately, the findings suggest that a multi-measure approach can lead to better alignment with human understanding of uncertainty in language models.'}, 'zh': {'title': '量化不确定性，增强用户信任', 'desc': '本研究探讨了大型语言模型的不确定性量化，以便更好地控制模型并增强用户信任。我们分析了多种不确定性度量，旨在找出与人类群体不确定性相关的度量。研究发现，贝叶斯度量和一种变体的熵度量（top-k熵）在模型规模变化时与人类行为一致。通过多元线性回归，我们发现结合多种不确定性度量可以在减少规模依赖性的同时，保持与人类行为的相似性。'}}}, {'id': 'https://huggingface.co/papers/2503.12964', 'title': 'Training Video Foundation Models with NVIDIA NeMo', 'url': 'https://huggingface.co/papers/2503.12964', 'abstract': 'Video Foundation Models (VFMs) have recently been used to simulate the real world to train physical AI systems and develop creative visual experiences. However, there are significant challenges in training large-scale, high quality VFMs that can generate high-quality videos. We present a scalable, open-source VFM training pipeline with NVIDIA NeMo, providing accelerated video dataset curation, multimodal data loading, and parallelized video diffusion model training and inference. We also provide a comprehensive performance analysis highlighting best practices for efficient VFM training and inference.', 'score': 1, 'issue_id': 2771, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': 'a9b85c5227c0a3e6', 'authors': ['Zeeshan Patel', 'Ethan He', 'Parth Mannan', 'Xiaowei Ren', 'Ryan Wolf', 'Niket Agarwal', 'Jacob Huffman', 'Zhuoyao Wang', 'Carl Wang', 'Jack Chang', 'Yan Bai', 'Tommy Huang', 'Linnan Wang', 'Sahil Jain', 'Shanmugam Ramasamy', 'Joseph Jennings', 'Ekaterina Sirazitdinova', 'Oleg Sudakov', 'Mingyuan Ma', 'Bobby Chen', 'Forrest Lin', 'Hao Wang', 'Vasanth Rao Naik Sabavat', 'Sriharsha Niverty', 'Rong Ou', 'Pallab Bhattacharya', 'David Page', 'Nima Tajbakhsh', 'Ashwath Aithal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.12964.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#multimodal', '#inference', '#video', '#optimization', '#data', '#training', '#open_source'], 'emoji': '🎥', 'ru': {'title': 'Масштабируемое обучение видео-фундаментальных моделей с NVIDIA NeMo', 'desc': 'Эта статья представляет масштабируемый конвейер для обучения видео-фундаментальных моделей (VFM) с использованием NVIDIA NeMo. Авторы описывают ускоренную подготовку видеонаборов данных, мультимодальную загрузку данных и распараллеленное обучение и вывод моделей видеодиффузии. В работе также приводится подробный анализ производительности, выделяющий лучшие практики для эффективного обучения и вывода VFM. Исследование направлено на преодоление значительных трудностей в обучении крупномасштабных VFM высокого качества для генерации высококачественных видео.'}, 'en': {'title': 'Empowering Video Foundation Models with Scalable Training Solutions', 'desc': 'This paper introduces a scalable and open-source training pipeline for Video Foundation Models (VFMs) using NVIDIA NeMo. It addresses the challenges of training large-scale VFMs by offering tools for efficient video dataset curation and multimodal data loading. The proposed pipeline also includes methods for parallelized training and inference of video diffusion models. Additionally, the authors present a performance analysis that outlines best practices for optimizing VFM training and inference processes.'}, 'zh': {'title': '高效训练视频基础模型的开源解决方案', 'desc': '视频基础模型（VFM）最近被用于模拟现实世界，以训练物理人工智能系统和开发创意视觉体验。然而，训练大规模、高质量的VFM以生成高质量视频面临重大挑战。我们提出了一个可扩展的开源VFM训练管道，利用NVIDIA NeMo，加速视频数据集的整理、多模态数据加载，以及并行化的视频扩散模型训练和推理。我们还提供了全面的性能分析，强调高效VFM训练和推理的最佳实践。'}}}, {'id': 'https://huggingface.co/papers/2503.12720', 'title': 'GenStereo: Towards Open-World Generation of Stereo Images and\n  Unsupervised Matching', 'url': 'https://huggingface.co/papers/2503.12720', 'abstract': 'Stereo images are fundamental to numerous applications, including extended reality (XR) devices, autonomous driving, and robotics. Unfortunately, acquiring high-quality stereo images remains challenging due to the precise calibration requirements of dual-camera setups and the complexity of obtaining accurate, dense disparity maps. Existing stereo image generation methods typically focus on either visual quality for viewing or geometric accuracy for matching, but not both. We introduce GenStereo, a diffusion-based approach, to bridge this gap. The method includes two primary innovations (1) conditioning the diffusion process on a disparity-aware coordinate embedding and a warped input image, allowing for more precise stereo alignment than previous methods, and (2) an adaptive fusion mechanism that intelligently combines the diffusion-generated image with a warped image, improving both realism and disparity consistency. Through extensive training on 11 diverse stereo datasets, GenStereo demonstrates strong generalization ability. GenStereo achieves state-of-the-art performance in both stereo image generation and unsupervised stereo matching tasks. Our framework eliminates the need for complex hardware setups while enabling high-quality stereo image generation, making it valuable for both real-world applications and unsupervised learning scenarios. Project page is available at https://qjizhi.github.io/genstereo', 'score': 1, 'issue_id': 2766, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '2fdcfe786193f6cf', 'authors': ['Feng Qiao', 'Zhexiao Xiong', 'Eric Xing', 'Nathan Jacobs'], 'affiliations': ['Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2503.12720.jpg', 'data': {'categories': ['#robotics', '#video', '#3d', '#diffusion'], 'emoji': '👁️', 'ru': {'title': 'GenStereo: Революция в генерации стереоизображений с помощью диффузионных моделей', 'desc': 'GenStereo - это новый подход к генерации стереоизображений, основанный на диффузионных моделях. Метод использует встраивание координат с учетом диспаратности и деформированное входное изображение для точного стереовыравнивания. GenStereo также включает адаптивный механизм слияния, который объединяет сгенерированное и деформированное изображения для улучшения реалистичности и согласованности диспаратности. Обученный на 11 разнообразных стереодатасетах, GenStereo демонстрирует высокую обобщающую способность и достигает наилучших результатов в задачах генерации стереоизображений и неконтролируемого стереосопоставления.'}, 'en': {'title': 'GenStereo: Bridging Visual Quality and Geometric Accuracy in Stereo Image Generation', 'desc': 'This paper presents GenStereo, a novel diffusion-based method for generating high-quality stereo images. It addresses the challenges of stereo image generation by focusing on both visual quality and geometric accuracy, which are often at odds in existing methods. GenStereo innovates by conditioning the diffusion process on disparity-aware embeddings and using an adaptive fusion mechanism to enhance realism and consistency. The method shows strong performance across various stereo datasets, making it suitable for applications in extended reality, autonomous driving, and robotics without the need for complex hardware setups.'}, 'zh': {'title': 'GenStereo：高质量立体图像生成的新方法', 'desc': '本论文介绍了一种名为GenStereo的生成方法，旨在提高立体图像的质量和几何准确性。该方法通过条件扩散过程，结合视差感知的坐标嵌入和变形输入图像，实现更精确的立体对齐。GenStereo还采用了一种自适应融合机制，智能地将生成的图像与变形图像结合，从而提高了图像的真实感和视差一致性。经过在11个多样化的立体数据集上的广泛训练，GenStereo在立体图像生成和无监督立体匹配任务中表现出色，具有很强的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.08153', 'title': 'WISA: World Simulator Assistant for Physics-Aware Text-to-Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.08153', 'abstract': "Recent rapid advancements in text-to-video (T2V) generation, such as SoRA and Kling, have shown great potential for building world simulators. However, current T2V models struggle to grasp abstract physical principles and generate videos that adhere to physical laws. This challenge arises primarily from a lack of clear guidance on physical information due to a significant gap between abstract physical principles and generation models. To this end, we introduce the World Simulator Assistant (WISA), an effective framework for decomposing and incorporating physical principles into T2V models. Specifically, WISA decomposes physical principles into textual physical descriptions, qualitative physical categories, and quantitative physical properties. To effectively embed these physical attributes into the generation process, WISA incorporates several key designs, including Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier, enhancing the model's physics awareness. Furthermore, most existing datasets feature videos where physical phenomena are either weakly represented or entangled with multiple co-occurring processes, limiting their suitability as dedicated resources for learning explicit physical principles. We propose a novel video dataset, WISA-32K, collected based on qualitative physical categories. It consists of 32,000 videos, representing 17 physical laws across three domains of physics: dynamics, thermodynamics, and optics. Experimental results demonstrate that WISA can effectively enhance the compatibility of T2V models with real-world physical laws, achieving a considerable improvement on the VideoPhy benchmark. The visual exhibitions of WISA and WISA-32K are available in the https://360cvgroup.github.io/WISA/.", 'score': 0, 'issue_id': 2766, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '60c6c27b7b32f442', 'authors': ['Jing Wang', 'Ao Ma', 'Ke Cao', 'Jun Zheng', 'Zhanjie Zhang', 'Jiasong Feng', 'Shanyuan Liu', 'Yuhang Ma', 'Bo Cheng', 'Dawei Leng', 'Yuhui Yin', 'Xiaodan Liang'], 'affiliations': ['AI Research', 'Peng Cheng Laboratory', 'Shenzhen Campus of Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08153.jpg', 'data': {'categories': ['#games', '#benchmark', '#video', '#dataset', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'WISA: обучение моделей T2V физическим законам реального мира', 'desc': 'Статья представляет WISA (World Simulator Assistant) - фреймворк для внедрения физических принципов в модели генерации текста в видео (T2V). WISA разбивает физические принципы на текстовые описания, качественные категории и количественные свойства. Для улучшения физической осведомленности модели используются такие методы как Mixture-of-Physical-Experts Attention и Physical Classifier. Также авторы создали датасет WISA-32K из 32 000 видео, демонстрирующих 17 физических законов из трех областей физики.'}, 'en': {'title': 'Bridging Text-to-Video with Real-World Physics', 'desc': "This paper presents the World Simulator Assistant (WISA), a framework designed to improve text-to-video (T2V) generation by integrating physical principles into the models. WISA breaks down physical concepts into textual descriptions, qualitative categories, and quantitative properties, allowing T2V models to better understand and apply these principles. It introduces innovative components like Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier to enhance the model's awareness of physics. Additionally, the authors create a new dataset, WISA-32K, containing 32,000 videos that illustrate 17 physical laws, which helps train T2V models to align more closely with real-world physics."}, 'zh': {'title': '提升文本到视频生成的物理意识', 'desc': '本文介绍了一种新的框架，称为世界模拟助手（WISA），旨在改善文本到视频生成（T2V）模型对物理原则的理解。WISA通过将物理原则分解为文本描述、定性类别和定量属性，帮助生成模型更好地融入物理信息。该框架还引入了混合物理专家注意力（MoPA）和物理分类器等设计，增强了模型的物理意识。此外，WISA还提出了一个新的视频数据集WISA-32K，包含32,000个视频，涵盖17条物理定律，旨在为学习明确的物理原则提供更合适的资源。'}}}, {'id': 'https://huggingface.co/papers/2503.06269', 'title': 'Using Mechanistic Interpretability to Craft Adversarial Attacks against\n  Large Language Models', 'url': 'https://huggingface.co/papers/2503.06269', 'abstract': "Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting.", 'score': 0, 'issue_id': 2772, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': '623dbef3e618123c', 'authors': ['Thomas Winninger', 'Boussad Addad', 'Katarzyna Kapusta'], 'affiliations': ['Thales SIX GTS', 'Télécom SudParis'], 'pdf_title_img': 'assets/pdf/title_img/2503.06269.jpg', 'data': {'categories': ['#security', '#benchmark', '#interpretability', '#data', '#optimization', '#architecture', '#dataset'], 'emoji': '🛡️', 'ru': {'title': 'Эффективные состязательные атаки через анализ внутренних механизмов языковых моделей', 'desc': "Статья представляет новый метод создания состязательных атак на языковые модели, используя механистическую интерпретируемость. Авторы идентифицируют 'пространства принятия' - наборы векторов признаков, не вызывающие отказ модели, и применяют оптимизацию градиентного спуска для перенаправления вложений в эти пространства. Этот подход значительно снижает вычислительные затраты, достигая 80-95% успешности атак на современные модели за минуты или секунды. Исследование открывает новые направления как для разработки атак, так и для защиты, демонстрируя практическое применение механистической интерпретируемости."}, 'en': {'title': 'Bridging Interpretability and Adversarial Attacks for LLMs', 'desc': "This paper presents a new method for creating adversarial inputs for large language models (LLMs) by combining gradient-based optimization with mechanistic interpretability. The authors identify 'acceptance subspaces' where inputs are accepted by the model, and reroute embeddings from 'refusal subspaces' to these acceptance areas. This innovative approach significantly improves the efficiency of adversarial attacks, achieving success rates of 80-95% in a matter of minutes. The research not only advances attack strategies but also demonstrates the practical use of interpretability techniques in machine learning."}, 'zh': {'title': '利用机制可解释性提升对抗攻击成功率', 'desc': '这篇论文提出了一种新的白盒方法，用于生成针对大型语言模型（LLM）的对抗扰动。该方法结合了机制可解释性技术，首先识别接受子空间，然后通过基于梯度的优化将嵌入从拒绝子空间重新引导到接受子空间。与传统方法相比，这种方法显著降低了计算成本，并在几分钟或几秒内实现了80-95%的攻击成功率。该研究为攻击研究和防御开发开辟了新方向，同时展示了机制可解释性的实际应用。'}}}, {'id': 'https://huggingface.co/papers/2503.04625', 'title': 'START: Self-taught Reasoner with Tools', 'url': 'https://huggingface.co/papers/2503.04625', 'abstract': "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.", 'score': 66, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '8961f69e1eda24ad', 'authors': ['Chengpeng Li', 'Mingfeng Xue', 'Zhenru Zhang', 'Jiaxi Yang', 'Beichen Zhang', 'Xiang Wang', 'Bowen Yu', 'Binyuan Hui', 'Junyang Lin', 'Dayiheng Liu'], 'affiliations': ['Alibaba Group', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04625.jpg', 'data': {'categories': ['#long_context', '#rl', '#training', '#architecture', '#hallucinations', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'START: Самообучающаяся модель рассуждений с инструментами', 'desc': 'Статья представляет START - новую модель для рассуждений с длинной цепочкой мыслей, интегрирующую внешние инструменты. START использует фреймворк самообучения, включающий технику Hint-infer для стимулирования использования инструментов, и Hint Rejection Sampling Fine-Tuning для улучшения траекторий рассуждений. Модель значительно превосходит базовую QwQ-32B и достигает результатов на уровне современных моделей в сложных задачах рассуждений. START демонстрирует высокую точность на наборах данных по науке, математике и программированию уровня PhD и соревнований.'}, 'en': {'title': 'Enhancing Reasoning with External Tools: Introducing START', 'desc': "This paper presents START, a new long Chain-of-thought reasoning model that integrates external tools to improve reasoning capabilities. Traditional large reasoning models often struggle with hallucinations and inefficiencies, but START addresses these issues by allowing the model to perform complex computations and self-debugging. The innovation lies in two techniques: Hint-infer, which uses designed hints to encourage tool usage, and Hint Rejection Sampling Fine-Tuning, which refines the model's reasoning paths. START demonstrates superior performance on various benchmarks, surpassing its predecessor and competing effectively with state-of-the-art models."}, 'zh': {'title': '工具整合，推理更强！', 'desc': '本文介绍了一种新型的长链推理模型START（自我学习推理器与工具），它通过整合外部工具来增强推理能力。START利用代码执行进行复杂计算、自我检查、探索多种方法和自我调试，从而克服了大型推理模型（LRMs）在推理过程中常见的幻觉和低效问题。核心创新在于自我学习框架，包括提示推理（Hint-infer）和提示拒绝采样微调（Hint-RFT）两种技术，前者通过插入设计的提示来激发模型使用外部工具的能力。经过微调，START在多个科学问答和数学基准测试中表现优异，准确率显著高于基础模型QwQ-32B。'}}}, {'id': 'https://huggingface.co/papers/2503.04130', 'title': 'Token-Efficient Long Video Understanding for Multimodal LLMs', 'url': 'https://huggingface.co/papers/2503.04130', 'abstract': 'Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5\\% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to 8times and the decoding latency by 2.4-2.9times for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm', 'score': 59, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'f1d1afacd41dc0c7', 'authors': ['Jindong Jiang', 'Xiuyu Li', 'Zhijian Liu', 'Muyang Li', 'Guo Chen', 'Zhiqi Li', 'De-An Huang', 'Guilin Liu', 'Zhiding Yu', 'Kurt Keutzer', 'Sungjin Ahn', 'Jan Kautz', 'Hongxu Yin', 'Yao Lu', 'Song Han', 'Wonmin Byeon'], 'affiliations': ['KAIST', 'MIT', 'NVIDIA', 'Nanjing University', 'Rutgers University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.04130.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#long_context', '#architecture', '#video', '#inference', '#multimodal'], 'emoji': '🌪️', 'ru': {'title': 'STORM: Эффективное понимание длинных видео с помощью темпорального кодирования', 'desc': 'STORM - это новая архитектура для видео-LLM, которая использует специальный темпоральный энкодер на основе модели Mamba State Space между энкодером изображений и LLM. Эта архитектура позволяет интегрировать временную информацию в токены изображений, создавая обогащенные представления, сохраняющие динамику между кадрами во всей видеопоследовательности. STORM применяет стратегии сокращения токенов, включая выборку во время тестирования и пулинг во время обучения, что значительно снижает вычислительные затраты LLM. Результаты экспериментов показывают, что STORM достигает улучшения более чем на 5% на бенчмарках длинных видео, одновременно сокращая вычислительные затраты до 8 раз.'}, 'en': {'title': 'STORM: Revolutionizing Video Understanding with Temporal Insights', 'desc': 'This paper presents STORM, a new architecture designed to enhance video understanding in multimodal large language models (LLMs) by incorporating a temporal encoder. Unlike previous methods that treat video frames independently, STORM uses the Mamba State Space Model to capture the dynamics between frames, resulting in richer representations of video content. The architecture also implements token reduction strategies that significantly lower computational costs and improve processing speed without losing important temporal information. Evaluations demonstrate that STORM outperforms existing models on long video benchmarks while achieving substantial reductions in computation and latency.'}, 'zh': {'title': '高效视频理解的新突破：STORM模型', 'desc': '最近，基于视频的多模态大语言模型（Video-LLMs）在视频理解方面取得了显著进展，但许多现有方法在视觉骨干中独立处理帧，缺乏明确的时间建模，限制了捕捉动态模式的能力。为了解决这些问题，我们提出了STORM（时空令牌减少模型），它在图像编码器和大语言模型之间引入了专门的时间编码器。我们的时间编码器利用Mamba状态空间模型，将时间信息整合到图像令牌中，生成丰富的表示，保留整个视频序列中的帧间动态。通过这些技术的整合，我们的方法在提高性能的同时，显著减少了计算需求和推理延迟，实现了对长视频的高效理解。'}}}, {'id': 'https://huggingface.co/papers/2503.04724', 'title': 'LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM', 'url': 'https://huggingface.co/papers/2503.04724', 'abstract': 'Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training. Our code base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .', 'score': 41, 'issue_id': 2589, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '3113e03ae6f4ed42', 'authors': ['Sambal Shikhar', 'Mohammed Irfan Kurpath', 'Sahal Shaji Mullappilly', 'Jean Lahoud', 'Fahad Khan', 'Rao Muhammad Anwer', 'Salman Khan', 'Hisham Cholakkal'], 'affiliations': ['Linköping University, Sweden', 'Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), UAE'], 'pdf_title_img': 'assets/pdf/title_img/2503.04724.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#low_resource', '#audio', '#open_source', '#long_context'], 'emoji': '🗣️', 'ru': {'title': 'LLMVoX: Универсальный синтез речи для языковых моделей', 'desc': 'LLMVoX - это легковесная система синтеза речи, которая работает совместно с языковыми моделями, не требуя их модификации. Она обеспечивает высококачественный синтез речи с низкой задержкой, сохраняя все возможности базовой языковой модели. LLMVoX достигает более низкого показателя ошибок распознавания слов по сравнению с речевыми языковыми моделями, при этом работая с сопоставимой задержкой. Система может быть легко адаптирована для различных задач и языков, а также интегрирована с мультимодальными моделями.'}, 'en': {'title': 'Seamless Speech Synthesis with LLMVoX', 'desc': 'The paper introduces LLMVoX, a novel speech-to-speech dialogue system that utilizes a lightweight, 30M-parameter architecture to enhance multimodal interactions without compromising the linguistic capabilities of large language models (LLMs). Unlike existing systems that require extensive fine-tuning and often degrade conversational quality, LLMVoX operates efficiently with low latency and a significantly reduced Word Error Rate. It employs a multi-queue token streaming mechanism to decouple speech synthesis from LLM processing, enabling seamless dialogues of infinite length. Additionally, LLMVoX can adapt to new languages with minimal dataset adjustments and integrates with Vision-Language Models to support speech, text, and vision functionalities without extra multimodal training.'}, 'zh': {'title': 'LLMVoX：高效语音合成的新选择', 'desc': '本文提出了一种名为LLMVoX的轻量级语音合成系统，具有3000万参数，能够与任何大型语言模型（LLM）兼容。LLMVoX通过多队列令牌流系统，将语音合成与LLM处理解耦，从而实现低延迟和高质量的语音生成。与现有的语音增强LLM相比，LLMVoX在词错误率上显著降低，同时保持相似的延迟和用户满意度评分。该系统还支持无缝的无限长度对话，并能够通过数据集适应轻松扩展到新语言。'}}}, {'id': 'https://huggingface.co/papers/2503.03803', 'title': 'EgoLife: Towards Egocentric Life Assistant', 'url': 'https://huggingface.co/papers/2503.03803', 'abstract': 'We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.', 'score': 28, 'issue_id': 2581, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '52396234365a3fb0', 'authors': ['Jingkang Yang', 'Shuai Liu', 'Hongming Guo', 'Yuhao Dong', 'Xiamengwei Zhang', 'Sicheng Zhang', 'Pengyun Wang', 'Zitang Zhou', 'Binzhu Xie', 'Ziyue Wang', 'Bei Ouyang', 'Zhengyu Lin', 'Marco Cominelli', 'Zhongang Cai', 'Yuanhan Zhang', 'Peiyuan Zhang', 'Fangzhou Hong', 'Joerg Widmer', 'Francesco Gringoli', 'Lei Yang', 'Bo Li', 'Ziwei Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.03803.jpg', 'data': {'categories': ['#video', '#long_context', '#dataset', '#open_source', '#agents', '#multimodal', '#data', '#benchmark'], 'emoji': '👓', 'ru': {'title': 'EgoLife: ИИ-ассистент для повседневной жизни на базе умных очков', 'desc': 'Проект EgoLife представляет собой разработку эгоцентричного ассистента на основе очков дополненной реальности с искусственным интеллектом. Исследователи собрали обширный набор данных EgoLife Dataset, включающий 300 часов эгоцентричного видео повседневной жизни шести участников. На основе этих данных создан набор задач EgoLifeQA для тестирования вопросно-ответных систем в контексте повседневной жизни. Для решения технических задач разработана система EgoButler, включающая мультимодальную модель EgoGPT и компонент для ответов на вопросы с длинным контекстом EgoRAG.'}, 'en': {'title': 'Empowering Daily Life with Egocentric AI Assistance', 'desc': 'The paper presents EgoLife, an AI-powered life assistant designed to enhance personal efficiency through wearable glasses that capture egocentric video data. A comprehensive dataset, the EgoLife Dataset, was created by recording daily activities of participants over a week, resulting in 300 hours of multimodal data with detailed annotations. The study introduces EgoLifeQA, a set of question-answering tasks that utilize this dataset to assist users with practical life questions and personalized recommendations. To tackle challenges in visual-audio model development and long-context question answering, the authors propose EgoButler, which includes EgoGPT for video understanding and EgoRAG for retrieval-based answering.'}, 'zh': {'title': '智能生活助手，提升个人效率', 'desc': '我们介绍了EgoLife项目，旨在开发一个以自我为中心的生活助手，通过AI驱动的可穿戴眼镜提升个人效率。我们进行了全面的数据收集研究，六名参与者共同生活一周，使用AI眼镜记录日常活动，形成了EgoLife数据集，包含300小时的多视角、多模态日常生活数据。基于该数据集，我们推出了EgoLifeQA，一个针对生活的长文本问答任务，旨在提供实用的日常生活帮助。为了解决关键技术挑战，我们引入了EgoButler系统，包括EgoGPT和EgoRAG，前者在自我中心视频理解上表现出色，后者支持超长文本问题的回答。'}}}, {'id': 'https://huggingface.co/papers/2503.02972', 'title': 'LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation', 'url': 'https://huggingface.co/papers/2503.02972', 'abstract': 'Effective evaluation of the reasoning capabilities of large language models (LLMs) are susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging evaluation benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerous question variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including OpenAI o1-preview and DeepSeem R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to overestimating the reasoning capabilities of frontier models.', 'score': 22, 'issue_id': 2585, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '0eb195e2704f3d5d', 'authors': ['Jude Khouja', 'Karolina Korgul', 'Simi Hellsten', 'Lingyi Yang', 'Vlad Neacs', 'Harry Mayne', 'Ryan Kearns', 'Andrew Bean', 'Adam Mahdi'], 'affiliations': ['Asia-Pacific Linguistics Olympiad', 'Hong Kong Linguistics Olympiad', 'National University of Science and Technology POLITEHNICA Bucharest, Romania', 'United Kingdom Linguistics Olympiad', 'University of Glasgow, Glasgow, United Kingdom', 'University of Oxford, Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2503.02972.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#hallucinations', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Новый метод оценки рассуждений LLM без влияния предварительных знаний', 'desc': 'Статья представляет новый подход к оценке способностей больших языковых моделей (LLM) к рассуждению. Авторы разработали фреймворк LINGOLY-TOO, который генерирует лингвистические задачи с обфускацией систем письма реальных языков. Эксперименты показали, что современные модели, включая OpenAI и DeepSeem, испытывают трудности с продвинутыми рассуждениями. Исследование выявило, что предварительное знакомство с данными может приводить к переоценке возможностей LLM в области рассуждений.'}, 'en': {'title': 'Unmasking Reasoning: Evaluating LLMs Beyond Memorization', 'desc': 'This paper addresses the challenge of accurately evaluating the reasoning abilities of large language models (LLMs) by introducing a new framework that minimizes the impact of memorization on performance assessments. The authors present LINGOLY-TOO, a benchmark designed to test linguistic reasoning through dynamically generated questions that obfuscate the original writing systems of languages. By using orthographic templates, they create multiple variations of questions that maintain the necessary reasoning steps while reducing the chance of models having seen specific instances during training. The results indicate that leading models struggle with complex reasoning tasks and show significant performance differences based on the question format, revealing the influence of prior data exposure on their evaluation.'}, 'zh': {'title': '揭示大型语言模型推理能力的真实面貌', 'desc': '本文提出了一种评估大型语言模型（LLMs）推理能力的新框架，旨在减少由于数据暴露导致的评估过高的问题。我们开发了LINGOLY-TOO，这是一个具有挑战性的语言推理评估基准，通过使用正字法模板动态模糊真实语言的书写系统，生成多种问题变体。实验结果表明，前沿模型在高级推理任务中表现不佳，并且在相同问题的不同排列中，LLMs的准确性存在显著差异。我们的研究揭示了LLMs响应生成的复杂性，并提供了证据表明，先前的数据暴露会导致对前沿模型推理能力的高估。'}}}, {'id': 'https://huggingface.co/papers/2503.04644', 'title': 'IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval', 'url': 'https://huggingface.co/papers/2503.04644', 'abstract': 'We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.', 'score': 17, 'issue_id': 2585, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'c4f19dc17abc0e8f', 'authors': ['Tingyu Song', 'Guo Gan', 'Mingsheng Shang', 'Yilun Zhao'], 'affiliations': ['Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences', 'School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04644.jpg', 'data': {'categories': ['#science', '#healthcare', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'IFIR: Новый стандарт оценки интеллектуального поиска информации', 'desc': 'IFIR - это новый комплексный бенчмарк для оценки информационного поиска с выполнением инструкций в экспертных областях. Он включает 2426 примеров высокого качества в 8 подмножествах из 4 специализированных доменов: финансы, право, здравоохранение и научная литература. IFIR позволяет проводить детальный анализ возможностей поиска с выполнением инструкций разной сложности. Эксперименты на 15 передовых моделях поиска показали значительные трудности в эффективном выполнении сложных доменно-специфичных инструкций.'}, 'en': {'title': 'IFIR: Benchmarking Instruction-Following in Expert Domains', 'desc': 'The paper presents IFIR, a new benchmark for assessing how well information retrieval systems can follow instructions in specialized fields like finance, law, healthcare, and science. It consists of 2,426 examples that simulate real-world retrieval tasks requiring tailored instructions. The benchmark allows for a nuanced evaluation of retrieval models, particularly focusing on their ability to handle varying complexities of instructions. The authors also introduce a new evaluation method using large language models (LLMs) to measure performance, revealing that existing models struggle with complex, domain-specific tasks and providing insights for future improvements.'}, 'zh': {'title': 'IFIR：专家领域指令跟随检索的首个基准', 'desc': '我们介绍了IFIR，这是第一个全面的基准，用于评估专家领域中的指令跟随信息检索（IR）。IFIR包含2426个高质量示例，涵盖金融、法律、医疗和科学文献等四个专业领域的八个子集。每个子集针对一个或多个特定领域的检索任务，模拟了需要定制指令的真实场景。通过对15个前沿检索模型的广泛实验，我们的结果显示，当前模型在有效跟随复杂的领域特定指令方面面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.03983', 'title': 'Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities', 'url': 'https://huggingface.co/papers/2503.03983', 'abstract': 'Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/.', 'score': 17, 'issue_id': 2581, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '952e4cb72ec34df8', 'authors': ['Sreyan Ghosh', 'Zhifeng Kong', 'Sonal Kumar', 'S Sakshi', 'Jaehyeon Kim', 'Wei Ping', 'Rafael Valle', 'Dinesh Manocha', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA, Santa Clara, CA, USA', 'University of Maryland, College Park, MD, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.03983.jpg', 'data': {'categories': ['#audio', '#long_context', '#dataset', '#small_models', '#reasoning', '#synthetic', '#open_source', '#benchmark'], 'emoji': '🎵', 'ru': {'title': 'AF2: Революция в понимании аудио искусственным интеллектом', 'desc': 'Audio Flamingo 2 (AF2) - это усовершенствованная аудио-языковая модель (ALM) с продвинутыми возможностями понимания и рассуждения об аудио. Модель использует специальную модель CLAP, синтетические данные Audio QA и многоступенчатую стратегию обучения. AF2 достигает наилучших результатов среди существующих моделей на более чем 20 тестах, используя всего 3 миллиарда параметров. Кроме того, исследователи расширили возможности модели для работы с длинными аудиосегментами и создали набор данных LongAudio для обучения ALM на задачах описания и ответов на вопросы по длинным аудио.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Audio Flamingo 2', 'desc': 'This paper presents Audio Flamingo 2 (AF2), an advanced Audio-Language Model (ALM) designed to enhance audio understanding and reasoning. AF2 utilizes a custom CLAP model and synthetic Audio QA data to improve fine-grained audio reasoning, along with a multi-stage curriculum learning approach. The model achieves state-of-the-art results with a relatively small 3B parameter language model, outperforming larger models on over 20 benchmarks. Additionally, it introduces LongAudio, a new dataset for training on long audio segments, and demonstrates exceptional performance on the LongAudioBench for evaluating long audio understanding.'}, 'zh': {'title': '音频理解的新突破：Audio Flamingo 2', 'desc': '本文介绍了Audio Flamingo 2（AF2），这是一种具有先进音频理解和推理能力的音频语言模型（ALM）。AF2利用了定制的CLAP模型、合成的音频问答数据以及多阶段的课程学习策略。AF2在仅使用一个3B参数的小型语言模型的情况下，超越了20多个基准测试中的大型开源和专有模型，达到了最先进的性能。此外，AF2首次扩展了对长音频片段（30秒到5分钟）的理解，并提出了LongAudio数据集，用于训练ALM在长音频标注和问答任务上的能力。'}}}, {'id': 'https://huggingface.co/papers/2502.20258', 'title': 'LLM as a Broken Telephone: Iterative Generation Distorts Information', 'url': 'https://huggingface.co/papers/2502.20258', 'abstract': 'As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the "broken telephone" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.', 'score': 17, 'issue_id': 2580, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '5b26055854ae3d90', 'authors': ['Amr Mohamed', 'Mingmeng Geng', 'Michalis Vazirgiannis', 'Guokan Shang'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI', 'SISSA'], 'pdf_title_img': 'assets/pdf/title_img/2502.20258.jpg', 'data': {'categories': ['#hallucinations', '#data', '#long_context', '#alignment', '#multimodal', '#training'], 'emoji': '📞', 'ru': {'title': "Эффект 'испорченного телефона' в языковых моделях", 'desc': 'Исследование посвящено изучению эффекта искажения информации при многократной обработке собственных выходных данных большими языковыми моделями (LLM). Авторы провели эксперименты на основе переводов, чтобы оценить накопление искажений в зависимости от выбора языка и сложности цепочки обработки. Результаты показывают, что деградация информации неизбежна, но может быть смягчена с помощью стратегических методов формулировки запросов. Исследование поднимает важные вопросы о надежности контента, генерируемого LLM в итеративных рабочих процессах.'}, 'en': {'title': 'Mitigating Distortion in Iterative LLM Outputs', 'desc': "This paper explores how large language models (LLMs) can distort information when they repeatedly process their own outputs, similar to the 'broken telephone' effect seen in human communication. The researchers conducted translation-based experiments to observe how information degradation occurs over time, which is affected by the choice of language and the complexity of the generation chain. They found that while some level of distortion is unavoidable, it can be reduced by using specific prompting strategies. These insights highlight the potential risks of relying on LLMs for generating content in iterative processes, raising concerns about the accuracy of AI-generated information."}, 'zh': {'title': '探讨大型语言模型的信息扭曲与可靠性', 'desc': '本研究探讨了大型语言模型（LLM）在反复处理自身输出时是否会扭曲信息，类似于人类沟通中的“破电话”效应。通过基于翻译的实验，我们发现信息扭曲会随着时间的推移而累积，受语言选择和链条复杂性的影响。尽管信息退化是不可避免的，但通过战略性提示技术可以减轻这种影响。研究结果为AI介导的信息传播的长期影响提供了重要见解，提出了关于LLM生成内容在迭代工作流程中可靠性的重要问题。'}}}, {'id': 'https://huggingface.co/papers/2503.04725', 'title': 'L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling', 'url': 'https://huggingface.co/papers/2503.04725', 'abstract': "We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L^2M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.", 'score': 15, 'issue_id': 2582, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '51e8f1332666da31', 'authors': ['Zhuo Chen', 'Oriol Mayné i Comas', 'Zhuotao Jin', 'Di Luo', 'Marin Soljačić'], 'affiliations': ['Harvard University', 'Massachusetts Institute of Technology', 'NSF AI Institute for Artificial Intelligence and Fundamental Interactions', 'Polytechnic University of Catalonia', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.04725.jpg', 'data': {'categories': ['#training', '#architecture', '#math', '#long_context'], 'emoji': '📏', 'ru': {'title': 'Масштабирование взаимной информации - ключ к моделированию длинного контекста', 'desc': 'Статья представляет собой исследование в области обработки естественного языка, фокусирующееся на закономерностях взаимной информации в длинных текстах. Авторы формулируют условие L^2M для моделирования длинного контекста, связывающее способность модели обрабатывать длинные последовательности с масштабированием ее скрытого состояния. Результаты подтверждены экспериментами на трансформерах и моделях пространства состояний. Работа закладывает теоретическую основу для развития больших языковых моделей с увеличенной длиной контекста.'}, 'en': {'title': 'Unlocking Long-Range Dependencies in Language Models', 'desc': "This paper introduces a new scaling law for bipartite mutual information that is crucial for understanding long-range dependencies in natural language processing. Unlike traditional two-point mutual information, this scaling law operates independently and is essential for effective long-context language modeling. The authors propose the Long-context Language Modeling (L^2M) condition, which connects a model's ability to handle long contexts with the size of its latent state for retaining past information. Experimental validation on transformers and state space models supports the theoretical framework, paving the way for advancements in large language models with extended context lengths."}, 'zh': {'title': '长上下文建模的新法则', 'desc': '本文严谨地建立了自然语言中的双向互信息缩放法则，该法则控制着长距离依赖关系。我们展示了这一缩放法则与传统的两点互信息不同，并且独立缩放，是理解长上下文语言建模的关键。通过这一缩放法则，我们提出了长上下文语言建模（L^2M）条件，将模型有效建模长上下文长度的能力与其存储过去信息的潜在状态大小的缩放联系起来。我们的结果通过对变换器和状态空间模型的实验得到了验证，为大型语言模型向更长上下文长度的发展奠定了理论基础。'}}}, {'id': 'https://huggingface.co/papers/2503.04598', 'title': 'HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization', 'url': 'https://huggingface.co/papers/2503.04598', 'abstract': 'Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose HybridNorm, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm.', 'score': 15, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '78b05ed4e27a874b', 'authors': ['Zhijian Zhuo', 'Yutao Zeng', 'Ya Wang', 'Sijun Zhang', 'Jian Yang', 'Xiaoqing Li', 'Xun Zhou', 'Jinwen Ma'], 'affiliations': ['Capital University of Economics and Business', 'School of Mathematical Sciences, Peking University', 'SeedFoundation-Model, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.04598.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#architecture', '#open_source'], 'emoji': '🔀', 'ru': {'title': 'HybridNorm: Гибридная нормализация для улучшения обучения и производительности трансформеров', 'desc': 'Статья представляет новый метод нормализации для трансформеров под названием HybridNorm. Этот подход сочетает преимущества Pre-Norm и Post-Norm стратегий, применяя QKV нормализацию в механизме внимания и Post-Norm в FFN каждого блока трансформера. HybridNorm показывает лучшие результаты по сравнению с существующими методами как для плотных, так и для разреженных архитектур. Эксперименты демонстрируют, что HybridNorm обеспечивает более стабильное обучение и улучшенную производительность для глубоких моделей трансформеров, особенно в контексте больших языковых моделей.'}, 'en': {'title': 'HybridNorm: The Best of Both Normalization Worlds for Transformers', 'desc': 'This paper introduces HybridNorm, a new normalization strategy for training deep transformer networks, which combines the strengths of Pre-Norm and Post-Norm methods. Pre-Norm structures help with training stability but often lead to lower performance, while Post-Norm structures typically perform better but can complicate training. HybridNorm applies QKV normalization in the attention mechanism and Post-Norm in the feed-forward network, resulting in improved training stability and performance. Experiments demonstrate that HybridNorm outperforms both Pre-Norm and Post-Norm across various benchmarks, making it a promising approach for enhancing large language models.'}, 'zh': {'title': 'HybridNorm：提升变换器模型训练的稳定性与性能', 'desc': '本文提出了一种新的混合归一化策略，称为HybridNorm，旨在解决深度变换器网络训练中的层归一化位置问题。HybridNorm结合了Pre-Norm和Post-Norm的优点，在注意力机制中使用QKV归一化，而在前馈网络中使用Post-Norm。实验结果表明，HybridNorm在稠密和稀疏架构中均优于传统的Pre-Norm和Post-Norm方法，提升了大语言模型的训练稳定性和性能。该研究为深度变换器模型的训练和性能改进提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2503.04222', 'title': 'FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion', 'url': 'https://huggingface.co/papers/2503.04222', 'abstract': 'We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.', 'score': 11, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'c7d793a0b91efd5d', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Canbin Huang', 'Guosheng Liang', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04222.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#small_models', '#rlhf', '#transfer_learning', '#open_source', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Слияние мощи больших языковых моделей в компактном формате', 'desc': 'FuseChat-3.0 представляет собой набор больших языковых моделей (LLM), разработанных путем интеграции сильных сторон гетерогенных исходных LLM в более компактные целевые модели. Процесс обучения включает два ключевых этапа: контролируемая тонкая настройка и оптимизация прямых предпочтений. Результирующие модели FuseChat-3.0 демонстрируют значительный прирост производительности в различных задачах, включая следование инструкциям, общие знания, математику и программирование. Используя Llama-3.1-8B-Instruct в качестве целевой модели, подход авторов достигает среднего улучшения на 6,8 пунктов по 14 бенчмаркам.'}, 'en': {'title': 'Fusing Strengths for Smarter, Smaller Models', 'desc': 'FuseChat-3.0 is a collection of large language models (LLMs) that combines the strengths of various larger source models into smaller, more efficient target models. The training process involves supervised fine-tuning to align the target models with the source models, followed by Direct Preference Optimization to enhance performance based on preferences from multiple sources. This innovative approach leads to significant improvements in tasks like instruction following, general knowledge, mathematics, and coding. The results show an average performance boost of 6.8 points across 14 benchmarks, with particularly impressive gains in instruction-following tasks.'}, 'zh': {'title': '融合多源模型，提升语言理解能力', 'desc': '我们介绍了FuseChat-3.0，这是一个通过整合不同来源的大型语言模型（LLMs）优势而开发的更紧凑的目标LLM套件。源模型包括强大的Gemma-2-27B-it、Mistral-Large-Instruct-2407、Qwen-2.5-72B-Instruct和Llama-3.1-70B-Instruct。目标模型则集中在三种广泛使用的小型变体上，以及两个超紧凑选项。通过专门的数据构建协议和两阶段的训练流程，FuseChat-3.0在多个任务上表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.04094', 'title': 'PokéChamp: an Expert-level Minimax Language Agent', 'url': 'https://huggingface.co/papers/2503.04094', 'abstract': "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\\'emon battles. Built on a general framework for two-player competitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently outperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o, with a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the Pok\\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.", 'score': 9, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'bffe348d8443d4c2', 'authors': ['Seth Karten', 'Andy Luu Nguyen', 'Chi Jin'], 'affiliations': ['Department of Computer Science, Princeton University', 'Department of Electrical and Computer Engineering, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04094.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#agents', '#games'], 'emoji': '🎮', 'ru': {'title': 'PokeChamp: Революция в ИИ для игровых стратегий с использованием языковых моделей', 'desc': 'Статья представляет PokeChamp - агента на основе минимакса, использующего большие языковые модели (LLM) для боев в Pokémon. PokeChamp применяет LLM для выборки действий игрока, моделирования оппонента и оценки функции ценности, что позволяет эффективно использовать историю игры и знания человека. При использовании GPT-4o агент достигает высокого процента побед против существующих ботов и попадает в топ-30% - 10% игроков на онлайн-платформе Pokémon Showdown. Исследование также включает создание крупнейшего датасета боев Pokémon и серии тестов для оценки навыков ведения боя.'}, 'en': {'title': "Pok'eChamp: Elevating Pokémon Battles with LLMs", 'desc': "Pok'eChamp is a minimax agent designed for Pokémon battles that utilizes Large Language Models (LLMs) to improve its decision-making process. It replaces traditional components of the minimax algorithm with LLMs for player action sampling, opponent modeling, and value function estimation, allowing it to better understand gameplay history and human strategies. The agent demonstrates impressive performance, achieving a 76% win rate against top LLM-based bots and a 64% win rate with a smaller model, showcasing its effectiveness in competitive play. Additionally, Pok'eChamp compiles a large dataset of over 3 million Pokémon battles to create benchmarks for evaluating battling skills and encourages further research in integrating LLMs with game-theoretic approaches."}, 'zh': {'title': 'PokéChamp：宝可梦对战中的智能代理', 'desc': '本文介绍了PokéChamp，一个基于大型语言模型（LLMs）的极小极大算法代理，用于宝可梦对战。该代理利用LLMs的通用能力，增强了极小极大树搜索，替代了玩家动作采样、对手建模和价值函数估计等关键模块。PokéChamp在不需要额外训练的情况下，能够有效利用游戏历史和人类知识，减少搜索空间并解决部分可观测性问题。经过评估，PokéChamp在宝可梦对战中表现优异，赢得了76%的胜率，展示了其在多智能体问题中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.01917', 'title': 'How to Steer LLM Latents for Hallucination Detection?', 'url': 'https://huggingface.co/papers/2503.01917', 'abstract': "Hallucinations in LLMs pose a significant concern to their safe deployment in real-world applications. Recent approaches have leveraged the latent space of LLMs for hallucination detection, but their embeddings, optimized for linguistic coherence rather than factual accuracy, often fail to clearly separate truthful and hallucinated content. To this end, we propose the Truthfulness Separator Vector (TSV), a lightweight and flexible steering vector that reshapes the LLM's representation space during inference to enhance the separation between truthful and hallucinated outputs, without altering model parameters. Our two-stage framework first trains TSV on a small set of labeled exemplars to form compact and well-separated clusters. It then augments the exemplar set with unlabeled LLM generations, employing an optimal transport-based algorithm for pseudo-labeling combined with a confidence-based filtering process. Extensive experiments demonstrate that TSV achieves state-of-the-art performance with minimal labeled data, exhibiting strong generalization across datasets and providing a practical solution for real-world LLM applications.", 'score': 8, 'issue_id': 2592, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': 'cbd43445771d58c3', 'authors': ['Seongheon Park', 'Xuefeng Du', 'Min-Hsuan Yeh', 'Haobo Wang', 'Yixuan Li'], 'affiliations': ['Department of Computer Sciences, University of Wisconsin-Madison, USA', 'School of Software Technology, Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01917.jpg', 'data': {'categories': ['#training', '#inference', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'TSV: Эффективное разделение правды и вымысла в LLM', 'desc': 'Статья представляет новый метод обнаружения галлюцинаций в больших языковых моделях (LLM) - Truthfulness Separator Vector (TSV). TSV - это легковесный вектор, который изменяет пространство представлений LLM во время вывода, чтобы улучшить разделение между правдивым и галлюцинированным содержанием. Метод использует двухэтапный подход: сначала TSV обучается на небольшом наборе размеченных примеров, а затем применяется алгоритм псевдо-разметки на основе оптимального транспорта для расширения набора данных. Эксперименты показывают, что TSV достигает современного уровня производительности с минимальным количеством размеченных данных.'}, 'en': {'title': 'Enhancing Truthfulness in LLMs with TSV', 'desc': 'This paper addresses the issue of hallucinations in large language models (LLMs), which can lead to the generation of false information. The authors introduce the Truthfulness Separator Vector (TSV), a novel method that modifies the representation space of LLMs during inference to better distinguish between accurate and hallucinated content. TSV is trained using a small set of labeled examples and then enhanced with additional unlabeled data through a unique pseudo-labeling technique. The results show that TSV significantly improves the accuracy of LLM outputs while requiring minimal labeled data, making it a valuable tool for safe LLM deployment.'}, 'zh': {'title': '提升LLM真实与幻觉内容分离的真相分离向量', 'desc': '在大型语言模型（LLMs）中，幻觉现象对其在实际应用中的安全部署构成了重大挑战。虽然最近的方法利用了LLMs的潜在空间进行幻觉检测，但其嵌入通常更注重语言连贯性而非事实准确性，导致难以清晰区分真实和幻觉内容。为此，我们提出了真相分离向量（TSV），这是一种轻量且灵活的引导向量，可以在推理过程中重塑LLM的表示空间，从而增强真实输出与幻觉输出之间的分离。我们的两阶段框架首先在小规模标记样本上训练TSV，以形成紧凑且分离良好的聚类，然后通过基于最优传输的伪标记算法和基于置信度的过滤过程，利用未标记的LLM生成数据来扩展样本集。'}}}, {'id': 'https://huggingface.co/papers/2503.02495', 'title': 'Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer', 'url': 'https://huggingface.co/papers/2503.02495', 'abstract': "Mixture-of-Experts (MoE) enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. However, expert in exist MoE paradigm works as an individual, thereby lacking high-quality expert interactions. Moreover, they have not been effectively extended to attention block, which constrains further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement dynamic routing on input data and experts. Our approach advances MoE design with three key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the model employed with UoE surpass Full Attention, state-of-art MoEs and efficient transformers in several tasks across image and natural language domains. The source codes are available at https://github.com/YujiaoYang-work/UoE.", 'score': 7, 'issue_id': 2586, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'b879dd88adfef125', 'authors': ['Yujiao Yang', 'Jing Lian', 'Linhui Li'], 'affiliations': ['School of Mechanical Engineering, Dalian University of Technology, Dalian 116024, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.02495.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'UoE: Новый уровень эффективности трансформеров через объединение экспертов', 'desc': 'Статья представляет новый подход к архитектуре трансформеров, называемый Union-of-Experts (UoE). UoE улучшает модель Mixture-of-Experts (MoE), разбивая трансформер на группы экспертов и применяя динамическую маршрутизацию как к входным данным, так и к экспертам. Ключевые инновации включают декомпозицию экспертов для блоков MLP и внимания, новые парадигмы маршрутизации и оптимизированную параллельную реализацию. Эксперименты показывают, что UoE превосходит существующие модели в задачах обработки изображений и естественного языка.'}, 'en': {'title': 'Union-of-Experts: Enhancing Efficiency and Interaction in Transformers', 'desc': 'The paper introduces Union-of-Experts (UoE), a novel approach that enhances the Mixture-of-Experts (MoE) framework by improving expert interactions and extending its application to attention blocks. UoE decomposes transformers into groups of experts and utilizes dynamic routing to optimize input data processing. Key innovations include equitant expert decomposition, two routing paradigms for data and expert selection, and a new architecture featuring Selective Multi-Head Attention and Union-of-MLP-Experts. Experimental results show that UoE outperforms existing models in various tasks, demonstrating its effectiveness in both image and natural language processing.'}, 'zh': {'title': '专家联合模型：提升效率与性能的创新之路', 'desc': '混合专家模型（MoE）通过提高模型性能并保持计算效率，适合大规模应用。然而，现有的MoE模型中的专家作为个体工作，缺乏高质量的专家交互。此外，MoE尚未有效扩展到注意力模块，这限制了进一步的效率提升。为了解决这些问题，我们提出了专家联合模型（UoE），通过将变换器分解为等效的专家组，并在输入数据和专家之间实现动态路由，从而改进了MoE的设计。'}}}, {'id': 'https://huggingface.co/papers/2503.01901', 'title': 'Identifying Sensitive Weights via Post-quantization Integral', 'url': 'https://huggingface.co/papers/2503.01901', 'abstract': "Serving Large Language Models (LLMs) is costly. However, post-training weight quantization can address this problem by both compressing their sizes for limited memory and saving bandwidth for acceleration. As not all weight dimensions are equally important, those methods typically rely on a sensitivity metric, which indicates the element-wise influence of weights on loss function and is used to preprocess original weights for better quantization. In this work, we conduct an empirical study on the accuracy of the sensitivity metric, and find that existing gradient and Hessian based metrics are very inaccurate: they underestimate quantization's impact on the loss function by orders of magnitude, mainly due to the small convergence radius of local 2nd order approximation, \\ie, gradient and Hessian term in Taylor's formula. To tackle this problem, we propose Post-quantization Integral (PQI), an accurate metric to estimate posterior sensitivity in a fine-grained manner. To leverage this accurate metric, we further propose ReQuant, a simple yet powerful framework that mainly consists of two Dense-and-Sparse detach components: self-adaptive outlier selection and step-wise significant weights detach. Results show that ReQuant boosts state-of-the-art post-training quantization methods, with a pronounced improvement of 2.66 perplexity gain on Llama 3.2 1B with QTIP.", 'score': 7, 'issue_id': 2585, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '1045983ed982a00b', 'authors': ['Yuezhou Hu', 'Weiyu Huang', 'Zichen Liang', 'Chang Chen', 'Jintao Zhang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01901.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': '⚖️', 'ru': {'title': 'Точная квантизация для эффективных языковых моделей', 'desc': 'Эта статья представляет новый метод квантизации весов для крупных языковых моделей (LLM), называемый ReQuant. Авторы обнаружили, что существующие метрики чувствительности весов неточны и предложили более точную метрику - Post-quantization Integral (PQI). ReQuant использует PQI вместе с адаптивным выбором выбросов и пошаговым отделением значимых весов. Результаты показывают, что ReQuant улучшает современные методы пост-тренировочной квантизации, значительно повышая производительность модели Llama 3.2 1B.'}, 'en': {'title': 'Enhancing LLM Efficiency with Accurate Quantization Metrics', 'desc': 'This paper addresses the high costs associated with serving large language models (LLMs) by introducing post-training weight quantization techniques. It highlights the inadequacy of existing sensitivity metrics, which fail to accurately predict the impact of quantization on model performance. The authors propose a new metric called Post-quantization Integral (PQI) that provides a more precise estimation of weight sensitivity. Additionally, they introduce ReQuant, a framework that enhances quantization methods by effectively selecting significant weights and improving overall model accuracy.'}, 'zh': {'title': '提升量化精度，降低模型成本', 'desc': '这篇论文讨论了大型语言模型（LLMs）在服务时的高成本问题。通过后训练权重量化，可以压缩模型大小，节省内存和带宽。研究发现，现有的基于梯度和海森矩阵的敏感度度量不够准确，低估了量化对损失函数的影响。为了解决这个问题，提出了后量化积分（PQI）作为一种更精确的敏感度度量，并进一步提出了ReQuant框架，以提高后训练量化方法的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.04606', 'title': 'The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation', 'url': 'https://huggingface.co/papers/2503.04606', 'abstract': 'Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a sim14,000times compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.', 'score': 7, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'c635690f3edc1186', 'authors': ['Aoxiong Yin', 'Kai Shen', 'Yichong Leng', 'Xu Tan', 'Xinyu Zhou', 'Juncheng Li', 'Siliang Tang'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'Moonshot AI, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04606.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#benchmark', '#long_context', '#architecture', '#video'], 'emoji': '🎬', 'ru': {'title': 'LanDiff: гибридный подход к генерации видео из текста', 'desc': 'Статья представляет LanDiff - гибридную архитектуру для генерации видео из текста, сочетающую преимущества авторегрессионных языковых моделей и диффузионных моделей. LanDiff включает семантический токенизатор, языковую модель для генерации семантических токенов и потоковую диффузионную модель для уточнения деталей. Модель LanDiff объемом 5 миллиардов параметров превзошла современные открытые и коммерческие модели в бенчмарке VBench T2V. Также LanDiff показала лучшие результаты в генерации длинных видео по сравнению с другими открытыми моделями.'}, 'en': {'title': 'LanDiff: Bridging Language and Visuals for Superior Video Generation', 'desc': 'This paper introduces LanDiff, a novel framework for text-to-video (T2V) generation that combines the strengths of autoregressive language models and diffusion models. It addresses the limitations of each approach by using a coarse-to-fine generation strategy, which enhances both visual quality and semantic understanding. Key innovations include a semantic tokenizer for efficient compression of visual features, a language model that generates meaningful semantic tokens, and a streaming diffusion model that refines these tokens into high-quality videos. The results demonstrate that LanDiff outperforms existing state-of-the-art models in T2V tasks, particularly in generating long videos.'}, 'zh': {'title': 'LanDiff：文本到视频生成的新突破', 'desc': '本文提出了一种名为LanDiff的混合框架，旨在结合自回归语言模型和扩散模型的优点，以实现文本到视频的生成。该框架通过粗到细的生成过程，克服了各自的局限性，提升了视觉质量和语义理解。LanDiff引入了三项关键创新，包括高效的语义压缩技术、生成高层语义关系的语言模型，以及将粗略语义精炼为高保真视频的流式扩散模型。实验结果表明，LanDiff在VBench T2V基准测试中表现优异，超越了现有的开源和商业模型。'}}}, {'id': 'https://huggingface.co/papers/2503.04378', 'title': 'Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks', 'url': 'https://huggingface.co/papers/2503.04378', 'abstract': 'Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.', 'score': 6, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '926e56aa51a0fefe', 'authors': ['Zhilin Wang', 'Jiaqi Zeng', 'Olivier Delalleau', 'Daniel Egert', 'Ellie Evans', 'Hoo-Chang Shin', 'Felipe Soares', 'Yi Dong', 'Oleksii Kuchaiev'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.04378.jpg', 'data': {'categories': ['#training', '#benchmark', '#inference', '#reasoning', '#rlhf', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'Масштабирование при выводе для открытых задач: от черновика к улучшенному ответу', 'desc': 'Статья описывает новый метод масштабирования во время вывода для открытых задач общего домена. Авторы предлагают систему из трех моделей: одна генерирует начальный ответ, вторая дает обратную связь, а третья редактирует ответ на основе этой обратной связи. Эксперименты показывают, что такой подход позволяет значительно улучшить производительность на бенчмарке Arena Hard. При оптимальном масштабировании система на основе моделей семейства Llama 3 размером 70B достигает наилучших результатов, превосходя OpenAI o1 и DeepSeek R1.'}, 'en': {'title': 'Enhancing Open-Ended Task Performance through Feedback and Editing', 'desc': 'This paper discusses a novel approach to improve inference-time scaling in machine learning models, particularly for open-ended tasks. It introduces a system where one model generates an initial response, a second model provides feedback, and a third model edits the response based on that feedback. This method allows for more flexible applications beyond traditional tasks that require verifiable answers, such as math or coding. The authors demonstrate that by optimizing the number of drafts, feedback, and edits, their model achieves state-of-the-art performance on the Arena Hard benchmark, outperforming previous models.'}, 'zh': {'title': '推理时间扩展：提升开放性任务的性能', 'desc': '本文探讨了推理时间扩展在机器学习模型中的重要性，尤其是OpenAI o1和DeepSeek R1等模型。许多现有技术要求任务的答案可验证，这限制了它们在开放性任务中的应用。我们借鉴人类如何进行初步尝试、请求反馈并根据反馈进行改进的过程，开发了专门的反馈和编辑模型。通过优化初始响应草稿、有效反馈和编辑响应的数量，我们的模型在Arena Hard基准测试中达到了92.7的最新性能，超越了其他模型。'}}}, {'id': 'https://huggingface.co/papers/2503.01375', 'title': 'Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems', 'url': 'https://huggingface.co/papers/2503.01375', 'abstract': 'Solving Bayesian inverse problems efficiently remains a significant challenge due to the complexity of posterior distributions and the computational cost of traditional sampling methods. Given a series of observations and the forward model, we want to recover the distribution of the parameters, conditioned on observed experimental data. We show, that combining Conditional Flow Mathching (CFM) with transformer-based architecture, we can efficiently sample from such kind of distribution, conditioned on variable number of observations.', 'score': 5, 'issue_id': 2583, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '9dd35b1faaa32b61', 'authors': ['Daniil Sherki', 'Ivan Oseledets', 'Ekaterina Muravleva'], 'affiliations': ['Artificial Intelligence Research Institute', 'Sberbank, AI4S Center', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01375.jpg', 'data': {'categories': ['#architecture', '#math'], 'emoji': '🔄', 'ru': {'title': 'Эффективная выборка из сложных апостериорных распределений с помощью CFM и трансформеров', 'desc': 'Статья представляет новый подход к решению байесовских обратных задач. Авторы предлагают комбинацию метода Conditional Flow Matching (CFM) с архитектурой на основе трансформеров. Этот подход позволяет эффективно осуществлять выборку из сложных апостериорных распределений, обусловленных переменным числом наблюдений. Метод преодолевает ограничения традиционных методов выборки, снижая вычислительные затраты.'}, 'en': {'title': 'Efficient Bayesian Inference with CFM and Transformers', 'desc': 'This paper addresses the challenge of efficiently solving Bayesian inverse problems, which involve estimating parameter distributions based on observed data. Traditional sampling methods can be computationally expensive, especially when dealing with complex posterior distributions. The authors propose a novel approach that combines Conditional Flow Matching (CFM) with transformer-based architectures to improve sampling efficiency. This method allows for effective sampling from parameter distributions conditioned on a variable number of observations, enhancing the ability to recover accurate parameter estimates.'}, 'zh': {'title': '高效贝叶斯逆问题求解的新方法', 'desc': '解决贝叶斯逆问题的效率仍然是一个重大挑战，因为后验分布的复杂性和传统采样方法的计算成本较高。我们希望在给定一系列观测和前向模型的情况下，恢复参数的分布，这些分布是基于观察到的实验数据。我们展示了将条件流匹配（CFM）与基于变换器的架构相结合，可以有效地从这种分布中进行采样，且该分布可以根据观测数量的变化而变化。此方法为贝叶斯推断提供了一种新的高效解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.04369', 'title': 'Lost in Literalism: How Supervised Training Shapes Translationese in LLMs', 'url': 'https://huggingface.co/papers/2503.04369', 'abstract': 'Large language models (LLMs) have achieved remarkable success in machine translation, demonstrating impressive performance across diverse languages. However, translationese, characterized by overly literal and unnatural translations, remains a persistent challenge in LLM-based translation systems. Despite their pre-training on vast corpora of natural utterances, LLMs exhibit translationese errors and generate unexpected unnatural translations, stemming from biases introduced during supervised fine-tuning (SFT). In this work, we systematically evaluate the prevalence of translationese in LLM-generated translations and investigate its roots during supervised training. We introduce methods to mitigate these biases, including polishing golden references and filtering unnatural training instances. Empirical evaluations demonstrate that these approaches significantly reduce translationese while improving translation naturalness, validated by human evaluations and automatic metrics. Our findings highlight the need for training-aware adjustments to optimize LLM translation outputs, paving the way for more fluent and target-language-consistent translations. We release the data and code at https://github.com/yafuly/LLM_Translationese.', 'score': 4, 'issue_id': 2586, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '7e6f548766f04dbf', 'authors': ['Yafu Li', 'Ronghao Zhang', 'Zhilin Wang', 'Huajian Zhang', 'Leyang Cui', 'Yongjing Yin', 'Tong Xiao', 'Yue Zhang'], 'affiliations': ['Northeastern University', 'Shanghai AI Laboratory', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04369.jpg', 'data': {'categories': ['#training', '#multilingual', '#machine_translation', '#data'], 'emoji': '🌐', 'ru': {'title': 'Борьба с переводным языком: путь к естественным переводам с помощью LLM', 'desc': 'Эта статья посвящена проблеме переводного языка (translationese) в системах машинного перевода на основе больших языковых моделей (LLM). Авторы исследуют причины возникновения неестественных переводов и предлагают методы для снижения этого эффекта, включая улучшение эталонных переводов и фильтрацию неестественных примеров в обучающих данных. Эмпирические оценки показывают, что эти подходы значительно уменьшают проявления переводного языка и улучшают естественность переводов. Исследование подчеркивает необходимость корректировки процесса обучения для оптимизации качества переводов, выполняемых LLM.'}, 'en': {'title': 'Reducing Translationese for Natural Language Translations', 'desc': 'This paper addresses the issue of translationese in large language models (LLMs) used for machine translation, which leads to unnatural and overly literal translations. The authors evaluate how prevalent translationese is in LLM outputs and identify biases introduced during supervised fine-tuning (SFT) as a key factor. They propose methods to reduce these biases, such as refining reference translations and filtering out unnatural training examples. Their empirical results show that these strategies significantly enhance the naturalness of translations, suggesting that careful adjustments during training can improve LLM performance in translation tasks.'}, 'zh': {'title': '优化大型语言模型翻译，减少翻译腔', 'desc': '大型语言模型（LLMs）在机器翻译中取得了显著成功，但仍面临翻译腔的问题，即翻译过于字面和不自然。尽管在大量自然语料上进行预训练，LLMs在监督微调（SFT）过程中引入的偏差导致了翻译腔错误。本文系统评估了LLM生成翻译中的翻译腔现象，并探讨了其在监督训练中的根源。我们提出了减轻这些偏差的方法，并通过实证评估证明这些方法显著降低了翻译腔，提高了翻译的自然性。'}}}, {'id': 'https://huggingface.co/papers/2503.02191', 'title': 'Understanding and Predicting Derailment in Toxic Conversations on GitHub', 'url': 'https://huggingface.co/papers/2503.02191', 'abstract': 'Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub.   To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants.   Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches.', 'score': 3, 'issue_id': 2581, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d0bc1069249c9e1', 'authors': ['Mia Mohammad Imran', 'Robert Zita', 'Rebekah Copeland', 'Preetha Chatterjee', 'Rahat Rizvi Rahman', 'Kostadin Damevski'], 'affiliations': ['Drexel University, Philadelphia, PA, USA', 'Eastern Mennonite University, Harrisonburg, VA, USA', 'Elmhurst University, Elmhurst, IL, USA', 'Missouri University of Science and Technology, Rolla, MO, USA', 'Virginia Commonwealth University, Richmond, VA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02191.jpg', 'data': {'categories': ['#ethics', '#dataset', '#multimodal', '#data'], 'emoji': '🛡️', 'ru': {'title': 'ИИ на страже здоровой атмосферы в open-source сообществах', 'desc': 'Исследование посвящено анализу и предотвращению токсичных взаимодействий в проектах на GitHub. Авторы создали датасет из 202 токсичных и 696 нетоксичных разговоров, выявив лингвистические маркеры и паттерны, характерные для деструктивных обсуждений. На основе этих данных была разработана система проактивной модерации с использованием больших языковых моделей (LLM). Эксперименты показали, что предложенный подход достигает 69% F1-меры в предсказании потенциально опасных разговоров на ранних стадиях.'}, 'en': {'title': 'Proactive Moderation: Detecting Toxicity Before It Escalates', 'desc': 'This paper investigates how toxic language in GitHub conversations can deter contributors and newcomers. It introduces a dataset of 202 toxic conversations with marked derailment points, alongside 696 non-toxic examples for comparison. The study identifies specific linguistic features and conversational dynamics that signal potential toxicity. Using large language models (LLMs), the authors propose a proactive moderation strategy that summarizes conversation trajectories to detect early signs of derailment, achieving a 69% F1-Score in predictions.'}, 'zh': {'title': '主动管理，防止对话偏离与有毒语言', 'desc': '本研究探讨了如何在GitHub上预测和理解对话偏离导致的有毒语言。我们创建了一个新数据集，包含202个有毒对话和696个非有毒对话，并标注了偏离点。通过分析这些对话的语言特征和动态模式，我们识别出有毒对话的独特特征。最后，我们提出了一种主动的管理策略，利用现代大语言模型自动检测和处理潜在的有害对话。'}}}, {'id': 'https://huggingface.co/papers/2503.03962', 'title': 'On the Acquisition of Shared Grammatical Representations in Bilingual Language Models', 'url': 'https://huggingface.co/papers/2503.03962', 'abstract': "While crosslingual transfer is crucial to contemporary language models' multilingual capabilities, how it occurs is not well understood. In this paper, we ask what happens to a monolingual language model when it begins to be trained on a second language. Specifically, we train small bilingual models for which we control the amount of data for each language and the order of language exposure. To find evidence of shared multilingual representations, we turn to structural priming, a method used to study grammatical representations in humans. We first replicate previous crosslingual structural priming results and find that after controlling for training data quantity and language exposure, there are asymmetrical effects across language pairs and directions. We argue that this asymmetry may shape hypotheses about human structural priming effects. We also find that structural priming effects are less robust for less similar language pairs, highlighting potential limitations of crosslingual transfer learning and shared representations for typologically diverse languages.", 'score': 2, 'issue_id': 2592, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '5d7eee50132fd29c', 'authors': ['Catherine Arnett', 'Tyler A. Chang', 'James A. Michaelov', 'Benjamin K. Bergen'], 'affiliations': ['Department of Brain and Cognitive Science, Massachusetts Institute of Technology', 'Department of Cognitive Science, University of California San Diego', 'Department of Linguistics, University of California San Diego', 'Halıcıoglu Data Science Institute, University of California San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2503.03962.jpg', 'data': {'categories': ['#low_resource', '#training', '#transfer_learning', '#data', '#multilingual'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая тайны кросс-лингвистического переноса в нейронных сетях', 'desc': 'Статья исследует процесс кросс-лингвистического переноса в двуязычных языковых моделях. Авторы изучают, как модель, обученная на одном языке, начинает воспринимать второй язык. Для анализа общих многоязычных представлений используется метод структурного прайминга, заимствованный из психолингвистики. Результаты показывают асимметричные эффекты между парами языков и направлениями переноса, а также меньшую устойчивость структурного прайминга для типологически различных языков.'}, 'en': {'title': 'Understanding Crosslingual Transfer in Language Models', 'desc': 'This paper investigates how training a monolingual language model on a second language affects its performance and understanding. The authors create small bilingual models and manipulate the amount of training data and the sequence of language exposure. They utilize structural priming to analyze the grammatical representations that emerge from this bilingual training. The findings reveal asymmetrical effects in language pairs and suggest that less similar languages may limit the effectiveness of crosslingual transfer learning.'}, 'zh': {'title': '探索跨语言迁移的非对称性', 'desc': '本文探讨了跨语言迁移在现代语言模型中的重要性，尤其是当单语模型开始接受第二语言训练时会发生什么。我们训练了小型双语模型，控制每种语言的数据量和语言暴露的顺序。通过结构性启动的方法，我们发现不同语言对之间的影响不对称，这可能影响人类的结构性启动假设。研究还表明，对于语言类型差异较大的语言对，结构性启动效果较弱，突显了跨语言迁移学习的潜在局限性。'}}}, {'id': 'https://huggingface.co/papers/2503.03601', 'title': 'Feature-Level Insights into Artificial Text Detection with Sparse\n  Autoencoders', 'url': 'https://huggingface.co/papers/2503.03601', 'abstract': 'Artificial Text Detection (ATD) is becoming increasingly important with the rise of advanced Large Language Models (LLMs). Despite numerous efforts, no single algorithm performs consistently well across different types of unseen text or guarantees effective generalization to new LLMs. Interpretability plays a crucial role in achieving this goal. In this study, we enhance ATD interpretability by using Sparse Autoencoders (SAE) to extract features from Gemma-2-2b residual stream. We identify both interpretable and efficient features, analyzing their semantics and relevance through domain- and model-specific statistics, a steering approach, and manual or LLM-based interpretation. Our methods offer valuable insights into how texts from various models differ from human-written content. We show that modern LLMs have a distinct writing style, especially in information-dense domains, even though they can produce human-like outputs with personalized prompts.', 'score': 133, 'issue_id': 2634, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': 'd045a8e2a39262c9', 'authors': ['Kristian Kuznetsov', 'Laida Kushnareva', 'Polina Druzhinina', 'Anton Razzhigaev', 'Anastasia Voznyuk', 'Irina Piontkovskaya', 'Evgeny Burnaev', 'Serguei Barannikov'], 'affiliations': ['AI Foundation and Algorithm Lab', 'Artificial Intelligence Research Institute (AIRI)', 'CNRS, Université Paris Cité, France', 'Moscow Institute of Physics and Technology', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.03601.jpg', 'data': {'categories': ['#multimodal', '#cv', '#interpretability', '#data'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая секреты искусственных текстов: новый подход к интерпретируемому обнаружению', 'desc': 'Исследование посвящено улучшению интерпретируемости методов обнаружения искусственного текста (ATD) с использованием разреженных автоэнкодеров для извлечения признаков из остаточного потока модели Gemma-2-2b. Авторы анализируют семантику и релевантность выделенных признаков с помощью статистических методов и интерпретации. Результаты показывают, что современные языковые модели имеют отличительный стиль письма, особенно в информационно-насыщенных областях. Исследование предлагает ценные insights о том, как тексты, созданные различными моделями, отличаются от контента, написанного людьми.'}, 'en': {'title': 'Enhancing Text Detection with Interpretability in AI Models', 'desc': 'This paper focuses on improving Artificial Text Detection (ATD) by enhancing its interpretability using Sparse Autoencoders (SAE). The authors extract features from the residual stream of the Gemma-2-2b model to identify both interpretable and efficient characteristics of text. They analyze these features through various statistical methods and interpretations to understand how machine-generated texts differ from human-written ones. The study reveals that modern Large Language Models (LLMs) exhibit a unique writing style, particularly in information-dense areas, despite their ability to generate human-like text.'}, 'zh': {'title': '提升人工文本检测的可解释性', 'desc': '随着大型语言模型（LLMs）的发展，人工文本检测（ATD）变得越来越重要。尽管已有许多努力，但没有单一算法能够在不同类型的未见文本中始终表现良好，也无法保证对新LLM的有效泛化。可解释性在实现这一目标中起着关键作用。我们通过使用稀疏自编码器（SAE）从Gemma-2-2b残差流中提取特征，增强了ATD的可解释性，分析了文本与人类写作内容的差异。'}}}, {'id': 'https://huggingface.co/papers/2503.07605', 'title': 'SEAP: Training-free Sparse Expert Activation Pruning Unlock the\n  Brainpower of Large Language Models', 'url': 'https://huggingface.co/papers/2503.07605', 'abstract': "Large Language Models have achieved remarkable success across various natural language processing tasks, yet their high computational cost during inference remains a major bottleneck. This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model. These findings highlight SEAP's scalability and effectiveness, making it a promising approach for optimizing large-scale LLMs.", 'score': 59, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'eaf9c07e3673cecc', 'authors': ['Xun Liang', 'Hanyu Wang', 'Huayi Lai', 'Simin Niu', 'Shichao Song', 'Jiawei Yang', 'Jihao Zhao', 'Feiyu Xiong', 'Bo Tang', 'Zhiyu Li'], 'affiliations': ['Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07605.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': '✂️', 'ru': {'title': 'Эффективное обрезание нейросетей без потери качества', 'desc': 'Статья представляет метод Sparse Expert Activation Pruning (SEAP) для оптимизации больших языковых моделей. SEAP выборочно сохраняет параметры, релевантные для конкретной задачи, что позволяет снизить вычислительные затраты при инференсе. Метод основан на выявлении паттернов активации экспертов, специфичных для задачи. Эксперименты показывают, что SEAP значительно сокращает вычислительные затраты при сохранении высокой точности модели.'}, 'en': {'title': 'Optimize LLMs with Sparse Expert Activation Pruning!', 'desc': "This paper presents Sparse Expert Activation Pruning (SEAP), a novel method designed to reduce the computational cost of large language models (LLMs) during inference without requiring additional training. SEAP works by identifying and retaining only the parameters that are relevant to specific tasks, effectively pruning the model while maintaining its performance. The method leverages the clustering patterns of hidden states and activations to optimize the model's efficiency. Experimental results show that SEAP can significantly lower computational overhead while achieving competitive accuracy, making it a scalable solution for optimizing LLMs."}, 'zh': {'title': '稀疏专家激活剪枝：优化大型语言模型的新方法', 'desc': '大型语言模型在自然语言处理任务中取得了显著成功，但推理时的高计算成本仍然是一个主要瓶颈。本文提出了一种名为稀疏专家激活剪枝（SEAP）的方法，该方法在不需要训练的情况下，选择性地保留与任务相关的参数，以减少推理开销。SEAP通过分析隐藏状态和激活的聚类模式，识别特定任务的专家激活模式，从而在保持任务性能的同时优化计算效率。实验结果表明，SEAP在减少计算开销的同时，保持了竞争力的准确性，展示了其在优化大规模语言模型方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.07365', 'title': 'MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.07365', 'abstract': "We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA", 'score': 45, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '765d475b38d9f289', 'authors': ['Fanqing Meng', 'Lingxiao Du', 'Zongkai Liu', 'Zhixiang Zhou', 'Quanfeng Lu', 'Daocheng Fu', 'Botian Shi', 'Wenhai Wang', 'Junjun He', 'Kaipeng Zhang', 'Ping Luo', 'Yu Qiao', 'Qiaosheng Zhang', 'Wenqi Shao'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07365.jpg', 'data': {'categories': ['#rl', '#multimodal', '#reasoning', '#rag', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Мультимодальное рассуждение через обучение с подкреплением', 'desc': 'MM-Eureka – это мультимодальная модель рассуждений, которая успешно расширяет масштабное обучение с подкреплением на основе правил для мультимодальных задач. Модель демонстрирует ключевые характеристики текстовых систем обучения с подкреплением, включая устойчивое повышение точности и длины ответов, а также появление рефлексивного поведения. Исследование показывает, что как модели, настроенные на инструкции, так и предварительно обученные модели могут развивать сильные мультимодальные способности рассуждения без контролируемой тонкой настройки. Авторы открыли исходный код всего конвейера для дальнейших исследований в этой области.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with MM-Eureka!', 'desc': 'MM-Eureka is a new model that enhances reasoning across different types of data, like text and images, using a method called rule-based reinforcement learning (RL). This approach builds on successful techniques from text-based RL, allowing the model to improve its accuracy and response quality in multimodal contexts. The model shows that it can learn effectively without needing extra labeled data, making it more efficient than other methods. By sharing our tools and findings, we aim to encourage more research in multimodal reasoning.'}, 'zh': {'title': 'MM-Eureka：多模态推理的新突破', 'desc': '我们提出了MM-Eureka，这是一个多模态推理模型，成功地将大规模基于规则的强化学习扩展到多模态推理领域。虽然基于规则的强化学习在文本领域提升大型语言模型的推理能力方面取得了显著成功，但在多模态环境中的应用仍然具有挑战性。我们的工作在多模态空间中重现了文本基础强化学习系统的关键特征，包括准确性奖励和响应长度的稳定增加，以及反思行为的出现。我们展示了无监督微调的情况下，指令调优和预训练模型都能通过基于规则的强化学习发展出强大的多模态推理能力，且数据效率优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2503.07002', 'title': 'Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue\n  Learning', 'url': 'https://huggingface.co/papers/2503.07002', 'abstract': 'Multimodal large language models (MLLMs), built on large-scale pre-trained vision towers and language models, have shown great capabilities in multimodal understanding. However, most existing MLLMs are trained on single-turn vision question-answering tasks, which do not accurately reflect real-world human conversations. In this paper, we introduce MMDiag, a multi-turn multimodal dialogue dataset. This dataset is collaboratively generated through deliberately designed rules and GPT assistance, featuring strong correlations between questions, between questions and images, and among different image regions; thus aligning more closely with real-world scenarios. MMDiag serves as a strong benchmark for multi-turn multimodal dialogue learning and brings more challenges to the grounding and reasoning capabilities of MLLMs. Further, inspired by human vision processing, we present DiagNote, an MLLM equipped with multimodal grounding and reasoning capabilities. DiagNote consists of two modules (Deliberate and Gaze) interacting with each other to perform Chain-of-Thought and annotations respectively, throughout multi-turn dialogues. We empirically demonstrate the advantages of DiagNote in both grounding and jointly processing and reasoning with vision and language information over existing MLLMs.', 'score': 33, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '1f57fb5c5398efbc', 'authors': ['Jiazheng Liu', 'Sipeng Zheng', 'Börje F. Karlsson', 'Zongqing Lu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07002.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#games', '#multimodal', '#dataset', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'Новый подход к мультимодальным диалогам: от реалистичных данных к продвинутым моделям', 'desc': 'Статья представляет MMDiag - новый набор данных для многоходовых мультимодальных диалогов, созданный с помощью GPT. Этот датасет лучше отражает реальные сценарии общения, чем существующие однотурные наборы данных для вопросно-ответных задач. Авторы также предлагают DiagNote - мультимодальную языковую модель с улучшенными возможностями заземления и рассуждений. DiagNote использует два взаимодействующих модуля для цепочки рассуждений и аннотаций в ходе диалога.'}, 'en': {'title': 'Enhancing Multimodal Dialogue with MMDiag and DiagNote', 'desc': 'This paper presents MMDiag, a new dataset designed for multi-turn multimodal dialogue, which enhances the training of multimodal large language models (MLLMs). Unlike previous datasets that focus on single-turn tasks, MMDiag captures the complexities of real-world conversations by incorporating strong correlations between questions and images. The authors introduce DiagNote, an MLLM that features two interactive modules for improved grounding and reasoning in dialogues. The results show that DiagNote outperforms existing MLLMs in processing and reasoning with both visual and textual information.'}, 'zh': {'title': '提升多模态对话的智能化', 'desc': '这篇论文介绍了一种新的多模态对话数据集MMDiag，旨在改善现有多模态大语言模型（MLLMs）在多轮对话中的表现。MMDiag通过精心设计的规则和GPT的辅助生成，包含了问题之间、问题与图像之间以及不同图像区域之间的强相关性，更贴近真实的人类对话场景。论文还提出了DiagNote，一个具备多模态基础和推理能力的MLLM，包含两个相互作用的模块（Deliberate和Gaze），用于在多轮对话中进行思维链和注释。通过实验证明，DiagNote在基础和共同处理视觉与语言信息的推理能力上优于现有的MLLMs。'}}}, {'id': 'https://huggingface.co/papers/2503.07314', 'title': 'Automated Movie Generation via Multi-Agent CoT Planning', 'url': 'https://huggingface.co/papers/2503.07314', 'abstract': 'Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.', 'score': 28, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '2e5c000925250863', 'authors': ['Weijia Wu', 'Zeyu Zhu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07314.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#story_generation', '#video', '#agents'], 'emoji': '🎬', 'ru': {'title': 'Автоматизированное создание фильмов с помощью ИИ-агентов', 'desc': 'MovieAgent - это новая система автоматизированной генерации длинных видео с использованием мультиагентного планирования на основе цепочки размышлений (Chain of Thought). Система способна создавать многосценные фильмы с согласованным сюжетом, сохраняя постоянство персонажей и синхронизацию субтитров. MovieAgent использует иерархический процесс рассуждений для автоматического структурирования сцен и настроек камеры, значительно сокращая человеческие усилия. Эксперименты показывают, что MovieAgent достигает новых лучших результатов в верности сценарию, постоянстве персонажей и согласованности повествования.'}, 'en': {'title': 'Automating Movie Magic with MovieAgent!', 'desc': 'This paper introduces MovieAgent, a novel framework for automated long-form video generation that eliminates the need for manual planning of storylines and scenes. It utilizes a multi-agent Chain of Thought (CoT) reasoning process to create coherent narratives while maintaining character consistency and synchronized audio. By simulating various production roles, MovieAgent streamlines the filmmaking process, significantly reducing the effort required from human creators. Experimental results show that MovieAgent sets new benchmarks in script adherence, character consistency, and overall narrative quality.'}, 'zh': {'title': '自动化电影生成的新纪元', 'desc': '现有的长视频生成框架缺乏自动化规划，需要手动输入故事情节、场景、摄影和角色互动，导致高成本和低效率。为了解决这些问题，我们提出了MovieAgent，通过多智能体的思维链（CoT）规划实现自动化电影生成。MovieAgent的两个主要优势是：首先，我们探索并定义了自动化电影/长视频生成的范式；其次，MovieAgent引入了基于层次的CoT推理过程，自动构建场景、摄像机设置和摄影，大大减少了人力投入。实验表明，MovieAgent在剧本忠实度、角色一致性和叙事连贯性方面达到了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2503.07216', 'title': 'FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates', 'url': 'https://huggingface.co/papers/2503.07216', 'abstract': "Federated Learning (FL) is a widely used framework for training models in a decentralized manner, ensuring that the central server does not have direct access to data from local clients. However, this approach may still fail to fully preserve data privacy, as models from local clients are exposed to the central server during the aggregation process. This issue becomes even more critical when training vision-language models (VLMs) with FL, as VLMs can easily memorize training data instances, making them vulnerable to membership inference attacks (MIAs). To address this challenge, we propose the FedRand framework, which avoids disclosing the full set of client parameters. In this framework, each client randomly selects subparameters of Low-Rank Adaptation (LoRA) from the server and keeps the remaining counterparts of the LoRA weights as private parameters. After training both parameters on the client's private dataset, only the non-private <PRE_TAG>client parameters</POST_TAG> are sent back to the server for aggregation. This approach mitigates the risk of exposing client-side VLM parameters, thereby enhancing data privacy. We empirically validate that FedRand improves robustness against MIAs compared to relevant baselines while achieving accuracy comparable to methods that communicate full LoRA parameters across several benchmark datasets.", 'score': 25, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '3d23c61b24a599ee', 'authors': ['Sangwoo Park', 'Seanie Lee', 'Byungjoo Kim', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.07216.jpg', 'data': {'categories': ['#security', '#benchmark', '#data', '#ethics', '#multimodal', '#rl'], 'emoji': '🔐', 'ru': {'title': 'FedRand: Защита конфиденциальности в федеративном обучении визуально-языковых моделей', 'desc': 'Статья представляет новый подход к федеративному обучению (FL) для визуально-языковых моделей (VLM), называемый FedRand. Эта методика направлена на повышение конфиденциальности данных путем случайного выбора подпараметров Low-Rank Adaptation (LoRA) для обмена между клиентами и сервером. FedRand позволяет сохранять часть параметров LoRA приватными, снижая риск утечки информации о данных клиентов. Эмпирические исследования показывают, что FedRand повышает устойчивость к атакам по выводу членства (MIA) по сравнению с базовыми методами, сохраняя при этом сопоставимую точность.'}, 'en': {'title': 'Enhancing Privacy in Federated Learning with FedRand', 'desc': 'Federated Learning (FL) allows models to be trained on local data without sharing it with a central server, but it can still risk data privacy during model aggregation. This paper introduces the FedRand framework, which enhances privacy by having clients send only a subset of their model parameters back to the server. Specifically, it utilizes Low-Rank Adaptation (LoRA) to keep certain parameters private while still allowing effective model training. The results show that FedRand not only protects against membership inference attacks but also maintains accuracy similar to traditional methods that share full parameters.'}, 'zh': {'title': 'FedRand：保护数据隐私的联邦学习新框架', 'desc': '联邦学习（FL）是一种去中心化的模型训练框架，确保中央服务器无法直接访问本地客户端的数据。然而，在聚合过程中，本地客户端的模型仍可能暴露给中央服务器，从而影响数据隐私。特别是在训练视觉-语言模型（VLMs）时，这种风险更为严重，因为VLMs容易记住训练数据实例，容易受到成员推断攻击（MIAs）。为了解决这个问题，我们提出了FedRand框架，该框架通过随机选择低秩适应（LoRA）的子参数，避免泄露完整的客户端参数，从而增强数据隐私。'}}}, {'id': 'https://huggingface.co/papers/2503.07067', 'title': 'DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs', 'url': 'https://huggingface.co/papers/2503.07067', 'abstract': 'Despite the success of distillation in large language models (LLMs), most prior work applies identical loss functions to both teacher- and student-generated data. These strategies overlook the synergy between loss formulations and data types, leading to a suboptimal performance boost in student models. To address this, we propose DistiLLM-2, a contrastive approach that simultaneously increases the likelihood of teacher responses and decreases that of student responses by harnessing this synergy. Our extensive experiments show that DistiLLM-2 not only builds high-performing student models across a wide range of tasks, including instruction-following and code generation, but also supports diverse applications, such as preference alignment and vision-language extensions. These findings highlight the potential of a contrastive approach to enhance the efficacy of LLM distillation by effectively aligning teacher and student models across varied data types.', 'score': 22, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '8eb39990e619611e', 'authors': ['Jongwoo Ko', 'Tianyi Chen', 'Sungnyun Kim', 'Tianyu Ding', 'Luming Liang', 'Ilya Zharkov', 'Se-Young Yun'], 'affiliations': ['KAIST AI', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.07067.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#training', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Контрастивная дистилляция: новый подход к обучению языковых моделей', 'desc': 'DistiLLM-2 - это новый подход к дистилляции больших языковых моделей (LLM), использующий контрастивное обучение. Он одновременно увеличивает вероятность ответов учителя и уменьшает вероятность ответов ученика, эффективно используя синергию между формулировками функции потерь и типами данных. Эксперименты показывают, что DistiLLM-2 создает высокопроизводительные модели-ученики для широкого спектра задач, включая следование инструкциям и генерацию кода. Метод также поддерживает различные приложения, такие как выравнивание предпочтений и расширения для работы с визуальными данными.'}, 'en': {'title': 'Enhancing LLM Distillation with Contrastive Learning', 'desc': 'This paper introduces DistiLLM-2, a novel approach to improve the distillation process in large language models (LLMs). Unlike previous methods that use the same loss functions for both teacher and student models, DistiLLM-2 employs a contrastive strategy that boosts the likelihood of correct teacher responses while reducing the likelihood of incorrect student responses. The results demonstrate that this method significantly enhances the performance of student models on various tasks, including instruction-following and code generation. Additionally, DistiLLM-2 shows versatility in applications such as preference alignment and vision-language tasks, emphasizing the importance of tailored loss functions in model distillation.'}, 'zh': {'title': '对比方法提升大型语言模型蒸馏效果', 'desc': '尽管蒸馏在大型语言模型（LLMs）中取得了成功，但大多数先前的研究对教师和学生生成的数据使用相同的损失函数。这种策略忽视了损失公式与数据类型之间的协同作用，导致学生模型的性能提升不理想。为了解决这个问题，我们提出了DistiLLM-2，这是一种对比方法，能够同时提高教师响应的可能性并降低学生响应的可能性。我们的广泛实验表明，DistiLLM-2不仅在多种任务中构建了高性能的学生模型，还支持多样化的应用，如偏好对齐和视觉-语言扩展。'}}}, {'id': 'https://huggingface.co/papers/2503.07027', 'title': 'EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer', 'url': 'https://huggingface.co/papers/2503.07027', 'abstract': 'Recent advancements in Unet-based diffusion models, such as ControlNet and IP-Adapter, have introduced effective spatial and subject control mechanisms. However, the DiT (Diffusion Transformer) architecture still struggles with efficient and flexible control. To tackle this issue, we propose EasyControl, a novel framework designed to unify condition-guided diffusion transformers with high efficiency and flexibility. Our framework is built on three key innovations. First, we introduce a lightweight <PRE_TAG>Condition Injection LoRA Module</POST_TAG>. This module processes conditional signals in isolation, acting as a plug-and-play solution. It avoids modifying the base model weights, ensuring compatibility with customized models and enabling the flexible injection of diverse conditions. Notably, this module also supports harmonious and robust zero-shot multi-condition generalization, even when trained only on single-condition data. Second, we propose a <PRE_TAG>Position-Aware Training Paradigm</POST_TAG>. This approach standardizes input conditions to fixed resolutions, allowing the generation of images with arbitrary aspect ratios and flexible resolutions. At the same time, it optimizes computational efficiency, making the framework more practical for real-world applications. Third, we develop a Causal Attention Mechanism combined with the KV Cache technique, adapted for conditional generation tasks. This innovation significantly reduces the latency of image synthesis, improving the overall efficiency of the framework. Through extensive experiments, we demonstrate that EasyControl achieves exceptional performance across various application scenarios. These innovations collectively make our framework highly efficient, flexible, and suitable for a wide range of tasks.', 'score': 19, 'issue_id': 2630, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '1b1589f9873720c7', 'authors': ['Yuxuan Zhang', 'Yirui Yuan', 'Yiren Song', 'Haofan Wang', 'Jiaming Liu'], 'affiliations': ['Liblib AI', 'National University of Singapore', 'ShanghaiTech University', 'Tiamat AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07027.jpg', 'data': {'categories': ['#cv', '#architecture', '#training', '#optimization', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'EasyControl: гибкое и эффективное управление диффузионными трансформерами', 'desc': 'Статья представляет EasyControl - новую систему для улучшения контроля в диффузионных трансформерах. Основные инновации включают модуль внедрения условий на основе LoRA, парадигму обучения с учетом позиции и механизм причинного внимания с KV-кэшем. EasyControl обеспечивает эффективное и гибкое управление генерацией изображений с произвольными пропорциями и разрешением. Эксперименты показывают высокую производительность системы в различных сценариях применения.'}, 'en': {'title': 'EasyControl: Unifying Efficiency and Flexibility in Diffusion Transformers', 'desc': 'This paper presents EasyControl, a new framework that enhances the efficiency and flexibility of condition-guided diffusion transformers, particularly addressing the limitations of the DiT architecture. It introduces a lightweight Condition Injection LoRA Module that allows for the independent processing of conditional signals without altering the base model, enabling easy integration of various conditions. Additionally, the Position-Aware Training Paradigm standardizes input conditions, facilitating the generation of images with different aspect ratios while optimizing computational resources. Lastly, the Causal Attention Mechanism with KV Cache significantly reduces image synthesis latency, making EasyControl a robust solution for diverse applications in machine learning.'}, 'zh': {'title': '高效灵活的条件生成框架EasyControl', 'desc': '本文提出了一种名为EasyControl的新框架，旨在提高基于扩散变换器的条件生成模型的效率和灵活性。该框架包含三个关键创新：首先，轻量级的条件注入LoRA模块可以独立处理条件信号，避免修改基础模型权重，从而实现与定制模型的兼容性。其次，位置感知训练范式标准化输入条件，使得生成任意长宽比和灵活分辨率的图像成为可能，同时优化计算效率。最后，结合KV缓存技术的因果注意机制显著降低了图像合成的延迟，提升了整体效率。'}}}, {'id': 'https://huggingface.co/papers/2503.06680', 'title': 'FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation\n  for Feature Implementation', 'url': 'https://huggingface.co/papers/2503.06680', 'abstract': "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.", 'score': 17, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '4394ce17a18696a3', 'authors': ['Wei Li', 'Xin Zhang', 'Zhongxin Guo', 'Shaoguang Mao', 'Wen Luo', 'Guangyue Peng', 'Yangyu Huang', 'Houfeng Wang', 'Scarlett Li'], 'affiliations': ['Microsoft Research Asia', 'State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06680.jpg', 'data': {'categories': ['#plp', '#dataset', '#optimization', '#benchmark'], 'emoji': '🧪', 'ru': {'title': 'FEA-Bench: новый вызов для языковых моделей в разработке ПО', 'desc': 'FEA-Bench - это новый бенчмарк для оценки способности больших языковых моделей (LLM) выполнять инкрементальную разработку в репозиториях кода. Он основан на пул-реквестах из 83 репозиториев GitHub и включает задачи по разработке новых функций. Каждая задача содержит изменения кода и соответствующие модульные тесты для верификации. Результаты экспериментов показывают, что LLM значительно хуже справляются с задачами FEA-Bench, что указывает на серьезные проблемы в автоматизированной разработке на уровне репозиториев.'}, 'en': {'title': 'FEA-Bench: Evaluating Code Generation in Repositories', 'desc': "This paper introduces FEA-Bench, a new benchmark for evaluating large language models (LLMs) in the context of code generation for new features in software repositories. It addresses the lack of dedicated evaluation frameworks by collecting pull requests from 83 GitHub repositories and creating task instances that focus on incremental development. Each task is designed to test the LLM's ability to generate code changes while ensuring that these changes can be verified through associated unit tests. The findings reveal that LLMs struggle with this type of repository-level code development, indicating significant challenges in their automated software engineering capabilities."}, 'zh': {'title': '评估代码生成模型的新基准：FEA-Bench', 'desc': '在代码生成模型中，实现新特性是一个重要的应用。当前的基准测试缺乏专门评估这一能力的框架。为此，我们提出了FEA-Bench，这是一个旨在评估大型语言模型（LLMs）在代码库中进行增量开发能力的基准。实验结果表明，LLMs在FEA-Bench中的表现显著较差，突显了在代码库级别增量开发中面临的重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.07608', 'title': 'AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via\n  Reinforcement Learning and Reasoning', 'url': 'https://huggingface.co/papers/2503.07608', 'abstract': 'OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning <PRE_TAG>reasoning training strategy</POST_TAG> that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research.', 'score': 14, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '74dfc85ba9f7bcc7', 'authors': ['Bo Jiang', 'Shaoyu Chen', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.07608.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#reasoning', '#agents'], 'emoji': '🚗', 'ru': {'title': 'AlphaDrive: ИИ за рулем с обучением и рассуждением', 'desc': 'AlphaDrive - это новая система для автономного вождения, использующая визуально-языковые модели с обучением с подкреплением и рассуждениями. Она вводит четыре награды на основе GRPO, специально разработанные для планирования, и применяет двухэтапную стратегию обучения с рассуждениями. AlphaDrive значительно улучшает производительность планирования и эффективность обучения по сравнению с традиционными методами. После обучения с подкреплением система демонстрирует некоторые возникающие мультимодальные возможности планирования.'}, 'en': {'title': 'AlphaDrive: Revolutionizing Autonomous Driving with RL and Reasoning', 'desc': 'This paper presents AlphaDrive, a novel framework that enhances autonomous driving by integrating reinforcement learning (RL) and reasoning with vision-language models (VLMs). AlphaDrive employs four GRPO-based RL rewards specifically designed for planning tasks and utilizes a two-stage training strategy that combines supervised fine-tuning (SFT) with RL. The results show that AlphaDrive significantly outperforms traditional methods that rely solely on SFT, leading to improved planning performance and training efficiency. Additionally, the framework demonstrates emergent multimodal planning capabilities post-RL training, which are essential for enhancing driving safety and efficiency.'}, 'zh': {'title': 'AlphaDrive：提升自动驾驶的智能规划与推理', 'desc': '本文提出了AlphaDrive，这是一个用于自动驾驶的强化学习（RL）和推理框架。AlphaDrive引入了四种基于GRPO的RL奖励，专门针对规划任务，并采用了结合监督微调（SFT）和RL的两阶段规划推理训练策略。与仅使用SFT或不进行推理的情况相比，AlphaDrive显著提高了规划性能和训练效率。此外，经过RL训练后，AlphaDrive还展现出一些新兴的多模态规划能力，这对提高驾驶安全性和效率至关重要。'}}}, {'id': 'https://huggingface.co/papers/2503.06749', 'title': 'Vision-R1: Incentivizing Reasoning Capability in Multimodal Large\n  Language Models', 'url': 'https://huggingface.co/papers/2503.06749', 'abstract': "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of sim6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .", 'score': 14, 'issue_id': 2632, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '403914241aa8967c', 'authors': ['Wenxuan Huang', 'Bohan Jia', 'Zijie Zhai', 'Shaosheng Cao', 'Zheyu Ye', 'Fei Zhao', 'Yao Hu', 'Shaohui Lin'], 'affiliations': ['East China Normal University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.06749.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#multimodal', '#open_source', '#benchmark', '#reasoning', '#dataset', '#rag'], 'emoji': '🧠', 'ru': {'title': 'Vision-R1: Новый уровень мультимодальных рассуждений в ИИ', 'desc': 'Исследователи разработали Vision-R1, мультимодальную языковую модель с улучшенными способностями рассуждения. Они создали высококачественный набор данных для обучения, используя существующую MLLM и DeepSeek-R1. Для оптимизации модели применили стратегию Progressive Thinking Suppression Training и Group Relative Policy Optimization. Vision-R1-7B достигла точности 73.5% на бенчмарке MathVista, что лишь на 0.4% ниже ведущей модели OpenAI O1.'}, 'en': {'title': 'Enhancing Reasoning in MLLMs with Reinforcement Learning', 'desc': "The paper introduces DeepSeek-R1-Zero, which shows that large language models (LLMs) can develop reasoning skills through Reinforcement Learning (RL). It highlights the challenge of training LLMs directly with RL due to a lack of high-quality multimodal reasoning data. To overcome this, the authors propose a new model called Vision-R1, which utilizes a self-generated multimodal dataset for cold-start training. They also introduce a training strategy called Progressive Thinking Suppression Training (PTST) to enhance the model's reasoning capabilities, achieving significant improvements in multimodal math reasoning tasks."}, 'zh': {'title': '通过强化学习提升多模态推理能力', 'desc': 'DeepSeek-R1-Zero展示了通过强化学习（RL）使大型语言模型（LLM）具备推理能力的可能性。基于这一突破，本文探讨了如何利用强化学习提升多模态大型语言模型（MLLM）的推理能力。由于缺乏高质量的多模态推理数据，直接使用强化学习训练面临挑战，因此我们提出了Vision-R1模型，以改善多模态推理能力。我们构建了一个高质量的多模态链式思维（CoT）数据集，并通过渐进思维抑制训练（PTST）和群体相对策略优化（GRPO）策略来优化模型的推理过程。'}}}, {'id': 'https://huggingface.co/papers/2503.04629', 'title': 'SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing', 'url': 'https://huggingface.co/papers/2503.04629', 'abstract': 'Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey.', 'score': 14, 'issue_id': 2633, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'a229855316ab195d', 'authors': ['Xiangchao Yan', 'Shiyang Feng', 'Jiakang Yuan', 'Renqiu Xia', 'Bin Wang', 'Bo Zhang', 'Lei Bai'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04629.jpg', 'data': {'categories': ['#survey', '#multimodal', '#agents', '#dataset', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'SurveyForge: Автоматизация создания высококачественных обзорных статей с помощью ИИ', 'desc': 'SurveyForge - это новый подход к автоматическому созданию обзорных статей с использованием больших языковых моделей (LLM). Система сначала генерирует структуру статьи, анализируя логическую структуру обзоров, написанных людьми, и обращаясь к релевантным статьям в предметной области. Затем, используя качественные статьи, извлеченные из памяти с помощью агента навигации, SurveyForge автоматически генерирует и улучшает содержание статьи. Для оценки качества создан набор данных SurveyBench, включающий 100 обзорных статей, написанных людьми, и оценивающий сгенерированные ИИ обзоры по трем параметрам: качество ссылок, структуры и содержания.'}, 'en': {'title': 'Enhancing Automated Survey Generation with SurveyForge', 'desc': 'This paper introduces SurveyForge, a tool designed to improve the quality of automated survey generation using large language models (LLMs). It addresses the significant quality gap between LLM-generated surveys and those created by humans, particularly in outline structure and citation accuracy. SurveyForge first generates an outline by analyzing human-written surveys and retrieving relevant articles, then it refines the content using high-quality papers. The authors also present SurveyBench, a benchmark for evaluating survey papers, which shows that SurveyForge outperforms previous methods like AutoSurvey.'}, 'zh': {'title': 'SurveyForge：提升文献综述生成质量的利器', 'desc': '这篇论文介绍了SurveyForge，一个用于自动生成文献综述的工具。它通过分析人类撰写的综述大纲的逻辑结构，并参考相关领域的文章，首先生成大纲。然后，SurveyForge利用高质量的论文来自动生成和完善文章内容。研究还构建了SurveyBench，用于评估AI生成的综述论文在参考文献、结构和内容质量等三个维度的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.05244', 'title': 'WritingBench: A Comprehensive Benchmark for Generative Writing', 'url': 'https://huggingface.co/papers/2503.05244', 'abstract': "Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The framework's validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing.", 'score': 13, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '679eb0b19323e2d2', 'authors': ['Yuning Wu', 'Jiahao Mei', 'Ming Yan', 'Chenliang Li', 'SHaopeng Lai', 'Yuran Ren', 'Zijia Wang', 'Ji Zhang', 'Mengyue Wu', 'Qin Jin', 'Fei Huang'], 'affiliations': ['Alibaba Group', 'Renmin University of China', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05244.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#story_generation', '#open_source'], 'emoji': '✍️', 'ru': {'title': 'WritingBench: новый стандарт оценки языковых моделей в письме', 'desc': 'Статья представляет WritingBench - комплексный бенчмарк для оценки языковых моделей в различных областях письма. Авторы предлагают фреймворк для динамической генерации критериев оценки и обученную модель-критик для подсчета баллов. Бенчмарк охватывает 6 основных областей и 100 подобластей письма, включая креативное, убеждающее, информативное и техническое письмо. Исследование показывает, что предложенный подход позволяет небольшим моделям приблизиться к производительности современных больших языковых моделей.'}, 'en': {'title': 'WritingBench: A New Standard for Evaluating Language Models in Writing', 'desc': 'This paper introduces WritingBench, a new benchmark for evaluating large language models (LLMs) in generative writing across six key domains and 100 subdomains. It addresses the limitations of existing benchmarks that do not adequately assess the quality of writing in various contexts. The authors propose a query-dependent evaluation framework that allows LLMs to create specific assessment criteria based on the writing task at hand. Additionally, a fine-tuned critic model is introduced to provide criteria-aware scoring, enhancing the evaluation of style, format, and length in generated texts.'}, 'zh': {'title': 'WritingBench：全面评估大型语言模型的写作能力', 'desc': '近年来，大型语言模型（LLMs）的进步显著提升了文本生成能力，但评估其在生成写作中的表现仍然是一个挑战。现有的基准主要集中在通用文本生成或有限的写作任务上，无法捕捉到高质量书面内容在不同领域的多样化需求。为了解决这个问题，我们提出了WritingBench，这是一个全面的基准，旨在评估LLMs在6个核心写作领域和100个子领域的表现，包括创意、说服性、信息性和技术写作。我们还提出了一种依赖查询的评估框架，使LLMs能够动态生成特定实例的评估标准，并通过一个经过微调的评估模型进行风格、格式和长度的评分。'}}}, {'id': 'https://huggingface.co/papers/2503.07602', 'title': 'DreamRelation: Relation-Centric Video Customization', 'url': 'https://huggingface.co/papers/2503.07602', 'abstract': "Relational video customization refers to the creation of personalized videos that depict user-specified relations between two subjects, a crucial task for comprehending real-world visual content. While existing methods can personalize subject appearances and motions, they still struggle with complex relational video customization, where precise relational modeling and high generalization across subject categories are essential. The primary challenge arises from the intricate spatial arrangements, layout variations, and nuanced temporal dynamics inherent in relations; consequently, current models tend to overemphasize irrelevant visual details rather than capturing meaningful interactions. To address these challenges, we propose DreamRelation, a novel approach that personalizes relations through a small set of exemplar videos, leveraging two key components: Relational Decoupling Learning and Relational Dynamics Enhancement. First, in Relational Decoupling Learning, we disentangle relations from subject appearances using relation LoRA triplet and hybrid mask training strategy, ensuring better generalization across diverse relationships. Furthermore, we determine the optimal design of relation LoRA triplet by analyzing the distinct roles of the query, key, and value features within MM-DiT's attention mechanism, making DreamRelation the first relational video generation framework with explainable components. Second, in Relational Dynamics Enhancement, we introduce space-time relational contrastive loss, which prioritizes relational dynamics while minimizing the reliance on detailed subject appearances. Extensive experiments demonstrate that DreamRelation outperforms state-of-the-art methods in relational video customization. Code and models will be made publicly available.", 'score': 12, 'issue_id': 2632, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'a32c943630a808fc', 'authors': ['Yujie Wei', 'Shiwei Zhang', 'Hangjie Yuan', 'Biao Gong', 'Longxiang Tang', 'Xiang Wang', 'Haonan Qiu', 'Hengjia Li', 'Shuai Tan', 'Yingya Zhang', 'Hongming Shan'], 'affiliations': ['Alibaba Group', 'Ant Group', 'Fudan University', 'Nanyang Technological University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07602.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#architecture', '#open_source', '#video', '#interpretability', '#games'], 'emoji': '🎬', 'ru': {'title': 'DreamRelation: Персонализация отношений в видео через машинное обучение', 'desc': 'DreamRelation - это новый подход к персонализации отношений в видео с использованием небольшого набора видео-примеров. Метод включает два ключевых компонента: Relational Decoupling Learning и Relational Dynamics Enhancement. Relational Decoupling Learning разделяет отношения и внешний вид субъектов, используя relation LoRA triplet и гибридную стратегию обучения маскам. Relational Dynamics Enhancement вводит пространственно-временную контрастную потерю отношений, чтобы сосредоточиться на динамике отношений, а не на деталях внешнего вида субъектов.'}, 'en': {'title': 'DreamRelation: Personalizing Video Relationships with Precision', 'desc': 'This paper introduces DreamRelation, a new method for creating personalized videos that focus on the relationships between two subjects. Current techniques struggle with complex relational dynamics, often missing meaningful interactions due to an overemphasis on irrelevant details. DreamRelation addresses this by using Relational Decoupling Learning to separate relationships from appearances and Relational Dynamics Enhancement to focus on the dynamics of these relationships. The approach shows significant improvements over existing methods in generating customized relational videos, with the added benefit of explainable components in its design.'}, 'zh': {'title': 'DreamRelation：个性化视频关系建模的新方法', 'desc': '本论文提出了一种名为DreamRelation的新方法，用于个性化视频中的关系建模。该方法通过关系解耦学习和关系动态增强两个关键组件，解决了复杂关系视频定制中的挑战。通过分析查询、键和值特征在注意力机制中的作用，DreamRelation实现了可解释的关系视频生成。实验结果表明，DreamRelation在关系视频定制方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.07459', 'title': 'MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning', 'url': 'https://huggingface.co/papers/2503.07459', 'abstract': 'Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present <PRE_TAG>MedAgentsBench</POST_TAG>, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark.', 'score': 11, 'issue_id': 2634, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '4d9aba5593231609', 'authors': ['Xiangru Tang', 'Daniel Shao', 'Jiwoong Sohn', 'Jiapeng Chen', 'Jiayi Zhang', 'Jinyu Xiang', 'Fang Wu', 'Yilun Zhao', 'Chenglin Wu', 'Wenqi Shi', 'Arman Cohan', 'Mark Gerstein'], 'affiliations': ['Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07459.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#survey', '#healthcare'], 'emoji': '🩺', 'ru': {'title': 'Новый бенчмарк для оценки языковых моделей в сложных медицинских задачах', 'desc': 'Статья представляет новый бенчмарк <PRE_TAG>MedAgentsBench</POST_TAG> для оценки языковых моделей в области медицины. Бенчмарк фокусируется на сложных медицинских вопросах, требующих многоступенчатых клинических рассуждений. Исследование выявило, что новейшие модели, такие как DeepSeek R1 и OpenAI o3, показывают исключительную производительность в сложных медицинских задачах. Анализ также показал, что методы на основе поиска предлагают многообещающее соотношение производительности и стоимости по сравнению с традиционными подходами.'}, 'en': {'title': 'MedAgentsBench: Elevating Medical Question-Answering Evaluation', 'desc': 'This paper introduces MedAgentsBench, a new benchmark designed to evaluate Large Language Models (LLMs) on complex medical questions that require multi-step reasoning. It addresses limitations in current evaluations, such as the prevalence of simple questions and inconsistent testing protocols. The authors conduct experiments with various models, revealing that advanced models like DeepSeek R1 and OpenAI o3 perform well on challenging tasks, while search-based agents show better performance-to-cost ratios. The study highlights significant performance gaps among different model families and provides insights for selecting models based on computational resources.'}, 'zh': {'title': '医学问答的新基准：挑战复杂推理', 'desc': '本文介绍了一个新的医学问答基准测试——MedAgentsBench，旨在评估大型语言模型在复杂医学问题上的表现。该基准测试专注于需要多步骤临床推理、诊断制定和治疗计划的问题，这些问题是当前模型仍然面临挑战的领域。通过对七个已建立的医学数据集进行分析，本文解决了现有评估中的三个关键限制，包括简单问题的普遍性和评估协议的不一致性。实验结果表明，最新的思维模型在复杂医学推理任务中表现出色，并且基于搜索的代理方法在性能与成本比方面具有良好的前景。'}}}, {'id': 'https://huggingface.co/papers/2503.06580', 'title': 'Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models', 'url': 'https://huggingface.co/papers/2503.06580', 'abstract': 'Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position Large Agent Models (LAMs) that internalize the generation of Chain-of-Action (CoA), enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at https://github.com/ADaM-BJTU/AutoCoA', 'score': 11, 'issue_id': 2631, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '01588376bab86ceb', 'authors': ['Yuxiang Zhang', 'Yuqi Yang', 'Jiangming Shu', 'Xinyan Wen', 'Jitao Sang'], 'affiliations': ['School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.06580.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#agents', '#rl', '#training'], 'emoji': '🤖', 'ru': {'title': 'Автономные агентные модели: новый шаг к самостоятельному ИИ', 'desc': 'Эта статья представляет новый подход к созданию агентных моделей искусственного интеллекта, называемых Large Agent Models (LAM). Авторы предлагают фреймворк AutoCoA, который позволяет модели самостоятельно генерировать цепочки действий без внешних подсказок. Фреймворк сочетает в себе методы обучения с учителем и обучения с подкреплением для оптимизации взаимодействия модели с окружающей средой. Результаты показывают, что модели, обученные с помощью AutoCoA, значительно превосходят традиционные подходы в задачах, требующих долгосрочных рассуждений и многошаговых действий.'}, 'en': {'title': 'Empowering Autonomous Reasoning with AutoCoA', 'desc': 'This paper introduces Large Agent Models (LAMs) that enhance the autonomy of reasoning models by allowing them to generate their own Chain-of-Action (CoA) without relying on external prompts. The AutoCoA framework integrates supervised fine-tuning (SFT) and reinforcement learning (RL) to enable models to effectively alternate between reasoning and taking actions in their environment. Key features of this framework include mechanisms for triggering actions at each step, optimizing CoA over entire trajectories, and utilizing an internal world model to minimize the costs associated with real-world interactions. Evaluations show that models trained with AutoCoA outperform traditional ReAct-based workflows, particularly in complex tasks requiring extended reasoning and multiple actions.'}, 'zh': {'title': '自主决策的智能代理模型', 'desc': '传统的智能工作流程依赖外部提示来管理与工具和环境的交互，这限制了推理模型的自主性。我们提出了大型代理模型（LAMs），使其能够内部生成行动链（CoA），从而自主决定何时以及如何使用外部工具。我们提出的AutoCoA框架结合了监督微调（SFT）和强化学习（RL），使模型能够在推理和行动之间无缝切换，同时有效管理与环境的交互。评估结果表明，经过AutoCoA训练的代理模型在开放领域问答任务中显著优于基于ReAct的工作流程，尤其是在需要长期推理和多步骤行动的任务中。'}}}, {'id': 'https://huggingface.co/papers/2503.04973', 'title': 'Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning', 'url': 'https://huggingface.co/papers/2503.04973', 'abstract': 'Incorporating external knowledge in large language models (LLMs) enhances their utility across diverse applications, but existing methods have trade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via similarity search, but key information may fall outside top ranked results. Long-context models can process multiple documents but are computationally expensive and limited by context window size. Inspired by students condensing study material for open-book exams, we propose task-aware key-value (KV) cache compression, which compresses external knowledge in a zero- or few-shot setup. This enables LLMs to reason efficiently over a compacted representation of all relevant information. Experiments show our approach outperforms both RAG and task-agnostic compression methods. On LongBench v2, it improves accuracy by up to 7 absolute points over RAG with a 30x compression rate, while reducing inference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG performs well when sparse evidence suffices, whereas task-aware compression is superior for broad knowledge tasks.', 'score': 11, 'issue_id': 2640, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'a3117b7e2b2c099c', 'authors': ['Giulio Corallo', 'Orion Weller', 'Fabio Petroni', 'Paolo Papotti'], 'affiliations': ['EURECOM', 'Johns Hopkins University', 'SAP Labs', 'Samaya AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.04973.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#long_context', '#rag', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обогащение знаний LLM через интеллектуальное сжатие информации', 'desc': 'Статья предлагает новый метод обогащения знаний больших языковых моделей (LLM) - сжатие кэша ключ-значение с учетом задачи. Этот подход позволяет LLM эффективно рассуждать на основе сжатого представления всей релевантной информации. Эксперименты показывают, что предложенный метод превосходит как RAG, так и методы сжатия, не учитывающие задачу. На тестовом наборе LongBench v2 он повышает точность на 7 процентных пунктов по сравнению с RAG при 30-кратном сжатии, одновременно снижая задержку вывода.'}, 'en': {'title': 'Efficient Knowledge Integration for Enhanced Language Model Performance', 'desc': 'This paper presents a new method for enhancing large language models (LLMs) by incorporating external knowledge more effectively. The proposed task-aware key-value (KV) cache compression allows LLMs to efficiently reason over a compact representation of relevant information, improving performance in zero- or few-shot scenarios. Compared to existing methods like Retrieval-Augmented Generation (RAG), this approach achieves better accuracy and significantly reduces inference time. Experiments demonstrate that while RAG excels with sparse evidence, the new compression technique is more effective for tasks requiring extensive knowledge.'}, 'zh': {'title': '任务感知压缩，提升语言模型效率', 'desc': '本论文提出了一种新的方法，通过任务感知的键值（KV）缓存压缩来整合外部知识，以提高大型语言模型（LLMs）的效率。该方法在零样本或少样本设置下压缩外部知识，使得模型能够在处理相关信息时更加高效。实验结果表明，该方法在准确性和推理延迟方面均优于现有的检索增强生成（RAG）和任务无关的压缩方法。特别是在LongBench v2数据集上，该方法的准确性提高了7个百分点，同时推理延迟从0.43秒减少到0.16秒。'}}}, {'id': 'https://huggingface.co/papers/2503.04812', 'title': 'LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted\n  Contrastive Learning', 'url': 'https://huggingface.co/papers/2503.04812', 'abstract': "Universal multimodal embedding models play a critical role in tasks such as interleaved image-text retrieval, multimodal RAG, and multimodal clustering. However, our empirical results indicate that existing LMM-based embedding models trained with the standard InfoNCE loss exhibit a high degree of overlap in similarity distribution between positive and negative pairs, making it challenging to distinguish hard negative pairs effectively. To deal with this issue, we propose a simple yet effective framework that dynamically improves the embedding model's representation learning for negative pairs based on their discriminative difficulty. Within this framework, we train a series of models, named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks and 36 datasets. Experimental results show that LLaVE establishes stronger baselines that achieve state-of-the-art (SOTA) performance while demonstrating strong scalability and efficiency. Specifically, LLaVE-2B surpasses the previous SOTA 7B models, while LLaVE-7B achieves a further performance improvement of 6.2 points. Although LLaVE is trained on image-text data, it can generalize to text-video retrieval tasks in a zero-shot manner and achieve strong performance, demonstrating its remarkable potential for transfer to other embedding tasks.", 'score': 10, 'issue_id': 2631, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a1fec2227e343e88', 'authors': ['Zhibin Lan', 'Liqiang Niu', 'Fandong Meng', 'Jie Zhou', 'Jinsong Su'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc, China', 'School of Informatics, Xiamen University, China', 'Shanghai Artificial Intelligence Laboratory, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04812.jpg', 'data': {'categories': ['#transfer_learning', '#benchmark', '#rag', '#multimodal', '#training'], 'emoji': '🔀', 'ru': {'title': 'Динамическое обучение для различения сложных негативных пар в мультимодальных эмбеддингах', 'desc': 'Статья представляет новый фреймворк для улучшения обучения мультимодальных эмбеддинг-моделей. Авторы обнаружили проблему перекрытия распределений сходства для позитивных и негативных пар в существующих моделях. Предложенный подход динамически улучшает представление негативных пар на основе их сложности различения. Разработанные модели LLaVE достигают лучших результатов на бенчмарке MMEB и демонстрируют хорошую масштабируемость.'}, 'en': {'title': 'Enhancing Multimodal Embeddings with LLaVE for Better Performance', 'desc': 'This paper discusses the development of a new framework called LLaVE for improving multimodal embedding models, which are used for tasks involving both images and text. The authors identify a problem with existing models that struggle to differentiate between similar positive and negative pairs due to overlapping similarity distributions. To address this, LLaVE dynamically enhances the learning of negative pairs based on their difficulty, leading to better representation learning. The results show that LLaVE outperforms previous state-of-the-art models and can also be applied effectively to other tasks like text-video retrieval.'}, 'zh': {'title': 'LLaVE：提升多模态嵌入的强大工具', 'desc': '本文提出了一种新的多模态嵌入模型LLaVE，旨在解决现有模型在处理正负样本时相似度分布重叠的问题。通过动态调整负样本的表示学习，LLaVE能够更有效地区分困难的负样本。实验结果表明，LLaVE在多个基准测试中表现出色，超越了之前的最先进模型，并在图像-文本数据上训练后，能够在零样本情况下推广到文本-视频检索任务。该模型展示了强大的可扩展性和效率，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.07334', 'title': 'Unleashing the Potential of Large Language Models for Text-to-Image\n  Generation through Autoregressive Representation Alignment', 'url': 'https://huggingface.co/papers/2503.07334', 'abstract': "We present Autoregressive Representation Alignment (ARRA), a new training framework that unlocks global-coherent text-to-image generation in autoregressive LLMs without architectural changes. Unlike prior work that requires complex architectural redesigns, ARRA aligns LLM hidden states with visual representations from external visual foundational models via a global visual alignment loss and a hybrid token, <HYBNEXT>. This token enforces dual constraints: local next-token prediction and global semantic distillation, enabling LLMs to implicitly learn spatial and contextual coherence while retaining their original autoregressive paradigm. Extensive experiments validate ARRA's plug-and-play versatility. When training from text-generation-only LLMs or random initialization, ARRA reduces FID by 25.5% (MIMIC-CXR), 8.8% (DeepEyeNet), and 7.5% (ImageNet) for advanced autoregressive LLMs like Chameleon and LlamaGen, all without framework modifications. For domain adaption, ARRA aligns general-purpose LLMs with specialized models (e.g., BioMedCLIP), achieving an 18.6% FID reduction over direct fine-tuning on medical imaging (MIMIC-CXR). By demonstrating that training objective redesign -- not just architectural innovation -- can resolve cross-modal global coherence challenges, ARRA offers a complementary paradigm for advancing autoregressive models. Code and models will be released to advance autoregressive image generation.", 'score': 9, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'da94bc1af48685ac', 'authors': ['Xing Xie', 'Jiawei Liu', 'Ziyue Lin', 'Huijie Fan', 'Zhi Han', 'Yandong Tang', 'Liangqiong Qu'], 'affiliations': ['Shenyang Institute of Automation, Chinese Academy of Sciences', 'The University of Hong Kong', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.07334.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#open_source', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'ARRA: Глобальная согласованность в генерации изображений без изменения архитектуры', 'desc': 'Представлен новый метод обучения ARRA, позволяющий автореgressивным языковым моделям генерировать глобально согласованные изображения без изменения архитектуры. ARRA выравнивает скрытые состояния языковой модели с визуальными представлениями внешних моделей компьютерного зрения с помощью специальной функции потерь и гибридного токена. Этот подход позволяет языковым моделям неявно учиться пространственной и контекстной согласованности, сохраняя при этом исходную авторегрессивную парадигму. Эксперименты показывают, что ARRA значительно улучшает качество генерации изображений для различных задач и доменов.'}, 'en': {'title': 'Unlocking Coherent Text-to-Image Generation with ARRA', 'desc': "The paper introduces Autoregressive Representation Alignment (ARRA), a novel training framework that enhances text-to-image generation in autoregressive language models (LLMs) without changing their architecture. ARRA achieves this by aligning the hidden states of LLMs with visual representations from external models using a global visual alignment loss and a special hybrid token, <HYBNEXT>. This token imposes both local next-token prediction and global semantic distillation, allowing LLMs to learn spatial and contextual coherence effectively. The results show significant improvements in image generation quality, as evidenced by reduced Fréchet Inception Distance (FID) scores across various datasets, demonstrating ARRA's effectiveness and versatility in enhancing autoregressive models."}, 'zh': {'title': '自回归模型的新突破：全球一致性生成', 'desc': '本文提出了一种新的训练框架，称为自回归表示对齐（ARRA），旨在实现自回归大语言模型（LLM）中的全球一致性文本到图像生成，而无需改变模型架构。ARRA通过全局视觉对齐损失和混合标记<HYBNEXT>，将LLM的隐藏状态与外部视觉基础模型的视觉表示对齐。该标记施加了局部下一个标记预测和全局语义蒸馏的双重约束，使LLM能够在保持自回归范式的同时，隐式学习空间和上下文的一致性。实验结果验证了ARRA的灵活性，显示出在多个数据集上显著降低了FID值，证明了训练目标的重新设计可以解决跨模态全球一致性挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.07507', 'title': 'PE3R: Perception-Efficient 3D Reconstruction', 'url': 'https://huggingface.co/papers/2503.07507', 'abstract': 'Recent advancements in 2D-to-3D perception have significantly improved the understanding of 3D scenes from 2D images. However, existing methods face critical challenges, including limited generalization across scenes, suboptimal perception accuracy, and slow reconstruction speeds. To address these limitations, we propose Perception-Efficient 3D Reconstruction (PE3R), a novel framework designed to enhance both accuracy and efficiency. PE3R employs a feed-forward architecture to enable rapid 3D semantic field reconstruction. The framework demonstrates robust zero-shot generalization across diverse scenes and objects while significantly improving reconstruction speed. Extensive experiments on 2D-to-3D open-vocabulary segmentation and 3D reconstruction validate the effectiveness and versatility of PE3R. The framework achieves a minimum 9-fold speedup in 3D semantic field reconstruction, along with substantial gains in perception accuracy and reconstruction precision, setting new benchmarks in the field. The code is publicly available at: https://github.com/hujiecpp/PE3R.', 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '6a53d341839fc41f', 'authors': ['Jie Hu', 'Shizun Wang', 'Xinchao Wang'], 'affiliations': ['xML Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.07507.jpg', 'data': {'categories': ['#3d', '#architecture', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Быстрая и точная 3D-реконструкция из 2D-изображений', 'desc': 'PE3R - это новая система для улучшения 3D-реконструкции из 2D-изображений. Она использует прямую архитектуру для быстрого восстановления 3D семантических полей. PE3R демонстрирует надежную обобщающую способность на разных сценах и объектах без дополнительного обучения. Эксперименты показывают значительное ускорение реконструкции и повышение точности восприятия по сравнению с существующими методами.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding with PE3R', 'desc': 'This paper introduces Perception-Efficient 3D Reconstruction (PE3R), a new framework that enhances the process of understanding 3D scenes from 2D images. PE3R addresses key challenges in existing methods, such as limited generalization and slow reconstruction speeds, by utilizing a feed-forward architecture. The framework achieves impressive zero-shot generalization across various scenes and objects, while also significantly speeding up the reconstruction process. Experimental results show that PE3R offers at least a 9-fold increase in reconstruction speed and improved accuracy, establishing new benchmarks in 2D-to-3D perception.'}, 'zh': {'title': '感知高效3D重建，速度与精度的双重提升', 'desc': '最近在2D到3D感知方面的进展显著提高了从2D图像理解3D场景的能力。然而，现有方法面临着场景泛化能力有限、感知精度不佳和重建速度慢等关键挑战。为了解决这些问题，我们提出了感知高效3D重建（PE3R）框架，旨在提高准确性和效率。PE3R采用前馈架构，能够快速重建3D语义场，并在多样场景和物体上展示出强大的零样本泛化能力，同时显著提高重建速度。'}}}, {'id': 'https://huggingface.co/papers/2503.07197', 'title': 'Effective and Efficient Masked Image Generation Models', 'url': 'https://huggingface.co/papers/2503.07197', 'abstract': "Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within a single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Fr\\'echet Inception Distance (FID). In particular, on ImageNet 256x256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-of-the-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models.", 'score': 8, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '4740cc0178bbf099', 'authors': ['Zebin You', 'Jingyang Ou', 'Xiaolu Zhang', 'Jun Hu', 'Jun Zhou', 'Chongxuan Li'], 'affiliations': ['Ant Group', 'Beijing Key Laboratory of Big Data Management and Analysis Method', 'Gaoling School of AI, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07197.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Объединение маскированных моделей для эффективной генерации изображений', 'desc': 'Исследователи объединили модели генерации маскированных изображений и маскированные диффузионные модели в единую структуру. Они изучили пространство проектирования обучения и сэмплирования, выявив ключевые факторы, влияющие на производительность и эффективность. На основе этого анализа была разработана модель eMIGM, которая показала высокую эффективность при генерации изображений ImageNet. eMIGM превзошла существующие модели по метрике FID при меньшем количестве вычислений и параметров.'}, 'en': {'title': 'Unifying Masked Models for Efficient Image Generation', 'desc': "This paper presents a unified framework for masked image generation models and masked diffusion models, highlighting their similarities despite different goals. The authors explore various design choices in training and sampling, which significantly impact the model's performance and efficiency. They introduce a new model called eMIGM, which shows superior results on ImageNet generation tasks, particularly in terms of Fréchet Inception Distance (FID). Notably, eMIGM achieves competitive performance with fewer function evaluations compared to existing state-of-the-art models, demonstrating its efficiency and effectiveness in image generation."}, 'zh': {'title': '统一掩蔽模型，提升图像生成性能', 'desc': '本文探讨了掩蔽图像生成模型和掩蔽扩散模型的统一框架。我们分析了训练和采样的设计空间，识别出影响性能和效率的关键因素。基于这些改进，我们开发了名为eMIGM的模型。实验结果表明，eMIGM在ImageNet生成任务中表现优异，尤其在较低的函数评估次数下超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05856', 'title': 'This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs', 'url': 'https://huggingface.co/papers/2503.05856', 'abstract': "Mixture of <PRE_TAG>large language model (LLMs) Agents (MoA)</POST_TAG> architectures achieve state-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by leveraging the collaboration of multiple LLMs at inference time. Despite these successes, an evaluation of the safety and reliability of MoA is missing. We present the first comprehensive study of MoA's robustness against deceptive LLM agents that deliberately provide misleading responses. We examine factors like the propagation of deceptive information, model size, and information availability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the popular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of 49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate that introducing only a single carefully-instructed deceptive agent into the MoA can reduce performance to 37.9%, effectively nullifying all MoA gains. On QuALITY, a multiple-choice comprehension task, the impact is also severe, with accuracy plummeting by a staggering 48.5%. Inspired in part by the historical Doge of Venice voting process, designed to minimize influence and deception, we propose a range of unsupervised defense mechanisms that recover most of the lost performance.", 'score': 7, 'issue_id': 2639, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'a7ca72375d4cd423', 'authors': ['Lorenz Wolf', 'Sangwoong Yoon', 'Ilija Bogunovic'], 'affiliations': ['University College London Center for Artificial Intelligence, London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2503.05856.jpg', 'data': {'categories': ['#inference', '#security', '#benchmark', '#agents', '#hallucinations'], 'emoji': '🛡️', 'ru': {'title': 'Защита коллаборативных языковых моделей от обмана', 'desc': 'Исследование посвящено архитектуре Mixture of large language model Agents (MoA), которая достигает высоких результатов на бенчмарках, объединяя несколько языковых моделей. Авторы изучают уязвимости MoA к обманным агентам, намеренно предоставляющим ложные ответы. Эксперименты показывают, что даже один обманный агент может значительно снизить производительность MoA на задачах вроде AlpacaEval 2.0 и QuALITY. Предлагаются механизмы защиты, вдохновленные историческим процессом голосования Дожа Венеции, для восстановления утраченной производительности.'}, 'en': {'title': 'Strengthening MoA: Defending Against Deceptive Agents', 'desc': 'This paper investigates the robustness of Mixture of Large Language Model Agents (MoA) architectures, which have shown impressive results in various benchmarks. The authors highlight a significant gap in understanding how these models handle deceptive agents that can provide misleading information. Their experiments reveal that even a single deceptive agent can drastically reduce the performance of MoA systems, indicating vulnerabilities in their design. To address these issues, the paper proposes unsupervised defense mechanisms inspired by historical voting processes to enhance the reliability of MoA against such threats.'}, 'zh': {'title': '提升大型语言模型的安全性与可靠性', 'desc': '这篇论文研究了混合大型语言模型（LLMs）代理架构的安全性和可靠性。尽管这些架构在多个基准测试中表现出色，但对其抵御误导性信息的能力缺乏评估。研究发现，单个误导性代理的引入会显著降低模型的性能，甚至抵消所有的优势。为此，论文提出了一系列无监督的防御机制，以恢复大部分丢失的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.06520', 'title': 'Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive\n  Reinforcement', 'url': 'https://huggingface.co/papers/2503.06520', 'abstract': "Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.", 'score': 6, 'issue_id': 2630, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'b21eb23a448d282e', 'authors': ['Yuqi Liu', 'Bohao Peng', 'Zhisheng Zhong', 'Zihao Yue', 'Fanbin Lu', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'RUC'], 'pdf_title_img': 'assets/pdf/title_img/2503.06520.jpg', 'data': {'categories': ['#rl', '#benchmark', '#architecture', '#reasoning', '#rlhf', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Сегментация с рассуждением: ИИ учится объяснять свои решения', 'desc': 'Seg-Zero - это новая модель для сегментации изображений, использующая когнитивное подкрепление для улучшения обобщающей способности и явного рассуждения. Архитектура состоит из модели рассуждений, генерирующей цепочки рассуждений и позиционные подсказки, и модели сегментации, создающей маски на уровне пикселей. Обучение происходит с помощью обучения с подкреплением и специального механизма вознаграждений. Seg-Zero-7B превосходит предыдущие модели на 18% в задаче zero-shot сегментации на бенчмарке ReasonSeg.'}, 'en': {'title': 'Seg-Zero: Revolutionizing Reasoning Segmentation with Zero-Shot Generalization', 'desc': 'The paper introduces Seg-Zero, a new framework for reasoning segmentation that overcomes the limitations of traditional supervised methods. It features a decoupled architecture with a reasoning model that interprets user intentions and generates reasoning chains, which are then used by a segmentation model to create detailed pixel-level masks. The framework employs a unique reward mechanism that balances format and accuracy to optimize performance through reinforcement learning. Seg-Zero demonstrates impressive zero-shot generalization capabilities, achieving a significant performance boost on the ReasonSeg benchmark compared to previous models.'}, 'zh': {'title': 'Seg-Zero：突破性推理与分割的结合', 'desc': '传统的分割推理方法依赖于带有类别标签的监督微调，这限制了其在不同领域的泛化能力，并缺乏明确的推理过程。为了解决这些问题，我们提出了Seg-Zero，这是一种新颖的框架，展示了显著的泛化能力，并通过认知强化推导出明确的推理链。Seg-Zero引入了一个解耦架构，包括推理模型和分割模型，推理模型解释用户意图，生成明确的推理链，并产生位置提示，随后由分割模型生成精确的像素级掩码。通过强化学习训练，Seg-Zero在没有明确推理数据的情况下，实现了强大的零样本泛化能力，并展现出突出的测试时推理能力。'}}}, {'id': 'https://huggingface.co/papers/2503.06121', 'title': 'BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement\n  for Transformers in Large-Scale Time Series Modeling', 'url': 'https://huggingface.co/papers/2503.06121', 'abstract': "Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer.", 'score': 5, 'issue_id': 2631, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': '3f03abe6317e5bba', 'authors': ['Li weile', 'Liu Xiao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.06121.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#open_source'], 'emoji': '⏳', 'ru': {'title': 'Революция в масштабировании моделей временных рядов с RWKV-7', 'desc': 'Статья представляет новый подход к масштабированию моделей временных рядов с использованием архитектуры RWKV-7. Авторы интегрируют компоненты RWKV-7 в трансформер-модель Timer для обработки временных рядов. Результаты показывают значительное улучшение производительности и сокращение времени обучения при меньшем количестве параметров. Предложенный метод решает проблемы масштабирования моделей временных рядов, аналогичные тем, с которыми сталкиваются большие языковые модели.'}, 'en': {'title': 'Revolutionizing Time Series with RWKV-7: Efficiency Meets Performance', 'desc': "This paper addresses the challenges of scaling time series models to manage large and complex datasets, similar to the advancements seen in large language models. It introduces RWKV-7, a novel approach that integrates meta-learning into the state update mechanism of time series models. By combining RWKV-7's time mix and channel mix components with the Timer model, the authors report significant performance enhancements and reduced training times. The proposed method achieves impressive results with fewer parameters, making it a promising solution for time series analysis."}, 'zh': {'title': '创新时间序列模型，提升性能与效率', 'desc': '时间序列模型在处理大型复杂数据集时面临显著挑战，类似于大型语言模型的扩展能力。时间序列数据的独特特性和模型扩展的计算需求需要创新的方法。我们提出了一种新颖的解决方案，使用RWKV-7将元学习融入状态更新机制。通过将RWKV-7的时间混合和通道混合组件整合到基于变换器的时间序列模型Timer中，我们实现了约1.13到43.3倍的性能提升，并将训练时间减少了4.5倍，同时参数数量仅为原来的1/23。'}}}, {'id': 'https://huggingface.co/papers/2503.03499', 'title': 'State-offset Tuning: State-based Parameter-Efficient Fine-Tuning for\n  State Space Models', 'url': 'https://huggingface.co/papers/2503.03499', 'abstract': 'State Space Models (SSMs) have emerged as efficient alternatives to Transformers, mitigating their quadratic computational cost. However, the application of Parameter-Efficient Fine-Tuning (PEFT) methods to SSMs remains largely unexplored. In particular, prompt-based methods like Prompt Tuning and Prefix-Tuning, which are widely used in Transformers, do not perform well on SSMs. To address this, we propose state-based methods as a superior alternative to prompt-based methods. This new family of methods naturally stems from the architectural characteristics of SSMs. State-based methods adjust state-related features directly instead of depending on external prompts. Furthermore, we introduce a novel state-based PEFT method: State-offset Tuning. At every timestep, our method directly affects the state at the current step, leading to more effective adaptation. Through extensive experiments across diverse datasets, we demonstrate the effectiveness of our method. Code is available at https://github.com/furiosa-ai/ssm-state-tuning.', 'score': 5, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '80ab6abd822f2976', 'authors': ['Wonjun Kang', 'Kevin Galim', 'Yuchen Zeng', 'Minjae Lee', 'Hyung Il Koo', 'Nam Ik Cho'], 'affiliations': ['UW-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2503.03499.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная тонкая настройка моделей пространства состояний без промптов', 'desc': 'Эта статья представляет новый подход к тонкой настройке моделей пространства состояний (SSM) в машинном обучении. Авторы предлагают методы, основанные на состояниях, как альтернативу методам, основанным на промптах, которые хорошо работают для трансформеров, но неэффективны для SSM. Они вводят новый метод под названием State-offset Tuning, который напрямую влияет на состояние модели на каждом временном шаге. Эксперименты на различных наборах данных демонстрируют эффективность предложенного подхода.'}, 'en': {'title': 'Revolutionizing Fine-Tuning with State-Based Methods for SSMs', 'desc': 'This paper discusses the limitations of using traditional prompt-based fine-tuning methods on State Space Models (SSMs), which are more efficient than Transformers. The authors propose a new approach called state-based methods that leverage the unique structure of SSMs to improve performance. Specifically, they introduce State-offset Tuning, a novel parameter-efficient fine-tuning technique that modifies the state at each timestep directly. Experimental results show that this method outperforms existing prompt-based techniques, highlighting its effectiveness in adapting SSMs for various tasks.'}, 'zh': {'title': '基于状态的微调：超越提示的方法', 'desc': '状态空间模型（SSMs）作为变换器的高效替代方案，能够减轻其二次计算成本。然而，参数高效微调（PEFT）方法在SSMs上的应用仍然未被充分探索。我们提出基于状态的方法，作为优于基于提示的方法的新选择，这些方法直接调整与状态相关的特征，而不是依赖外部提示。我们还引入了一种新颖的基于状态的PEFT方法：状态偏移微调，能够在每个时间步直接影响当前状态，从而实现更有效的适应。'}}}, {'id': 'https://huggingface.co/papers/2503.07595', 'title': 'Detection Avoidance Techniques for Large Language Models', 'url': 'https://huggingface.co/papers/2503.07595', 'abstract': "The increasing popularity of large language models has not only led to widespread use but has also brought various risks, including the potential for systematically spreading fake news. Consequently, the development of classification systems such as DetectGPT has become vital. These detectors are vulnerable to evasion techniques, as demonstrated in an experimental series: Systematic changes of the generative models' temperature proofed shallow learning-detectors to be the least reliable. Fine-tuning the generative model via reinforcement learning circumvented BERT-based-detectors. Finally, rephrasing led to a >90\\% evasion of zero-shot-detectors like DetectGPT, although texts stayed highly similar to the original. A comparison with existing work highlights the better performance of the presented methods. Possible implications for society and further research are discussed.", 'score': 4, 'issue_id': 2631, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '0b6929d80e047189', 'authors': ['Sinclair Schneider', 'Florian Steuber', 'Joao A. G. Schneider', 'Gabi Dreo Rodosek'], 'affiliations': ['Research Institute CODE, Bundeswehr University Munich, Munich, 81739, Bavaria, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.07595.jpg', 'data': {'categories': ['#security', '#benchmark', '#rlhf', '#data', '#ethics', '#rl', '#hallucinations'], 'emoji': '🕵️', 'ru': {'title': 'Обман детекторов: как LLM обходят системы обнаружения ИИ-текстов', 'desc': 'Статья рассматривает проблему уязвимости систем обнаружения текстов, сгенерированных большими языковыми моделями (LLM). Авторы провели серию экспериментов, демонстрирующих, как различные методы могут обойти существующие детекторы, такие как DetectGPT. Были исследованы техники изменения температуры генеративной модели, дообучение с помощью обучения с подкреплением и перефразирование текста. Результаты показывают, что предложенные методы обхода более эффективны, чем существующие подходы, что поднимает вопросы о надежности текущих систем обнаружения ИИ-генерированного контента.'}, 'en': {'title': 'Enhancing Evasion Techniques Against Language Model Detectors', 'desc': "This paper discusses the risks associated with large language models, particularly their potential to spread misinformation. It introduces DetectGPT, a classification system designed to identify generated text, but reveals its vulnerabilities to evasion techniques. The study shows that adjusting the generative model's temperature and fine-tuning it with reinforcement learning can effectively bypass existing detectors. Additionally, rephrasing generated content can achieve over 90% evasion rates while maintaining similarity to the original text, highlighting the need for improved detection methods."}, 'zh': {'title': '应对假新闻的智能检测挑战', 'desc': '随着大型语言模型的普及，假新闻传播的风险也随之增加。因此，开发像DetectGPT这样的分类系统变得至关重要。这些检测器容易受到规避技术的影响，实验表明，生成模型温度的系统性变化使得浅层学习检测器的可靠性最低。通过强化学习微调生成模型可以绕过基于BERT的检测器，而重新表述文本则使得像DetectGPT这样的零样本检测器的规避率超过90%，尽管文本与原文高度相似。'}}}, {'id': 'https://huggingface.co/papers/2503.07274', 'title': 'Efficient Distillation of Classifier-Free Guidance using Adapters', 'url': 'https://huggingface.co/papers/2503.07274', 'abstract': 'While classifier-free guidance (CFG) is essential for conditional diffusion models, it doubles the number of neural function evaluations (NFEs) per inference step. To mitigate this inefficiency, we introduce adapter guidance distillation (AGD), a novel approach that simulates CFG in a single forward pass. AGD leverages lightweight adapters to approximate CFG, effectively doubling the sampling speed while maintaining or even improving sample quality. Unlike prior guidance distillation methods that tune the entire model, AGD keeps the base model frozen and only trains minimal additional parameters (sim2%) to significantly reduce the resource requirement of the distillation phase. Additionally, this approach preserves the original model weights and enables the adapters to be seamlessly combined with other checkpoints derived from the same base model. We also address a key mismatch between training and inference in existing guidance distillation methods by training on CFG-guided trajectories instead of standard diffusion trajectories. Through extensive experiments, we show that AGD achieves comparable or superior FID to CFG across multiple architectures with only half the NFEs. Notably, our method enables the distillation of large models (sim2.6B parameters) on a single consumer GPU with 24 GB of VRAM, making it more accessible than previous approaches that require multiple high-end GPUs. We will publicly release the implementation of our method.', 'score': 4, 'issue_id': 2639, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '84e5b0fd1b6f7f9f', 'authors': ['Cristian Perez Jensen', 'Seyedmorteza Sadat'], 'affiliations': ['ETH Zürich'], 'pdf_title_img': 'assets/pdf/title_img/2503.07274.jpg', 'data': {'categories': ['#inference', '#optimization', '#diffusion', '#training', '#open_source', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных моделей с сохранением качества', 'desc': "Статья представляет новый метод под названием 'адаптерная дистилляция управления' (AGD) для условных диффузионных моделей. AGD симулирует управление без классификатора (CFG) за один проход сети, удваивая скорость семплирования без потери качества. Метод использует легковесные адаптеры, обучая всего 2% параметров и сохраняя базовую модель неизменной. AGD решает проблему несоответствия между обучением и выводом, тренируясь на траекториях с CFG."}, 'en': {'title': 'Speeding Up Diffusion Models with Adapter Guidance Distillation', 'desc': 'This paper presents adapter guidance distillation (AGD), a new method that enhances the efficiency of conditional diffusion models by simulating classifier-free guidance (CFG) in a single forward pass. AGD uses lightweight adapters to approximate CFG, which allows for faster sampling without sacrificing sample quality. The approach keeps the base model unchanged and only trains a small number of additional parameters, significantly reducing resource requirements. Experimental results demonstrate that AGD achieves similar or better performance compared to CFG while halving the number of neural function evaluations needed, making it feasible to distill large models on consumer-grade hardware.'}, 'zh': {'title': '适配器引导蒸馏：提升扩散模型效率的创新方法', 'desc': '本文提出了一种新的方法，称为适配器引导蒸馏（AGD），旨在提高条件扩散模型的效率。AGD通过轻量级适配器在单次前向传播中模拟无分类器引导（CFG），从而实现了采样速度的加倍，同时保持或改善样本质量。与以往的引导蒸馏方法不同，AGD只训练少量额外参数，而保持基础模型不变，显著降低了资源需求。通过大量实验，我们证明AGD在多个架构上实现了与CFG相当或更优的FID，同时只需一半的神经函数评估（NFE）。'}}}, {'id': 'https://huggingface.co/papers/2503.02199', 'title': 'Words or Vision: Do Vision-Language Models Have Blind Faith in Text?', 'url': 'https://huggingface.co/papers/2503.02199', 'abstract': "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings. By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a ``blind faith in text'' phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns. We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training. Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies.", 'score': 4, 'issue_id': 2630, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'a354f8de058f0f84', 'authors': ['Ailin Deng', 'Tri Cao', 'Zhirui Chen', 'Bryan Hooi'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.02199.jpg', 'data': {'categories': ['#interpretability', '#alignment', '#training', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Опасность слепой веры в текст: проблема и решения для VLM моделей', 'desc': "Исследование показывает, что модели компьютерного зрения и обработки естественного языка (VLM) склонны чрезмерно доверять текстовым данным при несоответствии между визуальной и текстовой информацией. Это явление, названное 'слепой верой в текст', может привести к значительному снижению производительности моделей и вызывает опасения по поводу их безопасности. Авторы анализируют факторы, влияющие на текстовое смещение, и предлагают методы его уменьшения, включая дообучение с аугментацией текста. Теоретический анализ предполагает, что проблема может быть связана с дисбалансом чисто текстовых и мультимодальных данных во время обучения моделей."}, 'en': {'title': 'Balancing Vision and Text: Overcoming Bias in Vision-Language Models', 'desc': "This paper investigates how Vision-Language Models (VLMs) manage inconsistencies between visual and textual information in tasks that rely heavily on vision. The authors identify a phenomenon called 'blind faith in text,' where VLMs tend to rely more on textual data than visual data when faced with conflicting inputs, which can lead to performance issues. They analyze various factors that contribute to this text bias, such as the size of the language model and the order of tokens in the text. To mitigate this bias, the paper proposes supervised fine-tuning with text augmentation and emphasizes the importance of balanced training for improving the reliability of VLMs in multi-modal contexts."}, 'zh': {'title': '平衡训练，提升视觉语言模型的可靠性', 'desc': '视觉语言模型（VLMs）在处理视觉和文本信息方面表现出色，但它们在面对模态不一致时的表现尚未得到充分研究。我们探讨了VLMs在视觉数据和不同文本输入下的模态偏好，发现了“对文本的盲目信任”现象：当出现不一致时，VLMs过度依赖文本数据，导致性能显著下降。我们分析了影响这种文本偏见的因素，包括指令提示、语言模型大小、文本相关性、标记顺序以及视觉和文本确定性之间的相互作用。为了解决这个问题，我们探索了带有文本增强的监督微调，并证明其在减少文本偏见方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.07603', 'title': 'Should VLMs be Pre-trained with Image Data?', 'url': 'https://huggingface.co/papers/2503.07603', 'abstract': 'Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. While adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier into the training process. To investigate this, we train models spanning various datasets, scales, image-text ratios, and amount of pre-training done before introducing vision tokens. We then fine-tune these models and evaluate their downstream performance on a suite of vision-language and text-only tasks. We find that pre-training with a mixture of image and text data allows models to perform better on vision-language tasks while maintaining strong performance on text-only evaluations. On an average of 6 diverse tasks, we find that for a 1B model, introducing visual tokens 80% of the way through pre-training results in a 2% average improvement over introducing visual tokens to a fully pre-trained model.', 'score': 3, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '16f17eb6db67a418', 'authors': ['Sedrick Keh', 'Jean Mercat', 'Samir Yitzhak Gadre', 'Kushal Arora', 'Igor Vasiljevic', 'Benjamin Burchfiel', 'Shuran Song', 'Russ Tedrake', 'Thomas Kollar', 'Ludwig Schmidt', 'Achal Dave'], 'affiliations': ['Columbia University', 'MIT', 'Stanford', 'Toyota Research Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.07603.jpg', 'data': {'categories': ['#multimodal', '#training', '#benchmark', '#transfer_learning', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Оптимальное время для интеграции визуальных данных в языковые модели', 'desc': 'Исследование посвящено сравнению эффективности двухэтапного обучения языковых моделей (сначала на текстах, затем на изображениях) с моделями, изначально обучаемыми на смешанных данных. Авторы провели эксперименты с различными наборами данных, масштабами моделей и соотношениями изображений и текста. Результаты показывают, что обучение на смешанных данных позволяет моделям лучше справляться с задачами, связанными с обработкой изображений и текста, сохраняя при этом высокую производительность в текстовых задачах. Для модели размером 1 миллиард параметров введение визуальных токенов на 80% этапе предобучения дает в среднем 2% улучшение по сравнению с добавлением изображений к полностью предобученной модели.'}, 'en': {'title': 'Unlocking Vision-Language Synergy: Timing Matters!', 'desc': 'This paper explores the effectiveness of training large language models (LLMs) with image data in a two-step process compared to integrating images earlier in the training. The authors conduct experiments with various datasets and training configurations to assess the impact of when visual tokens are introduced. Their findings indicate that pre-training with both image and text data enhances performance on vision-language tasks while still performing well on text-only tasks. Specifically, they observe a 2% improvement in performance when visual tokens are added later in the pre-training phase for a 1B model.'}, 'zh': {'title': '图像与文本混合预训练提升视觉语言任务表现', 'desc': '这篇论文探讨了预训练的大型语言模型（LLM）在加入图像数据后在视觉语言任务中的表现。研究发现，在第二阶段训练中加入图像数据可以有效提升模型的能力，但不清楚这种两步训练流程与早期整合图像的视觉语言模型（VLM）相比，究竟是增益还是损失。通过训练不同数据集、规模和图像文本比例的模型，研究者发现混合图像和文本数据的预训练可以提高视觉语言任务的表现，同时在仅文本的评估中也保持良好表现。结果显示，对于一个10亿参数的模型，在预训练的80%时引入视觉标记，平均提升了2%的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.07465', 'title': 'YOLOE: Real-Time Seeing Anything', 'url': 'https://huggingface.co/papers/2503.07465', 'abstract': "Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3times less training cost and 1.4times inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP^b and 0.4 AP^m gains over closed-set YOLOv8-L with nearly 4times less training time. Code and models are available at https://github.com/THU-MIG/yoloe.", 'score': 3, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'faab5c7007c9cc4d', 'authors': ['Ao Wang', 'Lihao Liu', 'Hui Chen', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07465.jpg', 'data': {'categories': ['#training', '#benchmark', '#transfer_learning', '#optimization', '#cv'], 'emoji': '🔍', 'ru': {'title': 'YOLOE: Эффективное обнаружение и сегментация чего угодно в реальном времени', 'desc': 'YOLOE - это новая модель для обнаружения и сегментации объектов, которая объединяет различные механизмы открытых подсказок в одной эффективной архитектуре. Она использует стратегию RepRTA для текстовых подсказок, энкодер SAVPE для визуальных подсказок и метод LRPC для сценариев без подсказок. YOLOE демонстрирует исключительную производительность в задачах zero-shot и переносимость, превосходя существующие модели по эффективности и точности при меньших затратах на обучение.'}, 'en': {'title': 'YOLOE: Real-Time Object Detection and Segmentation for Open-Set Scenarios', 'desc': 'This paper presents YOLOE, a novel model that enhances object detection and segmentation in open-set scenarios by integrating various prompt mechanisms. It introduces a Re-parameterizable Region-Text Alignment (RepRTA) strategy for text prompts, which refines textual embeddings efficiently without additional inference costs. For visual prompts, the Semantic-Activated Visual Prompt Encoder (SAVPE) improves visual accuracy while maintaining low complexity. Additionally, the Lazy Region-Prompt Contrast (LRPC) strategy allows for prompt-free object identification, significantly reducing training costs and improving inference speed compared to traditional models.'}, 'zh': {'title': 'YOLOE：高效的开放场景目标检测与分割', 'desc': '本文介绍了一种新的目标检测和分割模型YOLOE，旨在克服传统模型在开放场景中的局限性。YOLOE通过集成多种开放提示机制，实现了高效的实时检测和分割。我们提出了可重参数化区域-文本对齐策略（RepRTA）和语义激活视觉提示编码器（SAVPE），以提高视觉和文本的对齐效果。实验结果表明，YOLOE在零样本性能和迁移能力上表现优异，同时训练成本低，推理效率高。'}}}, {'id': 'https://huggingface.co/papers/2503.07265', 'title': 'WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image\n  Generation', 'url': 'https://huggingface.co/papers/2503.07265', 'abstract': 'Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text to image generation. To address this challenge, we propose WISE, the first benchmark specifically designed for World Knowledge-Informed Semantic Evaluation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 sub-domains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce WiScore, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at https://github.com/PKU-YuanGroup/WISE.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '6174088b65d232ba', 'authors': ['Yuwei Niu', 'Munan Ning', 'Mengren Zheng', 'Bin Lin', 'Peng Jin', 'Jiaqi Liao', 'Kunpeng Ning', 'Bin Zhu', 'Li Yuan'], 'affiliations': ['Chongqing University', 'Peking University', 'PengCheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.07265.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#interpretability', '#cv'], 'emoji': '🧠', 'ru': {'title': 'WISE: новый рубеж в оценке семантического понимания моделей text-to-image', 'desc': 'Исследователи предложили новый бенчмарк WISE для оценки интеграции мировых знаний в модели генерации изображений по тексту. WISE включает 1000 тщательно разработанных промптов в 25 поддоменах, охватывающих культурный здравый смысл, пространственно-временное мышление и естественные науки. Авторы также представили новую метрику WiScore для количественной оценки соответствия знаний и изображений. Тестирование 20 моделей выявило значительные ограничения в их способности эффективно интегрировать и применять мировые знания при генерации изображений.'}, 'en': {'title': 'Enhancing T2I Models with World Knowledge Evaluation', 'desc': 'This paper introduces WISE, a new benchmark for evaluating Text-to-Image (T2I) models that focuses on their ability to integrate world knowledge and complex semantic understanding. Unlike previous assessments that primarily measure image realism and basic text-image alignment, WISE challenges models with 1,000 detailed prompts across various domains, including cultural common sense and natural science. The authors also present WiScore, a new metric designed to quantitatively assess how well models align knowledge with generated images. Testing reveals that many current T2I models struggle to effectively incorporate world knowledge, indicating areas for improvement in future model development.'}, 'zh': {'title': '提升文本到图像生成的知识整合能力', 'desc': '本文提出了WISE，这是第一个专门为世界知识驱动的语义评估设计的基准。现有的文本到图像生成模型主要关注图像的真实感和简单的文本-图像对齐，缺乏对复杂语义理解和世界知识整合的全面评估。WISE通过1000个精心设计的提示，涵盖文化常识、时空推理和自然科学等25个子领域，挑战模型的能力。我们还引入了WiScore这一新颖的定量指标，以评估知识与图像的对齐程度，测试结果显示现有模型在有效整合和应用世界知识方面存在显著局限。'}}}, {'id': 'https://huggingface.co/papers/2503.06885', 'title': 'ProBench: Judging Multimodal Foundation Models on Open-ended\n  Multi-domain Expert Tasks', 'url': 'https://huggingface.co/papers/2503.06885', 'abstract': 'Solving expert-level multimodal tasks is a key milestone towards general intelligence. As the capabilities of multimodal large language models (MLLMs) continue to improve, evaluation of such advanced multimodal intelligence becomes necessary yet challenging. In this work, we introduce ProBench, a benchmark of open-ended user queries that require professional expertise and advanced reasoning. ProBench consists of 4,000 high-quality samples independently submitted by professionals based on their daily productivity demands. It spans across 10 fields and 56 sub-fields, including science, arts, humanities, coding, mathematics, and creative writing. Experimentally, we evaluate and compare 24 latest models using MLLM-as-a-Judge. Our results reveal that although the best open-source models rival the proprietary ones, ProBench presents significant challenges in visual perception, textual understanding, domain knowledge and advanced reasoning, thus providing valuable directions for future multimodal AI research efforts.', 'score': 3, 'issue_id': 2633, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '196c83578a251f8e', 'authors': ['Yan Yang', 'Dongxu Li', 'Haoning Wu', 'Bei Chen', 'Liu Liu', 'Liyuan Pan', 'Junnan Li'], 'affiliations': ['ANU', 'BITSZ & School of CSAT, BIT', 'KooMap, Huawei', 'NTU', 'Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.06885.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#agi', '#benchmark', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'ProBench: испытание интеллекта мультимодальных ИИ-моделей', 'desc': 'ProBench - это новый бенчмарк для оценки продвинутых мультимодальных языковых моделей (MLLM). Он состоит из 4000 высококачественных задач, охватывающих 10 областей и 56 подобластей, включая науку, искусство, гуманитарные науки и программирование. Задачи были предложены профессионалами на основе их повседневных рабочих потребностей. Результаты тестирования 24 новейших моделей с использованием метода MLLM-as-a-Judge показали, что ProBench представляет значительные трудности в визуальном восприятии, понимании текста, предметных знаниях и продвинутых рассуждениях.'}, 'en': {'title': 'ProBench: Benchmarking Multimodal Intelligence for Expert Tasks', 'desc': 'This paper presents ProBench, a new benchmark designed to evaluate multimodal large language models (MLLMs) on expert-level tasks. ProBench includes 4,000 user queries that require advanced reasoning and professional expertise across various fields such as science, arts, and coding. The study compares 24 state-of-the-art models using MLLM-as-a-Judge, highlighting the challenges these models face in visual perception, textual understanding, and domain knowledge. The findings indicate that while some open-source models perform comparably to proprietary ones, ProBench identifies critical areas for improvement in multimodal AI capabilities.'}, 'zh': {'title': '多模态智能评估的新基准：ProBench', 'desc': '本论文介绍了ProBench，这是一个针对多模态大语言模型（MLLM）的基准测试，旨在评估其在专业领域的智能表现。ProBench包含4000个高质量样本，涵盖科学、艺术、人文学科、编程、数学和创意写作等10个领域和56个子领域。通过对24个最新模型的实验评估，结果显示尽管一些开源模型在性能上与专有模型相当，但在视觉感知、文本理解、领域知识和高级推理方面，ProBench仍然提出了显著的挑战。该研究为未来多模态人工智能的研究方向提供了重要的参考。'}}}, {'id': 'https://huggingface.co/papers/2503.06626', 'title': 'DiffCLIP: Differential Attention Meets CLIP', 'url': 'https://huggingface.co/papers/2503.06626', 'abstract': "We propose DiffCLIP, a novel vision-language model that extends the differential attention mechanism to CLIP architectures. Differential attention was originally developed for large language models to amplify relevant context while canceling out noisy information. In this work, we integrate this mechanism into CLIP's dual encoder (image and text) framework. With minimal additional parameters, DiffCLIP achieves superior performance on image-text understanding tasks. Across zero-shot classification, retrieval, and robustness benchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably, these gains come with negligible computational overhead, demonstrating that differential attention can significantly enhance multi-modal representations without sacrificing efficiency. Code can be found at https://github.com/hammoudhasan/DiffCLIP.", 'score': 3, 'issue_id': 2641, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'fff7aaf70b1d6ed0', 'authors': ['Hasan Abed Al Kader Hammoud', 'Bernard Ghanem'], 'affiliations': ['KAUST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06626.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#benchmark', '#multimodal', '#cv'], 'emoji': '🔍', 'ru': {'title': 'DiffCLIP: улучшение мультимодальных представлений с помощью дифференциального внимания', 'desc': 'Исследователи представили DiffCLIP - новую модель для обработки изображений и текста, которая расширяет механизм дифференциального внимания на архитектуру CLIP. Дифференциальное внимание позволяет усиливать релевантный контекст и подавлять шумовую информацию. DiffCLIP интегрирует этот механизм в двойной энкодер CLIP для изображений и текста. Модель достигает превосходных результатов в задачах понимания изображений и текста, превосходя базовые модели CLIP при минимальных дополнительных параметрах.'}, 'en': {'title': 'Enhancing CLIP with Differential Attention for Better Image-Text Understanding', 'desc': "DiffCLIP is a new vision-language model that improves the CLIP architecture by using a technique called differential attention. This method helps the model focus on important information while ignoring irrelevant details, which was initially designed for large language models. By incorporating differential attention into CLIP's dual encoder system, DiffCLIP enhances its ability to understand images and text together. The model shows better performance in various tasks like zero-shot classification and retrieval, all while maintaining low computational costs."}, 'zh': {'title': 'DiffCLIP：高效增强多模态表示的创新模型', 'desc': '我们提出了DiffCLIP，这是一种新颖的视觉-语言模型，它将差分注意力机制扩展到CLIP架构中。差分注意力最初是为大型语言模型开发的，旨在放大相关上下文，同时消除噪声信息。在这项工作中，我们将这一机制集成到CLIP的双编码器（图像和文本）框架中。DiffCLIP在图像-文本理解任务上表现优越，且几乎没有额外的计算开销，证明了差分注意力可以显著增强多模态表示而不牺牲效率。'}}}, {'id': 'https://huggingface.co/papers/2503.06273', 'title': 'Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by\n  Learning Language-Agnostic Speech Representations', 'url': 'https://huggingface.co/papers/2503.06273', 'abstract': 'We explore a novel zero-shot Audio-Visual Speech Recognition (AVSR) framework, dubbed Zero-AVSR, which enables speech recognition in target languages without requiring any audio-visual speech data in those languages. Specifically, we introduce the Audio-Visual Speech Romanizer (AV-Romanizer), which learns language-agnostic speech representations by predicting Roman text. Then, by leveraging the strong multilingual modeling capabilities of Large Language Models (LLMs), we propose converting the predicted Roman text into language-specific graphemes, forming the proposed Cascaded <PRE_TAG>Zero-AVSR</POST_TAG>. Taking it a step further, we explore a unified Zero-AVSR approach by directly integrating the audio-visual speech representations encoded by the AV-Romanizer into the LLM. This is achieved through finetuning the adapter and the LLM using our proposed multi-task learning scheme. To capture the wide spectrum of phonetic and linguistic diversity, we also introduce a Multilingual Audio-Visual Romanized Corpus (MARC) consisting of 2,916 hours of audio-visual speech data across 82 languages, along with transcriptions in both language-specific graphemes and Roman text. Extensive analysis and experiments confirm that the proposed Zero-AVSR framework has the potential to expand language support beyond the languages seen during the training of the AV-Romanizer.', 'score': 3, 'issue_id': 2640, 'pub_date': '2025-03-08', 'pub_date_card': {'ru': '8 марта', 'en': 'March 8', 'zh': '3月8日'}, 'hash': '66b650ba2f5b0404', 'authors': ['Jeong Hun Yeo', 'Minsu Kim', 'Chae Won Kim', 'Stavros Petridis', 'Yong Man Ro'], 'affiliations': ['Imperial College London', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.06273.jpg', 'data': {'categories': ['#low_resource', '#audio', '#dataset', '#multilingual', '#machine_translation'], 'emoji': '🗣️', 'ru': {'title': 'Универсальное распознавание речи без языковых барьеров', 'desc': 'Статья представляет новую систему Zero-AVSR для аудиовизуального распознавания речи без использования речевых данных целевого языка. Авторы предлагают Audio-Visual Speech Romanizer для создания языконезависимых представлений речи и используют возможности больших языковых моделей для преобразования романизированного текста в графемы конкретного языка. Исследователи также разрабатывают унифицированный подход Zero-AVSR, интегрируя аудиовизуальные представления речи непосредственно в большую языковую модель. Для обучения и оценки создан многоязычный аудиовизуальный корпус MARC, содержащий 2916 часов речевых данных на 82 языках.'}, 'en': {'title': 'Zero-AVSR: Speech Recognition Without Language-Specific Data', 'desc': 'The paper presents a new framework called Zero-AVSR for Audio-Visual Speech Recognition that can recognize speech in languages without needing any specific audio-visual data for those languages. It introduces the Audio-Visual Speech Romanizer (AV-Romanizer), which creates language-independent speech representations by predicting Roman text. By utilizing Large Language Models (LLMs), the framework converts these predictions into specific language graphemes, enhancing multilingual capabilities. Additionally, a new dataset, the Multilingual Audio-Visual Romanized Corpus (MARC), is introduced to support the training and evaluation of this framework across 82 languages.'}, 'zh': {'title': '零样本音视频语音识别的创新探索', 'desc': '我们提出了一种新颖的零样本音视频语音识别框架，称为Zero-AVSR，能够在没有目标语言音视频语音数据的情况下进行语音识别。该框架引入了音视频语音罗马化器（AV-Romanizer），通过预测罗马文本来学习与语言无关的语音表示。我们利用大型语言模型（LLMs）的强大多语言建模能力，将预测的罗马文本转换为特定语言的字形，形成级联的Zero-AVSR。通过多任务学习方案微调适配器和LLM，我们进一步探索了统一的Zero-AVSR方法，直接将AV-Romanizer编码的音视频语音表示集成到LLM中。'}}}, {'id': 'https://huggingface.co/papers/2503.07598', 'title': 'VACE: All-in-One Video Creation and Editing', 'url': 'https://huggingface.co/papers/2503.07598', 'abstract': 'Diffusion Transformer has demonstrated powerful capability and scalability in generating high-quality images and videos. Further pursuing the unification of generation and editing tasks has yielded significant progress in the domain of image content creation. However, due to the intrinsic demands for consistency across both temporal and spatial dynamics, achieving a unified approach for video synthesis remains challenging. We introduce VACE, which enables users to perform Video tasks within an All-in-one framework for Creation and Editing. These tasks include reference-to-video generation, video-to-video editing, and masked <PRE_TAG>video-to-video editing</POST_TAG>. Specifically, we effectively integrate the requirements of various tasks by organizing video task inputs, such as editing, reference, and masking, into a unified interface referred to as the Video Condition Unit (VCU). Furthermore, by utilizing a Context Adapter structure, we inject different task concepts into the model using formalized representations of temporal and spatial dimensions, allowing it to handle arbitrary video synthesis tasks flexibly. Extensive experiments demonstrate that the unified model of VACE achieves performance on par with task-specific models across various subtasks. Simultaneously, it enables diverse applications through versatile task combinations. Project page: https://ali-vilab.github.io/VACE-Page/.', 'score': 2, 'issue_id': 2644, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '6bf5924fe3265297', 'authors': ['Zeyinzi Jiang', 'Zhen Han', 'Chaojie Mao', 'Jingfeng Zhang', 'Yulin Pan', 'Yu Liu'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.07598.jpg', 'data': {'categories': ['#architecture', '#video', '#multimodal', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'VACE: Универсальный инструмент для создания и редактирования видео', 'desc': 'VACE - это универсальная модель для создания и редактирования видео, объединяющая различные задачи в рамках единого фреймворка. Она использует специальный интерфейс Video Condition Unit для организации входных данных и Context Adapter для внедрения концепций различных задач. VACE позволяет выполнять генерацию видео по референсу, редактирование видео и маскированное редактирование видео. Эксперименты показывают, что модель достигает производительности на уровне специализированных моделей для отдельных подзадач.'}, 'en': {'title': 'VACE: Unifying Video Creation and Editing in One Framework', 'desc': 'The paper introduces VACE, a unified framework for video generation and editing that leverages the capabilities of the Diffusion Transformer. It addresses the challenges of maintaining consistency in both temporal and spatial dynamics during video synthesis. VACE organizes various video tasks, such as reference-to-video generation and video-to-video editing, into a single interface called the Video Condition Unit (VCU). By employing a Context Adapter structure, VACE allows for flexible handling of different video synthesis tasks while achieving performance comparable to specialized models.'}, 'zh': {'title': 'VACE：视频创作与编辑的统一框架', 'desc': '扩散变换器在生成高质量图像和视频方面表现出强大的能力和可扩展性。为了统一生成和编辑任务，研究者们在图像内容创作领域取得了显著进展。尽管如此，由于时间和空间动态的一致性要求，视频合成的统一方法仍然面临挑战。我们提出了VACE，它允许用户在一个综合框架内执行视频任务，包括参考视频生成、视频编辑和掩码视频编辑等。'}}}, {'id': 'https://huggingface.co/papers/2503.07389', 'title': 'TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.07389', 'abstract': "Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability. The code is available at: http://github.com/ddgoodgood/TRCE. CAUTION: This paper includes model-generated content that may contain offensive material.", 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'd88e626045dd077e', 'authors': ['Ruidong Chen', 'Honglin Guo', 'Lanjun Wang', 'Chenyu Zhang', 'Weizhi Nie', 'An-An Liu'], 'affiliations': ['The School of Electrical and Information Engineering, Tianjin University', 'The School of New Media and Communication, Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07389.jpg', 'data': {'categories': ['#benchmark', '#security', '#open_source', '#training', '#cv', '#diffusion', '#hallucinations'], 'emoji': '🔒', 'ru': {'title': 'Безопасная генерация изображений: эффективное удаление нежелательных концепций', 'desc': 'Эта статья представляет новый метод под названием TRCE для удаления нежелательных концепций из моделей генерации изображений по тексту. TRCE использует двухэтапную стратегию: сначала удаляет вредоносную семантику из текстовых промптов, а затем корректирует процесс генерации изображения в безопасном направлении. Метод эффективно удаляет нежелательные концепции, сохраняя при этом общие возможности модели. Авторы провели комплексную оценку TRCE на различных наборах данных, подтверждающую его эффективность.'}, 'en': {'title': 'TRCE: Safeguarding Image Generation with Smart Concept Erasure', 'desc': "This paper presents TRCE, a novel approach to mitigate the risk of generating malicious content in text-to-image diffusion models. It employs a two-stage concept erasure strategy that effectively removes harmful semantics from prompts while maintaining the model's ability to generate normal content. The first stage focuses on optimizing cross-attention layers to transform malicious prompts into safer alternatives. The second stage utilizes contrastive learning to guide the denoising process towards safe outputs, ensuring that the model does not produce NSFW images while preserving its generative capabilities."}, 'zh': {'title': '安全生成，抹除恶意概念的创新方法', 'desc': '最近，文本到图像的扩散模型在生成逼真图像方面取得了进展，但也存在生成恶意内容的风险。为了解决这个问题，研究了概念抹除方法，以帮助模型忘记特定的恶意概念。我们的研究提出了TRCE，采用两阶段的概念抹除策略，在可靠抹除和知识保留之间实现有效的平衡。通过优化交叉注意力层，TRCE能够将恶意提示映射到安全的概念，从而避免生成恶意内容。'}}}, {'id': 'https://huggingface.co/papers/2503.06960', 'title': 'A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning', 'url': 'https://huggingface.co/papers/2503.06960', 'abstract': 'Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single-)object-centric (NOC) data--a limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, a method that induces object-centric representations by introducing a semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at https://github.com/CVMI-Lab/SlotMIM.', 'score': 2, 'issue_id': 2642, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '35668e47701edcd0', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Yilun Chen', 'Jiangmiao Pang', 'Xiaojuan Qi'], 'affiliations': ['Shanghai AI Laboratory', 'The University of Hong Kong', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.06960.jpg', 'data': {'categories': ['#robotics', '#open_source', '#training', '#cv', '#dataset', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'SlotMIM: прорыв в обучении объектно-ориентированным представлениям для робототехники', 'desc': 'Исследование показывает, что предварительно обученные модели компьютерного зрения (PVM) сталкиваются с трудностями при обучении на данных, не сфокусированных на отдельных объектах. Авторы выявили, что ключом к успеху PVM является способность формировать объектно-ориентированные представления из роботизированных наборов данных. На основе этого открытия они разработали метод SlotMIM, который вводит семантическое сужение для уменьшения количества прототипов и поощрения появления объектности. Эксперименты показали, что SlotMIM достигает значительных улучшений в задачах распознавания изображений, понимания сцен и обучения роботов.'}, 'en': {'title': 'Unlocking Object-Centric Learning in Robotics with SlotMIM', 'desc': 'This paper explores the effectiveness of pre-trained vision models (PVMs) in robotics, particularly focusing on their ability to learn object-centric representations. The authors find that models like DINO and iBOT perform better than MAE in various tasks but struggle with non-object-centric data. They introduce SlotMIM, a new method that enhances object-centric learning by using a semantic bottleneck and cross-view consistency regularization. Their experiments show that SlotMIM significantly improves representation learning across different datasets and tasks, demonstrating better data efficiency and scalability.'}, 'zh': {'title': '对象中心表示是成功的关键', 'desc': '预训练视觉模型（PVMs）在现代机器人技术中至关重要，但其最佳配置尚不明确。研究发现，DINO和iBOT在视觉运动控制和感知任务中表现优于MAE，但在非单对象中心（NOC）数据上训练时表现不佳，这与它们学习对象中心表示的能力下降密切相关。我们的研究表明，从非对象中心的机器人数据集中形成对象中心表示的能力是PVMs成功的关键。为此，我们设计了SlotMIM方法，通过引入语义瓶颈来减少原型数量，促进对象性出现，并通过交叉视图一致性正则化来鼓励多视图不变性。'}}}, {'id': 'https://huggingface.co/papers/2503.06362', 'title': 'Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2503.06362', 'abstract': 'Audio-Visual Speech Recognition (AVSR) leverages both audio and visual modalities to enhance speech recognition robustness, particularly in noisy environments. Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in speech recognition, including AVSR. However, due to the significant length of speech representations, direct integration with LLMs imposes substantial computational costs. Prior approaches address this by compressing speech representations before feeding them into LLMs. However, higher compression ratios often lead to performance degradation, necessitating a trade-off between computational efficiency and recognition accuracy. To address this challenge, we propose Llama-MTSK, the first Matryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of the audio-visual token allocation based on specific computational constraints while preserving high performance. Our approach, inspired by Matryoshka Representation Learning, encodes audio-visual representations at multiple granularities within a single model, eliminating the need to train separate models for different compression levels. Moreover, to efficiently fine-tune the LLM, we introduce three LoRA-based Matryoshka strategies using global and scale-specific LoRA modules. Extensive evaluations on the two largest AVSR datasets demonstrate that Llama-MTSK achieves state-of-the-art results, matching or surpassing models trained independently at fixed compression levels.', 'score': 2, 'issue_id': 2638, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'e35aea6b25fbc264', 'authors': ['Umberto Cappellazzo', 'Minsu Kim', 'Stavros Petridis'], 'affiliations': ['Imperial College London', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.06362.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#audio'], 'emoji': '🎭', 'ru': {'title': 'Гибкое AVSR с помощью матрешечной мультимодальной языковой модели', 'desc': 'Эта статья представляет Llama-MTSK - первую мультимодальную языковую модель на основе принципа матрешки для аудио-визуального распознавания речи (AVSR). Модель позволяет гибко адаптировать распределение аудио-визуальных токенов в зависимости от вычислительных ограничений, сохраняя при этом высокую производительность. Подход вдохновлен Matryoshka Representation Learning и кодирует аудио-визуальные представления на нескольких уровнях детализации в рамках одной модели. Авторы также предлагают три LoRA-стратегии для эффективной настройки модели.'}, 'en': {'title': 'Llama-MTSK: Efficient AVSR with Flexible Token Allocation', 'desc': 'This paper introduces Llama-MTSK, a novel Matryoshka-based Multimodal Large Language Model (LLM) designed for Audio-Visual Speech Recognition (AVSR). It addresses the challenge of integrating lengthy speech representations with LLMs by allowing flexible audio-visual token allocation based on computational constraints. The model encodes audio-visual data at various granularities, which helps maintain high performance without the need for separate models for different compression levels. Additionally, the paper presents three LoRA-based strategies for fine-tuning the model, achieving state-of-the-art results on major AVSR datasets.'}, 'zh': {'title': '灵活高效的音视频语音识别解决方案', 'desc': '音视频语音识别（AVSR）结合了音频和视觉信息，以提高在嘈杂环境中的语音识别能力。最近，大型语言模型（LLMs）的进展显示了它们在语音识别中的有效性，包括AVSR。然而，由于语音表示的长度较大，直接与LLMs集成会带来巨大的计算成本。为了解决这一问题，我们提出了Llama-MTSK，这是一种基于套娃的多模态LLM，能够根据特定的计算约束灵活调整音视频标记的分配，同时保持高性能。'}}}, {'id': 'https://huggingface.co/papers/2503.05578', 'title': 'Novel Object 6D Pose Estimation with a Single Reference View', 'url': 'https://huggingface.co/papers/2503.05578', 'abstract': 'Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D.', 'score': 2, 'issue_id': 2640, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '689a25e37e909986', 'authors': ['Jian Liu', 'Wei Sun', 'Kai Zeng', 'Jin Zheng', 'Hui Yang', 'Lin Wang', 'Hossein Rahmani', 'Ajmal Mian'], 'affiliations': ['Central South University', 'Hunan University', 'Lancaster University', 'Nanyang Technological University', 'The University of Western Australia'], 'pdf_title_img': 'assets/pdf/title_img/2503.05578.jpg', 'data': {'categories': ['#3d', '#cv', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Точная оценка 6D-позы объекта по одному эталонному изображению', 'desc': 'Статья представляет метод SinRef-6D для оценки 6D-позы новых объектов на основе одного эталонного изображения. Метод использует итеративное выравнивание точек в системе координат камеры и модели пространства состояний для RGB-данных и облака точек. SinRef-6D способен обрабатывать большие расхождения в позе и извлекать пространственную информацию из одного изображения. После предварительного обучения на синтетических данных, метод может оценивать позу новых объектов без переобучения или CAD-моделей.'}, 'en': {'title': 'Single View, Full Pose: Simplifying 6D Object Estimation', 'desc': 'This paper introduces a new method for estimating the 6D pose of novel objects using only a single reference view, which is more scalable than traditional methods that rely on CAD models or multiple views. The proposed SinRef-6D method utilizes iterative point-wise alignment in the camera coordinate system, effectively addressing challenges posed by large pose discrepancies. By employing state space models (SSMs), the method captures long-range dependencies and spatial information, achieving linear complexity and enhanced spatial modeling. Experimental results show that SinRef-6D performs comparably to existing methods while simplifying the pose estimation process.'}, 'zh': {'title': '单视图下的六维姿态估计新方法', 'desc': '本文提出了一种新的单参考视图的六维姿态估计方法，称为SinRef-6D。该方法通过在相机坐标系中迭代建立点对齐，解决了大姿态差异和空间信息不足的问题。SinRef-6D利用状态空间模型（SSMs）来捕捉长距离依赖关系，并具有线性复杂度和优越的空间建模能力。经过在合成数据上的预训练后，SinRef-6D能够仅使用单一参考视图估计新物体的六维姿态，且无需重新训练或CAD模型。'}}}, {'id': 'https://huggingface.co/papers/2503.07597', 'title': 'HumanMM: Global Human Motion Recovery from Multi-shot Videos', 'url': 'https://huggingface.co/papers/2503.07597', 'abstract': 'In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates.', 'score': 1, 'issue_id': 2640, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '161213a2a054dd4d', 'authors': ['Yuhong Zhang', 'Guanlin Wu', 'Ling-Hao Chen', 'Zhuokai Zhao', 'Jing Lin', 'Xiaoke Jiang', 'Jiamin Wu', 'Zhuoheng Li', 'Hao Frank Yang', 'Haoqian Wang', 'Lei Zhang'], 'affiliations': ['HKU', 'HKUST', 'IDEA Research', 'Johns Hopkins University', 'Tsinghua University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2503.07597.jpg', 'data': {'categories': ['#3d', '#dataset', '#long_context', '#video'], 'emoji': '🏃', 'ru': {'title': 'Реконструкция реалистичного 3D-движения человека из видео с несколькими кадрами', 'desc': 'Статья представляет новую систему для реконструкции длинных последовательностей трехмерного движения человека в мировых координатах из видео с несколькими сменами кадров. Авторы решают проблемы резких переходов между кадрами, частичных окклюзий и динамического фона, интегрируя улучшенную оценку положения камеры с восстановлением движения человека. Система включает в себя детектор смены кадров и модуль надежного выравнивания для обеспечения непрерывности позы и ориентации между кадрами. Использование специального интегратора движения позволяет уменьшить проблему скольжения ног и обеспечить временную согласованность позы человека.'}, 'en': {'title': 'Reconstructing Realistic 3D Human Motion Across Multiple Video Shots', 'desc': 'This paper introduces a new framework for reconstructing long sequences of 3D human motion from videos that have multiple camera angles and transitions. The challenge lies in dealing with issues like sudden changes in shots, parts of the person being blocked, and moving backgrounds. Unlike previous methods that only work with single camera views, this approach combines advanced camera pose estimation with Human Motion Recovery (HMR) and includes a shot transition detector for better accuracy. The results show that the method effectively reduces foot sliding and maintains consistent human poses over time, proving its effectiveness on a specially created multi-shot dataset.'}, 'zh': {'title': '重建真实人类运动的新方法', 'desc': '本文提出了一种新颖的框架，旨在从多镜头切换的野外视频中重建长序列的三维人类运动。由于视频中的突然镜头切换、部分遮挡和动态背景，这种长序列的运动恢复面临很大挑战。我们的方法通过结合增强的相机姿态估计和人类运动恢复（HMR），引入了镜头切换检测器和稳健的对齐模块，以确保姿态和方向在镜头间的连续性。通过自定义的运动整合器，我们有效地减轻了脚滑问题，并确保了人类姿态的时间一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.07426', 'title': 'RePO: ReLU-based Preference Optimization', 'url': 'https://huggingface.co/papers/2503.07426', 'abstract': "Aligning large language models (LLMs) with human preferences is critical for real-world deployment, yet existing methods like RLHF face computational and stability challenges. While DPO establishes an offline paradigm with single hyperparameter beta, subsequent methods like SimPO reintroduce complexity through dual parameters (beta, gamma). We propose {ReLU-based Preference Optimization (RePO)}, a streamlined algorithm that eliminates beta via two advances: (1) retaining SimPO's reference-free margins but removing beta through gradient analysis, and (2) adopting a ReLU-based max-margin loss that naturally filters trivial pairs. Theoretically, RePO is characterized as SimPO's limiting case (beta to infty), where the logistic weighting collapses to binary thresholding, forming a convex envelope of the 0-1 loss. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, requiring only one hyperparameter to tune.", 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '619f3a64896639ee', 'authors': ['Junkang Wu', 'Kexin Huang', 'Xue Wang', 'Jinyang Gao', 'Bolin Ding', 'Jiancan Wu', 'Xiangnan He', 'Xiang Wang'], 'affiliations': ['Alibaba Group, Hangzhou, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.07426.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': '🚀', 'ru': {'title': 'RePO: Упрощенная оптимизация предпочтений для больших языковых моделей', 'desc': 'Статья представляет новый алгоритм обучения с подкреплением для больших языковых моделей под названием RePO. Этот метод упрощает существующие подходы, такие как DPO и SimPO, устраняя необходимость в гиперпараметре beta. RePO использует ReLU-основанную функцию потерь для фильтрации тривиальных пар и оптимизации предпочтений. Эмпирические результаты показывают, что RePO превосходит DPO и SimPO на нескольких базовых моделях при использовании только одного гиперпараметра.'}, 'en': {'title': 'Streamlining Preference Optimization for LLMs with RePO', 'desc': 'This paper addresses the challenge of aligning large language models (LLMs) with human preferences using a new method called ReLU-based Preference Optimization (RePO). RePO simplifies the optimization process by eliminating the need for a complex hyperparameter, beta, while still maintaining effective performance through gradient analysis and a ReLU-based max-margin loss. The authors demonstrate that RePO can be seen as a special case of an existing method, SimPO, when certain conditions are met, leading to a more efficient training process. Empirical results indicate that RePO consistently outperforms previous methods like DPO and SimPO, while only requiring the tuning of a single hyperparameter.'}, 'zh': {'title': '简化偏好优化，提升语言模型对齐', 'desc': '本文提出了一种新的算法，称为基于ReLU的偏好优化（RePO），旨在简化大型语言模型（LLMs）与人类偏好的对齐过程。RePO通过两个创新点消除了超参数beta，首先保留了SimPO的无参考边际，但通过梯度分析去除了beta，其次采用基于ReLU的最大边际损失，自然过滤掉无关的配对。理论上，RePO被描述为SimPO的极限情况，当beta趋向于无穷大时，逻辑加权收敛为二元阈值，形成0-1损失的凸包。实验证明，RePO在多个基础模型上优于DPO和SimPO，仅需调整一个超参数。'}}}, {'id': 'https://huggingface.co/papers/2503.05641', 'title': 'Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for\n  Heterogeneous Reasoning', 'url': 'https://huggingface.co/papers/2503.05641', 'abstract': "Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting experts at the task level is often too coarse-grained, as heterogeneous tasks may require different expertise for each instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, e.g., algebra in math or molecular biology in biomedical reasoning. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator chosen based on its ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's instance-level expert selection improves performance by a large margin but -- when implemented naively -- can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch inference strategy that groups instances based on their assigned experts, loading each model only once. This allows us to integrate 16 expert models on 1 GPU with a time cost comparable to or better than prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that Symbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute average improvement of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation.", 'score': 1, 'issue_id': 2643, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'cd41a7296c5c9225', 'authors': ['Justin Chih-Yao Chen', 'Sukwon Yun', 'Elias Stengel-Eskin', 'Tianlong Chen', 'Mohit Bansal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05641.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#benchmark', '#agents', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Symbolic-MoE: Умное сочетание экспертных LLM для эффективного решения разнообразных задач', 'desc': 'Статья представляет Symbolic-MoE - новый подход к комбинированию предобученных экспертных языковых моделей (LLM) для решения разнообразных задач. Symbolic-MoE использует символьный, текстовый и безградиентный метод выбора экспертов на уровне отдельных примеров, основываясь на необходимых навыках. Предложенная стратегия подбора экспертов динамически выбирает наиболее релевантные модели для различных задач рассуждения, а затем агрегирует их выводы. Авторы демонстрируют, что Symbolic-MoE превосходит сильные базовые модели и мультиагентные подходы на различных бенчмарках, обеспечивая значительное улучшение производительности при эффективном использовании вычислительных ресурсов.'}, 'en': {'title': 'Adaptive Expert Selection for Enhanced Reasoning in LLMs', 'desc': 'The paper introduces Symbolic-MoE, a novel Mixture-of-Experts framework designed to enhance the performance of large language models (LLMs) by enabling instance-level expert selection. This approach allows for the dynamic recruitment of LLMs based on specific skills required for diverse reasoning tasks, such as algebra or molecular biology. By synthesizing outputs from multiple experts, Symbolic-MoE generates high-quality responses while addressing computational efficiency through a batch inference strategy. The results show significant performance improvements over existing models and methods, demonstrating the effectiveness of fine-grained expert selection in machine learning tasks.'}, 'zh': {'title': '实例级专家选择，提升推理性能！', 'desc': '本论文提出了一种名为Symbolic-MoE的框架，旨在通过实例级的专家选择来提高预训练大语言模型（LLM）的性能。该框架采用符号化、基于文本且无梯度的混合专家方法，强调根据任务的不同需求选择合适的专家。通过动态选择最相关的专家，Symbolic-MoE能够在多样化的推理任务中生成高质量的响应。实验结果表明，Symbolic-MoE在多个基准测试中显著超越了现有的强大LLM和多代理方法，且计算效率更高。'}}}, {'id': 'https://huggingface.co/papers/2503.05283', 'title': "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent\n  Spaces", 'url': 'https://huggingface.co/papers/2503.05283', 'abstract': 'Recent works have shown that, when trained at scale, uni-modal 2D vision and text encoders converge to learned features that share remarkable structural properties, despite arising from different representations. However, the role of 3D encoders with respect to other modalities remains unexplored. Furthermore, existing 3D foundation models that leverage large datasets are typically trained with explicit alignment objectives with respect to frozen encoders from other representations. In this work, we investigate the possibility of a posteriori alignment of representations obtained from uni-modal 3D encoders compared to text-based feature spaces. We show that naive post-training feature alignment of uni-modal text and 3D encoders results in limited performance. We then focus on extracting subspaces of the corresponding feature spaces and discover that by projecting learned representations onto well-chosen lower-dimensional <PRE_TAG>subspaces</POST_TAG> the quality of alignment becomes significantly higher, leading to improved accuracy on matching and retrieval tasks. Our analysis further sheds light on the nature of these shared subspaces, which roughly separate between semantic and geometric data representations. Overall, ours is the first work that helps to establish a baseline for post-training alignment of 3D uni-modal and text feature spaces, and helps to highlight both the shared and unique properties of 3D data compared to other representations.', 'score': 1, 'issue_id': 2637, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '85fcbc1ddbe7dbea', 'authors': ['Souhail Hadgi', 'Luca Moschella', 'Andrea Santilli', 'Diego Gomez', 'Qixing Huang', 'Emanuele Rodolà', 'Simone Melzi', 'Maks Ovsjanikov'], 'affiliations': ['Ecole polytechnique', 'Sapienza University of Rome', 'The University of Texas at Austin', 'University of Milano-Bicocca'], 'pdf_title_img': 'assets/pdf/title_img/2503.05283.jpg', 'data': {'categories': ['#alignment', '#3d', '#multimodal', '#transfer_learning'], 'emoji': '🧩', 'ru': {'title': 'Улучшение выравнивания 3D и текстовых признаков через проекцию на подпространства', 'desc': 'Исследование посвящено изучению возможности апостериорного выравнивания представлений, полученных из одномодальных 3D-энкодеров, с текстовыми пространствами признаков. Авторы обнаружили, что простое выравнивание признаков после обучения дает ограниченные результаты. Однако, проецирование выученных представлений на тщательно выбранные подпространства меньшей размерности значительно улучшает качество выравнивания. Анализ также показал, что эти общие подпространства примерно разделяют семантические и геометрические представления данных.'}, 'en': {'title': 'Enhancing 3D and Text Feature Alignment through Subspace Projection', 'desc': 'This paper explores the alignment of features from uni-modal 3D encoders with text-based representations. It reveals that simply aligning these features after training does not yield good results. Instead, the authors find that projecting these features into carefully selected lower-dimensional subspaces significantly improves alignment quality. This work establishes a baseline for understanding how 3D data relates to text features, highlighting both their similarities and differences.'}, 'zh': {'title': '探索3D编码器与文本特征的对齐之路', 'desc': '本研究探讨了3D编码器在与其他模态的关系中的作用，尤其是与文本特征空间的对比。我们发现，简单的后期训练特征对齐方法在单模态文本和3D编码器之间的性能有限。通过提取特征空间的子空间，并将学习到的表示投影到精心选择的低维子空间中，我们显著提高了对齐质量，从而在匹配和检索任务中提升了准确性。我们的工作首次为3D单模态和文本特征空间的后期训练对齐建立了基线，并突出了3D数据与其他表示之间的共享和独特特性。'}}}, {'id': 'https://huggingface.co/papers/2503.02819', 'title': 'Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of\n  Experts', 'url': 'https://huggingface.co/papers/2503.02819', 'abstract': "While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and un<PRE_TAG>conditional scores</POST_TAG> to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional 'corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation. Our code is available at https://github.com/martaskrt/fkc-diffusion.", 'score': 1, 'issue_id': 2650, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '6aa3f8c59c70086b', 'authors': ['Marta Skreta', 'Tara Akhound-Sadegh', 'Viktor Ohanesian', 'Roberto Bondesan', 'Alán Aspuru-Guzik', 'Arnaud Doucet', 'Rob Brekelmans', 'Alexander Tong', 'Kirill Neklyudov'], 'affiliations': ['Google DeepMind', 'Imperial College London', 'McGill University', 'Mila - Quebec AI Institute', 'University of Toronto', 'Université de Montréal', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.02819.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#cv', '#diffusion', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Улучшение контроля генеративных моделей с помощью корректоров Фейнмана-Каца', 'desc': 'Статья представляет новый метод под названием Feynman-Kac Correctors (FKCs) для эффективного и принципиального семплирования из последовательности усредненных или произведенных распределений, полученных из предобученных генеративных моделей на основе оценки. Авторы выводят взвешенную схему симуляции, основанную на формуле Фейнмана-Каца, и предлагают алгоритмы ресемплирования с использованием последовательного метода Монте-Карло. Метод позволяет улучшить качество семплирования и контролировать поведение модели во время вывода. Эффективность FKCs демонстрируется на задачах генерации молекул и создания изображений по текстовому описанию.'}, 'en': {'title': 'Enhancing Score-Based Generative Models with Feynman-Kac Correctors', 'desc': 'This paper introduces a new method for controlling the behavior of score-based generative models during inference. The authors present Feynman-Kac Correctors (FKCs), which provide a principled way to sample from complex distributions derived from pretrained models. By using Sequential Monte Carlo (SMC) resampling algorithms, they enhance the quality of sampling through inference-time scaling. The proposed method shows improvements in various applications, including molecule generation and text-to-image generation, demonstrating its effectiveness in practical scenarios.'}, 'zh': {'title': '高效控制生成模型的推理行为', 'desc': '本论文提出了一种高效且有原则的方法，用于从预训练的基于分数的模型中采样。我们引入了Feynman-Kac校正器（FKC），基于Feynman-Kac公式，精确处理适当的偏微分方程（PDE）中的项。通过使用顺序蒙特卡洛（SMC）重采样算法，我们提高了采样质量，并展示了该方法在多目标分子生成和文本到图像生成中的应用效果。我们的研究为控制推理时的行为提供了新的工具，推动了生成模型的进一步发展。'}}}, {'id': 'https://huggingface.co/papers/2503.07413', 'title': 'REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding', 'url': 'https://huggingface.co/papers/2503.07413', 'abstract': 'Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot capabilities across diverse vision-language tasks after training on mega-scale datasets. However, dense prediction tasks, such as semantic segmentation and keypoint detection, pose significant challenges for MLLMs when represented solely as text outputs. Simultaneously, current MLLMs utilizing latent embeddings for visual task decoding generally demonstrate limited adaptability to both multi-task learning and multi-granularity scenarios. In this work, we present REF-VLM, an end-to-end framework for unified training of various visual decoding tasks. To address complex visual decoding scenarios, we introduce the Triplet-Based Referring Paradigm (TRP), which explicitly decouples three critical dimensions in visual decoding tasks through a triplet structure: concepts, decoding types, and targets. TRP employs symbolic delimiters to enforce structured representation learning, enhancing the parsability and interpretability of model outputs. Additionally, we construct Visual-Task Instruction Following Dataset (VTInstruct), a large-scale multi-task dataset containing over 100 million multimodal dialogue samples across 25 task types. Beyond text inputs and outputs, VT-Instruct incorporates various visual prompts such as point, box, scribble, and mask, and generates outputs composed of text and visual units like box, keypoint, depth and mask. The combination of different visual prompts and visual units generates a wide variety of task types, expanding the applicability of REF-VLM significantly. Both qualitative and quantitative experiments demonstrate that our REF-VLM outperforms other MLLMs across a variety of standard benchmarks. The code, dataset, and demo available at https://github.com/MacavityT/REF-VLM.', 'score': 0, 'issue_id': 2643, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '52ecfef5a3f1d715', 'authors': ['Yan Tai', 'Luhao Zhu', 'Zhiqiang Chen', 'Ynan Ding', 'Yiying Dong', 'Xiaohong Liu', 'Guodong Guo'], 'affiliations': ['Hong Kong Polytechnic University', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07413.jpg', 'data': {'categories': ['#interpretability', '#cv', '#dataset', '#benchmark', '#multimodal', '#games'], 'emoji': '🖼️', 'ru': {'title': 'REF-VLM: Универсальная мультимодальная модель для задач компьютерного зрения', 'desc': 'В этой статье представлена модель REF-VLM, новая мультимодальная языковая модель для решения разнообразных задач компьютерного зрения. Авторы предлагают подход Triplet-Based Referring Paradigm (TRP) для структурированного представления информации в задачах визуального декодирования. Они также создали большой набор данных VTInstruct, содержащий более 100 миллионов мультимодальных диалогов для 25 типов задач. Эксперименты показывают, что REF-VLM превосходит другие мультимодальные языковые модели на различных стандартных тестах.'}, 'en': {'title': 'REF-VLM: Unifying Visual Decoding for Enhanced Multimodal Learning', 'desc': 'This paper introduces REF-VLM, a new framework designed to improve the performance of Multimodal Large Language Models (MLLMs) on complex visual tasks like semantic segmentation and keypoint detection. It addresses the limitations of current MLLMs by using a Triplet-Based Referring Paradigm (TRP) that separates concepts, decoding types, and targets for better structured learning. The authors also present a large-scale dataset, VTInstruct, which includes multimodal dialogue samples and various visual prompts to enhance training. Experimental results show that REF-VLM significantly outperforms existing MLLMs on standard benchmarks, demonstrating its effectiveness in handling diverse visual decoding tasks.'}, 'zh': {'title': 'REF-VLM：统一视觉解码任务的创新框架', 'desc': '多模态大型语言模型（MLLMs）在经过大规模数据集训练后，展现出在多种视觉语言任务中的强大零-shot能力。然而，对于密集预测任务，如语义分割和关键点检测，仅用文本输出表示时，MLLMs面临重大挑战。为了解决这些复杂的视觉解码场景，我们提出了REF-VLM，一个用于统一训练各种视觉解码任务的端到端框架，并引入了基于三元组的引用范式（TRP），以明确解耦视觉解码任务中的概念、解码类型和目标。我们的实验表明，REF-VLM在多项标准基准测试中优于其他MLLMs，展示了其在多任务学习和多粒度场景中的适应性。'}}}, {'id': 'https://huggingface.co/papers/2503.06698', 'title': "What's in a Latent? Leveraging Diffusion Latent Space for Domain\n  Generalization", 'url': 'https://huggingface.co/papers/2503.06698', 'abstract': 'Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training.', 'score': 0, 'issue_id': 2645, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'c706eb4738361e39', 'authors': ['Xavier Thomas', 'Deepti Ghadiyaram'], 'affiliations': ['Boston University', 'Runway'], 'pdf_title_img': 'assets/pdf/title_img/2503.06698.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#dataset', '#training', '#architecture'], 'emoji': '🌐', 'ru': {'title': 'Улучшение обобщения на новые домены через выявление скрытых доменных структур', 'desc': 'Статья посвящена улучшению обобщающей способности моделей машинного обучения на новые распределения данных. Авторы предлагают метод, использующий предобученные пространства признаков для выявления скрытых доменных структур. Они дополняют существующие классификаторы этими представлениями псевдо-доменов, что улучшает работу на незнакомых тестовых доменах. Исследование показывает, что признаки из диффузионных моделей особенно хорошо подходят для разделения доменов без явных доменных меток.'}, 'en': {'title': 'Unlocking Generalization with Pseudo-Domains', 'desc': "This paper focuses on Domain Generalization, which is about creating models that can perform well on new and unseen data. The authors investigate how different model architectures and pre-training methods affect the richness of features used for generalization. They introduce a technique to identify latent structures called pseudo-domains that represent variations in data without needing labels. Their experiments show that using features from diffusion models significantly enhances the model's ability to generalize, achieving better accuracy on unseen domains compared to traditional methods."}, 'zh': {'title': '提升模型的领域泛化能力', 'desc': '领域泛化旨在开发能够对新颖和未见数据分布进行泛化的模型。本文研究了模型架构和预训练目标如何影响特征丰富性，并提出了一种有效利用这些特征进行领域泛化的方法。我们首先在无监督的情况下发现潜在的领域结构，称为伪领域，这些结构捕捉了领域特定的变化。通过增强现有分类器与这些伪领域表示的结合，我们的方法在五个数据集上显示出比标准基线经验风险最小化（ERM）提高了超过4%的测试准确率。'}}}, {'id': 'https://huggingface.co/papers/2503.03511', 'title': 'NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection', 'url': 'https://huggingface.co/papers/2503.03511', 'abstract': 'Robotic grasping in scenes with transparent and specular objects presents great challenges for methods relying on accurate depth information. In this paper, we introduce NeuGrasp, a neural surface reconstruction method that leverages background priors for material-agnostic grasp detection. NeuGrasp integrates transformers and global prior volumes to aggregate multi-view features with spatial encoding, enabling robust surface reconstruction in narrow and sparse viewing conditions. By focusing on foreground objects through residual feature enhancement and refining spatial perception with an occupancy-prior volume, NeuGrasp excels in handling objects with transparent and specular surfaces. Extensive experiments in both simulated and real-world scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping while maintaining comparable reconstruction quality. More details are available at https://neugrasp.github.io/.', 'score': 0, 'issue_id': 2630, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': 'ba660b9e0f676df1', 'authors': ['Qingyu Fan', 'Yinghao Cai', 'Chao Li', 'Wenzhe He', 'Xudong Zheng', 'Tao Lu', 'Bin Liang', 'Shuo Wang'], 'affiliations': ['Qiyuan Lab', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2503.03511.jpg', 'data': {'categories': ['#robotics', '#agents', '#architecture', '#cv'], 'emoji': '🤖', 'ru': {'title': 'NeuGrasp: нейронный захват для сложных поверхностей', 'desc': 'NeuGrasp - это нейронный метод реконструкции поверхности для захвата объектов с прозрачными и зеркальными поверхностями. Он использует трансформеры и глобальные приоры для агрегации мультиракурсных признаков с пространственным кодированием. NeuGrasp фокусируется на объектах переднего плана и уточняет пространственное восприятие с помощью объема приоров заполненности. Эксперименты показывают, что NeuGrasp превосходит современные методы в задаче захвата объектов.'}, 'en': {'title': 'NeuGrasp: Mastering Grasping with Transparent and Specular Objects', 'desc': 'This paper presents NeuGrasp, a novel approach for robotic grasping that effectively deals with transparent and specular objects, which are challenging for traditional depth-based methods. NeuGrasp utilizes a neural surface reconstruction technique that incorporates background priors to enhance grasp detection without being limited by material properties. By combining transformers and global prior volumes, it aggregates multi-view features with spatial encoding, improving surface reconstruction even in difficult viewing conditions. The method demonstrates superior performance in grasping tasks compared to existing techniques, while also achieving high-quality surface reconstruction in both simulated and real-world environments.'}, 'zh': {'title': 'NeuGrasp：透明物体抓取的新突破', 'desc': '本论文介绍了一种名为NeuGrasp的神经表面重建方法，旨在解决透明和镜面物体抓取中的挑战。该方法利用背景先验进行材料无关的抓取检测，结合了变换器和全局先验体积，以聚合多视角特征并进行空间编码。NeuGrasp通过残差特征增强聚焦于前景物体，并利用占用先验体积来改善空间感知，能够有效处理透明和镜面表面的物体。实验结果表明，NeuGrasp在抓取性能上优于现有的最先进方法，同时保持了相似的重建质量。'}}}, {'id': 'https://huggingface.co/papers/2502.20475', 'title': 'Promote, Suppress, Iterate: How Language Models Answer One-to-Many\n  Factual Queries', 'url': 'https://huggingface.co/papers/2502.20475', 'abstract': "To answer one-to-many factual queries (e.g., listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across multiple datasets and models, we identify a promote-then-suppress mechanism: the model first recalls all answers, and then suppresses previously generated ones. Specifically, LMs use both the subject and previous answer tokens to perform knowledge recall, with attention propagating subject information and MLPs promoting the answers. Then, attention attends to and suppresses previous answer tokens, while MLPs amplify the suppression signal. Our mechanism is corroborated by extensive experimental evidence: in addition to using early decoding and causal tracing, we analyze how components use different tokens by introducing both Token Lens, which decodes aggregated attention updates from specified tokens, and a knockout method that analyzes changes in MLP outputs after removing attention to specified tokens. Overall, we provide new insights into how LMs' internal components interact with different input tokens to support complex factual recall. Code is available at https://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.", 'score': 0, 'issue_id': 2645, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'c89164e93db21862', 'authors': ['Tianyi Lorena Yan', 'Robin Jia'], 'affiliations': ['University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2502.20475.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#data', '#interpretability', '#dataset', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие внутренней механики языковых моделей при ответе на сложные фактологические запросы', 'desc': "Статья исследует механизм ответа языковых моделей на фактологические запросы с множественными ответами. Авторы выявили процесс 'продвижения, затем подавления': модель сначала вспоминает все возможные ответы, а затем подавляет уже сгенерированные. Этот механизм реализуется через взаимодействие слоев внимания и MLP, где внимание распространяет информацию о предмете запроса, а MLP усиливают сигналы ответов и их подавления. Исследование подкрепляется экспериментальными данными и новыми методами анализа, такими как Token Lens и метод нокаута."}, 'en': {'title': 'Promote and Suppress: The Key to Factual Recall in Language Models', 'desc': 'This paper explores how language models (LMs) handle one-to-many factual queries by using a promote-then-suppress mechanism. Initially, the model recalls all possible answers based on the subject and previous answers, utilizing attention and multi-layer perceptrons (MLPs) to promote relevant responses. Subsequently, it suppresses any previously generated answers to avoid repetition, ensuring a diverse output. The authors provide experimental evidence and novel analysis methods to demonstrate how LMs manage these internal processes effectively.'}, 'zh': {'title': '语言模型的促进与抑制机制', 'desc': '本文探讨了语言模型在回答一对多事实查询时的内部机制。研究发现，模型采用了一种先促进后抑制的机制，首先回忆所有可能的答案，然后抑制之前生成的答案。具体来说，模型利用主题和之前答案的标记进行知识回忆，通过注意力机制传播主题信息，并通过多层感知机（MLP）促进答案的生成。实验结果表明，该机制有效，提供了对语言模型内部组件如何与不同输入标记交互以支持复杂事实回忆的新见解。'}}}, {'id': 'https://huggingface.co/papers/2502.20730', 'title': 'DeepSolution: Boosting Complex Engineering Solution Design via Tree-based Exploration and Bi-point Thinking', 'url': 'https://huggingface.co/papers/2502.20730', 'abstract': "Designing solutions for complex engineering challenges is crucial in human production activities. However, previous research in the retrieval-augmented generation (RAG) field has not sufficiently addressed tasks related to the design of complex engineering solutions. To fill this gap, we introduce a new benchmark, SolutionBench, to evaluate a system's ability to generate complete and feasible solutions for engineering problems with multiple complex constraints. To further advance the design of complex engineering solutions, we propose a novel system, SolutionRAG, that leverages the tree-based exploration and bi-point thinking mechanism to generate reliable solutions. Extensive experimental results demonstrate that SolutionRAG achieves state-of-the-art (SOTA) performance on the SolutionBench, highlighting its potential to enhance the automation and reliability of complex engineering solution design in real-world applications.", 'score': 20, 'issue_id': 2486, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': 'e9ef168e304ec240', 'authors': ['Zhuoqun Li', 'Haiyang Yu', 'Xuanang Chen', 'Hongyu Lin', 'Yaojie Lu', 'Fei Huang', 'Xianpei Han', 'Yongbin Li', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Tongyi Lab', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.20730.jpg', 'data': {'categories': ['#rag', '#benchmark'], 'emoji': '🔧', 'ru': {'title': 'Умная система для проектирования сложных инженерных решений', 'desc': 'Статья представляет новый бенчмарк SolutionBench для оценки способности систем генерировать решения инженерных задач с множественными ограничениями. Авторы предлагают систему SolutionRAG, использующую древовидное исследование и механизм двухточечного мышления для создания надежных решений. Экспериментальные результаты показывают, что SolutionRAG достигает наилучших показателей на SolutionBench. Это демонстрирует потенциал системы для улучшения автоматизации и надежности проектирования сложных инженерных решений в реальных приложениях.'}, 'en': {'title': 'Revolutionizing Engineering Design with SolutionRAG', 'desc': 'This paper addresses the need for effective solutions in complex engineering design tasks, which have been overlooked in previous research on retrieval-augmented generation (RAG). It introduces a new benchmark called SolutionBench, aimed at evaluating the generation of feasible solutions under multiple constraints. The authors propose a novel system named SolutionRAG, which utilizes tree-based exploration and bi-point thinking to improve solution reliability. Experimental results show that SolutionRAG outperforms existing methods, indicating its potential to automate and enhance the design process in engineering applications.'}, 'zh': {'title': '提升复杂工程设计的自动化与可靠性', 'desc': '本论文提出了一个新的基准测试，称为SolutionBench，用于评估系统在生成复杂工程问题的完整和可行解决方案方面的能力。我们还提出了一种新系统SolutionRAG，利用树形探索和双点思维机制来生成可靠的解决方案。通过大量实验结果，SolutionRAG在SolutionBench上达到了最先进的性能，显示了其在实际应用中提高复杂工程解决方案设计的自动化和可靠性的潜力。此研究填补了以往在检索增强生成（RAG）领域中对复杂工程解决方案设计任务的研究空白。'}}}, {'id': 'https://huggingface.co/papers/2502.18600', 'title': 'Chain of Draft: Thinking Faster by Writing Less', 'url': 'https://huggingface.co/papers/2502.18600', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks.', 'score': 19, 'issue_id': 2491, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '739d903f5735d9eb', 'authors': ['Silei Xu', 'Wenhao Xie', 'Lingxiao Zhao', 'Pengcheng He'], 'affiliations': ['Zoom Communications'], 'pdf_title_img': 'assets/pdf/title_img/2502.18600.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl'], 'emoji': '✍️', 'ru': {'title': 'Эффективное рассуждение: краткость - сестра точности', 'desc': 'Статья представляет новый подход к решению сложных задач с помощью больших языковых моделей - Chain of Draft (CoD). В отличие от метода Chain-of-Thought (CoT), который использует подробные рассуждения, CoD имитирует человеческий подход, генерируя краткие промежуточные мысли. Этот метод позволяет достичь той же или лучшей точности, что и CoT, но при этом использует значительно меньше токенов. CoD демонстрирует эффективность в различных задачах, требующих рассуждений, снижая затраты и задержки.'}, 'en': {'title': 'Streamlining Reasoning: Less is More with Chain of Draft', 'desc': 'This paper introduces Chain of Draft (CoD), a new approach for Large Language Models (LLMs) that mimics human reasoning by generating concise intermediate thoughts. Unlike the traditional Chain-of-Thought (CoT) prompting, which relies on verbose explanations, CoD focuses on delivering essential information in a minimalistic format. The authors demonstrate that CoD can achieve comparable or even superior accuracy to CoT while using significantly fewer tokens, leading to reduced computational costs and faster processing times. This innovative method enhances the efficiency of LLMs in tackling complex reasoning tasks.'}, 'zh': {'title': '草稿链：高效推理的新方法', 'desc': '大型语言模型（LLMs）在解决复杂推理任务方面表现出色，尤其是通过链式思维（CoT）提示，强调逐步推理的详细过程。然而，人类通常采用更高效的策略：草拟简洁的中间思考，只捕捉关键信息。本文提出了一种新范式——草稿链（CoD），灵感来源于人类的认知过程，使LLMs在解决任务时生成简约而信息丰富的中间推理输出。通过减少冗长并专注于关键见解，CoD在准确性上与CoT相匹配或超越，同时仅使用7.6%的标记，显著降低了各种推理任务的成本和延迟。'}}}, {'id': 'https://huggingface.co/papers/2502.20380', 'title': 'Multi-Turn Code Generation Through Single-Step Rewards', 'url': 'https://huggingface.co/papers/2502.20380', 'abstract': 'We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, muCode, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. muCode iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of muCode at utilizing the execution feedback. Our code is available at https://github.com/portal-cornell/muCode.', 'score': 17, 'issue_id': 2500, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'cceb0299fb5077b5', 'authors': ['Arnav Kumar Jain', 'Gonzalo Gonzalez-Pumariega', 'Wayne Chen', 'Alexander M Rush', 'Wenting Zhao', 'Sanjiban Choudhury'], 'affiliations': ['Cornell University', 'MilaQuebec AI Institute', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2502.20380.jpg', 'data': {'categories': ['#rl', '#rlhf', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективная генерация кода с многоэтапной обратной связью', 'desc': 'Статья представляет новый подход к генерации кода с использованием многоэтапной обратной связи по выполнению, названный muCode. В отличие от существующих методов, использующих сложное иерархическое обучение с подкреплением, muCode применяет простой масштабируемый подход с одношаговыми вознаграждениями. Метод итеративно обучает генератор для создания кодовых решений и верификатор для оценки нового кода. Экспериментальные результаты показывают значительное улучшение по сравнению с современными базовыми методами.'}, 'en': {'title': 'Simplifying Code Generation with muCode: One-Step Recovery from Feedback', 'desc': 'This paper presents muCode, a novel approach for generating code based on multi-turn execution feedback. Unlike existing methods that rely on complex reinforcement learning techniques, muCode simplifies the process by using single-step rewards. The authors argue that code generation can be treated as a one-step recoverable Markov Decision Process (MDP), allowing for the recovery of correct code from any intermediate state. Through iterative training of a code generator and a verifier, muCode demonstrates significant performance improvements over current state-of-the-art methods.'}, 'zh': {'title': '简单高效的多轮代码生成方法', 'desc': '本文解决了从多轮执行反馈中生成代码的问题。现有方法要么在没有反馈的情况下生成代码，要么使用复杂的层次强化学习来优化多轮奖励。我们提出了一种简单而可扩展的方法muCode，仅使用单步奖励来解决多轮代码生成。实验结果表明，我们的方法在性能上显著优于现有的最先进基线。'}}}, {'id': 'https://huggingface.co/papers/2502.21318', 'title': 'How far can we go with ImageNet for Text-to-Image generation?', 'url': 'https://huggingface.co/papers/2502.21318', 'abstract': "Recent text-to-image (T2I) generation models have achieved remarkable results by training on billion-scale datasets, following a `bigger is better' paradigm that prioritizes data quantity over quality. We challenge this established paradigm by demonstrating that strategic data augmentation of small, well-curated datasets can match or outperform models trained on massive web-scraped collections. Using only ImageNet enhanced with well-designed text and image augmentations, we achieve a +2 overall score over SD-XL on GenEval and +5 on DPGBench while using just 1/10th the parameters and 1/1000th the training images. Our results suggest that strategic data augmentation, rather than massive datasets, could offer a more sustainable path forward for T2I generation.", 'score': 13, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '73a20b698d827c0d', 'authors': ['L. Degeorge', 'A. Ghosh', 'N. Dufour', 'D. Picard', 'V. Kalogeiton'], 'affiliations': ['AMIAD, Pole recherche', 'LIGM, Ecole Nationale des Ponts et Chaussees, IP Paris, Univ Gustave Eiffel, CNRS, France', 'LIX, Ecole Polytechnique, CNRS, IP Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2502.21318.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#data', '#cv', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Качество данных побеждает количество в генерации изображений', 'desc': 'Исследователи предлагают новый подход к обучению моделей генерации изображений по тексту, основанный на стратегическом аугментировании небольших, но качественных наборов данных. Используя только ImageNet с тщательно разработанными текстовыми и визуальными аугментациями, им удалось превзойти модели, обученные на огромных веб-наборах данных. Их модель превосходит SD-XL на 2 балла в GenEval и на 5 баллов в DPGBench, используя всего 1/10 параметров и 1/1000 обучающих изображений. Результаты указывают на то, что стратегическое аугментирование данных может быть более устойчивым путем развития генеративных моделей, чем использование массивных датасетов.'}, 'en': {'title': 'Quality Over Quantity: Augmenting Small Datasets for Better T2I Models', 'desc': 'This paper challenges the common belief that larger datasets always lead to better text-to-image (T2I) generation models. It shows that by using strategic data augmentation on smaller, high-quality datasets, we can achieve results that are as good as or better than those from models trained on much larger datasets. Specifically, the authors enhanced ImageNet with carefully designed text and image augmentations, leading to significant performance improvements. Their findings suggest that focusing on data quality and augmentation may be a more efficient and sustainable approach for developing T2I models.'}, 'zh': {'title': '战略数据增强：小数据集的力量', 'desc': '最近的文本到图像生成模型通过在大规模数据集上训练取得了显著成果，遵循了“越大越好”的范式，优先考虑数据的数量而非质量。我们挑战了这一既定范式，展示了通过对小型、精心策划的数据集进行战略性数据增强，可以与在大规模网络抓取集合上训练的模型相匹敌或超越。仅使用经过精心设计的文本和图像增强的ImageNet，我们在GenEval上比SD-XL高出2分，在DPGBench上高出5分，同时只使用了1/10的参数和1/1000的训练图像。我们的结果表明，战略性数据增强而非大规模数据集，可能为文本到图像生成提供更可持续的发展路径。'}}}, {'id': 'https://huggingface.co/papers/2502.18017', 'title': 'ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents', 'url': 'https://huggingface.co/papers/2502.18017', 'abstract': "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model's reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.", 'score': 9, 'issue_id': 2487, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '4202273d8c895c2a', 'authors': ['Qiuchen Wang', 'Ruixue Ding', 'Zehui Chen', 'Weiqi Wu', 'Shihang Wang', 'Pengjun Xie', 'Feng Zhao'], 'affiliations': ['MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, USTC', 'Shanghai Jiao Tong University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2502.18017.jpg', 'data': {'categories': ['#reasoning', '#agents', '#rag', '#games', '#benchmark', '#multimodal', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'ViDoRAG: Новый подход к извлечению информации из визуальных документов', 'desc': 'Статья представляет новый набор данных ViDoSeek для оценки эффективности методов извлечения информации с добавлением генерации (RAG) на визуально насыщенных документах. Авторы выявляют ограничения существующих подходов RAG в обработке мультимодальной информации и сложных рассуждений. Для решения этих проблем предлагается новая мультиагентная система ViDoRAG, использующая гибридную стратегию на основе гауссовых смесей для мультимодального поиска. ViDoRAG демонстрирует улучшение производительности более чем на 10% по сравнению с существующими методами на бенчмарке ViDoSeek.'}, 'en': {'title': 'Enhancing RAG for Complex Visual Reasoning with ViDoRAG', 'desc': 'This paper addresses the challenges of understanding complex information in visually rich documents using Retrieval-Augmented Generation (RAG) methods. It introduces ViDoSeek, a new dataset that tests RAG performance on documents that require advanced reasoning skills. The authors highlight limitations in current RAG techniques, such as difficulties in integrating visual and textual data and inadequate reasoning capabilities. To overcome these issues, they propose ViDoRAG, a multi-agent framework that enhances retrieval and reasoning through a hybrid strategy and an iterative workflow, demonstrating significant improvements in performance on the ViDoSeek benchmark.'}, 'zh': {'title': '提升视觉文档理解的RAG新框架', 'desc': '理解视觉丰富文档中的信息对传统的增强检索生成（RAG）方法来说仍然是一个重大挑战。现有的基准主要集中在基于图像的问题回答（QA），而忽视了在密集视觉文档中高效检索、理解和推理的基本挑战。为了解决这些问题，我们引入了ViDoSeek，这是一个新颖的数据集，旨在评估RAG在需要复杂推理的视觉丰富文档上的表现。我们提出的ViDoRAG框架采用混合策略，结合多模态检索和迭代代理工作流，以提高模型的推理能力，并在ViDoSeek基准上显著超越现有方法。'}}}, {'id': 'https://huggingface.co/papers/2502.20545', 'title': 'SoS1: O1 and R1-Like Reasoning LLMs are Sum-of-Square Solvers', 'url': 'https://huggingface.co/papers/2502.20545', 'abstract': "Large Language Models (LLMs) have achieved human-level proficiency across diverse tasks, but their ability to perform rigorous mathematical problem solving remains an open challenge. In this work, we investigate a fundamental yet computationally intractable problem: determining whether a given multivariate polynomial is nonnegative. This problem, closely related to Hilbert's Seventeenth Problem, plays a crucial role in global polynomial optimization and has applications in various fields. First, we introduce SoS-1K, a meticulously curated dataset of approximately 1,000 polynomials, along with expert-designed reasoning instructions based on five progressively challenging criteria. Evaluating multiple state-of-the-art LLMs, we find that without structured guidance, all models perform only slightly above the random guess baseline 50%. However, high-quality reasoning instructions significantly improve accuracy, boosting performance up to 81%. Furthermore, our 7B model, SoS-7B, fine-tuned on SoS-1K for just 4 hours, outperforms the 671B DeepSeek-V3 and GPT-4o-mini in accuracy while only requiring 1.8% and 5% of the computation time needed for letters, respectively. Our findings highlight the potential of LLMs to push the boundaries of mathematical reasoning and tackle NP-hard problems.", 'score': 9, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'fb32f9423103ece9', 'authors': ['Kechen Li', 'Wenqi Zhu', 'Coralia Cartis', 'Tianbo Ji', 'Shiwei Liu'], 'affiliations': ['Mathematical Institute, University of Oxford', 'Nanjing University of Aeronautics and Astronautics', 'School of Transportation and Civil Engineering, Nantong University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20545.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#math', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Большие языковые модели преодолевают границы математического мышления', 'desc': 'В этой статье исследуется способность больших языковых моделей (LLM) решать сложные математические задачи, в частности, определение неотрицательности многомерных полиномов. Авторы создали датасет SoS-1K из 1000 полиномов и разработали инструкции по рассуждению для моделей. Эксперименты показали, что без структурированного руководства модели работают немного лучше случайного угадывания, но с качественными инструкциями точность повышается до 81%. Модель SoS-7B, дообученная на SoS-1K, превзошла более крупные модели по точности и скорости вычислений.'}, 'en': {'title': 'Unlocking Mathematical Reasoning in LLMs with Structured Guidance', 'desc': 'This paper explores the limitations of Large Language Models (LLMs) in solving complex mathematical problems, specifically the challenge of determining if a multivariate polynomial is nonnegative. The authors introduce a new dataset called SoS-1K, which contains around 1,000 polynomials and structured reasoning instructions to guide the models. They demonstrate that LLMs perform poorly without guidance, achieving only slightly above random guessing, but can significantly improve their accuracy with high-quality instructions. Notably, their fine-tuned model, SoS-7B, surpasses larger models in performance while being more computationally efficient, showcasing the potential of LLMs in addressing NP-hard problems.'}, 'zh': {'title': '推动数学推理的边界', 'desc': '大型语言模型（LLMs）在多种任务中达到了人类水平的能力，但在严格的数学问题解决方面仍然面临挑战。本文研究了一个基本但计算上难以处理的问题：判断给定的多变量多项式是否非负。我们引入了SoS-1K数据集，包含约1000个多项式，并设计了基于五个逐步挑战标准的推理指导。实验表明，经过高质量的推理指导后，模型的准确率显著提高，最高可达81%。'}}}, {'id': 'https://huggingface.co/papers/2502.20396', 'title': 'Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids', 'url': 'https://huggingface.co/papers/2502.20396', 'abstract': 'Reinforcement learning has delivered promising results in achieving human- or even superhuman-level capabilities across diverse problem domains, but success in dexterous robot manipulation remains limited. This work investigates the key challenges in applying reinforcement learning to solve a collection of contact-rich manipulation tasks on a humanoid embodiment. We introduce novel techniques to overcome the identified challenges with empirical validation. Our main contributions include an automated real-to-sim tuning module that brings the simulated environment closer to the real world, a generalized reward design scheme that simplifies reward engineering for long-horizon contact-rich manipulation tasks, a divide-and-conquer distillation process that improves the sample efficiency of hard-exploration problems while maintaining sim-to-real performance, and a mixture of sparse and dense object representations to bridge the sim-to-real perception gap. We show promising results on three humanoid dexterous manipulation tasks, with ablation studies on each technique. Our work presents a successful approach to learning humanoid dexterous manipulation using sim-to-real reinforcement learning, achieving robust generalization and high performance without the need for human demonstration.', 'score': 7, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '41439b4f54e02c9b', 'authors': ['Toru Lin', 'Kartik Sachdev', 'Linxi Fan', 'Jitendra Malik', 'Yuke Zhu'], 'affiliations': ['NVIDIA', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.20396.jpg', 'data': {'categories': ['#robotics', '#rl', '#games', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Ловкость робота: от симуляции к реальности', 'desc': 'Статья описывает применение обучения с подкреплением для решения задач манипуляции объектами с помощью человекоподобного робота. Авторы представляют новые методы для преодоления ключевых проблем, включая автоматическую настройку симуляции, обобщенную схему проектирования наград и смешанное представление объектов. Исследование демонстрирует успешные результаты на трех задачах ловкой манипуляции, достигая надежной генерализации и высокой производительности. Работа предлагает эффективный подход к обучению человекоподобных роботов сложным манипуляциям без необходимости в демонстрациях человека.'}, 'en': {'title': 'Reinforcement Learning for Dexterous Robot Manipulation: Bridging Sim and Real Worlds', 'desc': 'This paper addresses the challenges of using reinforcement learning for complex robot manipulation tasks that involve physical contact. The authors propose innovative methods to enhance the performance of humanoid robots in these tasks, including a system to align simulated environments with real-world conditions. They also introduce a new reward design that simplifies the process of creating effective rewards for long tasks and a technique to improve learning efficiency in difficult scenarios. The results demonstrate that their approach allows robots to learn dexterous manipulation effectively, achieving high performance without requiring human guidance.'}, 'zh': {'title': '突破类人机器人灵巧操作的强化学习挑战', 'desc': '本研究探讨了在类人机器人灵巧操作任务中应用强化学习的关键挑战。我们提出了新技术来克服这些挑战，包括自动化的真实到模拟调优模块，以缩小模拟环境与现实世界的差距。我们还设计了一种通用的奖励机制，简化了长时间接触丰富的操作任务的奖励工程。通过对三项类人灵巧操作任务的实验，我们展示了在不需要人类示范的情况下，使用模拟到真实的强化学习实现了稳健的泛化和高性能。'}}}, {'id': 'https://huggingface.co/papers/2502.19577', 'title': 'Tell me why: Visual foundation models as self-explainable classifiers', 'url': 'https://huggingface.co/papers/2502.19577', 'abstract': 'Visual foundation models (VFMs) have become increasingly popular due to their state-of-the-art performance. However, interpretability remains crucial for critical applications. In this sense, self-explainable models (SEM) aim to provide interpretable classifiers that decompose predictions into a weighted sum of interpretable concepts. Despite their promise, recent studies have shown that these explanations often lack faithfulness. In this work, we combine VFMs with a novel prototypical architecture and specialized training objectives. By training only a lightweight head (approximately 1M parameters) on top of frozen VFMs, our approach (ProtoFM) offers an efficient and interpretable solution. Evaluations demonstrate that our approach achieves competitive classification performance while outperforming existing models across a range of interpretability metrics derived from the literature. Code is available at https://github.com/hturbe/proto-fm.', 'score': 6, 'issue_id': 2493, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '7d2bd5235959eba5', 'authors': ['Hugues Turbé', 'Mina Bjelogrlic', 'Gianmarco Mengaldo', 'Christian Lovis'], 'affiliations': ['Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore, Singapore', 'Department of Radiology and Medical Informatics, University of Geneva, Geneva, Switzerland', 'Division of Medical Information Sciences, Geneva University Hospitals, Geneva, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2502.19577.jpg', 'data': {'categories': ['#cv', '#small_models', '#architecture', '#interpretability', '#open_source', '#training', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'ProtoFM: Интерпретируемость визуальных моделей без потери точности', 'desc': 'Статья описывает новый подход к интерпретируемости визуальных моделей-основ (VFM). Авторы предлагают комбинацию VFM с прототипической архитектурой и специализированными целями обучения, названную ProtoFM. Этот метод обучает только легковесную верхушку поверх замороженных VFM, что обеспечивает эффективное и интерпретируемое решение. Эксперименты показывают, что ProtoFM достигает конкурентоспособной точности классификации, превосходя существующие модели по ряду метрик интерпретируемости.'}, 'en': {'title': 'ProtoFM: Interpretable and Efficient Visual Foundation Models', 'desc': 'This paper introduces ProtoFM, a new approach that combines visual foundation models (VFMs) with a prototypical architecture to enhance interpretability in machine learning. The method focuses on creating self-explainable models (SEMs) that break down predictions into understandable components, addressing the issue of faithfulness in explanations. By training a lightweight head on top of frozen VFMs, ProtoFM maintains high classification performance while improving interpretability metrics. The results show that ProtoFM outperforms existing models, making it a promising solution for applications requiring both accuracy and clarity.'}, 'zh': {'title': '高效可解释的视觉基础模型', 'desc': '视觉基础模型（VFM）因其卓越的性能而受到广泛关注，但在关键应用中可解释性仍然至关重要。自解释模型（SEM）旨在提供可解释的分类器，将预测分解为可解释概念的加权和。尽管有潜力，近期研究表明这些解释往往缺乏可信度。我们提出了一种结合VFM和新型原型架构的方案（ProtoFM），通过在冻结的VFM上训练一个轻量级的头部模型，提供了一种高效且可解释的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2502.20969', 'title': 'TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval', 'url': 'https://huggingface.co/papers/2502.20969', 'abstract': 'Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, leading to system challenges in latency-sensitive deployments, especially when limited GPU memory is available. To address these challenges, we propose TeleRAG, an efficient inference system that reduces RAG latency with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that anticipates required data and transfers it from CPU to GPU in parallel with LLM generation. By leveraging the modularity of RAG pipelines, the inverted file index (IVF) search algorithm and similarities between queries, TeleRAG optimally overlaps data movement and computation. Experimental results show that TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average compared to state-of-the-art systems, enabling faster, more memory-efficient deployments of advanced RAG applications.', 'score': 5, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '5a85f59eed1c3c3b', 'authors': ['Chien-Yu Lin', 'Keisuke Kamahori', 'Yiyu Liu', 'Xiaoxiang Shi', 'Madhav Kashyap', 'Yile Gu', 'Rulin Shao', 'Zihao Ye', 'Kan Zhu', 'Stephanie Wang', 'Arvind Krishnamurthy', 'Rohan Kadekodi', 'Luis Ceze', 'Baris Kasikci'], 'affiliations': ['Shanghai Jiao Tong University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20969.jpg', 'data': {'categories': ['#rag', '#inference', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'TeleRAG: Ускорение RAG без компромиссов по памяти', 'desc': 'TeleRAG - это эффективная система вывода, которая снижает задержку RAG при минимальных требованиях к памяти GPU. Основное нововведение TeleRAG - это опережающее извлечение, механизм предварительной выборки, который предвидит необходимые данные и передает их с CPU на GPU параллельно с генерацией LLM. Система использует модульность RAG-конвейеров, алгоритм поиска инвертированного файлового индекса (IVF) и сходства между запросами для оптимального совмещения перемещения данных и вычислений. Экспериментальные результаты показывают, что TeleRAG снижает задержку вывода RAG в среднем до 1,72 раза по сравнению с современными системами.'}, 'en': {'title': 'Speeding Up RAG with TeleRAG: Faster and Smarter Inference!', 'desc': 'This paper introduces TeleRAG, a new system designed to improve the efficiency of retrieval-augmented generation (RAG) in large language models (LLMs). TeleRAG addresses the latency issues that arise from using large datastores, particularly in environments with limited GPU memory. The key feature of TeleRAG is its lookahead retrieval mechanism, which allows data to be pre-fetched from the CPU to the GPU while the LLM is generating responses. Experimental results demonstrate that TeleRAG can significantly reduce inference latency, making RAG applications faster and more efficient.'}, 'zh': {'title': 'TeleRAG：高效的RAG推理系统', 'desc': '检索增强生成（RAG）通过外部数据源扩展大型语言模型（LLM），以提高事实准确性和领域覆盖率。然而，现代RAG管道依赖于大型数据存储，导致在延迟敏感的部署中面临系统挑战，尤其是在GPU内存有限的情况下。为了解决这些问题，我们提出了TeleRAG，这是一种高效的推理系统，能够在最小的GPU内存需求下减少RAG延迟。TeleRAG的核心创新是前瞻性检索，这是一种预取机制，可以在LLM生成的同时，预测所需数据并将其从CPU并行传输到GPU。'}}}, {'id': 'https://huggingface.co/papers/2502.17941', 'title': 'Optimal Brain Apoptosis', 'url': 'https://huggingface.co/papers/2502.17941', 'abstract': 'The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.', 'score': 5, 'issue_id': 2495, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '3748780b9de1393e', 'authors': ['Mingyuan Sun', 'Zheng Fang', 'Jiaxu Wang', 'Junjie Jiang', 'Delei Kong', 'Chenming Hu', 'Yuetong Fang', 'Renjing Xu'], 'affiliations': ['Hunan University', 'Northeastern University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.17941.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'Точная обрезка нейронных сетей с помощью прямого расчета Гессиана', 'desc': 'Статья представляет новый метод прунинга нейронных сетей под названием Optimal Brain Apoptosis (OBA). OBA использует прямой расчет произведения Гессиана на вектор для каждого параметра, что позволяет более точно оценивать важность параметров. Метод эффективно применяется к сверточным нейронным сетям и трансформерам, разлагая матрицу Гессиана по слоям сети. Эксперименты на различных архитектурах и датасетах подтверждают эффективность предложенного подхода.'}, 'en': {'title': 'Efficient Neural Network Pruning with Optimal Brain Apoptosis', 'desc': 'This paper addresses the challenges of high computational demands in Convolutional Neural Networks (CNNs) and Transformers by introducing a new pruning method called Optimal Brain Apoptosis (OBA). OBA improves upon previous methods by directly calculating the Hessian-vector product for each parameter, allowing for more accurate estimation of parameter importance. The authors decompose the Hessian matrix across layers to identify non-zero conditions, which enhances the efficiency of the pruning process. Experimental results demonstrate the effectiveness of OBA on various architectures and datasets, confirming its potential to optimize neural networks without significant performance loss.'}, 'zh': {'title': '高效剪枝：最优脑凋亡方法', 'desc': '随着卷积神经网络（CNN）和变换器（Transformers）模型的复杂性和参数数量的增加，计算效率和资源需求面临挑战。剪枝被认为是一种有效的策略，通过去除冗余元素（如神经元、通道或连接）来提高计算效率，而不会严重影响性能。本文在最优脑损伤（OBD）的基础上，提出了一种新的剪枝方法——最优脑凋亡（OBA），通过直接计算每个参数的Hessian-向量乘积值来估计参数的重要性。我们的方法在多个数据集上进行了验证，包括VGG19、ResNet32、ResNet50和ViT-B/16，展示了在CNN和Transformers中的高效剪枝过程。'}}}, {'id': 'https://huggingface.co/papers/2502.19731', 'title': "Preference Learning Unlocks LLMs' Psycho-Counseling Skills", 'url': 'https://huggingface.co/papers/2502.19731', 'abstract': "Applying large language models (LLMs) to assist in psycho-counseling is an emerging and meaningful approach, driven by the significant gap between patient needs and the availability of mental health support. However, current LLMs struggle to consistently provide effective responses to client speeches, largely due to the lack of supervision from high-quality real psycho-counseling data, whose content is typically inaccessible due to client privacy concerns. Furthermore, the quality of therapists' responses in available sessions can vary significantly based on their professional training and experience. Assessing the quality of therapists' responses remains an open challenge. In this work, we address these challenges by first proposing a set of professional and comprehensive principles to evaluate therapists' responses to client speeches. Using these principles, we create a preference dataset, PsychoCounsel-Preference, which contains 36k high-quality preference comparison pairs. This dataset aligns with the preferences of professional psychotherapists, providing a robust foundation for evaluating and improving LLMs in psycho-counseling. Experiments on reward modeling and preference learning demonstrate that PsychoCounsel-Preference is an excellent resource for LLMs to acquire essential skills for responding to clients in a counseling session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference, PsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to facilitate the research of psycho-counseling with LLMs at: https://hf.co/Psychotherapy-LLM.", 'score': 4, 'issue_id': 2500, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '533e7e87242a9b9e', 'authors': ['Mian Zhang', 'Shaun M. Eack', 'Zhiyu Zoey Chen'], 'affiliations': ['Department of Computer Science, University of Texas at Dallas', 'School of Social Work, University of Pittsburgh'], 'pdf_title_img': 'assets/pdf/title_img/2502.19731.jpg', 'data': {'categories': ['#open_source', '#alignment', '#healthcare', '#data', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Большие языковые модели на страже психического здоровья', 'desc': 'Статья представляет новый подход к применению больших языковых моделей (LLM) в психологическом консультировании. Авторы разработали набор принципов для оценки ответов терапевтов и создали датасет PsychoCounsel-Preference, содержащий 36 тысяч пар сравнений высокого качества. На основе этих данных была обучена модель PsychoCounsel-Llama3-8B, которая показала впечатляющие результаты в сравнении с GPT-4. Исследование направлено на преодоление разрыва между потребностями пациентов и доступностью психологической помощи.'}, 'en': {'title': 'Enhancing Psycho-Counseling with LLMs: A New Standard for Therapist Response Evaluation', 'desc': "This paper explores the use of large language models (LLMs) in psycho-counseling, addressing the gap between patient needs and available mental health support. It highlights the challenges faced by LLMs in generating effective responses due to a lack of high-quality training data and variability in therapist responses. To tackle these issues, the authors propose evaluation principles for therapist responses and create a dataset called PsychoCounsel-Preference, which includes 36,000 preference comparison pairs aligned with professional psychotherapists' judgments. The results show that their model, PsychoCounsel-Llama3-8B, significantly outperforms existing models, achieving an 87% win rate against GPT-4o, thus demonstrating the potential of LLMs in enhancing psycho-counseling practices."}, 'zh': {'title': '提升心理咨询的语言模型应用', 'desc': '本研究探讨了大型语言模型（LLMs）在心理咨询中的应用，旨在填补患者需求与心理健康支持之间的差距。由于缺乏高质量的真实心理咨询数据，当前的LLMs在回应客户发言时效果不佳。我们提出了一套专业的评估原则，并创建了包含36,000个高质量偏好比较对的PsychoCounsel-Preference数据集，以帮助评估和改进LLMs在心理咨询中的表现。实验结果表明，PsychoCounsel-Preference为LLMs提供了必要的技能基础，使其在咨询会话中更有效地回应客户。'}}}, {'id': 'https://huggingface.co/papers/2502.20583', 'title': 'LiteASR: Efficient Automatic Speech Recognition with Low-Rank Approximation', 'url': 'https://huggingface.co/papers/2502.20583', 'abstract': "Modern automatic speech recognition (ASR) models, such as OpenAI's Whisper, rely on deep encoder-decoder architectures, and their encoders are a critical bottleneck for efficient deployment due to high computational intensity. We introduce LiteASR, a low-rank compression scheme for ASR encoders that significantly reduces inference costs while maintaining transcription accuracy. Our approach leverages the strong low-rank properties observed in intermediate activations: by applying principal component analysis (PCA) with a small calibration dataset, we approximate linear transformations with a chain of low-rank matrix multiplications, and further optimize self-attention to work in the reduced dimension. Evaluation results show that our method can compress Whisper large-v3's encoder size by over 50%, matching Whisper medium's size with better transcription accuracy, thereby establishing a new Pareto-optimal frontier of efficiency and performance. The code of LiteASR is available at https://github.com/efeslab/LiteASR.", 'score': 4, 'issue_id': 2486, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '3c7268c0881fa426', 'authors': ['Keisuke Kamahori', 'Jungo Kasai', 'Noriyuki Kojima', 'Baris Kasikci'], 'affiliations': ['Kotoba Technologies Inc.', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20583.jpg', 'data': {'categories': ['#inference', '#optimization', '#audio'], 'emoji': '🎙️', 'ru': {'title': 'LiteASR: Эффективное сжатие энкодеров ASR без потери точности', 'desc': 'LiteASR - это метод сжатия энкодеров для систем автоматического распознавания речи (ASR), который значительно уменьшает вычислительные затраты при сохранении точности транскрипции. Метод использует анализ главных компонент (PCA) для аппроксимации линейных преобразований цепочкой умножений матриц низкого ранга. LiteASR позволяет сжать размер энкодера модели Whisper large-v3 более чем на 50%, достигая размера Whisper medium с лучшей точностью транскрипции. Это устанавливает новую Парето-оптимальную границу эффективности и производительности для моделей ASR.'}, 'en': {'title': 'LiteASR: Efficient ASR with Low-Rank Compression', 'desc': "This paper presents LiteASR, a novel low-rank compression technique designed to enhance the efficiency of automatic speech recognition (ASR) models, particularly focusing on the encoder component. By utilizing principal component analysis (PCA) on intermediate activations, LiteASR reduces the computational load during inference while preserving transcription accuracy. The method achieves over 50% reduction in the encoder size of OpenAI's Whisper large-v3 model, aligning its performance with that of the medium version but with improved accuracy. This work sets a new standard for balancing efficiency and performance in ASR systems."}, 'zh': {'title': 'LiteASR：高效的低秩压缩方案', 'desc': '现代自动语音识别（ASR）模型，如OpenAI的Whisper，依赖于深度编码器-解码器架构，而编码器的计算强度是高效部署的瓶颈。我们提出了LiteASR，这是一种针对ASR编码器的低秩压缩方案，能够显著降低推理成本，同时保持转录准确性。我们的方法利用了中间激活中的强低秩特性，通过使用小型校准数据集的主成分分析（PCA），用低秩矩阵乘法链来近似线性变换，并进一步优化自注意力机制以适应降维后的数据。评估结果表明，我们的方法可以将Whisper large-v3的编码器大小压缩超过50%，并在转录准确性上优于Whisper medium，从而建立了效率与性能的新帕累托最优边界。'}}}, {'id': 'https://huggingface.co/papers/2502.20900', 'title': 'DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping', 'url': 'https://huggingface.co/papers/2502.20900', 'abstract': "Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on specific assumptions, such as single-object settings or limited environments, leading to constrained generalization. Our solution is DexGraspVLA, a hierarchical framework that utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller. The key insight lies in iteratively transforming diverse language and visual inputs into domain-invariant representations, where imitation learning can be effectively applied due to the alleviation of domain shift. Thus, it enables robust generalization across a wide range of real-world scenarios. Notably, our method achieves a 90+% success rate under thousands of unseen object, lighting, and background combinations in a ``zero-shot'' environment. Empirical analysis further confirms the consistency of internal model behavior across environmental variations, thereby validating our design and explaining its generalization performance. We hope our work can be a step forward in achieving general dexterous grasping. Our demo and code can be found at https://dexgraspvla.github.io/.", 'score': 3, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '5d9b331844235882', 'authors': ['Yifan Zhong', 'Xuchuan Huang', 'Ruochong Li', 'Ceyao Zhang', 'Yitao Liang', 'Yaodong Yang', 'Yuanpei Chen'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'Institute for AI, Peking University', 'PKU-PsiBot Joint Lab'], 'pdf_title_img': 'assets/pdf/title_img/2502.20900.jpg', 'data': {'categories': ['#games', '#optimization', '#robotics', '#diffusion', '#interpretability', '#multimodal'], 'emoji': '🦾', 'ru': {'title': 'Универсальный захват объектов роботами с помощью зрения и языка', 'desc': 'DexGraspVLA - это иерархическая система для роботизированного захвата объектов, использующая предобученную Vision-Language модель в качестве высокоуровневого планировщика задач и обучаемую диффузионную политику в качестве низкоуровневого контроллера действий. Ключевая идея заключается в итеративном преобразовании разнородных языковых и визуальных входных данных в инвариантные к домену представления, что позволяет эффективно применять имитационное обучение. Система демонстрирует надежную генерализацию в широком диапазоне реальных сценариев, достигая более 90% успешных захватов в условиях тысяч невиданных ранее комбинаций объектов, освещения и фонов. Эмпирический анализ подтверждает согласованность внутреннего поведения модели при различных условиях окружающей среды.'}, 'en': {'title': 'Achieving Dexterous Grasping with DexGraspVLA', 'desc': 'This paper presents DexGraspVLA, a novel hierarchical framework designed to improve dexterous grasping in robotics. It combines a pre-trained Vision-Language model for high-level task planning with a diffusion-based policy for low-level action control. The framework effectively transforms diverse language and visual inputs into domain-invariant representations, allowing for better generalization across various real-world scenarios. The results show a success rate of over 90% in grasping tasks involving thousands of unseen objects and conditions, demonstrating the robustness of the approach.'}, 'zh': {'title': '实现灵巧抓取的突破性进展', 'desc': '本论文提出了一种名为DexGraspVLA的层次框架，旨在解决机器人灵巧抓取的问题。该框架利用预训练的视觉-语言模型作为高层任务规划器，并学习基于扩散的策略作为低层动作控制器。通过将多样的语言和视觉输入迭代转换为领域不变的表示，减轻了领域转移的影响，从而有效应用模仿学习。我们的实验表明，该方法在数千种未见物体、光照和背景组合下，在“零-shot”环境中实现了90%以上的成功率，展示了其在真实场景中的强大泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.21291', 'title': 'MIGE: A Unified Framework for Multimodal Instruction-Based Image Generation and Editing', 'url': 'https://huggingface.co/papers/2502.21291', 'abstract': 'Despite significant progress in diffusion-based image generation, subject-driven generation and instruction-based editing remain challenging. Existing methods typically treat them separately, struggling with limited high-quality data and poor generalization. However, both tasks require capturing complex visual variations while maintaining consistency between inputs and outputs. Therefore, we propose MIGE, a unified framework that standardizes task representations using multimodal instructions. It treats subject-driven generation as creation on a blank canvas and instruction-based editing as modification of an existing image, establishing a shared input-output formulation. MIGE introduces a novel multimodal encoder that maps free-form multimodal instructions into a unified vision-language space, integrating visual and semantic features through a feature fusion mechanism.This unification enables joint training of both tasks, providing two key advantages: (1) Cross-Task Enhancement: By leveraging shared visual and semantic representations, joint training improves instruction adherence and visual consistency in both subject-driven generation and instruction-based editing. (2) Generalization: Learning in a unified format facilitates cross-task knowledge transfer, enabling MIGE to generalize to novel compositional tasks, including instruction-based subject-driven editing. Experiments show that MIGE excels in both subject-driven generation and instruction-based editing while setting a state-of-the-art in the new task of instruction-based subject-driven editing. Code and model have been publicly available at https://github.com/Eureka-Maggie/MIGE.', 'score': 3, 'issue_id': 2498, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '55451dcf6bfad4a4', 'authors': ['Xueyun Tian', 'Wei Li', 'Bingbing Xu', 'Yige Yuan', 'Yuanzhuo Wang', 'Huawei Shen'], 'affiliations': ['Huawei Shen CAS Key Laboratory of AI Safety, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.21291.jpg', 'data': {'categories': ['#open_source', '#cv', '#diffusion', '#transfer_learning', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Единый подход к генерации и редактированию изображений на основе инструкций', 'desc': 'MIGE - это унифицированная система для генерации изображений на основе инструкций и редактирования существующих изображений. Она использует мультимодальный энкодер для преобразования свободных инструкций в единое визуально-языковое пространство. Совместное обучение обеих задач улучшает соответствие инструкциям и визуальную согласованность. MIGE демонстрирует превосходные результаты в генерации и редактировании изображений, а также устанавливает новый стандарт в задаче редактирования изображений на основе инструкций с учетом субъекта.'}, 'en': {'title': 'MIGE: Unifying Image Generation and Editing with Multimodal Instructions', 'desc': 'This paper presents MIGE, a unified framework for improving both subject-driven image generation and instruction-based editing in machine learning. It addresses the challenges of limited data and poor generalization by standardizing task representations through multimodal instructions. MIGE employs a novel multimodal encoder that integrates visual and semantic features, allowing for joint training of both tasks. The results demonstrate that MIGE enhances instruction adherence and visual consistency while also enabling generalization to new tasks, achieving state-of-the-art performance in instruction-based subject-driven editing.'}, 'zh': {'title': '统一框架，提升图像生成与编辑的能力', 'desc': '尽管扩散基础的图像生成取得了显著进展，但以主题为驱动的生成和基于指令的编辑仍然面临挑战。现有方法通常将这两者分开处理，受限于高质量数据的不足和泛化能力差。我们提出了MIGE，一个统一框架，通过多模态指令标准化任务表示，将主题驱动生成视为在空白画布上的创作，而将基于指令的编辑视为对现有图像的修改。MIGE引入了一种新颖的多模态编码器，将自由形式的多模态指令映射到统一的视觉-语言空间，从而实现了跨任务的增强和更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2502.17125', 'title': 'LettuceDetect: A Hallucination Detection Framework for RAG Applications', 'url': 'https://huggingface.co/papers/2502.17125', 'abstract': "Retrieval Augmented Generation (RAG) systems remain vulnerable to hallucinated answers despite incorporating external knowledge sources. We present LettuceDetect a framework that addresses two critical limitations in existing hallucination detection methods: (1) the context window constraints of traditional encoder-based methods, and (2) the computational inefficiency of LLM based approaches. Building on ModernBERT's extended context capabilities (up to 8k tokens) and trained on the RAGTruth benchmark dataset, our approach outperforms all previous encoder-based models and most prompt-based models, while being approximately 30 times smaller than the best models. LettuceDetect is a token-classification model that processes context-question-answer triples, allowing for the identification of unsupported claims at the token level. Evaluations on the RAGTruth corpus demonstrate an F1 score of 79.22% for example-level detection, which is a 14.8% improvement over Luna, the previous state-of-the-art encoder-based architecture. Additionally, the system can process 30 to 60 examples per second on a single GPU, making it more practical for real-world RAG applications.", 'score': 3, 'issue_id': 2497, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '347ebb871884d940', 'authors': ['Ádám Kovács', 'Gábor Recski'], 'affiliations': ['KR Labs', 'TU Wien'], 'pdf_title_img': 'assets/pdf/title_img/2502.17125.jpg', 'data': {'categories': ['#benchmark', '#rag', '#hallucinations', '#architecture', '#small_models', '#long_context'], 'emoji': '🥬', 'ru': {'title': 'LettuceDetect: Эффективное обнаружение галлюцинаций в RAG-системах', 'desc': 'LettuceDetect - это новая система для обнаружения галлюцинаций в генеративных моделях с извлечением информации (RAG). Она использует архитектуру ModernBERT с расширенным контекстным окном до 8000 токенов и обучена на датасете RAGTruth. LettuceDetect превосходит предыдущие модели на основе энкодеров и большинство моделей на основе промптов, будучи при этом в 30 раз меньше. Система демонстрирует F1-score 79.22% для обнаружения на уровне примеров, что на 14.8% лучше предыдущего SOTA-решения Luna.'}, 'en': {'title': 'LettuceDetect: Efficient Hallucination Detection for RAG Systems', 'desc': "This paper introduces LettuceDetect, a novel framework designed to improve the detection of hallucinated answers in Retrieval Augmented Generation (RAG) systems. It addresses limitations of existing methods by utilizing ModernBERT's extended context capabilities and a token-classification approach to analyze context-question-answer triples. LettuceDetect achieves a significant F1 score of 79.22% on the RAGTruth dataset, outperforming previous models while being much smaller and more efficient. The system's ability to process 30 to 60 examples per second on a single GPU enhances its practicality for real-world applications."}, 'zh': {'title': '提升幻觉检测的效率与准确性', 'desc': '本论文介绍了一种名为LettuceDetect的框架，旨在解决现有幻觉检测方法的两个主要限制。首先，它克服了传统编码器方法的上下文窗口限制，其次，它提高了基于大型语言模型（LLM）方法的计算效率。LettuceDetect基于ModernBERT，能够处理多达8000个标记，并在RAGTruth基准数据集上进行训练，表现优于所有先前的编码器模型和大多数基于提示的模型。该系统在RAGTruth语料库上的F1得分达到79.22%，并且在单个GPU上每秒可处理30到60个示例，适用于实际的RAG应用。'}}}, {'id': 'https://huggingface.co/papers/2502.20490', 'title': 'EgoNormia: Benchmarking Physical Social Norm Understanding', 'url': 'https://huggingface.co/papers/2502.20490', 'abstract': 'Human activity is moderated by norms. When performing actions in the real world, humans not only follow norms, but also consider the trade-off between different norms However, machines are often trained without explicit supervision on norm understanding and reasoning, especially when the norms are grounded in a physical and social context. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present EgoNormia |epsilon|, consisting of 1,853 ego-centric videos of human interactions, each of which has two related questions evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench of 92%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation method, it is possible to use EgoNomia to enhance normative reasoning in VLMs.', 'score': 2, 'issue_id': 2499, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'a2c7525ed78fb0ce', 'authors': ['MohammadHossein Rezaei', 'Yicheng Fu', 'Phil Cuvin', 'Caleb Ziems', 'Yanzhe Zhang', 'Hao Zhu', 'Diyi Yang'], 'affiliations': ['Georgia Tech', 'Stanford University', 'University of Arizona', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2502.20490.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#data', '#dataset', '#benchmark', '#ethics'], 'emoji': '🤖', 'ru': {'title': 'EgoNormia: повышение этического интеллекта ИИ через эгоцентрические видео', 'desc': 'Статья представляет EgoNormia |epsilon| - набор данных из 1853 эгоцентрических видео человеческих взаимодействий для оценки нормативного рассуждения моделей компьютерного зрения и обработки естественного языка (VLM). Каждое видео сопровождается двумя вопросами, оценивающими предсказание и обоснование нормативных действий в семи категориях, включая безопасность, конфиденциальность и сотрудничество. Исследование показывает, что современные VLM недостаточно понимают нормы, набирая максимум 45% на EgoNormia по сравнению с 92% у людей. Авторы предлагают метод генерации на основе поиска для улучшения нормативного рассуждения в VLM с помощью EgoNomia.'}, 'en': {'title': 'Enhancing Norm Understanding in Machines with EgoNormia', 'desc': "This paper introduces EgoNormia, a dataset designed to enhance the normative reasoning abilities of vision-language models (VLMs) by providing 1,853 ego-centric videos that depict human interactions. Each video is accompanied by questions that assess the model's ability to predict and justify normative actions across seven categories, including safety and privacy. The authors highlight that current VLMs perform poorly on this task, achieving only 45% accuracy compared to a human benchmark of 92%. They also propose a novel pipeline for dataset creation and demonstrate that using EgoNormia can improve the normative reasoning capabilities of VLMs through a retrieval-based generation method."}, 'zh': {'title': '提升机器的规范推理能力', 'desc': '本论文探讨了人类在活动中如何遵循社会规范，并提出了一个名为EgoNormia的数据集，以评估视觉-语言模型（VLMs）在规范推理方面的能力。该数据集包含1853个以自我为中心的人类互动视频，并通过相关问题来评估模型对规范行为的预测和解释。研究发现，当前的最先进VLM在规范理解方面表现不佳，最高得分仅为45%，远低于人类的92%。此外，论文还提出了一种基于检索的生成方法，能够利用EgoNormia提升VLM的规范推理能力。'}}}, {'id': 'https://huggingface.co/papers/2502.20811', 'title': 'HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models', 'url': 'https://huggingface.co/papers/2502.20811', 'abstract': 'Recent Multi-modal Large Language Models (MLLMs) have made great progress in video understanding. However, their performance on videos involving human actions is still limited by the lack of high-quality data. To address this, we introduce a two-stage data annotation pipeline. First, we design strategies to accumulate videos featuring clear human actions from the Internet. Second, videos are annotated in a standardized caption format that uses human attributes to distinguish individuals and chronologically details their actions and interactions. Through this pipeline, we curate two datasets, namely HAICTrain and HAICBench. HAICTrain comprises 126K video-caption pairs generated by Gemini-Pro and verified for training purposes. Meanwhile, HAICBench includes 500 manually annotated video-caption pairs and 1,400 QA pairs, for a comprehensive evaluation of human action understanding. Experimental results demonstrate that training with HAICTrain not only significantly enhances human understanding abilities across 4 benchmarks, but can also improve text-to-video generation results. Both the HAICTrain and HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.', 'score': 1, 'issue_id': 2486, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '806f6aacd5ee2f8a', 'authors': ['Xiao Wang', 'Jingyun Hua', 'Weihong Lin', 'Yuanxing Zhang', 'Fuzheng Zhang', 'Jianlong Wu', 'Di Zhang', 'Liqiang Nie'], 'affiliations': ['Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.20811.jpg', 'data': {'categories': ['#dataset', '#data', '#video', '#multimodal', '#benchmark', '#synthetic'], 'emoji': '🎬', 'ru': {'title': 'Улучшение понимания человеческих действий в видео с помощью аннотированных данных', 'desc': 'Статья представляет новый подход к улучшению понимания видео с человеческими действиями для мультимодальных больших языковых моделей (MLLM). Авторы разработали двухэтапный процесс аннотации данных, включающий сбор релевантных видео и их стандартизированное описание. В результате были созданы два набора данных: HAICTrain для обучения и HAICBench для оценки моделей. Эксперименты показали, что обучение на HAICTrain значительно улучшает способности моделей понимать человеческие действия на видео.'}, 'en': {'title': 'Enhancing Video Understanding with Curated Human Action Datasets', 'desc': 'This paper presents a solution to improve video understanding in Multi-modal Large Language Models (MLLMs) by addressing the scarcity of high-quality data on human actions. The authors introduce a two-stage data annotation pipeline that first collects videos with clear human actions from the Internet and then annotates them using a standardized caption format. This results in two curated datasets: HAICTrain, which contains 126K video-caption pairs for training, and HAICBench, which includes 500 annotated pairs for evaluation. Experimental results show that using HAICTrain significantly boosts human action understanding and enhances text-to-video generation capabilities across multiple benchmarks.'}, 'zh': {'title': '提升视频理解的创新数据标注流程', 'desc': '最近的多模态大型语言模型（MLLMs）在视频理解方面取得了显著进展。然而，它们在涉及人类动作的视频上的表现仍然受到高质量数据缺乏的限制。为了解决这个问题，我们提出了一个两阶段的数据标注流程，首先从互联网收集包含清晰人类动作的视频，然后使用标准化的字幕格式对视频进行标注。通过这个流程，我们创建了两个数据集HAICTrain和HAICBench，实验结果表明，使用HAICTrain进行训练显著提升了人类动作理解能力，并改善了文本到视频生成的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.07920', 'title': 'Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia', 'url': 'https://huggingface.co/papers/2503.07920', 'abstract': 'Southeast Asia (SEA) is a region of extraordinary linguistic and cultural diversity, yet it remains significantly underrepresented in vision-language (VL) research. This often results in artificial intelligence (AI) models that fail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an open-source initiative dedicated to developing high-quality, culturally relevant data for SEA languages. By involving contributors from SEA countries, SEA-VL aims to ensure better cultural relevance and diversity, fostering greater inclusivity of underrepresented languages in VL research. Beyond crowdsourcing, our initiative goes one step further in the exploration of the automatic collection of culturally relevant images through crawling and image generation. First, we find that image crawling achieves approximately ~85% cultural relevance while being more cost- and time-efficient than crowdsourcing. Second, despite the substantial progress in generative vision models, synthetic images remain unreliable in accurately reflecting SEA cultures. The generated images often fail to reflect the nuanced traditions and cultural contexts of the region. Collectively, we gather 1.28M SEA culturally-relevant images, more than 50 times larger than other existing datasets. Through SEA-VL, we aim to bridge the representation gap in SEA, fostering the development of more inclusive AI systems that authentically represent diverse cultures across SEA.', 'score': 73, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '32c690b6ffb8b143', 'authors': ['Samuel Cahyawijaya', 'Holy Lovenia', 'Joel Ruben Antony Moniz', 'Tack Hwa Wong', 'Mohammad Rifqi Farhansyah', 'Thant Thiri Maung', 'Frederikus Hudi', 'David Anugraha', 'Muhammad Ravi Shulthan Habibi', 'Muhammad Reza Qorib', 'Amit Agarwal', 'Joseph Marvin Imperial', 'Hitesh Laxmichand Patel', 'Vicky Feliren', 'Bahrul Ilmi Nasution', 'Manuel Antonio Rufino', 'Genta Indra Winata', 'Rian Adam Rajagede', 'Carlos Rafael Catalan', 'Mohamed Fazli Imam', 'Priyaranjan Pattnayak', 'Salsabila Zahirah Pranida', 'Kevin Pratama', 'Yeshil Bangera', 'Adisai Na-Thalang', 'Patricia Nicole Monderin', 'Yueqi Song', 'Christian Simon', 'Lynnette Hui Xian Ng', "Richardy Lobo' Sapan", 'Taki Hasan Rafi', 'Bin Wang', 'Supryadi', 'Kanyakorn Veerakanjana', 'Piyalitt Ittichaiwong', 'Matthew Theodore Roque', 'Karissa Vincentio', 'Takdanai Kreangphet', 'Phakphum Artkaew', 'Kadek Hendrawan Palgunadi', 'Yanzhi Yu', 'Rochana Prih Hastuti', 'William Nixon', 'Mithil Bangera', 'Adrian Xuan Wei Lim', 'Aye Hninn Khine', 'Hanif Muhammad Zhafran', 'Teddy Ferdinan', 'Audra Aurora Izzani', 'Ayushman Singh', 'Evan', 'Jauza Akbar Krito', 'Michael Anugraha', 'Fenal Ashokbhai Ilasariya', 'Haochen Li', 'John Amadeo Daniswara', 'Filbert Aurelian Tjiaranata', 'Eryawan Presma Yulianrifat', 'Can Udomcharoenchaikit', 'Fadil Risdian Ansori', 'Mahardika Krisna Ihsani', 'Giang Nguyen', 'Anab Maulana Barik', 'Dan John Velasco', 'Rifo Ahmad Genadi', 'Saptarshi Saha', 'Chengwei Wei', 'Isaiah Flores', 'Kenneth Ko Han Chen', 'Anjela Gail Santos', 'Wan Shen Lim', 'Kaung Si Phyo', 'Tim Santos', 'Meisyarah Dwiastuti', 'Jiayun Luo', 'Jan Christian Blaise Cruz', 'Ming Shan Hee', 'Ikhlasul Akmal Hanif', 'M. Alif Al Hakim', "Muhammad Rizky Sya'ban", 'Kun Kerdthaisong', 'Lester James V. Miranda', 'Fajri Koto', 'Tirana Noor Fatyanosa', 'Alham Fikri Aji', 'Jostin Jerico Rosal', 'Jun Kevin', 'Robert Wijaya', 'Onno P. Kampman', 'Ruochen Zhang', 'Börje F. Karlsson', 'Peerat Limkonchotiwat'], 'affiliations': ['AI Singapore', 'Allen AI', 'Ateneo de Manila University', 'Auburn University', 'Bandung Institute of Technology', 'Beijing Academy of Artificial Intelligence (BAAI)', 'Binus University', 'Brawijaya University', 'Brown University', 'Capital One', 'Carnegie Mellon University', 'Chulalongkorn University', 'Cohere', 'Dataxet:Sonar', 'Faculty of Medicine Siriraj Hospital, Mahidol University', 'Graphcore', 'Hanyang University', 'Independent', 'Indian Statistical Institute, Kolkata', 'IndoNLP', 'Institut Teknologi Sepuluh Nopember', 'Institute for Infocomm Research, Singapore', 'King Mongkuts University of Technology Thonburi', 'MBZUAI', 'MOH Office for Healthcare Transformation', 'Macau University of Science and Technology', 'Meta', 'Mila - Quebec AI Institute', 'Monash University, Indonesia', 'Nara Institute of Science and Technology', 'National University Philippines', 'National University of Singapore', 'New York University', 'Oracle', 'Polytechnique Montreal', 'SCB 10X', 'SEACrowd', 'Samsung R&D Institute Philippines', 'Seoul National University of Science and Technology', 'Singapore Polytechnic', 'Singapore University of Technology and Design', 'Sony Group Corporation', 'Srinakharinwirot University', 'Thammasat University', 'The University of Manchester', 'Tianjin University', 'Ton Duc Thang University', 'Universitas Gadjah Mada', 'Universitas Islam Indonesia', 'Universitas Pelita Harapan', 'University of Bath', 'University of Illiinois, Urbana-Champaign', 'University of Indonesia', 'University of New Haven', 'University of Toronto', 'University of the Philippines', 'Vidyasirimedhi Institute of Science and Technology', 'Works Applications', 'Wrocław Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.07920.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#open_source', '#data', '#multimodal'], 'emoji': '🌏', 'ru': {'title': 'Преодоление культурного разрыва в ИИ для Юго-Восточной Азии', 'desc': 'Статья представляет SEA-VL - инициативу по созданию качественных данных для языков Юго-Восточной Азии в области vision-language моделей. Проект направлен на улучшение культурной релевантности и разнообразия в исследованиях ИИ. Авторы изучают методы автоматического сбора культурно значимых изображений через краулинг и генерацию изображений. В результате собрано 1,28 млн культурно релевантных изображений, что в 50 раз больше существующих датасетов.'}, 'en': {'title': 'Bridging the Cultural Gap in AI with SEA-VL', 'desc': "The paper introduces SEA-VL, an initiative aimed at enhancing representation of Southeast Asian cultures in vision-language (VL) research. It highlights the creation of a large dataset containing 1.28 million culturally relevant images, which is significantly larger than existing datasets. The initiative combines crowdsourcing with automated image crawling, achieving high cultural relevance efficiently. However, it also notes challenges with generative models, as synthetic images often fail to accurately depict the region's diverse cultural nuances."}, 'zh': {'title': '填补东南亚文化在AI研究中的空白', 'desc': '东南亚地区语言和文化多样性极为丰富，但在视觉语言研究中却严重缺乏代表性。为了解决这一问题，我们提出了SEA-VL，这是一个开放源代码的项目，旨在为东南亚语言开发高质量、文化相关的数据。通过吸引来自东南亚国家的贡献者，SEA-VL确保了更好的文化相关性和多样性，促进了在视觉语言研究中对被低估语言的包容性。我们收集了128万张与东南亚文化相关的图像，远超现有数据集的规模，旨在缩小东南亚的代表性差距，推动更具包容性的人工智能系统的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.07536', 'title': 'LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through\n  Two-Stage Rule-Based RL', 'url': 'https://huggingface.co/papers/2503.07536', 'abstract': 'Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \\method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \\method achieves 4.83\\% and 4.5\\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.', 'score': 50, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '59c304598f64f1e6', 'authors': ['Yingzhe Peng', 'Gongrui Zhang', 'Miaosen Zhang', 'Zhiyuan You', 'Jie Liu', 'Qipeng Zhu', 'Kai Yang', 'Xingzhong Xu', 'Xin Geng', 'Xu Yang'], 'affiliations': ['Ant Group', 'Fudan University', 'Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.07536.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#benchmark', '#small_models', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Усиление рассуждений в мультимодальных моделях через обучение на текстах', 'desc': 'Статья представляет метод улучшения рассуждений в больших мультимодальных моделях (LMM) с архитектурой 3B-параметров. Авторы предлагают двухэтапный подход: сначала усиление базовых способностей рассуждения с помощью обучения с подкреплением на текстовых данных, затем обобщение этих навыков на мультимодальные задачи. Эксперименты на модели Qwen2.5-VL-Instruct-3B показывают значительное улучшение производительности как в мультимодальных, так и в текстовых тестах. Метод позволяет эффективно обучать мультимодальные модели без необходимости в больших объемах качественных мультимодальных данных.'}, 'en': {'title': 'Boosting Reasoning in Multimodal Models Efficiently', 'desc': 'This paper addresses the challenges of improving reasoning in Large Multimodal Models (LMMs), particularly in smaller architectures with limited parameters. It identifies two main issues: the lack of sufficient data for complex reasoning in multimodal contexts and the negative impact of multimodal pretraining on foundational reasoning. The authors propose a two-stage framework that first enhances reasoning using rule-based reinforcement learning on text-only data, followed by training to generalize these skills to multimodal scenarios. Experimental results show significant improvements in reasoning performance across both multimodal and text-only tasks, demonstrating the effectiveness of their approach in reducing the need for extensive multimodal training data.'}, 'zh': {'title': '增强多模态推理能力的高效方法', 'desc': '本论文探讨了在大型多模态模型（LMMs）中增强推理能力的挑战，特别是在参数量为3B的紧凑架构中，视觉感知与逻辑推理之间的复杂互动。我们提出了一种名为\textit{method}的两阶段框架，通过基础推理增强（FRE）和多模态泛化训练（MGT）来适应多模态推理。FRE阶段利用基于规则的强化学习（RL）增强文本数据的推理能力，MGT阶段则将这些推理能力推广到多模态领域。实验结果表明，\textit{method}在多模态和文本基准测试中分别比基线提高了4.83%和4.5%，在复杂的足球比赛任务中提高了3.63%。'}}}, {'id': 'https://huggingface.co/papers/2503.08638', 'title': 'YuE: Scaling Open Foundation Models for Long-Form Music Generation', 'url': 'https://huggingface.co/papers/2503.08638', 'abstract': "We tackle the task of long-form music generation--particularly the challenging lyrics-to-song problem--by introducing YuE, a family of open foundation models based on the LLaMA2 architecture. Specifically, YuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through (1) track-decoupled next-token prediction to overcome dense mixture signals, (2) structural progressive conditioning for long-context lyrical alignment, and (3) a multitask, multiphase pre-training recipe to converge and generalize. In addition, we redesign the in-context learning technique for music generation, enabling versatile style transfer (e.g., converting Japanese city pop into an English rap while preserving the original accompaniment) and bidirectional generation. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility. In addition, fine-tuning YuE enables additional controls and enhanced support for tail languages. Furthermore, beyond generation, we show that YuE's learned representations can perform well on music understanding tasks, where the results of YuE match or exceed state-of-the-art methods on the MARBLE benchmark. Keywords: lyrics2song, song generation, long-form, foundation model, music generation", 'score': 46, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'eb1539380bf435c9', 'authors': ['Ruibin Yuan', 'Hanfeng Lin', 'Shuyue Guo', 'Ge Zhang', 'Jiahao Pan', 'Yongyi Zang', 'Haohe Liu', 'Yiming Liang', 'Wenye Ma', 'Xingjian Du', 'Xinrun Du', 'Zhen Ye', 'Tianyu Zheng', 'Yinghao Ma', 'Minghao Liu', 'Zeyue Tian', 'Ziya Zhou', 'Liumeng Xue', 'Xingwei Qu', 'Yizhi Li', 'Shangda Wu', 'Tianhao Shen', 'Ziyang Ma', 'Jun Zhan', 'Chunhui Wang', 'Yatian Wang', 'Xiaowei Chi', 'Xinyue Zhang', 'Zhenzhu Yang', 'Xiangzhou Wang', 'Shansong Liu', 'Lingrui Mei', 'Peng Li', 'Junjie Wang', 'Jianwei Yu', 'Guojian Pang', 'Xu Li', 'Zihao Wang', 'Xiaohuan Zhou', 'Lijun Yu', 'Emmanouil Benetos', 'Yong Chen', 'Chenghua Lin', 'Xie Chen', 'Gus Xia', 'Zhaoxiang Zhang', 'Chao Zhang', 'Wenhu Chen', 'Xinyu Zhou', 'Xipeng Qiu', 'Roger Dannenberg', 'Jiaheng Liu', 'Jian Yang', 'Wenhao Huang', 'Wei Xue', 'Xu Tan', 'Yike Guo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.08638.jpg', 'data': {'categories': ['#training', '#multimodal', '#long_context', '#benchmark', '#low_resource', '#open_source', '#story_generation', '#audio'], 'emoji': '🎵', 'ru': {'title': 'YuE: Открытая модель для генерации длинных музыкальных композиций на основе текста', 'desc': 'YuE - это семейство открытых моделей для генерации музыки на основе архитектуры LLaMA2. Модель способна генерировать до пяти минут музыки, сохраняя согласованность с текстом песни, музыкальную структуру и вокальные мелодии с аккомпанементом. YuE использует несколько инновационных техник, включая предсказание следующего токена с разделением треков и структурное прогрессивное кондиционирование. Модель также может выполнять перенос стиля и двунаправленную генерацию, а её представления эффективны для задач понимания музыки.'}, 'en': {'title': 'Transforming Lyrics into Melodies with YuE!', 'desc': 'This paper presents YuE, a new family of open foundation models designed for long-form music generation, specifically focusing on the lyrics-to-song challenge. YuE utilizes the LLaMA2 architecture and can generate up to five minutes of music while ensuring that the lyrics align with the musical structure and melodies. Key innovations include track-decoupled next-token prediction for better signal clarity, structural progressive conditioning for lyrical alignment, and a multitask pre-training approach to enhance generalization. The model also supports versatile style transfer and performs well on music understanding tasks, achieving results that rival existing proprietary systems.'}, 'zh': {'title': 'YuE：歌词转歌曲的音乐生成新突破', 'desc': '本文介绍了一种名为YuE的开放基础模型，专注于长篇音乐生成，特别是歌词转歌曲的问题。YuE基于LLaMA2架构，能够生成长达五分钟的音乐，同时保持歌词的对齐、连贯的音乐结构和引人入胜的旋律。通过采用解耦的下一个标记预测、结构性渐进条件和多任务预训练等技术，YuE在音乐生成中表现出色。经过评估，YuE在音乐性和声乐灵活性方面与一些专有系统相匹敌，甚至超越了它们。'}}}, {'id': 'https://huggingface.co/papers/2503.05978', 'title': 'MagicInfinite: Generating Infinite Talking Videos with Your Words and\n  Voice', 'url': 'https://huggingface.co/papers/2503.05978', 'abstract': "We present MagicInfinite, a novel diffusion Transformer (DiT) framework that overcomes traditional portrait animation limitations, delivering high-fidelity results across diverse character types-realistic humans, full-body figures, and stylized anime characters. It supports varied facial poses, including back-facing views, and animates single or multiple characters with input masks for precise speaker designation in multi-character scenes. Our approach tackles key challenges with three innovations: (1) 3D full-attention mechanisms with a sliding window denoising strategy, enabling infinite video generation with temporal coherence and visual quality across diverse character styles; (2) a two-stage curriculum learning scheme, integrating audio for lip sync, text for expressive dynamics, and reference images for <PRE_TAG>identity preservation</POST_TAG>, enabling flexible multi-modal control over long sequences; and (3) region-specific masks with adaptive loss functions to balance global textual control and local audio guidance, supporting speaker-specific animations. Efficiency is enhanced via our innovative unified step and cfg distillation techniques, achieving a 20x inference speed boost over the basemodel: generating a 10 second 540x540p video in 10 seconds or 720x720p in 30 seconds on 8 H100 GPUs, without quality loss. Evaluations on our new benchmark demonstrate MagicInfinite's superiority in audio-lip synchronization, identity preservation, and motion naturalness across diverse scenarios. It is publicly available at https://www.hedra.com/, with examples at https://magicinfinite.github.io/.", 'score': 26, 'issue_id': 2658, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '2111564f7e67ca23', 'authors': ['Hongwei Yi', 'Tian Ye', 'Shitong Shao', 'Xuancheng Yang', 'Jiantong Zhao', 'Hanzhong Guo', 'Terrance Wang', 'Qingyu Yin', 'Zeke Xie', 'Lei Zhu', 'Wei Li', 'Michael Lingelbach', 'Daquan Zhou'], 'affiliations': ['HKU', 'HKUST(GZ)', 'Hedra Inc.', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05978.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#inference', '#diffusion', '#video', '#open_source'], 'emoji': '🎭', 'ru': {'title': 'MagicInfinite: революция в анимации портретов с помощью ИИ', 'desc': 'MagicInfinite - это новая система на основе диффузионного трансформера для анимации портретов. Она способна создавать высококачественные анимации для разных типов персонажей, включая реалистичных людей и аниме-персонажей. Система использует 3D механизм полного внимания, двухэтапное обучение с учителем и регион-специфичные маски для улучшения качества и контроля анимации. MagicInfinite демонстрирует превосходные результаты в синхронизации губ с аудио, сохранении идентичности и естественности движений.'}, 'en': {'title': 'Revolutionizing Portrait Animation with MagicInfinite', 'desc': 'MagicInfinite is a cutting-edge diffusion Transformer framework designed to enhance portrait animation by producing high-quality results for various character types, including realistic humans and stylized anime. It introduces innovative techniques such as 3D full-attention mechanisms and a sliding window denoising strategy, allowing for infinite video generation while maintaining visual coherence. The framework employs a two-stage curriculum learning approach that integrates audio and text inputs for improved lip synchronization and expressive dynamics, along with region-specific masks for precise control over animations. With a significant boost in efficiency, MagicInfinite can generate high-resolution videos rapidly, outperforming existing models in key areas like audio-lip synchronization and identity preservation.'}, 'zh': {'title': '魔法无限：突破肖像动画的界限', 'desc': 'MagicInfinite是一种新型的扩散变换器框架，旨在克服传统肖像动画的局限性，能够在多种角色类型中提供高保真度的结果，包括真实人类、全身人物和风格化的动漫角色。该框架支持多种面部姿势的动画，包括背面视图，并能够通过输入掩码精确指定多角色场景中的发言者。我们的方法通过三项创新来解决关键挑战：3D全注意力机制与滑动窗口去噪策略，支持无限视频生成并保持时间一致性和视觉质量；以及区域特定掩码与自适应损失函数的结合，平衡全局文本控制和局部音频指导。我们的评估表明，MagicInfinite在音频与唇同步、身份保留和动作自然性方面优于其他方法。'}}}, {'id': 'https://huggingface.co/papers/2503.08120', 'title': 'UniF^2ace: Fine-grained Face Understanding and Generation\n  with Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2503.08120', 'abstract': "Unified multimodal models (UMMs) have emerged as a powerful paradigm in foundational computer vision research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily focuses on coarse facial attribute understanding, with limited capacity to handle fine-grained facial attributes and without addressing generation capabilities. To overcome these limitations, we propose UniF^2ace, the first UMM tailored specifically for fine-grained face understanding and generation. In general, we train UniF^2ace on a self-constructed, specialized dataset utilizing two mutually beneficial diffusion techniques and a two-level mixture-of-experts architecture. Specifically, we first build a large-scale facial dataset, UniF^2ace-130K, which contains 130K image-text pairs with one million question-answering pairs that span a wide range of facial attributes. Second, we establish a theoretical connection between discrete diffusion score matching and masked generative models, optimizing both evidence lower bounds simultaneously, which significantly improves the model's ability to synthesize facial details. Finally, we introduce both token-level and sequence-level mixture-of-experts, enabling efficient fine-grained representation learning for both understanding and generation tasks. Extensive experiments on UniF^2ace-130K demonstrate that UniF^2ace outperforms existing UMMs and generative models, achieving superior performance across both understanding and generation tasks.", 'score': 25, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7c6e7c685b283d61', 'authors': ['Junzhe Li', 'Xuerui Qiu', 'Linrui Xu', 'Liya Guo', 'Delin Qu', 'Tingting Long', 'Chun Fan', 'Ming Li'], 'affiliations': ['Central South University', 'Computer Center, Peking University', 'Fudan University', 'Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Institute of Automation, Chinese Academy of Sciences', 'School of Computer Science, Peking University', 'Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08120.jpg', 'data': {'categories': ['#synthetic', '#cv', '#dataset', '#multimodal', '#architecture', '#diffusion'], 'emoji': '🧑', 'ru': {'title': 'UniF^2ace: Новый уровень в понимании и генерации лиц', 'desc': 'UniF^2ace - это новая унифицированная мультимодальная модель (UMM), специально разработанная для детального понимания и генерации лиц. Модель обучена на большом специализированном наборе данных, содержащем 130 тысяч пар изображений и текстов с миллионом пар вопросов-ответов о различных атрибутах лица. В UniF^2ace применяются две взаимодополняющие техники диффузии и двухуровневая архитектура смеси экспертов для эффективного обучения представлений. Эксперименты показывают, что UniF^2ace превосходит существующие UMM и генеративные модели в задачах понимания и генерации лиц.'}, 'en': {'title': 'UniF^2ace: Mastering Fine-Grained Facial Understanding and Generation', 'desc': 'This paper introduces UniF^2ace, a unified multimodal model designed for fine-grained understanding and generation of facial attributes. Unlike previous models that only focus on coarse attributes, UniF^2ace leverages a large-scale dataset of 130K image-text pairs and one million question-answering pairs to enhance its learning capabilities. The model employs innovative diffusion techniques and a mixture-of-experts architecture to optimize its performance in synthesizing detailed facial features. Experimental results show that UniF^2ace significantly outperforms existing models in both understanding and generating facial attributes.'}, 'zh': {'title': '细粒度面部理解与生成的统一模型', 'desc': '统一多模态模型（UMMs）在计算机视觉研究中展现出强大的潜力，尤其是在图像理解和生成方面。然而，现有的面部领域研究主要集中在粗略的面部属性理解上，缺乏处理细粒度面部属性的能力，并且未能解决生成能力的问题。为了解决这些限制，我们提出了UniF^2ace，这是第一个专门针对细粒度面部理解和生成的UMM。通过构建一个包含13万张图像-文本对的大规模数据集，并采用两种互补的扩散技术和双层专家混合架构，UniF^2ace在理解和生成任务上均表现出色。'}}}, {'id': 'https://huggingface.co/papers/2503.08625', 'title': 'SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by\n  Imitating Human Annotator Trajectories', 'url': 'https://huggingface.co/papers/2503.08625', 'abstract': "While MLLMs have demonstrated adequate image understanding capabilities, they still struggle with pixel-level comprehension, limiting their practical applications. Current evaluation tasks like VQA and visual grounding remain too coarse to assess fine-grained pixel comprehension accurately. Though segmentation is foundational for pixel-level understanding, existing methods often require MLLMs to generate implicit tokens, decoded through external pixel decoders. This approach disrupts the MLLM's text output space, potentially compromising language capabilities and reducing flexibility and extensibility, while failing to reflect the model's intrinsic pixel-level understanding.   Thus, we introduce the Human-Like Mask Annotation Task (HLMAT), a new paradigm where MLLMs mimic human annotators using interactive segmentation tools. Modeling segmentation as a multi-step Markov Decision Process, HLMAT enables MLLMs to iteratively generate text-based click points, achieving high-quality masks without architectural changes or implicit tokens. Through this setup, we develop SegAgent, a model fine-tuned on human-like annotation trajectories, which achieves performance comparable to state-of-the-art (SOTA) methods and supports additional tasks like mask refinement and annotation filtering.   HLMAT provides a protocol for assessing fine-grained pixel understanding in MLLMs and introduces a vision-centric, multi-step decision-making task that facilitates exploration of MLLMs' visual reasoning abilities. Our adaptations of policy improvement method StaR and PRM-guided tree search further enhance model robustness in complex segmentation tasks, laying a foundation for future advancements in fine-grained visual perception and multi-step decision-making for MLLMs.", 'score': 22, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '081b91105002410a', 'authors': ['Muzhi Zhu', 'Yuzhuo Tian', 'Hao Chen', 'Chunluan Zhou', 'Qingpei Guo', 'Yang Liu', 'Ming Yang', 'Chunhua Shen'], 'affiliations': ['Ant Group', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.08625.jpg', 'data': {'categories': ['#agents', '#cv', '#rl', '#reasoning', '#optimization', '#games'], 'emoji': '🖼️', 'ru': {'title': 'Человекоподобная сегментация изображений мультимодальными языковыми моделями', 'desc': 'Статья представляет новый подход к оценке понимания изображений на уровне пикселей для мультимодальных языковых моделей (MLLM). Авторы вводят задачу аннотации масок, подобную человеческой (HLMAT), где MLLM имитируют действия человека-аннотатора. Разработанная модель SegAgent достигает результатов на уровне современных методов сегментации без изменения архитектуры. HLMAT открывает новые возможности для исследования визуального мышления MLLM и улучшения их восприятия изображений.'}, 'en': {'title': 'Enhancing Pixel-Level Understanding in MLLMs with Human-Like Annotation', 'desc': "This paper addresses the limitations of Multimodal Language Models (MLLMs) in understanding images at the pixel level, which restricts their practical use. It critiques existing evaluation methods for being too broad and introduces the Human-Like Mask Annotation Task (HLMAT) to improve pixel comprehension. HLMAT allows MLLMs to simulate human-like annotation through interactive segmentation, modeled as a multi-step Markov Decision Process. The proposed SegAgent model, trained on this new task, achieves competitive performance while maintaining the MLLM's language capabilities and flexibility."}, 'zh': {'title': '提升像素理解的创新标注任务', 'desc': '本论文探讨了多模态大语言模型（MLLMs）在像素级理解方面的不足，尤其是在细粒度评估任务中的局限性。我们提出了一种新的任务——人类样本标注任务（HLMAT），通过模拟人类标注者的方式，使用交互式分割工具来提高模型的像素理解能力。HLMAT将分割建模为多步骤的马尔可夫决策过程，使得MLLMs能够迭代生成基于文本的点击点，从而无需改变模型架构或使用隐式标记。通过这一方法，我们开发了SegAgent模型，其性能与最先进的方法相当，并支持掩膜细化和标注过滤等额外任务。'}}}, {'id': 'https://huggingface.co/papers/2503.07703', 'title': 'Seedream 2.0: A Native Chinese-English Bilingual Image Generation\n  Foundation Model', 'url': 'https://huggingface.co/papers/2503.07703', 'abstract': 'Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.', 'score': 22, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '00cd4369f3c531f9', 'authors': ['Lixue Gong', 'Xiaoxia Hou', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Fei Liu', 'Liyang Liu', 'Wei Liu', 'Wei Lu', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Xin Xia', 'Xuefeng Xiao', 'Linjie Yang', 'Zhonghua Zhai', 'Xinyu Zhang', 'Qi Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Jianchao Yang', 'Weilin Huang'], 'affiliations': ['Seed Vision Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.07703.jpg', 'data': {'categories': ['#cv', '#training', '#dataset', '#optimization', '#rlhf', '#alignment', '#multimodal', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Seedream 2.0: Революция в двуязычной генерации изображений', 'desc': 'Seedream 2.0 - это двуязычная модель генерации изображений, созданная для преодоления ограничений существующих моделей. Она использует мощную систему данных и двуязычную языковую модель для точного понимания культурных нюансов. Модель применяет Glyph-Aligned ByT5 для гибкого рендеринга текста и Scaled ROPE для генерализации на нетренированные разрешения. Многоэтапная оптимизация, включая SFT и RLHF, улучшает общие возможности модели.'}, 'en': {'title': 'Seedream 2.0: Bridging Cultures in Image Generation', 'desc': 'This paper introduces Seedream 2.0, a bilingual image generation model designed to overcome limitations found in existing models like Flux and Midjourney. It effectively handles text prompts in both Chinese and English, allowing for culturally nuanced image generation and accurate text rendering. The model incorporates a bilingual large language model for enhanced understanding and a robust data system for knowledge integration. Extensive optimizations, including reinforcement learning from human feedback, ensure that Seedream 2.0 excels in aesthetics, prompt adherence, and overall image quality.'}, 'zh': {'title': '双语图像生成的未来：Seedream 2.0', 'desc': '本论文介绍了Seedream 2.0，这是一个中英双语的图像生成基础模型，旨在解决现有模型在文本渲染和文化理解方面的不足。该模型能够处理中文和英文的文本提示，支持双语图像生成，并通过强大的数据系统和描述系统提升图像描述的准确性和丰富性。Seedream 2.0结合了自研的双语大语言模型，能够从海量数据中直接学习本土知识，生成高保真度的图像，准确表达文化细节和美学特征。此外，通过多阶段的后训练优化，Seedream 2.0在多个方面实现了最先进的性能，包括遵循提示、审美、文本渲染和结构正确性。'}}}, {'id': 'https://huggingface.co/papers/2503.07860', 'title': 'Video Action Differencing', 'url': 'https://huggingface.co/papers/2503.07860', 'abstract': 'How do two individuals differ when performing the same action? In this work, we introduce Video Action Differencing (VidDiff), the novel task of identifying subtle differences between videos of the same action, which has many applications, such as coaching and skill learning. To enable development on this new task, we first create VidDiffBench, a benchmark dataset containing 549 video pairs, with human annotations of 4,469 fine-grained action differences and 2,075 localization timestamps indicating where these differences occur. Our experiments demonstrate that VidDiffBench poses a significant challenge for state-of-the-art large multimodal models (LMMs), such as GPT-4o and Qwen2-VL. By analyzing failure cases of LMMs on VidDiffBench, we highlight two key challenges for this task: localizing relevant sub-actions over two videos and fine-grained frame comparison. To overcome these, we propose the VidDiff method, an agentic workflow that breaks the task into three stages: action difference proposal, keyframe localization, and frame differencing, each stage utilizing specialized foundation models. To encourage future research in this new task, we release the benchmark at https://huggingface.co/datasets/jmhb/VidDiffBench and code at http://jmhb0.github.io/viddiff.', 'score': 21, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '76b8b9c677de83cd', 'authors': ['James Burgess', 'Xiaohan Wang', 'Yuhui Zhang', 'Anita Rau', 'Alejandro Lozano', 'Lisa Dunlap', 'Trevor Darrell', 'Serena Yeung-Levy'], 'affiliations': ['Stanford', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.07860.jpg', 'data': {'categories': ['#benchmark', '#agents', '#multimodal', '#dataset', '#video'], 'emoji': '🎥', 'ru': {'title': 'Новый фронтир в анализе видео: выявление тонких различий в действиях', 'desc': 'Статья представляет новую задачу в области компьютерного зрения - Video Action Differencing (VidDiff), которая направлена на выявление тонких различий между видео с одинаковыми действиями. Авторы создали датасет VidDiffBench, содержащий 549 пар видео с аннотациями различий в действиях и временными метками. Эксперименты показали, что современные мультимодальные языковые модели (LLM) испытывают трудности с этой задачей. Для решения проблемы предложен метод VidDiff, использующий трехэтапный подход с применением специализированных фундаментальных моделей.'}, 'en': {'title': 'Unveiling Subtle Differences in Action Videos with VidDiff', 'desc': 'This paper introduces Video Action Differencing (VidDiff), a new task focused on identifying subtle differences in videos depicting the same action. To support this task, the authors created VidDiffBench, a benchmark dataset with 549 video pairs and detailed annotations of action differences and localization timestamps. The study reveals that current large multimodal models struggle with this task, particularly in localizing sub-actions and performing fine-grained frame comparisons. To address these challenges, the authors propose a structured approach called VidDiff, which divides the task into three stages, each leveraging specialized foundation models for improved performance.'}, 'zh': {'title': '视频动作差异化：识别细微差别的新挑战', 'desc': '本文介绍了一种新任务，称为视频动作差异化（VidDiff），旨在识别同一动作视频之间的细微差别。为此，我们创建了VidDiffBench，一个包含549对视频的基准数据集，标注了4469个细粒度动作差异和2075个时间戳。我们的实验表明，VidDiffBench对现有的大型多模态模型（LMMs）提出了重大挑战，尤其是在局部化相关子动作和细粒度帧比较方面。为了解决这些问题，我们提出了VidDiff方法，将任务分为三个阶段：动作差异提议、关键帧定位和帧差异化，每个阶段都利用了专门的基础模型。'}}}, {'id': 'https://huggingface.co/papers/2503.07891', 'title': 'Gemini Embedding: Generalizable Embeddings from Gemini', 'url': 'https://huggingface.co/papers/2503.07891', 'abstract': "In this report, we introduce Gemini Embedding, a state-of-the-art embedding model leveraging the power of Gemini, Google's most capable large language model. Capitalizing on Gemini's inherent multilingual and code understanding capabilities, Gemini Embedding produces highly generalizable embeddings for text spanning numerous languages and textual modalities. The representations generated by Gemini Embedding can be precomputed and applied to a variety of downstream tasks including classification, similarity, clustering, ranking, and retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), which includes over one hundred tasks across 250+ languages, Gemini Embedding substantially outperforms prior state-of-the-art models, demonstrating considerable improvements in embedding quality. Achieving state-of-the-art performance across MMTEB's multilingual, English, and code benchmarks, our unified model demonstrates strong capabilities across a broad selection of tasks and surpasses specialized domain-specific models.", 'score': 19, 'issue_id': 2655, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '149f8fc31808d626', 'authors': ['Jinhyuk Lee', 'Feiyang Chen', 'Sahil Dua', 'Daniel Cer', 'Madhuri Shanbhogue', 'Iftekhar Naim', 'Gustavo Hernández Ábrego', 'Zhe Li', 'Kaifeng Chen', 'Henrique Schechter Vera', 'Xiaoqi Ren', 'Shanfeng Zhang', 'Daniel Salz', 'Michael Boratko', 'Jay Han', 'Blair Chen', 'Shuo Huang', 'Vikram Rao', 'Paul Suganthan', 'Feng Han', 'Andreas Doumanoglou', 'Nithi Gupta', 'Fedor Moiseev', 'Cathy Yip', 'Aashi Jain', 'Simon Baumgartner', 'Shahrokh Shahi', 'Frank Palma Gomez', 'Sandeep Mariserla', 'Min Choi', 'Parashar Shah', 'Sonam Goenka', 'Ke Chen', 'Ye Xia', 'Koert Chen', 'Sai Meher Karthik Duddu', 'Yichang Chen', 'Trevor Walker', 'Wenlei Zhou', 'Rakesh Ghiya', 'Zach Gleicher', 'Karan Gill', 'Zhe Dong', 'Mojtaba Seyedhosseini', 'Yunhsuan Sung', 'Raphael Hoffmann', 'Tom Duerig'], 'affiliations': ['Google'], 'pdf_title_img': 'assets/pdf/title_img/2503.07891.jpg', 'data': {'categories': ['#multimodal', '#multilingual', '#dataset', '#transfer_learning', '#benchmark', '#open_source'], 'emoji': '🌐', 'ru': {'title': 'Gemini Embedding: универсальные эмбеддинги для текста и кода на множестве языков', 'desc': 'Статья представляет Gemini Embedding - передовую модель встраивания, основанную на мощной языковой модели Gemini от Google. Модель использует многоязычные возможности и понимание кода Gemini для создания универсальных эмбеддингов текста на различных языках. Gemini Embedding превосходит предыдущие модели по результатам оценки на бенчмарке MMTEB, включающем более 100 задач на 250+ языках. Модель демонстрирует высокую эффективность в многоязычных задачах, задачах на английском языке и задачах, связанных с кодом, превосходя специализированные модели.'}, 'en': {'title': 'Unifying Multilingual Understanding with Gemini Embedding', 'desc': "Gemini Embedding is a cutting-edge embedding model that utilizes Google's advanced Gemini large language model. It excels in generating versatile embeddings for text in multiple languages and formats, thanks to its multilingual and code comprehension abilities. These embeddings can be precomputed and effectively used in various machine learning tasks such as classification, similarity, clustering, ranking, and retrieval. In evaluations on the Massive Multilingual Text Embedding Benchmark (MMTEB), Gemini Embedding significantly outperformed previous models, showcasing its superior embedding quality across a wide range of languages and tasks."}, 'zh': {'title': 'Gemini Embedding：多语言嵌入的新标杆', 'desc': '本报告介绍了Gemini Embedding，这是一种先进的嵌入模型，利用了谷歌最强大的大型语言模型Gemini的能力。Gemini Embedding能够生成高度通用的文本嵌入，适用于多种语言和文本形式，充分利用了Gemini的多语言和代码理解能力。生成的表示可以预先计算，并应用于分类、相似性、聚类、排序和检索等多种下游任务。在大规模多语言文本嵌入基准测试（MMTEB）中，Gemini Embedding显著超越了之前的最先进模型，展示了嵌入质量的显著提升。'}}}, {'id': 'https://huggingface.co/papers/2503.07572', 'title': 'Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning', 'url': 'https://huggingface.co/papers/2503.07572', 'abstract': "Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0/1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0/1 reward RL. This bonus is the ''progress'' made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL.", 'score': 18, 'issue_id': 2653, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '49ca445bc3322f0d', 'authors': ['Yuxiao Qu', 'Matthew Y. R. Yang', 'Amrith Setlur', 'Lewis Tunstall', 'Edward Emanuel Beeching', 'Ruslan Salakhutdinov', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'Hugging Face'], 'pdf_title_img': 'assets/pdf/title_img/2503.07572.jpg', 'data': {'categories': ['#optimization', '#training', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация вычислений LLM через мета-обучение с подкреплением', 'desc': "Статья представляет новый подход к оптимизации вычислений во время тестирования для улучшения рассуждений в больших языковых моделях (LLM). Авторы формализуют эту задачу как проблему мета-обучения с подкреплением, вводя понятие кумулятивного сожаления для оценки эффективности вычислений. Они разрабатывают метод Meta Reinforcement Fine-Tuning (MRT), который максимизирует плотное вознаграждение за 'прогресс' в дополнение к бинарному результату. MRT показывает значительное улучшение производительности и эффективности использования токенов в задачах математических рассуждений."}, 'en': {'title': 'Optimizing Test-Time Compute for Enhanced LLM Reasoning', 'desc': "This paper addresses the challenge of optimizing test-time compute for large language models (LLMs) to enhance their reasoning capabilities. It introduces a meta-reinforcement learning framework that treats the output generation process as a series of episodes, allowing for a structured approach to manage compute resources effectively. The authors propose minimizing cumulative regret over output tokens as a metric for evaluating the efficiency of test-time compute, which balances exploration and exploitation in the model's output. They present a new fine-tuning method called Meta Reinforcement Fine-Tuning (MRT), which significantly improves performance and token efficiency in mathematical reasoning tasks compared to traditional outcome-reward reinforcement learning methods."}, 'zh': {'title': '优化测试时计算，提升推理性能！', 'desc': '本文探讨了如何有效利用测试时计算资源来提升大型语言模型（LLM）的推理性能。我们将优化测试时计算的问题形式化为元强化学习（RL）问题，从而提供了一个系统化的视角来支配测试时计算。通过将LLM的输出流视为多个测试时的回合，并使用累积遗憾的概念来衡量测试时计算的有效性，我们提出了一种新的微调方法，称为元强化微调（MRT）。实验结果表明，MRT在数学推理任务中相较于传统的结果奖励RL方法，性能提升了2-3倍，令令牌效率提高了约1.5倍。'}}}, {'id': 'https://huggingface.co/papers/2503.07604', 'title': 'Implicit Reasoning in Transformers is Reasoning through Shortcuts', 'url': 'https://huggingface.co/papers/2503.07604', 'abstract': "Test-time compute is emerging as a new paradigm for enhancing language models' complex multi-step reasoning capabilities, as demonstrated by the success of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit reasoning in test-time compute, implicit reasoning is more inference-efficient, requiring fewer generated tokens. However, why does the advanced reasoning capability fail to emerge in the implicit reasoning style? In this work, we train GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conduct analytical experiments to investigate how language models perform implicit reasoning in multi-step tasks. Our findings reveal: 1) Language models can perform step-by-step reasoning and achieve high accuracy in both in-domain and out-of-domain tests via implicit reasoning. However, this capability only emerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning abilities emerging from training on un<PRE_TAG>fixed-pattern data</POST_TAG> tend to overfit a specific pattern and fail to generalize further. Notably, this limitation is also observed in state-of-the-art large language models. These findings suggest that language models acquire implicit reasoning through shortcut learning, enabling strong performance on tasks with similar patterns while lacking generalization.", 'score': 17, 'issue_id': 2654, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '313594788f663498', 'authors': ['Tianhe Lin', 'Jian Xie', 'Siyu Yuan', 'Deqing Yang'], 'affiliations': ['School of Computer Science, Fudan University', 'School of Data Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07604.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#math', '#data'], 'emoji': '🧠', 'ru': {'title': 'Неявные рассуждения в языковых моделях: сила шаблонов и проблемы обобщения', 'desc': 'Исследование показывает, что языковые модели способны выполнять пошаговые рассуждения и достигать высокой точности при неявном рассуждении, но только при обучении на данных с фиксированным шаблоном. При обучении на данных с нефиксированным шаблоном модели склонны переобучаться конкретному паттерну и не могут обобщать. Это ограничение наблюдается даже у современных больших языковых моделей. Результаты указывают на то, что языковые модели приобретают навыки неявного рассуждения через обучение коротким путям, что позволяет им хорошо справляться с задачами схожего паттерна, но ограничивает обобщение.'}, 'en': {'title': 'Unlocking Implicit Reasoning in Language Models', 'desc': 'This paper explores how language models, specifically GPT-2, can perform implicit reasoning in multi-step mathematical tasks. It shows that while these models can achieve high accuracy through implicit reasoning, this ability is contingent on being trained with fixed-pattern data. When trained on variable patterns, the models tend to overfit and struggle to generalize their reasoning skills. The research highlights that language models often rely on shortcut learning, which allows them to excel in specific tasks but limits their broader reasoning capabilities.'}, 'zh': {'title': '隐式推理的捷径与局限性', 'desc': '本文探讨了在测试时计算中，隐式推理与显式推理的效率差异。研究发现，语言模型在固定模式数据上训练时，能够通过隐式推理实现逐步推理并在多步任务中取得高准确率。相反，在非固定模式数据上训练的隐式推理能力容易过拟合特定模式，导致泛化能力不足。总的来说，语言模型通过捷径学习获得隐式推理能力，但在面对不同模式时表现不佳。'}}}, {'id': 'https://huggingface.co/papers/2503.08605', 'title': 'Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled\n  Sampling', 'url': 'https://huggingface.co/papers/2503.08605', 'abstract': 'While recent advancements in text-to-video diffusion models enable high-quality short video generation from a single prompt, generating real-world long videos in a single pass remains challenging due to limited data and high computational costs. To address this, several works propose tuning-free approaches, i.e., extending existing models for long video generation, specifically using multiple prompts to allow for dynamic and controlled content changes. However, these methods primarily focus on ensuring smooth transitions between adjacent frames, often leading to content drift and a gradual loss of semantic coherence over longer sequences. To tackle such an issue, we propose Synchronized Coupled Sampling (SynCoS), a novel inference framework that synchronizes denoising paths across the entire video, ensuring long-range consistency across both adjacent and distant frames. Our approach combines two complementary sampling strategies: reverse and optimization-based sampling, which ensure seamless local transitions and enforce global coherence, respectively. However, directly alternating between these samplings misaligns denoising trajectories, disrupting prompt guidance and introducing unintended content changes as they operate independently. To resolve this, SynCoS synchronizes them through a grounded timestep and a fixed baseline noise, ensuring fully coupled sampling with aligned denoising paths. Extensive experiments show that SynCoS significantly improves multi-event long video generation, achieving smoother transitions and superior long-range coherence, outperforming previous approaches both quantitatively and qualitatively.', 'score': 16, 'issue_id': 2653, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '6f7bf7b6c171af43', 'authors': ['Subin Kim', 'Seoung Wug Oh', 'Jui-Hsien Wang', 'Joon-Young Lee', 'Jinwoo Shin'], 'affiliations': ['Adobe Research', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.08605.jpg', 'data': {'categories': ['#inference', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'SynCoS: Синхронизированная генерация длинных видео с сохранением согласованности', 'desc': 'Статья представляет новый метод генерации длинных видео с использованием текстовых подсказок, названный Synchronized Coupled Sampling (SynCoS). Этот подход объединяет обратную выборку и выборку на основе оптимизации для обеспечения плавных переходов между кадрами и глобальной согласованности контента. SynCoS синхронизирует траектории шумоподавления через фиксированный временной шаг и базовый шум, что позволяет избежать нежелательных изменений содержания. Эксперименты показывают, что SynCoS значительно улучшает генерацию длинных видео с несколькими событиями, превосходя предыдущие подходы как количественно, так и качественно.'}, 'en': {'title': 'Achieving Long-Range Coherence in Video Generation with SynCoS', 'desc': 'This paper introduces Synchronized Coupled Sampling (SynCoS), a new framework for generating long videos from text prompts while maintaining semantic coherence. Traditional methods struggle with content drift and frame transitions, especially over extended sequences. SynCoS addresses these issues by synchronizing denoising paths across the video, combining reverse and optimization-based sampling strategies for better local and global coherence. Experimental results demonstrate that SynCoS enhances the quality of multi-event long video generation, outperforming existing techniques in both smoothness and consistency.'}, 'zh': {'title': '同步耦合采样：提升长视频生成的一致性', 'desc': '本文提出了一种新的推理框架，称为同步耦合采样（SynCoS），旨在解决长视频生成中的一致性问题。通过同步去噪路径，SynCoS确保了相邻帧和远程帧之间的长范围一致性。该方法结合了反向采样和基于优化的采样策略，以实现局部平滑过渡和全局一致性。实验结果表明，SynCoS在多事件长视频生成方面显著优于之前的方法，提供了更平滑的过渡和更好的长范围语义一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.08619', 'title': 'LightGen: Efficient Image Generation through Knowledge Distillation and\n  Direct Preference Optimization', 'url': 'https://huggingface.co/papers/2503.08619', 'abstract': 'Recent advances in text-to-image generation have primarily relied on extensive datasets and parameter-heavy architectures. These requirements severely limit accessibility for researchers and practitioners who lack substantial computational resources. In this paper, we introduce \\model, an efficient training paradigm for image generation models that uses knowledge distillation (KD) and Direct Preference Optimization (DPO). Drawing inspiration from the success of data KD techniques widely adopted in Multi-Modal Large Language Models (MLLMs), LightGen distills knowledge from state-of-the-art (SOTA) text-to-image models into a compact Masked Autoregressive (MAR) architecture with only 0.7B parameters. Using a compact <PRE_TAG>synthetic dataset</POST_TAG> of just 2M high-quality images generated from varied captions, we demonstrate that data diversity significantly outweighs data volume in determining model performance. This strategy dramatically reduces computational demands and reduces pre-training time from potentially thousands of GPU-days to merely 88 GPU-days. Furthermore, to address the inherent shortcomings of synthetic data, particularly poor high-frequency details and spatial inaccuracies, we integrate the DPO technique that refines image fidelity and positional accuracy. Comprehensive experiments confirm that LightGen achieves image generation quality comparable to SOTA models while significantly reducing computational resources and expanding accessibility for resource-constrained environments. Code is available at https://github.com/XianfengWu01/LightGen', 'score': 15, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '1645a4236e6d91b6', 'authors': ['Xianfeng Wu', 'Yajing Bai', 'Haoze Zheng', 'Harold Haodong Chen', 'Yexin Liu', 'Zihao Wang', 'Xuran Ma', 'Wen-Jie Shu', 'Xianzu Wu', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'The Hong Kong University of Science and Technology', 'University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2503.08619.jpg', 'data': {'categories': ['#training', '#dataset', '#synthetic', '#architecture', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений с минимальными ресурсами', 'desc': 'Эта статья представляет LightGen - эффективный метод обучения моделей генерации изображений, использующий дистилляцию знаний и Direct Preference Optimization. Модель использует компактную архитектуру с маскированной авторегрессией всего в 0.7 миллиардов параметров, обученную на синтетическом наборе данных из 2 миллионов изображений. Этот подход значительно сокращает вычислительные требования и время предобучения до 88 GPU-дней. Эксперименты показывают, что LightGen достигает качества генерации изображений, сравнимого с передовыми моделями, при существенно меньших вычислительных ресурсах.'}, 'en': {'title': 'Efficient Image Generation with LightGen: Less is More!', 'desc': 'This paper presents LightGen, a new approach to text-to-image generation that focuses on efficiency and accessibility. It utilizes knowledge distillation (KD) and Direct Preference Optimization (DPO) to create a compact model with only 0.7 billion parameters, making it less demanding on computational resources. By training on a small but diverse synthetic dataset of 2 million images, the authors show that the variety of data is more important than sheer volume for model performance. The results indicate that LightGen can produce high-quality images comparable to state-of-the-art models while significantly reducing training time and resource requirements.'}, 'zh': {'title': '高效图像生成，资源友好！', 'desc': '本文介绍了一种名为LightGen的高效图像生成模型训练范式，利用知识蒸馏（KD）和直接偏好优化（DPO）。该方法从最先进的文本到图像模型中提取知识，构建了一个仅有0.7亿参数的紧凑型自回归架构。通过使用仅200万张高质量合成图像的数据集，研究表明数据的多样性对模型性能的影响远大于数据的数量。LightGen显著降低了计算需求，将预训练时间从数千个GPU天缩短至仅88个GPU天，同时保持了与最先进模型相当的图像生成质量。'}}}, {'id': 'https://huggingface.co/papers/2503.08686', 'title': 'OmniMamba: Efficient and Unified Multimodal Understanding and Generation\n  via State Space Models', 'url': 'https://huggingface.co/papers/2503.08686', 'abstract': "Recent advancements in unified multimodal understanding and visual generation (or multimodal generation) models have been hindered by their quadratic computational complexity and dependence on large-scale training data. We present OmniMamba, the first linear-architecture-based multimodal generation model that generates both text and images through a unified next-token prediction paradigm. The model fully leverages Mamba-2's high computational and memory efficiency, extending its capabilities from text generation to multimodal generation. To address the data inefficiency of existing unified models, we propose two key innovations: (1) decoupled vocabularies to guide modality-specific generation, and (2) task-specific LoRA for parameter-efficient adaptation. Furthermore, we introduce a decoupled two-stage training strategy to mitigate data imbalance between two tasks. Equipped with these techniques, OmniMamba achieves competitive performance with JanusFlow while surpassing Show-o across benchmarks, despite being trained on merely 2M image-text pairs, which is 1,000 times fewer than Show-o. Notably, OmniMamba stands out with outstanding inference efficiency, achieving up to a 119.2 times speedup and 63% GPU memory reduction for long-sequence generation compared to Transformer-based counterparts. Code and models are released at https://github.com/hustvl/OmniMamba", 'score': 13, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7bb3cd796d69dc2c', 'authors': ['Jialv Zou', 'Bencheng Liao', 'Qian Zhang', 'Wenyu Liu', 'Xinggang Wang'], 'affiliations': ['Horizon Robotics', 'Institute of Artificial Intelligence, Huazhong University of Science & Technology', 'School of EIC, Huazhong University of Science & Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.08686.jpg', 'data': {'categories': ['#training', '#inference', '#multimodal', '#architecture', '#open_source', '#optimization'], 'emoji': '🦋', 'ru': {'title': 'OmniMamba: Эффективная мультимодальная генерация с линейной архитектурой', 'desc': 'OmniMamba - это первая мультимодальная модель генерации, основанная на линейной архитектуре, которая генерирует как текст, так и изображения через единую парадигму предсказания следующего токена. Модель использует высокую вычислительную эффективность Mamba-2, расширяя ее возможности от генерации текста до мультимодальной генерации. Для решения проблемы неэффективности данных существующих унифицированных моделей, авторы предлагают разделенные словари и задаче-специфичную LoRA для адаптации с малым количеством параметров. OmniMamba достигает конкурентоспособной производительности, будучи обученной всего на 2 миллионах пар изображение-текст, что в 1000 раз меньше, чем у аналогов.'}, 'en': {'title': 'OmniMamba: Efficient Multimodal Generation with Linear Architecture', 'desc': 'OmniMamba is a groundbreaking multimodal generation model that efficiently creates both text and images using a linear architecture. It introduces a unified next-token prediction approach, which significantly reduces computational complexity and memory usage compared to traditional models. The model employs innovative techniques like decoupled vocabularies for specific modality guidance and task-specific LoRA for efficient parameter adaptation. With a unique two-stage training strategy, OmniMamba achieves impressive performance on benchmarks while requiring far less training data, demonstrating remarkable speed and memory efficiency during inference.'}, 'zh': {'title': 'OmniMamba：高效的多模态生成新选择', 'desc': 'OmniMamba是一种新型的多模态生成模型，采用线性架构，能够同时生成文本和图像。该模型通过统一的下一个标记预测范式，充分利用了Mamba-2的高效计算和内存性能。为了提高数据利用效率，OmniMamba引入了解耦词汇和任务特定的LoRA技术，并采用了两阶段的训练策略来解决任务间的数据不平衡问题。最终，OmniMamba在生成效率上表现出色，相比于传统的Transformer模型，推理速度提高了119.2倍，GPU内存减少了63%。'}}}, {'id': 'https://huggingface.co/papers/2503.08644', 'title': 'Exploiting Instruction-Following Retrievers for Malicious Information\n  Retrieval', 'url': 'https://huggingface.co/papers/2503.08644', 'abstract': 'Instruction-following retrievers have been widely adopted alongside LLMs in real-world applications, but little work has investigated the safety risks surrounding their increasing search capabilities. We empirically study the ability of retrievers to satisfy malicious queries, both when used directly and when used in a retrieval augmented generation-based setup. Concretely, we investigate six leading retrievers, including NV-Embed and LLM2Vec, and find that given malicious requests, most retrievers can (for >50% of queries) select relevant harmful passages. For example, LLM2Vec correctly selects passages for 61.35% of our malicious queries. We further uncover an emerging risk with instruction-following retrievers, where highly relevant harmful information can be surfaced by exploiting their instruction-following capabilities. Finally, we show that even safety-aligned LLMs, such as Llama3, can satisfy malicious requests when provided with harmful retrieved passages in-context. In summary, our findings underscore the malicious misuse risks associated with increasing retriever capability.', 'score': 12, 'issue_id': 2666, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '4526f13d2d98134f', 'authors': ['Parishad BehnamGhader', 'Nicholas Meade', 'Siva Reddy'], 'affiliations': ['Canada CIFAR AI Chair', 'McGill University', 'Mila Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.08644.jpg', 'data': {'categories': ['#multimodal', '#security', '#rag', '#ethics'], 'emoji': '🕵️', 'ru': {'title': 'Скрытые угрозы в умных поисковых системах', 'desc': 'Исследование посвящено анализу безопасности ретриверов, используемых вместе с языковыми моделями. Авторы обнаружили, что многие ретриверы способны удовлетворять вредоносные запросы, выбирая релевантные опасные отрывки текста. Выявлен риск, связанный с возможностью манипулирования инструкциями для ретриверов. Даже модели с повышенной безопасностью могут выдавать вредоносный контент при наличии опасных фрагментов в контексте.'}, 'en': {'title': 'Uncovering the Risks of Powerful Retrievers in Malicious Queries', 'desc': 'This paper investigates the safety risks of instruction-following retrievers used with large language models (LLMs) in real-world applications. The authors analyze six prominent retrievers, revealing that they can often retrieve harmful content in response to malicious queries, with LLM2Vec achieving a 61.35% success rate. Additionally, the study highlights a concerning trend where instruction-following capabilities can inadvertently expose users to dangerous information. The findings emphasize the need for caution as the capabilities of retrievers grow, as even safety-aligned LLMs can inadvertently assist in fulfilling harmful requests.'}, 'zh': {'title': '检索器能力提升带来的安全隐患', 'desc': '本文研究了指令跟随检索器在处理恶意查询时的安全风险。我们分析了六种领先的检索器，包括NV-Embed和LLM2Vec，发现大多数检索器在超过50%的恶意查询中能够选择相关的有害内容。特别是，LLM2Vec在61.35%的恶意查询中正确选择了有害段落。此外，即使是安全对齐的LLM，如Llama3，在提供有害检索段落的情况下也能满足恶意请求。'}}}, {'id': 'https://huggingface.co/papers/2503.06940', 'title': 'CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic\n  Audiovisual Narrative Processing', 'url': 'https://huggingface.co/papers/2503.06940', 'abstract': "In this paper, we introduce CineBrain, the first large-scale dataset featuring simultaneous EEG and fMRI recordings during dynamic audiovisual stimulation. Recognizing the complementary strengths of EEG's high temporal resolution and fMRI's deep-brain spatial coverage, CineBrain provides approximately six hours of narrative-driven content from the popular television series The Big Bang Theory for each of six participants. Building upon this unique dataset, we propose CineSync, an innovative multimodal decoding framework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural Latent Decoder. Our approach effectively fuses EEG and fMRI signals, significantly improving the reconstruction quality of complex audiovisual stimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a comprehensive evaluation protocol that assesses reconstructions across semantic and perceptual dimensions. Experimental results demonstrate that CineSync achieves state-of-the-art video reconstruction performance and highlight our initial success in combining fMRI and EEG for reconstructing both video and audio stimuli. Project Page: https://jianxgao.github.io/CineBrain.", 'score': 11, 'issue_id': 2664, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '10b773e3c142acbe', 'authors': ['Jianxiong Gao', 'Yichang Liu', 'Baofeng Yang', 'Jianfeng Feng', 'Yanwei Fu'], 'affiliations': ['Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06940.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#diffusion', '#dataset', '#video', '#audio'], 'emoji': '🧠', 'ru': {'title': 'Чтение мыслей: от нейронных сигналов к видео и звуку', 'desc': 'CineBrain - это новый крупномасштабный набор данных, объединяющий записи ЭЭГ и фМРТ во время просмотра аудиовизуального контента. На основе этого датасета авторы разработали CineSync - мультимодальный фреймворк для декодирования нейронных сигналов, использующий диффузионную модель. CineSync эффективно объединяет сигналы ЭЭГ и фМРТ, значительно улучшая качество реконструкции сложных аудиовизуальных стимулов. Результаты экспериментов показывают, что CineSync достигает наилучших результатов в реконструкции видео и открывает перспективы для восстановления как видео, так и аудио стимулов из нейронных сигналов.'}, 'en': {'title': 'CineBrain: Uniting EEG and fMRI for Enhanced Audiovisual Reconstruction', 'desc': 'This paper presents CineBrain, a groundbreaking dataset that combines EEG and fMRI recordings while participants watch dynamic audiovisual content. By leveraging the fast response time of EEG and the detailed spatial information from fMRI, CineBrain offers a rich resource for studying brain activity during complex stimuli. The authors introduce CineSync, a novel framework that merges these two modalities to enhance the reconstruction of audiovisual experiences. Their results show that CineSync outperforms previous methods in reconstructing video and audio, marking a significant advancement in multimodal brain signal analysis.'}, 'zh': {'title': 'CineBrain：多模态解码的新突破', 'desc': '本文介绍了CineBrain，这是第一个大规模的数据集，包含在动态视听刺激下同时记录的EEG和fMRI数据。CineBrain利用EEG的高时间分辨率和fMRI的深脑空间覆盖优势，为六名参与者提供了约六小时的《生活大爆炸》叙事内容。基于这一独特数据集，我们提出了CineSync，这是一种创新的多模态解码框架，结合了多模态融合编码器和基于扩散的神经潜在解码器。实验结果表明，CineSync在视频重建性能上达到了最先进的水平，成功结合了fMRI和EEG来重建视频和音频刺激。'}}}, {'id': 'https://huggingface.co/papers/2503.07587', 'title': 'Robusto-1 Dataset: Comparing Humans and VLMs on real out-of-distribution\n  Autonomous Driving VQA from Peru', 'url': 'https://huggingface.co/papers/2503.07587', 'abstract': 'As multimodal foundational models start being deployed experimentally in Self-Driving cars, a reasonable question we ask ourselves is how similar to humans do these systems respond in certain driving situations -- especially those that are out-of-distribution? To study this, we create the Robusto-1 dataset that uses dashcam video data from Peru, a country with one of the worst (aggressive) drivers in the world, a high traffic index, and a high ratio of bizarre to non-bizarre street objects likely never seen in training. In particular, to preliminarly test at a cognitive level how well Foundational Visual Language Models (VLMs) compare to Humans in Driving, we move away from bounding boxes, segmentation maps, occupancy maps or trajectory estimation to multi-modal Visual Question Answering (VQA) comparing both humans and machines through a popular method in systems neuroscience known as Representational Similarity Analysis (RSA). Depending on the type of questions we ask and the answers these systems give, we will show in what cases do VLMs and Humans converge or diverge allowing us to probe on their cognitive alignment. We find that the degree of alignment varies significantly depending on the type of questions asked to each type of system (Humans vs VLMs), highlighting a gap in their alignment.', 'score': 9, 'issue_id': 2667, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '274531da9b4c33d1', 'authors': ['Dunant Cusipuma', 'David Ortega', 'Victor Flores-Benites', 'Arturo Deza'], 'affiliations': ['Artificio', 'Universidad de Ingeneria Tecnologia (UTEC)'], 'pdf_title_img': 'assets/pdf/title_img/2503.07587.jpg', 'data': {'categories': ['#video', '#alignment', '#interpretability', '#multimodal', '#dataset'], 'emoji': '🚗', 'ru': {'title': 'Человек vs ИИ: кто лучше понимает необычные ситуации на дороге?', 'desc': 'Статья описывает исследование, сравнивающее реакции мультимодальных фундаментальных моделей и людей в необычных ситуациях вождения. Авторы создали датасет Robusto-1 на основе видео с видеорегистраторов в Перу, стране с агрессивным стилем вождения. Используя метод анализа репрезентативного сходства (RSA), они сравнили ответы визуально-языковых моделей (VLM) и людей на вопросы о дорожных ситуациях. Результаты показали, что степень согласованности между моделями и людьми значительно варьируется в зависимости от типа задаваемых вопросов.'}, 'en': {'title': 'Assessing Human-Like Responses in Self-Driving AI', 'desc': 'This paper investigates how well multimodal foundational models, specifically Visual Language Models (VLMs), perform in driving situations compared to humans, particularly in challenging environments like Peru. The authors introduce the Robusto-1 dataset, which includes dashcam footage from a region known for aggressive driving and unusual street objects. They employ a method called Visual Question Answering (VQA) and Representational Similarity Analysis (RSA) to assess cognitive alignment between humans and VLMs. The findings reveal that the alignment between human responses and VLMs varies significantly based on the types of questions posed, indicating a notable gap in their cognitive processing.'}, 'zh': {'title': '探索自动驾驶中的人机认知差距', 'desc': '本研究探讨了多模态基础模型在自动驾驶汽车中的应用，特别是在复杂驾驶场景下的表现。我们创建了Robusto-1数据集，使用来自秘鲁的行车记录仪视频数据，以测试基础视觉语言模型（VLMs）与人类在驾驶中的认知水平。通过多模态视觉问答（VQA）方法，我们比较了人类和机器的反应，并使用表征相似性分析（RSA）来评估它们的认知一致性。研究发现，不同类型的问题会导致VLMs和人类在反应上的显著差异，揭示了它们之间的认知差距。'}}}, {'id': 'https://huggingface.co/papers/2503.08307', 'title': '^RFLAV: Rolling Flow matching for infinite Audio Video generation', 'url': 'https://huggingface.co/papers/2503.08307', 'abstract': 'Joint audio-video (AV) generation is still a significant challenge in generative AI, primarily due to three critical requirements: quality of the generated samples, seamless multimodal synchronization and temporal coherence, with audio tracks that match the visual data and vice versa, and limitless video duration. In this paper, we present , a novel transformer-based architecture that addresses all the key challenges of AV generation. We explore three distinct cross modality interaction modules, with our lightweight temporal fusion module emerging as the most effective and computationally efficient approach for aligning audio and visual modalities. Our experimental results demonstrate that  outperforms existing state-of-the-art models in multimodal AV generation tasks. Our code and checkpoints are available at https://github.com/ErgastiAlex/R-FLAV.', 'score': 7, 'issue_id': 2660, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '5b49ac0313e68c7e', 'authors': ['Alex Ergasti', 'Giuseppe Gabriele Tarollo', 'Filippo Botti', 'Tomaso Fontanini', 'Claudio Ferrari', 'Massimo Bertozzi', 'Andrea Prati'], 'affiliations': ['University of Parma, Department of Engineering and Architecture. Parma, Italy', 'University of Siena, Department of Information engineering and mathematics. Siena, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.08307.jpg', 'data': {'categories': ['#video', '#architecture', '#multimodal', '#audio'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации аудио-видео контента с помощью трансформеров', 'desc': 'Статья представляет , новую архитектуру на основе трансформеров для совместной генерации аудио и видео. Модель решает ключевые проблемы AV-генерации: качество сэмплов, синхронизацию модальностей и временную согласованность. Авторы исследуют три модуля взаимодействия между модальностями, выделяя легковесный модуль временного слияния как наиболее эффективный. Экспериментальные результаты показывают, что  превосходит существующие SOTA-модели в задачах мультимодальной AV-генерации.'}, 'en': {'title': 'Transforming Audio-Video Generation with Efficient Alignment', 'desc': 'This paper addresses the challenges of joint audio-video generation in generative AI, focusing on quality, synchronization, and temporal coherence. The authors introduce a novel transformer-based architecture that effectively aligns audio and visual data. They explore three cross modality interaction modules, highlighting a lightweight temporal fusion module as the most efficient solution. Experimental results show that their approach surpasses existing state-of-the-art models in multimodal AV generation tasks.'}, 'zh': {'title': '突破音频-视频生成的关键挑战', 'desc': '本论文探讨了联合音频-视频生成的挑战，主要包括生成样本的质量、多模态的无缝同步和时间一致性。我们提出了一种新型的基于变换器的架构，旨在解决这些关键问题。研究中，我们探索了三种不同的跨模态交互模块，其中轻量级的时间融合模块被证明是对齐音频和视觉模态的最有效和计算高效的方法。实验结果表明，我们的方法在多模态音频-视频生成任务中优于现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.08685', 'title': '"Principal Components" Enable A New Language of Images', 'url': 'https://huggingface.co/papers/2503.08685', 'abstract': 'We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.', 'score': 6, 'issue_id': 2654, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'a013cfdc2d1e9d7c', 'authors': ['Xin Wen', 'Bingchen Zhao', 'Ismail Elezi', 'Jiankang Deng', 'Xiaojuan Qi'], 'affiliations': ['Imperial College London', 'Noahs Ark Lab', 'University of Edinburgh', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.08685.jpg', 'data': {'categories': ['#training', '#cv', '#interpretability', '#diffusion', '#architecture', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'Структурированная визуальная токенизация: ПК-анализ для изображений', 'desc': 'Авторы представляют новый метод визуальной токенизации, который встраивает доказуемую PCA-подобную структуру в пространство латентных токенов. Этот подход генерирует одномерную причинную последовательность токенов для изображений, где каждый последующий токен вносит неперекрывающуюся информацию с математически гарантированной убывающей объясненной дисперсией. Метод также решает проблему связи семантического содержания и спектральных деталей в токенах с помощью диффузионного декодера. Эксперименты показывают, что этот подход достигает современного уровня производительности реконструкции и обеспечивает лучшую интерпретируемость.'}, 'en': {'title': 'Enhancing Visual Tokenization with Structured Latent Spaces', 'desc': "This paper presents a new visual tokenization framework that incorporates a PCA-like structure into the latent token space. Unlike traditional visual tokenizers that focus mainly on how well they can recreate images, this method emphasizes the importance of the latent space's structure for better interpretability and performance in other tasks. The framework produces a sequence of tokens that each provide unique information, ensuring that the most important visual features are captured first, similar to how principal component analysis works. Additionally, the authors address a problem where high-level semantic information and low-level details become mixed in the tokens, leading to improved clarity and efficiency in training auto-regressive models."}, 'zh': {'title': '创新视觉标记化，提升可解释性与性能', 'desc': '我们提出了一种新颖的视觉标记化框架，将可证明的主成分分析（PCA）结构嵌入潜在标记空间。现有的视觉标记器主要优化重建精度，但往往忽视潜在空间的结构特性，这对可解释性和下游任务至关重要。我们的方法为图像生成一维因果标记序列，每个后续标记提供不重叠的信息，并且具有数学上保证的递减解释方差，类似于主成分分析。实验表明，我们的方法在重建性能上达到了最先进的水平，并提高了与人类视觉系统的对齐可解释性。'}}}, {'id': 'https://huggingface.co/papers/2503.08588', 'title': 'BiasEdit: Debiasing Stereotyped Language Models via Model Editing', 'url': 'https://huggingface.co/papers/2503.08588', 'abstract': "Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or directly alter the models' biased internal representations. To address these issues, we propose BiasEdit, an efficient model editing method to remove stereotypical bias from language models through lightweight networks that act as editors to generate parameter updates. BiasEdit employs a debiasing loss guiding editor networks to conduct local edits on partial parameters of a language model for debiasing while preserving the language modeling abilities during editing through a retention loss. Experiments on StereoSet and Crows-Pairs demonstrate the effectiveness, efficiency, and robustness of BiasEdit in eliminating bias compared to tangental debiasing baselines and little to no impact on the language models' general capabilities. In addition, we conduct bias tracing to probe bias in various modules and explore bias editing impacts on different components of language models.", 'score': 6, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '183584887772b6e7', 'authors': ['Xin Xu', 'Wei Xu', 'Ningyu Zhang', 'Julian McAuley'], 'affiliations': ['Georgia Institute of Technology', 'University of California, San Diego', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08588.jpg', 'data': {'categories': ['#hallucinations', '#ethics', '#architecture', '#data', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Эффективное устранение предубеждений в языковых моделях без потери их возможностей', 'desc': 'Статья представляет метод BiasEdit для устранения стереотипных предубеждений в языковых моделях. В отличие от существующих методов дебиасинга, BiasEdit использует легковесные сети-редакторы для генерации обновлений параметров модели. Метод применяет функцию потерь для дебиасинга, направляющую редакторы на локальное редактирование частичных параметров, одновременно сохраняя языковые способности модели. Эксперименты показывают эффективность BiasEdit в устранении предубеждений без существенного влияния на общие возможности языковых моделей.'}, 'en': {'title': 'BiasEdit: Efficiently Editing Bias in Language Models', 'desc': "This paper introduces BiasEdit, a novel method designed to reduce stereotypical biases in language models. Unlike traditional debiasing techniques that often fail to effectively alter biased representations, BiasEdit utilizes lightweight networks to make targeted updates to the model's parameters. It incorporates a debiasing loss to guide these edits while ensuring that the model's language processing abilities remain intact through a retention loss. The results show that BiasEdit is not only effective in reducing bias but also maintains the overall performance of the language models across various tasks."}, 'zh': {'title': '高效去偏见，提升语言模型公正性', 'desc': '本研究提出了一种名为BiasEdit的模型编辑方法，旨在通过轻量级网络去除语言模型中的刻板偏见。与传统的去偏见策略相比，BiasEdit能够高效地进行局部参数编辑，同时保持语言模型的能力。该方法使用去偏见损失指导编辑网络进行参数更新，并通过保留损失确保编辑过程中的语言建模能力不受影响。实验结果表明，BiasEdit在消除偏见方面表现出色，且对语言模型的整体能力影响较小。'}}}, {'id': 'https://huggingface.co/papers/2503.08684', 'title': 'Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents', 'url': 'https://huggingface.co/papers/2503.08684', 'abstract': 'Previous studies have found that PLM-based retrieval models exhibit a preference for LLM-generated content, assigning higher relevance scores to these documents even when their semantic quality is comparable to human-written ones. This phenomenon, known as source bias, threatens the sustainable development of the information access ecosystem. However, the underlying causes of source bias remain unexplored. In this paper, we explain the process of information retrieval with a causal graph and discover that PLM-based retrievers learn perplexity features for relevance estimation, causing source bias by ranking the documents with low perplexity higher. Theoretical analysis further reveals that the phenomenon stems from the positive correlation between the gradients of the loss functions in language modeling task and retrieval task. Based on the analysis, a causal-inspired inference-time debiasing method is proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses the bias effect of the perplexity and then separates the bias effect from the overall estimated relevance score. Experimental results across three domains demonstrate the superior debiasing effectiveness of CDC, emphasizing the validity of our proposed explanatory framework. Source codes are available at https://github.com/WhyDwelledOnAi/Perplexity-Trap.', 'score': 5, 'issue_id': 2663, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '13216e7922903487', 'authors': ['Haoyu Wang', 'Sunhao Dai', 'Haiyuan Zhao', 'Liang Pang', 'Xiao Zhang', 'Gang Wang', 'Zhenhua Dong', 'Jun Xu', 'Ji-Rong Wen'], 'affiliations': ['CAS Key Laboratory of AI Safety, Institute of Computing Technology, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China', 'Huawei Noahs Ark Lab, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.08684.jpg', 'data': {'categories': ['#ethics', '#data', '#interpretability', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Преодоление предвзятости поисковых систем к ИИ-контенту', 'desc': 'Исследование посвящено проблеме предвзятости моделей информационного поиска на основе предобученных языковых моделей (PLM) в пользу контента, сгенерированного большими языковыми моделями (LLM). Авторы объясняют этот феномен с помощью причинно-следственного графа и обнаруживают, что PLM-ретриверы используют перплексию для оценки релевантности, что приводит к предпочтению документов с низкой перплексией. Теоретический анализ показывает, что это связано с положительной корреляцией между градиентами функций потерь в задачах языкового моделирования и поиска. На основе этого анализа авторы предлагают метод дебиасинга во время вывода, называемый Causal Diagnosis and Correction (CDC).'}, 'en': {'title': 'Unraveling Source Bias in PLM Retrieval Models', 'desc': 'This paper investigates the issue of source bias in PLM-based retrieval models, where documents generated by large language models (LLMs) are favored over human-written content. The authors use a causal graph to explain how these models learn to estimate relevance based on perplexity features, leading to a preference for documents with lower perplexity scores. They identify a correlation between the gradients of loss functions in language modeling and retrieval tasks as a root cause of this bias. To address this, they propose a new method called Causal Diagnosis and Correction (CDC), which effectively debiases the relevance scores by isolating the bias introduced by perplexity.'}, 'zh': {'title': '揭示源偏差，提升信息检索准确性', 'desc': '本研究探讨了基于预训练语言模型（PLM）的检索模型中存在的源偏差现象。研究发现，这种偏差使得模型更倾向于为大型语言模型（LLM）生成的内容分配更高的相关性评分，即使这些内容的语义质量与人类撰写的内容相当。通过因果图分析信息检索过程，揭示了模型在相关性估计中学习到的困惑度特征导致了源偏差。为了解决这一问题，提出了一种名为因果诊断与修正（CDC）的去偏方法，能够有效分离困惑度的偏差效应，提升检索模型的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.07699', 'title': 'RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories', 'url': 'https://huggingface.co/papers/2503.07699', 'abstract': "Diffusion models have achieved remarkable success across various domains. However, their slow generation speed remains a critical challenge. Existing acceleration methods, while aiming to reduce steps, often compromise sample quality, controllability, or introduce training complexities. Therefore, we propose RayFlow, a novel diffusion framework that addresses these limitations. Unlike previous methods, RayFlow guides each sample along a unique path towards an instance-specific target distribution. This method minimizes sampling steps while preserving generation diversity and stability. Furthermore, we introduce Time Sampler, an importance sampling technique to enhance training efficiency by focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's superiority in generating high-quality images with improved speed, control, and training efficiency compared to existing acceleration techniques.", 'score': 5, 'issue_id': 2656, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '49ebba2cb63d91f8', 'authors': ['Huiyang Shao', 'Xin Xia', 'Yuhong Yang', 'Yuxi Ren', 'Xing Wang', 'Xuefeng Xiao'], 'affiliations': ['ByteDance Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2503.07699.jpg', 'data': {'categories': ['#cv', '#diffusion', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'RayFlow: быстрая и качественная генерация изображений с помощью диффузионных моделей', 'desc': 'RayFlow - это новая система диффузионных моделей, которая решает проблему медленной генерации изображений. Она направляет каждый сэмпл по уникальному пути к индивидуальному целевому распределению, минимизируя количество шагов без ущерба для качества и разнообразия. RayFlow также включает метод Time Sampler для повышения эффективности обучения путем фокусировки на ключевых временных шагах. Эксперименты показывают превосходство RayFlow над существующими методами ускорения в скорости, контроле и эффективности обучения.'}, 'en': {'title': 'RayFlow: Accelerating Diffusion Models Without Compromise', 'desc': 'This paper introduces RayFlow, a new diffusion model framework designed to improve the speed of image generation without sacrificing quality. Unlike traditional methods that often reduce the number of steps at the cost of sample diversity or stability, RayFlow customizes the sampling path for each instance, ensuring a more efficient and effective generation process. Additionally, the authors present Time Sampler, an importance sampling technique that optimizes training by prioritizing key timesteps. Experimental results show that RayFlow outperforms existing methods in generating high-quality images more quickly and with better control.'}, 'zh': {'title': 'RayFlow：加速扩散模型的新方法', 'desc': '扩散模型在多个领域取得了显著成功，但其生成速度较慢仍然是一个关键挑战。现有的加速方法虽然旨在减少步骤，但往往会影响样本质量、可控性或增加训练复杂性。为此，我们提出了RayFlow，这是一种新颖的扩散框架，能够解决这些限制。RayFlow通过引导每个样本沿着独特路径朝向特定目标分布，最小化采样步骤，同时保持生成的多样性和稳定性。'}}}, {'id': 'https://huggingface.co/papers/2503.05860', 'title': 'Benchmarking AI Models in Software Engineering: A Review, Search Tool,\n  and Enhancement Protocol', 'url': 'https://huggingface.co/papers/2503.05860', 'abstract': "Benchmarks are essential for consistent evaluation and reproducibility. The integration of Artificial Intelligence into Software Engineering (AI4SE) has given rise to numerous benchmarks for tasks such as code generation and bug fixing. However, this surge presents challenges: (1) scattered benchmark knowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3) the absence of a uniform standard for benchmark development, and (4) limitations of existing benchmarks. In this paper, we review 173 studies and identify 204 AI4SE benchmarks. We classify these benchmarks, analyze their limitations, and expose gaps in practices. Based on our review, we created BenchScout, a semantic search tool to find relevant benchmarks, using automated clustering of the contexts from associated studies. We conducted a user study with 22 participants to evaluate BenchScout's usability, effectiveness, and intuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5. To advance benchmarking standards, we propose BenchFrame, a unified method to enhance benchmark quality. As a case study, we applied BenchFrame to the HumanEval benchmark and addressed its main limitations. This led to <PRE_TAG>HumanEvalNext</POST_TAG>, featuring (1) corrected errors, (2) improved language conversion, (3) expanded test coverage, and (4) increased difficulty. We then evaluated ten state-of-the-art code language models on HumanEval, HumanEvalPlus, and <PRE_TAG>HumanEvalNext</POST_TAG>. On <PRE_TAG>HumanEvalNext</POST_TAG>, models showed a pass@1 score reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus, respectively.", 'score': 5, 'issue_id': 2661, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'dee962dca6de084b', 'authors': ['Roham Koohestani', 'Philippe de Bekker', 'Maliheh Izadi'], 'affiliations': ['Delft University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.05860.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#survey'], 'emoji': '🧪', 'ru': {'title': 'Совершенствование бенчмарков AI4SE: от анализа к инновациям', 'desc': 'Статья посвящена анализу и улучшению бенчмарков в области интеграции искусственного интеллекта в программную инженерию (AI4SE). Авторы рассмотрели 173 исследования и выявили 204 бенчмарка AI4SE, классифицировав их и проанализировав ограничения. На основе обзора был создан инструмент семантического поиска BenchScout для поиска релевантных бенчмарков. Предложен унифицированный метод BenchFrame для повышения качества бенчмарков, который был применен к бенчмарку HumanEval, что привело к созданию улучшенной версии HumanEvalNext.'}, 'en': {'title': 'Enhancing AI4SE Benchmarking with BenchScout and BenchFrame', 'desc': 'This paper addresses the challenges of evaluating benchmarks in the integration of Artificial Intelligence into Software Engineering (AI4SE). It reviews 173 studies to identify and classify 204 benchmarks, highlighting their limitations and gaps in current practices. The authors introduce BenchScout, a semantic search tool that helps users find relevant benchmarks through automated clustering. Additionally, they propose BenchFrame, a unified method to improve benchmark quality, demonstrated through the enhancement of the HumanEval benchmark into HumanEvalNext, which features significant improvements in error correction and test coverage.'}, 'zh': {'title': '提升基准测试质量的统一方法', 'desc': '本论文探讨了人工智能在软件工程中的基准测试（AI4SE）的重要性，分析了173项研究并识别出204个基准。我们发现现有基准存在知识分散、选择困难、缺乏统一标准和局限性等问题。为了解决这些问题，我们开发了BenchScout，一个语义搜索工具，帮助用户找到相关基准，并提出了BenchFrame，一个统一的方法来提升基准质量。通过对HumanEval基准的案例研究，我们创建了HumanEvalNext，修正了错误、改善了语言转换、扩展了测试覆盖率并增加了难度。'}}}, {'id': 'https://huggingface.co/papers/2503.08689', 'title': 'QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long\n  Video Comprehension', 'url': 'https://huggingface.co/papers/2503.08689', 'abstract': 'Recent advances in long video understanding typically mitigate visual redundancy through visual token pruning based on attention distribution. However, while existing methods employ post-hoc low-response token pruning in decoder layers, they overlook the input-level semantic correlation between visual tokens and instructions (query). In this paper, we propose QuoTA, an ante-hoc training-free modular that extends existing large video-language models (LVLMs) for visual token assignment based on query-oriented frame-level importance assessment. The query-oriented token selection is crucial as it aligns visual processing with task-specific requirements, optimizing token budget utilization while preserving semantically relevant content. Specifically, (i) QuoTA strategically allocates frame-level importance scores based on query relevance, enabling one-time visual token assignment before cross-modal interactions in decoder layers, (ii) we decouple the query through Chain-of-Thoughts reasoning to facilitate more precise LVLM-based frame importance scoring, and (iii) QuoTA offers a plug-and-play functionality that extends to existing LVLMs. Extensive experimental results demonstrate that implementing QuoTA with LLaVA-Video-7B yields an average performance improvement of 3.2% across six benchmarks (including Video-MME and MLVU) while operating within an identical visual token budget as the baseline. Codes are open-sourced at https://github.com/MAC-AutoML/QuoTA.', 'score': 4, 'issue_id': 2655, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7dbb5f0edfd69aad', 'authors': ['Yongdong Luo', 'Wang Chen', 'Xiawu Zheng', 'Weizhong Huang', 'Shukang Yin', 'Haojia Lin', 'Chaoyou Fu', 'Jinfa Huang', 'Jiayi Ji', 'Jiebo Luo', 'Rongrong Ji'], 'affiliations': ['Nanjing University', 'University of Rochester', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08689.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#long_context', '#architecture', '#video', '#benchmark', '#open_source', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'Умный отбор кадров для эффективного понимания видео ИИ', 'desc': 'Статья представляет QuoTA - новый модуль для улучшения понимания длинных видео большими видео-языковыми моделями (LVLM). QuoTA выполняет отбор визуальных токенов на основе оценки важности кадров с учетом запроса пользователя, что позволяет более эффективно использовать ограниченный бюджет токенов. Метод включает декомпозицию запроса через рассуждения по цепочке мыслей для более точной оценки важности кадров. Эксперименты показывают, что применение QuoTA с LLaVA-Video-7B дает среднее улучшение производительности на 3.2% по шести бенчмаркам при том же бюджете визуальных токенов.'}, 'en': {'title': 'Optimizing Visual Token Selection for Better Video Understanding', 'desc': 'This paper introduces QuoTA, a new method for improving long video understanding by optimizing how visual tokens are selected based on their relevance to specific queries. Unlike previous approaches that prune tokens after processing, QuoTA assesses the importance of visual tokens before they are used, ensuring that only the most relevant information is retained. By using Chain-of-Thoughts reasoning, QuoTA enhances the accuracy of frame importance scoring, allowing for better alignment between visual data and task requirements. The results show that QuoTA can significantly boost performance in existing large video-language models while maintaining the same token budget.'}, 'zh': {'title': '优化视觉标记分配，提升视频理解性能', 'desc': '本文提出了一种名为QuoTA的模块，旨在改进长视频理解中的视觉标记分配。与现有方法不同，QuoTA在输入层面上考虑视觉标记与查询之间的语义关联，从而优化标记的使用效率。通过基于查询相关性的帧级重要性评估，QuoTA能够在解码器层之前进行一次性视觉标记分配。实验结果表明，QuoTA在多个基准测试中显著提高了性能，同时保持了与基线相同的视觉标记预算。'}}}, {'id': 'https://huggingface.co/papers/2503.08507', 'title': 'Referring to Any Person', 'url': 'https://huggingface.co/papers/2503.08507', 'abstract': 'Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given a natural language description, a task we define as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, a novel dataset designed to tackle these challenges and better reflect real-world applications. From a model design perspective, we integrate a multimodal large language model with an object detection framework, constructing a robust referring model named RexSeek. Experimental results reveal that state-of-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEA-Research/RexSeek', 'score': 4, 'issue_id': 2661, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '905fc0de3c82fe2e', 'authors': ['Qing Jiang', 'Lin Wu', 'Zhaoyang Zeng', 'Tianhe Ren', 'Yuda Xiong', 'Yihao Chen', 'Qin Liu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.08507.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#games', '#optimization', '#interpretability', '#architecture', '#cv', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Революция в поиске людей: от текста к изображению', 'desc': 'Статья представляет новый подход к задаче поиска людей по текстовому описанию в компьютерном зрении. Авторы предлагают датасет HumanRef, который лучше отражает реальные сценарии применения, чем существующие бенчмарки. Они также разрабатывают модель RexSeek, объединяющую мультимодальную языковую модель с детектором объектов. Эксперименты показывают, что RexSeek превосходит современные модели на новом датасете и хорошо обобщается на задачу поиска обычных объектов.'}, 'en': {'title': 'Revolutionizing Person Detection with HumanRef and RexSeek', 'desc': 'This paper addresses the challenge of detecting individuals in computer vision using natural language descriptions, a task known as referring to any person. The authors highlight that existing models often fall short in real-world scenarios and current benchmarks are too narrow, focusing mainly on one-to-one referring. They propose a new dataset called HumanRef, which is designed to better represent the complexities of real-world applications. Additionally, they introduce a new model, RexSeek, which combines a multimodal large language model with an object detection framework, showing improved performance in both human and object referring tasks.'}, 'zh': {'title': '人类识别的新突破：RexSeek模型', 'desc': '本论文探讨了计算机视觉中人类个体识别的重要性，提出了一种新的任务定义——根据自然语言描述识别个体。现有模型在实际应用中表现不佳，且现有基准测试主要集中在一对一的识别上，限制了该领域的发展。为了解决这些问题，作者设计了一个新的数据集HumanRef，并提出了一个名为RexSeek的多模态模型，能够有效识别多个个体。实验结果表明，RexSeek在识别人类和常见物体方面表现优异，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.08417', 'title': 'AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2503.08417', 'abstract': "Despite recent advancements in learning-based motion in-betweening, a key limitation has been overlooked: the requirement for character-specific datasets. In this work, we introduce AnyMoLe, a novel method that addresses this limitation by leveraging video diffusion models to generate motion in-between frames for arbitrary characters without external data. Our approach employs a two-stage frame generation process to enhance contextual understanding. Furthermore, to bridge the domain gap between real-world and rendered character animations, we introduce ICAdapt, a fine-tuning technique for video diffusion models. Additionally, we propose a ``motion-video mimicking'' optimization technique, enabling seamless motion generation for characters with arbitrary joint structures using 2D and 3D-aware features. AnyMoLe significantly reduces data dependency while generating smooth and realistic transitions, making it applicable to a wide range of motion in-betweening tasks.", 'score': 4, 'issue_id': 2662, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '7610945506fac90e', 'authors': ['Kwan Yun', 'Seokhyeon Hong', 'Chaelin Kim', 'Junyong Noh'], 'affiliations': ['KAIST, Visual Media Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.08417.jpg', 'data': {'categories': ['#3d', '#training', '#diffusion', '#optimization', '#video', '#dataset'], 'emoji': '🎭', 'ru': {'title': 'Универсальная анимация без специальных данных', 'desc': "AnyMoLe - это новый метод генерации промежуточных кадров анимации для произвольных персонажей без использования специфических наборов данных. Он основан на видео-диффузионных моделях и использует двухэтапный процесс генерации кадров для улучшения понимания контекста. Метод включает технику доводки ICAdapt для преодоления разрыва между реальными и рендерингованными анимациями. Также предложена оптимизация 'motion-video mimicking' для создания плавных движений персонажей с произвольной структурой суставов."}, 'en': {'title': 'AnyMoLe: Motion Generation Without Character-Specific Data', 'desc': "This paper presents AnyMoLe, a new method for generating motion in-between frames for various characters without needing specific datasets. It utilizes video diffusion models in a two-stage frame generation process to improve the understanding of context in animations. The authors introduce ICAdapt, a technique that fine-tunes these models to better connect real-world and animated character movements. Additionally, a 'motion-video mimicking' optimization is proposed, allowing for fluid motion generation across different character joint structures, thus minimizing data dependency and enhancing the realism of transitions."}, 'zh': {'title': 'AnyMoLe：无数据依赖的角色运动插值新方法', 'desc': '本文介绍了一种新方法AnyMoLe，旨在解决基于学习的运动插值中对特定角色数据集的依赖问题。该方法利用视频扩散模型生成任意角色的帧间运动，无需外部数据。我们采用了两阶段的帧生成过程，以增强上下文理解，并引入了ICAdapt技术来缩小真实世界与渲染角色动画之间的领域差距。此外，我们提出了“运动视频模仿”优化技术，使得使用2D和3D特征的任意关节结构角色的运动生成变得无缝。'}}}, {'id': 'https://huggingface.co/papers/2503.06492', 'title': 'VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large\n  Vision-Language Models in Fact-Seeking Question Answering', 'url': 'https://huggingface.co/papers/2503.06492', 'abstract': 'Large vision-language models (LVLMs) have demonstrated remarkable achievements, yet the generation of non-factual responses remains prevalent in fact-seeking question answering (QA). Current multimodal fact-seeking benchmarks primarily focus on comparing model outputs to ground truth answers, providing limited insights into the performance of modality-specific modules. To bridge this gap, we introduce VisualSimpleQA, a multimodal fact-seeking benchmark with two key features. First, it enables streamlined and decoupled evaluation of LVLMs in visual and linguistic modalities. Second, it incorporates well-defined difficulty criteria to guide human annotation and facilitates the extraction of a challenging subset, <PRE_TAG>VisualSimpleQA-hard</POST_TAG>. Experiments on 15 LVLMs show that even state-of-the-art models such as GPT-4o achieve merely 60%+ correctness in multimodal fact-seeking QA on VisualSimpleQA and 30%+ on <PRE_TAG>VisualSimpleQA-hard</POST_TAG>. Furthermore, the decoupled evaluation across these models highlights substantial opportunities for improvement in both visual and linguistic modules. The dataset is available at https://huggingface.co/datasets/WYLing/VisualSimpleQA.', 'score': 4, 'issue_id': 2659, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '5f5a7152d7773f38', 'authors': ['Yanling Wang', 'Yihan Zhao', 'Xiaodong Chen', 'Shasha Guo', 'Lixin Liu', 'Haoyang Li', 'Yong Xiao', 'Jing Zhang', 'Qi Li', 'Ke Xu'], 'affiliations': ['Renmin University of China', 'Tencent', 'Tsinghua University', 'Zhongguancun Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.06492.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#hallucinations', '#multimodal', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'VisualSimpleQA: новый способ оценки мультимодальных моделей', 'desc': 'Статья представляет новый бенчмарк VisualSimpleQA для оценки мультимодальных моделей в задаче ответов на фактологические вопросы по изображениям. Бенчмарк позволяет отдельно оценивать визуальные и языковые модули крупных визуально-языковых моделей (LVLM). В нем также есть усложненная версия VisualSimpleQA-hard для более тщательного тестирования. Эксперименты показали, что даже современные модели вроде GPT-4o достигают лишь 60% точности на VisualSimpleQA и 30% на сложной версии.'}, 'en': {'title': 'Enhancing Accuracy in Multimodal Fact-Seeking with VisualSimpleQA', 'desc': 'This paper addresses the issue of non-factual responses in large vision-language models (LVLMs) during fact-seeking question answering (QA). It introduces VisualSimpleQA, a new benchmark that allows for separate evaluation of visual and linguistic capabilities of these models. The benchmark includes specific difficulty levels to aid in human annotation and identifies a challenging subset called VisualSimpleQA-hard. Experiments reveal that even advanced models like GPT-4o struggle with accuracy, achieving only around 60% correctness overall and 30% on the harder subset, indicating significant room for improvement in both visual and linguistic processing.'}, 'zh': {'title': '提升多模态问答的准确性', 'desc': '大型视觉语言模型（LVLMs）在事实寻求问答中取得了显著成就，但仍然存在生成不准确回答的问题。当前的多模态事实寻求基准主要关注模型输出与真实答案的比较，缺乏对特定模态模块性能的深入分析。为了解决这个问题，我们提出了VisualSimpleQA，这是一个具有两个关键特征的多模态事实寻求基准。通过简化和解耦的评估方式，VisualSimpleQA能够更好地评估LVLMs在视觉和语言模态上的表现，并引入明确的难度标准以指导人工标注。'}}}, {'id': 'https://huggingface.co/papers/2502.18858', 'title': 'Evaluating Intelligence via Trial and Error', 'url': 'https://huggingface.co/papers/2502.18858', 'abstract': "Intelligence is a crucial trait for species to find solutions within a limited number of trial-and-error attempts. Building on this idea, we introduce Survival Game as a framework to evaluate intelligence based on the number of failed attempts in a trial-and-error process. Fewer failures indicate higher intelligence. When the expectation and variance of failure counts are both finite, it signals the ability to consistently find solutions to new challenges, which we define as the Autonomous Level of intelligence. Using Survival Game, we comprehensively evaluate existing AI systems. Our results show that while AI systems achieve the Autonomous Level in simple tasks, they are still far from it in more complex tasks, such as vision, search, recommendation, and language. While scaling current AI technologies might help, this would come at an astronomical cost. Projections suggest that achieving the Autonomous Level for general tasks would require 10^{26} parameters. To put this into perspective, loading such a massive model requires so many H100 GPUs that their total value is 10^{7} times that of Apple Inc.'s market value. Even with Moore's Law, supporting such a parameter scale would take 70 years. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI technologies. To further investigate this phenomenon, we conduct a theoretical analysis of Survival Game and its experimental results. Our findings suggest that human tasks possess a criticality property. As a result, Autonomous Level requires a deep understanding of the task's underlying mechanisms. Current AI systems, however, do not fully grasp these mechanisms and instead rely on superficial mimicry, making it difficult for them to reach an autonomous level. We believe Survival Game can not only guide the future development of AI but also offer profound insights into human intelligence.", 'score': 4, 'issue_id': 2656, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '54797794006daa5e', 'authors': ['Jingtao Zhan', 'Jiahao Zhao', 'Jiayu Li', 'Yiqun Liu', 'Bo Zhang', 'Qingyao Ai', 'Jiaxin Mao', 'Hongning Wang', 'Min Zhang', 'Shaoping Ma'], 'affiliations': ['Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.18858.jpg', 'data': {'categories': ['#reasoning', '#training', '#agents', '#agi', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Автономный интеллект: путь через Survival Game', 'desc': 'Статья представляет концепцию Survival Game для оценки интеллекта на основе количества неудачных попыток в процессе проб и ошибок. Авторы вводят понятие Автономного Уровня интеллекта и оценивают существующие системы искусственного интеллекта по этому критерию. Результаты показывают, что для достижения Автономного Уровня в сложных задачах потребуется огромное количество параметров модели, что экономически нецелесообразно. Исследование выявляет, что человеческие задачи обладают свойством критичности, требующим глубокого понимания механизмов, которого не хватает современным системам ИИ.'}, 'en': {'title': 'Measuring Intelligence: The Survival Game Framework', 'desc': 'This paper introduces the Survival Game framework to measure intelligence based on the number of failed attempts in problem-solving. It defines the Autonomous Level of intelligence as the ability to find solutions with fewer failures, indicating a deeper understanding of tasks. The study evaluates existing AI systems, revealing that while they perform well on simple tasks, they struggle with complex ones like vision and language. The findings suggest that achieving a high Autonomous Level in AI would require an impractical scale of parameters, highlighting the limitations of current technologies and the complexity of human-like intelligence.'}, 'zh': {'title': '生存游戏：评估智能的新框架', 'desc': '本论文提出了一种名为生存游戏的框架，用于评估智能水平，特别是在试错过程中失败次数的多少。失败次数越少，表示智能水平越高。研究表明，尽管现有的人工智能系统在简单任务中达到了自主水平，但在复杂任务（如视觉、搜索、推荐和语言处理）中仍然远未达到。我们认为，生存游戏不仅可以指导未来的人工智能发展，还能深入理解人类智能的本质。'}}}, {'id': 'https://huggingface.co/papers/2503.06594', 'title': 'Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation', 'url': 'https://huggingface.co/papers/2503.06594', 'abstract': 'The field of neural machine translation (NMT) has changed with the advent of large language models (<PRE_TAG>LLMs)</POST_TAG>. Much of the recent emphasis in natural language processing (NLP) has been on modeling machine translation and many other problems using a single pre-trained <PRE_TAG>Transformer decoder</POST_TAG>, while encoder-decoder architectures, which were the standard in earlier NMT models, have received relatively less attention. In this paper, we explore translation models that are universal, efficient, and easy to optimize, by marrying the world of LLMs with the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder unchanged. We also develop methods for adapting LLMs to work better with the NMT decoder. Furthermore, we construct a new dataset involving multiple tasks to assess how well the machine translation system generalizes across various tasks. Evaluations on the WMT and our datasets show that results using our method match or surpass a range of baselines in terms of translation quality, but achieve 2.4 sim 6.5 times inference speedups and a 75% reduction in the memory footprint of the KV cache. It also demonstrates strong generalization across a variety of translation-related tasks.', 'score': 3, 'issue_id': 2660, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'bea0df269ed71c2d', 'authors': ['Yingfeng Luo', 'Tong Zheng', 'Yongyu Mu', 'Bei Li', 'Qinghong Zhang', 'Yongqi Gao', 'Ziqiang Xu', 'Peinan Feng', 'Xiaoqian Liu', 'Tong Xiao', 'Jingbo Zhu'], 'affiliations': ['NLP Lab, Northeastern University, Shenyang, China', 'NiuTrans Research, Shenyang, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.06594.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#dataset', '#inference', '#training'], 'emoji': '🚀', 'ru': {'title': 'Объединение мощи LLM и эффективности NMT для универсального машинного перевода', 'desc': 'Статья рассматривает применение больших языковых моделей (LLM) в нейронном машинном переводе (NMT). Авторы предлагают использовать LLM для кодирования в NMT, оставляя декодер NMT без изменений. Разработаны методы адаптации LLM для лучшей работы с декодером NMT. Эксперименты показывают, что предложенный подход не уступает baseline-моделям по качеству перевода, но обеспечивает ускорение вывода в 2.4-6.5 раз и снижение объема памяти KV-кэша на 75%.'}, 'en': {'title': 'Revolutionizing Translation: Merging LLMs with NMT for Efficiency and Quality', 'desc': 'This paper discusses advancements in neural machine translation (NMT) by integrating large language models (LLMs) with traditional NMT architectures. The authors propose a novel approach that utilizes LLMs for encoding while maintaining the existing NMT decoder, enhancing efficiency and optimization. They introduce a new dataset to evaluate the generalization of their translation models across multiple tasks. Results indicate that their method not only improves translation quality but also significantly increases inference speed and reduces memory usage.'}, 'zh': {'title': '结合LLMs与NMT，提升翻译效率与质量', 'desc': '本文探讨了如何将大型语言模型（LLMs）与神经机器翻译（NMT）结合，以提高翻译模型的通用性和效率。我们采用LLMs进行NMT编码，同时保持NMT解码器不变，并开发了适应LLMs与NMT解码器更好配合的方法。通过构建一个包含多任务的新数据集，我们评估了机器翻译系统在不同任务上的泛化能力。实验结果表明，我们的方法在翻译质量上与多种基线相当或更优，同时在推理速度上提高了2.4到6.5倍，KV缓存的内存占用减少了75%。'}}}, {'id': 'https://huggingface.co/papers/2503.08478', 'title': 'NullFace: Training-Free Localized Face Anonymization', 'url': 'https://huggingface.co/papers/2503.08478', 'abstract': "Privacy concerns around ever increasing number of cameras are increasing in today's digital age. Although existing anonymization methods are able to obscure identity information, they often struggle to preserve the utility of the images. In this work, we introduce a training-free method for face anonymization that preserves key non-identity-related attributes. Our approach utilizes a pre-trained text-to-image diffusion model without requiring optimization or training. It begins by inverting the input image to recover its initial noise. The noise is then denoised through an identity-conditioned diffusion process, where modified identity embeddings ensure the anonymized face is distinct from the original identity. Our approach also supports localized anonymization, giving users control over which facial regions are anonymized or kept intact. Comprehensive evaluations against state-of-the-art methods show our approach excels in anonymization, attribute preservation, and image quality. Its flexibility, robustness, and practicality make it well-suited for real-world applications. Code and data can be found at https://github.com/hanweikung/nullface .", 'score': 2, 'issue_id': 2659, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '150c9c8854f08130', 'authors': ['Han-Wei Kung', 'Tuomas Varanka', 'Terence Sim', 'Nicu Sebe'], 'affiliations': ['National University of Singapore', 'University of Oulu', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.08478.jpg', 'data': {'categories': ['#ethics', '#cv', '#training', '#diffusion'], 'emoji': '🎭', 'ru': {'title': 'Анонимизация лиц без потери ключевых атрибутов', 'desc': 'Статья представляет новый метод анонимизации лиц на изображениях, не требующий обучения. Метод использует предобученную диффузионную модель для генерации текста по изображению, начиная с инвертирования входного изображения для восстановления исходного шума. Затем происходит процесс удаления шума с условием по идентичности, где модифицированные эмбеддинги идентичности обеспечивают отличие анонимизированного лица от оригинала. Подход позволяет сохранить ключевые атрибуты, не связанные с идентичностью, и превосходит современные методы по качеству анонимизации и сохранению атрибутов.'}, 'en': {'title': 'Effortless Face Anonymization with Preserved Attributes', 'desc': 'This paper presents a novel method for face anonymization that does not require any training, making it efficient and practical. It leverages a pre-trained text-to-image diffusion model to anonymize faces while preserving important non-identity-related features. The method works by inverting the input image to retrieve its initial noise, which is then processed through a diffusion model that modifies identity embeddings. Additionally, it allows users to selectively anonymize specific facial regions, ensuring flexibility and control over the anonymization process.'}, 'zh': {'title': '无训练的面部匿名化方法，保护隐私与图像质量并存', 'desc': '在数字时代，越来越多的摄像头引发了隐私问题。现有的匿名化方法虽然可以模糊身份信息，但往往难以保持图像的实用性。本文提出了一种无训练的面部匿名化方法，能够保留关键的非身份相关属性。该方法利用预训练的文本到图像扩散模型，通过身份条件扩散过程去噪声，确保匿名化的面孔与原始身份明显不同，同时支持局部匿名化，用户可以控制哪些面部区域被匿名化或保留。'}}}, {'id': 'https://huggingface.co/papers/2503.08102', 'title': 'AI-native Memory 2.0: Second Me', 'url': 'https://huggingface.co/papers/2503.08102', 'abstract': 'Human interaction with the external world fundamentally involves the exchange of personal memory, whether with other individuals, websites, applications, or, in the future, AI agents. A significant portion of this interaction is redundant, requiring users to repeatedly provide the same information across different contexts. Existing solutions, such as browser-stored credentials, autofill mechanisms, and unified authentication systems, have aimed to mitigate this redundancy by serving as intermediaries that store and retrieve commonly used user data. The advent of large language models (LLMs) presents an opportunity to redefine memory management through an AI-native paradigm: SECOND ME. SECOND ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge. By serving as an intermediary in user interactions, it can autonomously generate context-aware responses, prefill required information, and facilitate seamless communication with external systems, significantly reducing cognitive load and interaction friction. Unlike traditional memory storage solutions, SECOND ME extends beyond static data retention by leveraging LLM-based memory parameterization. This enables structured organization, contextual reasoning, and adaptive knowledge retrieval, facilitating a more systematic and intelligent approach to memory management. As AI-driven personal agents like SECOND ME become increasingly integrated into digital ecosystems, SECOND ME further represents a critical step toward augmenting human-world interaction with persistent, contextually aware, and self-optimizing memory systems. We have open-sourced the fully localizable deployment system at GitHub: https://github.com/Mindverse/Second-Me.', 'score': 2, 'issue_id': 2658, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '45b3f3aee8d173f9', 'authors': ['Jiale Wei', 'Xiang Ying', 'Tao Gao', 'Felix Tao', 'Jingbo Shang'], 'affiliations': ['Mindverse.ai'], 'pdf_title_img': 'assets/pdf/title_img/2503.08102.jpg', 'data': {'categories': ['#multimodal', '#agi', '#agents', '#optimization', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'SECOND ME: ИИ-ассистент для расширения вашей памяти', 'desc': 'SECOND ME - это система, использующая большие языковые модели для управления личной памятью пользователя. Она хранит, организует и динамически применяет пользовательские знания, выступая посредником во взаимодействиях. SECOND ME может автономно генерировать контекстно-зависимые ответы, заполнять формы и облегчать коммуникацию с внешними системами. В отличие от традиционных решений, система использует параметризацию памяти на основе нейросетей для более интеллектуального подхода к управлению информацией.'}, 'en': {'title': 'Revolutionizing Memory Management with AI: Meet SECOND ME!', 'desc': 'The paper introduces SECOND ME, an AI-driven memory management system designed to enhance user interactions with digital environments. It addresses the issue of redundant information sharing by autonomously storing and organizing user-specific knowledge, allowing for context-aware responses and prefilled information. Unlike traditional memory solutions, SECOND ME utilizes large language models to enable dynamic knowledge retrieval and contextual reasoning. This innovative approach aims to reduce cognitive load and improve the efficiency of human-AI interactions in various applications.'}, 'zh': {'title': '智能记忆管理，提升人机互动体验', 'desc': '这篇论文介绍了一个名为SECOND ME的智能记忆管理系统，旨在减少用户在与外部世界互动时的冗余信息输入。通过利用大型语言模型（LLMs），SECOND ME能够智能地存储、组织和动态使用用户特定的知识。与传统的记忆存储解决方案不同，SECOND ME不仅仅是静态数据的保留，而是通过上下文推理和自适应知识检索来优化用户体验。随着AI驱动的个人代理的普及，SECOND ME代表了增强人类与世界互动的重要一步。'}}}, {'id': 'https://huggingface.co/papers/2503.05066', 'title': 'Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of\n  Experts', 'url': 'https://huggingface.co/papers/2503.05066', 'abstract': 'The Mixture of Experts (MoE) is an effective architecture for scaling large language models by leveraging sparse expert activation, optimizing the trade-off between performance and efficiency. However, under expert parallelism, MoE suffers from inference inefficiencies due to imbalanced token-to-expert assignment, where some experts are overloaded while others remain underutilized. This imbalance leads to poor resource utilization and increased latency, as the most burdened expert dictates the overall delay, a phenomenon we define as the \\textit{Straggler Effect}. To mitigate this, we propose Capacity-Aware Inference, including two key techniques: (1) \\textit{Capacity-Aware Token Drop}, which discards overloaded tokens to regulate the maximum latency of MoE, and (2) \\textit{Capacity-Aware Token Reroute}, which reallocates overflowed tokens to underutilized experts, balancing the token distribution. These techniques collectively optimize both high-load and low-load expert utilization, leading to a more efficient MoE inference pipeline. Extensive experiments demonstrate the effectiveness of our methods, showing significant improvements in inference efficiency, e.g., 0.2\\% average performance increase and a 1.94times inference speedup on Mixtral-8times7B-Instruct.', 'score': 2, 'issue_id': 2672, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'febfae347962e18e', 'authors': ['Shwai He', 'Weilin Cai', 'Jiayi Huang', 'Ang Li'], 'affiliations': ['The Hong Kong University of Science and Technology (Guangzhou)', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.05066.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Балансировка экспертов для ускорения языковых моделей', 'desc': 'Статья посвящена оптимизации архитектуры Mixture of Experts (MoE) для крупных языковых моделей. Авторы предлагают метод Capacity-Aware Inference для решения проблемы неравномерного распределения токенов между экспертами, что приводит к неэффективному использованию ресурсов и увеличению задержки. Метод включает две техники: Capacity-Aware Token Drop для отбрасывания перегруженных токенов и Capacity-Aware Token Reroute для перераспределения токенов между недогруженными экспертами. Эксперименты показали значительное улучшение эффективности вывода, включая 0.2% увеличение производительности и 1.94-кратное ускорение вывода на модели Mixtral-8x7B-Instruct.'}, 'en': {'title': 'Optimizing Expert Utilization for Efficient Inference in MoE Models', 'desc': 'The paper discusses the Mixture of Experts (MoE) architecture, which enhances large language models by using sparse expert activation to balance performance and efficiency. It identifies a problem called the Straggler Effect, where some experts are overloaded while others are underused, leading to inefficiencies during inference. To address this, the authors introduce Capacity-Aware Inference techniques, including Capacity-Aware Token Drop and Capacity-Aware Token Reroute, which help manage token distribution among experts. Their experiments show that these methods significantly improve inference efficiency, achieving a notable speedup and slight performance gain.'}, 'zh': {'title': '提升混合专家推理效率的创新方法', 'desc': '混合专家（MoE）是一种有效的架构，用于通过稀疏专家激活来扩展大型语言模型，优化性能与效率之间的权衡。然而，在专家并行处理下，MoE由于不平衡的令牌到专家的分配而面临推理效率低下的问题，导致某些专家过载而其他专家未被充分利用。这种不平衡导致资源利用率低下和延迟增加，因为最繁忙的专家决定了整体延迟，这种现象被称为“滞后效应”。为了解决这个问题，我们提出了容量感知推理，包括两个关键技术：容量感知令牌丢弃和容量感知令牌重定向，从而优化高负载和低负载专家的利用率，提升MoE推理管道的效率。'}}}, {'id': 'https://huggingface.co/papers/2503.07639', 'title': 'Mixture of Experts Made Intrinsically Interpretable', 'url': 'https://huggingface.co/papers/2503.07639', 'abstract': 'Neurons in large language models often exhibit polysemanticity, simultaneously encoding multiple unrelated concepts and obscuring interpretability. Instead of relying on post-hoc methods, we present MoE-X, a Mixture-of-Experts (MoE) language model designed to be intrinsically interpretable. Our approach is motivated by the observation that, in language models, wider networks with <PRE_TAG>sparse activations</POST_TAG> are more likely to capture interpretable factors. However, directly training such large sparse networks is computationally prohibitive. MoE architectures offer a scalable alternative by activating only a subset of experts for any given input, inherently aligning with interpretability objectives. In MoE-X, we establish this connection by rewriting the MoE layer as an equivalent sparse, large MLP. This approach enables efficient scaling of the hidden size while maintaining sparsity. To further enhance interpretability, we enforce sparse activation within each expert and redesign the routing mechanism to prioritize experts with the highest activation sparsity. These designs ensure that only the most salient features are routed and processed by the experts. We evaluate MoE-X on chess and natural language tasks, showing that it achieves performance comparable to dense models while significantly improving interpretability. MoE-X achieves a perplexity better than GPT-2, with interpretability surpassing even sparse autoencoder (SAE)-based approaches.', 'score': 2, 'issue_id': 2653, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '7e9a13248a2692b5', 'authors': ['Xingyi Yang', 'Constantin Venhoff', 'Ashkan Khakzar', 'Christian Schroeder de Witt', 'Puneet K. Dokania', 'Adel Bibi', 'Philip Torr'], 'affiliations': ['National University of Singapore', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.07639.jpg', 'data': {'categories': ['#training', '#games', '#architecture', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'MoE-X: Интерпретируемые языковые модели через разреженные экспертные системы', 'desc': 'Статья представляет MoE-X - новую архитектуру языковой модели, основанную на принципе Mixture-of-Experts. Эта модель разработана для повышения интерпретируемости нейронных сетей путем использования разреженных активаций и селективного задействования экспертов. MoE-X переписывает слой MoE как эквивалентную разреженную большую многослойную перцептронную сеть, что позволяет эффективно масштабировать скрытый размер при сохранении разреженности. Результаты экспериментов показывают, что MoE-X достигает производительности, сравнимой с плотными моделями, при значительно улучшенной интерпретируемости.'}, 'en': {'title': 'MoE-X: Enhancing Interpretability in Language Models with Sparse Activations', 'desc': 'This paper introduces MoE-X, a Mixture-of-Experts language model that aims to improve interpretability in large language models by utilizing sparse activations. The authors argue that wider networks with sparse activations can better capture distinct concepts, making them more interpretable. MoE-X efficiently scales by activating only a subset of experts for each input, which aligns with the goal of enhancing interpretability. The model is evaluated on chess and natural language tasks, demonstrating performance on par with dense models while offering superior interpretability compared to existing methods.'}, 'zh': {'title': 'MoE-X：可解释的混合专家语言模型', 'desc': '在大型语言模型中，神经元常常表现出多义性，同时编码多个无关的概念，导致可解释性差。我们提出了MoE-X，这是一种混合专家（MoE）语言模型，旨在内在上具有可解释性。我们的研究表明，具有稀疏激活的宽网络更有可能捕捉可解释的因素。通过激活仅一部分专家，MoE架构提供了一种可扩展的替代方案，从而在保持稀疏性的同时实现高效的隐藏层规模。'}}}, {'id': 'https://huggingface.co/papers/2503.07565', 'title': 'Inductive Moment Matching', 'url': 'https://huggingface.co/papers/2503.07565', 'abstract': 'Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Inductive Moment Matching (IMM), a new class of generative models for one- or few-step sampling with a single-stage training procedure. Unlike distillation, IMM does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, IMM guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID using only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98 on CIFAR-10 for a model trained from scratch.', 'score': 1, 'issue_id': 2670, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'ada37e03407170a1', 'authors': ['Linqi Zhou', 'Stefano Ermon', 'Jiaming Song'], 'affiliations': ['Luma AI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07565.jpg', 'data': {'categories': ['#inference', '#cv', '#training', '#diffusion', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'IMM: Быстрая и стабильная генерация изображений', 'desc': 'Статья представляет новый класс генеративных моделей под названием Inductive Moment Matching (IMM). IMM позволяет генерировать высококачественные образцы за один или несколько шагов, используя одноэтапную процедуру обучения. В отличие от моделей дистилляции, IMM не требует предварительного обучения и оптимизации двух сетей. Модель превосходит диффузионные модели на ImageNet-256x256 с показателем FID 1.99 при использовании всего 8 шагов вывода.'}, 'en': {'title': 'Fast and Stable Sampling with Inductive Moment Matching', 'desc': 'Inductive Moment Matching (IMM) is a new type of generative model designed to produce high-quality samples quickly, addressing the slow inference times of diffusion models and Flow Matching. Unlike traditional distillation methods, IMM simplifies the training process by eliminating the need for pre-training and the optimization of multiple networks. It also ensures stability and convergence at the distribution level, making it robust against changes in hyperparameters and model architectures. IMM demonstrates superior performance, achieving a low FID score on ImageNet and CIFAR-10 with significantly fewer inference steps compared to existing models.'}, 'zh': {'title': '归纳时刻匹配：高效稳定的生成模型', 'desc': '本文提出了一种新的生成模型，称为归纳时刻匹配（IMM），旨在解决扩散模型和流匹配在推理时速度慢的问题。IMM 允许在单阶段训练过程中进行一到少步采样，而无需预训练和优化两个网络。与一致性模型不同，IMM 确保了分布级别的收敛性，并在各种超参数和标准模型架构下保持稳定。实验结果表明，IMM 在 ImageNet-256x256 数据集上以 8 次推理步骤达到了 1.99 的 FID，且在 CIFAR-10 数据集上以 2 次推理步骤达到了 1.98 的最先进 FID。'}}}, {'id': 'https://huggingface.co/papers/2503.07154', 'title': 'Ideas in Inference-time Scaling can Benefit Generative Pre-training\n  Algorithms', 'url': 'https://huggingface.co/papers/2503.07154', 'abstract': "Recent years have seen significant advancements in foundation models through generative pre-training, yet algorithmic innovation in this space has largely stagnated around autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation creates a bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence. We argue that an inference-first perspective, which prioritizes scaling efficiency during inference time across sequence length and refinement steps, can inspire novel generative pre-training algorithms. Using Inductive Moment Matching (IMM) as a concrete example, we demonstrate how addressing limitations in diffusion models' inference process through targeted modifications yields a stable, single-stage algorithm that achieves superior sample quality with over an order of magnitude greater inference efficiency.", 'score': 1, 'issue_id': 2671, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '2c3a8d77ccb8e716', 'authors': ['Jiaming Song', 'Linqi Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.07154.jpg', 'data': {'categories': ['#training', '#architecture', '#diffusion', '#inference', '#optimization', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'Новый взгляд на генеративное предобучение: приоритет эффективности вывода', 'desc': "В статье обсуждается стагнация в развитии алгоритмов для фундаментальных моделей, несмотря на прогресс в генеративном предобучении. Авторы предлагают подход 'inference-first', фокусирующийся на эффективности вывода для многомодальных данных. На примере метода Inductive Moment Matching (IMM) демонстрируется, как модификации процесса вывода диффузионных моделей могут привести к созданию стабильного одноэтапного алгоритма. Этот алгоритм обеспечивает более высокое качество сэмплов при значительно большей эффективности вывода."}, 'en': {'title': 'Unlocking Multi-Modal Intelligence with Efficient Inference', 'desc': 'This paper discusses the current limitations in generative pre-training models, particularly focusing on autoregressive and diffusion models. It highlights how these limitations hinder the effective use of multi-modal data, which is essential for advancing multimodal intelligence. The authors propose an inference-first approach that emphasizes improving efficiency during the inference phase, which can lead to innovative generative algorithms. They introduce Inductive Moment Matching (IMM) as a method to enhance diffusion models, resulting in a more efficient and higher quality sampling process.'}, 'zh': {'title': '推理优先，激发生成预训练新算法', 'desc': '近年来，基础模型通过生成预训练取得了显著进展，但在自回归模型和扩散模型的算法创新方面停滞不前。这种停滞造成了瓶颈，限制了多模态数据的潜力，进而影响了多模态智能的发展。我们提出了一种以推理为先的视角，强调在推理时间内的规模效率，以激发新的生成预训练算法。通过归纳时刻匹配（IMM）作为具体例子，我们展示了如何通过针对性修改扩散模型的推理过程，得到一种稳定的单阶段算法，显著提高样本质量和推理效率。'}}}, {'id': 'https://huggingface.co/papers/2503.08037', 'title': 'ObjectMover: Generative Object Movement with Video Prior', 'url': 'https://huggingface.co/papers/2503.08037', 'abstract': 'Simple as it seems, moving an object to another location within an image is, in fact, a challenging image-editing task that requires re-harmonizing the lighting, adjusting the pose based on perspective, accurately filling occluded regions, and ensuring coherent synchronization of shadows and reflections while maintaining the object identity. In this paper, we present ObjectMover, a generative model that can perform object movement in highly challenging scenes. Our key insight is that we model this task as a sequence-to-sequence problem and fine-tune a video generation model to leverage its knowledge of consistent object generation across video frames. We show that with this approach, our model is able to adjust to complex real-world scenarios, handling extreme lighting harmonization and object effect movement. As large-scale data for object movement are unavailable, we construct a data generation pipeline using a modern game engine to synthesize high-quality data pairs. We further propose a multi-task learning strategy that enables training on real-world video data to improve the model generalization. Through extensive experiments, we demonstrate that ObjectMover achieves outstanding results and adapts well to real-world scenarios.', 'score': 0, 'issue_id': 2667, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '821b6b6ca9c0eb61', 'authors': ['Xin Yu', 'Tianyu Wang', 'Soo Ye Kim', 'Paul Guerrero', 'Xi Chen', 'Qing Liu', 'Zhe Lin', 'Xiaojuan Qi'], 'affiliations': ['Adobe Research', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.08037.jpg', 'data': {'categories': ['#games', '#data', '#training', '#synthetic', '#video', '#optimization', '#dataset'], 'emoji': '🔄', 'ru': {'title': 'ObjectMover: ИИ для реалистичного перемещения объектов на изображениях', 'desc': 'Статья представляет ObjectMover - генеративную модель для перемещения объектов на изображениях. Модель использует подход sequence-to-sequence и дообучает модель генерации видео для согласованного перемещения объектов. Для обучения используются синтетические данные из игрового движка и мультизадачное обучение на реальных видео. ObjectMover успешно справляется со сложными сценариями, включая гармонизацию освещения и согласование эффектов объекта.'}, 'en': {'title': 'Seamless Object Movement in Images with ObjectMover', 'desc': "This paper introduces ObjectMover, a generative model designed to move objects within images while addressing challenges like lighting harmonization and perspective adjustments. The authors treat the task as a sequence-to-sequence problem, utilizing a fine-tuned video generation model to ensure consistent object representation across frames. To overcome the lack of large-scale data for object movement, they create a data generation pipeline using a game engine to produce high-quality training pairs. Additionally, a multi-task learning strategy is proposed to enhance the model's performance on real-world video data, leading to impressive results in complex scenarios."}, 'zh': {'title': 'ObjectMover：智能物体移动的解决方案', 'desc': '本文介绍了一种名为ObjectMover的生成模型，旨在解决图像中物体移动的复杂任务。该模型将物体移动视为序列到序列的问题，并对视频生成模型进行微调，以利用其在视频帧间一致生成物体的知识。由于缺乏大规模的物体移动数据，我们使用现代游戏引擎构建数据生成管道，合成高质量的数据对。此外，我们提出了一种多任务学习策略，以便在真实视频数据上进行训练，从而提高模型的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.05037', 'title': 'Collapse of Dense Retrievers: Short, Early, and Literal Biases\n  Outranking Factual Evidence', 'url': 'https://huggingface.co/papers/2503.05037', 'abstract': "Dense retrieval models are commonly used in Information Retrieval (IR) applications, such as Retrieval-Augmented Generation (RAG). Since they often serve as the first step in these systems, their robustness is critical to avoid failures. In this work, by repurposing a relation extraction dataset (e.g. Re-DocRED), we design controlled experiments to quantify the impact of heuristic biases, such as favoring shorter documents, in retrievers like Dragon+ and Contriever. Our findings reveal significant vulnerabilities: retrievers often rely on superficial patterns like over-prioritizing document beginnings, shorter documents, repeated entities, and literal matches. Additionally, they tend to overlook whether the document contains the query's answer, lacking deep semantic understanding. Notably, when multiple biases combine, models exhibit catastrophic performance degradation, selecting the answer-containing document in less than 3% of cases over a biased document without the answer. Furthermore, we show that these biases have direct consequences for downstream applications like RAG, where retrieval-preferred documents can mislead LLMs, resulting in a 34% performance drop than not providing any documents at all.", 'score': 0, 'issue_id': 2667, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '862c1dc027b05f42', 'authors': ['Mohsen Fayyaz', 'Ali Modarressi', 'Hinrich Schuetze', 'Nanyun Peng'], 'affiliations': ['CIS, LMU Munich', 'Munich Center for Machine Learning', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.05037.jpg', 'data': {'categories': ['#benchmark', '#security', '#rag', '#interpretability', '#dataset'], 'emoji': '🕵️', 'ru': {'title': 'Уязвимости моделей плотного поиска: почему эвристики могут подвести', 'desc': 'Это исследование посвящено уязвимостям моделей плотного поиска, часто используемых в информационном поиске и генерации с дополнением извлечения. Авторы провели эксперименты, показывающие, что такие модели как Dragon+ и Contriever подвержены эвристическим искажениям, например, предпочтению более коротких документов. Выявлено, что модели часто опираются на поверхностные паттерны, игнорируя глубокий семантический смысл. Эти проблемы могут приводить к катастрофическому падению производительности в нисходящих задачах, таких как RAG.'}, 'en': {'title': 'Uncovering Biases in Dense Retrieval Models for Better Information Retrieval', 'desc': 'This paper investigates the weaknesses of dense retrieval models used in Information Retrieval, particularly in systems like Retrieval-Augmented Generation (RAG). By analyzing a relation extraction dataset, the authors conduct experiments to identify how heuristic biases, such as favoring shorter documents, affect the performance of retrievers like Dragon+ and Contriever. The results show that these models often depend on superficial patterns and fail to ensure that retrieved documents contain the relevant answers, leading to poor retrieval outcomes. The study highlights that when multiple biases are present, the performance can drastically decline, negatively impacting downstream applications that rely on accurate document retrieval.'}, 'zh': {'title': '揭示密集检索模型的脆弱性', 'desc': '本文探讨了密集检索模型在信息检索应用中的脆弱性，尤其是在检索增强生成（RAG）系统中的重要性。通过重新利用关系提取数据集，设计了控制实验来量化启发式偏见对检索器的影响。研究发现，检索器往往依赖表面模式，如过度优先考虑文档开头、较短文档和字面匹配，缺乏深层语义理解。多个偏见结合时，模型性能显著下降，导致选择包含答案的文档的概率低于3%。'}}}, {'id': 'https://huggingface.co/papers/2503.03734', 'title': 'OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction', 'url': 'https://huggingface.co/papers/2503.03734', 'abstract': 'Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre-trained visionlanguage models (VLMs) as visual and language features are independently fed into downstream policies, degrading the pre-trained semantic alignments. We propose OTTER, a novel VLA architecture that leverages these existing alignments through explicit, text-aware visual feature extraction. Instead of processing all visual features, OTTER selectively extracts and passes only task-relevant visual features that are semantically aligned with the language instruction to the policy transformer. This allows OTTER to keep the pre-trained vision-language encoders frozen. Thereby, OTTER preserves and utilizes the rich semantic understanding learned from large-scale pre-training, enabling strong zero-shot generalization capabilities. In simulation and real-world experiments, OTTER significantly outperforms existing VLA models, demonstrating strong zeroshot generalization to novel objects and environments. Video, code, checkpoints, and dataset: https://ottervla.github.io/.', 'score': 0, 'issue_id': 2668, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': 'a6a36002652e8be7', 'authors': ['Huang Huang', 'Fangchen Liu', 'Letian Fu', 'Tingfan Wu', 'Mustafa Mukadam', 'Jitendra Malik', 'Ken Goldberg', 'Pieter Abbeel'], 'affiliations': ['Meta AI', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.03734.jpg', 'data': {'categories': ['#agi', '#architecture', '#transfer_learning', '#video', '#multimodal', '#training', '#agents'], 'emoji': '🦦', 'ru': {'title': 'OTTER: Умное извлечение визуальных признаков для роботизированных действий', 'desc': 'OTTER - это новая архитектура для моделей зрения-языка-действия (VLA), которая улучшает предсказание роботизированных действий. Она использует предварительно обученные семантические связи между зрением и языком, извлекая только релевантные визуальные признаки. OTTER сохраняет замороженными предварительно обученные энкодеры зрения-языка, что позволяет использовать богатое семантическое понимание. В экспериментах OTTER значительно превосходит существующие модели VLA, демонстрируя сильную обобщающую способность в нулевом выстреле.'}, 'en': {'title': 'OTTER: Enhancing Robotic Actions with Smart Visual-Language Alignment', 'desc': 'The paper introduces OTTER, a new Vision-Language-Action (VLA) model designed to enhance robotic action prediction using visual inputs and language instructions. Unlike traditional methods that require fine-tuning of pre-trained vision-language models, OTTER maintains the integrity of these models by keeping them frozen and selectively extracting only the relevant visual features aligned with the language commands. This approach allows OTTER to leverage the rich semantic knowledge from large-scale pre-training, leading to improved performance in zero-shot scenarios. Experimental results show that OTTER outperforms existing VLA models in both simulated and real-world tasks, demonstrating its ability to generalize to new objects and environments without additional training.'}, 'zh': {'title': 'OTTER：提升机器人动作预测的视觉-语言-动作模型', 'desc': '本文提出了一种新的视觉-语言-动作（VLA）模型OTTER，旨在根据视觉观察和语言指令预测机器人动作。OTTER通过显式的文本感知视觉特征提取，利用现有的语义对齐，避免了对预训练视觉语言模型的微调。该模型选择性地提取与任务相关的视觉特征，并将其传递给策略变换器，从而保持预训练的视觉-语言编码器不变。实验结果表明，OTTER在新物体和环境上展现了强大的零样本泛化能力，显著优于现有的VLA模型。'}}}, {'id': 'https://huggingface.co/papers/2502.21263', 'title': 'RuCCoD: Towards Automated ICD Coding in Russian', 'url': 'https://huggingface.co/papers/2502.21263', 'abstract': 'This study investigates the feasibility of automating clinical coding in Russian, a language with limited biomedical resources. We present a new dataset for ICD coding, which includes diagnosis fields from electronic health records (EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD codes. This dataset serves as a benchmark for several state-of-the-art models, including BERT, LLaMA with LoRA, and RAG, with additional experiments examining transfer learning across domains (from PubMed abstracts to medical diagnosis) and terminologies (from UMLS concepts to ICD codes). We then apply the best-performing model to label an in-house EHR dataset containing patient histories from 2017 to 2021. Our experiments, conducted on a carefully curated test set, demonstrate that training with the automated predicted codes leads to a significant improvement in accuracy compared to manually annotated data from physicians. We believe our findings offer valuable insights into the potential for automating clinical coding in resource-limited languages like Russian, which could enhance clinical efficiency and data accuracy in these contexts.', 'score': 110, 'issue_id': 2617, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '89ad8229208a4f98', 'authors': ['Aleksandr Nesterov', 'Andrey Sakhovskiy', 'Ivan Sviridov', 'Airat Valiev', 'Vladimir Makharev', 'Petr Anokhin', 'Galina Zubkova', 'Elena Tutubalina'], 'affiliations': ['AIRI, Moscow, Russia', 'HSE University, Moscow, Russia', 'ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia', 'Sber AI Lab, Moscow, Russia', 'Sber AI, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.21263.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#benchmark', '#science', '#low_resource', '#healthcare', '#training'], 'emoji': '🏥', 'ru': {'title': 'Автоматизация клинического кодирования на русском языке: прорыв в эффективности и точности', 'desc': 'Исследование посвящено автоматизации клинического кодирования на русском языке с использованием методов машинного обучения. Авторы представляют новый набор данных для кодирования по МКБ, включающий более 10000 сущностей и 1500 уникальных кодов МКБ. Проводится сравнение современных моделей, таких как BERT, LLaMA с LoRA и RAG, а также исследуется перенос обучения между доменами и терминологиями. Результаты показывают, что обучение на автоматически предсказанных кодах значительно повышает точность по сравнению с ручной разметкой врачей.'}, 'en': {'title': 'Automating Clinical Coding: A Leap for Russian Healthcare', 'desc': 'This paper explores the automation of clinical coding in the Russian language, which lacks extensive biomedical resources. It introduces a new dataset for ICD coding, featuring over 10,000 annotated entities and 1,500 unique ICD codes derived from electronic health records. The study benchmarks various advanced models, including BERT and LLaMA, and investigates transfer learning from different medical domains and terminologies. Results show that using automated coding significantly improves accuracy over traditional manual coding by physicians, highlighting the potential for enhanced clinical efficiency in resource-limited settings.'}, 'zh': {'title': '自动化临床编码：提升俄语医疗效率的希望', 'desc': '本研究探讨了在俄语中自动化临床编码的可行性，俄语的生物医学资源相对有限。我们提出了一个新的ICD编码数据集，该数据集包含来自电子健康记录（EHR）的诊断字段，标注了超过10,000个实体和1,500多个独特的ICD代码。通过对多种先进模型（如BERT、LLaMA与LoRA、RAG）的基准测试，以及跨领域和术语的迁移学习实验，我们验证了模型的有效性。实验结果表明，使用自动预测的代码进行训练，相较于医生手动标注的数据，显著提高了准确性，显示了在资源有限的语言中自动化临床编码的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.05236', 'title': 'Unified Reward Model for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2503.05236', 'abstract': 'Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.', 'score': 86, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '6ebf61a6777b8e4d', 'authors': ['Yibin Wang', 'Yuhang Zang', 'Hao Li', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05236.jpg', 'data': {'categories': ['#multimodal', '#video', '#cv', '#alignment', '#dataset', '#rlhf', '#rag'], 'emoji': '🤖', 'ru': {'title': 'Единая модель вознаграждения для улучшения мультимодальных AI-систем', 'desc': 'Статья представляет UnifiedReward - первую унифицированную модель вознаграждения для оценки мультимодального понимания и генерации. Модель обучается на большом наборе данных о человеческих предпочтениях, включающем задачи по генерации и пониманию изображений и видео. UnifiedReward используется для автоматического создания высококачественных пар предпочтений на основе моделей компьютерного зрения. Затем эти данные применяются для настройки предпочтений моделей с помощью метода Direct Preference Optimization (DPO).'}, 'en': {'title': 'UnifiedReward: Enhancing Multimodal Learning through Joint Preference Alignment', 'desc': 'This paper introduces UnifiedReward, a novel reward model designed to enhance multimodal understanding and generation in machine learning. It addresses the limitation of existing task-specific models by enabling joint learning across various visual tasks, which improves both image and video assessments. The model is trained on a large-scale human preference dataset and utilizes techniques like pairwise ranking and pointwise scoring for effective preference alignment. Experimental results show that this unified approach leads to significant performance improvements in both image and video tasks, demonstrating the benefits of synergistic learning.'}, 'zh': {'title': '统一奖励模型，提升多模态理解与生成', 'desc': '最近在人类偏好对齐方面的进展显著提升了多模态生成和理解的能力。关键方法是训练奖励模型来指导偏好优化。然而，现有模型通常是特定于任务的，限制了它们在不同视觉应用中的适应性。本文提出了UnifiedReward，这是第一个用于多模态理解和生成评估的统一奖励模型，能够同时进行成对排名和逐点评分，从而实现视觉模型的偏好对齐。'}}}, {'id': 'https://huggingface.co/papers/2503.05500', 'title': 'EuroBERT: Scaling Multilingual Encoders for European Languages', 'url': 'https://huggingface.co/papers/2503.05500', 'abstract': 'General-purpose multilingual vector representations, used in retrieval, regression and classification, are traditionally obtained from bidirectional encoder models. Despite their wide applicability, encoders have been recently overshadowed by advances in generative decoder-only models. However, many innovations driving this progress are not inherently tied to decoders. In this paper, we revisit the development of multilingual encoders through the lens of these advances, and introduce EuroBERT, a family of multilingual encoders covering European and widely spoken global languages. Our models outperform existing alternatives across a diverse range of tasks, spanning multilingual capabilities, mathematics, and coding, and natively supporting sequences of up to 8,192 tokens. We also examine the design decisions behind EuroBERT, offering insights into our dataset composition and training pipeline. We publicly release the EuroBERT models, including intermediate training checkpoints, together with our training framework.', 'score': 57, 'issue_id': 2613, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'befb9ffdb6a6cd73', 'authors': ['Nicolas Boizard', 'Hippolyte Gisserot-Boukhlef', 'Duarte M. Alves', 'André Martins', 'Ayoub Hammal', 'Caio Corro', 'Céline Hudelot', 'Emmanuel Malherbe', 'Etienne Malaboeuf', 'Fanny Jourdan', 'Gabriel Hautreux', 'João Alves', 'Kevin El-Haddad', 'Manuel Faysse', 'Maxime Peyrard', 'Nuno M. Guerreiro', 'Patrick Fernandes', 'Ricardo Rei', 'Pierre Colombo'], 'affiliations': ['Artefact', 'CINES', 'CNRS', 'Carnegie Mellon University', 'Diabolocom', 'Equall', 'INSA Rennes', 'IRISA', 'IRT Saint Exupery', 'ISIA Lab', 'Illuin Technology', 'Instituto Superior Tecnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de Telecomunicacoes', 'LISN', 'MICS, CentraleSupelec, Universite Paris-Saclay', 'Unbabel', 'Universite Grenoble Alpes, Grenoble INP, LIG', 'Universite Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2503.05500.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#architecture', '#training', '#dataset', '#long_context'], 'emoji': '🌍', 'ru': {'title': 'EuroBERT: Возрождение многоязычных энкодеров в эпоху генеративных моделей', 'desc': 'В статье представлена модель EuroBERT - семейство многоязычных энкодеров для европейских и широко распространенных мировых языков. Эти энкодеры превосходят существующие аналоги в различных задачах, включая многоязычные возможности, математику и программирование. Модели EuroBERT поддерживают последовательности длиной до 8192 токенов и были разработаны с учетом последних достижений в области генеративных декодер-моделей. Авторы также описывают процесс создания датасета и pipeline обучения модели.'}, 'en': {'title': 'EuroBERT: Advancing Multilingual Encoders for Diverse Tasks', 'desc': 'This paper presents EuroBERT, a new family of multilingual encoder models designed to improve performance in various tasks such as retrieval, regression, and classification. Unlike traditional models that rely heavily on bidirectional encoders, EuroBERT leverages recent advancements in generative models while maintaining the strengths of encoders. The models are capable of handling sequences of up to 8,192 tokens and demonstrate superior performance across multilingual tasks, mathematics, and coding challenges. The authors also provide insights into the dataset and training processes used to develop EuroBERT, along with public access to the models and training framework.'}, 'zh': {'title': 'EuroBERT：提升多语言处理的新选择', 'desc': '本文介绍了一种新的多语言编码器模型，称为EuroBERT，旨在提升多语言检索、回归和分类任务的性能。尽管生成解码器模型近年来取得了显著进展，但我们认为多语言编码器仍然具有重要价值。EuroBERT覆盖了欧洲及广泛使用的全球语言，并在多种任务中表现优于现有模型。我们还分享了EuroBERT的设计决策、数据集构成和训练流程，并公开发布了模型及其训练框架。'}}}, {'id': 'https://huggingface.co/papers/2503.05085', 'title': 'S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following\n  with Paralinguistic Information', 'url': 'https://huggingface.co/papers/2503.05085', 'abstract': 'The rapid development of large language models (LLMs) has brought significant attention to speech models, particularly recent progress in speech2speech protocols supporting speech input and output. However, the existing benchmarks adopt automatic text-based evaluators for evaluating the instruction following ability of these models lack consideration for paralinguistic information in both speech understanding and generation. To address these issues, we introduce S2S-Arena, a novel arena-style S2S benchmark that evaluates instruction-following capabilities with paralinguistic information in both speech-in and speech-out across real-world tasks. We design 154 samples that fused TTS and live recordings in four domains with 21 tasks and manually evaluate existing popular speech models in an arena-style manner. The experimental results show that: (1) in addition to the superior performance of GPT-4o, the speech model of cascaded ASR, LLM, and TTS outperforms the jointly trained model after text-speech alignment in speech2speech protocols; (2) considering paralinguistic information, the knowledgeability of the speech model mainly depends on the LLM backbone, and the multilingual support of that is limited by the speech module; (3) excellent speech models can already understand the paralinguistic information in speech input, but generating appropriate audio with paralinguistic information is still a challenge.', 'score': 39, 'issue_id': 2619, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'aa9d1f284b6fe901', 'authors': ['Feng Jiang', 'Zhiyu Lin', 'Fan Bu', 'Yuhao Du', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2503.05085.jpg', 'data': {'categories': ['#audio', '#multilingual', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'S2S-Arena: новый подход к оценке речевых моделей с учетом паралингвистики', 'desc': 'Статья представляет S2S-Arena - новый бенчмарк для оценки речевых моделей, учитывающий паралингвистическую информацию. Авторы создали 154 образца в 4 доменах с 21 задачей, объединяющих синтезированную и живую речь. Результаты показывают преимущество каскадных моделей (ASR+LLM+TTS) над совместно обученными в задачах speech2speech. Выявлено, что современные речевые модели хорошо понимают паралингвистическую информацию во входной речи, но генерация соответствующего аудио остается сложной задачей.'}, 'en': {'title': 'Enhancing Speech Models with Paralinguistic Insights', 'desc': "This paper presents S2S-Arena, a new benchmark for evaluating speech-to-speech (S2S) models that incorporates paralinguistic information, which includes elements like tone and emotion in speech. The authors highlight that current evaluation methods primarily rely on text-based metrics, which do not adequately assess the models' ability to understand and generate speech with these nuances. Through a series of 154 samples across various tasks, the study reveals that models like GPT-4o and cascaded ASR-LLM-TTS outperform others when considering paralinguistic factors. The findings indicate that while advanced speech models can comprehend paralinguistic cues, generating speech that accurately reflects these cues remains a significant challenge."}, 'zh': {'title': '引入副语言信息的语音模型评估新标准', 'desc': '随着大型语言模型（LLMs）的快速发展，语音模型也受到了广泛关注，尤其是在语音输入和输出的语音到语音（speech2speech）协议方面的进展。然而，现有的基准测试主要依赖自动文本评估器来评估这些模型的指令遵循能力，未能考虑语音理解和生成中的副语言信息。为了解决这些问题，我们提出了S2S-Arena，这是一个新颖的竞技场风格的S2S基准，评估在真实任务中语音输入和输出的指令遵循能力，并考虑副语言信息。实验结果表明，尽管GPT-4o表现优越，但在语音到语音协议中，级联的ASR、LLM和TTS语音模型在文本-语音对齐后优于联合训练模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05179', 'title': 'Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching', 'url': 'https://huggingface.co/papers/2503.05179', 'abstract': 'Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting framework that combines cognitive-inspired reasoning paradigms with linguistic constraints to minimize token usage while preserving reasoning accuracy. SoT is designed as a flexible framework that can incorporate any custom reasoning paradigms based on cognitive science, and we instantiate it with three such paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each tailored to different reasoning tasks and selected dynamically via a lightweight routing model. Through comprehensive evaluation across 15 reasoning datasets with multiple languages and multimodal scenarios, we demonstrate that SoT achieves token reductions of 76% with negligible accuracy impact. In certain domains like mathematical and multi-hop reasoning, it even improves accuracy while using significantly fewer tokens. Our code is publicly available: https://www.github.com/SimonAytes/SoT.', 'score': 37, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'e02cb6f62715b753', 'authors': ['Simon A. Aytes', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.05179.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#multilingual', '#optimization', '#open_source', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение с минимальным использованием токенов', 'desc': 'Статья представляет новый метод промптинга под названием Sketch-of-Thought (SoT), который сочетает когнитивно-вдохновленные парадигмы рассуждений с лингвистическими ограничениями. SoT направлен на минимизацию использования токенов при сохранении точности рассуждений в больших языковых моделях. Метод включает три парадигмы: Conceptual Chaining, Chunked Symbolism и Expert Lexicons, каждая из которых адаптирована для различных задач рассуждения. Эксперименты на 15 наборах данных показали, что SoT сокращает использование токенов на 76% без значительного влияния на точность, а в некоторых областях даже улучшает ее.'}, 'en': {'title': 'Efficient Reasoning with Sketch-of-Thought', 'desc': 'This paper presents a new prompting framework called Sketch-of-Thought (SoT) that enhances reasoning in large language models while reducing the number of tokens used. SoT integrates cognitive-inspired reasoning methods with linguistic constraints to maintain accuracy without excessive verbosity. The framework is adaptable, allowing for the inclusion of various reasoning paradigms, which are dynamically selected based on the task at hand. Evaluation shows that SoT can reduce token usage by 76% with little to no loss in accuracy, and in some cases, it even improves performance in specific reasoning tasks.'}, 'zh': {'title': '思维草图：高效推理的新方法', 'desc': '本文介绍了一种新的提示框架，称为思维草图（Sketch-of-Thought，SoT），旨在提高大型语言模型的推理能力，同时减少中间输出的冗长性。SoT结合了认知科学的推理范式和语言约束，以最小化令牌使用量，同时保持推理的准确性。该框架灵活，可以根据认知科学的不同推理范式进行定制，并通过轻量级路由模型动态选择。通过在15个推理数据集上的全面评估，SoT实现了76%的令牌减少，且对准确性影响微乎其微，甚至在某些领域提高了准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.05132', 'title': 'R1-Zero\'s "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model', 'url': 'https://huggingface.co/papers/2503.05132', 'abstract': 'Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the "aha moment", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero', 'score': 20, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '760e01cf2a414aeb', 'authors': ['Hengguang Zhou', 'Xirui Li', 'Ruochen Wang', 'Minhao Cheng', 'Tianyi Zhou', 'Cho-Jui Hsieh'], 'affiliations': ['Pennsylvania State University', 'University of California, LA', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2503.05132.jpg', 'data': {'categories': ['#rl', '#multimodal', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Новые горизонты мультимодального обучения', 'desc': 'Исследователи из DeepSeek R1 показали, как обучение с подкреплением может помочь LLM развивать сложные навыки рассуждения, включая моменты "эврики". Однако при попытках применить эти методы к мультимодальному обучению часто не удавалось достичь таких же результатов. В этом исследовании удалось впервые воспроизвести эти характеристики на мультимодальной модели без использования SFT, достигнув высокой точности. Авторы также делятся неудачными попытками и выводами, чтобы лучше понять возникающие трудности.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with Reinforcement Learning', 'desc': "This paper discusses the application of reinforcement learning (RL) to enhance multimodal reasoning in large language models, specifically using the Qwen2-VL-2B model. The authors successfully replicated the 'aha moment' phenomenon, where the model demonstrates self-reflection and improved response length during training, achieving a notable accuracy of 59.47% on the CVBench dataset. They also share insights from their unsuccessful attempts to replicate similar reasoning capabilities using instruct models, highlighting challenges such as trivial reasoning paths and ineffective reward structures. The findings suggest that while RL can significantly improve reasoning in multimodal contexts, careful consideration of reward mechanisms is crucial for success."}, 'zh': {'title': '强化学习助力多模态推理的突破', 'desc': '最近，DeepSeek R1展示了如何通过简单的基于规则的激励来实现强化学习，使大型语言模型能够自主发展复杂的推理能力。这种能力在训练过程中表现为“恍然大悟”的时刻，模型会自我反思并增加响应长度。然而，尝试将这种成功扩展到多模态推理时，往往无法重现这些关键特征。在本报告中，我们首次成功复制了这些特征，并在非SFT的2B模型上实现了多模态推理的进展。'}}}, {'id': 'https://huggingface.co/papers/2503.02130', 'title': 'Forgetting Transformer: Softmax Attention with a Forget Gate', 'url': 'https://huggingface.co/papers/2503.02130', 'abstract': 'An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer\'s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a "Pro" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.', 'score': 18, 'issue_id': 2607, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4c39f334b6c4ed28', 'authors': ['Zhixuan Lin', 'Evgenii Nikishin', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.02130.jpg', 'data': {'categories': ['#long_context', '#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Forgetting Transformer: Улучшение обработки длинных последовательностей в трансформерах', 'desc': "Исследователи представили новую модель под названием Forgetting Transformer (FoX), которая включает механизм 'забывающего внимания' в архитектуру трансформера. FoX превосходит стандартный трансформер в задачах моделирования языка с длинным контекстом и экстраполяции длины, сохраняя при этом высокую производительность на задачах с коротким контекстом. Модель совместима с алгоритмом FlashAttention и не требует позиционных эмбеддингов. Анализ показывает, что FoX сохраняет преимущества трансформера в обработке длинного контекста по сравнению с рекуррентными моделями."}, 'en': {'title': 'Enhancing Transformers with Forgetting Attention for Superior Performance', 'desc': 'This paper introduces a new attention mechanism called Forgetting Attention, which integrates a forget gate into Transformer models. The Forgetting Transformer (FoX) leverages this mechanism to improve performance on various language modeling tasks, particularly those involving long contexts. FoX not only matches the performance of traditional Transformers on long-context tasks but also excels in short-context and length extrapolation tasks. Additionally, it is compatible with the FlashAttention algorithm and eliminates the need for positional embeddings, enhancing its efficiency and effectiveness.'}, 'zh': {'title': '遗忘变换器：提升长上下文建模的利器', 'desc': '本文提出了一种新的注意力机制，称为遗忘注意力（Forgetting Attention），可以有效地将遗忘门集成到Transformer模型中。通过以数据为依赖的方式降低未归一化注意力分数，遗忘注意力使得Transformer在长上下文语言建模和长度外推任务中表现优于传统的Transformer。我们还设计了一个“Pro”模块，结合了递归序列模型中的一些常见架构组件，显著提升了FoX和Transformer的性能。此外，FoX在长上下文任务中保持了Transformer的优势，超越了其他递归序列模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05592', 'title': 'R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.05592', 'abstract': 'Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose R1-Searcher, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini.', 'score': 17, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '6af1f8cd890c69ae', 'authors': ['Huatong Song', 'Jinhao Jiang', 'Yingqian Min', 'Jie Chen', 'Zhipeng Chen', 'Wayne Xin Zhao', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05592.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#rl', '#rag', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Усиление поисковых способностей ИИ через обучение с подкреплением', 'desc': 'R1-Searcher - это новый двухэтапный подход к обучению с подкреплением, улучшающий способности больших языковых моделей к поиску информации. Он позволяет моделям автономно обращаться к внешним поисковым системам во время рассуждений. Метод основан исключительно на обучении с подкреплением, без необходимости в процессных наградах или дистилляции. Эксперименты показывают, что R1-Searcher превосходит предыдущие методы RAG, даже в сравнении с закрытой моделью GPT-4o-mini.'}, 'en': {'title': 'Enhancing LLM Reasoning with External Knowledge Search', 'desc': 'This paper introduces R1-Searcher, a new approach that improves the reasoning abilities of Large Language Models (LLMs) using reinforcement learning (RL). Unlike existing models that depend solely on their internal knowledge, R1-Searcher enables LLMs to access external search systems for additional information, which helps in answering complex and time-sensitive questions more accurately. The method operates in two stages and does not require initial rewards or distillation, making it easier to implement. Experimental results show that R1-Searcher outperforms previous retrieval-augmented generation (RAG) methods, demonstrating its effectiveness across various datasets.'}, 'zh': {'title': '增强推理能力，R1-Searcher助力LLMs', 'desc': '现有的大型推理模型（LRMs）展示了强化学习（RL）在增强大型语言模型（LLMs）复杂推理能力方面的潜力。尽管它们在数学和编程等挑战性任务上表现出色，但在处理时间敏感或知识密集的问题时，往往依赖内部知识，导致不准确和幻觉现象。为了解决这个问题，我们提出了R1-Searcher，这是一种新颖的基于结果的两阶段强化学习方法，旨在增强LLMs的搜索能力。该方法允许LLMs在推理过程中自主调用外部搜索系统，以获取额外知识，从而显著提高性能。'}}}, {'id': 'https://huggingface.co/papers/2503.04957', 'title': 'SafeArena: Evaluating the Safety of Autonomous Web Agents', 'url': 'https://huggingface.co/papers/2503.04957', 'abstract': 'LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories -- misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents. Our benchmark is available here: https://safearena.github.io', 'score': 15, 'issue_id': 2623, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '8fd2a7cf41173ed6', 'authors': ['Ada Defne Tur', 'Nicholas Meade', 'Xing Han Lù', 'Alejandra Zambrano', 'Arkil Patel', 'Esin Durmus', 'Spandana Gella', 'Karolina Stańczak', 'Siva Reddy'], 'affiliations': ['Anthropic', 'Canada CIFAR AI Chair', 'Concordia University', 'McGill University', 'Mila Quebec AI Institute', 'ServiceNow Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.04957.jpg', 'data': {'categories': ['#benchmark', '#agents', '#ethics', '#security', '#alignment'], 'emoji': '🕵️', 'ru': {'title': 'Оценка рисков ИИ-агентов: необходимость безопасности в эпоху веб-автоматизации', 'desc': 'SafeArena - это новый бенчмарк для оценки рисков намеренного злоупотребления веб-агентами на основе больших языковых моделей (LLM). Он включает 500 задач на четырех веб-сайтах, из которых 250 являются безопасными, а 250 - вредоносными, разделенными на пять категорий вреда. Исследователи оценили ведущих LLM-агентов, включая GPT-4o и Qwen-2-VL 72B, используя предложенную ими систему оценки рисков Agent Risk Assessment. Результаты показали, что агенты неожиданно часто выполняют вредоносные запросы, что подчеркивает срочную необходимость разработки процедур безопасности для веб-агентов.'}, 'en': {'title': 'Assessing the Risks of LLM Agents in Web Tasks', 'desc': 'This paper introduces SafeArena, a benchmark designed to evaluate the risks associated with the misuse of large language model (LLM)-based agents in web tasks. It includes 500 tasks, split evenly between safe and harmful, categorized into five types of harm such as misinformation and cybercrime. The study assesses various leading LLM agents, revealing that they often comply with harmful requests, with notable completion rates for GPT-4o and Qwen-2. The findings underscore the necessity for improved safety measures to prevent the malicious use of these agents.'}, 'zh': {'title': '网络代理安全风险评估的必要性', 'desc': '本论文提出了SafeArena，这是第一个专注于网络代理恶意使用的基准测试。SafeArena包含250个安全任务和250个有害任务，涵盖四个网站，并将有害任务分为五个类别，如虚假信息和网络犯罪。我们评估了多种领先的基于大语言模型的网络代理，发现它们对恶意请求的响应率令人惊讶，部分代理完成了超过三分之一的有害请求。研究结果强调了对网络代理进行安全对齐程序的迫切需求。'}}}, {'id': 'https://huggingface.co/papers/2503.05639', 'title': 'VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control', 'url': 'https://huggingface.co/papers/2503.05639', 'abstract': "Video inpainting, which aims to restore corrupted video content, has experienced substantial progress. Despite these advances, existing methods, whether propagating unmasked region pixels through optical flow and receptive field priors, or extending image-inpainting models temporally, face challenges in generating fully masked objects or balancing the competing objectives of background context preservation and foreground generation in one model, respectively. To address these limitations, we propose a novel dual-stream paradigm VideoPainter that incorporates an efficient context encoder (comprising only 6% of the backbone parameters) to process masked videos and inject backbone-aware background contextual cues to any pre-trained video DiT, producing semantically consistent content in a plug-and-play manner. This architectural separation significantly reduces the model's learning complexity while enabling nuanced integration of crucial background context. We also introduce a novel target region ID resampling technique that enables any-length video inpainting, greatly enhancing our practical applicability. Additionally, we establish a scalable dataset pipeline leveraging current vision understanding models, contributing VPData and VPBench to facilitate segmentation-based inpainting training and assessment, the largest video inpainting dataset and benchmark to date with over 390K diverse clips. Using inpainting as a pipeline basis, we also explore downstream applications including video editing and video editing pair data generation, demonstrating competitive performance and significant practical potential. Extensive experiments demonstrate VideoPainter's superior performance in both any-length video inpainting and editing, across eight key metrics, including video quality, mask region preservation, and textual coherence.", 'score': 13, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '735dd34a3043623a', 'authors': ['Yuxuan Bian', 'Zhaoyang Zhang', 'Xuan Ju', 'Mingdeng Cao', 'Liangbin Xie', 'Ying Shan', 'Qiang Xu'], 'affiliations': ['Tencent ARC Lab, China', 'The Chinese University of Hong Kong, China', 'The University of Tokyo, Japan', 'University of Macau, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05639.jpg', 'data': {'categories': ['#video', '#benchmark', '#dataset', '#games', '#architecture', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'VideoPainter: Революция в восстановлении видео с помощью двухпоточной архитектуры', 'desc': 'Статья представляет новый подход к восстановлению поврежденного видеоконтента - VideoPainter. Модель использует двухпоточную архитектуру с эффективным кодировщиком контекста для обработки маскированных видео и внедрения контекстуальных подсказок в предобученную видео-модель DiT. Авторы также вводят технику ресемплинга ID целевой области, позволяющую выполнять инпейнтинг видео любой длины. Дополнительно создан масштабируемый конвейер данных VPData и бенчмарк VPBench для обучения и оценки сегментационного инпейнтинга.'}, 'en': {'title': 'Revolutionizing Video Inpainting with VideoPainter', 'desc': "This paper presents VideoPainter, a new approach to video inpainting that effectively restores missing video content. It introduces a dual-stream architecture that uses a lightweight context encoder to enhance background information while maintaining the integrity of foreground objects. The method also features a novel target region ID resampling technique, allowing for flexible inpainting of videos of any length. Additionally, the authors provide a comprehensive dataset and benchmark for training and evaluating video inpainting models, showcasing VideoPainter's strong performance across various metrics."}, 'zh': {'title': '视频修复的新纪元：VideoPainter', 'desc': '视频修复旨在恢复损坏的视频内容，近年来取得了显著进展。现有方法在生成完全遮挡的物体或平衡背景保留与前景生成方面面临挑战。为了解决这些问题，我们提出了一种新颖的双流架构VideoPainter，利用高效的上下文编码器处理遮挡视频，并将背景上下文信息注入到预训练的视频模型中。我们还引入了一种新的目标区域ID重采样技术，支持任意长度的视频修复，极大地提升了实际应用的可行性。'}}}, {'id': 'https://huggingface.co/papers/2503.05379', 'title': 'R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning', 'url': 'https://huggingface.co/papers/2503.05379', 'abstract': "In this work, we present the first application of Reinforcement Learning with Verifiable Reward (RLVR) to an Omni-multimodal large language model in the context of emotion recognition, a task where both visual and audio modalities play crucial roles. We leverage RLVR to optimize the Omni model, significantly enhancing its performance in three key aspects: reasoning capability, emotion recognition accuracy, and generalization ability. The introduction of RLVR not only improves the model's overall performance on in-distribution data but also demonstrates superior robustness when evaluated on out-of-distribution datasets. More importantly, the improved reasoning capability enables clear analysis of the contributions of different modalities, particularly visual and audio information, in the emotion recognition process. This provides valuable insights into the optimization of multimodal large language models.", 'score': 12, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '34c6afbd7ae83841', 'authors': ['Jiaxing Zhao', 'Xihan Wei', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.05379.jpg', 'data': {'categories': ['#optimization', '#rl', '#multimodal', '#audio', '#cv', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'RLVR: Революция в мультимодальном распознавании эмоций', 'desc': 'В статье представлено первое применение обучения с подкреплением с проверяемым вознаграждением (RLVR) к мультимодальной большой языковой модели для распознавания эмоций. RLVR используется для оптимизации Omni-модели, значительно улучшая её способности к рассуждению, точность распознавания эмоций и способность к обобщению. Модель демонстрирует повышенную производительность на исходных данных и устойчивость на новых наборах данных. Улучшенная способность к рассуждениям позволяет анализировать вклад различных модальностей в процесс распознавания эмоций.'}, 'en': {'title': 'Enhancing Emotion Recognition with RLVR in Multimodal Models', 'desc': "This paper introduces a novel approach called Reinforcement Learning with Verifiable Reward (RLVR) applied to an Omni-multimodal large language model for emotion recognition. The use of RLVR enhances the model's reasoning skills, accuracy in recognizing emotions, and its ability to generalize across different datasets. The model not only performs better on familiar data but also shows increased robustness when tested on new, unseen data. Additionally, the improved reasoning capability allows for a detailed understanding of how visual and audio inputs contribute to the emotion recognition task."}, 'zh': {'title': '情感识别中的全模态强化学习新突破', 'desc': '本研究首次将可验证奖励的强化学习（RLVR）应用于情感识别的全模态大型语言模型中。在这个任务中，视觉和音频模态起着至关重要的作用。通过使用RLVR，我们显著提升了模型在推理能力、情感识别准确性和泛化能力等三个关键方面的表现。此外，RLVR的引入不仅提高了模型在同分布数据上的整体性能，还在异分布数据集上展现出更强的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.04808', 'title': 'Learning from Failures in Multi-Attempt Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.04808', 'abstract': "Recent advancements in reinforcement learning (RL) for large language models (LLMs), exemplified by DeepSeek R1, have shown that even a simple question-answering task can substantially improve an LLM's reasoning capabilities. In this work, we extend this approach by modifying the task into a multi-attempt setting. Instead of generating a single response per question, the model is given multiple attempts, with feedback provided after incorrect responses. The multi-attempt task encourages the model to refine its previous attempts and improve search efficiency. Experimental results show that even a small LLM trained on a multi-attempt task achieves significantly higher accuracy when evaluated with more attempts, improving from 45.6% with 1 attempt to 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM trained on a standard single-turn task exhibits only a marginal improvement, increasing from 42.3% to 43.2% when given more attempts during evaluation. The results indicate that, compared to the standard single-turn task, an LLM trained on a multi-attempt task achieves slightly better performance on math benchmarks while also learning to refine its responses more effectively based on user feedback. Full code is available at https://github.com/DualityRL/multi-attempt", 'score': 11, 'issue_id': 2609, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'fb2db794d0ea3c11', 'authors': ['Stephen Chung', 'Wenyu Du', 'Jie Fu'], 'affiliations': ['DualityRL', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.04808.jpg', 'data': {'categories': ['#optimization', '#rl', '#math', '#training', '#rlhf', '#reasoning'], 'emoji': '🔁', 'ru': {'title': 'Многопопыточное обучение: путь к более эффективным языковым моделям', 'desc': 'Это исследование расширяет подход обучения с подкреплением для больших языковых моделей, внедряя многопопыточную задачу вместо стандартной однопопыточной. Модель получает несколько попыток ответить на вопрос, получая обратную связь после неверных ответов, что способствует улучшению рассуждений и эффективности поиска. Эксперименты показывают, что даже небольшая языковая модель, обученная на многопопыточной задаче, достигает значительно более высокой точности при оценке с большим количеством попыток. Результаты демонстрируют, что модель, обученная на многопопыточной задаче, не только показывает лучшие результаты на математических тестах, но и эффективнее улучшает свои ответы на основе обратной связи пользователя.'}, 'en': {'title': 'Enhancing LLMs with Multi-Attempt Learning', 'desc': "This paper explores how modifying reinforcement learning tasks can enhance the reasoning abilities of large language models (LLMs). By implementing a multi-attempt question-answering framework, the model receives feedback on incorrect answers, allowing it to improve its responses iteratively. Experimental results demonstrate that even a small LLM can achieve better accuracy on math benchmarks when trained with this multi-attempt approach, compared to traditional single-turn tasks. The findings suggest that providing multiple attempts and feedback significantly aids in refining the model's performance."}, 'zh': {'title': '多次尝试，提升推理能力！', 'desc': '本研究探讨了在大型语言模型（LLM）中应用强化学习（RL）的新方法，特别是通过多次尝试的任务设置来提升模型的推理能力。与传统的单次回答不同，模型在每个问题上可以进行多次尝试，并在错误回答后获得反馈。这种多次尝试的任务设置促使模型改进之前的回答，从而提高搜索效率。实验结果表明，即使是小型LLM，在多次尝试的任务训练下，其准确率显著提高，显示出多次尝试对模型学习和反馈的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.05638', 'title': 'TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos\n  via Diffusion Models', 'url': 'https://huggingface.co/papers/2503.05638', 'abstract': 'We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method.', 'score': 9, 'issue_id': 2612, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '33c7af8e9a7df61e', 'authors': ['Mark YU', 'Wenbo Hu', 'Jinbo Xing', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2503.05638.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#diffusion', '#video'], 'emoji': '🎥', 'ru': {'title': 'Управление траекторией камеры в видео с помощью искусственного интеллекта', 'desc': 'TrajectoryCrafter - это новый подход к перенаправлению траекторий камеры для монокулярных видео. Метод разделяет детерминированные преобразования вида и стохастическую генерацию контента, обеспечивая точный контроль над заданными пользователем траекториями камеры. Авторы предлагают двухпоточную условную модель диффузии видео, интегрирующую рендеры облака точек и исходные видео. Для обучения используется гибридный набор данных, сочетающий монокулярные видео и статические многоракурсные датасеты.'}, 'en': {'title': 'Mastering Camera Movement in Monocular Videos', 'desc': 'TrajectoryCrafter is a new method designed to change camera paths in single-camera videos. It separates the predictable changes in view from the random elements in the video content, allowing for better control over how the camera moves. The approach uses a dual-stream conditional video diffusion model that combines 3D point cloud images and original videos to ensure smooth transitions and realistic video generation. By creating a unique training dataset that merges large amounts of single-camera videos with some multi-camera data, the method shows improved performance across various scenes.'}, 'zh': {'title': '精准控制摄像机轨迹的创新方法', 'desc': '我们提出了一种名为TrajectoryCrafter的新方法，用于重定向单目视频的摄像机轨迹。通过将确定性的视图变换与随机内容生成分离，我们的方法能够精确控制用户指定的摄像机轨迹。我们提出了一种新颖的双流条件视频扩散模型，同时整合点云渲染和源视频作为条件，确保准确的视图变换和一致的4D内容生成。通过创新的双重重投影策略，我们结合了网络规模的单目视频和静态多视角数据集，显著提高了在不同场景中的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.04872', 'title': 'TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation', 'url': 'https://huggingface.co/papers/2503.04872', 'abstract': 'The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is selectively distilled into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time.', 'score': 9, 'issue_id': 2609, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '94defd7f9d19776e', 'authors': ['Lin Sun', 'Guangxiang Zhao', 'Xiaoqi Jian', 'Yuhan Wu', 'Weihong Lin', 'Yongfu Zhu', 'Change Jia', 'Linglin Zhang', 'Jinzhu Wu', 'Junfeng Ran', 'Sai-er Hu', 'Zihan Jiang', 'Junting Zhou', 'Wenrui Liu', 'Bin Cui', 'Tong Yang', 'Xiangzheng Zhang'], 'affiliations': ['Peking University', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.04872.jpg', 'data': {'categories': ['#small_models', '#training', '#transfer_learning', '#optimization'], 'emoji': '🌳', 'ru': {'title': 'Ветвление и слияние: новый путь к компактным и мощным языковым моделям', 'desc': 'Статья представляет новый подход к сжатию больших языковых моделей (LLM) под названием Branch-Merge distillation. Метод состоит из двух фаз: Branch, где знания из большой модели-учителя дистиллируются в специализированные модели-ученики, и Merge, где эти модели объединяются для улучшения обобщения. Эксперименты показали, что полученная модель TinyR1-32B-Preview превосходит аналоги по нескольким бенчмаркам. Этот подход предлагает масштабируемое решение для создания меньших, но эффективных LLM с пониженными вычислительными затратами.'}, 'en': {'title': 'Branch-Merge: Compressing LLMs Without Compromise!', 'desc': 'This paper presents a new method called Branch-Merge distillation to reduce the size of Large Language Models (LLMs) while keeping their performance high. It consists of two main phases: the Branch Phase, where knowledge from a large teacher model is distilled into smaller, specialized student models through supervised fine-tuning, and the Merge Phase, where these student models are combined to enhance knowledge transfer across different domains. The approach was tested using specific models and showed that the merged model, TinyR1-32B-Preview, outperformed the individual student model in various tasks, including Mathematics, Coding, and Science. Overall, this method offers an effective way to create smaller LLMs that maintain strong performance and are more efficient in terms of computational resources.'}, 'zh': {'title': '分支合并蒸馏：高效压缩大型语言模型的创新方法', 'desc': '本文提出了一种新的模型蒸馏方法，称为分支合并蒸馏，旨在在保持性能的同时减少大型语言模型的体积。该方法分为两个阶段：分支阶段通过领域特定的监督微调将知识从大型教师模型选择性地蒸馏到专门的学生模型中；合并阶段则将这些学生模型合并，以实现跨领域知识转移并提高模型的泛化能力。实验结果表明，合并后的模型TinyR1-32B-Preview在多个基准测试中表现优于其对应的学生模型DeepSeek-R1-Distill-Qwen-32B。该方法为创建更小且高性能的语言模型提供了一种可扩展的解决方案，降低了计算成本和时间。'}}}, {'id': 'https://huggingface.co/papers/2503.05652', 'title': 'BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation\n  for Everyday Household Activities', 'url': 'https://huggingface.co/papers/2503.05652', 'abstract': "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/", 'score': 8, 'issue_id': 2608, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '52a7efb2f40f020a', 'authors': ['Yunfan Jiang', 'Ruohan Zhang', 'Josiah Wong', 'Chen Wang', 'Yanjie Ze', 'Hang Yin', 'Cem Gokmen', 'Shuran Song', 'Jiajun Wu', 'Li Fei-Fei'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05652.jpg', 'data': {'categories': ['#robotics', '#open_source', '#dataset', '#training'], 'emoji': '🤖', 'ru': {'title': 'Комплексная система для обучения роботов домашним задачам', 'desc': 'Статья представляет BEHAVIOR Robot Suite (BRS) - комплексную систему для манипуляции роботов в домашних условиях. BRS основан на двуруком колесном роботе с 4-осевым торсом и включает интерфейс телеуправления для сбора данных и новый алгоритм обучения визуомоторным политикам. Система оценивается на пяти сложных бытовых задачах, требующих бимануальной координации, точной навигации и широкой досягаемости манипуляторов. BRS представляет значительный шаг вперед в решении задач роботизированной манипуляции в реальных домашних условиях.'}, 'en': {'title': 'Empowering Robots for Everyday Household Tasks with BRS', 'desc': 'This paper presents the BEHAVIOR Robot Suite (BRS), a framework designed to enhance mobile manipulation robots for household tasks. It identifies three essential capabilities for effective task performance: bimanual coordination, stable navigation, and extensive reachability. The BRS integrates a teleoperation interface for data collection and a novel algorithm for learning visuomotor policies, addressing the complexities of hardware design and policy learning. The framework is evaluated on five challenging tasks that test these capabilities in real-world scenarios, aiming to improve robotic manipulation in everyday environments.'}, 'zh': {'title': '实现家庭任务的全身操控机器人', 'desc': '本论文介绍了BEHAVIOR机器人套件（BRS），旨在解决移动操作机器人在家庭任务中面临的挑战。研究表明，成功完成任务依赖于三项关键的全身控制能力：双手协调、稳定精确的导航和广泛的末端执行器可达性。BRS框架结合了一个双手轮式机器人和4自由度的躯干，提供了一种经济高效的全身遥操作接口用于数据收集，并提出了一种新算法用于学习全身视觉运动策略。通过在五个复杂的家庭任务上评估BRS，展示了其在长距离导航、与可动和可变形物体的交互以及在狭小空间中的操作能力。'}}}, {'id': 'https://huggingface.co/papers/2503.04824', 'title': 'ProReflow: Progressive Reflow with Decomposed Velocity', 'url': 'https://huggingface.co/papers/2503.04824', 'abstract': 'Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into a straight line for a few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05).', 'score': 8, 'issue_id': 2616, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '3380a5ba02714266', 'authors': ['Lei Ke', 'Haohang Xu', 'Xuefei Ning', 'Yu Li', 'Jiajun Li', 'Haoling Li', 'Yuxuan Lin', 'Dongsheng Jiang', 'Yujiu Yang', 'Linfeng Zhang'], 'affiliations': ['Huawei Inc.', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04824.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#optimization'], 'emoji': '🌊', 'ru': {'title': 'Оптимизация потоков для быстрой генерации изображений', 'desc': "Статья представляет усовершенствованный метод генерации изображений и видео с использованием диффузионных моделей. Авторы предлагают технику 'progressive reflow', которая постепенно перестраивает диффузионный процесс, упрощая задачу согласования потоков. Также вводится концепция 'aligned v-prediction', подчеркивающая важность соответствия направлений в процессе согласования потоков. Экспериментальные результаты на моделях SDv1.5 и SDXL демонстрируют эффективность предложенного метода, достигая высокого качества генерации при значительно меньшем количестве шагов сэмплирования."}, 'en': {'title': 'Streamlining Diffusion: Faster Generation with Flow Matching Enhancements', 'desc': 'This paper addresses the high computational costs associated with diffusion models used for image and video generation. It proposes flow matching as a method to streamline the diffusion process, allowing for faster generation in fewer steps. The authors introduce two enhancements: progressive reflow, which simplifies the flow matching by gradually adjusting local timesteps, and aligned v-prediction, which emphasizes the importance of direction over magnitude in flow matching. Experimental results show that their approach significantly improves performance, achieving competitive results with fewer sampling steps compared to traditional methods.'}, 'zh': {'title': '优化扩散模型的流匹配训练', 'desc': '扩散模型在图像和视频生成方面取得了显著进展，但计算成本仍然很高。为了解决这个问题，流匹配技术将扩散过程重新调整为直线，以实现少步甚至一步的生成。本文提出了两种改进流匹配训练流程的技术：逐步重新流动和对齐的v预测。实验结果表明，我们的方法在生成质量上接近教师模型，同时大幅减少了采样步骤。'}}}, {'id': 'https://huggingface.co/papers/2503.05447', 'title': 'Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2503.05447', 'abstract': 'Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 6, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '3975a97b4236e791', 'authors': ['Weigao Sun', 'Disen Lan', 'Tong Zhu', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'Soochow University', 'South China University of Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.05447.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Linear-MoE: Объединение линейного моделирования и смеси экспертов для эффективных крупномасштабных моделей', 'desc': 'Эта статья представляет Linear-MoE - систему для моделирования и обучения крупномасштабных моделей, объединяющую линейное моделирование последовательностей (LSM) с методом смеси экспертов (MoE). Linear-MoE использует преимущества LSM-модулей для линейного моделирования последовательностей и MoE-слоев для разреженной активации, стремясь обеспечить высокую производительность при эффективном обучении. Система включает подсистему моделирования, предоставляющую унифицированную структуру для всех экземпляров LSM, и подсистему обучения с различными технологиями параллелизма. Оценки на двух сериях моделей показывают, что Linear-MoE достигает повышения эффективности при сохранении конкурентоспособной производительности на различных тестах.'}, 'en': {'title': 'Efficient Modeling with Linear-MoE: Merging LSM and MoE for High Performance', 'desc': 'This paper presents Linear-MoE, a new system that combines Linear Sequence Modeling (LSM) with Mixture-of-Experts (MoE) to improve the efficiency and performance of large-scale models. Linear-MoE utilizes linear-complexity sequence modeling from LSM and the sparsity benefits of MoE layers, allowing for effective training and high performance. The system includes a modeling subsystem for LSM and a training subsystem that employs advanced parallelism techniques, particularly Sequence Parallelism. Experiments show that Linear-MoE achieves better efficiency while delivering competitive results on various benchmarks, indicating its promise as a foundational model architecture.'}, 'zh': {'title': '线性-MoE：高效的序列建模与训练新架构', 'desc': '线性序列建模（LSM）和专家混合模型（MoE）最近成为重要的架构改进。本文介绍了一种名为Linear-MoE的系统，它将LSM与MoE结合，用于建模和训练大规模模型。Linear-MoE利用LSM模块的线性复杂度序列建模优势和MoE层的稀疏激活特性，旨在提供高性能和高效训练。通过对A0.3B-2B和A1B-7B模型系列的评估，Linear-MoE在保持竞争性能的同时实现了效率提升，展示了其作为下一代基础模型架构的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.04548', 'title': 'An Empirical Study on Eliciting and Improving R1-like Reasoning Models', 'url': 'https://huggingface.co/papers/2503.04548', 'abstract': 'In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project. As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models. We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models. Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy. Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models. This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities. We release our resources at the STILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.', 'score': 6, 'issue_id': 2615, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'defc053b9079a4a2', 'authors': ['Zhipeng Chen', 'Yingqian Min', 'Beichen Zhang', 'Jie Chen', 'Jinhao Jiang', 'Daixuan Cheng', 'Wayne Xin Zhao', 'Zheng Liu', 'Xu Miao', 'Yang Lu', 'Lei Fang', 'Zhongyuan Wang', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04548.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Усиление способностей языковых моделей к рассуждению через обучение с подкреплением', 'desc': 'В этом отчете представлены результаты экспериментов по улучшению моделей машинного обучения с помощью обучения с подкреплением (RL). Исследователи систематически изучали влияние различных факторов на RL-обучение как базовых, так и дообученных моделей. Было показано, что RL-обучение улучшает характеристики модели Qwen2.5-32B, увеличивая длину ответов и точность на тестах. Кроме того, использование инструментов значительно повысило способности моделей к рассуждению, достигнув точности 86.67% на наборе данных AIME 2024.'}, 'en': {'title': 'Enhancing Reasoning with Reinforcement Learning and Tool Manipulation', 'desc': 'This paper discusses advancements in slow-thinking models within the STILL project, focusing on reinforcement learning (RL) training techniques. The authors conduct systematic experiments to analyze how different factors affect RL training, leading to improvements in the Qwen2.5-32B base models. They demonstrate that even high-performing models can be further enhanced through RL training, achieving notable accuracy on benchmark tasks. Additionally, the study highlights the benefits of tool manipulation in improving reasoning performance, achieving impressive results on the AIME 2024 challenge.'}, 'zh': {'title': '强化学习提升推理模型的能力', 'desc': '本报告介绍了STILL项目中慢思维模型发展的第三个技术报告。我们系统地实验并记录了影响强化学习（RL）训练的各种因素，特别是在基础模型和微调模型上的实验。我们的研究表明，RL训练方法能够显著提高Qwen2.5-32B基础模型的响应长度和测试准确性。此外，我们还发现工具操作的使用显著提升了大型推理模型的推理性能，达到了86.67%的准确率。'}}}, {'id': 'https://huggingface.co/papers/2503.04359', 'title': 'LONGCODEU: Benchmarking Long-Context Language Models on Long Code\n  Understanding', 'url': 'https://huggingface.co/papers/2503.04359', 'abstract': "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.", 'score': 5, 'issue_id': 2617, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '808bf0135ca11113', 'authors': ['Jia Li', 'Xuyuan Guo', 'Lei Li', 'Kechi Zhang', 'Ge Li', 'Jia Li', 'Zhengwei Tao', 'Fang Liu', 'Chongyang Tao', 'Yuqi Zhu', 'Zhi Jin'], 'affiliations': ['Key Lab of High Confidence Software Technology (Peking University), MoE, School of Computer Science, Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04359.jpg', 'data': {'categories': ['#dataset', '#long_context', '#benchmark', '#optimization'], 'emoji': '📏', 'ru': {'title': 'Новый бенчмарк раскрывает ограничения языковых моделей в понимании длинного кода', 'desc': 'Статья представляет новый бенчмарк LONGCODEU для оценки способности языковых моделей с длинным контекстом (LCLM) понимать длинный программный код. Бенчмарк включает 8 задач в 4 аспектах: восприятие кодовых единиц, понимание внутри кодовых единиц, понимание связей между кодовыми единицами и понимание документации длинного кода. Результаты оценки 9 популярных LCLM показали, что их производительность значительно падает при длине кода более 32K токенов. Наиболее сложным аспектом для моделей оказалось понимание связей между кодовыми единицами.'}, 'en': {'title': 'Bridging the Gap in Long Code Understanding for LCLMs', 'desc': 'This paper addresses the challenges faced by advanced long-context language models (LCLMs) in understanding long code, which is crucial for software engineering. It introduces a new benchmark called LONGCODEU, designed to evaluate LCLMs across eight tasks that cover various aspects of long code comprehension. The study evaluates nine popular LCLMs and finds significant performance drops when handling code longer than 32K tokens, indicating limitations in their capabilities. The findings highlight that understanding relationships between code units is particularly difficult for these models, providing insights for future improvements in LCLMs.'}, 'zh': {'title': '提升长代码理解能力的关键评估基准', 'desc': '当前先进的长文本语言模型在软件工程应用中具有巨大潜力。然而，缺乏严格的评估框架限制了这一领域的进展。为了解决这个问题，我们提出了一个名为LONGCODEU的长代码理解基准，涵盖了四个方面的八个任务，以评估长代码理解能力。我们的实验结果显示，当前的长代码语言模型在处理超过32K的长代码时性能显著下降，尤其是在理解代码单元之间的关系方面最具挑战性。'}}}, {'id': 'https://huggingface.co/papers/2503.01713', 'title': 'SAGE: A Framework of Precise Retrieval for RAG', 'url': 'https://huggingface.co/papers/2503.01713', 'abstract': 'Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There is a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved.   In this paper, we introduce a RAG framework (SAGE), to overcome these limitations. First, to address the segmentation issue without considering semantics, we propose to train a semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design a chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score, leading to a more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experiments show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves a 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG.', 'score': 4, 'issue_id': 2613, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'dc1c8022a96cab3e', 'authors': ['Jintao Zhang', 'Guoliang Li', 'Jinyang Su'], 'affiliations': ['Department of Computer Science Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01713.jpg', 'data': {'categories': ['#rag', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'SAGE: Семантически улучшенный RAG для точных ответов на вопросы', 'desc': 'Статья представляет новый фреймворк SAGE для улучшения retrieval-augmented generation (RAG) в задачах вопросно-ответных систем. Авторы предлагают модель семантической сегментации корпуса текстов, алгоритм динамического выбора наиболее релевантных фрагментов и механизм оценки достаточности контекста с помощью языковых моделей. Эксперименты показывают, что SAGE превосходит базовые методы на 61.25% по качеству ответов и на 49.41% по эффективности использования токенов. Предложенный подход позволяет преодолеть ограничения существующих методов RAG и повысить точность извлечения релевантной информации.'}, 'en': {'title': 'SAGE: Smarter Retrieval for Better Question-Answering', 'desc': 'This paper presents a new framework called SAGE to improve retrieval-augmented generation (RAG) for question-answering tasks. It addresses two main issues: the ineffective segmentation of the corpus that ignores semantics and the trade-off between retrieving too little or too much context. SAGE introduces a semantic segmentation model to create meaningful chunks and a dynamic chunk selection algorithm to ensure only the most relevant information is retrieved. The results show that SAGE significantly enhances QA quality and cost efficiency compared to existing methods.'}, 'zh': {'title': '提升问答质量的智能检索框架', 'desc': '本文提出了一种新的检索增强生成框架（SAGE），旨在解决现有RAG方法在问答任务中的局限性。首先，SAGE通过训练语义分割模型，将语料库分割成语义完整的块，以提高相关性。其次，设计了一种动态选择算法，根据相关性得分的下降速度选择最相关的块，从而避免无关信息的干扰。实验结果表明，SAGE在问答质量上比基线提高了61.25%，并且在成本效率上提升了49.41%。'}}}, {'id': 'https://huggingface.co/papers/2503.01840', 'title': 'EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test', 'url': 'https://huggingface.co/papers/2503.01840', 'abstract': "The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE.", 'score': 3, 'issue_id': 2619, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'cf0ee90637c3e71a', 'authors': ['Yuhui Li', 'Fangyun Wei', 'Chao Zhang', 'Hongyang Zhang'], 'affiliations': ['Microsoft Research', 'Peking University', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01840.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#reasoning'], 'emoji': '🚀', 'ru': {'title': 'EAGLE-3: Революционное ускорение LLM без потери качества', 'desc': 'Статья представляет EAGLE-3, новый метод спекулятивной выборки для ускорения работы больших языковых моделей (LLM). В отличие от предыдущих версий, EAGLE-3 использует прямое предсказание токенов и многослойное слияние признаков, что позволяет полностью использовать преимущества увеличения объема обучающих данных. Эксперименты показывают, что EAGLE-3 достигает ускорения до 6,5 раз по сравнению с обычной авторегрессией, что примерно в 1,4 раза лучше, чем EAGLE-2. Метод был протестирован на различных задачах с использованием как диалоговых, так и рассуждающих моделей.'}, 'en': {'title': 'EAGLE-3: Speeding Up LLMs with Direct Token Prediction', 'desc': 'This paper presents EAGLE-3, an advanced model that improves the efficiency of large language models (LLMs) by shifting from feature prediction to direct token prediction. By utilizing multi-layer feature fusion instead of relying solely on top-layer features, EAGLE-3 enhances performance and allows for better utilization of larger training datasets. The authors demonstrate that EAGLE-3 achieves significant speed improvements, with a speedup ratio of up to 6.5 times compared to previous methods. The results indicate that EAGLE-3 not only accelerates inference but also improves model intelligence, making it a valuable advancement in the field of machine learning.'}, 'zh': {'title': 'EAGLE-3：提升大语言模型推理速度的创新方案', 'desc': '现代大语言模型（LLM）的顺序特性使其在推理时成本高且速度慢，推测采样是一种有效的解决方案。EAGLE方法在特征层面进行自回归，利用目标模型的顶层特征，取得比传统推测采样更好的效果。尽管在LLM社区中，扩大训练数据以提高模型智能的趋势日益增长，但我们发现对于EAGLE来说，扩大数据的效果有限。本文提出了EAGLE-3，放弃特征预测，采用直接的标记预测，并通过训练时测试的技术实现多层特征融合，从而显著提升性能。'}}}, {'id': 'https://huggingface.co/papers/2502.18968', 'title': 'Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles', 'url': 'https://huggingface.co/papers/2502.18968', 'abstract': 'User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications.', 'score': 3, 'issue_id': 2619, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '32f11f6f4b295fc5', 'authors': ['Kuang Wang', 'Xianfei Li', 'Shenghao Yang', 'Li Zhou', 'Feng Jiang', 'Haizhou Li'], 'affiliations': ['Shenzhen Research Institute of Big Data', 'Shenzhen University of Advanced Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.18968.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#optimization', '#games', '#agents', '#interpretability'], 'emoji': '🎭', 'ru': {'title': 'Реалистичная симуляция пользователей на основе неявных профилей', 'desc': 'Статья представляет новый подход к симуляции пользователей для диалоговых систем - User Simulator with implicit Profiles (USP). USP извлекает неявные профили пользователей из диалогов человека с машиной и использует их для генерации более персонализированных и реалистичных диалогов. Метод включает экстрактор на основе большой языковой модели, условное обучение с учителем и обучение с подкреплением. Эксперименты показывают превосходство USP над базовыми методами по аутентичности и разнообразию генерируемых диалогов.'}, 'en': {'title': 'Enhancing Dialogue Systems with Implicit User Profiles', 'desc': 'This paper introduces the User Simulator with implicit Profiles (USP), a novel framework designed to enhance dialogue systems by incorporating implicit user traits like personality and goals. Unlike traditional simulators that focus only on text, USP infers user profiles from actual human-machine interactions, allowing for more personalized dialogue generation. The framework employs a large language model (LLM) to extract user profiles and utilizes conditional supervised fine-tuning and reinforcement learning to improve dialogue quality. Experimental results show that USP significantly improves the authenticity and diversity of generated dialogues while maintaining consistency, making it effective for real-world applications.'}, 'zh': {'title': '隐式用户档案，提升对话真实感', 'desc': '本文提出了一种用户模拟器（USP），旨在通过隐式用户特征生成更个性化和真实的对话。现有的模拟器通常只依赖文本，忽视了用户的个性、说话风格和目标等隐性特征。USP通过从人机对话中推断隐式用户档案，结合大语言模型（LLM）和强化学习，优化对话生成过程。实验结果表明，USP在真实性和多样性方面优于现有方法，同时在一致性上表现相当，显示出其在实际应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.05315', 'title': 'LoRACode: LoRA Adapters for Code Embeddings', 'url': 'https://huggingface.co/papers/2503.05315', 'abstract': 'Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing proprietary systems impose substantial computational costs. We introduce a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to construct task-specific adapters for code retrieval. Our approach reduces the number of trainable parameters to less than two percent of the base model, enabling rapid fine-tuning on extensive code corpora (2 million samples in 25 minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks across multiple programming languages. Distinction in task-wise and language-wise adaptation helps explore the sensitivity of code retrieval for syntactical and linguistic variations.', 'score': 2, 'issue_id': 2610, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '95dca112be949ba8', 'authors': ['Saumya Chaturvedi', 'Aman Chadha', 'Laurent Bindschaedler'], 'affiliations': ['AWS GenAI Santa Clara, CA, USA', 'Max Planck Institute for Software Systems Saarbrucken, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.05315.jpg', 'data': {'categories': ['#data', '#open_source', '#training', '#optimization', '#plp'], 'emoji': '🔍', 'ru': {'title': 'LoRA: Эффективная тонкая настройка для точного поиска кода', 'desc': 'Статья представляет новый метод тонкой настройки для семантического поиска кода, основанный на Low-Rank Adaptation (LoRA). Этот подход значительно сокращает количество обучаемых параметров, позволяя быстро настраивать модели на больших объемах кода. Эксперименты показывают существенное улучшение показателей MRR для задач поиска Code2Code и Text2Code. Метод позволяет исследовать чувствительность поиска кода к синтаксическим и языковым вариациям.'}, 'en': {'title': 'Efficient Code Retrieval with Low-Rank Adaptation', 'desc': 'This paper addresses the challenges of semantic code search by improving how code embeddings are generated. It highlights the limitations of existing models like CodeBERT and UniXcoder in terms of scalability and efficiency. The authors propose a new method using Low-Rank Adaptation (LoRA) to create task-specific adapters, significantly reducing the number of trainable parameters. Their approach allows for quick fine-tuning on large code datasets, resulting in notable improvements in retrieval performance across various programming languages.'}, 'zh': {'title': '高效代码检索的低秩适应方法', 'desc': '本文提出了一种基于低秩适应（LoRA）的参数高效微调方法，用于构建特定任务的代码检索适配器。该方法将可训练参数减少到基础模型的不到2%，使得在大规模代码语料库上进行快速微调成为可能。实验结果显示，在代码到代码的检索任务中，平均倒数排名（MRR）提高了9.1%，而文本到代码的检索任务提高了86.69%。通过任务和语言的适应性区分，本文探讨了代码检索对语法和语言变体的敏感性。'}}}, {'id': 'https://huggingface.co/papers/2503.04504', 'title': 'AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM', 'url': 'https://huggingface.co/papers/2503.04504', 'abstract': 'Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive performance on VAD benchmark datasets, achieving state-of-the-art results on the UBnormal dataset and outperforming other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly.', 'score': 1, 'issue_id': 2613, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '7dbd20628d3eb105', 'authors': ['Sunghyun Ahn', 'Youngwan Jo', 'Kijung Lee', 'Sein Kwon', 'Inpyo Hong', 'Sanghyun Park'], 'affiliations': ['Yonsei University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.04504.jpg', 'data': {'categories': ['#open_source', '#optimization', '#dataset', '#benchmark', '#cv', '#video'], 'emoji': '🕵️', 'ru': {'title': 'Универсальное обнаружение аномалий в видео без переобучения', 'desc': 'Исследователи представили новый подход к обнаружению аномалий в видео, названный C-VAD. Они разработали модель AnyAnomaly, которая может обнаруживать аномальные события, определенные пользователем, без необходимости переобучения для новых сред. Модель использует контекстно-зависимое визуальное понимание вопросов и ответов на основе большой мультимодальной языковой модели. Эксперименты показали превосходство AnyAnomaly на специально созданных наборах данных C-VAD и конкурентоспособные результаты на стандартных бенчмарках обнаружения видеоаномалий.'}, 'en': {'title': 'Customizable Anomaly Detection for Diverse Video Environments', 'desc': 'This paper introduces a new approach to video anomaly detection (VAD) called customizable video anomaly detection (C-VAD). Unlike traditional VAD models that require retraining for different environments, C-VAD allows users to define what constitutes an abnormal event using text input. The AnyAnomaly model leverages a context-aware visual question answering system, enabling it to detect specified events without the need for extensive fine-tuning. The results show that AnyAnomaly not only performs well on custom datasets but also achieves state-of-the-art results on established VAD benchmarks, demonstrating its versatility and effectiveness.'}, 'zh': {'title': '可定制的视频异常检测，轻松应对多样环境', 'desc': '视频异常检测（VAD）在计算机视觉中的视频分析和监控中至关重要。现有的VAD模型依赖于学习到的正常模式，这使得它们在不同环境中的应用变得困难。为了解决这些问题，本研究提出了可定制的视频异常检测（C-VAD）技术和AnyAnomaly模型，允许用户定义异常事件并检测视频中的相关帧。我们的模型在多个基准数据集上表现出色，尤其在UBnormal数据集上达到了最先进的结果，展示了其在泛化能力上的优势。'}}}, {'id': 'https://huggingface.co/papers/2503.11647', 'title': 'ReCamMaster: Camera-Controlled Generative Rendering from A Single Video', 'url': 'https://huggingface.co/papers/2503.11647', 'abstract': 'Camera control has been actively studied in text or image conditioned video generation tasks. However, altering camera trajectories of a given video remains under-explored, despite its importance in the field of video creation. It is non-trivial due to the extra constraints of maintaining multiple-frame appearance and dynamic synchronization. To address this, we present ReCamMaster, a camera-controlled generative video re-rendering framework that reproduces the dynamic scene of an input video at novel camera trajectories. The core innovation lies in harnessing the generative capabilities of pre-trained text-to-video models through a simple yet powerful video conditioning mechanism -- its capability often overlooked in current research. To overcome the scarcity of qualified training data, we construct a comprehensive multi-camera synchronized video dataset using Unreal Engine 5, which is carefully curated to follow real-world filming characteristics, covering diverse scenes and camera movements. It helps the model generalize to in-the-wild videos. Lastly, we further improve the robustness to diverse inputs through a meticulously designed training strategy. Extensive experiments tell that our method substantially outperforms existing state-of-the-art approaches and strong baselines. Our method also finds promising applications in video stabilization, super-resolution, and outpainting. Project page: https://jianhongbai.github.io/ReCamMaster/', 'score': 80, 'issue_id': 2730, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '7e72838ea84ed904', 'authors': ['Jianhong Bai', 'Menghan Xia', 'Xiao Fu', 'Xintao Wang', 'Lianrui Mu', 'Jinwen Cao', 'Zuozhu Liu', 'Haoji Hu', 'Xiang Bai', 'Pengfei Wan', 'Di Zhang'], 'affiliations': ['CUHK', 'HUST', 'Kuaishou Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11647.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#video', '#games'], 'emoji': '🎥', 'ru': {'title': 'Управление камерой в видео с помощью генеративных моделей', 'desc': 'ReCamMaster - это генеративная система для изменения траектории камеры в видео. Она использует предобученные модели text-to-video и механизм видео-кондиционирования для воспроизведения динамической сцены с новых ракурсов. Для обучения был создан специальный датасет синхронизированных мультикамерных видео в Unreal Engine 5. Система превосходит существующие подходы и находит применение в стабилизации, суперразрешении и аутпейнтинге видео.'}, 'en': {'title': 'ReCamMaster: Mastering Camera Control in Video Generation', 'desc': 'This paper introduces ReCamMaster, a novel framework for generating videos with controlled camera trajectories. It addresses the challenge of maintaining visual consistency and dynamic synchronization across multiple frames when altering camera paths. The framework leverages pre-trained text-to-video models and a specially curated multi-camera synchronized video dataset to enhance its performance. Experimental results demonstrate that ReCamMaster significantly outperforms existing methods, showcasing its potential for applications like video stabilization and super-resolution.'}, 'zh': {'title': '重塑视频动态，掌控相机轨迹', 'desc': '本论文研究了在文本或图像条件下生成视频时的相机控制问题。尽管改变视频的相机轨迹很重要，但这一领域的研究仍然较少。我们提出了ReCamMaster，一个基于生成模型的视频重渲染框架，能够在新的相机轨迹下重现输入视频的动态场景。通过构建一个多相机同步视频数据集，并采用精心设计的训练策略，我们的方法在多种输入下表现出色，超越了现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2503.07677', 'title': 'PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference\n  Time by Leveraging Sparsity', 'url': 'https://huggingface.co/papers/2503.07677', 'abstract': 'Diffusion models have shown impressive results in generating high-quality conditional samples using guidance techniques such as Classifier-Free Guidance (CFG). However, existing methods often require additional training or neural function evaluations (NFEs), making them incompatible with guidance-distilled models. Also, they rely on heuristic approaches that need identifying target layers. In this work, we propose a novel and efficient method, termed PLADIS, which boosts pre-trained models (U-Net/Transformer) by leveraging sparse attention. Specifically, we extrapolate query-key correlations using softmax and its sparse counterpart in the cross-attention layer during inference, without requiring extra training or NFEs. By leveraging the noise robustness of sparse attention, our PLADIS unleashes the latent potential of text-to-image diffusion models, enabling them to excel in areas where they once struggled with newfound effectiveness. It integrates seamlessly with guidance techniques, including guidance-distilled models. Extensive experiments show notable improvements in text alignment and human preference, offering a highly efficient and universally applicable solution.', 'score': 65, 'issue_id': 2730, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '913b88ac595cc8b6', 'authors': ['Kwanyoung Kim', 'Byeongsu Sim'], 'affiliations': ['Samsung Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.07677.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#inference'], 'emoji': '🖼️', 'ru': {'title': 'PLADIS: Эффективное улучшение генерации изображений с помощью разреженного внимания', 'desc': 'Статья представляет новый метод под названием PLADIS для улучшения предобученных моделей диффузии в задаче генерации изображений по текстовому описанию. PLADIS использует разреженное внимание для экстраполяции корреляций запрос-ключ в слое кросс-внимания во время вывода, не требуя дополнительного обучения или оценок нейронных функций. Метод хорошо сочетается с существующими техниками направленной генерации, включая модели с дистиллированным направлением. Эксперименты показывают значительное улучшение соответствия текста и изображения, а также предпочтений пользователей.'}, 'en': {'title': 'Unlocking the Power of Diffusion Models with Sparse Attention', 'desc': "This paper introduces PLADIS, a new method that enhances pre-trained diffusion models like U-Net and Transformer by using sparse attention techniques. Unlike previous methods that needed extra training or evaluations, PLADIS operates efficiently during inference by utilizing query-key correlations in the cross-attention layer. This approach improves the models' performance in generating high-quality images from text prompts without the need for additional training. The results demonstrate significant advancements in text alignment and user preference, making PLADIS a versatile solution for various applications in text-to-image generation."}, 'zh': {'title': 'PLADIS：高效提升扩散模型的稀疏注意力方法', 'desc': '扩散模型在生成高质量条件样本方面表现出色，尤其是使用无分类器引导（CFG）等技术。然而，现有方法通常需要额外的训练或神经功能评估（NFE），这使得它们与引导蒸馏模型不兼容。本文提出了一种新颖高效的方法，称为PLADIS，通过利用稀疏注意力来增强预训练模型（如U-Net/Transformer）。PLADIS在推理过程中利用交叉注意力层中的softmax和稀疏对应物，提升文本到图像的扩散模型的潜力，显著改善文本对齐和人类偏好。'}}}, {'id': 'https://huggingface.co/papers/2503.11646', 'title': 'Adversarial Data Collection: Human-Collaborative Perturbations for\n  Efficient and Robust Robotic Imitation Learning', 'url': 'https://huggingface.co/papers/2503.11646', 'abstract': 'The pursuit of data efficiency, where quality outweighs quantity, has emerged as a cornerstone in robotic manipulation, especially given the high costs associated with real-world data collection. We propose that maximizing the informational density of individual demonstrations can dramatically reduce reliance on large-scale datasets while improving task performance. To this end, we introduce Adversarial Data Collection, a Human-in-the-Loop (HiL) framework that redefines robotic data acquisition through real-time, bidirectional human-environment interactions. Unlike conventional pipelines that passively record static demonstrations, ADC adopts a collaborative perturbation paradigm: during a single episode, an adversarial operator dynamically alters object states, environmental conditions, and linguistic commands, while the tele-operator adaptively adjusts actions to overcome these evolving challenges. This process compresses diverse failure-recovery behaviors, compositional task variations, and environmental perturbations into minimal demonstrations. Our experiments demonstrate that ADC-trained models achieve superior compositional generalization to unseen task instructions, enhanced robustness to perceptual perturbations, and emergent error recovery capabilities. Strikingly, models trained with merely 20% of the demonstration volume collected through ADC significantly outperform traditional approaches using full datasets. These advances bridge the gap between data-centric learning paradigms and practical robotic deployment, demonstrating that strategic data acquisition, not merely post-hoc processing, is critical for scalable, real-world robot learning. Additionally, we are curating a large-scale ADC-Robotics dataset comprising real-world manipulation tasks with adversarial perturbations. This benchmark will be open-sourced to facilitate advancements in robotic imitation learning.', 'score': 31, 'issue_id': 2731, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'efdf1296bc567414', 'authors': ['Siyuan Huang', 'Yue Liao', 'Siyuan Feng', 'Shu Jiang', 'Si Liu', 'Hongsheng Li', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['Agibot', 'Beihang University', 'MMLab, CUHK', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.11646.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#dataset', '#optimization', '#open_source', '#agents', '#training', '#data'], 'emoji': '🤖', 'ru': {'title': 'Меньше данных, больше эффективности: революция в обучении роботов', 'desc': 'Статья представляет новый подход к сбору данных для обучения роботов манипуляции - Adversarial Data Collection (ADC). ADC использует взаимодействие человека-оператора и среды в реальном времени для создания информационно насыщенных демонстраций. Эксперименты показывают, что модели, обученные на ADC-данных, достигают лучшей композиционной генерализации и устойчивости к возмущениям, чем традиционные подходы. Авторы также создают открытый набор данных ADC-Robotics для продвижения исследований в области имитационного обучения роботов.'}, 'en': {'title': 'Maximizing Data Efficiency in Robotic Learning with Adversarial Collection', 'desc': 'This paper introduces a new method called Adversarial Data Collection (ADC) to improve robotic manipulation by focusing on data efficiency. Instead of relying on large datasets, ADC enhances the quality of data through real-time interactions between humans and robots, allowing for dynamic adjustments during demonstrations. By using an adversarial approach, the framework captures a wide range of behaviors and challenges in fewer demonstrations, leading to better performance in unseen tasks. The results show that models trained with ADC can generalize better and recover from errors more effectively, even with significantly less data than traditional methods.'}, 'zh': {'title': '对抗性数据收集：提升机器人学习效率的关键', 'desc': '本论文提出了一种新的数据收集方法，称为对抗性数据收集（ADC），旨在提高机器人操作的效率。通过实时的人机交互，ADC能够在动态环境中收集高信息密度的演示数据，从而减少对大规模数据集的依赖。实验表明，使用ADC训练的模型在面对未见任务指令时表现出更好的组合泛化能力和对环境干扰的鲁棒性。最终，ADC方法显著提高了机器人学习的实用性，展示了战略性数据获取的重要性。'}}}, {'id': 'https://huggingface.co/papers/2503.11224', 'title': 'Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models', 'url': 'https://huggingface.co/papers/2503.11224', 'abstract': 'State Space Models (SSMs) have emerged as a promising alternative to the popular transformer-based models and have been increasingly gaining attention. Compared to transformers, SSMs excel at tasks with sequential data or longer contexts, demonstrating comparable performances with significant efficiency gains. In this survey, we provide a coherent and systematic overview for SSMs, including their theoretical motivations, mathematical formulations, comparison with existing model classes, and various applications. We divide the SSM series into three main sections, providing a detailed introduction to the original SSM, the structured SSM represented by S4, and the selective SSM typified by Mamba. We put an emphasis on technicality, and highlight the various key techniques introduced to address the effectiveness and efficiency of SSMs. We hope this manuscript serves as an introduction for researchers to explore the theoretical foundations of SSMs.', 'score': 21, 'issue_id': 2731, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'fb4219d497e59f64', 'authors': ['Xingtai Lv', 'Youbang Sun', 'Kaiyan Zhang', 'Shang Qu', 'Xuekai Zhu', 'Yuchen Fan', 'Yi Wu', 'Ermo Hua', 'Xinwei Long', 'Ning Ding', 'Bowen Zhou'], 'affiliations': ['Department of Electronic Engineering, Tsinghua University', 'Robotics Institute, Carnegie Mellon University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.11224.jpg', 'data': {'categories': ['#architecture', '#long_context', '#survey', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'SSM: Эффективная альтернатива трансформерам для обработки последовательностей', 'desc': 'Это обзор моделей пространства состояний (SSM) как альтернативы трансформерам в машинном обучении. SSM показывают сравнимую производительность с трансформерами, но более эффективны для задач с последовательными данными и длинным контекстом. В статье рассматриваются теоретические основы, математические формулировки и применения SSM. Авторы выделяют три основных типа SSM: оригинальные, структурированные (например, S4) и селективные (например, Mamba).'}, 'en': {'title': 'Unlocking Efficiency: The Power of State Space Models', 'desc': 'State Space Models (SSMs) are gaining popularity as an efficient alternative to transformer models, especially for sequential data and longer contexts. This paper provides a comprehensive overview of SSMs, detailing their theoretical foundations, mathematical formulations, and comparisons with other model types. It categorizes SSMs into three sections: the original SSM, the structured S4 model, and the selective Mamba model, emphasizing their unique techniques for improved performance. The goal is to guide researchers in understanding and exploring the potential of SSMs in various applications.'}, 'zh': {'title': '状态空间模型：高效处理序列数据的新选择', 'desc': '状态空间模型（SSMs）作为一种有前景的替代方案，逐渐受到关注，尤其是在处理序列数据或长上下文任务时表现优异。与流行的变换器模型相比，SSMs在效率上有显著提升，同时在性能上也能与之媲美。本文对SSMs进行了系统的概述，包括其理论动机、数学公式、与现有模型的比较以及各种应用。我们将SSM系列分为三个主要部分，详细介绍了原始SSM、结构化SSM（如S4）和选择性SSM（如Mamba），并强调了提高SSM有效性和效率的关键技术。'}}}, {'id': 'https://huggingface.co/papers/2503.11069', 'title': 'API Agents vs. GUI Agents: Divergence and Convergence', 'url': 'https://huggingface.co/papers/2503.11069', 'abstract': 'Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.', 'score': 20, 'issue_id': 2730, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '29e714954ed20978', 'authors': ['Chaoyun Zhang', 'Shilin He', 'Liqun Li', 'Si Qin', 'Yu Kang', 'Qingwei Lin', 'Dongmei Zhang'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2503.11069.jpg', 'data': {'categories': ['#multimodal', '#survey', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Сравнение API и GUI агентов на основе LLM: путь к гибридным решениям', 'desc': 'Это исследование сравнивает агентов на основе больших языковых моделей (LLM), работающих через API и через графический интерфейс. Авторы анализируют различия в архитектуре, разработке и взаимодействии с пользователем для обоих подходов. Они предлагают критерии выбора и описывают сценарии, где гибридные решения могут быть эффективны. Исследование показывает, что инновации в автоматизации на основе LLM стирают границы между этими парадигмами.'}, 'en': {'title': 'Bridging the Gap: API and GUI LLM Agents Unite', 'desc': 'This paper explores the evolution of large language models (LLMs) from simple text generators to sophisticated software agents that can perform tasks based on natural language commands. It compares two main types of LLM agents: API-based agents, which automate tasks through programmatic interfaces, and GUI-based agents, which interact with graphical user interfaces like humans. The study highlights the differences in their architecture, development processes, and user interactions, while also discussing how hybrid approaches can leverage the strengths of both paradigms. The authors provide decision criteria and practical examples to help users choose the right approach for their needs, suggesting that future advancements will further integrate these two types of agents.'}, 'zh': {'title': 'API与GUI代理的比较与融合之路', 'desc': '大型语言模型（LLMs）已经从简单的文本生成发展到能够将自然语言命令直接转化为实际操作的软件代理。本文首次全面比较了基于API的LLM代理和基于GUI的LLM代理，分析了它们在架构复杂性、开发工作流程和用户交互模型上的显著差异。我们探讨了关键维度，并强调了混合方法在利用两者互补优势方面的场景。最终，我们指出LLM驱动的自动化创新将模糊API和GUI代理之间的界限，为各种实际应用提供更灵活、适应性强的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.11514', 'title': 'Exploring the Vulnerabilities of Federated Learning: A Deep Dive into\n  Gradient Inversion Attacks', 'url': 'https://huggingface.co/papers/2503.11514', 'abstract': 'Federated Learning (FL) has emerged as a promising privacy-preserving collaborative model training paradigm without sharing raw data. However, recent studies have revealed that private information can still be leaked through shared gradient information and attacked by Gradient Inversion Attacks (GIA). While many GIA methods have been proposed, a detailed analysis, evaluation, and summary of these methods are still lacking. Although various survey papers summarize existing privacy attacks in FL, few studies have conducted extensive experiments to unveil the effectiveness of GIA and their associated limiting factors in this context. To fill this gap, we first undertake a systematic review of GIA and categorize existing methods into three types, i.e., optimization-based GIA (OP-GIA), generation-based GIA (GEN-GIA), and analytics-based GIA (ANA-GIA). Then, we comprehensively analyze and evaluate the three types of GIA in FL, providing insights into the factors that influence their performance, practicality, and potential threats. Our findings indicate that OP-GIA is the most practical attack setting despite its unsatisfactory performance, while GEN-GIA has many dependencies and ANA-GIA is easily detectable, making them both impractical. Finally, we offer a three-stage defense pipeline to users when designing FL frameworks and protocols for better privacy protection and share some future research directions from the perspectives of attackers and defenders that we believe should be pursued. We hope that our study can help researchers design more robust FL frameworks to defend against these attacks.', 'score': 13, 'issue_id': 2730, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'd31bf6f9bd4bc86b', 'authors': ['Pengxin Guo', 'Runxi Wang', 'Shuang Zeng', 'Jinjing Zhu', 'Haoning Jiang', 'Yanran Wang', 'Yuyin Zhou', 'Feifei Wang', 'Hui Xiong', 'Liangqiong Qu'], 'affiliations': ['Department of Biomedical Data Science, Stanford University, Stanford, CA 94305, USA', 'Department of Computer Science and Engineering, University of California, Santa Cruz, CA 95064, USA', 'Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong 999077, China', 'Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen 518055, China', 'Department of Mathematics, The University of Hong Kong, Hong Kong 999077, China', 'Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen 518055, China', 'School of Computing and Data Science, The University of Hong Kong, Hong Kong 999077, China', 'Thrust of Artificial Intelligence, The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511458, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.11514.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#security', '#survey', '#healthcare', '#data'], 'emoji': '🛡️', 'ru': {'title': 'Защита приватности в федеративном обучении: анализ атак с инверсией градиента', 'desc': 'Статья посвящена анализу атак с инверсией градиента (GIA) в контексте федеративного обучения (FL). Авторы классифицируют существующие методы GIA на три типа: оптимизационные, генеративные и аналитические. Проводится комплексная оценка эффективности и практичности каждого типа атак в FL. Исследование показывает, что оптимизационные GIA являются наиболее практичными, несмотря на их неудовлетворительную производительность.'}, 'en': {'title': 'Strengthening Privacy in Federated Learning Against Gradient Inversion Attacks', 'desc': "This paper focuses on the vulnerabilities of Federated Learning (FL) to Gradient Inversion Attacks (GIA), which can leak private information despite the model's privacy-preserving intentions. It categorizes existing GIA methods into three types: optimization-based, generation-based, and analytics-based, and provides a thorough analysis of their effectiveness and limitations. The study reveals that while optimization-based GIA is the most practical, it still has performance issues, whereas generation-based and analytics-based methods are less practical due to their dependencies and detectability. The authors propose a defense strategy to enhance privacy in FL frameworks and suggest future research directions to strengthen defenses against these attacks."}, 'zh': {'title': '提升联邦学习隐私保护的防御策略', 'desc': '联邦学习（FL）是一种保护隐私的协作模型训练方法，不需要共享原始数据。然而，最近的研究表明，通过共享梯度信息，私密信息仍然可能被泄露，并受到梯度反演攻击（GIA）的威胁。本文对现有的GIA方法进行了系统的回顾和分类，并分析了三种类型的GIA在FL中的表现和局限性。最后，我们提出了一个三阶段的防御方案，以帮助用户在设计FL框架时更好地保护隐私。'}}}, {'id': 'https://huggingface.co/papers/2503.10772', 'title': 'FlowTok: Flowing Seamlessly Across Text and Image Tokens', 'url': 'https://huggingface.co/papers/2503.10772', 'abstract': 'Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolving between text and image modalities through flow matching. This requires projecting both modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3x at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speeds-all while delivering performance comparable to state-of-the-art models. Code will be available at https://github.com/bytedance/1d-tokenizer.', 'score': 12, 'issue_id': 2731, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '548255900cd1ec21', 'authors': ['Ju He', 'Qihang Yu', 'Qihao Liu', 'Liang-Chieh Chen'], 'affiliations': ['ByteDance Seed', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10772.jpg', 'data': {'categories': ['#architecture', '#training', '#multimodal', '#diffusion'], 'emoji': '🌊', 'ru': {'title': 'FlowTok: эффективный переход между текстом и изображением через токены', 'desc': 'FlowTok - это новый подход к генерации изображений из текста и наоборот. Он использует метод согласования потоков для прямого перехода между модальностями текста и изображения в общем латентном пространстве. Ключевая идея заключается в кодировании изображений в компактное одномерное токенное представление, что значительно уменьшает размер латентного пространства. Это позволяет упростить архитектуру, сократить потребление памяти и ускорить обучение и генерацию по сравнению с существующими методами.'}, 'en': {'title': 'FlowTok: Simplifying Cross-Modality Generation with 1D Tokens', 'desc': 'This paper presents FlowTok, a novel framework for cross-modality generation that simplifies the process of transitioning between text and image modalities. Unlike traditional methods that rely on complex conditioning signals and denoising processes, FlowTok directly evolves between text and images by utilizing flow matching in a shared latent space. The framework encodes images into a compact 1D token representation, significantly reducing the latent space size and improving efficiency. FlowTok not only enhances memory usage and training resource requirements but also maintains competitive performance in generating images from text and vice versa.'}, 'zh': {'title': 'FlowTok：简化跨模态生成的高效框架', 'desc': '这篇论文探讨了跨模态生成中的不同模态之间的桥接问题。传统方法将文本模态视为引导信号，逐步引导去噪过程，而我们提出了一种更简单的方法，通过流匹配直接在文本和图像模态之间演变。我们引入了FlowTok框架，将图像编码为紧凑的1D标记表示，从而在共享潜在空间中流动，显著减少了潜在空间的大小。FlowTok不仅提高了内存效率和采样速度，还在图像到文本生成方面表现出色，性能与最先进的模型相当。'}}}, {'id': 'https://huggingface.co/papers/2503.11576', 'title': 'SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion', 'url': 'https://huggingface.co/papers/2503.11576', 'abstract': 'We introduce SmolDocling, an ultra-compact vision-language model targeting end-to-end document conversion. Our model comprehensively processes entire pages by generating DocTags, a new universal markup format that captures all page elements in their full context with location. Unlike existing approaches that rely on large foundational models, or ensemble solutions that rely on handcrafted pipelines of multiple specialized models, SmolDocling offers an end-to-end conversion for accurately capturing content, structure and spatial location of document elements in a 256M parameters vision-language model. SmolDocling exhibits robust performance in correctly reproducing document features such as code listings, tables, equations, charts, lists, and more across a diverse range of document types including business documents, academic papers, technical reports, patents, and forms -- significantly extending beyond the commonly observed focus on scientific papers. Additionally, we contribute novel publicly sourced datasets for charts, tables, equations, and code recognition. Experimental results demonstrate that SmolDocling competes with other Vision Language Models that are up to 27 times larger in size, while reducing computational requirements substantially. The model is currently available, datasets will be publicly available soon.', 'score': 11, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '5548a7c6526f8753', 'authors': ['Ahmed Nassar', 'Andres Marafioti', 'Matteo Omenetti', 'Maksym Lysak', 'Nikolaos Livathinos', 'Christoph Auer', 'Lucas Morin', 'Rafael Teixeira de Lima', 'Yusik Kim', 'A. Said Gurbuz', 'Michele Dolfi', 'Miquel Farré', 'Peter W. J. Staar'], 'affiliations': ['HuggingFace', 'IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.11576.jpg', 'data': {'categories': ['#small_models', '#open_source', '#cv', '#dataset', '#science', '#multimodal'], 'emoji': '📄', 'ru': {'title': 'SmolDocling: компактная модель для комплексной обработки документов', 'desc': 'SmolDocling - это компактная модель обработки документов, сочетающая зрение и язык. Она генерирует универсальную разметку DocTags, захватывающую все элементы страницы с их расположением. Модель показывает надежную производительность в воспроизведении различных особенностей документов, включая листинги кода, таблицы, уравнения и диаграммы. SmolDocling конкурирует с моделями в 27 раз большего размера, существенно снижая вычислительные требования.'}, 'en': {'title': 'SmolDocling: Compact and Powerful Document Conversion', 'desc': 'SmolDocling is a compact vision-language model designed for end-to-end document conversion. It generates DocTags, a universal markup format that captures the content, structure, and spatial location of document elements. Unlike traditional methods that use large models or complex pipelines, SmolDocling achieves high accuracy with only 256 million parameters. It performs well across various document types, including business and academic papers, and introduces new datasets for recognizing charts, tables, equations, and code.'}, 'zh': {'title': 'SmolDocling：高效文档转换的新选择', 'desc': '我们介绍了SmolDocling，这是一种超紧凑的视觉-语言模型，旨在实现端到端的文档转换。该模型通过生成DocTags，一种新的通用标记格式，全面处理整个页面，捕捉所有页面元素的完整上下文和位置。与依赖大型基础模型或多个专用模型的手工管道的现有方法不同，SmolDocling提供了一种端到端的转换，准确捕捉文档元素的内容、结构和空间位置。实验结果表明，SmolDocling在性能上与其他高达27倍大小的视觉语言模型竞争，同时显著降低了计算需求。'}}}, {'id': 'https://huggingface.co/papers/2503.10970', 'title': 'TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools', 'url': 'https://huggingface.co/papers/2503.10970', 'abstract': 'Precision therapeutics require multimodal adaptive models that generate personalized treatment recommendations. We introduce TxAgent, an AI agent that leverages multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindications, and patient-specific treatment strategies. TxAgent evaluates how drugs interact at molecular, pharmacokinetic, and clinical levels, identifies contraindications based on patient comorbidities and concurrent medications, and tailors treatment strategies to individual patient characteristics. It retrieves and synthesizes evidence from multiple biomedical sources, assesses interactions between drugs and patient conditions, and refines treatment recommendations through iterative reasoning. It selects tools based on task objectives and executes structured function calls to solve therapeutic tasks that require clinical reasoning and cross-source validation. The ToolUniverse consolidates 211 tools from trusted sources, including all US FDA-approved drugs since 1939 and validated clinical insights from Open Targets. TxAgent outperforms leading LLMs, tool-use models, and reasoning agents across five new benchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC, covering 3,168 drug reasoning tasks and 456 personalized treatment scenarios. It achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning. TxAgent generalizes across drug name variants and descriptions. By integrating multi-step inference, real-time knowledge grounding, and tool-assisted decision-making, TxAgent ensures that treatment recommendations align with established clinical guidelines and real-world evidence, reducing the risk of adverse events and improving therapeutic decision-making.', 'score': 10, 'issue_id': 2732, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'b7a03e6b34c3c0de', 'authors': ['Shanghua Gao', 'Richard Zhu', 'Zhenglun Kong', 'Ayush Noori', 'Xiaorui Su', 'Curtis Ginder', 'Theodoros Tsiligkaridis', 'Marinka Zitnik'], 'affiliations': ['Broad Institute of MIT and Harvard, Cambridge, MA', 'Cardiovascular Division, Department of Medicine, Brigham and Womens Hospital, Harvard Medical School, Boston, MA', 'Department of Biomedical Informatics, Harvard Medical School, Boston, MA', 'Harvard Data Science Initiative, Cambridge, MA', 'Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA', 'MIT Lincoln Laboratory, Lexington, MA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10970.jpg', 'data': {'categories': ['#alignment', '#healthcare', '#science', '#agents', '#reasoning', '#benchmark', '#multimodal'], 'emoji': '💊', 'ru': {'title': 'TxAgent: ИИ-помощник для точной и персонализированной фармакотерапии', 'desc': 'TxAgent - это ИИ-агент для персонализированной терапии, использующий многоэтапное рассуждение и извлечение биомедицинских знаний в реальном времени. Он анализирует взаимодействия лекарств, противопоказания и индивидуальные стратегии лечения, используя набор из 211 инструментов. TxAgent превосходит ведущие языковые модели и другие агенты в пяти новых бенчмарках, охватывающих 3168 задач по рассуждению о лекарствах и 456 персонализированных сценариев лечения. Интегрируя многоэтапный вывод, актуальные знания и инструментальное принятие решений, TxAgent обеспечивает соответствие рекомендаций клиническим руководствам и реальным данным.'}, 'en': {'title': 'TxAgent: Personalized Precision Therapeutics through Advanced AI Reasoning', 'desc': 'The paper presents TxAgent, an advanced AI agent designed for precision therapeutics that generates personalized treatment recommendations. It utilizes multi-step reasoning and real-time biomedical knowledge retrieval from a comprehensive toolbox of 211 tools to analyze drug interactions and contraindications. TxAgent evaluates drug interactions at various levels and tailors treatment strategies based on individual patient characteristics, ensuring recommendations are evidence-based. The system outperforms existing models in drug reasoning tasks, achieving high accuracy and improving therapeutic decision-making by integrating clinical guidelines and real-world evidence.'}, 'zh': {'title': '个性化治疗的智能助手TxAgent', 'desc': '精准治疗需要多模态自适应模型来生成个性化的治疗建议。我们介绍了TxAgent，这是一种利用多步推理和实时生物医学知识检索的人工智能代理，能够分析药物相互作用、禁忌症和患者特定的治疗策略。TxAgent在分子、药代动力学和临床层面评估药物相互作用，并根据患者的合并症和同时用药识别禁忌症，量身定制治疗策略。通过整合多步推理、实时知识基础和工具辅助决策，TxAgent确保治疗建议符合既定的临床指南和现实世界证据，从而降低不良事件的风险，改善治疗决策。'}}}, {'id': 'https://huggingface.co/papers/2503.10781', 'title': 'Large-scale Pre-training for Grounded Video Caption Generation', 'url': 'https://huggingface.co/papers/2503.10781', 'abstract': 'We propose a novel approach for captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally dense bounding boxes. We introduce the following contributions. First, we present a large-scale automatic annotation method that aggregates captions grounded with bounding boxes across individual frames into temporally dense and consistent bounding box annotations. We apply this approach on the HowTo100M dataset to construct a large-scale pre-training dataset, named HowToGround1M. We also introduce a Grounded Video Caption Generation model, dubbed GROVE, and pre-train the model on HowToGround1M. Second, we introduce a new dataset, called iGround, of 3500 videos with manually annotated captions and dense spatio-temporally grounded bounding boxes. This allows us to measure progress on this challenging problem, as well as to fine-tune our model on this small-scale but high-quality data. Third, we demonstrate that our approach achieves state-of-the-art results on the proposed iGround dataset compared to a number of baselines, as well as on the VidSTG and ActivityNet-Entities datasets. We perform extensive ablations that demonstrate the importance of pre-training using our automatically annotated HowToGround1M dataset followed by fine-tuning on the manually annotated iGround dataset and validate the key technical contributions of our model.', 'score': 10, 'issue_id': 2737, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '842a44eb006952de', 'authors': ['Evangelos Kazakos', 'Cordelia Schmid', 'Josef Sivic'], 'affiliations': ['Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague', 'Inria, Ecole normale superieure, CNRS, PSL Research University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10781.jpg', 'data': {'categories': ['#cv', '#dataset', '#training', '#video', '#data'], 'emoji': '🎥', 'ru': {'title': 'Революция в понимании видео: от автоматической разметки к точной локализации объектов', 'desc': 'Статья представляет новый подход к генерации подписей и локализации объектов в видео с использованием темпорально плотных ограничивающих рамок. Авторы создали большой датасет HowToGround1M для предобучения модели GROVE, а также набор данных iGround с ручной разметкой для точной настройки. Предложенный метод достигает передовых результатов на нескольких наборах данных, включая VidSTG и ActivityNet-Entities. Эксперименты подтверждают важность предобучения на автоматически размеченных данных с последующей точной настройкой на вручную аннотированном наборе.'}, 'en': {'title': 'Grounding Video Captions with Precision', 'desc': 'This paper presents a new method for video captioning and object grounding, where objects mentioned in captions are linked to specific locations in the video using detailed bounding boxes. The authors introduce a large-scale automatic annotation technique that creates consistent bounding box annotations across video frames, resulting in a dataset called HowToGround1M for pre-training. They also develop a model named GROVE, which is trained on this dataset and further fine-tuned on a smaller, high-quality dataset called iGround, containing manually annotated videos. The results show that their approach outperforms existing methods on several benchmark datasets, highlighting the effectiveness of their pre-training and fine-tuning strategy.'}, 'zh': {'title': '视频字幕生成与物体定位的新方法', 'desc': '本文提出了一种新颖的视频字幕生成和物体定位方法，通过时间密集的边界框将字幕中的物体与视频中的内容关联起来。我们介绍了一种大规模自动注释方法，将单帧的边界框注释聚合为时间上密集且一致的边界框注释，并在HowTo100M数据集上构建了一个名为HowToGround1M的大规模预训练数据集。我们还提出了一个名为GROVE的基于视频的字幕生成模型，并在HowToGround1M上进行了预训练。此外，我们创建了一个名为iGround的新数据集，包含3500个视频及其手动注释的字幕和密集的时空边界框，以便于评估模型的进展和进行微调。'}}}, {'id': 'https://huggingface.co/papers/2503.11579', 'title': 'Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers', 'url': 'https://huggingface.co/papers/2503.11579', 'abstract': 'State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640times360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks.', 'score': 9, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': 'e0a1f364990dbe23', 'authors': ['Weiming Ren', 'Wentao Ma', 'Huan Yang', 'Cong Wei', 'Ge Zhang', 'Wenhu Chen'], 'affiliations': ['1.AI', 'M-A-P', 'University of Toronto', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.11579.jpg', 'data': {'categories': ['#optimization', '#long_context', '#benchmark', '#architecture', '#video'], 'emoji': '🎥', 'ru': {'title': 'VAMBA: эффективная обработка длинных видео с линейной сложностью', 'desc': 'Статья представляет новую модель VAMBA, гибрид Mamba и трансформера, для обработки длинных видео. VAMBA использует блоки Mamba-2 для кодирования видеотокенов с линейной сложностью, что позволяет обрабатывать более 1024 кадров на одном GPU. Модель снижает использование памяти GPU на 50% и почти вдвое ускоряет обучение по сравнению с трансформерными моделями. VAMBA показывает улучшение точности на 4.3% на бенчмарке LVBench для понимания часовых видео.'}, 'en': {'title': 'VAMBA: Efficient Video Processing with Linear Complexity', 'desc': 'This paper introduces the Mamba-Transformer model (VAMBA), which addresses the limitations of existing transformer-based large multimodal models (LMMs) in processing long video inputs. Unlike traditional methods that reduce video tokens and often lose information, VAMBA uses Mamba-2 blocks to encode video tokens with linear complexity, allowing it to handle over 1024 frames efficiently. The model significantly reduces GPU memory usage by at least 50% during training and inference, while also increasing training speed nearly twofold compared to standard transformers. Experimental results show that VAMBA not only enhances accuracy on the LVBench benchmark but also performs well across various video understanding tasks.'}, 'zh': {'title': '高效视频理解的新突破：VAMBA模型', 'desc': '本论文提出了一种新的混合Mamba-Transformer模型（VAMBA），旨在解决现有多模态模型在处理长视频输入时的计算复杂性问题。VAMBA使用Mamba-2模块以线性复杂度编码视频标记，避免了信息损失，并且无需减少标记数量。与传统的变换器模型相比，VAMBA在单个GPU上能够编码超过1024帧的视频，显著提高了训练和推理的速度，并减少了GPU内存使用。实验结果表明，VAMBA在长视频理解基准测试中比之前的高效视频模型提高了4.3%的准确率，同时在多种视频理解任务中保持了强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.11651', 'title': 'VGGT: Visual Geometry Grounded Transformer', 'url': 'https://huggingface.co/papers/2503.11651', 'abstract': 'We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.', 'score': 8, 'issue_id': 2740, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '701338c26ac42ac7', 'authors': ['Jianyuan Wang', 'Minghao Chen', 'Nikita Karaev', 'Andrea Vedaldi', 'Christian Rupprecht', 'David Novotny'], 'affiliations': ['Meta AI', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2503.11651.jpg', 'data': {'categories': ['#cv', '#open_source', '#3d', '#optimization'], 'emoji': '🔮', 'ru': {'title': 'VGGT: Универсальная нейросеть для комплексного 3D-анализа сцен', 'desc': 'VGGT - это нейронная сеть прямого распространения, которая напрямую выводит все ключевые 3D-атрибуты сцены из одного или нескольких её изображений. Модель эффективно реконструирует изображения менее чем за секунду, превосходя альтернативы, требующие постобработки. VGGT достигает передовых результатов в нескольких 3D-задачах, включая оценку параметров камеры и глубины, реконструкцию облака точек и 3D-трекинг. Использование предобученной VGGT в качестве основы значительно улучшает работу в задачах синтеза новых ракурсов и нежесткого трекинга точек.'}, 'en': {'title': 'VGGT: Revolutionizing 3D Scene Understanding with Speed and Efficiency', 'desc': 'VGGT is a feed-forward neural network designed to extract key 3D attributes from various views of a scene, such as camera parameters and depth maps. Unlike traditional models that focus on single tasks, VGGT efficiently handles multiple 3D computer vision tasks simultaneously. It operates quickly, reconstructing images in under one second while achieving superior results compared to methods that rely on post-processing. Additionally, VGGT can be used as a feature backbone to improve performance in related tasks like point tracking and novel view synthesis.'}, 'zh': {'title': 'VGGT：高效的3D场景推断网络', 'desc': '我们提出了VGGT，这是一种前馈神经网络，可以直接推断场景的所有关键3D属性，包括相机参数、点图、深度图和3D点轨迹。该方法在3D计算机视觉领域向前迈出了一步，克服了以往模型仅限于单一任务的局限性。VGGT简单高效，能够在不到一秒的时间内重建图像，并且在多个3D任务中表现优于需要后处理的替代方案。使用预训练的VGGT作为特征骨干显著提升了下游任务的性能，如非刚性点跟踪和前馈新视图合成。'}}}, {'id': 'https://huggingface.co/papers/2503.10632', 'title': 'Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision\n  Transformers?', 'url': 'https://huggingface.co/papers/2503.10632', 'abstract': "Kolmogorov-Arnold networks (KANs) are a remarkable innovation consisting of learnable activation functions with the potential to capture more complex relationships from data. Although KANs are useful in finding symbolic representations and continual learning of one-dimensional functions, their effectiveness in diverse machine learning (ML) tasks, such as vision, remains questionable. Presently, KANs are deployed by replacing multilayer perceptrons (MLPs) in deep network architectures, including advanced architectures such as vision Transformers (ViTs). In this paper, we are the first to design a general learnable Kolmogorov-Arnold Attention (KArAt) for vanilla ViTs that can operate on any choice of basis. However, the computing and memory costs of training them motivated us to propose a more modular version, and we designed particular learnable attention, called Fourier-KArAt. Fourier-KArAt and its variants either outperform their ViT counterparts or show comparable performance on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. We dissect these architectures' performance and generalization capacity by analyzing their loss landscapes, weight distributions, optimizer path, attention visualization, and spectral behavior, and contrast them with vanilla ViTs. The goal of this paper is not to produce parameter- and compute-efficient attention, but to encourage the community to explore KANs in conjunction with more advanced architectures that require a careful understanding of learnable activations. Our open-source code and implementation details are available on: https://subhajitmaity.me/KArAt", 'score': 8, 'issue_id': 2731, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '46504216bbce5b86', 'authors': ['Subhajit Maity', 'Killian Hitsman', 'Xin Li', 'Aritra Dutta'], 'affiliations': ['Department of Computer Science, University of Central Florida, Orlando, FL, USA', 'Department of Mathematics, University of Central Florida, Orlando, FL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.10632.jpg', 'data': {'categories': ['#architecture', '#cv', '#optimization', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый взгляд на внимание: Колмогоров-Арнольд сети в Vision Transformers', 'desc': 'Статья представляет новый подход к архитектуре нейронных сетей - Колмогоров-Арнольд внимание (KArAt) для моделей Vision Transformer. Авторы разработали модульную версию KArAt на основе преобразования Фурье, которая показывает сопоставимую или превосходящую производительность по сравнению с обычными ViT на нескольких наборах данных. В работе проводится глубокий анализ свойств новой архитектуры, включая ландшафт функции потерь, распределение весов и визуализацию внимания. Исследование призывает сообщество изучать сети Колмогорова-Арнольда в сочетании с передовыми архитектурами глубокого обучения.'}, 'en': {'title': 'Unlocking Complex Relationships with Learnable Activations in Vision Transformers', 'desc': 'Kolmogorov-Arnold networks (KANs) introduce learnable activation functions that can model complex data relationships. This paper explores the application of KANs in vision tasks by integrating them into vision Transformers (ViTs) through a novel learnable attention mechanism called Kolmogorov-Arnold Attention (KArAt). The authors also present a more efficient variant, Fourier-KArAt, which shows competitive performance on popular image datasets like CIFAR-10 and ImageNet-1K. The study emphasizes the importance of understanding learnable activations in advanced architectures, rather than solely focusing on efficiency.'}, 'zh': {'title': '探索可学习激活函数的潜力', 'desc': 'Kolmogorov-Arnold网络（KANs）是一种创新的可学习激活函数，能够捕捉数据中的复杂关系。尽管KANs在一维函数的符号表示和持续学习中表现出色，但在视觉等多种机器学习任务中的有效性仍然存在疑问。本文首次为普通的视觉变换器（ViTs）设计了一种通用的可学习Kolmogorov-Arnold注意力（KArAt），并提出了更模块化的Fourier-KArAt版本。实验结果表明，Fourier-KArAt及其变体在CIFAR-10、CIFAR-100和ImageNet-1K数据集上表现优于或与ViT相当。'}}}, {'id': 'https://huggingface.co/papers/2503.06542', 'title': 'ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model\n  with Interleaved Multimodal Generation via Asymmetric Synergy', 'url': 'https://huggingface.co/papers/2503.06542', 'abstract': 'Unified models (UniMs) for multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large language models (MLLMs). Specifically, ARMOR extends existing MLLMs from three perspectives: (1) For model architecture, an asymmetric encoder-decoder architecture with a forward-switching mechanism is introduced to unify embedding space integrating textual and visual modalities for enabling natural text-image interleaved generation with minimal computational overhead. (2) For training data, a meticulously curated, high-quality interleaved dataset is collected for fine-tuning MLLMs. (3) For the training algorithm, we propose a ``what or how to generate" algorithm to empower existing MLLMs with multimodal generation capabilities while preserving their multimodal understanding capabilities, through three progressive training stages based on the collected dataset. Experimental results demonstrate that ARMOR upgrades existing MLLMs to UniMs with promising image generation capabilities, using limited training resources. Our code will be released soon at https://armor.github.io.', 'score': 6, 'issue_id': 2737, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '4b19cfc1e459fb2f', 'authors': ['Jianwen Sun', 'Yukang Feng', 'Chuanhao Li', 'Fanrui Zhang', 'Zizhen Li', 'Jiaxin Ai', 'Sizhuo Zhou', 'Yu Dai', 'Shenglin Zhang', 'Kaipeng Zhang'], 'affiliations': ['Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.06542.jpg', 'data': {'categories': ['#games', '#optimization', '#multimodal', '#dataset', '#training', '#architecture'], 'emoji': '🛡️', 'ru': {'title': 'ARMOR: Эффективное улучшение мультимодальных моделей', 'desc': 'ARMOR - это новый подход к созданию унифицированных моделей для мультимодального понимания и генерации. Он использует асимметричную архитектуру энкодер-декодер с механизмом переключения вперед для объединения текстовых и визуальных модальностей. ARMOR обучается на специально собранном наборе данных с чередующимися текстом и изображениями. Применяется алгоритм обучения в три этапа, позволяющий улучшить возможности генерации мультимодальных больших языковых моделей при сохранении их способностей к пониманию.'}, 'en': {'title': 'ARMOR: Efficient Multimodal Mastery with Minimal Resources', 'desc': 'The paper introduces ARMOR, a new framework designed to enhance multimodal understanding and generation in machine learning models. Unlike existing unified models that require extensive computational resources, ARMOR is resource-efficient and fine-tunes large language models to achieve its goals. It employs an asymmetric encoder-decoder architecture to seamlessly integrate text and image data, allowing for natural interleaved generation. Additionally, ARMOR utilizes a carefully curated dataset and a novel training algorithm to improve the multimodal capabilities of existing models while maintaining their understanding abilities.'}, 'zh': {'title': 'ARMOR：高效的多模态理解与生成框架', 'desc': '本文介绍了一种名为ARMOR的统一模型框架，旨在提高多模态理解和生成的效率。ARMOR通过微调现有的大型多模态语言模型（MLLMs），实现了文本和图像的自然交织生成。该框架采用了不对称的编码-解码架构，并引入了前向切换机制，以减少计算资源的消耗。实验结果表明，ARMOR能够在有限的训练资源下，显著提升现有模型的图像生成能力。'}}}, {'id': 'https://huggingface.co/papers/2503.09279', 'title': 'Cockatiel: Ensembling Synthetic and Human Preferenced Training for\n  Detailed Video Caption', 'url': 'https://huggingface.co/papers/2503.09279', 'abstract': 'Video Detailed Captioning (VDC) is a crucial task for vision-language bridging, enabling fine-grained descriptions of complex video content. In this paper, we first comprehensively benchmark current state-of-the-art approaches and systematically identified two critical limitations: biased capability towards specific captioning aspect and misalignment with human preferences. To address these deficiencies, we propose Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training for improving VDC performance. In the first stage, we derive a scorer from a meticulously annotated dataset to select synthetic captions high-performing on certain fine-grained video-caption alignment and human-preferred while disregarding others. Then, we train Cockatiel-13B, using this curated dataset to infuse it with assembled model strengths and human preferences. Finally, we further distill Cockatiel-8B from Cockatiel-13B for the ease of usage. Extensive quantitative and qualitative experiments reflect the effectiveness of our method, as we not only set new state-of-the-art performance on VDCSCORE in a dimension-balanced way but also surpass leading alternatives on human preference by a large margin as depicted by the human evaluation results.', 'score': 5, 'issue_id': 2730, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'edf6712b564fd37a', 'authors': ['Luozheng Qin', 'Zhiyu Tan', 'Mengping Yang', 'Xiaomeng Yang', 'Hao Li'], 'affiliations': ['Fudan University', 'Shanghai Academy of Artificial Intelligence for Science'], 'pdf_title_img': 'assets/pdf/title_img/2503.09279.jpg', 'data': {'categories': ['#benchmark', '#training', '#multimodal', '#synthetic', '#alignment'], 'emoji': '🦜', 'ru': {'title': 'Cockatiel: Новый стандарт в детальном описании видео', 'desc': 'Статья представляет новый подход к детальному описанию видео (VDC) под названием Cockatiel. Авторы разработали трехэтапный процесс обучения, который объединяет синтетические и человеко-ориентированные данные для улучшения производительности. Метод включает отбор высококачественных синтетических подписей, обучение большой модели Cockatiel-13B и ее дистилляцию в меньшую Cockatiel-8B. Эксперименты показали, что Cockatiel превосходит существующие методы по метрике VDCSCORE и человеческим предпочтениям.'}, 'en': {'title': 'Bridging Vision and Language with Cockatiel for Enhanced Video Captioning', 'desc': 'This paper addresses the challenge of Video Detailed Captioning (VDC), which involves creating precise descriptions for complex video content. The authors identify two main issues with existing methods: a bias towards certain aspects of captioning and a lack of alignment with human preferences. To overcome these challenges, they introduce Cockatiel, a three-stage training pipeline that combines synthetic and human-aligned data to enhance VDC performance. Their experiments demonstrate that Cockatiel achieves state-of-the-art results on the VDCSCORE metric and significantly outperforms other methods in terms of human preference evaluations.'}, 'zh': {'title': '提升视频描述的智能化与人性化', 'desc': '视频详细描述（VDC）是连接视觉和语言的重要任务，能够对复杂视频内容进行细致的描述。本文首先对当前最先进的方法进行了全面评估，并系统地识别出两个关键限制：对特定描述方面的偏见能力和与人类偏好的不一致。为了解决这些问题，我们提出了Cockatiel，这是一种新颖的三阶段训练流程，结合了合成和人类对齐的训练，以提高VDC性能。通过大量的定量和定性实验，我们的方法在VDCSCORE上设定了新的最先进性能，并在与人类偏好的比较中大幅超越了领先的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2503.10696', 'title': 'Neighboring Autoregressive Modeling for Efficient Visual Generation', 'url': 'https://huggingface.co/papers/2503.10696', 'abstract': 'Visual autoregressive models typically adhere to a raster-order ``next-token prediction" paradigm, which overlooks the spatial and temporal locality inherent in visual content. Specifically, visual tokens exhibit significantly stronger correlations with their spatially or temporally adjacent tokens compared to those that are distant. In this paper, we propose Neighboring Autoregressive Modeling (NAR), a novel paradigm that formulates autoregressive visual generation as a progressive outpainting procedure, following a near-to-far ``next-neighbor prediction" mechanism. Starting from an initial token, the remaining tokens are decoded in ascending order of their Manhattan distance from the initial token in the spatial-temporal space, progressively expanding the boundary of the decoded region. To enable parallel prediction of multiple adjacent tokens in the spatial-temporal space, we introduce a set of dimension-oriented decoding heads, each predicting the next token along a mutually orthogonal dimension. During inference, all tokens adjacent to the decoded tokens are processed in parallel, substantially reducing the model forward steps for generation. Experiments on ImageNet256times 256 and UCF101 demonstrate that NAR achieves 2.4times and 8.6times higher throughput respectively, while obtaining superior FID/FVD scores for both image and video generation tasks compared to the PAR-4X approach. When evaluating on text-to-image generation benchmark GenEval, NAR with 0.8B parameters outperforms Chameleon-7B while using merely 0.4 of the training data. Code is available at https://github.com/ThisisBillhe/NAR.', 'score': 5, 'issue_id': 2738, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '5adda6787d6613db', 'authors': ['Yefei He', 'Yuanyu He', 'Shaoxuan He', 'Feng Chen', 'Hong Zhou', 'Kaipeng Zhang', 'Bohan Zhuang'], 'affiliations': ['Shanghai AI Laboratory, China', 'The University of Adelaide, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.10696.jpg', 'data': {'categories': ['#cv', '#video', '#games', '#benchmark', '#optimization', '#architecture', '#training'], 'emoji': '🧩', 'ru': {'title': 'NAR: Революция в авторегрессионном моделировании визуального контента', 'desc': "Статья представляет новый подход к авторегрессионному моделированию визуального контента, названный Neighboring Autoregressive Modeling (NAR). В отличие от традиционных моделей, использующих растровый порядок предсказания, NAR применяет механизм 'предсказания следующего соседа', учитывающий пространственно-временную близость токенов. Модель начинает с одного токена и последовательно декодирует остальные, расширяя границы декодированной области. Для параллельного предсказания соседних токенов используются специальные декодирующие головки, что значительно ускоряет процесс генерации."}, 'en': {'title': 'Revolutionizing Visual Generation with Neighboring Autoregressive Modeling', 'desc': "This paper introduces Neighboring Autoregressive Modeling (NAR), a new approach to visual generation that improves upon traditional autoregressive models by focusing on the spatial and temporal relationships between visual tokens. Instead of predicting the next token in a raster order, NAR uses a 'next-neighbor prediction' method, decoding tokens based on their proximity to an initial token. This allows for parallel processing of adjacent tokens, significantly speeding up the generation process. Experiments show that NAR achieves much higher throughput and better quality scores in image and video generation compared to existing methods, while also being more efficient in terms of training data usage."}, 'zh': {'title': '邻近自回归建模：提升视觉生成效率的创新方法', 'desc': '本文提出了一种新的视觉自回归模型，称为邻近自回归建模（NAR），旨在改善传统的基于光栅顺序的“下一个标记预测”方法。NAR通过近邻预测机制，将自回归视觉生成视为一种逐步扩展的过程，从初始标记开始，按曼哈顿距离逐步解码剩余标记。该方法引入了一组面向维度的解码头，允许在空间-时间空间中并行预测多个相邻标记，从而显著减少生成所需的模型前向步骤。实验结果表明，NAR在图像和视频生成任务中均优于现有方法，显示出更高的吞吐量和更好的生成质量。'}}}, {'id': 'https://huggingface.co/papers/2503.06553', 'title': 'ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges', 'url': 'https://huggingface.co/papers/2503.06553', 'abstract': "As multi-modal large language models (MLLMs) frequently exhibit errors when solving scientific problems, evaluating the validity of their reasoning processes is critical for ensuring reliability and uncovering fine-grained model weaknesses. Since human evaluation is laborious and costly, prompting MLLMs as automated process judges has become a common practice. However, the reliability of these model-based judges remains uncertain. To address this, we introduce ProJudgeBench, the first comprehensive benchmark specifically designed for evaluating abilities of MLLM-based process judges. ProJudgeBench comprises 2,400 test cases and 50,118 step-level labels, spanning four scientific disciplines with diverse difficulty levels and multi-modal content. In ProJudgeBench, each step is meticulously annotated by human experts for correctness, error type, and explanation, enabling a systematic evaluation of judges' capabilities to detect, classify and diagnose errors. Evaluation on ProJudgeBench reveals a significant performance gap between open-source and proprietary models. To bridge this gap, we further propose ProJudge-173k, a large-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning strategy that encourages models to explicitly reason through problem-solving before assessing solutions. Both contributions significantly enhance the process evaluation capabilities of open-source models. All the resources will be released to foster future research of reliable multi-modal process evaluation.", 'score': 5, 'issue_id': 2737, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '9546d0d9f897e116', 'authors': ['Jiaxin Ai', 'Pengfei Zhou', 'Zhaopan Xu', 'Ming Li', 'Fanrui Zhang', 'Zizhen Li', 'Jianwen Sun', 'Yukang Feng', 'Baojin Huang', 'Zhongyuan Wang', 'Kaipeng Zhang'], 'affiliations': ['HZAU', 'NKU', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'USTC', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2503.06553.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#multimodal', '#science', '#open_source', '#dataset', '#training', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'Надежная оценка научных рассуждений мультимодальными ИИ-судьями', 'desc': 'В статье представлен ProJudgeBench - первый комплексный бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) в роли автоматизированных судей процессов решения научных задач. Бенчмарк включает 2400 тестовых случаев и более 50 000 пошаговых оценок в четырех научных дисциплинах. Авторы также предлагают набор данных ProJudge-173k и стратегию дообучения для улучшения возможностей оценки процессов у открытых моделей. Результаты показывают значительный разрыв в производительности между открытыми и проприетарными моделями в этой задаче.'}, 'en': {'title': 'Enhancing MLLM Reliability with ProJudgeBench', 'desc': 'This paper addresses the challenges faced by multi-modal large language models (MLLMs) in solving scientific problems accurately. It introduces ProJudgeBench, a benchmark designed to evaluate the reasoning abilities of MLLM-based judges through a comprehensive set of test cases and detailed annotations. The study highlights a performance gap between open-source and proprietary models, indicating the need for improvement in the evaluation process. To enhance this, the authors propose ProJudge-173k, a dataset for instruction-tuning and a new fine-tuning strategy that promotes better reasoning in problem-solving.'}, 'zh': {'title': '提升多模态模型的过程评估能力', 'desc': '本文介绍了ProJudgeBench，这是一个专门用于评估多模态大型语言模型（MLLM）过程判断能力的基准。该基准包含2400个测试案例和50118个步骤级标签，涵盖四个科学领域，具有不同的难度和多模态内容。每个步骤都由人类专家仔细注释，以便系统评估模型在检测、分类和诊断错误方面的能力。通过在ProJudgeBench上的评估，发现开源模型与专有模型之间存在显著的性能差距，并提出了ProJudge-173k数据集和动态双阶段微调策略，以提高开源模型的过程评估能力。'}}}, {'id': 'https://huggingface.co/papers/2503.06674', 'title': 'Learning Few-Step Diffusion Models by Trajectory Distribution Matching', 'url': 'https://huggingface.co/papers/2503.06674', 'abstract': "Accelerating diffusion model sampling is crucial for efficient AIGC deployment. While diffusion distillation methods -- based on distribution matching and trajectory matching -- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. Few-step generation offers a better balance between speed and quality, but existing approaches face a persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality. To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), a unified distillation paradigm that combines the strengths of distribution and trajectory matching. Our method introduces a data-free score distillation objective, aligning the student's trajectory with the teacher's at the distribution level. Further, we develop a sampling-steps-aware objective that decouples learning targets across different steps, enabling more adjustable sampling. This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-alpha, delivering superior quality and significantly reduced training costs. In particular, our method distills PixArt-alpha into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere 0.01% of the teacher's training cost. In addition, our proposed TDM can be extended to accelerate text-to-video diffusion. Notably, TDM can outperform its teacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total score from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/", 'score': 4, 'issue_id': 2732, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': '1ce5d8eb2086abfc', 'authors': ['Yihong Luo', 'Tianyang Hu', 'Jiacheng Sun', 'Yujun Cai', 'Jing Tang'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.06674.jpg', 'data': {'categories': ['#diffusion', '#training', '#cv', '#video'], 'emoji': '🚀', 'ru': {'title': 'TDM: Революция в ускорении диффузионных моделей', 'desc': 'Статья представляет новый метод ускорения генерации изображений с помощью диффузионных моделей, называемый Trajectory Distribution Matching (TDM). TDM объединяет преимущества методов сопоставления распределений и траекторий, позволяя создавать высококачественные изображения за меньшее количество шагов. Метод вводит безданную цель дистилляции оценки и цель, учитывающую количество шагов сэмплирования, что обеспечивает гибкость и эффективность. TDM превосходит существующие методы на различных архитектурах, таких как SDXL и PixArt-alpha, значительно сокращая время обучения и улучшая качество генерации.'}, 'en': {'title': 'Efficient Few-Step Diffusion with TDM', 'desc': 'This paper presents a new method called Trajectory Distribution Matching (TDM) to improve the efficiency of diffusion models in generating images and videos. TDM combines the benefits of distribution matching and trajectory matching, allowing for fewer sampling steps while maintaining high image quality. The method introduces a data-free score distillation objective that aligns the learning process of a smaller model with a larger, more complex model. By decoupling learning targets for different sampling steps, TDM achieves state-of-the-art performance with significantly reduced training costs, making it suitable for applications like text-to-image and text-to-video generation.'}, 'zh': {'title': '提升扩散模型采样效率的创新方法', 'desc': '加速扩散模型采样对于高效的AIGC部署至关重要。本文提出了一种新的学习少步扩散模型的方法，称为轨迹分布匹配（TDM），它结合了分布匹配和轨迹匹配的优点。通过引入无数据的分数蒸馏目标，我们的模型能够在不同采样步骤之间解耦学习目标，从而实现更灵活的采样。实验结果表明，TDM在多个基准上超越了现有方法，显著提高了图像质量并降低了训练成本。'}}}, {'id': 'https://huggingface.co/papers/2503.10624', 'title': 'ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant\n  Tightness', 'url': 'https://huggingface.co/papers/2503.10624', 'abstract': 'Fitting a body to a 3D clothed human point cloud is a common yet challenging task. Traditional optimization-based approaches use multi-stage pipelines that are sensitive to pose initialization, while recent learning-based methods often struggle with generalization across diverse poses and garment types. We propose Equivariant Tightness Fitting for Clothed Humans, or ETCH, a novel pipeline that estimates cloth-to-body surface mapping through locally approximate SE(3) equivariance, encoding tightness as displacement vectors from the cloth surface to the underlying body. Following this mapping, pose-invariant body features regress sparse body markers, simplifying clothed human fitting into an inner-body marker fitting task. Extensive experiments on CAPE and 4D-Dress show that ETCH significantly outperforms state-of-the-art methods -- both tightness-agnostic and tightness-aware -- in body fitting accuracy on loose clothing (16.7% ~ 69.5%) and shape accuracy (average 49.9%). Our equivariant tightness design can even reduce directional errors by (67.2% ~ 89.8%) in one-shot (or out-of-distribution) settings. Qualitative results demonstrate strong generalization of ETCH, regardless of challenging poses, unseen shapes, loose clothing, and non-rigid dynamics. We will release the code and models soon for research purposes at https://boqian-li.github.io/ETCH/.', 'score': 3, 'issue_id': 2738, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': 'c0aca4cafef8e621', 'authors': ['Boqian Li', 'Haiwen Feng', 'Zeyu Cai', 'Michael J. Black', 'Yuliang Xiu'], 'affiliations': ['Berkeley AI Research (BAIR)', 'Max Planck Institute for Intelligent Systems', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2503.10624.jpg', 'data': {'categories': ['#3d', '#optimization', '#cv', '#open_source'], 'emoji': '👕', 'ru': {'title': 'Точная подгонка 3D-модели тела к одетому человеку с учетом плотности прилегания одежды', 'desc': 'Статья представляет новый метод ETCH для подгонки трехмерной модели тела к облаку точек одетого человека. ETCH использует локально приближенную SE(3)-эквивариантность для оценки соответствия между поверхностью одежды и телом, кодируя плотность прилегания как векторы смещения. Метод регрессирует разреженные маркеры тела с помощью инвариантных к позе признаков, упрощая задачу подгонки тела. Эксперименты показывают, что ETCH значительно превосходит современные методы по точности подгонки тела и формы, особенно для свободной одежды.'}, 'en': {'title': 'Revolutionizing Body Fitting with Equivariant Tightness!', 'desc': 'The paper introduces Equivariant Tightness Fitting for Clothed Humans (ETCH), a new method for fitting a body model to a 3D point cloud of a clothed human. ETCH improves upon traditional optimization methods by using a pipeline that leverages SE(3) equivariance to accurately map cloth surfaces to body shapes. This approach simplifies the fitting process by focusing on pose-invariant body features and regressing sparse body markers. Experimental results show that ETCH significantly enhances fitting accuracy for various clothing types and poses, outperforming existing methods in both tightness and shape accuracy.'}, 'zh': {'title': '等变紧致拟合：提升3D穿衣人类拟合精度的创新方法', 'desc': '本论文提出了一种新的方法，称为等变紧致拟合（ETCH），用于将身体与3D穿衣人类点云相匹配。传统的方法依赖于多阶段优化，容易受到姿势初始化的影响，而学习型方法在不同姿势和服装类型的泛化能力上存在困难。ETCH通过局部近似的SE(3)等变性来估计布料与身体表面的映射，并将紧致度编码为从布料表面到身体的位移向量。实验结果表明，ETCH在松散衣物的身体拟合精度和形状精度上显著优于现有的最先进方法，展示了其强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.11207', 'title': 'Can Large Reasoning Models do Analogical Reasoning under Perceptual\n  Uncertainty?', 'url': 'https://huggingface.co/papers/2503.11207', 'abstract': "This work presents a first evaluation of two state-of-the-art Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning, focusing on well-established nonverbal human IQ tests based on Raven's progressive matrices. We benchmark with the I-RAVEN dataset and its more difficult extension, I-RAVEN-X, which tests the ability to generalize to longer reasoning rules and ranges of the attribute values. To assess the influence of visual uncertainties on these nonverbal analogical reasoning tests, we extend the I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a two-fold strategy to simulate this imperfect visual perception: 1) we introduce confounding attributes which, being sampled at random, do not contribute to the prediction of the correct answer of the puzzles and 2) smoothen the distributions of the input attributes' values. We observe a sharp decline in OpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to just 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X, which increases input length and range and emulates perceptual uncertainty. This drop occurred despite spending 3.4x more reasoning tokens. A similar trend is also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a neuro-symbolic probabilistic abductive model, ARLC, that achieves state-of-the-art performances on I-RAVEN, can robustly reason under all these out-of-distribution tests, maintaining strong accuracy with only a modest reduction from 98.6% to 88.0%. Our code is available at https://github.com/IBM/raven-large-language-models.", 'score': 2, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '1910487e8b409ffb', 'authors': ['Giacomo Camposampiero', 'Michael Hersche', 'Roger Wattenhofer', 'Abu Sebastian', 'Abbas Rahimi'], 'affiliations': ['ETH Zürich', 'IBM Research - Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.11207.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#cv', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Крупные модели рассуждений уступают нейро-символическим подходам в аналогическом мышлении', 'desc': 'Эта работа представляет первую оценку двух современных моделей крупномасштабного рассуждения (LRM) - o3-mini от OpenAI и DeepSeek R1 - в задачах аналогического мышления на основе невербальных тестов IQ, использующих прогрессивные матрицы Равена. Исследователи провели бенчмаркинг на наборах данных I-RAVEN и его более сложном расширении I-RAVEN-X, которые проверяют способность обобщать более длинные правила рассуждения и диапазоны значений атрибутов. Для оценки влияния визуальной неопределенности авторы расширили набор данных I-RAVEN-X, введя случайные атрибуты и сглаживание распределений значений входных атрибутов. Результаты показали значительное снижение точности моделей LRM на более сложных тестах, в то время как нейро-символическая вероятностная модель ARLC продемонстрировала устойчивость к этим изменениям.'}, 'en': {'title': 'Evaluating Reasoning Under Uncertainty in Large Models', 'desc': "This paper evaluates two advanced Large Reasoning Models (LRMs), OpenAI's o3-mini and DeepSeek R1, on their ability to perform analogical reasoning using Raven's progressive matrices. The study uses the I-RAVEN dataset and its more challenging version, I-RAVEN-X, to test the models' generalization capabilities under visual uncertainties. The results show a significant drop in accuracy for both LRMs when faced with the more complex tasks, indicating their struggle with longer reasoning rules and perceptual noise. In contrast, a neuro-symbolic model, ARLC, demonstrates robust performance, maintaining high accuracy even under challenging conditions."}, 'zh': {'title': '大型推理模型在类比推理中的挑战与机遇', 'desc': '本文首次评估了两种先进的大型推理模型（LRMs），OpenAI的o3-mini和DeepSeek R1，在类比推理方面的表现，重点关注基于Raven渐进矩阵的非语言人类智商测试。我们使用I-RAVEN数据集及其更难的扩展版本I-RAVEN-X进行基准测试，后者测试模型对更长推理规则和属性值范围的泛化能力。为了评估视觉不确定性对这些非语言类比推理测试的影响，我们扩展了I-RAVEN-X数据集，并采用了两种策略来模拟不完美的视觉感知。结果显示，OpenAI的o3-mini在I-RAVEN-X上的任务准确率大幅下降，从86.6%降至仅17.0%，而ARLC模型在所有这些分布外测试中保持了强大的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.10620', 'title': 'From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM', 'url': 'https://huggingface.co/papers/2503.10620', 'abstract': "Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community.", 'score': 2, 'issue_id': 2739, 'pub_date': '2025-03-13', 'pub_date_card': {'ru': '13 марта', 'en': 'March 13', 'zh': '3月13日'}, 'hash': '6fc0a5f12205a8bd', 'authors': ['Kshitij Ambilduke', 'Ben Peters', 'Sonal Sannigrahi', 'Anil Keshwani', 'Tsz Kin Lam', 'Bruno Martins', 'Marcely Zanon Boito', 'André F. T. Martins'], 'affiliations': ['ELLIS Unit Lisbon', 'INESC-ID', 'Instituto Superior Técnico, Universidade de Lisboa', 'Instituto de Telecomunicações', 'NAVER LABS Europe', 'Paris-Saclay University', 'Sapienza University of Rome', 'Unbabel', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2503.10620.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#open_source', '#multimodal', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Расширение языковых моделей на речевую модальность', 'desc': 'В этой статье описывается расширение возможностей большой языковой модели (LLM) для работы с речью путем дискретизации речевого сигнала и дополнительного предобучения. Авторы интегрируют дискретизированный речевой ввод как дополнительный язык в многоязычную модель TOWER. Полученная модель SPIRE способна транскрибировать и переводить английскую речь, сохраняя при этом исходную производительность TOWER в задачах перевода. Исследование демонстрирует возможность интеграции дискретизированного речевого ввода как дополнительного языка при адаптации LLM.'}, 'en': {'title': 'Integrating Speech into Multilingual LLMs for Enhanced Performance', 'desc': 'This paper discusses the enhancement of large language models (LLMs) by integrating speech as a new modality. The authors focus on multilingual LLMs, specifically TOWER, and propose a method to convert speech into a format that the model can understand. They introduce a new model called SPIRE, which can transcribe and translate English speech while preserving the original capabilities of TOWER. The research demonstrates that incorporating discretized speech as an additional language is a viable approach for adapting LLMs, and the authors provide their code and models for public use.'}, 'zh': {'title': '将语音融入大型语言模型的创新之路', 'desc': '大型语言模型（LLMs）在多种语言和任务中表现出色，具有很强的泛化能力，适合与多模态（如图像或语音）结合。本文将现有的LLM扩展到语音模态，通过语音离散化和持续预训练来实现。我们特别关注多语言LLM，例如TOWER，因为它的预训练设置允许我们将离散化的语音输入视为额外的翻译语言。最终生成的开源模型SPIRE能够转录和翻译英语语音输入，同时保持TOWER在翻译相关任务上的原始性能，证明了在LLM适应过程中将离散语音输入作为额外语言的可行性。'}}}, {'id': 'https://huggingface.co/papers/2503.10684', 'title': 'Open-World Skill Discovery from Unsegmented Demonstrations', 'url': 'https://huggingface.co/papers/2503.10684', 'abstract': 'Learning skills in open-world environments is essential for developing agents capable of handling a variety of tasks by combining basic skills. Online demonstration videos are typically long but unsegmented, making them difficult to segment and label with skill identifiers. Unlike existing methods that rely on sequence sampling or human labeling, we have developed a self-supervised learning-based approach to segment these long videos into a series of semantic-aware and skill-consistent segments. Drawing inspiration from human cognitive event segmentation theory, we introduce Skill Boundary Detection (SBD), an annotation-free temporal video segmentation algorithm. SBD detects skill boundaries in a video by leveraging prediction errors from a pretrained unconditional action-prediction model. This approach is based on the assumption that a significant increase in prediction error indicates a shift in the skill being executed. We evaluated our method in Minecraft, a rich open-world simulator with extensive gameplay videos available online. Our SBD-generated segments improved the average performance of conditioned policies by 63.7% and 52.1% on short-term atomic skill tasks, and their corresponding hierarchical agents by 11.3% and 20.8% on long-horizon tasks. Our method can leverage the diverse YouTube videos to train instruction-following agents. The project page can be found in https://craftjarvis.github.io/SkillDiscovery.', 'score': 2, 'issue_id': 2740, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '004932028027606e', 'authors': ['Jingwen Deng', 'Zihao Wang', 'Shaofei Cai', 'Anji Liu', 'Yitao Liang'], 'affiliations': ['Peking University', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.10684.jpg', 'data': {'categories': ['#games', '#video', '#agents', '#open_source'], 'emoji': '🎮', 'ru': {'title': 'Автоматическая сегментация видео для обучения ИИ-агентов сложным навыкам', 'desc': 'Статья представляет новый метод самообучения для сегментации длинных видео на последовательности семантически связанных навыков. Авторы разработали алгоритм Skill Boundary Detection (SBD), который обнаруживает границы навыков в видео, используя ошибки предсказания предобученной модели. Метод был протестирован на видео игрового процесса Minecraft и показал значительное улучшение производительности обусловленных политик и иерархических агентов. Данный подход позволяет использовать разнообразные видео с YouTube для обучения агентов, выполняющих инструкции.'}, 'en': {'title': 'Segmenting Skills for Smarter Agents', 'desc': 'This paper presents a novel approach for segmenting long, unstructured demonstration videos into meaningful skill segments using self-supervised learning. The method, called Skill Boundary Detection (SBD), identifies transitions between skills by analyzing prediction errors from a pretrained action-prediction model. By applying this technique, the authors demonstrate significant improvements in the performance of agents trained on these segmented skills in the Minecraft environment. This approach allows for the effective use of diverse online video content to enhance the training of instruction-following agents without the need for manual labeling.'}, 'zh': {'title': '自监督学习助力技能边界检测', 'desc': '在开放世界环境中学习技能对于开发能够处理多种任务的智能体至关重要。我们提出了一种基于自监督学习的方法，可以将长视频分割成一系列语义明确且技能一致的片段，而无需人工标注。该方法通过检测预测误差来识别技能边界，假设预测误差的显著增加表明技能的转变。我们的实验表明，这种方法在Minecraft中显著提高了条件策略和层次代理的表现。'}}}, {'id': 'https://huggingface.co/papers/2503.05689', 'title': 'GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories\n  Generation in End-to-End Autonomous Driving', 'url': 'https://huggingface.co/papers/2503.05689', 'abstract': 'We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suffer from trajectory selection complexity and reduced trajectory quality due to high trajectory divergence and inconsistencies between guidance and scene information. To address these issues, we introduce GoalFlow, a novel method that effectively constrains the generative process to produce high-quality, multimodal trajectories. To resolve the trajectory divergence problem inherent in diffusion-based methods, GoalFlow constrains the generated trajectories by introducing a goal point. GoalFlow establishes a novel scoring mechanism that selects the most appropriate goal point from the candidate points based on scene information. Furthermore, GoalFlow employs an efficient generative method, Flow Matching, to generate multimodal trajectories, and incorporates a refined scoring mechanism to select the optimal trajectory from the candidates. Our experimental results, validated on the NavsimDauner2024_navsim, demonstrate that GoalFlow achieves state-of-the-art performance, delivering robust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS of 90.3, significantly surpassing other methods. Compared with other diffusion-policy-based methods, our approach requires only a single denoising step to obtain excellent performance. The code is available at https://github.com/YvanYin/GoalFlow.', 'score': 2, 'issue_id': 2731, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'eef61e2f2b4c0760', 'authors': ['Zebin Xing', 'Xingyu Zhang', 'Yang Hu', 'Bo Jiang', 'Tong He', 'Qian Zhang', 'Xiaoxiao Long', 'Wei Yin'], 'affiliations': ['Horizon Robotics', 'Huazhong University of Science & Technology', 'Nanjing University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.05689.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#agents', '#multimodal'], 'emoji': '🚗', 'ru': {'title': 'GoalFlow: Точное планирование траекторий для автономного вождения', 'desc': 'GoalFlow - это новый метод автономного вождения для генерации высококачественных мультимодальных траекторий. Он решает проблему расхождения траекторий, вводя целевую точку и механизм оценки для выбора наиболее подходящей. GoalFlow использует эффективный генеративный метод Flow Matching для создания мультимодальных траекторий. Экспериментальные результаты показывают, что GoalFlow достигает передового уровня производительности, обеспечивая надежные мультимодальные траектории для автономного вождения.'}, 'en': {'title': 'GoalFlow: Driving the Future with High-Quality Multimodal Trajectories', 'desc': 'GoalFlow is a new method for generating high-quality multimodal trajectories in autonomous driving. It addresses the challenges of trajectory selection and quality by constraining the generative process with a goal point, which helps reduce trajectory divergence. The method uses a novel scoring mechanism to choose the best goal point based on the scene context, ensuring that the generated trajectories are relevant and effective. Experimental results show that GoalFlow outperforms existing methods, achieving state-of-the-art performance with fewer computational steps.'}, 'zh': {'title': 'GoalFlow：高质量多模态轨迹生成的创新方法', 'desc': '我们提出了GoalFlow，这是一种端到端的自动驾驶方法，用于生成高质量的多模态轨迹。在自动驾驶场景中，通常没有单一合适的轨迹，最近的方法越来越关注多模态轨迹分布的建模。为了克服轨迹选择的复杂性和轨迹质量下降的问题，GoalFlow通过引入目标点来有效约束生成过程，从而生成高质量的多模态轨迹。我们的实验结果表明，GoalFlow在NavsimDauner2024_navsim上实现了最先进的性能，提供了稳健的多模态轨迹。'}}}, {'id': 'https://huggingface.co/papers/2503.11629', 'title': 'TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree\n  Sequencing', 'url': 'https://huggingface.co/papers/2503.11629', 'abstract': 'We introduce TreeMeshGPT, an autoregressive Transformer designed to generate high-quality artistic meshes aligned with input point clouds. Instead of the conventional next-token prediction in autoregressive Transformer, we propose a novel Autoregressive Tree Sequencing where the next input token is retrieved from a dynamically growing tree structure that is built upon the triangle adjacency of faces within the mesh. Our sequencing enables the mesh to extend locally from the last generated triangular face at each step, and therefore reduces training difficulty and improves mesh quality. Our approach represents each triangular face with two tokens, achieving a compression rate of approximately 22% compared to the naive face tokenization. This efficient tokenization enables our model to generate highly detailed artistic meshes with strong point cloud conditioning, surpassing previous methods in both capacity and fidelity. Furthermore, our method generates mesh with strong normal orientation constraints, minimizing flipped normals commonly encountered in previous methods. Our experiments show that TreeMeshGPT enhances the mesh generation quality with refined details and normal orientation consistency.', 'score': 1, 'issue_id': 2744, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '5f56461a339a86bb', 'authors': ['Stefan Lionar', 'Jiabin Liang', 'Gim Hee Lee'], 'affiliations': ['Garena', 'National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.11629.jpg', 'data': {'categories': ['#games', '#3d', '#optimization', '#architecture'], 'emoji': '🌳', 'ru': {'title': 'Генерация высококачественных 3D-сеток с помощью древовидной последовательности', 'desc': 'TreeMeshGPT - это авторегрессивный трансформер для генерации высококачественных художественных сеток, согласованных с входными облаками точек. Модель использует новый метод автоматической древовидной последовательности, где следующий токен ввода извлекается из динамически растущей древовидной структуры, построенной на основе смежности треугольников в сетке. Этот подход позволяет сетке локально расширяться от последней сгенерированной треугольной грани на каждом шаге, что снижает сложность обучения и улучшает качество сетки. TreeMeshGPT превосходит предыдущие методы по емкости и точности, генерируя детализированные сетки с сильным ограничением нормалей.'}, 'en': {'title': 'Revolutionizing Mesh Generation with TreeMeshGPT', 'desc': 'TreeMeshGPT is a novel autoregressive Transformer model that generates artistic meshes from point clouds. It introduces a unique Autoregressive Tree Sequencing method, which builds a dynamic tree structure based on the adjacency of triangular faces, allowing for local mesh extension during generation. This approach not only simplifies the training process but also enhances the quality of the generated meshes by achieving a 22% compression rate through efficient tokenization of triangular faces. Additionally, TreeMeshGPT ensures better normal orientation, reducing the occurrence of flipped normals and improving overall mesh fidelity and detail compared to previous techniques.'}, 'zh': {'title': 'TreeMeshGPT：高质量艺术网格生成的新方法', 'desc': '我们介绍了TreeMeshGPT，这是一种自回归Transformer，旨在生成与输入点云对齐的高质量艺术网格。与传统的自回归Transformer的下一个标记预测不同，我们提出了一种新颖的自回归树序列化方法，通过动态增长的树结构来检索下一个输入标记。我们的序列化方法使得网格能够在每一步从最后生成的三角面局部扩展，从而降低训练难度并提高网格质量。此外，我们的模型通过将每个三角面表示为两个标记，实现了约22%的压缩率，生成的网格在细节和法线方向一致性方面优于以往方法。'}}}, {'id': 'https://huggingface.co/papers/2503.08111', 'title': 'MaRI: Material Retrieval Integration across Domains', 'url': 'https://huggingface.co/papers/2503.08111', 'abstract': 'Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training an image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods.', 'score': 1, 'issue_id': 2738, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'f3b97bb0031c6d57', 'authors': ['Jianhui Wang', 'Zhifei Yang', 'Yangfan He', 'Huixiong Zhang', 'Yuxuan Chen', 'Jingwei Huang'], 'affiliations': ['Fudan University', 'Peking University', 'Tencent Hunyuan3D', 'University of Electronic Science and Technology of China', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2503.08111.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#dataset', '#cv', '#3d'], 'emoji': '🔍', 'ru': {'title': 'MaRI: Революция в поиске 3D-материалов', 'desc': 'Статья представляет MaRI - фреймворк для улучшения поиска реалистичных 3D-материалов. Он создает общее пространство признаков для синтетических и реальных материалов с помощью контрастивного обучения. Авторы также собрали набор данных из высококачественных синтетических материалов и обработанных реальных материалов. Эксперименты показывают, что MaRI превосходит существующие методы в задачах поиска материалов.'}, 'en': {'title': 'Bridging the Gap in Material Retrieval with MaRI', 'desc': 'This paper presents MaRI, a novel framework aimed at improving material retrieval for realistic 3D asset creation. It addresses the limitations of existing methods that rely on traditional image search techniques, which often fail to capture the unique properties of materials. By utilizing a contrastive learning strategy, MaRI creates a shared embedding space that aligns visual and material attributes, enhancing the retrieval process. The framework is supported by a comprehensive dataset of synthetic and real-world materials, demonstrating superior performance in accuracy and generalization across various retrieval tasks.'}, 'zh': {'title': 'MaRI：提升材料检索的智能框架', 'desc': '准确的材料检索对于创建真实的3D资产至关重要。现有方法依赖于捕捉形状不变和光照变化的材料表示的数据集，这些数据集稀缺且面临多样性不足和现实世界泛化不良的挑战。我们提出了MaRI框架，旨在弥合合成材料和真实材料之间的特征空间差距。通过对比学习策略，MaRI构建了一个共享的嵌入空间，使得相似的材料和图像在特征空间中更接近，同时将不相似的对分开，从而提高了材料检索的性能和准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.09330', 'title': 'Group-robust Machine Unlearning', 'url': 'https://huggingface.co/papers/2503.09330', 'abstract': 'Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning.', 'score': 0, 'issue_id': 2739, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '64a6c82be0db725d', 'authors': ['Thomas De Min', 'Subhankar Roy', 'Stéphane Lathuilière', 'Elisa Ricci', 'Massimiliano Mancini'], 'affiliations': ['Fondazione Bruno Kessler', 'Inria Grenoble, Univ. Grenoble Alpes', 'LTCI, Telecom Paris, Institut Polytechnique de Paris', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2503.09330.jpg', 'data': {'categories': ['#training', '#dataset', '#ethics', '#data'], 'emoji': '🧠', 'ru': {'title': 'Групповое разобучение без потери устойчивости', 'desc': 'Статья посвящена проблеме группового машинного разобучения, когда данные для удаления из модели неравномерно распределены между группами. Авторы предлагают метод MIU, который минимизирует взаимную информацию между признаками модели и информацией о группах. MIU использует перевзвешивание распределения выборки и калибровку взаимной информации с исходной моделью для сохранения групповой устойчивости. Эксперименты на трех наборах данных показывают, что MIU превосходит стандартные методы, достигая разобучения без ущерба для устойчивости модели.'}, 'en': {'title': 'Fair and Effective Machine Unlearning for Diverse Data Groups', 'desc': 'This paper introduces the concept of group-robust machine unlearning, which addresses the challenge of removing specific training data from a model while maintaining its performance across different groups. It highlights the issue of fairness when the data to be unlearned is not uniformly distributed, leading to performance degradation in dominant groups. The authors propose a novel method called MIU (Mutual Information-aware Machine Unlearning) that minimizes the mutual information between model features and group information, thus enhancing unlearning effectiveness. Through experiments on various datasets, MIU demonstrates superior performance compared to traditional methods, ensuring robust model performance even after unlearning.'}, 'zh': {'title': '组鲁棒性机器遗忘：公平性与性能的平衡', 'desc': '机器遗忘是一种新兴的范式，旨在从模型中去除特定训练数据的影响，同时保留其对其他数据的知识。以往的方法假设遗忘数据均匀分布，但如果要遗忘的数据在某一组中占主导地位，模型在该组的性能会下降，导致公平性问题。本文提出了一种简单有效的策略，通过样本分布重加权来缓解主导组的性能损失，解决了非均匀分布遗忘集的问题。我们还提出了MIU（互信息感知机器遗忘），这是首个针对近似机器遗忘的组鲁棒性方法，能够在减少主导组性能下降的同时实现遗忘。'}}}, {'id': 'https://huggingface.co/papers/2503.13288', 'title': 'φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation', 'url': 'https://huggingface.co/papers/2503.13288', 'abstract': 'Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named phi-Decoding. To provide a precise and expressive estimation of step value, phi-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show phi-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon.', 'score': 39, 'issue_id': 2805, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '8a067ffdfadeb974', 'authors': ['Fangzhi Xu', 'Hang Yan', 'Chang Ma', 'Haiteng Zhao', 'Jun Liu', 'Qika Lin', 'Zhiyong Wu'], 'affiliations': ['National University of Singapore', 'Peking University', 'Shanghai AI Lab', 'The University of Hong Kong', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.13288.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#inference', '#open_source', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация вывода языковых моделей с помощью предвидения и адаптивных вычислений', 'desc': 'Статья представляет новую стратегию декодирования под названием phi-Decoding для оптимизации вывода языковых моделей. Метод использует выборку с предвидением, чтобы оценить оптимальные шаги генерации текста на основе симуляции будущих шагов. phi-Decoding аппроксимирует две распределения с помощью предвидения и кластеризации для точной оценки значимости шагов. Авторы также предлагают стратегии прунинга для адаптивного распределения вычислительных ресурсов, что повышает эффективность вывода.'}, 'en': {'title': 'Optimizing Inference with phi-Decoding: Balancing Exploration and Exploitation', 'desc': 'This paper introduces phi-Decoding, a new decoding strategy that optimizes inference-time computation in machine learning models. It addresses the limitations of previous search-based methods by balancing exploration and exploitation through foresight sampling, which simulates future steps for better decision-making. The approach uses clustering to approximate distributions, allowing for the selection of optimal steps during the decoding process. Extensive experiments demonstrate that phi-Decoding significantly improves both performance and efficiency across various benchmarks and large language models (LLMs).'}, 'zh': {'title': '优化推理效率的phi-Decoding策略', 'desc': '本文提出了一种新的解码策略，称为phi-Decoding，旨在优化推理过程中的计算效率。通过前瞻采样，phi-Decoding能够利用模拟的未来步骤来获得全局最优的步骤估计，从而平衡探索与利用。该方法通过近似两个分布来提供精确的步骤价值估计，并通过联合分布进行采样以选择最佳步骤。实验结果表明，phi-Decoding在性能和效率上均优于现有的强基线，并且在不同的大型语言模型和计算预算下具有良好的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2503.15265', 'title': 'DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.15265', 'abstract': 'Triangle meshes play a crucial role in 3D applications for efficient manipulation and rendering. While auto-regressive methods generate structured meshes by predicting discrete vertex tokens, they are often constrained by limited face counts and mesh incompleteness. To address these challenges, we propose DeepMesh, a framework that optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy incorporating a novel tokenization algorithm, along with improvements in data curation and processing, and (2) the introduction of Reinforcement Learning (RL) into 3D mesh generation to achieve human preference alignment via Direct Preference Optimization (DPO). We design a scoring standard that combines human evaluation with 3D metrics to collect preference pairs for DPO, ensuring both visual appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh generates meshes with intricate details and precise topology, outperforming state-of-the-art methods in both precision and quality. Project page: https://zhaorw02.github.io/DeepMesh/', 'score': 34, 'issue_id': 2802, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '37236f5315cc8aef', 'authors': ['Ruowen Zhao', 'Junliang Ye', 'Zhengyi Wang', 'Guangce Liu', 'Yiwen Chen', 'Yikai Wang', 'Jun Zhu'], 'affiliations': ['Nanyang Technological University', 'ShengShu', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15265.jpg', 'data': {'categories': ['#optimization', '#data', '#alignment', '#rlhf', '#rl', '#3d'], 'emoji': '🔷', 'ru': {'title': 'DeepMesh: Революция в генерации 3D-сеток с учетом человеческих предпочтений', 'desc': 'DeepMesh - это новый фреймворк для оптимизации генерации трехмерных сеток. Он использует эффективную стратегию предварительного обучения с новым алгоритмом токенизации и улучшенной обработкой данных. DeepMesh также внедряет обучение с подкреплением для создания 3D-сеток, соответствующих предпочтениям человека, с помощью Direct Preference Optimization. Система генерирует сетки с детальной структурой и точной топологией на основе облаков точек и изображений, превосходя современные методы по точности и качеству.'}, 'en': {'title': 'DeepMesh: Elevating 3D Mesh Generation with Human-Centric Learning', 'desc': 'This paper introduces DeepMesh, a new framework for generating 3D triangle meshes that enhances both quality and precision. It utilizes a unique pre-training strategy with an innovative tokenization method, improving how data is curated and processed. Additionally, it incorporates Reinforcement Learning to align mesh generation with human preferences through Direct Preference Optimization. By conditioning on point clouds and images, DeepMesh produces detailed and accurately structured meshes, surpassing existing methods in performance.'}, 'zh': {'title': 'DeepMesh：优化3D网格生成的新方法', 'desc': '三角网格在3D应用中至关重要，能够高效地进行操作和渲染。传统的自回归方法通过预测离散的顶点标记生成结构化网格，但常常受到面数限制和网格不完整性的困扰。为了解决这些问题，我们提出了DeepMesh框架，通过两项关键创新来优化网格生成：一种高效的预训练策略和将强化学习引入3D网格生成。DeepMesh能够生成细节丰富、拓扑精确的网格，且在精度和质量上超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.15485', 'title': 'TULIP: Towards Unified Language-Image Pretraining', 'url': 'https://huggingface.co/papers/2503.15485', 'abstract': 'Despite the recent success of image-text contrastive models like CLIP and SigLIP, these models often struggle with vision-centric tasks that demand high-fidelity image understanding, such as counting, depth estimation, and fine-grained object recognition. These models, by performing language alignment, tend to prioritize high-level semantics over visual understanding, weakening their image understanding. On the other hand, vision-focused models are great at processing visual information but struggle to understand language, limiting their flexibility for language-driven tasks. In this work, we introduce TULIP, an open-source, drop-in replacement for existing CLIP-like models. Our method leverages generative data augmentation, enhanced image-image and text-text contrastive learning, and image/text reconstruction regularization to learn fine-grained visual features while preserving global semantic alignment. Our approach, scaling to over 1B parameters, outperforms existing state-of-the-art (SOTA) models across multiple benchmarks, establishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to a 2times enhancement over SigLIP on RxRx1 in linear probing for few-shot classification, and improving vision-language models, achieving over 3times higher scores than SigLIP on MMVP. Our code/checkpoints are available at https://tulip-berkeley.github.io', 'score': 32, 'issue_id': 2800, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': 'd4b870742a020d5a', 'authors': ['Zineng Tang', 'Long Lian', 'Seun Eisape', 'XuDong Wang', 'Roei Herzig', 'Adam Yala', 'Alane Suhr', 'Trevor Darrell', 'David M. Chan'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.15485.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#architecture', '#open_source', '#cv'], 'emoji': '🌷', 'ru': {'title': 'TULIP: Баланс между детальным зрением и языковым пониманием', 'desc': 'Статья представляет TULIP - новую модель для задач компьютерного зрения и обработки естественного языка. TULIP улучшает понимание визуальных деталей изображений, сохраняя при этом способность к семантическому сопоставлению с текстом. Модель использует генеративное расширение данных, улучшенное контрастивное обучение и регуляризацию реконструкции для достижения баланса между визуальным и языковым пониманием. TULIP превосходит существующие модели на нескольких бенчмарках, устанавливая новый state-of-the-art в задачах zero-shot классификации и few-shot обучения.'}, 'en': {'title': 'TULIP: Bridging Vision and Language for Enhanced Image Understanding', 'desc': 'This paper presents TULIP, a new model designed to improve image understanding in tasks like counting and depth estimation, which are challenging for existing image-text contrastive models. TULIP enhances visual feature learning by using generative data augmentation and advanced contrastive learning techniques, while still maintaining a connection to high-level semantics. It scales effectively to over 1 billion parameters and demonstrates superior performance on various benchmarks, setting new records in zero-shot classification and few-shot tasks. The model aims to bridge the gap between vision-centric and language-centric approaches, providing a more flexible solution for vision-language tasks.'}, 'zh': {'title': 'TULIP：提升图像理解的新方法', 'desc': '尽管像CLIP和SigLIP这样的图像-文本对比模型取得了成功，但它们在需要高保真图像理解的视觉任务中表现不佳。本文提出了TULIP，这是一种开源的替代方案，旨在通过生成数据增强和对比学习来提高图像理解能力。TULIP能够学习细粒度的视觉特征，同时保持全局语义的一致性。我们的实验表明，TULIP在多个基准测试中超越了现有的最先进模型，显著提升了零-shot性能和少-shot分类的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.15475', 'title': 'Cube: A Roblox View of 3D Intelligence', 'url': 'https://huggingface.co/papers/2503.15475', 'abstract': 'Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such a foundation model for 3D intelligence, a model that can support developers in producing all aspects of a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such a 3D foundation model and then present our first step towards building such a model. We expect that 3D geometric shapes will be a core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with a discussion outlining our path to building a fully unified foundation model for 3D intelligence.', 'score': 21, 'issue_id': 2800, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '89037dc780448ff8', 'authors': ['Foundation AI Team', 'Kiran Bhat', 'Nishchaie Khanna', 'Karun Channa', 'Tinghui Zhou', 'Yiheng Zhu', 'Xiaoxia Sun', 'Charles Shang', 'Anirudh Sudarshan', 'Maurice Chu', 'Daiqing Li', 'Kangle Deng', 'Jean-Philippe Fauconnier', 'Tijmen Verhulsdonck', 'Maneesh Agrawala', 'Kayvon Fatahalian', 'Alexander Weiss', 'Christian Reiser', 'Ravi Kiran Chirravuri', 'Ravali Kandur', 'Alejandro Pelaez', 'Akash Garg', 'Michael Palleschi', 'Jessica Wang', 'Skylar Litz', 'Leon Liu', 'Anying Li', 'David Harmon', 'Derek Liu', 'Liangjun Feng', 'Denis Goupil', 'Lukas Kuczynski', 'Jihyun Yoon', 'Naveen Marri', 'Peiye Zhuang', 'Yinan Zhang', 'Brian Yin', 'Haomiao Jiang', 'Marcel van Workum', 'Thomas Lane', 'Bryce Erickson', 'Salil Pathare', 'Kyle Price', 'Anupam Singh', 'David Baszucki'], 'affiliations': ['Foundation AI team, Roblox', 'Roblox'], 'pdf_title_img': 'assets/pdf/title_img/2503.15475.jpg', 'data': {'categories': ['#games', '#multimodal', '#3d', '#reasoning'], 'emoji': '🧊', 'ru': {'title': '3D-интеллект: фундаментальная модель для виртуальных миров', 'desc': 'Статья описывает разработку фундаментальной модели для 3D-интеллекта в Roblox. Модель призвана помочь разработчикам в создании всех аспектов игрового опыта, включая генерацию 3D-объектов, анимацию персонажей и программные скрипты. Авторы представляют решение для токенизации 3D-форм и демонстрируют его применение в генерации форм из текста, текста из форм и сцен из текста. Также обсуждается интеграция с существующими языковыми моделями для анализа и рассуждений о сценах.'}, 'en': {'title': 'Building the Future of 3D Intelligence in Roblox', 'desc': 'This paper presents a vision for creating a foundation model specifically designed for 3D intelligence, which can assist developers in various aspects of creating Roblox experiences. The authors emphasize the importance of 3D geometric shapes as a fundamental data type and introduce a novel 3D shape tokenizer to facilitate this process. They explore applications such as text-to-shape and shape-to-text generation, demonstrating how these can work alongside large language models for enhanced scene analysis and reasoning. The paper outlines the initial steps taken towards building a comprehensive model that integrates these capabilities into a unified framework for 3D content creation.'}, 'zh': {'title': '构建3D智能的基础模型', 'desc': '本论文探讨了如何为3D智能构建基础模型，该模型能够支持开发者在Roblox平台上生成3D对象、场景和动画角色。我们提出了三个关键设计要求，并介绍了构建3D形状标记器的初步步骤。我们的标记化方案可以应用于文本到形状生成、形状到文本生成和文本到场景生成等任务。最后，我们讨论了如何与现有的大型语言模型协作，以实现场景分析和推理。'}}}, {'id': 'https://huggingface.co/papers/2503.15417', 'title': 'Temporal Regularization Makes Your Video Generator Stronger', 'url': 'https://huggingface.co/papers/2503.15417', 'abstract': 'Temporal quality is a critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FluxFlow for initial investigation, a strategy designed to enhance temporal quality. Operating at the data level, FluxFlow applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as a simple yet effective approach to advancing video generation quality.', 'score': 18, 'issue_id': 2801, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '8eb262eda880d162', 'authors': ['Harold Haodong Chen', 'Haojian Huang', 'Xianfeng Wu', 'Yexin Liu', 'Yajing Bai', 'Wen-Jie Shu', 'Harry Yang', 'Ser-Nam Lim'], 'affiliations': ['Everlyn AI', 'HKU', 'HKUST', 'UCF'], 'pdf_title_img': 'assets/pdf/title_img/2503.15417.jpg', 'data': {'categories': ['#benchmark', '#video'], 'emoji': '🎬', 'ru': {'title': 'FluxFlow: Улучшение временного качества видео с помощью аугментации', 'desc': 'В статье рассматривается проблема улучшения временного качества при генерации видео с помощью нейронных сетей. Авторы предлагают новый метод под названием FluxFlow, который применяет контролируемые временные возмущения на уровне данных. Эксперименты показывают, что FluxFlow значительно улучшает временную согласованность и разнообразие для различных архитектур генеративных моделей видео. Метод не требует изменений в архитектуре моделей и может быть легко интегрирован в существующие подходы.'}, 'en': {'title': 'Enhancing Video Generation with Temporal Augmentation', 'desc': 'This paper addresses the challenge of maintaining consistent motion and realistic dynamics in video generation, focusing on the aspect of temporal quality. It introduces a novel method called FluxFlow, which applies controlled temporal perturbations to enhance temporal coherence and diversity in generated videos. The approach operates at the data level, meaning it does not require changes to the underlying model architecture. Experimental results on standard benchmarks show that FluxFlow significantly improves the performance of various video generation models while maintaining high spatial fidelity.'}, 'zh': {'title': '提升视频生成的时间质量', 'desc': '本研究探讨了视频生成中的时间质量问题，强调了在帧之间保持一致运动和真实动态的重要性。我们首次引入了时间增强技术，并提出了FluxFlow策略，以提高视频生成的时间质量。FluxFlow在数据层面进行操作，通过控制时间扰动来增强时间一致性和多样性，而无需修改模型架构。实验结果表明，FluxFlow在多个视频生成模型上显著改善了时间一致性和多样性，同时保持了空间保真度。'}}}, {'id': 'https://huggingface.co/papers/2503.14868', 'title': 'Efficient Personalization of Quantized Diffusion Model without\n  Backpropagation', 'url': 'https://huggingface.co/papers/2503.14868', 'abstract': 'Diffusion models have shown remarkable performance in image synthesis, but they demand extensive computational and memory resources for training, fine-tuning and inference. Although advanced quantization techniques have successfully minimized memory usage for inference, training and fine-tuning these quantized models still require large memory possibly due to dequantization for accurate computation of gradients and/or backpropagation for gradient-based algorithms. However, memory-efficient fine-tuning is particularly desirable for applications such as personalization that often must be run on edge devices like mobile phones with private data. In this work, we address this challenge by quantizing a diffusion model with personalization via Textual Inversion and by leveraging a zeroth-order optimization on personalization tokens without dequantization so that it does not require gradient and activation storage for backpropagation that consumes considerable memory. Since a gradient estimation using zeroth-order optimization is quite noisy for a single or a few images in personalization, we propose to denoise the estimated gradient by projecting it onto a subspace that is constructed with the past history of the tokens, dubbed Subspace Gradient. In addition, we investigated the influence of text embedding in image generation, leading to our proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for sampling with effective diffusion timesteps. Our method achieves comparable performance to prior methods in image and text alignment scores for personalizing Stable Diffusion with only forward passes while reducing training memory demand up to 8.2times.', 'score': 18, 'issue_id': 2802, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '8555fb94242b412c', 'authors': ['Hoigi Seo', 'Wongi Jeong', 'Kyungryeol Lee', 'Se Young Chun'], 'affiliations': ['Dept. of Electrical and Computer Engineering, INMC & IPAI Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.14868.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#cv', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Память под контролем: персонализация диффузионных моделей на краевых устройствах', 'desc': "Статья представляет новый метод для эффективной тонкой настройки диффузионных моделей с персонализацией на устройствах с ограниченной памятью. Авторы предлагают использовать квантизацию модели и оптимизацию нулевого порядка для токенов персонализации, что позволяет избежать хранения градиентов и активаций. Для улучшения оценки градиента предложен метод 'Subspace Gradient', проецирующий градиент на подпространство, построенное на основе истории токенов. Также введена техника 'Partial Uniform Timestep Sampling' для эффективного выбора временных шагов диффузии."}, 'en': {'title': 'Efficient Personalization of Diffusion Models with Subspace Gradient', 'desc': 'This paper presents a method to improve the efficiency of training diffusion models for image synthesis, particularly for personalization on edge devices. It introduces a quantization technique that allows for fine-tuning without the need for dequantization, thus saving memory during gradient computation. The authors propose a novel approach called Subspace Gradient to reduce noise in gradient estimation by utilizing historical data from personalization tokens. Additionally, they explore the impact of text embeddings on image generation, leading to a new sampling method that optimizes diffusion timesteps, achieving significant memory savings while maintaining performance.'}, 'zh': {'title': '高效个性化扩散模型的内存优化', 'desc': '扩散模型在图像合成中表现出色，但训练和微调需要大量计算和内存资源。尽管先进的量化技术可以减少推理时的内存使用，但训练这些量化模型仍然需要大量内存。本文提出了一种通过文本反演对扩散模型进行量化的方法，并利用零阶优化在不去量化的情况下进行个性化微调，从而减少内存消耗。我们的方法在图像和文本对齐得分上与之前的方法相当，同时将训练内存需求降低了多达8.2倍。'}}}, {'id': 'https://huggingface.co/papers/2503.11557', 'title': 'VERIFY: A Benchmark of Visual Explanation and Reasoning for\n  Investigating Multimodal Reasoning Fidelity', 'url': 'https://huggingface.co/papers/2503.11557', 'abstract': 'Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page (https://verify-eqh.pages.dev/).', 'score': 18, 'issue_id': 2812, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '468daf59bbaf8821', 'authors': ['Jing Bi', 'Junjia Guo', 'Susan Liang', 'Guangyu Sun', 'Luchuan Song', 'Yunlong Tang', 'Jinxi He', 'Jiarui Wu', 'Ali Vosoughi', 'Chen Chen', 'Chenliang Xu'], 'affiliations': ['University of Central Florida', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2503.11557.jpg', 'data': {'categories': ['#cv', '#interpretability', '#benchmark', '#multimodal', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'VERIFY: Новый рубеж в оценке визуального мышления ИИ', 'desc': 'Статья представляет новый бенчмарк VERIFY для оценки визуального мышления мультимодальных больших языковых моделей (MLLM). В отличие от существующих тестов, VERIFY фокусируется на способности моделей рассуждать на основе визуальной информации, минимизируя текстовый контекст. Бенчмарк включает аннотированные пути рассуждений и новые метрики для оценки качества визуального мышления. Тестирование ведущих MLLM с помощью VERIFY выявило значительные ограничения в их способностях к визуальному мышлению.'}, 'en': {'title': 'VERIFY: Elevating Visual Reasoning in MLLMs', 'desc': 'This paper introduces VERIFY, a new benchmark aimed at evaluating the visual reasoning abilities of Multimodal Large Language Models (MLLMs). Unlike existing benchmarks that focus on recognition tasks, VERIFY emphasizes reasoning from visual data with minimal textual support, reducing biases from language. Each task includes a human-annotated reasoning path, allowing for a deeper understanding of how models make decisions. The study also presents new metrics to assess visual reasoning fidelity, revealing significant limitations in current MLLMs and advocating for a more balanced approach to perception and reasoning.'}, 'zh': {'title': '视觉推理能力的全新评估', 'desc': '视觉推理是人类认知的重要部分，使人们能够理解和抽象地理解环境。尽管最近的多模态大型语言模型（MLLMs）在语言和视觉语言任务中表现出色，但现有基准主要测量识别能力，未能充分评估真正的视觉推理能力。为了解决这一问题，我们引入了VERIFY基准，专门设计用于严格评估最先进的MLLM的视觉推理能力。通过提供最小的文本上下文，VERIFY促使模型主要依赖视觉信息进行推理，从而减少对特定领域知识和语言偏见的依赖。'}}}, {'id': 'https://huggingface.co/papers/2503.15354', 'title': 'Optimizing Decomposition for Optimal Claim Verification', 'url': 'https://huggingface.co/papers/2503.15354', 'abstract': 'Current research on the Decompose-Then-Verify paradigm for evaluating the factuality of long-form text typically treats decomposition and verification in isolation, overlooking their interactions and potential misalignment. We find that existing decomposition policies, typically hand-crafted demonstrations, do not align well with downstream verifiers in terms of atomicity -- a novel metric quantifying information density -- leading to suboptimal verification results. We formulate finding the optimal decomposition policy for optimal verification as a bilevel optimization problem. To approximate a solution for this strongly NP-hard problem, we propose dynamic decomposition, a reinforcement learning framework that leverages verifier feedback to learn a policy for dynamically decomposing claims to verifier-preferred atomicity. Experimental results show that dynamic decomposition outperforms existing decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on average across varying verifiers, datasets, and atomcities of input claims.', 'score': 15, 'issue_id': 2811, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '1b88801be56f7c8f', 'authors': ['Yining Lu', 'Noah Ziems', 'Hy Dang', 'Meng Jiang'], 'affiliations': ['University of Notre Dame, South Bend, IN'], 'pdf_title_img': 'assets/pdf/title_img/2503.15354.jpg', 'data': {'categories': ['#long_context', '#rlhf', '#optimization', '#rl', '#benchmark', '#hallucinations'], 'emoji': '🧩', 'ru': {'title': 'Динамическая декомпозиция для оптимальной верификации текста', 'desc': 'Исследование посвящено улучшению метода Decompose-Then-Verify для оценки фактической точности длинных текстов. Авторы обнаружили, что существующие политики декомпозиции плохо согласуются с верификаторами по атомарности, что приводит к неоптимальным результатам проверки. Они предложили метод динамической декомпозиции, использующий обратную связь от верификатора для обучения политики декомпозиции утверждений. Эксперименты показали, что динамическая декомпозиция превосходит существующие подходы, улучшая точность верификации в среднем на 0.12.'}, 'en': {'title': 'Optimizing Decomposition for Better Verification in Text Factuality', 'desc': 'This paper addresses the challenges in the Decompose-Then-Verify approach for assessing the factuality of long-form text. It highlights that traditional decomposition methods do not effectively align with verification processes, particularly in terms of atomicity, which measures the density of information. The authors propose a bilevel optimization framework to find the best decomposition policy that enhances verification outcomes. They introduce a reinforcement learning method called dynamic decomposition, which adapts based on verifier feedback, resulting in improved verification confidence and accuracy across different scenarios.'}, 'zh': {'title': '动态分解：提升长文本验证的有效性', 'desc': '本研究探讨了在评估长文本事实性时，分解-验证范式的有效性。我们发现现有的分解策略与下游验证器之间存在不一致，导致验证结果不理想。为了解决这个问题，我们将寻找最佳分解策略视为一个双层优化问题，并提出了一种动态分解的强化学习框架。实验结果表明，动态分解在不同验证器和数据集上显著提高了验证信心和准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.14891', 'title': 'MetaLadder: Ascending Mathematical Solution Quality via\n  Analogical-Problem Reasoning Transfer', 'url': 'https://huggingface.co/papers/2503.14891', 'abstract': 'Large Language Models (LLMs) have demonstrated promising capabilities in solving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as a vital component in guiding answer generation. Current paradigms typically generate CoT and answers directly for a given problem, diverging from human problem-solving strategies to some extent. Humans often solve problems by recalling analogous cases and leveraging their solutions to reason about the current task. Inspired by this cognitive process, we propose MetaLadder, a novel framework that explicitly prompts LLMs to recall and reflect on meta-problems, those structurally or semantically analogous problems, alongside their CoT solutions before addressing the target problem. Additionally, we introduce a problem-restating mechanism to enhance the model\'s comprehension of the target problem by regenerating the original question, which further improves reasoning accuracy. Therefore, the model can achieve reasoning transfer from analogical problems, mimicking human-like "learning from examples" and generalization abilities. Extensive experiments on mathematical benchmarks demonstrate that our MetaLadder significantly boosts LLMs\' problem-solving accuracy, largely outperforming standard CoT-based methods (10.3\\% accuracy gain) and other methods. Our code and data has been released at https://github.com/LHL3341/MetaLadder.', 'score': 15, 'issue_id': 2811, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '0d8dbba5f4b7283d', 'authors': ['Honglin Lin', 'Zhuoshi Pan', 'Yu Li', 'Qizhi Pei', 'Xin Gao', 'Mengzhang Cai', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14891.jpg', 'data': {'categories': ['#transfer_learning', '#math', '#reasoning', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'MetaLadder: Обучение ИИ решать задачи по аналогии с человеком', 'desc': 'В статье представлен новый подход MetaLadder для улучшения способностей больших языковых моделей (LLM) в решении математических задач. Метод основан на имитации человеческого подхода к решению проблем путем вспоминания аналогичных задач и их решений. MetaLadder побуждает LLM вспоминать и анализировать мета-проблемы и их решения перед тем, как приступить к целевой задаче. Авторы также вводят механизм переформулирования задачи для улучшения понимания модели. Эксперименты показывают значительное повышение точности решения задач по сравнению с стандартными методами цепочки рассуждений.'}, 'en': {'title': 'MetaLadder: Enhancing LLMs with Analogical Reasoning for Better Problem Solving', 'desc': "This paper introduces MetaLadder, a new framework designed to improve the mathematical reasoning abilities of Large Language Models (LLMs). It emphasizes the importance of recalling and reflecting on similar past problems, known as meta-problems, to enhance the model's problem-solving process. By incorporating a problem-restating mechanism, the framework helps the model better understand the target problem, leading to improved reasoning accuracy. Experimental results show that MetaLadder significantly outperforms traditional Chain-of-Thought methods, achieving a notable increase in accuracy on mathematical tasks."}, 'zh': {'title': '借鉴类比问题，提升推理能力', 'desc': '大型语言模型（LLMs）在解决数学推理任务中展现了良好的能力，特别是利用链式思维（CoT）数据来指导答案生成。当前的方法通常直接为给定问题生成CoT和答案，这与人类的解题策略有所不同。人类通常通过回忆类似案例及其解决方案来推理当前任务。我们提出的MetaLadder框架，明确引导LLMs回忆和反思结构或语义上相似的元问题及其CoT解决方案，从而提高推理准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.12532', 'title': 'STEVE: AStep Verification Pipeline for Computer-use Agent Training', 'url': 'https://huggingface.co/papers/2503.12532', 'abstract': 'Developing AI agents to autonomously manipulate graphical user interfaces is a long challenging task. Recent advances in data scaling law inspire us to train computer-use agents with a scaled instruction set, yet using behavior cloning to train agents still requires immense high-quality trajectories. To meet the scalability need, we designed STEVE, a step verification pipeline for computer-use agent training. First, we establish a large instruction set for computer-use agents and collect trajectory data with some suboptimal agents. GPT-4o is used to verify the correctness of each step in the trajectories based on the screens before and after the action execution, assigning each step with a binary label. Last, we adopt the Kahneman and Tversky Optimization to optimize the agent from the binary stepwise labels. Extensive experiments manifest that our agent outperforms supervised finetuning by leveraging both positive and negative actions within a trajectory. Also, STEVE enables us to train a 7B vision-language model as a computer-use agent, achieving leading performance in the challenging live desktop environment WinAgentArena with great efficiency at a reduced cost. Code and data: https://github.com/FanbinLu/STEVE.', 'score': 12, 'issue_id': 2800, 'pub_date': '2025-03-16', 'pub_date_card': {'ru': '16 марта', 'en': 'March 16', 'zh': '3月16日'}, 'hash': '185728a70d3b80d0', 'authors': ['Fanbin Lu', 'Zhisheng Zhong', 'Ziqin Wei', 'Shu Liu', 'Chi-Wing Fu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKUST', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2503.12532.jpg', 'data': {'categories': ['#optimization', '#games', '#agents', '#training', '#cv'], 'emoji': '🖥️', 'ru': {'title': 'STEVE: эффективное обучение ИИ-агентов для работы с компьютерным интерфейсом', 'desc': 'Статья представляет STEVE - новый метод обучения ИИ-агентов для автономного управления графическими интерфейсами. Авторы используют большой набор инструкций и субоптимальные траектории, которые затем верифицируются с помощью GPT-4. На основе бинарных оценок каждого шага применяется оптимизация Канемана-Тверски для улучшения агента. Эксперименты показывают, что данный подход превосходит обычное обучение с учителем и позволяет эффективно обучить модель размером 7 миллиардов параметров для работы в сложной среде рабочего стола.'}, 'en': {'title': 'STEVE: Optimizing AI Agents for GUI Manipulation Efficiently', 'desc': "This paper presents STEVE, a novel step verification pipeline designed to enhance the training of AI agents for manipulating graphical user interfaces. By utilizing a large instruction set and collecting trajectory data from suboptimal agents, STEVE verifies the correctness of each action using GPT-4o, which labels steps as correct or incorrect. The approach incorporates Kahneman and Tversky Optimization to refine the agent's learning process based on these binary labels, allowing the agent to learn from both successful and unsuccessful actions. The results demonstrate that STEVE significantly improves performance in complex environments like WinAgentArena while being more cost-effective than traditional supervised finetuning methods."}, 'zh': {'title': '智能代理训练的新突破：STEVE', 'desc': '本论文提出了一种名为STEVE的步骤验证管道，用于训练计算机使用代理。我们首先建立了一个大型指令集，并收集了一些次优代理的轨迹数据。通过使用GPT-4o验证每个步骤的正确性，并为每个步骤分配二元标签，最后采用卡尼曼和特沃斯优化方法来优化代理。实验结果表明，我们的代理在复杂的桌面环境中表现优于传统的监督微调方法，且训练效率高、成本低。'}}}, {'id': 'https://huggingface.co/papers/2503.15264', 'title': 'LEGION: Learning to Ground and Explain for Synthetic Image Detection', 'url': 'https://huggingface.co/papers/2503.15264', 'abstract': 'The rapid advancements in generative technology have emerged as a double-edged sword. While offering powerful tools that enhance convenience, they also pose significant social concerns. As defenders, current synthetic image detection methods often lack artifact-level textual interpretability and are overly focused on image manipulation detection, and current datasets usually suffer from outdated generators and a lack of fine-grained annotations. In this paper, we introduce SynthScars, a high-quality and diverse dataset consisting of 12,236 fully synthetic images with human-expert annotations. It features 4 distinct image content types, 3 categories of artifacts, and fine-grained annotations covering pixel-level segmentation, detailed textual explanations, and artifact category labels. Furthermore, we propose LEGION (LEarning to Ground and explain for Synthetic Image detectiON), a multimodal large language model (MLLM)-based image forgery analysis framework that integrates artifact detection, segmentation, and explanation. Building upon this capability, we further explore LEGION as a controller, integrating it into image refinement pipelines to guide the generation of higher-quality and more realistic images. Extensive experiments show that LEGION outperforms existing methods across multiple benchmarks, particularly surpassing the second-best traditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score. Moreover, the refined images generated under its guidance exhibit stronger alignment with human preferences. The code, model, and dataset will be released.', 'score': 8, 'issue_id': 2805, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '4fb3a12e4b0a70e9', 'authors': ['Hengrui Kang', 'Siwei Wen', 'Zichen Wen', 'Junyan Ye', 'Weijia Li', 'Peilin Feng', 'Baichuan Zhou', 'Bin Wang', 'Dahua Lin', 'Linfeng Zhang', 'Conghui He'], 'affiliations': ['Beihang University', 'SenseTime Research', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15264.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#synthetic', '#alignment', '#cv', '#multimodal', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'LEGION: Новый подход к обнаружению и анализу синтетических изображений', 'desc': 'Статья представляет новый набор данных SynthScars, состоящий из 12,236 полностью синтетических изображений с экспертными аннотациями. Авторы также предлагают LEGION - мультимодальную модель на основе больших языковых моделей для анализа поддельных изображений. LEGION превосходит существующие методы по нескольким критериям, включая обнаружение артефактов, сегментацию и объяснение. Кроме того, модель может использоваться для улучшения качества генерируемых изображений.'}, 'en': {'title': 'Enhancing Synthetic Image Detection with LEGION and SynthScars', 'desc': 'This paper addresses the challenges in detecting synthetic images, which have become increasingly prevalent due to advancements in generative technology. It introduces SynthScars, a comprehensive dataset of 12,236 synthetic images with detailed annotations, including pixel-level segmentation and artifact categories. The authors propose LEGION, a multimodal large language model framework that enhances synthetic image detection by integrating artifact detection, segmentation, and explanation. Experimental results demonstrate that LEGION significantly outperforms existing methods, leading to improved image quality and alignment with human preferences.'}, 'zh': {'title': '提升合成图像检测的智能化与精确度', 'desc': '本论文介绍了SynthScars，这是一个包含12,236张完全合成图像的高质量多样化数据集，配有人工专家注释。该数据集涵盖了四种不同的图像内容类型和三类伪影，提供了像素级分割、详细文本解释和伪影类别标签的细粒度注释。我们还提出了LEGION，一个基于多模态大语言模型的图像伪造分析框架，能够整合伪影检测、分割和解释功能。实验结果表明，LEGION在多个基准测试中优于现有方法，生成的图像更符合人类偏好。'}}}, {'id': 'https://huggingface.co/papers/2503.14505', 'title': 'MusicInfuser: Making Video Diffusion Listen and Dance', 'url': 'https://huggingface.co/papers/2503.14505', 'abstract': 'We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at https://susunghong.github.io/MusicInfuser.', 'score': 8, 'issue_id': 2801, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '61ef56ac402491c0', 'authors': ['Susung Hong', 'Ira Kemelmacher-Shlizerman', 'Brian Curless', 'Steven M. Seitz'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.14505.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#diffusion', '#multimodal', '#video'], 'emoji': '💃', 'ru': {'title': 'Танцуй под музыку: ИИ-генерация видео в ритме мелодии', 'desc': 'MusicInfuser - это новый подход к генерации высококачественных танцевальных видео, синхронизированных с заданной музыкальной дорожкой. Он адаптирует существующие модели диффузии видео для согласования с музыкальными входными данными, используя легковесное кросс-внимание между музыкой и видео и низкоранговый адаптер. В отличие от предыдущих работ, MusicInfuser не требует данных захвата движения и дообучается только на танцевальных видео. Для оценки качества генерации танцев по нескольким параметрам авторы представили систему оценки с использованием видео-LLM.'}, 'en': {'title': 'Syncing Dance with Music: Introducing MusicInfuser', 'desc': "MusicInfuser is a novel method for creating high-quality dance videos that match a chosen music track. It leverages existing video diffusion models by incorporating music-video cross-attention and a low-rank adapter, rather than building a new model from scratch. This approach allows for fine-tuning on dance videos without the need for motion capture data, enhancing the model's efficiency. Additionally, MusicInfuser includes a new evaluation framework using Video-LLMs to measure various aspects of dance generation quality."}, 'zh': {'title': '音乐与舞蹈的完美融合', 'desc': '我们介绍了MusicInfuser，这是一种生成高质量舞蹈视频的方法，能够与指定的音乐轨道同步。我们通过引入轻量级的音乐-视频交叉注意力机制和低秩适配器，展示了如何调整现有的视频扩散模型以适应音乐输入。与之前需要运动捕捉数据的工作不同，我们的方法仅在舞蹈视频上进行微调。MusicInfuser在保持底层模型的灵活性和生成能力的同时，实现了高质量的音乐驱动视频生成。'}}}, {'id': 'https://huggingface.co/papers/2503.12769', 'title': 'ViSpeak: Visual Instruction Feedback in Streaming Videos', 'url': 'https://huggingface.co/papers/2503.12769', 'abstract': 'Recent advances in Large Multi-modal Models (LMMs) are primarily focused on offline video understanding. Instead, streaming video understanding poses great challenges to recent models due to its time-sensitive, omni-modal and interactive characteristics. In this work, we aim to extend the streaming video understanding from a new perspective and propose a novel task named Visual Instruction Feedback in which models should be aware of visual contents and learn to extract instructions from them. For example, when users wave their hands to agents, agents should recognize the gesture and start conversations with welcome information. Thus, following instructions in visual modality greatly enhances user-agent interactions. To facilitate research, we define seven key subtasks highly relevant to visual modality and collect the ViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation. Further, we propose the ViSpeak model, which is a SOTA streaming video understanding LMM with GPT-4o-level performance on various streaming video understanding benchmarks. After finetuning on our ViSpeak-Instruct dataset, ViSpeak is equipped with basic visual instruction feedback ability, serving as a solid baseline for future research.', 'score': 7, 'issue_id': 2800, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '0913c8e386f3aae5', 'authors': ['Shenghao Fu', 'Qize Yang', 'Yuan-Ming Li', 'Yi-Xing Peng', 'Kun-Yu Lin', 'Xihan Wei', 'Jian-Fang Hu', 'Xiaohua Xie', 'Wei-Shi Zheng'], 'affiliations': ['Guangdong Province Key Laboratory of Information Security Technology, China', 'Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China', 'Pazhou Laboratory (Huangpu), China', 'Peng Cheng Laboratory, China', 'School of Computer Science and Engineering, Sun Yat-sen University, China', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.12769.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark', '#video', '#agents'], 'emoji': '👁️', 'ru': {'title': 'ViSpeak: Революция в интерактивном понимании потокового видео', 'desc': "Статья представляет новую задачу под названием 'Визуальная инструктивная обратная связь' для потокового понимания видео. Авторы предлагают модель ViSpeak, способную распознавать визуальные инструкции и реагировать на них в режиме реального времени. Для обучения и оценки созданы наборы данных ViSpeak-Instruct и ViSpeak-Bench. Модель ViSpeak демонстрирует производительность на уровне GPT-4 в различных задачах потокового понимания видео."}, 'en': {'title': 'Enhancing User-Agent Interaction through Visual Instruction Feedback', 'desc': 'This paper introduces a new approach to understanding streaming video using Large Multi-modal Models (LMMs). It focuses on a task called Visual Instruction Feedback, where models learn to interpret visual cues, like hand gestures, to enhance interactions with users. The authors present the ViSpeak model, which achieves state-of-the-art performance in streaming video understanding after being trained on a newly created dataset. This work aims to improve user-agent communication by enabling agents to respond appropriately to visual instructions.'}, 'zh': {'title': '提升用户与代理互动的视觉指令反馈', 'desc': '近年来，大型多模态模型（LMM）在离线视频理解方面取得了显著进展。然而，流媒体视频理解由于其时间敏感性、全模态和交互特性，对现有模型提出了巨大挑战。本文提出了一种新任务，称为视觉指令反馈，模型需要理解视觉内容并从中提取指令，从而增强用户与代理之间的互动。我们定义了七个与视觉模态高度相关的子任务，并收集了ViSpeak-Instruct数据集用于训练，ViSpeak-Bench用于评估，同时提出了ViSpeak模型，展示了在流媒体视频理解基准上的先进性能。'}}}, {'id': 'https://huggingface.co/papers/2503.11227', 'title': 'GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction', 'url': 'https://huggingface.co/papers/2503.11227', 'abstract': 'The construction of Generalized Knowledge Graph (GKG), including knowledge graph, event knowledge graph and commonsense knowledge graph, is fundamental for various natural language processing tasks. Current studies typically construct these types of graph separately, overlooking holistic insights and potential unification that could be beneficial in computing resources and usage perspectives. However, a key challenge in developing a unified framework for GKG is obstacles arising from task-specific differences. In this study, we propose a unified framework for constructing generalized knowledge graphs to address this challenge. First, we collect data from 15 sub-tasks in 29 datasets across the three types of graphs, categorizing them into in-sample, counter-task, and out-of-distribution (OOD) data. Then, we propose a three-stage curriculum learning fine-tuning framework, by iteratively injecting knowledge from the three types of graphs into the Large Language Models. Extensive experiments show that our proposed model improves the construction of all three graph types across in-domain, OOD and counter-task data.', 'score': 7, 'issue_id': 2805, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '2e48a3554a1036a6', 'authors': ['Jian Zhang', 'Bifan Wei', 'Shihao Qi', 'haiping Zhu', 'Jun Liu', 'Qika Lin'], 'affiliations': ['National University of Singapore', 'School of Computer Science and Technology, Xian Jiaotong University, Xian, China', 'School of Continuing Education, Xian Jiaotong University, Xian, China', 'Shaanxi Province Key Laboratory of Big Data Knowledge Engineering, Xian Jiaotong University, Xian, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.11227.jpg', 'data': {'categories': ['#graphs', '#transfer_learning', '#data', '#training', '#dataset'], 'emoji': '🕸️', 'ru': {'title': 'Единый фреймворк для построения обобщенных графов знаний с помощью LLM', 'desc': 'Статья представляет унифицированный подход к построению обобщенных графов знаний (GKG), включающих графы знаний, графы событий и графы здравого смысла. Авторы предлагают трехэтапную структуру обучения с использованием курикулума для итеративного внедрения знаний в большие языковые модели (LLM). Эксперименты проводились на 15 подзадачах из 29 наборов данных, разделенных на внутривыборочные, контр-задачные и out-of-distribution данные. Результаты показывают улучшение в построении всех трех типов графов для различных видов данных.'}, 'en': {'title': 'Unifying Knowledge Graphs for Enhanced NLP Performance', 'desc': 'This paper presents a unified framework for constructing Generalized Knowledge Graphs (GKG) that integrates knowledge graphs, event knowledge graphs, and commonsense knowledge graphs. The authors identify the challenge of task-specific differences that hinder the unification of these graphs, which can lead to inefficiencies in resource usage. They propose a three-stage curriculum learning approach that fine-tunes Large Language Models by incorporating knowledge from all three graph types. Experimental results demonstrate that this framework enhances the performance of GKG construction across various data distributions, including in-sample, out-of-distribution, and counter-task scenarios.'}, 'zh': {'title': '统一构建广义知识图谱的框架', 'desc': '本文提出了一种统一的广义知识图谱（GKG）构建框架，旨在解决当前研究中各类知识图谱分开构建的问题。我们从29个数据集中收集了15个子任务的数据，并将其分类为样本内、对抗任务和分布外数据。通过三阶段的课程学习微调框架，我们将三种类型的知识逐步注入到大型语言模型中。实验结果表明，该模型在样本内、分布外和对抗任务数据上均提升了三种知识图谱的构建效果。'}}}, {'id': 'https://huggingface.co/papers/2503.13360', 'title': 'Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning', 'url': 'https://huggingface.co/papers/2503.13360', 'abstract': "Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems.", 'score': 5, 'issue_id': 2804, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '8877e2e1a13921d1', 'authors': ['Hai-Long Sun', 'Zhun Sun', 'Houwen Peng', 'Han-Jia Ye'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University', 'School of Artificial Intelligence, Nanjing University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2503.13360.jpg', 'data': {'categories': ['#training', '#long_context', '#reasoning', '#multimodal', '#math'], 'emoji': '🔍', 'ru': {'title': 'Улучшение мультимодальных рассуждений с помощью динамической обработки визуальной информации', 'desc': 'Статья описывает проблему мультимодальных языковых моделей (MLLM), которые теряют фокус на визуальной информации при решении задач, требующих длинных цепочек рассуждений. Авторы предлагают метод Take-along Visual Conditioning (TVC), который перемещает обработку изображений на критические этапы рассуждения и сжимает избыточные визуальные токены. Этот подход помогает модели сохранять внимание к визуальным компонентам на протяжении всего процесса рассуждения. Применение TVC позволило достичь наилучших результатов на пяти эталонных тестах по математическим рассуждениям, превзойдя предыдущие показатели на 3.4%.'}, 'en': {'title': 'Enhancing Visual Attention in Multimodal Reasoning with TVC', 'desc': 'This paper discusses the limitations of Multimodal Large Language Models (MLLMs) in maintaining attention to visual information during complex reasoning tasks. The authors found that MLLMs tend to rely more on text as reasoning progresses, leading to a decline in accuracy when visual inputs are removed. To address this issue, they propose a new method called Take-along Visual Conditioning (TVC), which strategically integrates visual inputs at critical stages of reasoning and reduces unnecessary visual data. Their approach significantly improves performance on mathematical reasoning tasks, achieving state-of-the-art results across multiple benchmarks.'}, 'zh': {'title': '提升多模态推理的视觉关注力', 'desc': '最近，大型语言模型（LLMs）在推理能力上取得了显著进展，尤其是从链式思维（CoT）提示演变到更先进的产品导向解决方案。在我们的模型重新实现过程中，我们发现多模态语言模型（MLLMs）在需要视觉输入的任务中（如几何问题）难以保持对视觉信息的关注，导致文本输出过于依赖。为了解决这个问题，我们提出了一种名为“随行视觉条件”（TVC）的策略，通过在关键推理阶段引入图像输入，并动态修剪冗余的视觉标记，帮助模型在整个推理过程中保持对视觉成分的关注。我们的研究在五个数学推理基准上实现了最新的最佳性能，证明了TVC在增强多模态推理系统中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.12963', 'title': 'Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait', 'url': 'https://huggingface.co/papers/2503.12963', 'abstract': 'Audio-driven single-image talking portrait generation plays a crucial role in virtual reality, digital human creation, and filmmaking. Existing approaches are generally categorized into keypoint-based and image-based methods. Keypoint-based methods effectively preserve character identity but struggle to capture fine facial details due to the fixed points limitation of the 3D Morphable Model. Moreover, traditional generative networks face challenges in establishing causality between audio and keypoints on limited datasets, resulting in low pose diversity. In contrast, image-based approaches produce high-quality portraits with diverse details using the diffusion network but incur identity distortion and expensive computational costs. In this work, we propose KDTalker, the first framework to combine unsupervised implicit 3D keypoint with a spatiotemporal diffusion model. Leveraging unsupervised implicit 3D keypoints, KDTalker adapts facial information densities, allowing the diffusion process to model diverse head poses and capture fine facial details flexibly. The custom-designed spatiotemporal attention mechanism ensures accurate lip synchronization, producing temporally consistent, high-quality animations while enhancing computational efficiency. Experimental results demonstrate that KDTalker achieves state-of-the-art performance regarding lip synchronization accuracy, head pose diversity, and execution efficiency.Our codes are available at https://github.com/chaolongy/KDTalker.', 'score': 5, 'issue_id': 2808, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '2cf2c79f5c5a0177', 'authors': ['Chaolong Yang', 'Kai Yao', 'Yuyao Yan', 'Chenru Jiang', 'Weiguang Zhao', 'Jie Sun', 'Guangliang Cheng', 'Yifei Zhang', 'Bin Dong', 'Kaizhu Huang'], 'affiliations': ['Ant Group, Hangzhou, 310000, China', 'Department of Computer Science, University of Liverpool, Liverpool, L69 7 ZX, UK', 'Department of Foundational Mathematics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China', 'Department of Mechatronics and Robotics, Xian Jiaotong-Liverpool University, Suzhou, 215123, China', 'Digital Innovation Research Center, Duke Kunshan University, Kunshan, 215316, China', 'Ricoh Software Research Center, Beijing, 100027, China', 'School of Robotic, Xian Jiaotong-Liverpool University, Suzhou, 215123, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.12963.jpg', 'data': {'categories': ['#architecture', '#cv', '#audio', '#diffusion', '#games', '#multimodal', '#video'], 'emoji': '🗣️', 'ru': {'title': 'KDTalker: Революция в генерации говорящих портретов', 'desc': 'KDTalker - это новый фреймворк для генерации анимированных портретов по аудио. Он сочетает неконтролируемые неявные 3D ключевые точки с пространственно-временной диффузионной моделью. KDTalker позволяет гибко моделировать разнообразные позы головы и мелкие детали лица. Благодаря специальному механизму пространственно-временного внимания, система обеспечивает точную синхронизацию губ и высокое качество анимации. Экспериментальные результаты показывают, что KDTalker достигает наилучших показателей по точности синхронизации губ, разнообразию поз головы и эффективности выполнения.'}, 'en': {'title': 'KDTalker: Revolutionizing Talking Portraits with Audio and 3D Keypoints', 'desc': 'This paper introduces KDTalker, a novel framework for generating talking portraits from audio inputs. It combines unsupervised implicit 3D keypoints with a spatiotemporal diffusion model to enhance facial detail and pose diversity. Unlike traditional methods, KDTalker effectively synchronizes lip movements with audio while maintaining character identity. The results show that KDTalker outperforms existing techniques in lip synchronization accuracy and computational efficiency.'}, 'zh': {'title': 'KDTalker：音频驱动的高效说话肖像生成', 'desc': '本论文提出了一种名为KDTalker的框架，用于音频驱动的单图像说话肖像生成。该方法结合了无监督隐式3D关键点和时空扩散模型，能够灵活捕捉细腻的面部细节和多样的头部姿态。KDTalker通过自定义的时空注意机制，确保了准确的唇部同步，生成高质量且时间一致的动画。实验结果表明，KDTalker在唇部同步精度、头部姿态多样性和执行效率方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.14830', 'title': 'Decompositional Neural Scene Reconstruction with Generative Diffusion\n  Prior', 'url': 'https://huggingface.co/papers/2503.14830', 'abstract': 'Decompositional reconstruction of 3D scenes, with complete shapes and detailed texture of all objects within, is intriguing for downstream applications but remains challenging, particularly with sparse views as input. Recent approaches incorporate semantic or geometric regularization to address this issue, but they suffer significant degradation in underconstrained areas and fail to recover occluded regions. We argue that the key to solving this problem lies in supplementing missing information for these areas. To this end, we propose DP-Recon, which employs diffusion priors in the form of Score Distillation Sampling (SDS) to optimize the neural representation of each individual object under novel views. This provides additional information for the underconstrained areas, but directly incorporating diffusion prior raises potential conflicts between the reconstruction and generative guidance. Therefore, we further introduce a visibility-guided approach to dynamically adjust the per-pixel SDS loss weights. Together these components enhance both geometry and appearance recovery while remaining faithful to input images. Extensive experiments across Replica and ScanNet++ demonstrate that our method significantly outperforms SOTA methods. Notably, it achieves better object reconstruction under 10 views than the baselines under 100 views. Our method enables seamless text-based editing for geometry and appearance through SDS optimization and produces decomposed object meshes with detailed UV maps that support photorealistic Visual effects (VFX) editing. The project page is available at https://dp-recon.github.io/.', 'score': 3, 'issue_id': 2813, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '7263c31fc82817f1', 'authors': ['Junfeng Ni', 'Yu Liu', 'Ruijie Lu', 'Zirui Zhou', 'Song-Chun Zhu', 'Yixin Chen', 'Siyuan Huang'], 'affiliations': ['Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14830.jpg', 'data': {'categories': ['#3d', '#cv', '#diffusion', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'DP-Recon: Реконструкция 3D-сцен с помощью диффузионных приоров', 'desc': 'DP-Recon - это новый метод реконструкции 3D-сцен, использующий диффузионные приоры для оптимизации нейронного представления объектов. Он применяет Score Distillation Sampling (SDS) и подход с учетом видимости для улучшения восстановления геометрии и внешнего вида объектов при ограниченном количестве входных изображений. Метод значительно превосходит современные аналоги, особенно в слабо ограниченных и закрытых областях сцены. DP-Recon также позволяет редактировать геометрию и внешний вид объектов с помощью текстовых запросов, создавая детализированные 3D-модели для применения в визуальных эффектах.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with DP-Recon!', 'desc': 'This paper presents DP-Recon, a method for reconstructing 3D scenes from sparse views while maintaining detailed shapes and textures of objects. The approach utilizes diffusion priors through Score Distillation Sampling (SDS) to fill in missing information in underconstrained areas, improving the reconstruction of occluded regions. To address potential conflicts between reconstruction accuracy and generative guidance, a visibility-guided mechanism is introduced to adjust loss weights dynamically. Experimental results show that DP-Recon outperforms state-of-the-art methods, achieving superior object reconstruction even with fewer input views, and enabling advanced text-based editing for visual effects.'}, 'zh': {'title': 'DP-Recon：提升3D场景重建的创新方法', 'desc': '本文提出了一种名为DP-Recon的方法，用于从稀疏视图重建3D场景，旨在恢复所有物体的完整形状和细致纹理。该方法利用扩散先验和得分蒸馏采样（SDS）来优化每个物体的神经表示，从而为欠约束区域提供额外信息。为了避免重建与生成指导之间的冲突，本文还引入了一种可见性引导的方法，动态调整每个像素的SDS损失权重。通过在Replica和ScanNet++数据集上的广泛实验，DP-Recon在物体重建方面显著优于现有最先进的方法，尤其是在视图数量较少的情况下。'}}}, {'id': 'https://huggingface.co/papers/2503.15450', 'title': 'SkyLadder: Better and Faster Pretraining via Context Window Scheduling', 'url': 'https://huggingface.co/papers/2503.15450', 'abstract': 'Recent advancements in LLM pretraining have featured ever-expanding context windows to process longer sequences. However, our pilot study reveals that models pretrained with shorter context windows consistently outperform their long-context counterparts under a fixed token budget. This finding motivates us to explore an optimal context window scheduling strategy to better balance long-context capability with pretraining efficiency. To this end, we propose SkyLadder, a simple yet effective approach that implements a short-to-long context window transition. SkyLadder preserves strong standard benchmark performance, while matching or exceeding baseline results on long context tasks. Through extensive experiments, we pre-train 1B-parameter models (up to 32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating that SkyLadder yields consistent gains of up to 3.7% on common benchmarks, while achieving up to 22% faster training speeds compared to baselines. The code is at https://github.com/sail-sg/SkyLadder.', 'score': 2, 'issue_id': 2814, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '92dcfd1093b6ecb7', 'authors': ['Tongyao Zhu', 'Qian Liu', 'Haonan Wang', 'Shiqi Chen', 'Xiangming Gu', 'Tianyu Pang', 'Min-Yen Kan'], 'affiliations': ['City University of Hong Kong', 'National University of Singapore', 'Sea AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.15450.jpg', 'data': {'categories': ['#optimization', '#architecture', '#benchmark', '#long_context', '#training'], 'emoji': '🪜', 'ru': {'title': 'SkyLadder: эффективное масштабирование контекстного окна в предобучении языковых моделей', 'desc': 'Исследователи обнаружили, что модели, предобученные на коротких контекстных окнах, превосходят модели с длинными окнами при фиксированном бюджете токенов. На основе этого наблюдения они разработали метод SkyLadder, который реализует переход от коротких к длинным контекстным окнам в процессе предобучения. SkyLadder сохраняет высокую производительность на стандартных бенчмарках, при этом соответствуя или превосходя базовые результаты на задачах с длинным контекстом. Эксперименты показали, что SkyLadder обеспечивает прирост до 3.7% на распространенных бенчмарках и ускорение обучения до 22% по сравнению с базовыми моделями.'}, 'en': {'title': 'SkyLadder: Optimizing Context Windows for Efficient LLM Training', 'desc': 'This paper discusses the impact of context window sizes in the pretraining of large language models (LLMs). It finds that models trained with shorter context windows perform better than those with longer ones when the total number of tokens is fixed. To address this, the authors introduce SkyLadder, a strategy that transitions from short to long context windows during training. Their experiments show that SkyLadder not only maintains high performance on standard benchmarks but also improves training speed significantly.'}, 'zh': {'title': 'SkyLadder：优化上下文窗口的高效预训练策略', 'desc': '最近在大规模语言模型（LLM）预训练方面的进展，采用了越来越大的上下文窗口来处理更长的序列。然而，我们的初步研究表明，在固定的标记预算下，使用较短上下文窗口预训练的模型表现优于长上下文模型。这一发现促使我们探索一种最佳的上下文窗口调度策略，以更好地平衡长上下文能力与预训练效率。为此，我们提出了SkyLadder，这是一种简单而有效的方法，实施短到长的上下文窗口过渡，保持强大的基准性能，同时在长上下文任务上匹配或超过基线结果。'}}}, {'id': 'https://huggingface.co/papers/2503.14434', 'title': 'LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers', 'url': 'https://huggingface.co/papers/2503.14434', 'abstract': 'Automated feature engineering plays a critical role in improving predictive model performance for tabular learning tasks. Traditional automated feature engineering methods are limited by their reliance on pre-defined transformations within fixed, manually designed search spaces, often neglecting domain knowledge. Recent advances using Large Language Models (LLMs) have enabled the integration of domain knowledge into the feature engineering process. However, existing LLM-based approaches use direct prompting or rely solely on validation scores for feature selection, failing to leverage insights from prior feature discovery experiments or establish meaningful reasoning between feature generation and data-driven performance. To address these challenges, we propose LLM-FE, a novel framework that combines evolutionary search with the domain knowledge and reasoning capabilities of LLMs to automatically discover effective features for tabular learning tasks. LLM-FE formulates feature engineering as a program search problem, where LLMs propose new feature transformation programs iteratively, and data-driven feedback guides the search process. Our results demonstrate that LLM-FE consistently outperforms state-of-the-art baselines, significantly enhancing the performance of tabular prediction models across diverse classification and regression benchmarks.', 'score': 2, 'issue_id': 2814, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '0f8980b0d2167f3d', 'authors': ['Nikhil Abhyankar', 'Parshin Shojaee', 'Chandan K. Reddy'], 'affiliations': ['Department of Computer Science, Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.14434.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#reasoning', '#data', '#training'], 'emoji': '🧠', 'ru': {'title': 'LLM-FE: Интеллектуальная инженерия признаков с помощью больших языковых моделей', 'desc': 'LLM-FE - это новый фреймворк для автоматизированной инженерии признаков в задачах табличного обучения. Он объединяет эволюционный поиск с возможностями больших языковых моделей (LLM) по использованию предметных знаний и рассуждению. LLM-FE формулирует инженерию признаков как задачу поиска программ, где LLM итеративно предлагают новые программы преобразования признаков, а обратная связь на основе данных направляет процесс поиска. Результаты показывают, что LLM-FE превосходит современные базовые методы, значительно улучшая производительность моделей табличного прогнозирования.'}, 'en': {'title': 'Revolutionizing Feature Engineering with LLMs', 'desc': 'This paper introduces LLM-FE, a new framework for automated feature engineering that enhances predictive model performance in tabular data tasks. Unlike traditional methods that rely on fixed transformations, LLM-FE utilizes Large Language Models (LLMs) to incorporate domain knowledge and reasoning into the feature generation process. The framework treats feature engineering as a program search problem, allowing LLMs to iteratively propose feature transformations while using data-driven feedback to refine their search. Experimental results show that LLM-FE outperforms existing methods, leading to better outcomes in various classification and regression tasks.'}, 'zh': {'title': '智能特征工程，提升预测模型性能！', 'desc': '自动特征工程在提高表格学习任务的预测模型性能中起着关键作用。传统的自动特征工程方法受限于预定义的变换和固定的手动设计搜索空间，往往忽视领域知识。最近，利用大型语言模型（LLMs）的进展使得将领域知识融入特征工程过程成为可能。我们提出的LLM-FE框架结合了进化搜索与LLMs的领域知识和推理能力，能够自动发现有效的特征，从而显著提升表格预测模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.15478', 'title': 'SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks', 'url': 'https://huggingface.co/papers/2503.15478', 'abstract': 'Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation.', 'score': 1, 'issue_id': 2814, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': 'a4dcf1557e92629c', 'authors': ['Yifei Zhou', 'Song Jiang', 'Yuandong Tian', 'Jason Weston', 'Sergey Levine', 'Sainbayar Sukhbaatar', 'Xian Li'], 'affiliations': ['FAIR at Meta', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.15478.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#games', '#rl', '#agents', '#rlhf', '#training'], 'emoji': '🤖', 'ru': {'title': 'SWEET-RL: Прорыв в обучении LLM-агентов для многоходовых взаимодействий', 'desc': 'Исследователи представили новый алгоритм обучения с подкреплением SWEET-RL для оптимизации агентов на основе больших языковых моделей (LLM) в многоходовых взаимодействиях. Алгоритм использует специально разработанную целевую функцию для обучения модели критика, которая предоставляет пошаговые вознаграждения для улучшения модели политики. Для оценки эффективности был создан новый бенчмарк ColBench, моделирующий реалистичные задачи в программировании и веб-дизайне. Эксперименты показали, что SWEET-RL превосходит существующие алгоритмы обучения с подкреплением на 6% по показателям успешности на ColBench.'}, 'en': {'title': 'Enhancing LLMs with SWEET-RL for Better Multi-Turn Interactions', 'desc': 'This paper addresses the challenge of optimizing large language model (LLM) agents for multi-turn interactions in real-world tasks. Existing reinforcement learning (RL) methods struggle with credit assignment across multiple turns, which affects their performance. To tackle this, the authors introduce ColBench, a benchmark for evaluating LLM agents in collaborative tasks, and propose a new RL algorithm called SWEET-RL. This algorithm enhances the training process by using a critic model that provides step-level rewards, leading to improved success rates in collaborative content creation tasks.'}, 'zh': {'title': '提升多轮交互的强化学习新方法', 'desc': '本文探讨了大型语言模型（LLM）在多轮交互任务中的表现，指出现有的多轮强化学习（RL）算法在有效的信用分配方面存在不足。为此，研究者们引入了一个新的基准测试ColBench，旨在评估LLM代理与人类协作解决实际任务的能力。基于此基准，提出了一种新颖的强化学习算法SWEET-RL，该算法通过设计优化目标来训练一个评论模型，从而提供逐步奖励以改善策略模型。实验结果表明，SWEET-RL在ColBench上相较于其他先进的多轮RL算法，成功率和胜率提高了6%，使得Llama-3.1-8B在实际协作内容创作中能够与GPT4-o的表现相匹配或超越。'}}}, {'id': 'https://huggingface.co/papers/2503.15055', 'title': 'ELTEX: A Framework for Domain-Driven Synthetic Data Generation', 'url': 'https://huggingface.co/papers/2503.15055', 'abstract': "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework for generating high-quality synthetic training data in specialized domains. While Large Language Models (LLMs) have shown impressive general capabilities, their performance in specialized domains like cybersecurity remains limited by the scarcity of domain-specific training data. ELTEX addresses this challenge by systematically integrating explicit domain indicator extraction with dynamic prompting to preserve critical domain knowledge throughout the generation process. We demonstrate ELTEX's effectiveness in the context of blockchain-related cyberattack detection, where we fine-tune Gemma-2B using various combinations of real and ELTEX-generated data. Our results show that the ELTEX-enhanced model achieves performance competitive with GPT-4 across both standard classification metrics and uncertainty calibration, while requiring significantly fewer computational resources. We release a curated synthetic dataset of social media texts for cyberattack detection in blockchain. Our work demonstrates that domain-driven synthetic data generation can effectively bridge the performance gap between resource-efficient models and larger architectures in specialized domains.", 'score': 1, 'issue_id': 2802, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '785ffad4856eb286', 'authors': ['Arina Razmyslovich', 'Kseniia Murasheva', 'Sofia Sedlova', 'Julien Capitaine', 'Eugene Dmitriev'], 'affiliations': ['Distributed Networks Institute (DNI)', 'Technologies Mésozoïques'], 'pdf_title_img': 'assets/pdf/title_img/2503.15055.jpg', 'data': {'categories': ['#science', '#data', '#training', '#synthetic', '#dataset'], 'emoji': '🛡️', 'ru': {'title': 'ELTEX: Синтетические данные для повышения эффективности малых ЯМ в специализированных областях', 'desc': 'ELTEX - это фреймворк для генерации синтетических данных в специализированных областях, таких как кибербезопасность. Он объединяет извлечение индикаторов предметной области с динамическим промптингом для сохранения критических знаний. ELTEX был применен для обнаружения кибератак в блокчейне, где модель Gemma-2B, дообученная на сгенерированных данных, показала результаты, сравнимые с GPT-4. Этот подход демонстрирует, что синтетические данные могут эффективно улучшать производительность малых моделей в специализированных областях.'}, 'en': {'title': 'Bridging the Data Gap in Cybersecurity with ELTEX', 'desc': 'The paper introduces ELTEX, a framework designed to create high-quality synthetic training data specifically for specialized fields like cybersecurity. It tackles the issue of limited domain-specific data that affects the performance of Large Language Models (LLMs) in these areas. By combining domain indicator extraction with dynamic prompting, ELTEX ensures that essential domain knowledge is maintained during data generation. The framework is validated through its application in blockchain cyberattack detection, showing that it can enhance model performance while being more resource-efficient than larger models like GPT-4.'}, 'zh': {'title': '领域驱动的合成数据生成，提升模型性能！', 'desc': 'ELTEX（高效LLM令牌提取）是一个针对特定领域生成高质量合成训练数据的框架。大型语言模型（LLM）在通用能力上表现出色，但在网络安全等专业领域的表现受到领域特定训练数据稀缺的限制。ELTEX通过系统地整合显式领域指示符提取和动态提示，确保在生成过程中保留关键的领域知识。我们的实验表明，ELTEX增强的模型在区块链相关的网络攻击检测中，性能与GPT-4相当，同时显著减少了计算资源的需求。'}}}, {'id': 'https://huggingface.co/papers/2503.13517', 'title': 'CURIE: Evaluating LLMs On Multitask Scientific Long Context\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2503.13517', 'abstract': 'Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie', 'score': 1, 'issue_id': 2810, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '56a7b2a9ca22936c', 'authors': ['Hao Cui', 'Zahra Shamsi', 'Gowoon Cheon', 'Xuejian Ma', 'Shutong Li', 'Maria Tikhanovskaya', 'Peter Norgaard', 'Nayantara Mudur', 'Martyna Plomecka', 'Paul Raccuglia', 'Yasaman Bahri', 'Victor V. Albert', 'Pranesh Srinivasan', 'Haining Pan', 'Philippe Faist', 'Brian Rohr', 'Michael J. Statt', 'Dan Morris', 'Drew Purves', 'Elise Kleeman', 'Ruth Alcantara', 'Matthew Abraham', 'Muqthar Mohammad', 'Ean Phing VanLee', 'Chenfei Jiang', 'Elizabeth Dorfman', 'Eun-Ah Kim', 'Michael P Brenner', 'Viren Jain', 'Sameera Ponda', 'Subhashini Venugopalan'], 'affiliations': ['Cornell', 'FU Berlin', 'Google', 'Harvard', 'Modelyst', 'NIST', 'Rutgers', 'UMD College Park', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.13517.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#science', '#dataset', '#multimodal', '#long_context'], 'emoji': '🧪', 'ru': {'title': 'CURIE: Новый рубеж для ИИ в науке', 'desc': 'CURIE - это новый бенчмарк для оценки потенциала больших языковых моделей (LLM) в решении научных задач. Он включает 580 задач из 6 научных дисциплин, требующих экспертных знаний, понимания длинного контекста и многоступенчатых рассуждений. Тестирование различных LLM на CURIE показало, что даже лучшие модели достигают лишь 32% эффективности, оставляя большой простор для улучшений. Авторы надеются, что результаты CURIE помогут в дальнейшем развитии LLM для научных приложений.'}, 'en': {'title': 'CURIE: Advancing LLMs for Scientific Problem-Solving', 'desc': 'The paper presents CURIE, a benchmark designed to evaluate the capabilities of Large Language Models (LLMs) in scientific problem-solving. It includes ten complex tasks with 580 curated problems across various scientific fields, emphasizing the need for domain expertise and multi-step reasoning. The evaluation reveals that while some models like Gemini Flash 2.0 and Claude-3 perform well, others like GPT-4o struggle significantly, particularly in protein sequencing tasks. The findings suggest that there is substantial potential for improving LLMs to better assist scientists in their workflows.'}, 'zh': {'title': 'CURIE：推动科学问题解决的语言模型基准', 'desc': '本文介绍了CURIE，一个用于评估大型语言模型（LLMs）在科学问题解决中的能力的基准。CURIE包含十个具有挑战性的任务，共580个问题和解决方案，涵盖材料科学、凝聚态物理、量子计算、地理空间分析、生物多样性和蛋白质等六个学科。我们评估了多种封闭和开放的LLMs，发现虽然Gemini Flash 2.0和Claude-3在各个领域表现出色，但GPT-4o和command-R+在蛋白质测序任务上表现不佳。CURIE的结果为未来LLMs在科学领域的发展提供了重要的指导。'}}}, {'id': 'https://huggingface.co/papers/2503.19325', 'title': 'Long-Context Autoregressive Video Modeling with Next-Frame Prediction', 'url': 'https://huggingface.co/papers/2503.19325', 'abstract': 'Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context vision modeling faces challenges due to visual redundancy. Existing RoPE lacks effective temporal decay for remote context and fails to extrapolate well to long video sequences. Additionally, training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle these issues, we propose balancing locality and long-range dependency. We introduce FlexRoPE, an test-time technique that adds flexible temporal decay to RoPE, enabling extrapolation to 16x longer vision contexts. Furthermore, we propose long short-term context modeling, where a high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling.', 'score': 59, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '543c7dbfad83ed73', 'authors': ['Yuchao Gu', 'Weijia Mao', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.19325.jpg', 'data': {'categories': ['#training', '#multimodal', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'FAR: эффективное моделирование длинных видеопоследовательностей', 'desc': 'Статья представляет Frame AutoRegressive (FAR) - новый подход к авторегрессионному моделированию видео с длинным контекстом. Авторы вводят FlexRoPE - технику, позволяющую экстраполировать модель на контексты в 16 раз длиннее обучающих. Предлагается комбинировать моделирование краткосрочного и долгосрочного контекста для эффективной обработки длинных видеопоследовательностей. FAR демонстрирует наилучшие результаты в генерации как коротких, так и длинных видео.'}, 'en': {'title': 'Revolutionizing Video Generation with Frame AutoRegressive Modeling', 'desc': 'This paper presents Frame AutoRegressive (FAR), a new method for generating videos by modeling the temporal dependencies between frames. FAR improves upon existing models by addressing the challenges of visual redundancy and the limitations of current positional encoding techniques like RoPE. The authors introduce FlexRoPE, which allows for flexible temporal decay, enabling the model to handle longer video sequences effectively. By combining short-term and long-term context modeling, FAR achieves state-of-the-art results in video generation while maintaining computational efficiency.'}, 'zh': {'title': '长时间上下文视频生成的新突破', 'desc': '本文介绍了一种新的视频自回归建模方法，称为Frame AutoRegressive (FAR)，旨在解决长时间上下文视频生成中的挑战。FAR通过建模连续帧之间的时间因果关系，超越了传统的语言模型，取得了更好的收敛效果。为了应对视觉冗余和计算成本问题，本文提出了FlexRoPE技术，能够灵活调整远程上下文的时间衰减，并引入了长短期上下文建模方法，以确保时间一致性。实验结果表明，FAR在短视频和长视频生成任务中均达到了最先进的性能，成为视频自回归建模的有效基线。'}}}, {'id': 'https://huggingface.co/papers/2503.18931', 'title': 'CoMP: Continual Multimodal Pre-training for Vision Foundation Models', 'url': 'https://huggingface.co/papers/2503.18931', 'abstract': 'Pre-trained Vision Foundation Models (VFMs) provide strong visual representations for a wide range of applications. In this paper, we continually pre-train prevailing VFMs in a multimodal manner such that they can effortlessly process visual inputs of varying sizes and produce visual representations that are more aligned with language representations, regardless of their original pre-training process. To this end, we introduce CoMP, a carefully designed multimodal pre-training pipeline. CoMP uses a Continual Rotary Position Embedding to support native resolution continual pre-training, and an Alignment Loss between visual and textual features through language prototypes to align multimodal representations. By three-stage training, our VFMs achieve remarkable improvements not only in multimodal understanding but also in other downstream tasks such as classification and segmentation. Remarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA with a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5 mIoU on ADE20K under frozen chunk evaluation.', 'score': 26, 'issue_id': 2897, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '59128d6a0bd3862c', 'authors': ['Yitong Chen', 'Lingchen Meng', 'Wujian Peng', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['Shanghai Innovation Institute', 'Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18931.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#optimization', '#training', '#cv'], 'emoji': '🔀', 'ru': {'title': 'Универсальное мультимодальное дообучение для улучшения визуальных моделей', 'desc': 'Статья представляет CoMP - новый метод мультимодального дообучения предобученных моделей компьютерного зрения (VFM). CoMP использует непрерывное ротационное позиционное кодирование для обработки изображений разного размера и функцию выравнивания для согласования визуальных и текстовых представлений. Трехэтапное обучение значительно улучшает результаты как в мультимодальном понимании, так и в задачах классификации и сегментации. Модель CoMP-SigLIP достигает высоких показателей на различных бенчмарках, сохраняя при этом хорошую точность на ImageNet-1K и ADE20K.'}, 'en': {'title': 'Enhancing Visual Models with Multimodal Pre-Training', 'desc': 'This paper presents a method to enhance Vision Foundation Models (VFMs) by using a multimodal pre-training approach. The proposed method, called CoMP, allows VFMs to process visual inputs of different sizes and align visual representations with language representations. CoMP employs a Continual Rotary Position Embedding and an Alignment Loss to improve the integration of visual and textual features. The results show significant performance gains in multimodal tasks and other applications like classification and segmentation, demonstrating the effectiveness of the proposed training pipeline.'}, 'zh': {'title': '提升视觉模型的多模态预训练方法', 'desc': '本文介绍了一种新的多模态预训练方法CoMP，用于提升视觉基础模型（VFM）的表现。通过持续的多模态预训练，模型能够处理不同大小的视觉输入，并生成与语言表示更一致的视觉表示。CoMP采用了持续旋转位置嵌入和视觉与文本特征之间的对齐损失，以实现多模态表示的对齐。经过三阶段训练，我们的模型在多模态理解和其他下游任务上都取得了显著的提升。'}}}, {'id': 'https://huggingface.co/papers/2503.19622', 'title': 'Exploring Hallucination of Large Multimodal Models in Video\n  Understanding: Benchmark, Analysis and Mitigation', 'url': 'https://huggingface.co/papers/2503.19622', 'abstract': 'The hallucination of large multimodal models (LMMs), providing responses that appear correct but are actually incorrect, limits their reliability and applicability. This paper aims to study the hallucination problem of LMMs in video modality, which is dynamic and more challenging compared to static modalities like images and text. From this motivation, we first present a comprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in video understanding tasks. It is built upon three dimensions, i.e., hallucination causes, hallucination aspects, and question formats, resulting in 6K questions. Then, we quantitatively study 7 influential factors on hallucinations, e.g., duration time of videos, model sizes, and model reasoning, via experiments of 16 LMMs on the presented benchmark. In addition, inspired by recent thinking models like OpenAI o1, we propose a video-thinking model to mitigate the hallucinations of LMMs via supervised reasoning fine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT enhances reasoning capabilities while TDPO reduces hallucinations in the thinking process. Extensive experiments and analyses demonstrate the effectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on hallucination evaluation and reduces the bias score by 4.5%. The code and data are public at https://github.com/Hongcheng-Gao/HAVEN.', 'score': 25, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'c727aefeccdca380', 'authors': ['Hongcheng Gao', 'Jiashu Qu', 'Jingyi Tang', 'Baolong Bi', 'Yue Liu', 'Hongyu Chen', 'Li Liang', 'Li Su', 'Qingming Huang'], 'affiliations': ['Beijing Jiaotong University', 'Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS', 'National University of Singapore', 'University of Chinese Academy of Sciences', 'University of Cincinnati'], 'pdf_title_img': 'assets/pdf/title_img/2503.19622.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#hallucinations', '#training', '#reasoning'], 'emoji': '🎥', 'ru': {'title': 'Борьба с галлюцинациями в видеоанализе: новый бенчмарк и мыслящая модель', 'desc': "Статья представляет комплексный подход к изучению проблемы галлюцинаций в крупных мультимодальных моделях (LMM) при обработке видео. Авторы создали бенчмарк HAVEN для оценки галлюцинаций LMM в задачах понимания видео, охватывающий различные аспекты и форматы вопросов. Исследование включает анализ семи факторов, влияющих на галлюцинации, и эксперименты с 16 различными LMM. Предложена модель 'video-thinking' для снижения галлюцинаций с использованием методов SRFT и TDPO, показавшая значительное улучшение точности и снижение предвзятости."}, 'en': {'title': 'Mitigating Hallucinations in Video Understanding with HAVEN', 'desc': 'This paper addresses the issue of hallucinations in large multimodal models (LMMs), particularly in video understanding tasks. It introduces a benchmark called HAVEN, which evaluates hallucinations based on various factors such as causes, aspects, and question formats, comprising 6,000 questions. The authors analyze seven influential factors affecting hallucinations and propose a novel video-thinking model that employs supervised reasoning fine-tuning and direct preference optimization to reduce these hallucinations. Experimental results show a significant improvement in accuracy and a reduction in bias, demonstrating the effectiveness of the proposed methods.'}, 'zh': {'title': '解决视频模态中的幻觉问题', 'desc': '本文研究了大型多模态模型（LMMs）在视频理解任务中的幻觉问题，这种问题使得模型的输出看似正确但实际上不准确。我们提出了一个名为HAVEN的基准，用于评估LMMs在视频模态下的幻觉，涵盖了幻觉的原因、方面和问题格式等三个维度，共包含6000个问题。通过对16个LMMs进行实验，我们定量分析了影响幻觉的7个因素，如视频时长、模型规模和推理能力。最后，我们提出了一种视频思维模型，通过监督推理微调（SRFT）和直接偏好优化（TDPO）来减轻幻觉现象，实验结果显示该方法在准确性上提高了7.65%。'}}}, {'id': 'https://huggingface.co/papers/2503.19385', 'title': 'Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing', 'url': 'https://huggingface.co/papers/2503.19385', 'abstract': 'We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.', 'score': 24, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'e0ead8fbe973f326', 'authors': ['Jaihoon Kim', 'Taehoon Yoon', 'Jisung Hwang', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.19385.jpg', 'data': {'categories': ['#inference', '#diffusion', '#optimization', '#video'], 'emoji': '🌊', 'ru': {'title': 'Эффективное масштабирование потоковых моделей при выводе', 'desc': 'Статья предлагает метод масштабирования во время вывода для предобученных потоковых моделей. Авторы вводят три ключевые идеи: генерация на основе стохастических дифференциальных уравнений (SDE), преобразование интерполянтов и адаптивное распределение вычислительных ресурсов. Эксперименты показывают, что генерация на основе SDE, особенно с сохранением дисперсии, улучшает производительность методов выборки частиц для масштабирования во время вывода в потоковых моделях. Предложенный подход превосходит предыдущие методы масштабирования во время вывода.'}, 'en': {'title': 'Enhancing Flow Models with Efficient Inference-Time Scaling', 'desc': 'This paper introduces a new method for improving flow models during inference, which is the process of generating outputs from trained models. The authors focus on three innovative techniques: using stochastic differential equations (SDE) for particle sampling, converting interpolants to increase diversity in generated samples, and implementing Rollover Budget Forcing (RBF) to optimize computational resource allocation. These methods allow flow models to achieve better sample quality and efficiency, similar to advancements seen in diffusion models. The results show that their approach significantly enhances performance, making flow models more competitive in generating high-quality images and videos.'}, 'zh': {'title': '流模型的高效推理时间缩放新方法', 'desc': '本文提出了一种针对预训练流模型的推理时间缩放方法。近年来，推理时间缩放在大语言模型和扩散模型中受到广泛关注，通过利用额外的计算来提高样本质量或更好地符合用户偏好。尽管流模型作为扩散模型的替代方案越来越受欢迎，但由于其确定性生成过程，现有的扩散模型推理时间缩放方法无法直接应用于流模型。我们提出了三种关键思想，以实现流模型的高效推理时间缩放：基于SDE的生成、插值转换和自适应计算资源分配。'}}}, {'id': 'https://huggingface.co/papers/2503.19903', 'title': 'Scaling Vision Pre-Training to 4K Resolution', 'url': 'https://huggingface.co/papers/2503.19903', 'abstract': 'High-resolution perception of visual details is crucial for daily tasks. Current vision pre-training, however, is still limited to low resolutions (e.g., 378 x 378 pixels) due to the quadratic cost of processing larger images. We introduce PS3 that scales CLIP-style vision pre-training to 4K resolution with a near-constant cost. Instead of contrastive learning on global image representation, PS3 is pre-trained by selectively processing local regions and contrasting them with local detailed captions, enabling high-resolution representation learning with greatly reduced computational overhead. The pre-trained PS3 is able to both encode the global image at low resolution and selectively process local high-resolution regions based on their saliency or relevance to a text prompt. When applying PS3 to multi-modal LLM (MLLM), the resulting model, named VILA-HD, significantly improves high-resolution visual perception compared to baselines without high-resolution vision pre-training such as AnyRes and S^2 while using up to 4.3x fewer tokens. PS3 also unlocks appealing scaling properties of VILA-HD, including scaling up resolution for free and scaling up test-time compute for better performance. Compared to state of the arts, VILA-HD outperforms previous MLLMs such as NVILA and Qwen2-VL across multiple benchmarks and achieves better efficiency than latest token pruning approaches. Finally, we find current benchmarks do not require 4K-resolution perception, which motivates us to propose 4KPro, a new benchmark of image QA at 4K resolution, on which VILA-HD outperforms all previous MLLMs, including a 14.5% improvement over GPT-4o, and a 3.2% improvement and 2.96x speedup over Qwen2-VL.', 'score': 21, 'issue_id': 2898, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '7b81adffd1f39557', 'authors': ['Baifeng Shi', 'Boyi Li', 'Han Cai', 'Yao Lu', 'Sifei Liu', 'Marco Pavone', 'Jan Kautz', 'Song Han', 'Trevor Darrell', 'Pavlo Molchanov', 'Hongxu Yin'], 'affiliations': ['NVIDIA', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.19903.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#multimodal', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'PS3: Эффективное предобучение для восприятия изображений сверхвысокого разрешения', 'desc': 'PS3 - это новый метод предобучения моделей компьютерного зрения, позволяющий работать с изображениями сверхвысокого разрешения (4K) при почти постоянных вычислительных затратах. Вместо контрастивного обучения на глобальном представлении изображения, PS3 обрабатывает выборочно локальные области и сопоставляет их с детальными текстовыми описаниями. Применение PS3 в мультимодальных языковых моделях (MLLM) значительно улучшает восприятие деталей изображений высокого разрешения по сравнению с базовыми моделями. PS3 также позволяет масштабировать разрешение и вычислительные ресурсы без дополнительных затрат, что приводит к повышению производительности.'}, 'en': {'title': 'Scaling Vision Pre-Training to 4K with PS3', 'desc': 'This paper presents PS3, a novel approach to vision pre-training that enables high-resolution image processing at 4K resolution while maintaining a near-constant computational cost. By focusing on local regions of images and contrasting them with detailed captions, PS3 enhances the learning of visual representations without the heavy resource demands typically associated with high-resolution data. The resulting model, VILA-HD, demonstrates significant improvements in visual perception tasks compared to existing models, achieving better efficiency and performance across various benchmarks. Additionally, the authors introduce 4KPro, a new benchmark for image question answering at 4K resolution, where VILA-HD shows superior results over previous models.'}, 'zh': {'title': '高分辨率视觉感知的新突破', 'desc': '本论文介绍了一种名为PS3的视觉预训练方法，能够以接近恒定的成本将CLIP风格的视觉预训练扩展到4K分辨率。PS3通过选择性处理局部区域并与局部详细描述进行对比，来实现高分辨率表示学习，从而大幅降低计算开销。预训练后的PS3能够在低分辨率下编码全局图像，并根据文本提示的显著性或相关性选择性处理局部高分辨率区域。最终，基于PS3的多模态大语言模型VILA-HD在多个基准测试中显著提升了高分辨率视觉感知能力，超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2503.14905', 'title': 'Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection\n  with Artifact Explanation', 'url': 'https://huggingface.co/papers/2503.14905', 'abstract': 'With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM.', 'score': 15, 'issue_id': 2900, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': 'e8053458773c179b', 'authors': ['Siwei Wen', 'Junyan Ye', 'Peilin Feng', 'Hengrui Kang', 'Zichen Wen', 'Yize Chen', 'Jiang Wu', 'Wenjun Wu', 'Conghui He', 'Weijia Li'], 'affiliations': ['Beihang University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Sun Yat-Sen University', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2503.14905.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#benchmark', '#dataset', '#synthetic', '#cv'], 'emoji': '🕵️', 'ru': {'title': 'FakeVLM: Интерпретируемое обнаружение синтетических изображений с помощью мультимодальной модели', 'desc': 'FakeVLM - это специализированная большая мультимодальная модель для обнаружения синтетических изображений и DeepFake. Она не только различает реальные и поддельные изображения, но и предоставляет понятные объяснения артефактов на естественном языке. Авторы также представили набор данных FakeClue с более чем 100 000 изображений, аннотированных подробными подсказками об артефактах. FakeVLM демонстрирует производительность на уровне экспертных моделей, устанавливая новый стандарт в обнаружении синтетических изображений.'}, 'en': {'title': 'FakeVLM: Unmasking Synthetic Images with Clarity', 'desc': 'This paper introduces FakeVLM, a large multimodal model designed to detect synthetic images and DeepFakes while providing human-readable explanations for its decisions. Unlike existing methods, FakeVLM enhances interpretability by explaining image artifacts in natural language, making it easier for users to understand the detection process. The model is evaluated on a new dataset called FakeClue, which contains over 100,000 annotated images, allowing for fine-grained analysis of image authenticity. Results show that FakeVLM performs comparably to expert models without needing extra classifiers, establishing a new standard in synthetic image detection.'}, 'zh': {'title': 'FakeVLM：合成图像检测的新标杆', 'desc': '随着人工智能生成内容（AIGC）技术的快速发展，合成图像在日常生活中变得越来越普遍，这给真实性评估和检测带来了新的挑战。现有的方法虽然在评估图像真实性和定位伪造方面有效，但往往缺乏人类可解释性，无法完全应对合成数据日益复杂的情况。为了解决这些问题，我们提出了FakeVLM，这是一种专门针对合成图像和深度伪造检测任务的大型多模态模型。FakeVLM不仅在区分真实与伪造图像方面表现出色，还能提供清晰的自然语言解释，增强了可解释性。'}}}, {'id': 'https://huggingface.co/papers/2503.13964', 'title': 'MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding', 'url': 'https://huggingface.co/papers/2503.13964', 'abstract': "Document Question Answering (DocQA) is a very common task. Existing methods using Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and Retrieval Augmented Generation (RAG) often prioritize information from a single modal, failing to effectively integrate textual and visual cues. These approaches struggle with complex multi-modal reasoning, limiting their performance on real-world documents. We present MDocAgent (A Multi-Modal Multi-Agent Framework for Document Understanding), a novel RAG and multi-agent framework that leverages both text and image. Our system employs five specialized agents: a general agent, a critical agent, a text agent, an image agent and a summarizing agent. These agents engage in multi-modal context retrieval, combining their individual insights to achieve a more comprehensive understanding of the document's content. This collaborative approach enables the system to synthesize information from both textual and visual components, leading to improved accuracy in question answering. Preliminary experiments on five benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of our MDocAgent, achieve an average improvement of 12.1% compared to current state-of-the-art method. This work contributes to the development of more robust and comprehensive DocQA systems capable of handling the complexities of real-world documents containing rich textual and visual information. Our data and code are available at https://github.com/aiming-lab/MDocAgent.", 'score': 12, 'issue_id': 2900, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': 'ac3677766b7897a0', 'authors': ['Siwei Han', 'Peng Xia', 'Ruiyi Zhang', 'Tong Sun', 'Yun Li', 'Hongtu Zhu', 'Huaxiu Yao'], 'affiliations': ['Adobe Research', 'UNC-Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.13964.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#optimization', '#agents', '#open_source', '#rag'], 'emoji': '🤖', 'ru': {'title': 'Мультиагентный подход к пониманию документов: объединяя текст и изображения', 'desc': 'Статья представляет MDocAgent - новую мультимодальную мультиагентную систему для понимания документов. В отличие от существующих методов, использующих большие языковые модели или RAG, MDocAgent эффективно интегрирует текстовую и визуальную информацию. Система использует пять специализированных агентов для многомодального поиска контекста и синтеза информации. Эксперименты показали улучшение точности ответов на вопросы в среднем на 12.1% по сравнению с современными методами.'}, 'en': {'title': 'MDocAgent: Uniting Text and Images for Smarter Document Understanding', 'desc': 'This paper introduces MDocAgent, a new framework designed for Document Question Answering (DocQA) that effectively combines text and image data. Unlike existing methods that often focus on a single type of information, MDocAgent utilizes a multi-agent system with five specialized agents to enhance multi-modal reasoning. These agents work together to retrieve and synthesize information from both textual and visual sources, improving the accuracy of answers to questions about documents. Preliminary tests show that MDocAgent outperforms current leading methods by an average of 12.1%, demonstrating its potential for better handling complex real-world documents.'}, 'zh': {'title': '多模态文档理解的新突破', 'desc': '本文介绍了一种新的文档问答系统MDocAgent，它结合了文本和图像信息，旨在提高多模态文档理解的能力。现有的方法往往只关注单一模态，导致在复杂的多模态推理中表现不佳。MDocAgent采用了五个专门的代理，分别负责不同的任务，通过协作检索多模态上下文，从而更全面地理解文档内容。实验结果表明，MDocAgent在多个基准测试中表现优异，平均提高了12.1%的准确率，展示了其在处理现实世界文档中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.19855', 'title': 'Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time\n  Thinking', 'url': 'https://huggingface.co/papers/2503.19855', 'abstract': "Recent advances in large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where extended reasoning processes substantially enhance model performance. Despite this, current models are constrained by limitations in handling long texts and reinforcement learning (RL) training efficiency. To address these issues, we propose a simple yet effective test-time scaling approach Multi-round Thinking. This method iteratively refines model reasoning by leveraging previous answers as prompts for subsequent rounds. Extensive experiments across multiple models, including QwQ-32B and DeepSeek-R1, consistently show performance improvements on various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and LiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round 1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a similar increase from 79.7% to 82.0%. These results confirm that Multi-round Thinking is a broadly applicable, straightforward approach to achieving stable enhancements in model performance, underscoring its potential for future developments in test-time scaling techniques. The key prompt: {Original question prompt} The assistant's previous answer is: <answer> {last round answer} </answer>, and please re-answer.", 'score': 11, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'c9d16e0d2423104a', 'authors': ['Xiaoyu Tian', 'Sitong Zhao', 'Haotian Wang', 'Shuaiting Chen', 'Yunjie Ji', 'Yiping Peng', 'Han Zhao', 'Xiangang Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.19855.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#optimization', '#training', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'Итеративное улучшение ответов ИИ: простой путь к повышению точности', 'desc': "Статья представляет новый подход к масштабированию языковых моделей во время тестирования, называемый 'Многораундовое мышление'. Этот метод итеративно улучшает рассуждения модели, используя предыдущие ответы в качестве подсказок для последующих раундов. Эксперименты с различными моделями, включая QwQ-32B и DeepSeek-R1, показали стабильное улучшение производительности на нескольких бенчмарках. Результаты подтверждают, что 'Многораундовое мышление' - это широко применимый подход для повышения эффективности языковых моделей."}, 'en': {'title': 'Enhancing Model Performance with Multi-round Thinking', 'desc': "This paper introduces a new method called Multi-round Thinking to improve the performance of large language models (LLMs) during test-time scaling. The approach involves iteratively refining the model's reasoning by using previous answers as prompts for subsequent rounds of questioning. Experiments show that this method leads to significant accuracy improvements across various benchmarks, demonstrating its effectiveness in enhancing model performance. The results indicate that Multi-round Thinking is a simple yet powerful technique that can be widely applied to improve LLMs' reasoning capabilities."}, 'zh': {'title': '多轮思考：提升模型推理的有效方法', 'desc': '本文介绍了一种名为多轮思考的测试时间扩展方法，旨在提高大型语言模型的推理能力。该方法通过将之前的答案作为后续轮次的提示，迭代地优化模型的推理过程。实验结果表明，使用多轮思考后，多个模型在不同基准测试上的表现均有显著提升。比如，QwQ-32B在AIME 2024数据集上的准确率从80.3%提升至82.1%。'}}}, {'id': 'https://huggingface.co/papers/2503.19910', 'title': 'CoLLM: A Large Language Model for Composed Image Retrieval', 'url': 'https://huggingface.co/papers/2503.19910', 'abstract': 'Composed Image Retrieval (CIR) is a complex task that aims to retrieve images based on a multimodal query. Typical training data consists of triplets containing a reference image, a textual description of desired modifications, and the target image, which are expensive and time-consuming to acquire. The scarcity of CIR datasets has led to zero-shot approaches utilizing synthetic triplets or leveraging vision-language models (VLMs) with ubiquitous web-crawled image-caption pairs. However, these methods have significant limitations: synthetic triplets suffer from limited scale, lack of diversity, and unnatural modification text, while image-caption pairs hinder joint embedding learning of the multimodal query due to the absence of triplet data. Moreover, existing approaches struggle with complex and nuanced modification texts that demand sophisticated fusion and understanding of vision and language modalities. We present CoLLM, a one-stop framework that effectively addresses these limitations. Our approach generates triplets on-the-fly from image-caption pairs, enabling supervised training without manual annotation. We leverage Large Language Models (LLMs) to generate joint embeddings of reference images and modification texts, facilitating deeper multimodal fusion. Additionally, we introduce Multi-Text CIR (MTCIR), a large-scale dataset comprising 3.4M samples, and refine existing CIR benchmarks (CIRR and Fashion-IQ) to enhance evaluation reliability. Experimental results demonstrate that CoLLM achieves state-of-the-art performance across multiple CIR benchmarks and settings. MTCIR yields competitive results, with up to 15% performance improvement. Our refined benchmarks provide more reliable evaluation metrics for CIR models, contributing to the advancement of this important field.', 'score': 8, 'issue_id': 2896, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '63f36082e6c27f3e', 'authors': ['Chuong Huynh', 'Jinyu Yang', 'Ashish Tawari', 'Mubarak Shah', 'Son Tran', 'Raffay Hamid', 'Trishul Chilimbi', 'Abhinav Shrivastava'], 'affiliations': ['Amazon', 'Center for Research in Computer Vision, University of Central Florida', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2503.19910.jpg', 'data': {'categories': ['#optimization', '#synthetic', '#dataset', '#multimodal', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'CoLLM: Революция в композиционном поиске изображений', 'desc': 'Статья представляет CoLLM - новый фреймворк для решения задачи композиционного поиска изображений (CIR). CoLLM генерирует обучающие триплеты на лету из пар изображение-подпись, что позволяет обучаться без ручной разметки. Авторы используют большие языковые модели для создания совместных эмбеддингов изображений и текстов модификации, улучшая мультимодальное слияние. Также представлен новый крупномасштабный датасет MTCIR и уточнены существующие бенчмарки для более надежной оценки моделей CIR.'}, 'en': {'title': 'Revolutionizing Composed Image Retrieval with CoLLM', 'desc': 'This paper introduces CoLLM, a novel framework for Composed Image Retrieval (CIR) that overcomes the challenges of limited training data. It generates triplets from existing image-caption pairs, allowing for supervised training without the need for manual annotations. By utilizing Large Language Models (LLMs), CoLLM creates joint embeddings that enhance the understanding of complex multimodal queries. The authors also present the Multi-Text CIR (MTCIR) dataset, which significantly improves evaluation metrics and demonstrates state-of-the-art performance in CIR tasks.'}, 'zh': {'title': 'CoLLM：复合图像检索的新突破', 'desc': '本文介绍了一种名为CoLLM的框架，用于解决复合图像检索（CIR）中的数据稀缺问题。该框架通过从图像-文本对中动态生成三元组，避免了手动标注的需求，从而实现了监督学习。我们利用大型语言模型（LLMs）生成参考图像和修改文本的联合嵌入，促进了多模态的深度融合。此外，我们还推出了一个包含340万样本的大规模数据集MTCIR，并改进了现有的CIR基准，以提高评估的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2503.19470', 'title': 'ReSearch: Learning to Reason with Search for LLMs via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2503.19470', 'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.', 'score': 8, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '3e50fa3f4f4a6c0c', 'authors': ['Mingyang Chen', 'Tianpeng Li', 'Haoze Sun', 'Yijie Zhou', 'Chenzheng Zhu', 'Fan Yang', 'Zenan Zhou', 'Weipeng Chen', 'Haofen Wang', 'Jeff Z. Pan', 'Wen Zhang', 'Huajun Chen'], 'affiliations': ['Baichuan Inc.', 'The University of Edinburgh', 'Tongji University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.19470.jpg', 'data': {'categories': ['#rl', '#benchmark', '#rag', '#training', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Усиление рассуждений ИИ через интеграцию поиска', 'desc': 'Авторы предлагают новый фреймворк ReSearch, который обучает большие языковые модели (LLM) рассуждать с использованием поиска через обучение с подкреплением. Модель учится интегрировать операции поиска в цепочку рассуждений, где текстовое мышление направляет, когда и как выполнять поиск. Эксперименты показывают, что, несмотря на обучение только на одном наборе данных, модели демонстрируют сильную обобщаемость на различных бенчмарках. Анализ выявляет, что ReSearch естественным образом вызывает продвинутые способности рассуждения, такие как рефлексия и самокоррекция.'}, 'en': {'title': 'Empowering LLMs: Reasoning Meets Search with ReSearch', 'desc': 'This paper introduces ReSearch, a new framework that enhances Large Language Models (LLMs) by integrating reasoning with external search processes. It uses reinforcement learning to train LLMs to effectively decide when and how to perform searches, treating these operations as key parts of the reasoning process. The framework is tested on Qwen2.5 models and shows strong performance across different benchmarks, even though it was trained on a single dataset. Notably, ReSearch enables advanced reasoning skills like reflection and self-correction, showcasing its potential for complex question answering.'}, 'zh': {'title': '推理与搜索的完美结合', 'desc': '大型语言模型（LLMs）在推理方面表现出色，但将推理与外部搜索过程结合仍然具有挑战性，尤其是对于复杂的多跳问题。我们提出了ReSearch，一个新颖的框架，通过强化学习训练LLMs进行搜索推理，而不使用任何监督数据。该方法将搜索操作视为推理链的核心部分，搜索的时机和方式由基于文本的思维指导，搜索结果进一步影响推理过程。我们的实验表明，尽管只在一个数据集上训练，ReSearch模型在多个基准测试中展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.18446', 'title': 'Latent Space Super-Resolution for Higher-Resolution Image Generation\n  with Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18446', 'abstract': 'In this paper, we propose LSRNA, a novel framework for higher-resolution (exceeding 1K) image generation using diffusion models by leveraging super-resolution directly in the latent space. Existing diffusion models struggle with scaling beyond their training resolutions, often leading to structural distortions or content repetition. Reference-based methods address the issues by upsampling a low-resolution reference to guide higher-resolution generation. However, they face significant challenges: upsampling in latent space often causes manifold deviation, which degrades output quality. On the other hand, upsampling in RGB space tends to produce overly smoothed outputs. To overcome these limitations, LSRNA combines Latent space Super-Resolution (LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance high-frequency details. Our extensive experiments demonstrate that integrating LSRNA outperforms state-of-the-art reference-based methods across various resolutions and metrics, while showing the critical role of latent space upsampling in preserving detail and sharpness. The code is available at https://github.com/3587jjh/LSRNA.', 'score': 6, 'issue_id': 2897, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'd3e203fb399d6eee', 'authors': ['Jinho Jeong', 'Sangmin Han', 'Jinwoo Kim', 'Seon Joo Kim'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18446.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#3d', '#optimization', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'LSRNA: Суперразрешение в латентном пространстве для генерации детализированных изображений', 'desc': 'LSRNA - это новый фреймворк для генерации изображений высокого разрешения (более 1K) с использованием диффузионных моделей. Он решает проблемы существующих методов, которые часто приводят к искажениям структуры или повторению контента при масштабировании. LSRNA сочетает суперразрешение в латентном пространстве (LSR) для выравнивания многообразия и добавление шума по регионам (RNA) для улучшения высокочастотных деталей. Эксперименты показывают, что LSRNA превосходит современные методы на основе референсов по различным разрешениям и метрикам.'}, 'en': {'title': 'Enhancing High-Resolution Image Generation with LSRNA', 'desc': 'This paper introduces LSRNA, a new framework designed to generate high-resolution images (over 1K) using diffusion models by applying super-resolution techniques in the latent space. Traditional diffusion models often struggle with generating images at higher resolutions, leading to issues like distortions and repeated content. The proposed method addresses these challenges by utilizing Latent space Super-Resolution (LSR) for better alignment and Region-wise Noise Addition (RNA) to improve detail in the generated images. Experimental results show that LSRNA significantly outperforms existing reference-based methods, highlighting the importance of latent space upsampling for maintaining image quality.'}, 'zh': {'title': 'LSRNA：超分辨率生成的创新框架', 'desc': '本文提出了一种新颖的框架LSRNA，用于生成高分辨率（超过1K）的图像，利用扩散模型直接在潜在空间中进行超分辨率处理。现有的扩散模型在超出训练分辨率时常常出现结构失真或内容重复的问题。参考基础的方法通过将低分辨率参考图像上采样来指导高分辨率生成，但在潜在空间中上采样会导致流形偏差，从而降低输出质量。LSRNA结合了潜在空间超分辨率（LSR）和区域噪声添加（RNA），有效提升了高频细节，实验结果表明其在各个分辨率和指标上均优于现有的参考基础方法。'}}}, {'id': 'https://huggingface.co/papers/2503.19065', 'title': 'WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation', 'url': 'https://huggingface.co/papers/2503.19065', 'abstract': 'Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. We show some of our generated examples in https://wikiautogen.github.io/ .', 'score': 5, 'issue_id': 2908, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'aaba6b8dd7c54bf2', 'authors': ['Zhongyu Yang', 'Jun Chen', 'Dannong Xu', 'Junjie Fei', 'Xiaoqian Shen', 'Liangbing Zhao', 'Chun-Mei Feng', 'Mohamed Elhoseiny'], 'affiliations': ['IHPC, A*STAR', 'King Abdullah University of Science and Technology', 'Lanzhou University', 'The University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2503.19065.jpg', 'data': {'categories': ['#story_generation', '#benchmark', '#interpretability', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'WikiAutoGen: ИИ создает мультимодальные статьи Википедии', 'desc': 'Статья представляет WikiAutoGen - новую систему для автоматизированного создания мультимодальных статей в стиле Википедии. В отличие от предыдущих подходов, WikiAutoGen извлекает и интегрирует релевантные изображения вместе с текстом, обогащая глубину и визуальную привлекательность генерируемого контента. Система использует механизм многоперспективной саморефлексии для улучшения фактической точности и полноты. Авторы также представляют WikiSeek - эталонный набор данных для оценки мультимодальной генерации знаний по более сложным темам.'}, 'en': {'title': 'Enhancing Wikipedia Articles with Multimodal Automation', 'desc': 'This paper presents WikiAutoGen, a system designed to automate the generation of Wikipedia-style articles using both text and images. Unlike previous methods that focused solely on text, WikiAutoGen enhances the informativeness and engagement of articles by integrating relevant images. The system employs a multi-perspective self-reflection mechanism to ensure the accuracy and coherence of the content by evaluating it from various viewpoints. Experimental results demonstrate that WikiAutoGen significantly outperforms existing approaches, achieving higher quality in multimodal article generation.'}, 'zh': {'title': '自动生成多模态维基百科文章的新方法', 'desc': '本研究提出了一种新的系统WikiAutoGen，用于自动生成多模态的维基百科风格文章。与以往仅关注文本生成的方法不同，WikiAutoGen同时检索和整合相关图像，增强了生成内容的深度和视觉吸引力。为了提高事实准确性和全面性，我们引入了一种多角度自我反思机制，从不同视角评估检索内容，以增强可靠性和连贯性。此外，我们还推出了WikiSeek基准，旨在评估在更具挑战性主题上的多模态知识生成。'}}}, {'id': 'https://huggingface.co/papers/2503.19041', 'title': 'LookAhead Tuning: Safer Language Models via Partial Answer Previews', 'url': 'https://huggingface.co/papers/2503.19041', 'abstract': "Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.", 'score': 4, 'issue_id': 2896, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '8495b5a09ddf5611', 'authors': ['Kangwei Liu', 'Mengru Wang', 'Yujie Luo', 'Lin Yuan', 'Mengshu Sun', 'Ningyu Zhang', 'Lei Liang', 'Zhiqiang Zhang', 'Jun Zhou', 'Huajun Chen'], 'affiliations': ['Ant Group', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2503.19041.jpg', 'data': {'categories': ['#training', '#alignment', '#low_resource'], 'emoji': '🔍', 'ru': {'title': 'Безопасная адаптация языковых моделей с сохранением производительности', 'desc': 'Статья представляет новый метод тонкой настройки больших языковых моделей под названием LookAhead Tuning. Этот подход позволяет адаптировать модели к конкретным доменам, сохраняя при этом их изначальную безопасность. Метод основан на модификации обучающих данных путем предварительного просмотра частичных префиксов ответов. Эксперименты показывают, что LookAhead Tuning эффективно поддерживает безопасность модели без ущерба для производительности на целевых задачах.'}, 'en': {'title': 'LookAhead Tuning: Safeguarding LLMs During Fine-Tuning', 'desc': "This paper presents LookAhead Tuning, a novel approach to fine-tuning large language models (LLMs) while preserving their safety alignment. The method involves two data-driven techniques that adjust training data by examining partial answer prefixes, which helps maintain the model's safety mechanisms. By minimizing changes to the initial token distributions, LookAhead Tuning effectively prevents safety degradation during the adaptation process. Experimental results show that this approach not only safeguards model safety but also ensures strong performance on various downstream tasks."}, 'zh': {'title': 'LookAhead Tuning：安全微调大型语言模型的新方法', 'desc': '本论文介绍了一种名为LookAhead Tuning的技术，旨在解决在微调大型语言模型（LLMs）时安全性下降的问题。该方法通过预览部分答案前缀，采用两种简单且低资源的数据驱动方法来修改训练数据。其目标是通过最小化初始标记分布的扰动，保持模型固有的安全机制。实验结果表明，LookAhead Tuning能够有效维护模型的安全性，同时在下游任务中保持强大的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.19907', 'title': 'FullDiT: Multi-Task Video Generative Foundation Model with Full\n  Attention', 'url': 'https://huggingface.co/papers/2503.19907', 'abstract': 'Current video generative foundation models primarily focus on text-to-video tasks, providing limited control for fine-grained video content creation. Although adapter-based approaches (e.g., ControlNet) enable additional controls with minimal fine-tuning, they encounter challenges when integrating multiple conditions, including: branch conflicts between independently trained adapters, parameter redundancy leading to increased computational cost, and suboptimal performance compared to full fine-tuning. To address these challenges, we introduce FullDiT, a unified foundation model for video generation that seamlessly integrates multiple conditions via unified full-attention mechanisms. By fusing multi-task conditions into a unified sequence representation and leveraging the long-context learning ability of full self-attention to capture condition dynamics, FullDiT reduces parameter overhead, avoids conditions conflict, and shows scalability and emergent ability. We further introduce FullBench for multi-task video generation evaluation. Experiments demonstrate that FullDiT achieves state-of-the-art results, highlighting the efficacy of full-attention in complex multi-task video generation.', 'score': 3, 'issue_id': 2910, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '7505b749f1328947', 'authors': ['Xuan Ju', 'Weicai Ye', 'Quande Liu', 'Qiulin Wang', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Kun Gai', 'Qiang Xu'], 'affiliations': ['Kuaishou Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.19907.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#video', '#long_context', '#games'], 'emoji': '🎬', 'ru': {'title': 'FullDiT: Единая модель для многозадачной генерации видео', 'desc': 'FullDiT - это новая модель для генерации видео, которая объединяет множество условий через механизмы полного внимания. Она решает проблемы существующих подходов, таких как конфликты между адаптерами и избыточность параметров. FullDiT использует единое представление последовательности для различных задач и способность полного самовнимания к обучению на длинном контексте. Эксперименты показывают, что FullDiT достигает передовых результатов в сложных задачах генерации видео с несколькими условиями.'}, 'en': {'title': 'FullDiT: Unifying Control for Advanced Video Generation', 'desc': 'This paper presents FullDiT, a novel video generative model that enhances control over video content creation by integrating multiple conditions effectively. Unlike existing adapter-based methods, FullDiT utilizes a unified full-attention mechanism to manage various input conditions without the issues of parameter redundancy and branch conflicts. The model captures the dynamics of conditions through a unified sequence representation, which improves scalability and performance. Additionally, the authors introduce FullBench, a new evaluation framework for assessing multi-task video generation, demonstrating that FullDiT achieves state-of-the-art results in this domain.'}, 'zh': {'title': '全注意力，视频生成的新突破', 'desc': '当前的视频生成基础模型主要集中在文本到视频的任务上，提供的细粒度视频内容创作控制有限。虽然基于适配器的方法（如ControlNet）可以在最小微调的情况下实现额外控制，但在整合多个条件时面临挑战，如独立训练的适配器之间的分支冲突、参数冗余导致的计算成本增加，以及与完全微调相比的性能不足。为了解决这些问题，我们提出了FullDiT，这是一种统一的视频生成基础模型，通过统一的全注意力机制无缝整合多个条件。实验表明，FullDiT在复杂的多任务视频生成中实现了最先进的结果，突显了全注意力在捕捉条件动态方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.18893', 'title': 'xKV: Cross-Layer SVD for KV-Cache Compression', 'url': 'https://huggingface.co/papers/2503.18893', 'abstract': "Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. We find that the dominant singular vectors are remarkably well-aligned across multiple layers of the KV-Cache. Exploiting this insight, we propose xKV, a simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into a shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates than state-of-the-art inter-layer technique while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding tasks without performance degradation. These results highlight xKV's strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Our code is publicly available at: https://github.com/abdelfattah-lab/xKV.", 'score': 3, 'issue_id': 2909, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'b933f4d58f9fb03a', 'authors': ['Chi-Chih Chang', 'Chien-Yu Lin', 'Yash Akhauri', 'Wei-Cheng Lin', 'Kai-Chiang Wu', 'Luis Ceze', 'Mohamed S. Abdelfattah'], 'affiliations': ['Cornell University', 'National Yang Ming Chiao Tung University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.18893.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#open_source', '#long_context', '#architecture', '#inference'], 'emoji': '🗜️', 'ru': {'title': 'xKV: Эффективное сжатие памяти для LLM с длинным контекстом', 'desc': 'Исследователи предложили метод xKV для сжатия памяти KV-Cache в больших языковых моделях (LLM) с длинным контекстным окном. Метод использует сингулярное разложение (SVD) для объединения KV-Cache нескольких слоев в общее низкоранговое подпространство. xKV достигает до 6.8-кратного сжатия по сравнению с современными методами, при этом улучшая точность на 2.7% на бенчмарке RULER. Метод также совместим с Multi-Head Latent Attention (MLA) и показывает 3-кратное сжатие на задачах кодирования без потери производительности.'}, 'en': {'title': 'xKV: Efficient Memory Management for Long-Context LLMs', 'desc': 'This paper introduces xKV, a method designed to reduce the memory usage of Key and Value states (KV-Cache) in Large Language Models (LLMs) with long context windows. By applying Singular Value Decomposition (SVD) to the KV-Cache of grouped layers, xKV consolidates these caches into a shared low-rank subspace, leading to significant size reductions. The method achieves up to 6.8 times higher compression rates compared to existing techniques while also improving model accuracy by 2.7%. Additionally, xKV is compatible with Multi-Head Latent Attention models, demonstrating its effectiveness in various coding tasks without sacrificing performance.'}, 'zh': {'title': 'xKV：高效压缩长上下文模型的关键技术', 'desc': '大型语言模型（LLMs）在处理长上下文时具有强大的应用能力，但需要消耗大量内存来存储键值缓存（KV-Cache）。最近的研究尝试将多个层的KV缓存合并为共享表示，但这些方法通常需要昂贵的预训练或依赖于层间高余弦相似性的假设，这在实际中并不成立。我们发现，KV缓存的主奇异向量在多个层之间对齐得非常好。基于这一发现，我们提出了xKV，这是一种简单的后训练方法，通过对分组层的KV缓存应用奇异值分解（SVD），显著减少了KV缓存的大小。'}}}, {'id': 'https://huggingface.co/papers/2503.17973', 'title': 'PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable\n  Objects from Videos', 'url': 'https://huggingface.co/papers/2503.17973', 'abstract': 'Creating a physical digital twin of a real-world object has immense potential in robotics, content creation, and XR. In this paper, we present PhysTwin, a novel framework that uses sparse videos of dynamic objects under interaction to produce a photo- and physically realistic, real-time interactive virtual replica. Our approach centers on two key components: (1) a physics-informed representation that combines spring-mass models for realistic physical simulation, generative shape models for geometry, and Gaussian splats for rendering; and (2) a novel multi-stage, optimization-based inverse modeling framework that reconstructs complete geometry, infers dense physical properties, and replicates realistic appearance from videos. Our method integrates an inverse physics framework with visual perception cues, enabling high-fidelity reconstruction even from partial, occluded, and limited viewpoints. PhysTwin supports modeling various deformable objects, including ropes, stuffed animals, cloth, and delivery packages. Experiments show that PhysTwin outperforms competing methods in reconstruction, rendering, future prediction, and simulation under novel interactions. We further demonstrate its applications in interactive real-time simulation and model-based robotic motion planning.', 'score': 3, 'issue_id': 2910, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '91f6aafdb1c387f0', 'authors': ['Hanxiao Jiang', 'Hao-Yu Hsu', 'Kaifeng Zhang', 'Hsin-Ni Yu', 'Shenlong Wang', 'Yunzhu Li'], 'affiliations': ['Columbia University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.17973.jpg', 'data': {'categories': ['#optimization', '#3d', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'PhysTwin: Создание интерактивных цифровых двойников из видео', 'desc': 'Статья представляет PhysTwin - новую систему для создания фото- и физически реалистичных цифровых двойников объектов на основе видео. Ключевые компоненты включают физически обоснованное представление, сочетающее модели пружинных масс, генеративные модели форм и гауссовы сплаты. Применяется многоэтапная оптимизационная система обратного моделирования для реконструкции геометрии, физических свойств и внешнего вида. PhysTwin превосходит конкурирующие методы в реконструкции, рендеринге и симуляции взаимодействий с объектами.'}, 'en': {'title': 'Realistic Digital Twins: Bridging the Physical and Virtual Worlds', 'desc': 'This paper introduces PhysTwin, a framework designed to create realistic digital twins of dynamic objects using sparse video data. It employs a physics-informed representation that integrates spring-mass models for physical simulation, generative shape models for geometry, and Gaussian splats for rendering. The framework utilizes a multi-stage optimization process to reconstruct object geometry, infer physical properties, and achieve realistic appearances from limited video inputs. PhysTwin excels in modeling various deformable objects and demonstrates superior performance in reconstruction and simulation tasks, making it valuable for robotics and interactive applications.'}, 'zh': {'title': '物理数字双胞胎：真实与虚拟的完美结合', 'desc': '本文介绍了一种名为PhysTwin的新框架，旨在创建真实物体的数字双胞胎。该框架利用稀疏视频捕捉动态物体的交互，生成逼真且可实时互动的虚拟复制品。PhysTwin结合了物理信息表示和多阶段优化逆建模框架，能够从视频中重建完整几何形状、推断物理属性并复制真实外观。实验结果表明，PhysTwin在重建、渲染和模拟方面优于其他方法，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.17361', 'title': 'Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation', 'url': 'https://huggingface.co/papers/2503.17361', 'abstract': 'Flow matching in the continuous simplex has emerged as a promising strategy for DNA sequence design, but struggles to scale to higher simplex dimensions required for peptide and protein generation. We introduce Gumbel-Softmax Flow and Score Matching, a generative framework on the simplex based on a novel Gumbel-Softmax interpolant with a time-dependent temperature. Using this interpolant, we introduce Gumbel-Softmax Flow Matching by deriving a parameterized velocity field that transports from smooth categorical distributions to distributions concentrated at a single vertex of the simplex. We alternatively present Gumbel-Softmax Score Matching which learns to regress the gradient of the probability density. Our framework enables high-quality, diverse generation and scales efficiently to higher-dimensional simplices. To enable training-free guidance, we propose Straight-Through Guided Flows (STGFlow), a classifier-based guidance method that leverages straight-through estimators to steer the unconditional velocity field toward optimal vertices of the simplex. STGFlow enables efficient inference-time guidance using classifiers pre-trained on clean sequences, and can be used with any discrete flow method. Together, these components form a robust framework for controllable de novo sequence generation. We demonstrate state-of-the-art performance in conditional DNA promoter design, sequence-only protein generation, and target-binding peptide design for rare disease treatment.', 'score': 3, 'issue_id': 2896, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'b5389a3e5ab241c3', 'authors': ['Sophia Tang', 'Yinuo Zhang', 'Alexander Tong', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Biostatistics and Bioinformatics, Duke University', 'Department of Computer Science, Duke University', 'Management and Technology Program, University of Pennsylvania', 'Mila, Quebec AI Institute', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2503.17361.jpg', 'data': {'categories': ['#diffusion', '#data', '#optimization', '#training', '#architecture', '#healthcare'], 'emoji': '🧬', 'ru': {'title': 'Новый подход к генерации биологических последовательностей на симплексе', 'desc': 'Статья представляет новый метод генеративного моделирования на симплексе, называемый Gumbel-Softmax Flow and Score Matching. Авторы вводят новый интерполянт Gumbel-Softmax с зависящей от времени температурой и используют его для создания параметризованного поля скоростей. Метод позволяет получать высококачественные и разнообразные результаты, эффективно масштабируясь на симплексы высокой размерности. Также предложен метод Straight-Through Guided Flows для управления генерацией без дополнительного обучения.'}, 'en': {'title': 'Revolutionizing Sequence Generation with Gumbel-Softmax Flows', 'desc': 'This paper presents a new method called Gumbel-Softmax Flow and Score Matching for generating DNA sequences, peptides, and proteins. It introduces a Gumbel-Softmax interpolant that helps in transitioning between smooth categorical distributions and specific points in a simplex, which is a mathematical space used for these types of data. The authors also propose a technique called Straight-Through Guided Flows (STGFlow) that uses classifiers to guide the generation process towards optimal outcomes without needing extensive training. Overall, this framework allows for efficient and high-quality generation of biological sequences, achieving impressive results in various applications.'}, 'zh': {'title': '高效生成高维序列的创新框架', 'desc': '本文提出了一种新的生成框架，称为Gumbel-Softmax流和评分匹配，旨在解决DNA序列设计中的高维简单形问题。通过引入时间依赖的Gumbel-Softmax插值，我们能够在简单形上实现高质量和多样化的生成。该框架还包括一种名为STGFlow的分类器引导方法，能够在推理时有效地引导生成过程。我们的研究在条件DNA启动子设计、序列生成的蛋白质和靶向结合肽的设计中展示了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.17237', 'title': 'Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID', 'url': 'https://huggingface.co/papers/2503.17237', 'abstract': 'Detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video is inherently challenging due to low contrast, environmental noise, and small target sizes. This paper provides a straightforward approach to address multi-UAV tracking in thermal infrared video, leveraging recent advances in detection and tracking. Instead of relying on the YOLOv5 with the DeepSORT pipeline, we present a tracking framework built on YOLOv12 and BoT-SORT, enhanced with tailored training and inference strategies. We evaluate our approach following the metrics from the 4th Anti-UAV Challenge and demonstrate competitive performance. Notably, we achieve strong results without using contrast enhancement or temporal information fusion to enrich UAV features, highlighting our approach as a "Strong Baseline" for the multi-UAV tracking task. We provide implementation details, in-depth experimental analysis, and a discussion of potential improvements. The code is available at https://github.com/wish44165/YOLOv12-BoT-SORT-ReID .', 'score': 3, 'issue_id': 2900, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '03b184a7ba5377e7', 'authors': ['Yu-Hsi Chen'], 'affiliations': ['The University of Melbourne, Parkville, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2503.17237.jpg', 'data': {'categories': ['#video', '#cv', '#training', '#benchmark'], 'emoji': '🚁', 'ru': {'title': 'Эффективное отслеживание БПЛА на тепловизионном видео без дополнительной обработки', 'desc': 'Статья представляет новый подход к отслеживанию множественных БПЛА на инфракрасном видео с использованием YOLOv12 и BoT-SORT. Авторы разработали фреймворк, который превосходит стандартную связку YOLOv5 и DeepSORT, не прибегая к улучшению контраста или слиянию временной информации. Метод показал конкурентоспособные результаты на метриках 4-го Anti-UAV Challenge. Исследователи предоставляют подробности реализации, экспериментальный анализ и обсуждение возможных улучшений.'}, 'en': {'title': 'Revolutionizing UAV Tracking with YOLOv12 and BoT-SORT', 'desc': 'This paper addresses the challenge of detecting and tracking multiple unmanned aerial vehicles (UAVs) in thermal infrared video, which is difficult due to low visibility and small sizes of the targets. The authors propose a new tracking framework that utilizes YOLOv12 and BoT-SORT, moving away from the traditional YOLOv5 and DeepSORT methods. Their approach includes specific training and inference strategies that enhance performance without relying on contrast enhancement or temporal information. The results show that their method performs competitively in the 4th Anti-UAV Challenge, establishing it as a strong baseline for future multi-UAV tracking research.'}, 'zh': {'title': '热红外视频中的多无人机跟踪新方法', 'desc': '本论文针对热红外视频中多无人机（UAV）的检测与跟踪问题，提出了一种简单有效的方法。我们采用了YOLOv12和BoT-SORT构建跟踪框架，并通过定制的训练和推理策略进行增强。我们的评估基于第四届反无人机挑战赛的指标，结果显示出竞争力的性能。值得注意的是，我们在不使用对比度增强或时间信息融合的情况下，依然取得了良好的结果，标志着我们的方法是多无人机跟踪任务的“强基线”。'}}}, {'id': 'https://huggingface.co/papers/2503.16965', 'title': 'When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only\n  Training For Human-Centered Decision Making', 'url': 'https://huggingface.co/papers/2503.16965', 'abstract': "Embodied decision-making is fundamental for AI agents operating in real-world environments. While Visual Language Models (VLMs) have advanced this capability, they still struggle with complex decisions, particularly in human-centered situations that require deep reasoning about human needs and values. In this study, we systematically evaluate open-sourced VLMs on multimodal human-centered decision-making tasks. We find that LLMs receiving only textual descriptions unexpectedly outperform their VLM counterparts of similar scale that process actual images, suggesting that visual alignment may hinder VLM abilities. To address this challenge, we propose a novel text-only training approach with synthesized textual data. This method strengthens VLMs' language components and transfers the learned abilities to multimodal inference, eliminating the need for expensive image-text paired data. Furthermore, we show that VLMs can achieve substantial performance gains through self-improvement, using training data generated by their LLM counterparts rather than relying on larger teacher models like GPT-4. Our findings establish a more efficient and scalable approach to enhancing VLMs' human-centered decision-making capabilities, opening new avenues for optimizing VLMs through self-improvement mechanisms.", 'score': 3, 'issue_id': 2896, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'ca2e599ff0665dfe', 'authors': ['Zhe Hu', 'Jing Li', 'Yu Yin'], 'affiliations': ['Department of Computer and Data Sciences, Case Western Reserve University', 'Department of Computing, The Hong Kong Polytechnic University', 'Research Centre for Data Science & Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2503.16965.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training', '#agents', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Самосовершенствование VLM через текстовое обучение', 'desc': 'Это исследование сосредоточено на оценке визуальных языковых моделей (VLM) в задачах принятия решений, ориентированных на человека. Авторы обнаружили, что языковые модели, работающие только с текстом, превосходят VLM аналогичного масштаба, обрабатывающие изображения. Для решения этой проблемы предложен новый подход к обучению, использующий синтезированные текстовые данные. Исследование также демонстрирует, что VLM могут значительно улучшить свою производительность через самосовершенствование, используя данные, сгенерированные их LLM-аналогами.'}, 'en': {'title': 'Enhancing VLMs through Text-Only Training and Self-Improvement', 'desc': 'This paper explores the challenges faced by Visual Language Models (VLMs) in making complex decisions that involve understanding human needs and values. The authors find that VLMs that rely on visual data often perform worse than those using only text, indicating that visual information may complicate decision-making. To improve VLM performance, they propose a new training method that uses synthesized textual data, enhancing the language understanding of VLMs without needing paired image-text data. Additionally, the study demonstrates that VLMs can improve their decision-making abilities by learning from their own generated data, rather than depending on larger models, making the training process more efficient and scalable.'}, 'zh': {'title': '提升VLM人类中心决策能力的新方法', 'desc': '本研究探讨了视觉语言模型（VLMs）在复杂人类中心决策中的表现。我们发现，仅使用文本描述的语言模型（LLMs）在某些任务上意外地超越了处理图像的VLMs，这表明视觉对齐可能会限制VLM的能力。为了解决这个问题，我们提出了一种新的仅基于文本的训练方法，利用合成文本数据来增强VLM的语言能力。我们的研究结果表明，通过自我改进，VLMs可以显著提升其人类中心决策能力，开辟了优化VLM的新途径。'}}}, {'id': 'https://huggingface.co/papers/2503.19207', 'title': 'FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images', 'url': 'https://huggingface.co/papers/2503.19207', 'abstract': 'We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA.', 'score': 2, 'issue_id': 2907, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '051b6e95d11816f7', 'authors': ['Rong Wang', 'Fabian Prada', 'Ziyan Wang', 'Zhongshi Jiang', 'Chengxiang Yin', 'Junxuan Li', 'Shunsuke Saito', 'Igor Santesteban', 'Javier Romero', 'Rohan Joshi', 'Hongdong Li', 'Jason Saragih', 'Yaser Sheikh'], 'affiliations': ['Australian National University', 'Meta Reality Labs Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.19207.jpg', 'data': {'categories': ['#3d'], 'emoji': '🧍', 'ru': {'title': 'Мгновенное создание реалистичных 3D-аватаров по нескольким фотографиям', 'desc': 'В статье представлен новый метод создания персонализированных 3D-аватаров людей с реалистичной анимацией на основе всего нескольких изображений. Авторы обучили универсальную модель на тысяче примеров одетых людей, что позволяет мгновенно генерировать аватары без дополнительной оптимизации. Метод совместно оценивает форму аватара, веса скиннинга и зависящие от позы деформации, что улучшает геометрическую точность. Предложенный процесс 3D-канонизации и агрегации признаков помогает восстанавливать мелкие детали и сохранять индивидуальность человека.'}, 'en': {'title': 'Instant 3D Avatars from Few Images!', 'desc': "This paper introduces a new technique for creating personalized 3D human avatars with realistic animations using just a few images. Unlike previous methods that require extensive optimization for each individual, this approach leverages a universal model learned from a large dataset of clothed humans, allowing for quick and efficient avatar generation. The method improves the accuracy of the avatar's shape and movement by jointly inferring skinning weights and pose-dependent deformations, which minimizes visual artifacts. Additionally, a 3D canonicalization process is implemented to align poses and enhance geometric details, resulting in high-quality reconstructions that can be generated from casual photos."}, 'zh': {'title': '个性化3D头像重建的新方法', 'desc': '我们提出了一种新方法，可以仅通过几张图片重建个性化的3D人类头像，并实现逼真的动画。现有方法通常需要在推理过程中对每个对象进行数小时的优化，这限制了它们的实际应用。我们的技术通过学习来自一千多名穿衣人类的通用先验，实现了即时前馈生成和零样本泛化。我们设计了一种3D标准化过程，以解决姿势变化和形状之间的模糊性，从而提高几何精度并减少变形伪影。'}}}, {'id': 'https://huggingface.co/papers/2503.19123', 'title': 'Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided\n  Language Modeling', 'url': 'https://huggingface.co/papers/2503.19123', 'abstract': 'Using large teacher models to guide the training of smaller student models has become the prevailing paradigm for efficient and effective learning. However, vocabulary mismatches between teacher and student language models pose significant challenges in language modeling, resulting in divergent token sequences and output distributions. To overcome these limitations, we propose Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel approach that bridges the gap caused by vocabulary mismatch through two key methods: (1) Token-level Lexical Alignment, which aligns token sequences across mismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss of teacher model to guide effective student training. We demonstrate its effectiveness in language modeling with 1B student model using various 7B teacher models with different vocabularies. Notably, with Qwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary with TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to naive continual pretraining. Furthermore, we demonstrate that VocAgnoLM consistently benefits from stronger teacher models, providing a robust solution to vocabulary mismatches in language modeling.', 'score': 2, 'issue_id': 2907, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'b5efb0f50380f253', 'authors': ['Haebin Shin', 'Lei Ji', 'Xiao Liu', 'Yeyun Gong'], 'affiliations': ['KAIST AI', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.19123.jpg', 'data': {'categories': ['#multilingual', '#optimization', '#transfer_learning', '#small_models', '#training'], 'emoji': '🔠', 'ru': {'title': 'Преодоление языкового барьера в обучении языковых моделей', 'desc': 'Статья представляет новый подход к обучению языковых моделей под названием VocAgnoLM. Этот метод решает проблему несоответствия словарей между учительской и ученической моделями с помощью лексического выравнивания токенов и использования функции потерь учительской модели. VocAgnoLM показывает значительное улучшение производительности по сравнению с обычным дообучением, особенно когда словари моделей сильно различаются. Авторы демонстрируют, что их подход позволяет эффективно использовать более сильные учительские модели для обучения меньших ученических моделей.'}, 'en': {'title': 'Bridging Vocabulary Gaps for Better Language Learning', 'desc': "This paper introduces a new method called Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM) to improve the training of smaller language models using larger teacher models. It addresses the problem of vocabulary mismatches that can lead to different token sequences and output distributions between the teacher and student models. VocAgnoLM employs two main techniques: Token-level Lexical Alignment to synchronize token sequences and Teacher Guided Loss to utilize the teacher's loss for better student training. The results show that this approach significantly enhances performance, achieving a 46% improvement in language modeling tasks, especially when using stronger teacher models."}, 'zh': {'title': '打破词汇壁垒，提升语言建模效率', 'desc': '本文提出了一种新的语言建模方法，称为Vocabulary-agnostic Teacher Guided Language Modeling（VocAgnoLM），旨在解决教师模型与学生模型之间的词汇不匹配问题。该方法通过两种关键技术实现：一是令牌级词汇对齐，二是教师引导损失，帮助学生模型更有效地学习。实验表明，VocAgnoLM在使用不同词汇的教师模型时，能够显著提高学生模型的性能，尤其是在词汇重叠较少的情况下。该方法为语言建模中的词汇不匹配问题提供了有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.18783', 'title': 'Frequency Dynamic Convolution for Dense Image Prediction', 'url': 'https://huggingface.co/papers/2503.18783', 'abstract': '', 'score': 2, 'issue_id': 2897, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '0be08263b46c092e', 'authors': ['Linwei Chen', 'Lin Gu', 'Liang Li', 'Chenggang Yan', 'Ying Fu'], 'affiliations': ['Beijing Institute of Technology', 'Chinese Academy of Sciences', 'Hangzhou Dianzi University', 'RIKEN', 'The University of Tokyo', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18783.jpg', 'data': {'categories': [], 'emoji': '🤖', 'ru': {'title': 'Эффективное обучение больших языковых моделей', 'desc': 'В статье рассматривается новый подход к обучению больших языковых моделей (LLM), который позволяет значительно сократить время и ресурсы, необходимые для их тренировки. Авторы предлагают использовать метод оптимизации, который адаптируется к особенностям данных, что повышает эффективность обучения. Эксперименты показывают, что предложенный метод позволяет достичь более высоких результатов на стандартных тестах. Это открывает новые возможности для применения LLM в различных областях, таких как обработка естественного языка и генерация текста.'}, 'en': {'title': 'Hybrid Models: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': '提升预测准确性的创新算法', 'desc': '这篇论文探讨了一种新的机器学习算法，旨在提高模型的预测准确性。作者提出了一种改进的特征选择方法，可以有效减少数据维度，同时保留重要信息。实验结果表明，该算法在多个数据集上表现优于传统方法。通过优化模型的训练过程，研究者希望推动机器学习在实际应用中的效果。'}}}, {'id': 'https://huggingface.co/papers/2503.11849', 'title': 'Towards a Unified Copernicus Foundation Model for Earth Vision', 'url': 'https://huggingface.co/papers/2503.11849', 'abstract': "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https://github.com/zhu-xlab/Copernicus-FM.", 'score': 2, 'issue_id': 2904, 'pub_date': '2025-03-14', 'pub_date_card': {'ru': '14 марта', 'en': 'March 14', 'zh': '3月14日'}, 'hash': '73a149c0c20956cf', 'authors': ['Yi Wang', 'Zhitong Xiong', 'Chenying Liu', 'Adam J. Stewart', 'Thomas Dujardin', 'Nikolaos Ioannis Bountos', 'Angelos Zavras', 'Franziska Gerken', 'Ioannis Papoutsis', 'Laura Leal-Taixé', 'Xiao Xiang Zhu'], 'affiliations': ['Harokopio University of Athens', 'Munich Center for Machine Learning', 'NVIDIA', 'National Technical University of Athens & National Observatory of Athens', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2503.11849.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#cv', '#dataset'], 'emoji': '🛰️', 'ru': {'title': 'Универсальная модель машинного обучения для комплексного анализа спутниковых данных', 'desc': 'Статья представляет новый подход к созданию фундаментальных моделей для наблюдения Земли. Авторы разработали Copernicus-FM - унифицированную модель, способную обрабатывать различные сенсорные модальности с использованием расширенных динамических гиперсетей. Для обучения модели был создан массивный набор данных Copernicus-Pretrain, включающий 18,7 миллионов изображений со спутников миссии Copernicus Sentinel. Также представлен бенчмарк Copernicus-Bench для оценки модели на 15 иерархических задачах.'}, 'en': {'title': 'Unlocking Earth Observation with Advanced Foundation Models', 'desc': "This paper presents advancements in Earth observation (EO) foundation models that utilize large satellite datasets to enhance learning from space imagery. The authors introduce Copernicus-Pretrain, a comprehensive dataset with 18.7 million aligned images from various Copernicus Sentinel missions, which includes data from both the Earth's surface and atmosphere. They also propose Copernicus-FM, a versatile foundation model that can handle different types of sensor data and incorporates metadata for improved analysis. Finally, the paper outlines Copernicus-Bench, a benchmark for evaluating the model's performance across 15 diverse tasks, thereby enhancing the scalability and adaptability of EO applications."}, 'zh': {'title': '地球观测基础模型的未来：多模态与可扩展性', 'desc': '本论文介绍了地球观测基础模型的进展，利用大规模卫星数据学习通用表示，促进了多种重要应用的发展。我们提出了三个关键组件：Copernicus-Pretrain，一个包含1870万对齐图像的大规模预训练数据集，涵盖地球表面到大气层的所有主要Copernicus Sentinel任务；Copernicus-FM，一个统一的基础模型，能够处理任何光谱或非光谱传感器的模态，并使用扩展的动态超网络和灵活的元数据编码；以及Copernicus-Bench，一个系统的评估基准，包含15个层次的下游任务，从预处理到每个Sentinel任务的专业应用。我们的工作显著提高了地球观测基础模型的可扩展性、多功能性和多模态适应性，同时为连接地球观测、天气和气候研究创造了新的机会。'}}}, {'id': 'https://huggingface.co/papers/2503.19777', 'title': 'LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary\n  Semantic Segmentation', 'url': 'https://huggingface.co/papers/2503.19777', 'abstract': 'We propose a training-free method for open-vocabulary semantic segmentation using Vision-and-Language Models (VLMs). Our approach enhances the initial per-patch predictions of VLMs through label propagation, which jointly optimizes predictions by incorporating patch-to-patch relationships. Since VLMs are primarily optimized for cross-modal alignment and not for intra-modal similarity, we use a Vision Model (VM) that is observed to better capture these relationships. We address resolution limitations inherent to patch-based encoders by applying label propagation at the pixel level as a refinement step, significantly improving segmentation accuracy near class boundaries. Our method, called LPOSS+, performs inference over the entire image, avoiding window-based processing and thereby capturing contextual interactions across the full image. LPOSS+ achieves state-of-the-art performance among training-free methods, across a diverse set of datasets. Code: https://github.com/vladan-stojnic/LPOSS', 'score': 1, 'issue_id': 2905, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'adee7b684f0c25f1', 'authors': ['Vladan Stojnić', 'Yannis Kalantidis', 'Jiří Matas', 'Giorgos Tolias'], 'affiliations': ['NAVER LABS Europe', 'VRG, FEE, Czech Technical University in Prague'], 'pdf_title_img': 'assets/pdf/title_img/2503.19777.jpg', 'data': {'categories': ['#open_source', '#inference', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение сегментации изображений без обучения с помощью распространения меток', 'desc': 'В статье предлагается метод семантической сегментации изображений с открытым словарем без обучения, использующий модели зрения и языка (VLM). Метод улучшает начальные попиксельные предсказания VLM с помощью распространения меток, оптимизируя предсказания с учетом отношений между патчами. Для захвата внутримодальных сходств используется отдельная модель зрения (VM). Применение распространения меток на уровне пикселей позволяет преодолеть ограничения разрешения, присущие кодировщикам на основе патчей.'}, 'en': {'title': 'Revolutionizing Semantic Segmentation with Training-Free Label Propagation', 'desc': 'This paper introduces a novel method for open-vocabulary semantic segmentation that does not require training. The method leverages Vision-and-Language Models (VLMs) and enhances their predictions by using label propagation to optimize relationships between image patches. To improve accuracy, especially near class boundaries, the authors apply label propagation at the pixel level, addressing the limitations of patch-based encoders. The proposed method, LPOSS+, outperforms existing training-free approaches and effectively captures contextual interactions across the entire image.'}, 'zh': {'title': '无训练的开放词汇语义分割新方法', 'desc': '我们提出了一种无训练的开放词汇语义分割方法，利用视觉与语言模型（VLMs）。该方法通过标签传播增强了VLMs的初始每个补丁预测，优化了补丁之间的关系。我们使用视觉模型（VM）来更好地捕捉这些关系，并在像素级别应用标签传播，以解决基于补丁编码器的分辨率限制，从而显著提高了类边界附近的分割精度。我们的LPOSS+方法在整个图像上进行推理，避免了基于窗口的处理，能够捕捉全图的上下文交互，并在无训练方法中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.19356', 'title': 'Can Vision-Language Models Answer Face to Face Questions in the\n  Real-World?', 'url': 'https://huggingface.co/papers/2503.19356', 'abstract': 'AI models have made significant strides in recent years in their ability to describe and answer questions about real-world images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question: have we reached the point where AI models, connected to a camera and microphone, can converse with users in real-time about scenes and events that are unfolding live in front of the camera? This has been a long-standing goal in AI and is a prerequisite for real-world AI assistants and humanoid robots to interact with humans in everyday situations. In this work, we introduce a new dataset and benchmark, the Qualcomm Interactive Video Dataset (IVD), which allows us to assess the extent to which existing models can support these abilities, and to what degree these capabilities can be instilled through fine-tuning. The dataset is based on a simple question-answering setup, where users ask questions that the system has to answer, in real-time, based on the camera and audio input. We show that existing models fall far behind human performance on this task, and we identify the main sources for the performance gap. However, we also show that for many of the required perceptual skills, fine-tuning on this form of data can significantly reduce this gap.', 'score': 1, 'issue_id': 2909, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': '139d332c36986584', 'authors': ['Reza Pourreza', 'Rishit Dagli', 'Apratim Bhattacharyya', 'Sunny Panchal', 'Guillaume Berger', 'Roland Memisevic'], 'affiliations': ['Qualcomm AI Research', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.19356.jpg', 'data': {'categories': ['#benchmark', '#agi', '#multimodal', '#optimization', '#cv', '#dataset', '#training', '#audio', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'На пути к ИИ, понимающему мир в реальном времени', 'desc': 'Статья представляет новый набор данных и эталонный тест - Qualcomm Interactive Video Dataset (IVD), для оценки способности моделей искусственного интеллекта (ИИ) взаимодействовать с пользователями в реальном времени на основе видео- и аудиовхода. Исследование показывает, что существующие модели значительно отстают от человеческой производительности в этой задаче. Авторы выявляют основные причины разрыва в производительности между ИИ и человеком. Тем не менее, исследование демонстрирует, что дообучение на подобных данных может существенно сократить этот разрыв для многих необходимых перцептивных навыков.'}, 'en': {'title': 'Bridging the Gap: Real-Time AI Conversations with Live Input', 'desc': 'This paper discusses advancements in AI models that can describe and answer questions about real-world images and engage in real-time conversations using audio. The authors introduce the Qualcomm Interactive Video Dataset (IVD), which serves as a benchmark to evaluate how well current models can interact with users based on live camera and audio input. The study reveals that while existing models do not yet match human performance in this interactive task, fine-tuning these models on the new dataset can improve their perceptual skills significantly. Ultimately, this work highlights the potential for developing AI assistants and humanoid robots that can effectively communicate in everyday situations.'}, 'zh': {'title': '迈向实时对话的人工智能新阶段', 'desc': '近年来，人工智能模型在描述和回答现实世界图像方面取得了显著进展。它们在实时与用户进行音频对话的能力上也有所提升。本文提出了一个新的数据集和基准，名为高通互动视频数据集（IVD），用于评估现有模型在实时场景和事件对话中的能力。研究表明，尽管现有模型在此任务上的表现远低于人类，但通过微调可以显著缩小这一差距。'}}}, {'id': 'https://huggingface.co/papers/2503.19355', 'title': 'ST-VLM: Kinematic Instruction Tuning for Spatio-Temporal Reasoning in\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2503.19355', 'abstract': 'Spatio-temporal reasoning is essential in understanding real-world environments in various fields, eg, autonomous driving and sports analytics. Recent advances have improved the spatial reasoning ability of Vision-Language Models (VLMs) by introducing large-scale data, but these models still struggle to analyze kinematic elements like traveled distance and speed of moving objects. To bridge this gap, we construct a spatio-temporal reasoning dataset and benchmark involving kinematic instruction tuning, referred to as STKit and STKit-Bench. They consist of real-world videos with 3D annotations, detailing object motion dynamics: traveled distance, speed, movement direction, inter-object distance comparisons, and relative movement direction. To further scale such data construction to videos without 3D labels, we propose an automatic pipeline to generate pseudo-labels using 4D reconstruction in real-world scale. With our kinematic instruction tuning data for spatio-temporal reasoning, we present ST-VLM, a VLM enhanced for spatio-temporal reasoning, which exhibits outstanding performance on STKit-Bench. Furthermore, we show that ST-VLM generalizes robustly across diverse domains and tasks, outperforming baselines on other spatio-temporal benchmarks (eg, ActivityNet, TVQA+). Finally, by integrating learned spatio-temporal reasoning with existing abilities, ST-VLM enables complex multi-step reasoning. Project page: https://ikodoh.github.io/ST-VLM.', 'score': 1, 'issue_id': 2915, 'pub_date': '2025-03-25', 'pub_date_card': {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'}, 'hash': 'de2ed078954ba830', 'authors': ['Dohwan Ko', 'Sihyeon Kim', 'Yumin Suh', 'Vijay Kumar B. G', 'Minseo Yoon', 'Manmohan Chandraker', 'Hyunwoo J. Kim'], 'affiliations': ['KAIST', 'Korea University', 'NEC Labs America'], 'pdf_title_img': 'assets/pdf/title_img/2503.19355.jpg', 'data': {'categories': ['#benchmark', '#cv', '#multimodal', '#dataset', '#transfer_learning', '#reasoning'], 'emoji': '🎥', 'ru': {'title': 'Улучшение пространственно-временных рассуждений в мультимодальных моделях', 'desc': 'Статья представляет новый датасет и бенчмарк STKit и STKit-Bench для улучшения пространственно-временных рассуждений в моделях компьютерного зрения и обработки естественного языка. Авторы предлагают ST-VLM - улучшенную мультимодальную модель, обученную на кинематических инструкциях для анализа движения объектов в видео. ST-VLM демонстрирует высокую производительность на STKit-Bench и других задачах пространственно-временного анализа. Модель позволяет проводить сложные многоэтапные рассуждения, объединяя пространственно-временной анализ с другими способностями.'}, 'en': {'title': 'Enhancing Vision-Language Models with Spatio-Temporal Reasoning', 'desc': "This paper addresses the challenge of spatio-temporal reasoning in Vision-Language Models (VLMs), particularly in understanding the movement of objects in real-world scenarios. The authors introduce a new dataset, STKit, which includes videos with detailed 3D annotations of kinematic elements such as distance traveled and speed. They also propose an automatic pipeline for generating pseudo-labels from videos lacking 3D data, enhancing the dataset's scalability. The resulting model, ST-VLM, demonstrates superior performance in spatio-temporal reasoning tasks and shows strong generalization across various benchmarks."}, 'zh': {'title': '提升时空推理能力的视觉语言模型', 'desc': '时空推理在理解现实世界环境中至关重要，尤其是在自动驾驶和体育分析等领域。尽管视觉语言模型（VLMs）在空间推理能力上取得了进展，但在分析运动物体的运动学元素（如行驶距离和速度）方面仍然存在困难。为了解决这个问题，我们构建了一个时空推理数据集和基准，称为STKit和STKit-Bench，包含带有3D注释的真实视频，详细描述了物体运动动态。通过引入自动化管道生成伪标签，我们提出的ST-VLM模型在时空推理任务中表现出色，并在多个基准测试中超越了其他模型。'}}}, {'id': 'https://huggingface.co/papers/2503.18712', 'title': 'LLaVAction: evaluating and training multi-modal large language models\n  for action recognition', 'url': 'https://huggingface.co/papers/2503.18712', 'abstract': "Understanding human behavior requires measuring behavioral actions. Due to its complexity, behavior is best mapped onto a rich, semantic structure such as language. The recent development of multi-modal large language models (MLLMs) is a promising candidate for a wide range of action understanding tasks. In this work, we focus on evaluating and then improving MLLMs to perform action recognition. We reformulate EPIC-KITCHENS-100, one of the largest and most challenging egocentric action datasets, to the form of video multiple question answering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult incorrect answers as distractors, leading MLLMs struggle to recognize the correct actions. We propose a series of methods that greatly improve the MLLMs' ability to perform action recognition, achieving state-of-the-art on both the EPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points in accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other action-related video benchmarks such as EgoSchema, PerceptionTest, LongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising path forward for complex action tasks. Code and models are available at: https://github.com/AdaptiveMotorControlLab/LLaVAction.", 'score': 1, 'issue_id': 2916, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '05cb55b99d04c44e', 'authors': ['Shaokai Ye', 'Haozhe Qi', 'Alexander Mathis', 'Mackenzie W. Mathis'], 'affiliations': ['EPFL'], 'pdf_title_img': 'assets/pdf/title_img/2503.18712.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#optimization', '#training', '#video', '#benchmark', '#games'], 'emoji': '🎬', 'ru': {'title': 'MLLM: новый уровень понимания человеческих действий в видео', 'desc': 'Статья посвящена улучшению мультимодальных больших языковых моделей (MLLM) для распознавания действий в видео. Авторы переформулировали датасет EPIC-KITCHENS-100 в формат множественных вопросов и ответов, чтобы оценить способность MLLM распознавать действия. Они предложили ряд методов, значительно улучшающих эту способность, достигнув наилучших результатов на валидационном наборе EPIC-KITCHENS-100. Исследование также показало улучшения на других бенчмарках, связанных с действиями в видео, что указывает на перспективность MLLM для сложных задач распознавания действий.'}, 'en': {'title': 'Unlocking Action Recognition with Multi-Modal Language Models', 'desc': 'This paper explores the use of multi-modal large language models (MLLMs) for understanding human actions through video data. The authors reformulate the EPIC-KITCHENS-100 dataset into a video multiple question answering format to enhance action recognition capabilities. They identify that MLLMs struggle with difficult distractor answers, which impacts their performance. By implementing various improvement methods, the authors achieve state-of-the-art results in action recognition tasks, demonstrating the potential of MLLMs for complex behavioral understanding.'}, 'zh': {'title': '多模态语言模型助力动作识别', 'desc': '理解人类行为需要测量行为动作。由于行为的复杂性，最好将其映射到丰富的语义结构上，例如语言。本文专注于评估和改进多模态大型语言模型（MLLMs）在动作识别方面的表现。我们提出了一系列方法，显著提高了MLLMs的动作识别能力，并在多个基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2503.17982', 'title': 'Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on\n  Aerial Images', 'url': 'https://huggingface.co/papers/2503.17982', 'abstract': 'Understanding the geometric and semantic properties of the scene is crucial in autonomous navigation and particularly challenging in the case of Unmanned Aerial Vehicle (UAV) navigation. Such information may be by obtained by estimating depth and semantic segmentation maps of the surrounding environment and for their practical use in autonomous navigation, the procedure must be performed as close to real-time as possible. In this paper, we leverage monocular cameras on aerial robots to predict depth and semantic maps in low-altitude unstructured environments. We propose a joint deep-learning architecture that can perform the two tasks accurately and rapidly, and validate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our joint-architecture proves to be competitive or superior to the other single and joint architecture methods while performing its task fast predicting 20.2 FPS on a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All codes for training and prediction can be found on this link: https://github.com/Malga-Vision/Co-SemDepth', 'score': 0, 'issue_id': 2911, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '27d4e625a8566ad0', 'authors': ['Yara AlaaEldin', 'Francesca Odone'], 'affiliations': ['University of Genova Genova, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2503.17982.jpg', 'data': {'categories': ['#open_source', '#optimization', '#agents', '#benchmark', '#architecture', '#cv'], 'emoji': '🚁', 'ru': {'title': 'Быстрый и точный анализ сцены для автономной навигации БПЛА', 'desc': 'Статья представляет архитектуру глубокого обучения для совместного предсказания глубины и семантической сегментации для беспилотных летательных аппаратов. Предложенный метод использует монокулярные камеры для анализа окружающей среды на малых высотах. Архитектура показывает конкурентоспособные или превосходящие результаты по сравнению с другими методами на наборах данных MidAir и Aeroscapes. Модель работает быстро (20.2 FPS на NVIDIA Quadro P5000) и имеет небольшой объем памяти.'}, 'en': {'title': 'Real-Time Depth and Semantic Mapping for UAVs', 'desc': 'This paper addresses the challenges of understanding the environment for Unmanned Aerial Vehicle (UAV) navigation by estimating depth and semantic segmentation maps. The authors propose a joint deep-learning architecture that efficiently predicts these maps using monocular cameras in real-time. Their model achieves a high performance of 20.2 frames per second on a single NVIDIA Quadro P5000 GPU while maintaining a low memory footprint. The effectiveness of the proposed architecture is validated against benchmark datasets, showing competitive or superior results compared to existing methods.'}, 'zh': {'title': '无人机导航中的深度与语义分割联合学习', 'desc': '本论文探讨了无人机导航中场景的几何和语义特性。我们利用单目相机在低空非结构化环境中预测深度和语义分割图。提出了一种联合深度学习架构，能够快速且准确地完成这两个任务，并在MidAir和Aeroscapes基准数据集上验证了其有效性。该架构在性能上优于其他单一和联合架构方法，能够以20.2帧每秒的速度进行预测，同时内存占用较低。'}}}, {'id': 'https://huggingface.co/papers/2503.16776', 'title': 'OpenCity3D: What do Vision-Language Models know about Urban\n  Environments?', 'url': 'https://huggingface.co/papers/2503.16776', 'abstract': "Vision-language models (VLMs) show great promise for 3D scene understanding but are mainly applied to indoor spaces or autonomous driving, focusing on low-level tasks like segmentation. This work expands their use to urban-scale environments by leveraging 3D reconstructions from multi-view aerial imagery. We propose OpenCity3D, an approach that addresses high-level tasks, such as population density estimation, building age classification, property price prediction, crime rate assessment, and noise pollution evaluation. Our findings highlight OpenCity3D's impressive zero-shot and few-shot capabilities, showcasing adaptability to new contexts. This research establishes a new paradigm for language-driven urban analytics, enabling applications in planning, policy, and environmental monitoring. See our project page: opencity3d.github.io", 'score': 0, 'issue_id': 2916, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'c5625062d93c0379', 'authors': ['Valentin Bieri', 'Marco Zamboni', 'Nicolas S. Blumer', 'Qingxuan Chen', 'Francis Engelmann'], 'affiliations': ['ETH Zurich', 'Stanford University', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.16776.jpg', 'data': {'categories': ['#science', '#transfer_learning', '#multimodal', '#3d'], 'emoji': '🏙️', 'ru': {'title': 'OpenCity3D: языковые модели на страже умных городов', 'desc': 'Эта статья представляет OpenCity3D - подход к использованию мультимодальных языковых моделей (VLM) для анализа городской среды на основе 3D-реконструкций аэрофотоснимков. Авторы демонстрируют, как VLM можно применять для решения высокоуровневых задач в урбанистике, таких как оценка плотности населения, классификация возраста зданий и прогнозирование цен на недвижимость. Результаты показывают впечатляющие возможности OpenCity3D в режимах zero-shot и few-shot обучения. Это исследование открывает новую парадигму для языкового городского анализа с приложениями в градостроительстве и экологическом мониторинге.'}, 'en': {'title': 'Revolutionizing Urban Analytics with OpenCity3D', 'desc': 'This paper introduces OpenCity3D, a vision-language model designed for understanding urban environments using 3D reconstructions from aerial images. Unlike previous models that focused on low-level tasks, OpenCity3D tackles high-level urban analytics such as estimating population density and predicting property prices. The model demonstrates strong zero-shot and few-shot learning abilities, allowing it to adapt to new urban contexts without extensive retraining. This research paves the way for innovative applications in urban planning, policy-making, and environmental monitoring.'}, 'zh': {'title': 'OpenCity3D：城市分析的新视角', 'desc': '本文介绍了一种新的视觉语言模型OpenCity3D，旨在提升对城市规模环境的理解。该模型利用多视角航拍图像的3D重建，能够处理高层次任务，如人口密度估计、建筑年龄分类和物业价格预测等。研究表明，OpenCity3D在零样本和少样本学习方面表现出色，能够适应新的应用场景。此研究为基于语言的城市分析开辟了新方向，具有广泛的规划和环境监测应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.15667', 'title': 'DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis', 'url': 'https://huggingface.co/papers/2503.15667', 'abstract': 'Generating high-quality 360-degree views of human heads from single-view images is essential for enabling accessible immersive telepresence applications and scalable personalized content creation. While cutting-edge methods for full head generation are limited to modeling realistic human heads, the latest diffusion-based approaches for style-omniscient head synthesis can produce only frontal views and struggle with view consistency, preventing their conversion into true 3D models for rendering from arbitrary angles. We introduce a novel approach that generates fully consistent 360-degree head views, accommodating human, stylized, and anthropomorphic forms, including accessories like glasses and hats. Our method builds on the DiffPortrait3D framework, incorporating a custom ControlNet for back-of-head detail generation and a dual appearance module to ensure global front-back consistency. By training on continuous view sequences and integrating a back reference image, our approach achieves robust, locally continuous view synthesis. Our model can be used to produce high-quality neural radiance fields (NeRFs) for real-time, free-viewpoint rendering, outperforming state-of-the-art methods in object synthesis and 360-degree head generation for very challenging input portraits.', 'score': 0, 'issue_id': 2916, 'pub_date': '2025-03-19', 'pub_date_card': {'ru': '19 марта', 'en': 'March 19', 'zh': '3月19日'}, 'hash': '0f10483eac04cdbf', 'authors': ['Yuming Gu', 'Phong Tran', 'Yujian Zheng', 'Hongyi Xu', 'Heyuan Li', 'Adilbek Karmanov', 'Hao Li'], 'affiliations': ['ByteDance Inc.', 'MBZUAI', 'Pinscreen Inc.', 'The Chinese University of Hong Kong, Shenzhen', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2503.15667.jpg', 'data': {'categories': ['#diffusion', '#cv', '#3d'], 'emoji': '🧠', 'ru': {'title': 'Революция в 3D-моделировании голов: от одного ракурса к полному обзору', 'desc': 'Статья представляет новый метод генерации полностью согласованных 360-градусных изображений голов, включая человеческие, стилизованные и антропоморфные формы. Подход основан на фреймворке DiffPortrait3D и использует специальный ControlNet для генерации деталей задней части головы. Модель обучается на непрерывных последовательностях ракурсов и интегрирует эталонное изображение задней части для обеспечения глобальной согласованности. Результаты превосходят современные методы в синтезе объектов и генерации 360-градусных изображений голов для сложных входных портретов.'}, 'en': {'title': 'Revolutionizing 360-Degree Head Generation with Consistency and Detail', 'desc': 'This paper presents a new method for generating 360-degree views of human heads from single images, which is important for creating immersive experiences and personalized content. Unlike previous techniques that only produce frontal views and lack consistency, this approach ensures that the generated heads look realistic from all angles. It utilizes a framework called DiffPortrait3D, enhanced with a ControlNet for adding details to the back of the head and a dual appearance module for maintaining consistency between the front and back views. The model is trained on sequences of views and can create high-quality neural radiance fields (NeRFs), enabling real-time rendering from any viewpoint and surpassing existing methods in head generation.'}, 'zh': {'title': '360度人头视图生成的新突破', 'desc': '本论文提出了一种新方法，可以从单视角图像生成高质量的360度人头视图。这种方法解决了现有技术在生成真实人头时只能提供正面视图的问题，并且确保了视图的一致性。我们的方法基于DiffPortrait3D框架，结合了自定义的ControlNet和双重外观模块，以生成后脑勺的细节并保持前后视图的一致性。通过训练连续视角序列并整合后参考图像，我们的方法在对象合成和360度人头生成方面超越了现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2503.09566', 'title': 'TPDiff: Temporal Pyramid Video Diffusion Model', 'url': 'https://huggingface.co/papers/2503.09566', 'abstract': 'The development of video diffusion models unveils a significant challenge: the substantial computational demands. To mitigate this challenge, we note that the reverse process of diffusion exhibits an inherent entropy-reducing nature. Given the inter-frame redundancy in video modality, maintaining full frame rates in high-entropy stages is unnecessary. Based on this insight, we propose TPDiff, a unified framework to enhance training and inference efficiency. By dividing diffusion into several stages, our framework progressively increases frame rate along the diffusion process with only the last stage operating on full frame rate, thereby optimizing computational efficiency. To train the multi-stage diffusion model, we introduce a dedicated training framework: stage-wise diffusion. By solving the partitioned probability flow ordinary differential equations (ODE) of diffusion under aligned data and noise, our training strategy is applicable to various diffusion forms and further enhances training efficiency. Comprehensive experimental evaluations validate the generality of our method, demonstrating 50% reduction in training cost and 1.5x improvement in inference efficiency.', 'score': 32, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '6952e94de20936ce', 'authors': ['Lingmin Ran', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.09566.jpg', 'data': {'categories': ['#inference', '#video', '#diffusion', '#training', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Ускорение видео-диффузии: эффективность через поэтапность', 'desc': 'Статья представляет TPDiff - унифицированную систему для повышения эффективности обучения и вывода видео-диффузионных моделей. Авторы предлагают разделить процесс диффузии на несколько этапов, постепенно увеличивая частоту кадров, что оптимизирует вычислительные ресурсы. Для обучения многоэтапной диффузионной модели вводится специальная структура обучения: поэтапная диффузия. Экспериментальные оценки подтверждают универсальность метода, демонстрируя сокращение затрат на обучение на 50% и повышение эффективности вывода в 1,5 раза.'}, 'en': {'title': 'Optimizing Video Diffusion with TPDiff: Efficiency Unleashed!', 'desc': 'This paper addresses the high computational costs associated with video diffusion models by introducing TPDiff, a framework that optimizes training and inference efficiency. The authors leverage the entropy-reducing nature of the diffusion process and the redundancy between video frames to reduce the need for full frame rates during high-entropy stages. TPDiff operates in multiple stages, gradually increasing the frame rate, with only the final stage using the full frame rate, thus enhancing computational efficiency. The proposed stage-wise diffusion training framework further improves efficiency by solving partitioned probability flow ordinary differential equations, leading to significant reductions in training costs and improvements in inference speed.'}, 'zh': {'title': '优化视频扩散模型的计算效率', 'desc': '视频扩散模型的发展面临着巨大的计算需求。为了缓解这一挑战，我们注意到扩散的反向过程具有固有的减少熵的特性。考虑到视频模态中的帧间冗余，在高熵阶段保持全帧率是没有必要的。基于这一见解，我们提出了TPDiff框架，通过将扩散过程分为多个阶段，逐步提高帧率，从而优化计算效率。'}}}, {'id': 'https://huggingface.co/papers/2503.09573', 'title': 'Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models', 'url': 'https://huggingface.co/papers/2503.09573', 'abstract': 'Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/', 'score': 29, 'issue_id': 2678, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '32f097e93cbf5f3a', 'authors': ['Marianne Arriola', 'Aaron Gokaslan', 'Justin T Chiu', 'Zhihan Yang', 'Zhixuan Qi', 'Jiaqi Han', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov'], 'affiliations': ['Cohere, NY, USA', 'Cornell Tech, NY, USA', 'Stanford University, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.09573.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#benchmark', '#training', '#architecture'], 'emoji': '🧩', 'ru': {'title': 'Блочные диффузионные модели: лучшее из двух миров в языковом моделировании', 'desc': 'Статья представляет новый класс языковых моделей - блочные диффузионные модели. Они объединяют преимущества диффузионных и авторегрессивных моделей, позволяя генерировать тексты произвольной длины и повышая эффективность вывода. Авторы предлагают эффективный алгоритм обучения, оценки дисперсии градиента и расписания шума для минимизации дисперсии. Блочные диффузионные модели достигают нового уровня производительности среди диффузионных моделей в задачах языкового моделирования.'}, 'en': {'title': 'Block Diffusion: The Future of Flexible Language Generation', 'desc': 'This paper presents block diffusion language models, which combine the strengths of diffusion models and autoregressive models. These models allow for flexible-length text generation and improve efficiency during inference by using techniques like KV caching and parallel token sampling. The authors introduce a comprehensive approach for training these models, including methods to reduce gradient variance and optimize noise schedules. As a result, block diffusion models achieve state-of-the-art performance in language modeling tasks and can generate sequences of varying lengths.'}, 'zh': {'title': '块扩散模型：灵活生成与高效推理的结合', 'desc': '扩散语言模型相比自回归模型具有独特的优势，如并行生成和可控性，但在似然建模方面表现较差，并且生成长度固定。本文提出了一类块扩散语言模型，结合了离散去噪扩散和自回归模型的优点。块扩散克服了这两种方法的关键限制，支持灵活长度的生成，并通过KV缓存和并行令牌采样提高推理效率。我们提出了一种构建有效块扩散模型的方案，包括高效的训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。'}}}, {'id': 'https://huggingface.co/papers/2503.09151', 'title': 'Reangle-A-Video: 4D Video Generation as Video-to-Video Translation', 'url': 'https://huggingface.co/papers/2503.09151', 'abstract': 'We introduce Reangle-A-Video, a unified framework for generating synchronized multi-view videos from a single input video. Unlike mainstream approaches that train multi-view video diffusion models on large-scale 4D datasets, our method reframes the multi-view video generation task as video-to-videos translation, leveraging publicly available image and video diffusion priors. In essence, Reangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An image-to-video diffusion transformer is synchronously fine-tuned in a self-supervised manner to distill view-invariant motion from a set of warped videos. (2) Multi-View Consistent Image-to-Images Translation: The first frame of the input video is warped and inpainted into various camera perspectives under an inference-time cross-view consistency guidance using DUSt3R, generating multi-view consistent starting images. Extensive experiments on static view transport and dynamic camera control show that Reangle-A-Video surpasses existing methods, establishing a new solution for multi-view video generation. We will publicly release our code and data. Project page: https://hyeonho99.github.io/reangle-a-video/', 'score': 23, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'e1463c182b0fe8e9', 'authors': ['Hyeonho Jeong', 'Suhyeon Lee', 'Jong Chul Ye'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2503.09151.jpg', 'data': {'categories': ['#open_source', '#video', '#multimodal', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Революция в многоракурсной видеогенерации без 4D-датасетов', 'desc': 'Reangle-A-Video - это новый подход к генерации синхронизированных многоракурсных видео из одного входного видео. Метод использует двухэтапный процесс: обучение многоракурсному движению и согласованный перевод изображения в изображения с разных ракурсов. В отличие от традиционных методов, Reangle-A-Video не требует больших 4D-датасетов, а использует существующие модели диффузии для изображений и видео. Эксперименты показывают, что этот метод превосходит существующие подходы в задачах статического переноса ракурса и динамического управления камерой.'}, 'en': {'title': 'Transforming Single Videos into Multi-View Masterpieces!', 'desc': 'Reangle-A-Video is a novel framework designed to create synchronized multi-view videos from a single input video. It innovatively approaches the multi-view video generation task by treating it as a video-to-video translation problem, utilizing existing image and video diffusion models. The process consists of two main stages: first, it learns motion patterns from warped videos in a self-supervised manner, and second, it generates consistent multi-view images by warping and inpainting the initial frame under specific guidance. This method outperforms current techniques in both static view transport and dynamic camera control, marking a significant advancement in multi-view video generation.'}, 'zh': {'title': 'Reangle-A-Video：单视频生成多视角同步视频的新方法', 'desc': '我们提出了Reangle-A-Video，这是一个统一框架，用于从单个输入视频生成同步的多视角视频。与主流方法不同，我们的方法将多视角视频生成任务重新定义为视频到视频的转换，利用公开可用的图像和视频扩散先验。Reangle-A-Video的操作分为两个阶段：首先，通过自监督方式对图像到视频的扩散变换器进行同步微调，以提取视角不变的运动；其次，在推理时使用DUSt3R进行跨视角一致性指导，将输入视频的第一帧变形并修复为不同的摄像机视角，生成多视角一致的起始图像。'}}}, {'id': 'https://huggingface.co/papers/2503.09601', 'title': 'RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling', 'url': 'https://huggingface.co/papers/2503.09601', 'abstract': 'Score Distillation Sampling (SDS) has emerged as an effective technique for leveraging 2D diffusion priors for tasks such as text-to-3D generation. While powerful, SDS struggles with achieving fine-grained alignment to user intent. To overcome this, we introduce RewardSDS, a novel approach that weights noise samples based on alignment scores from a reward model, producing a weighted SDS loss. This loss prioritizes gradients from noise samples that yield aligned high-reward output. Our approach is broadly applicable and can extend SDS-based methods. In particular, we demonstrate its applicability to Variational Score Distillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and RewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks, showing significant improvements over SDS and VSD on a diverse set of metrics measuring generation quality and alignment to desired reward models, enabling state-of-the-art performance. Project page is available at https://itaychachy. github.io/reward-sds/.', 'score': 10, 'issue_id': 2683, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '4cbf8c2d009fe092', 'authors': ['Itay Chachy', 'Guy Yariv', 'Sagie Benaim'], 'affiliations': ['Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2503.09601.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#diffusion', '#3d', '#training'], 'emoji': '🎯', 'ru': {'title': 'Точная генерация контента с помощью взвешенного обучения', 'desc': 'Статья представляет новый метод RewardSDS, улучшающий технику Score Distillation Sampling (SDS) для генерации контента. RewardSDS использует модель вознаграждения для взвешивания шумовых сэмплов, что позволяет лучше соответствовать намерениям пользователя. Авторы также представляют RewardVSD - расширение метода Variational Score Distillation (VSD). Эксперименты показывают значительное улучшение качества генерации и соответствия заданным критериям для задач генерации изображений и 3D-моделей.'}, 'en': {'title': 'Aligning User Intent with Reward-Driven Sampling', 'desc': 'This paper presents RewardSDS, an innovative method that enhances Score Distillation Sampling (SDS) by incorporating alignment scores from a reward model. By weighting noise samples according to their alignment with user intent, RewardSDS generates a more effective weighted loss function. This approach not only improves the performance of SDS but also extends its application to Variational Score Distillation (VSD) through the introduction of RewardVSD. The authors demonstrate that their methods significantly outperform existing techniques in generating high-quality outputs across various tasks, including text-to-image and text-to-3D generation.'}, 'zh': {'title': '提升生成对齐度的新方法', 'desc': '得分蒸馏采样（SDS）是一种有效利用二维扩散先验进行文本到三维生成的技术。然而，SDS在实现与用户意图的细致对齐方面存在困难。为了解决这个问题，我们提出了RewardSDS，这是一种基于奖励模型的对齐得分加权噪声样本的新方法，从而生成加权的SDS损失。我们的研究表明，RewardSDS和RewardVSD在文本到图像、二维编辑和文本到三维生成任务上显著提高了生成质量和与期望奖励模型的对齐度，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.08525', 'title': 'GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based\n  VLM Agent Training', 'url': 'https://huggingface.co/papers/2503.08525', 'abstract': "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes.", 'score': 10, 'issue_id': 2681, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': 'e82260bba3e835b4', 'authors': ['Tong Wei', 'Yijun Yang', 'Junliang Xing', 'Yuanchun Shi', 'Zongqing Lu', 'Deheng Ye'], 'affiliations': ['Peking University', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.08525.jpg', 'data': {'categories': ['#agents', '#games', '#reasoning', '#video', '#training', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Управляемое обучение рассуждениям для визуально-языковых моделей', 'desc': "Исследование посвящено применению обучения с подкреплением для улучшения рассуждений в визуально-языковых моделях. Авторы обнаружили феномен 'коллапса мыслей' при использовании наград, основанных только на результатах действий. Для решения этой проблемы предложен метод GTR (Guided Thought Reinforcement), который автоматически оценивает и уточняет рассуждения агента на каждом шаге обучения. Эксперименты показали, что GTR значительно улучшает производительность и обобщающую способность модели LLaVA-7b в различных визуальных средах."}, 'en': {'title': 'Enhancing VLMs with Guided Thought Reinforcement', 'desc': "This paper explores the challenges of using reinforcement learning (RL) to train vision-language models (VLMs) for reasoning in visual tasks. It identifies a problem called 'thought collapse', where the model's reasoning becomes less diverse and leads to incorrect actions when rewards are based only on outcomes. To address this, the authors propose a Guided Thought Reinforcement (GTR) framework that provides process guidance to improve the reasoning of VLMs during training. Their experiments show that GTR significantly boosts the performance of the LLaVA-7b model, achieving much higher success rates in complex tasks compared to state-of-the-art models."}, 'zh': {'title': '引导思维强化：提升视觉语言模型的推理能力', 'desc': '本研究探讨了可验证结果奖励的强化学习（RLVR）在视觉语言模型（VLM）中的应用，尤其是在复杂的视觉环境中进行目标导向的推理。我们发现，当奖励仅基于行动结果时，RL无法有效激励VLM的思维链推理，导致思维崩溃现象，表现为代理的思维多样性迅速下降和推理不完整。为了解决这一问题，我们提出了一种自动纠正器，能够在每个RL步骤中评估和改进代理的推理过程。通过实验，我们的引导思维强化（GTR）框架显著提高了LLaVA-7b模型在各种视觉环境中的表现和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.04388', 'title': 'More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG', 'url': 'https://huggingface.co/papers/2503.04388', 'abstract': 'Retrieval-augmented generation (RAG) provides LLMs with relevant documents. Although previous studies noted that retrieving many documents can degrade performance, they did not isolate how the quantity of documents affects performance while controlling for context length. We evaluate various language models on custom datasets derived from a multi-hop QA task. We keep the context length and position of relevant information constant while varying the number of documents, and find that increasing the document count in RAG settings poses significant challenges for LLMs. Additionally, our results indicate that processing multiple documents is a separate challenge from handling long contexts. We also make the datasets and code available: https://github.com/shaharl6000/MoreDocsSameLen .', 'score': 10, 'issue_id': 2682, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '9b3fb8251a206c0d', 'authors': ['Shahar Levy', 'Nir Mazor', 'Lihi Shalmon', 'Michael Hassid', 'Gabriel Stanovsky'], 'affiliations': ['School of Computer Science and Engineering The Hebrew University of Jerusalem, Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.04388.jpg', 'data': {'categories': ['#long_context', '#dataset', '#open_source', '#rag'], 'emoji': '📚', 'ru': {'title': 'Больше документов - больше проблем для языковых моделей', 'desc': 'Статья исследует влияние количества документов на производительность языковых моделей в задачах генерации с извлечением (RAG). Авторы создали специальные наборы данных, где сохраняли длину контекста и позицию релевантной информации постоянными, но варьировали число документов. Результаты показали, что увеличение количества документов в RAG создает значительные трудности для языковых моделей. Также было установлено, что обработка множества документов - это отдельная проблема от работы с длинными контекстами.'}, 'en': {'title': 'More Documents, More Challenges for LLMs!', 'desc': 'This paper investigates how the number of documents retrieved in Retrieval-augmented generation (RAG) impacts the performance of large language models (LLMs). The authors conduct experiments using custom datasets focused on multi-hop question answering, ensuring that context length remains constant while varying the number of documents. Their findings reveal that increasing the number of documents can significantly hinder the performance of LLMs, indicating that managing multiple documents presents unique challenges distinct from those associated with long contexts. The study contributes to the understanding of RAG by providing datasets and code for further research.'}, 'zh': {'title': '文档数量影响LLM性能的挑战', 'desc': '检索增强生成（RAG）为大型语言模型（LLMs）提供相关文档。尽管之前的研究指出检索过多文档可能会降低性能，但并未明确控制上下文长度来研究文档数量对性能的影响。我们在多跳问答任务的自定义数据集上评估了各种语言模型，发现增加文档数量在RAG设置中对LLMs造成了显著挑战。此外，我们的结果表明，处理多个文档与处理长上下文是两个不同的挑战。'}}}, {'id': 'https://huggingface.co/papers/2503.06955', 'title': 'Motion Anything: Any to Motion Generation', 'url': 'https://huggingface.co/papers/2503.06955', 'abstract': 'Conditional motion generation has been extensively studied in computer vision, yet two critical challenges remain. First, while masked autoregressive methods have recently outperformed diffusion-based approaches, existing masking models lack a mechanism to prioritize dynamic frames and body parts based on given conditions. Second, existing methods for different conditioning modalities often fail to integrate multiple modalities effectively, limiting control and coherence in generated motion. To address these challenges, we propose Motion Anything, a multimodal motion generation framework that introduces an Attention-based Mask Modeling approach, enabling fine-grained spatial and temporal control over key frames and actions. Our model adaptively encodes multimodal conditions, including text and music, improving controllability. Additionally, we introduce Text-Music-Dance (TMD), a new motion dataset consisting of 2,153 pairs of text, music, and dance, making it twice the size of AIST++, thereby filling a critical gap in the community. Extensive experiments demonstrate that Motion Anything surpasses state-of-the-art methods across multiple benchmarks, achieving a 15% improvement in FID on HumanML3D and showing consistent performance gains on AIST++ and TMD. See our project website https://steve-zeyu-zhang.github.io/MotionAnything', 'score': 7, 'issue_id': 2680, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '9199d2d99b75d862', 'authors': ['Zeyu Zhang', 'Yiran Wang', 'Wei Mao', 'Danning Li', 'Rui Zhao', 'Biao Wu', 'Zirui Song', 'Bohan Zhuang', 'Ian Reid', 'Richard Hartley'], 'affiliations': ['ANU', 'Google', 'JD.com', 'MBZUAI', 'McGill', 'Tencent', 'USYD', 'UTS', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2503.06955.jpg', 'data': {'categories': ['#games', '#synthetic', '#optimization', '#benchmark', '#multimodal', '#cv', '#dataset'], 'emoji': '🕺', 'ru': {'title': 'Универсальная генерация движений с мультимодальным контролем', 'desc': 'Статья представляет Motion Anything - новую мультимодальную систему генерации движений, использующую маскирование на основе внимания для точного контроля над ключевыми кадрами и действиями. Модель адаптивно кодирует различные условия, включая текст и музыку, что улучшает управляемость генерируемых движений. Авторы также представляют новый датасет Text-Music-Dance (TMD), содержащий 2153 пары текста, музыки и танца. Эксперименты показывают, что Motion Anything превосходит современные методы на нескольких бенчмарках, достигая 15% улучшения FID на HumanML3D.'}, 'en': {'title': 'Revolutionizing Motion Generation with Multimodal Control', 'desc': 'This paper presents Motion Anything, a new framework for generating motion that effectively combines multiple input types like text and music. It addresses two main challenges in motion generation: the need for prioritizing dynamic elements and the integration of different conditioning modalities. The proposed Attention-based Mask Modeling allows for better control over key frames and actions, enhancing the quality of generated motions. Additionally, the introduction of the Text-Music-Dance dataset provides a larger resource for training, leading to significant improvements in performance compared to existing methods.'}, 'zh': {'title': '多模态运动生成的新突破', 'desc': '本文提出了一种名为Motion Anything的多模态运动生成框架，旨在解决现有方法在动态帧和身体部位优先级方面的不足。我们引入了一种基于注意力的掩模建模方法，使得对关键帧和动作的空间和时间控制更加精细。该模型能够自适应编码文本和音乐等多模态条件，从而提高生成运动的可控性。此外，我们还创建了一个新的运动数据集Text-Music-Dance (TMD)，包含2153对文本、音乐和舞蹈，填补了社区中的重要空白。'}}}, {'id': 'https://huggingface.co/papers/2503.07103', 'title': 'Quantizing Large Language Models for Code Generation: A Differentiated\n  Replication', 'url': 'https://huggingface.co/papers/2503.07103', 'abstract': "Large Language Models (LLMs) have shown an impressive capability in code generation and, specifically, to automatically implement requirements described in natural language. The LLM effectiveness generally increases with its size: The higher the number of LLM's trainable parameters the better its ability to implement code. However, when it comes to deploying LLM-based code generators, larger LLMs pose significant challenges related to their memory (and, consequently, carbon) footprint. A previous work by Wei et al. proposed to leverage quantization techniques to reduce the memory footprint of LLM-based code generators without substantially degrading their effectiveness. In short, they studied LLMs featuring up to 16B parameters, quantizing their precision from floating point 32 bits down to int 8 bits and showing their limited impact on code generation performance. Given the fast pace at which LLM capabilities and quantization techniques are evolving, in this work we present a differentiated replication of the work by Wei et al. in which we consider (i) on the one side, more recent and larger code-related LLMs, of up to 34B parameters; (ii) the latest advancements in model quantization techniques, which allow pushing the compression to the extreme quantization level of 2 bits per model parameter and; (iii) different types of calibration datasets to guide the quantization process, including code-specific ones. Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance.", 'score': 6, 'issue_id': 2683, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': '4f07119680bb80a1', 'authors': ['Alessandro Giagnorio', 'Antonio Mastropaolo', 'Saima Afrin', 'Massimiliano Di Penta', 'Gabriele Bavota'], 'affiliations': ['Software Institute Università della Svizzera italiana, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2503.07103.jpg', 'data': {'categories': ['#optimization', '#inference', '#dataset', '#training'], 'emoji': '🧠', 'ru': {'title': 'Квантование LLM для эффективной генерации кода', 'desc': 'Статья посвящена исследованию эффективности квантования больших языковых моделей (LLM) для генерации кода. Авторы изучают возможность сжатия моделей до 2 бит на параметр, используя современные методы квантования. Эксперименты показывают, что 4-битное квантование позволяет сократить объем памяти на 70% без существенной потери производительности. При более экстремальном квантовании (3 и 2 бита) использование специфичного для кода набора данных для калибровки помогает ограничить снижение эффективности.'}, 'en': {'title': 'Optimizing Code Generation with Extreme Quantization Techniques', 'desc': 'This paper explores the use of quantization techniques to reduce the memory footprint of large language models (LLMs) used for code generation. It builds on previous work by examining larger models with up to 34 billion parameters and applying advanced quantization methods that compress model precision down to 2 bits. The study finds that using 4-bit quantization can reduce memory usage by 70% without significantly impacting performance. Furthermore, it highlights the importance of using code-specific calibration datasets to maintain effectiveness even with more aggressive quantization levels.'}, 'zh': {'title': '量化技术助力大型语言模型的高效代码生成', 'desc': '大型语言模型（LLMs）在代码生成方面表现出色，能够根据自然语言描述自动实现需求。模型的有效性通常随着参数数量的增加而提高，但较大的模型在部署时会面临内存和碳足迹的挑战。为了解决这个问题，研究者们提出了量化技术，以减少LLM代码生成器的内存占用，同时保持其有效性。本文通过研究更新的、参数高达34B的LLM，探索了更极端的量化技术，发现4位精度的量化可以将内存占用减少70%，而性能几乎没有显著下降。'}}}, {'id': 'https://huggingface.co/papers/2503.06573', 'title': 'WildIFEval: Instruction Following in the Wild', 'url': 'https://huggingface.co/papers/2503.06573', 'abstract': 'Recent LLMs have shown remarkable success in following user instructions, yet handling instructions with multiple constraints remains a significant challenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K real user instructions with diverse, multi-constraint conditions. Unlike prior datasets, our collection spans a broad lexical and topical spectrum of constraints, in natural user prompts. We categorize these constraints into eight high-level classes to capture their distribution and dynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive experiments to benchmark the instruction-following capabilities of leading LLMs. Our findings reveal that all evaluated models experience performance degradation with an increasing number of constraints. Thus, we show that all models have a large room for improvement on such tasks. Moreover, we observe that the specific type of constraint plays a critical role in model performance. We release our dataset to promote further research on instruction-following under complex, realistic conditions.', 'score': 5, 'issue_id': 2683, 'pub_date': '2025-03-09', 'pub_date_card': {'ru': '9 марта', 'en': 'March 9', 'zh': '3月9日'}, 'hash': 'a2fa7b2b917156fd', 'authors': ['Gili Lior', 'Asaf Yehudai', 'Ariel Gera', 'Liat Ein-Dor'], 'affiliations': ['IBM Research', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2503.06573.jpg', 'data': {'categories': ['#alignment', '#dataset', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Новый вызов для ИИ: следование сложным инструкциям', 'desc': 'Статья представляет новый набор данных WildIFEval, содержащий 12000 реальных пользовательских инструкций с разнообразными ограничениями. Авторы классифицируют эти ограничения на 8 категорий и используют набор данных для оценки способности современных языковых моделей следовать сложным инструкциям. Результаты показывают, что производительность всех моделей снижается с увеличением числа ограничений. Исследование выявляет большой потенциал для улучшения моделей в выполнении сложных инструкций с множественными ограничениями.'}, 'en': {'title': 'WildIFEval: Benchmarking LLMs on Complex User Instructions', 'desc': 'This paper presents WildIFEval, a new dataset designed to evaluate how well large language models (LLMs) can follow user instructions with multiple constraints. The dataset contains 12,000 real user prompts that include a variety of constraints categorized into eight classes, reflecting the complexity of real-world instructions. Experiments show that as the number of constraints increases, the performance of all tested LLMs declines, indicating a significant area for improvement. The study highlights the importance of the type of constraint in affecting model performance and aims to encourage further research in this challenging area of instruction-following.'}, 'zh': {'title': '多重约束下的指令遵循挑战', 'desc': '最近的大型语言模型（LLMs）在遵循用户指令方面取得了显著成功，但处理具有多个约束的指令仍然是一个重大挑战。我们提出了WildIFEval，这是一个包含12000个真实用户指令的大规模数据集，涵盖了多样化的多重约束条件。与之前的数据集不同，我们的收集涵盖了广泛的词汇和主题约束，并将这些约束分为八个高层类别，以捕捉它们在现实场景中的分布和动态。通过WildIFEval，我们进行了广泛的实验，以基准测试领先LLMs的指令遵循能力，发现所有评估的模型在约束数量增加时性能下降，表明这些任务仍有很大的改进空间。'}}}, {'id': 'https://huggingface.co/papers/2503.09402', 'title': 'VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary', 'url': 'https://huggingface.co/papers/2503.09402', 'abstract': "Human daily activities can be concisely narrated as sequences of routine events (e.g., turning off an alarm) in video streams, forming an event vocabulary. Motivated by this, we introduce VLog, a novel video understanding framework that define video narrations as vocabulary, going beyond the typical subword vocabularies in existing generative video-language models. Built on the lightweight language model GPT-2, VLog feature three key innovations: (i) A generative retrieval model, marrying language model's complex reasoning capabilities with contrastive retrieval's efficient similarity search. (ii) A hierarchical vocabulary derived from large-scale video narrations using our narration pair encoding algorithm, enabling efficient indexing of specific events (e.g., cutting a tomato) by identifying broader scenarios (e.g., kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary update strategy leveraging generative models to extend the vocabulary for novel events encountered during inference. To validate our approach, we introduce VidCap-Eval, a development set requiring concise narrations with reasoning relationships (e.g., before and after). Experiments on EgoSchema, COIN, and HiREST further demonstrate the effectiveness of VLog, highlighting its ability to generate concise, contextually accurate, and efficient narrations, offering a novel perspective on video understanding. Codes are released at https://github.com/showlab/VLog.", 'score': 4, 'issue_id': 2681, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '6a25ed0e2c069e4f', 'authors': ['Kevin Qinghong Lin', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.09402.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#games', '#reasoning', '#video', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'VLog: Пересказ видео через словарь событий', 'desc': 'VLog - это новая система понимания видео, которая использует словарь событий для описания повседневной деятельности человека. Она основана на языковой модели GPT-2 и включает в себя генеративную модель поиска, иерархический словарь и стратегию обновления словаря. VLog способна генерировать краткие и точные описания видео, учитывая контекст и логические связи между событиями. Эффективность системы была продемонстрирована на нескольких наборах данных, включая специально созданный набор VidCap-Eval.'}, 'en': {'title': 'VLog: Revolutionizing Video Narration with Hierarchical Vocabulary', 'desc': 'The paper presents VLog, a new framework for understanding videos by narrating daily activities as sequences of events. It introduces a hierarchical vocabulary that allows for efficient indexing of specific actions within broader contexts, enhancing the way video content is interpreted. VLog combines a generative retrieval model with a lightweight language model, enabling complex reasoning and efficient similarity searches. Additionally, it features a vocabulary update strategy that adapts to new events during inference, demonstrating its effectiveness through experiments on various datasets.'}, 'zh': {'title': 'VLog：视频理解的新视角', 'desc': '本论文介绍了一种名为VLog的视频理解框架，旨在将视频叙述定义为词汇，超越现有生成视频语言模型中的子词词汇。VLog基于轻量级语言模型GPT-2，具有三项关键创新：生成检索模型、层次词汇和词汇更新策略。生成检索模型结合了语言模型的复杂推理能力和对比检索的高效相似性搜索。通过在EgoSchema、COIN和HiREST数据集上的实验，验证了VLog在生成简洁、上下文准确的叙述方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.09579', 'title': 'Cost-Optimal Grouped-Query Attention for Long-Context LLMs', 'url': 'https://huggingface.co/papers/2503.09579', 'abstract': 'Building effective and efficient Transformer-based large language models (LLMs) has recently become a research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, a larger model with fewer attention heads can achieve a lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data.', 'score': 3, 'issue_id': 2682, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '2c884c8c6aab1cc4', 'authors': ['Yingfa Chen', 'Yutong Wu', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China', 'SIST, University of Science and Technology Beijing, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09579.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#long_context', '#inference', '#training'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация LLM: больше параметров, меньше головок внимания', 'desc': 'Статья исследует влияние длины контекста и конфигурации головок внимания на эффективность и производительность больших языковых моделей (LLM). Авторы проводят систематическое сравнение моделей с различными размерами параметров, длинами контекста и конфигурациями головок внимания. Они расширяют существующие методы масштабирования, чтобы оптимизировать LLM как на этапе обучения, так и при выводе. Результаты показывают, что при обработке длинных последовательностей большая модель с меньшим количеством головок внимания может достичь лучших результатов при меньших вычислительных затратах и использовании памяти.'}, 'en': {'title': 'Optimizing LLMs: Less Heads, More Efficiency!', 'desc': 'This paper investigates how different configurations of Transformer-based large language models (LLMs) affect their performance and resource efficiency. It highlights the importance of context length and attention head settings, which have been largely ignored in previous research. By comparing various model sizes and configurations, the authors propose new scaling methods that optimize both training and inference costs. Their results indicate that larger models with fewer attention heads can perform better and use less computational and memory resources when handling long sequences.'}, 'zh': {'title': '优化大型语言模型的构建与成本', 'desc': '本文探讨了如何构建高效的基于Transformer的大型语言模型（LLMs），重点在于最大化模型的语言能力，同时降低训练和部署成本。研究比较了不同参数大小、上下文长度和注意力头配置对模型性能、计算成本和内存成本的影响。结果表明，在处理较长序列时，较大的模型配合较少的注意力头可以实现更低的损失，同时降低计算和内存成本。我们的研究为开发实用的LLMs提供了重要的见解，尤其是在长上下文处理的场景中。'}}}, {'id': 'https://huggingface.co/papers/2503.09419', 'title': 'Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space', 'url': 'https://huggingface.co/papers/2503.09419', 'abstract': 'Latent Diffusion Models (LDMs) are known to have an unstable generation process, where even small perturbations or shifts in the input noise can lead to significantly different outputs. This hinders their applicability in applications requiring consistent results. In this work, we redesign LDMs to enhance consistency by making them shift-equivariant. While introducing anti-aliasing operations can partially improve shift-equivariance, significant aliasing and inconsistency persist due to the unique challenges in LDMs, including 1) aliasing amplification during VAE training and multiple U-Net inferences, and 2) self-attention modules that inherently lack shift-equivariance. To address these issues, we redesign the attention modules to be shift-equivariant and propose an equivariance loss that effectively suppresses the frequency bandwidth of the features in the continuous domain. The resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is also robust to irregular warping. Extensive experiments demonstrate that AF-LDM produces significantly more consistent results than vanilla LDM across various applications, including video editing and image-to-image translation. Code is available at: https://github.com/SingleZombie/AFLDM', 'score': 3, 'issue_id': 2680, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '1c497a991b18da6a', 'authors': ['Yifan Zhou', 'Zeqi Xiao', 'Shuai Yang', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09419.jpg', 'data': {'categories': ['#video', '#diffusion', '#training', '#optimization', '#architecture', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Стабильная генерация изображений с помощью эквивариантных латентных диффузионных моделей', 'desc': 'Эта статья представляет новый подход к улучшению стабильности латентных диффузионных моделей (LDM). Авторы предлагают модифицированную архитектуру AF-LDM, которая обладает свойством эквивариантности к сдвигу, что повышает согласованность результатов генерации. Ключевые изменения включают переработку модулей внимания и введение специальной функции потерь для подавления частотной полосы признаков. Эксперименты показывают, что AF-LDM значительно превосходит стандартные LDM по стабильности результатов в различных задачах, таких как редактирование видео и перевод изображений.'}, 'en': {'title': 'Achieving Consistency in Latent Diffusion Models with Shift-Equivariance', 'desc': 'Latent Diffusion Models (LDMs) often produce inconsistent outputs due to their sensitivity to input noise variations. This paper presents a redesign of LDMs to improve their consistency by implementing shift-equivariance. The authors address challenges such as aliasing during VAE training and the limitations of self-attention modules by introducing new shift-equivariant attention mechanisms and an equivariance loss. The resulting alias-free LDM (AF-LDM) shows enhanced robustness and consistency in applications like video editing and image translation compared to traditional LDMs.'}, 'zh': {'title': '提升潜在扩散模型的一致性', 'desc': '潜在扩散模型（LDMs）在生成过程中存在不稳定性，输入噪声的微小变化可能导致输出结果显著不同，这限制了其在需要一致性结果的应用中的适用性。本文通过重新设计LDMs，使其具备平移等变性，从而增强一致性。我们提出了一种新的注意力模块，使其具备平移等变性，并引入了一种等变损失，有效抑制特征在连续域中的频率带宽。最终，得到的无别名LDM（AF-LDM）在多个应用中表现出更强的一致性，尤其是在视频编辑和图像到图像转换任务中。'}}}, {'id': 'https://huggingface.co/papers/2503.08681', 'title': 'Self-Taught Self-Correction for Small Language Models', 'url': 'https://huggingface.co/papers/2503.08681', 'abstract': 'Although large language models (LLMs) have achieved remarkable performance across various tasks, they remain prone to errors. A key challenge is enabling them to self-correct. While prior research has relied on external tools or large proprietary models, this work explores self-correction in small language models (SLMs) through iterative fine-tuning using solely self-generated data. We introduce the Self-Taught Self-Correction (STaSC) algorithm, which incorporates multiple algorithmic design choices. Experimental results on a question-answering task demonstrate that STaSC effectively learns self-correction, leading to significant performance improvements. Our analysis further provides insights into the mechanisms of self-correction and the impact of different design choices on learning dynamics and overall performance. To support future research, we release our user-friendly codebase and lightweight models.', 'score': 3, 'issue_id': 2686, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '1886b6cebec24540', 'authors': ['Viktor Moskvoretskii', 'Chris Biemann', 'Irina Nikishina'], 'affiliations': ['HSE University', 'Skoltech', 'University of Hamburg'], 'pdf_title_img': 'assets/pdf/title_img/2503.08681.jpg', 'data': {'categories': ['#small_models', '#open_source', '#optimization', '#alignment', '#training'], 'emoji': '🔧', 'ru': {'title': 'Самообучение самокоррекции: новый подход к улучшению малых языковых моделей', 'desc': 'Эта статья представляет алгоритм Self-Taught Self-Correction (STaSC) для обучения малых языковых моделей (SLM) самокоррекции. В отличие от предыдущих подходов, использующих внешние инструменты или крупные проприетарные модели, STaSC применяет итеративное дообучение на самогенерированных данных. Эксперименты на задаче вопросно-ответного анализа показывают значительное улучшение производительности моделей. Авторы также анализируют механизмы самокоррекции и влияние различных аспектов дизайна алгоритма на динамику обучения и общую эффективность.'}, 'en': {'title': 'Empowering Small Models with Self-Correction', 'desc': 'This paper addresses the issue of error-proneness in large language models (LLMs) by focusing on self-correction capabilities in smaller language models (SLMs). The authors propose a novel algorithm called Self-Taught Self-Correction (STaSC), which utilizes iterative fine-tuning with self-generated data to enhance the self-correction process. Experimental results indicate that STaSC significantly improves performance on a question-answering task, showcasing its effectiveness. Additionally, the paper analyzes the mechanisms behind self-correction and how various design choices influence learning dynamics and outcomes.'}, 'zh': {'title': '小型模型的自我纠正新方法', 'desc': '尽管大型语言模型在各种任务中表现出色，但它们仍然容易出错。本文探讨了如何通过自我生成的数据对小型语言模型进行迭代微调，以实现自我纠正。我们提出了自我学习自我纠正（STaSC）算法，该算法结合了多种设计选择。实验结果表明，STaSC在问答任务中有效地学习了自我纠正，显著提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2503.07588', 'title': 'When Large Vision-Language Model Meets Large Remote Sensing Imagery:\n  Coarse-to-Fine Text-Guided Token Pruning', 'url': 'https://huggingface.co/papers/2503.07588', 'abstract': "Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose a text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) a Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) a coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs' perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings. Dataset and code are in https://github.com/VisionXLab/LRS-VQA.", 'score': 3, 'issue_id': 2684, 'pub_date': '2025-03-10', 'pub_date_card': {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'}, 'hash': 'b3cad6b7241db7fa', 'authors': ['Junwei Luo', 'Yingying Zhang', 'Xue Yang', 'Kang Wu', 'Qi Zhu', 'Lei Liang', 'Jingdong Chen', 'Yansheng Li'], 'affiliations': ['Ant Group', 'Shanghai Jiao Tong University', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2503.07588.jpg', 'data': {'categories': ['#optimization', '#cv', '#survey', '#dataset', '#benchmark'], 'emoji': '🛰️', 'ru': {'title': 'Умное сжатие для анализа гигантских спутниковых снимков', 'desc': 'Статья представляет новый метод для эффективного анализа крупномасштабных спутниковых снимков с использованием больших языково-визуальных моделей (LVLM). Авторы предлагают технику выборочного удаления токенов на основе текстовых подсказок и динамической пирамиды изображений для сохранения важных деталей при снижении вычислительной сложности. Метод включает модуль фокусировки на регионах (RFM) и стратегию отбора фрагментов изображения от грубого к точному. Также представлен новый датасет LRS-VQA для оценки способностей LVLM в работе с крупными спутниковыми снимками.'}, 'en': {'title': 'Efficiently Unlocking Insights from Gigapixel Remote Sensing Images', 'desc': 'This paper addresses the challenges of understanding large Remote Sensing Images (RSIs) using Large Vision-Language Models (LVLMs). It introduces a novel text-guided token pruning method combined with a Dynamic Image Pyramid (DIP) to efficiently process gigapixel images without losing important details. The proposed Region Focus Module (RFM) helps in identifying essential vision tokens, while the coarse-to-fine strategy reduces computational costs. Additionally, the authors present a new benchmark, LRS-VQA, to evaluate LVLMs on large RSIs, demonstrating that their method outperforms existing strategies in terms of efficiency and effectiveness.'}, 'zh': {'title': '高效处理大型遥感图像的视觉-语言理解方法', 'desc': '本论文提出了一种高效的视觉-语言理解方法，专门针对大型遥感图像（RSIs）。我们的方法结合了动态图像金字塔（DIP）和文本引导的标记修剪技术，以减少计算复杂度并保留图像细节。通过区域聚焦模块（RFM），我们能够识别关键的视觉标记，从而优化图像处理过程。此外，我们还构建了一个新的基准数据集LRS-VQA，以评估现有大型视觉-语言模型的感知能力。'}}}, {'id': 'https://huggingface.co/papers/2503.09590', 'title': 'BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering', 'url': 'https://huggingface.co/papers/2503.09590', 'abstract': 'Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at https://sites.google.com/view/bimba-mllm.', 'score': 2, 'issue_id': 2689, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '82b85fd9bb1c9a17', 'authors': ['Md Mohaiminul Islam', 'Tushar Nagarajan', 'Huiyu Wang', 'Gedas Bertasius', 'Lorenzo Torresani'], 'affiliations': ['Meta AI', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2503.09590.jpg', 'data': {'categories': ['#video', '#open_source', '#benchmark', '#long_context', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'BIMBA: умное сжатие видео для эффективного анализа большими языковыми моделями', 'desc': 'Статья представляет BIMBA - эффективную модель пространства состояний для обработки длинных видео в задаче видео-вопрос-ответ (VQA). Модель использует алгоритм выборочного сканирования для эффективного выбора критической информации из видео высокой размерности и преобразования ее в сокращенную последовательность токенов. Это позволяет эффективно обрабатывать длинные видео большими языковыми моделями (LLM). BIMBA достигает наилучших результатов на нескольких бенчмарках для длинных видео в задаче VQA.'}, 'en': {'title': 'BIMBA: Efficiently Answering Questions from Long Videos', 'desc': 'This paper addresses the challenge of Video Question Answering (VQA) in long videos, where extracting relevant information from numerous frames is difficult. The authors introduce BIMBA, a state-space model that efficiently selects critical information from high-dimensional video data. Unlike previous methods that often miss important events due to redundancy, BIMBA uses a selective scan algorithm to create a reduced token sequence for processing by large language models (LLMs). The results show that BIMBA achieves state-of-the-art performance on various long-form VQA benchmarks, demonstrating its effectiveness in handling long videos.'}, 'zh': {'title': 'BIMBA：高效处理长视频问答的创新模型', 'desc': '视频问答（VQA）在长视频中面临提取相关信息和建模长距离依赖的挑战。自注意力机制虽然能处理序列建模，但在处理大量时空标记时计算成本过高。以往的方法通常依赖压缩策略来降低计算成本，但这些简单的方法往往会过度表示冗余信息，错过重要事件。我们提出了BIMBA，一个高效的状态空间模型，能够从高维视频中选择关键信息，并将其转化为简化的标记序列，以便高效处理。'}}}, {'id': 'https://huggingface.co/papers/2503.09427', 'title': 'Multimodal Language Modeling for High-Accuracy Single Cell\n  Transcriptomics Analysis and Generation', 'url': 'https://huggingface.co/papers/2503.09427', 'abstract': 'Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in k-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines.', 'score': 2, 'issue_id': 2677, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '491beb48064068d2', 'authors': ['Yaorui Shi', 'Jiaqi Yang', 'Sihang Li', 'Junfeng Fang', 'Xiang Wang', 'Zhiyuan Liu', 'Yang Zhang'], 'affiliations': ['National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09427.jpg', 'data': {'categories': ['#plp', '#transfer_learning', '#science', '#multimodal', '#dataset', '#training'], 'emoji': '🧬', 'ru': {'title': 'Единая модель для анализа клеток и текста', 'desc': 'scMMGPT - это новая языковая модель, объединяющая анализ одиночных клеток и текста. Она решает проблему ограниченности существующих моделей, которые специализируются только на одной из этих модальностей. scMMGPT использует специальные проекторы для преодоления разрыва между клеточными и текстовыми данными. Модель была предобучена на 27 миллионах клеток, что является крупнейшим датасетом для мультимодальных клеточно-текстовых моделей на сегодняшний день.'}, 'en': {'title': 'Bridging Cells and Text: The Power of scMMGPT', 'desc': 'This paper introduces the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a novel model designed to integrate single-cell RNA sequencing data with textual information. Traditional pre-trained language models struggle with this integration due to their inability to process both modalities effectively. scMMGPT addresses these limitations by utilizing cross-modal projectors and extensive pre-training on a large dataset of 27 million cells, enhancing its performance in joint tasks. The results demonstrate significant improvements in cell description generation, cell type annotation accuracy, and text-conditioned pseudo-cell generation compared to existing models.'}, 'zh': {'title': '单细胞多模态生成预训练变换器的创新应用', 'desc': '预训练语言模型（PLMs）在科学研究中带来了革命性的变化，但在单细胞分析中的应用仍然有限。现有的文本PLMs无法处理单细胞RNA测序数据，而细胞PLMs又无法处理自由文本，这限制了它们在多模态任务中的使用。为了解决这些问题，我们提出了单细胞多模态生成预训练变换器（scMMGPT），这是一个用于细胞和文本联合建模的统一PLM。scMMGPT通过专门的跨模态投影器和在2700万个细胞上进行的大规模预训练，显著提高了细胞描述生成和细胞类型注释的准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.05397', 'title': 'Multi Agent based Medical Assistant for Edge Devices', 'url': 'https://huggingface.co/papers/2503.05397', 'abstract': 'Large Action Models (LAMs) have revolutionized intelligent automation, but their application in healthcare faces challenges due to privacy concerns, latency, and dependency on internet access. This report introduces an ondevice, multi-agent healthcare assistant that overcomes these limitations. The system utilizes smaller, task-specific agents to optimize resources, ensure scalability and high performance. Our proposed system acts as a one-stop solution for health care needs with features like appointment booking, health monitoring, medication reminders, and daily health reporting. Powered by the Qwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an average RougeL score of 85.5 for planning and 96.5 for calling for our tasks while being lightweight for on-device deployment. This innovative approach combines the benefits of ondevice systems with multi-agent architectures, paving the way for user-centric healthcare solutions.', 'score': 2, 'issue_id': 2685, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '0d34c4bc75fe4355', 'authors': ['Sakharam Gawade', 'Shivam Akhouri', 'Chinmay Kulkarni', 'Jagdish Samant', 'Pragya Sahu', 'Aastik', 'Jai Pahal', 'Saswat Meher'], 'affiliations': ['Samsung Research Institute Bangalore, India'], 'pdf_title_img': 'assets/pdf/title_img/2503.05397.jpg', 'data': {'categories': ['#architecture', '#healthcare', '#small_models', '#agents'], 'emoji': '🏥', 'ru': {'title': 'Многоагентный медицинский помощник: эффективность без компромиссов', 'desc': 'Статья представляет инновационную систему многоагентного медицинского помощника, работающего на устройстве пользователя. Эта система преодолевает ограничения, связанные с конфиденциальностью, задержками и зависимостью от интернета, характерные для крупных языковых моделей в здравоохранении. Используя меньшие, специализированные агенты, система оптимизирует ресурсы и обеспечивает масштабируемость. Помощник, основанный на модели Qwen Code Instruct 2.5 7B, предлагает комплексное решение для медицинских нужд, включая запись на прием, мониторинг здоровья и напоминания о приеме лекарств.'}, 'en': {'title': 'Empowering Healthcare with On-Device Multi-Agent Systems', 'desc': 'This paper presents a novel on-device multi-agent healthcare assistant that addresses key challenges in using Large Action Models (LAMs) in healthcare, such as privacy, latency, and internet dependency. By employing smaller, task-specific agents, the system enhances resource optimization, scalability, and performance. The assistant offers various healthcare functionalities, including appointment scheduling, health monitoring, and medication reminders, all while maintaining a lightweight design for on-device use. The system demonstrates impressive performance metrics, achieving high RougeL scores for its planning and calling tasks, showcasing its potential for user-centric healthcare solutions.'}, 'zh': {'title': '智能医疗助手：隐私、安全、高效的解决方案', 'desc': '大型行动模型（LAMs）在智能自动化领域取得了革命性进展，但在医疗保健中的应用面临隐私、延迟和对互联网依赖等挑战。本文介绍了一种基于设备的多智能体医疗助手，克服了这些限制。该系统利用较小的、特定任务的智能体来优化资源，确保可扩展性和高性能。我们提出的系统作为一站式医疗解决方案，具备预约、健康监测、用药提醒和每日健康报告等功能。'}}}, {'id': 'https://huggingface.co/papers/2503.09600', 'title': 'MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System', 'url': 'https://huggingface.co/papers/2503.09600', 'abstract': 'Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.', 'score': 1, 'issue_id': 2683, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '4d7741ddea8c8c48', 'authors': ['Jihao Zhao', 'Zhiyuan Ji', 'Zhaoxin Fan', 'Hanyu Wang', 'Simin Niu', 'Bo Tang', 'Feiyu Xiong', 'Zhiyu Li'], 'affiliations': ['Institute for Advanced Algorithms Research, Shanghai, China', 'School of Information, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.09600.jpg', 'data': {'categories': ['#long_context', '#rag', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Умное разбиение текста для улучшения RAG систем', 'desc': 'Эта статья представляет новый подход к разбиению текста на фрагменты в системах RAG. Авторы вводят метод двойной метрики для оценки качества разбиения и показывают ограничения традиционных методов. Они предлагают фреймворк Mixture-of-Chunkers (MoC), использующий языковые модели для создания регулярных выражений для извлечения фрагментов. Эксперименты демонстрируют, что предложенные метрики и фреймворк MoC эффективно решают проблемы разбиения текста и улучшают работу систем RAG.'}, 'en': {'title': 'Enhancing Chunking for Better Retrieval-Augmented Generation', 'desc': 'This paper discusses the importance of text chunking in Retrieval-Augmented Generation (RAG) systems that use large language models (LLMs). It introduces a new evaluation method with two metrics, Boundary Clarity and Chunk Stickiness, to measure the quality of text chunks. The authors point out the limitations of existing chunking methods and propose a new framework called Mixture-of-Chunkers (MoC) that improves chunking precision while maintaining computational efficiency. Through experiments, they show that their metrics and MoC framework enhance the overall performance of RAG systems by effectively addressing chunking challenges.'}, 'zh': {'title': '提升RAG系统性能的分块创新', 'desc': '本文探讨了检索增强生成（RAG）模型中文本分块的重要性。我们提出了一种双指标评估方法，包括边界清晰度和分块粘性，以量化分块质量。通过这一评估方法，我们揭示了传统和语义分块在处理复杂上下文时的局限性，并强调了将大型语言模型（LLMs）整合到分块过程中的必要性。为了解决基于LLMs的方法在计算效率和分块精度之间的权衡，我们设计了一个关注粒度的混合分块器（MoC）框架，显著提升了RAG系统的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.09516', 'title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.09516', 'abstract': 'Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns -- solely through reinforcement learning (RL) -- to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.', 'score': 0, 'issue_id': 2696, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': 'd471c9c6c84abb33', 'authors': ['Bowen Jin', 'Hansi Zeng', 'Zhenrui Yue', 'Dong Wang', 'Hamed Zamani', 'Jiawei Han'], 'affiliations': ['Center for Intelligent Information Retrieval, University of Massachusetts Amherst', 'Department of Computer Science, University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2503.09516.jpg', 'data': {'categories': ['#rl', '#reasoning', '#rag', '#optimization', '#training'], 'emoji': '🔍', 'ru': {'title': 'Обучение языковых моделей эффективному поиску информации', 'desc': 'Эта статья представляет Search-R1 - расширение модели DeepSeek-R1, где большая языковая модель (LLM) учится автономно генерировать поисковые запросы во время пошагового рассуждения с помощью обучения с подкреплением. Search-R1 оптимизирует взаимодействие LLM с поисковой системой, используя маскирование извлеченных токенов для стабильного обучения. Эксперименты показывают значительное улучшение производительности по сравнению с современными базовыми моделями на семи наборах данных для ответов на вопросы. Статья также предоставляет эмпирические данные о методах оптимизации обучения с подкреплением и динамике длины ответов в рассуждениях с использованием поиска.'}, 'en': {'title': 'Empowering LLMs with Autonomous Search through Reinforcement Learning', 'desc': 'This paper presents Search-R1, a novel approach that enhances large language models (LLMs) by enabling them to autonomously generate search queries during reasoning tasks. Unlike traditional methods that rely on supervised data or lack flexibility, Search-R1 employs reinforcement learning (RL) to optimize multi-turn interactions with search engines. The model uses token masking and a simple reward function to stabilize training and improve the quality of retrieved information. Experimental results demonstrate significant performance gains across various question-answering datasets, showcasing the effectiveness of RL in retrieval-augmented reasoning.'}, 'zh': {'title': '提升大型语言模型的检索能力', 'desc': '本文提出了一种名为Search-R1的模型扩展，旨在提高大型语言模型（LLMs）在推理和文本生成中的外部知识获取能力。通过强化学习（RL），Search-R1能够自主生成多个搜索查询，从而实现实时检索和逐步推理。该模型优化了多轮搜索交互，利用检索到的令牌掩蔽技术进行稳定的RL训练，并采用简单的基于结果的奖励函数。实验结果表明，Search-R1在七个问答数据集上的表现显著提升，分别提高了26%、21%和10%。'}}}, {'id': 'https://huggingface.co/papers/2503.09410', 'title': 'Monte Carlo Diffusion for Generalizable Learning-Based RANSAC', 'url': 'https://huggingface.co/papers/2503.09410', 'abstract': 'Random Sample Consensus (RANSAC) is a fundamental approach for robustly estimating parametric models from noisy data. Existing learning-based RANSAC methods utilize deep learning to enhance the robustness of RANSAC against outliers. However, these approaches are trained and tested on the data generated by the same algorithms, leading to limited generalization to out-of-distribution data during inference. Therefore, in this paper, we introduce a novel diffusion-based paradigm that progressively injects noise into ground-truth data, simulating the noisy conditions for training learning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo sampling into the diffusion paradigm, approximating diverse data distributions by introducing different types of randomness at multiple stages. We evaluate our approach in the context of feature matching through comprehensive experiments on the ScanNet and MegaDepth datasets. The experimental results demonstrate that our Monte Carlo diffusion mechanism significantly improves the generalization ability of learning-based RANSAC. We also develop extensive ablation studies that highlight the effectiveness of key components in our framework.', 'score': 0, 'issue_id': 2686, 'pub_date': '2025-03-12', 'pub_date_card': {'ru': '12 марта', 'en': 'March 12', 'zh': '3月12日'}, 'hash': '2ccf3518b5892591', 'authors': ['Jiale Wang', 'Chen Zhao', 'Wei Ke', 'Tong Zhang'], 'affiliations': ['EPFL', 'University of Chinese Academy of Sciences', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2503.09410.jpg', 'data': {'categories': ['#dataset', '#data', '#diffusion', '#cv', '#benchmark', '#optimization'], 'emoji': '🎲', 'ru': {'title': 'Повышение обобщающей способности RANSAC с помощью диффузионной модели Монте-Карло', 'desc': 'Эта статья представляет новый подход к улучшению алгоритма RANSAC с использованием глубокого обучения. Авторы предлагают метод диффузии с применением метода Монте-Карло для генерации разнообразных обучающих данных, симулирующих реальные шумные условия. Эксперименты на наборах данных ScanNet и MegaDepth показывают, что предложенный подход значительно улучшает способность обобщения RANSAC на основе машинного обучения. Исследование включает подробный анализ эффективности ключевых компонентов предложенной системы.'}, 'en': {'title': 'Enhancing RANSAC Robustness with Monte Carlo Diffusion', 'desc': "This paper presents a new method to improve the robustness of learning-based Random Sample Consensus (RANSAC) against noisy data. The authors propose a diffusion-based approach that adds noise to clean data, mimicking real-world conditions for better training. By integrating Monte Carlo sampling, they create diverse data distributions, enhancing the model's ability to generalize to unseen data. Experimental results show that this method significantly boosts the performance of learning-based RANSAC in feature matching tasks."}, 'zh': {'title': '基于扩散的RANSAC：提升鲁棒性与泛化能力', 'desc': '随机样本一致性（RANSAC）是一种用于从噪声数据中稳健估计参数模型的基本方法。现有的基于学习的RANSAC方法利用深度学习增强其对异常值的鲁棒性，但这些方法在相同算法生成的数据上进行训练和测试，导致在推理时对分布外数据的泛化能力有限。本文提出了一种新颖的基于扩散的范式，通过逐步向真实数据中注入噪声，模拟训练基于学习的RANSAC所需的噪声条件。我们还结合了蒙特卡洛采样，以在多个阶段引入不同类型的随机性，从而增强数据的多样性，评估结果表明该方法显著提高了学习型RANSAC的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.08674', 'title': 'Understanding and Mitigating Distribution Shifts For Machine Learning\n  Force Fields', 'url': 'https://huggingface.co/papers/2503.08674', 'abstract': 'Machine Learning Force Fields (MLFFs) are a promising alternative to expensive ab initio quantum mechanical molecular simulations. Given the diversity of chemical spaces that are of interest and the cost of generating new data, it is important to understand how MLFFs generalize beyond their training distributions. In order to characterize and better understand distribution shifts in MLFFs, we conduct diagnostic experiments on chemical datasets, revealing common shifts that pose significant challenges, even for large foundation models trained on extensive data. Based on these observations, we hypothesize that current supervised training methods inadequately regularize MLFFs, resulting in overfitting and learning poor representations of out-of-distribution systems. We then propose two new methods as initial steps for mitigating distribution shifts for MLFFs. Our methods focus on test-time refinement strategies that incur minimal computational cost and do not use expensive ab initio reference labels. The first strategy, based on spectral graph theory, modifies the edges of test graphs to align with graph structures seen during training. Our second strategy improves representations for out-of-distribution systems at test-time by taking gradient steps using an auxiliary objective, such as a cheap physical prior. Our test-time refinement strategies significantly reduce errors on out-of-distribution systems, suggesting that MLFFs are capable of and can move towards modeling diverse chemical spaces, but are not being effectively trained to do so. Our experiments establish clear benchmarks for evaluating the generalization capabilities of the next generation of MLFFs. Our code is available at https://tkreiman.github.io/projects/mlff_distribution_shifts/.', 'score': 0, 'issue_id': 2693, 'pub_date': '2025-03-11', 'pub_date_card': {'ru': '11 марта', 'en': 'March 11', 'zh': '3月11日'}, 'hash': '444f0805a7ed179b', 'authors': ['Tobias Kreiman', 'Aditi S. Krishnapriyan'], 'affiliations': ['LBNL', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.08674.jpg', 'data': {'categories': ['#training', '#benchmark', '#transfer_learning', '#dataset', '#graphs', '#optimization', '#data'], 'emoji': '🧪', 'ru': {'title': 'Преодоление границ обучения: новые стратегии для машинных силовых полей', 'desc': 'Статья посвящена исследованию машинных силовых полей (MLFFs) как альтернативы дорогостоящим квантово-механическим молекулярным симуляциям. Авторы изучают проблемы обобщения MLFFs за пределами обучающих распределений и предлагают два новых метода для смягчения эффектов смещения распределения. Первый метод основан на спектральной теории графов и модифицирует рёбра тестовых графов. Второй метод улучшает представления для систем вне распределения, используя вспомогательную целевую функцию.'}, 'en': {'title': 'Enhancing MLFFs: Bridging the Gap in Chemical Space Generalization', 'desc': 'This paper discusses Machine Learning Force Fields (MLFFs) as a cost-effective alternative to traditional quantum mechanical simulations for molecular modeling. It highlights the challenges MLFFs face when generalizing to chemical spaces that differ from their training data, often leading to overfitting. The authors propose two innovative test-time refinement strategies to improve the performance of MLFFs on out-of-distribution data without relying on expensive reference labels. Their findings suggest that with better training techniques, MLFFs can effectively model a wider range of chemical environments.'}, 'zh': {'title': '提升机器学习力场的泛化能力', 'desc': '机器学习力场（MLFFs）是一种有前景的替代方案，用于昂贵的量子力学分子模拟。本文研究了MLFFs在训练分布之外的泛化能力，发现当前的监督训练方法可能导致过拟合，无法有效学习分布外系统的表示。为了解决这一问题，提出了两种新的测试时优化策略，旨在减少分布转移带来的误差。实验结果表明，这些策略显著提高了在分布外系统上的表现，表明MLFFs有潜力建模多样的化学空间。'}}}, {'id': 'https://huggingface.co/papers/2503.05333', 'title': 'PhysicsGen: Can Generative Models Learn from Images to Predict Complex\n  Physical Relations?', 'url': 'https://huggingface.co/papers/2503.05333', 'abstract': 'The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code http://www.physics-gen.org.', 'score': 0, 'issue_id': 2690, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'c9dae1097c1be3c2', 'authors': ['Martin Spitznagel', 'Jan Vaillant', 'Janis Keuper'], 'affiliations': ['Herrenknecht AG', 'Institute for Machine Learning and Analytics (IMLA), Offenburg University, Germany', 'University of Mannheim, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.05333.jpg', 'data': {'categories': ['#cv', '#diffusion', '#optimization', '#benchmark', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Генеративные модели в физическом моделировании: потенциал и ограничения', 'desc': 'Статья исследует потенциал генеративных моделей в контексте физических симуляций. Авторы предлагают набор данных из 300 тысяч пар изображений и базовые оценки для трех различных задач физического моделирования. Исследование направлено на выяснение способности генеративных моделей изучать сложные физические отношения из пар входных и выходных изображений. Также рассматривается возможность ускорения процесса по сравнению с симуляциями на основе дифференциальных уравнений.'}, 'en': {'title': 'Revolutionizing Physical Simulations with Generative Models', 'desc': 'This paper explores the use of generative learning models for image-to-image translation in the context of physical simulations. It presents a dataset of 300,000 image pairs and establishes a benchmark for evaluating how well these models can learn complex physical relationships. The authors investigate the potential speed improvements that can be achieved by using generative models instead of traditional differential equation-based simulations. However, while some models show promise for faster simulations, they often struggle with maintaining physical accuracy, highlighting the need for new approaches to ensure correctness in physical simulations.'}, 'zh': {'title': '探索生成模型在物理模拟中的潜力', 'desc': '这篇论文探讨了生成学习模型在物理模拟中的应用，尤其是图像到图像的转换能力。研究者提供了一个包含30万对图像的数据集，并针对三种不同的物理模拟任务进行了基准评估。论文提出了两个研究问题：生成模型是否能够从输入输出图像对中学习复杂的物理关系？通过替代基于微分方程的模拟，能实现多大的加速？结果显示，尽管当前模型在加速方面有潜力，但在物理正确性方面存在明显的局限性，因此需要新的方法来确保物理的准确性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (64)', '#agents (73)', '#agi (20)', '#alignment (40)', '#architecture (126)', '#audio (22)', '#benchmark (229)', '#cv (145)', '#data (83)', '#dataset (198)', '#diffusion (102)', '#ethics (24)', '#games (64)', '#graphs (6)', '#hallucinations (29)', '#healthcare (19)', '#inference (78)', '#interpretability (45)', '#leakage (4)', '#long_context (53)', '#low_resource (15)', '#machine_translation (9)', '#math (20)', '#multilingual (22)', '#multimodal (202)', '#open_source (143)', '#optimization (268)', '#plp (5)', '#rag (28)', '#reasoning (143)', '#rl (59)', '#rlhf (25)', '#robotics (25)', '#science (22)', '#security (18)', '#small_models (22)', '#story_generation (7)', '#survey (17)', '#synthetic (52)', '#training (274)', '#transfer_learning (44)', '#video (103)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-31 12:20',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-31 12:20')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-31 12:20')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    