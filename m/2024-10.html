
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 482 papers. October 2024.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Октябрь 2024</span> | <span id="title-articles-count">482 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/m/2024-09.html">⬅️ <span id="prev-date">09.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2024-11.html">➡️ <span id="next-date">11.2024</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Октябрь 2024', 'en': 'October 2024', 'zh': '10月2024年'};
        let feedDateNext = {'ru': '11.2024', 'en': '11/2024', 'zh': '11月2024年'};
        let feedDatePrev = {'ru': '09.2024', 'en': '09/2024', 'zh': '9月2024年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2409.20566', 'title': 'MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning', 'url': 'https://huggingface.co/papers/2409.20566', 'abstract': 'We present MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training, systematically exploring the impact of diverse data mixtures across the entire model training lifecycle. This includes high-quality OCR data and synthetic captions for continual pre-training, as well as an optimized visual instruction-tuning data mixture for supervised fine-tuning. Our models range from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE) variants, and demonstrate that careful data curation and training strategies can yield strong performance even at small scales (1B and 3B). Additionally, we introduce two specialized variants: MM1.5-Video, designed for video understanding, and MM1.5-UI, tailored for mobile UI understanding. Through extensive empirical studies and ablations, we provide detailed insights into the training processes and decisions that inform our final designs, offering valuable guidance for future research in MLLM development.', 'score': 51, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '4ee08e694f7ab69e', 'data': {'categories': ['#science', '#video', '#cv', '#training', '#data', '#optimization', '#benchmark', '#small_models', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Мультимодальное обучение: Сила данных и архитектуры', 'desc': 'MM1.5 - это новое семейство мультимодальных больших языковых моделей (MLLM), разработанных для улучшения понимания изображений с текстом, визуальной референции и обоснования, а также рассуждений по нескольким изображениям. Модель использует подход, ориентированный на данные, систематически исследуя влияние различных наборов данных на протяжении всего жизненного цикла обучения. MM1.5 включает варианты от 1B до 30B параметров, охватывая как плотные, так и MoE-архитектуры, и демонстрирует, что тщательный подбор данных и стратегии обучения могут обеспечить высокую производительность даже при небольших масштабах. Авторы также представляют специализированные варианты для понимания видео и мобильных интерфейсов.'}, 'en': {'title': 'Unlocking Multimodal Understanding with MM1.5', 'desc': 'The paper introduces MM1.5, a new family of multimodal large language models (MLLMs) that improve understanding of images with text, visual referencing, and reasoning across multiple images. It builds on the previous MM1 architecture by using a data-centric approach, focusing on the effects of various data types throughout the training process. The models, which range from 1B to 30B parameters, utilize high-quality OCR data and synthetic captions for continual pre-training, along with optimized data for supervised fine-tuning. Additionally, two specialized versions are presented: MM1.5-Video for video comprehension and MM1.5-UI for mobile user interface understanding, with insights from empirical studies guiding future MLLM research.'}, 'zh': {'title': 'MM1.5：多模态语言模型的新突破', 'desc': '我们介绍了MM1.5，这是一种新型的多模态大型语言模型（MLLM），旨在增强文本丰富的图像理解、视觉指代和基础，以及多图像推理的能力。MM1.5基于MM1架构，采用以数据为中心的训练方法，系统地探索不同数据混合对模型训练生命周期的影响。我们的模型参数范围从10亿到300亿，包括密集型和专家混合（MoE）变体，表明精心的数据策划和训练策略即使在小规模（10亿和30亿）下也能取得强劲的性能。此外，我们还推出了两个专门的变体：MM1.5-Video，专为视频理解设计，以及MM1.5-UI，专为移动用户界面理解量身定制。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.18943', 'title': 'Ruler: A Model-Agnostic Method to Control Generated Length for Large Language Models', 'url': 'https://huggingface.co/papers/2409.18943', 'abstract': "The instruction-following ability of large language models enables humans to interact with AI agents in a natural way. However, when required to generate responses of a specific length, large language models often struggle to meet users' needs due to their inherent difficulty in accurately perceiving numerical constraints. To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model's performance in adhering to specified response lengths. Furthermore, we introduce a novel, model-agnostic approach called Ruler, which employs Meta Length Tokens (MLTs) to enhance the instruction-following ability of large language models under length-constrained instructions. Specifically, Ruler equips LLMs with the ability to generate responses of a specified length based on length constraints within the instructions. Moreover, Ruler can automatically generate appropriate MLT when length constraints are not explicitly provided, demonstrating excellent versatility and generalization. Comprehensive experiments show the effectiveness of Ruler across different LLMs on Target Length Generation Task, e.g., at All Level 27.97 average gain on PM, 29.57 average gain on FM. In addition, we conduct extensive ablation experiments to further substantiate the efficacy and generalization of Ruler. Our code and data is available at https://github.com/Geaming2002/Ruler.", 'score': 26, 'issue_id': 1, 'pub_date': '2024-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '38f8e74fd0ae8f74', 'data': {'categories': ['#dataset', '#training', '#optimization', '#agents', '#benchmark', '#open_source', '#architecture'], 'emoji': '📏', 'ru': {'title': 'Точный контроль длины текста в больших языковых моделях', 'desc': 'Статья представляет новый подход под названием Ruler для улучшения способности больших языковых моделей генерировать ответы заданной длины. Авторы вводят задачу генерации целевой длины (TLG) и метрики точного и гибкого соответствия для оценки производительности моделей. Ruler использует мета-токены длины (MLT) для повышения способности моделей следовать инструкциям с ограничениями по длине. Эксперименты показывают значительное улучшение точности генерации заданной длины при использовании Ruler для различных языковых моделей.'}, 'en': {'title': 'Mastering Response Length with Ruler: A New Approach for LLMs', 'desc': "This paper addresses the challenge that large language models (LLMs) face when generating responses of specific lengths. It introduces the Target Length Generation Task (TLG) to evaluate how well these models can adhere to numerical constraints in their outputs. The authors propose a new method called Ruler, which uses Meta Length Tokens (MLTs) to improve the models' ability to follow length-specific instructions. Experimental results demonstrate that Ruler significantly enhances performance across various LLMs, showing its effectiveness and adaptability in generating responses that meet specified length requirements."}, 'zh': {'title': '提升语言模型的长度控制能力', 'desc': '本文探讨了大型语言模型在生成特定长度响应时的能力。我们提出了目标长度生成任务（TLG），并设计了精确匹配（PM）和灵活匹配（FM）两个指标来评估模型的表现。为了提高模型在长度限制下的指令跟随能力，我们引入了一种新颖的无模型方法Ruler，利用元长度标记（MLT）来增强生成能力。实验结果表明，Ruler在不同大型语言模型上的表现显著提升，验证了其有效性和通用性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19606', 'title': 'Hyper-Connections', 'url': 'https://huggingface.co/papers/2409.19606', 'abstract': 'We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.', 'score': 19, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': 'd4f17cdf9d999616', 'data': {'categories': ['#training', '#optimization', '#architecture', '#cv'], 'emoji': '🔗', 'ru': {'title': 'Гиперсвязи: новый путь к улучшению глубоких нейронных сетей', 'desc': 'Статья представляет гиперсвязи как альтернативу остаточным связям в нейронных сетях. Этот метод решает проблемы, связанные с вариантами остаточных связей, такие как эффект качелей между исчезновением градиента и коллапсом представления. Теоретически, гиперсвязи позволяют сети настраивать силу связей между признаками на разных уровнях и динамически перестраивать слои. Эксперименты показали значительное улучшение производительности при предобучении больших языковых моделей и в задачах компьютерного зрения.'}, 'en': {'title': 'Hyper-Connections: A New Path to Enhanced Neural Network Performance', 'desc': 'This paper introduces hyper-connections as a new method to improve neural networks, particularly as an alternative to traditional residual connections. Hyper-connections help to mitigate issues like gradient vanishing and representation collapse, which can hinder model performance. By allowing dynamic adjustment of feature connections across different layers, this method enhances the learning process. Experiments on large language models and vision tasks show that hyper-connections lead to significant performance gains, suggesting their potential for various AI applications.'}, 'zh': {'title': '超连接：提升深度学习模型性能的新方法', 'desc': '本文提出了一种名为超连接的方法，作为残差连接的有效替代方案。该方法专门解决了残差连接变体中常见的缺点，如梯度消失和表示崩溃之间的摇摆效应。理论上，超连接允许网络调整不同深度特征之间连接的强度，并动态重排层次结构。我们在大型语言模型的预训练实验中发现，超连接在性能上显著优于残差连接，并且在视觉任务中的实验也显示出类似的改进。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19020', 'title': 'DiaSynth -- Synthetic Dialogue Generation Framework', 'url': 'https://huggingface.co/papers/2409.19020', 'abstract': "The scarcity of domain specific dialogue datasets across various domains, from academic topics to everyday conversations, limits the development of dialogue systems for various applications. Existing research is often constrained either by dialogue datasets that are too general or by niche domain dialogue datasets whose scale does not match the required scale for training dialogue systems. To address this gap, we introduce DiaSynth - a synthetic dialogue generation framework capable of generating high quality, contextually rich dialogues across a wide range of domains. Our approach differs from existing frameworks by dynamically generating dialogues that incorporate simulated personas, subtopics, and diverse conversational characteristics, using a Large Language Model (LLM) with Chain of Thought (CoT) reasoning to create contextually rich, domain-specific dialogues that closely mimic natural human interactions. DiaSynth produces tailored dialogues that emulate realistic conversations. We perform our experiments by generating synthetic data using different LLMs and few-shot examples from DialogSum and SAMSum. The pretrained language models fine-tuned on the synthetic data outperform the base models by 16.47%, while the comparison between models fine-tuned on in-domain data and synthetic data shows that the synthetic data is able to capture 90.48% of the distribution of the in-domain data. The quality of the data generated also scales with the size of LLMs. These results validate DiaSynth's potential as a robust alternative to traditional data collection methods.", 'score': 19, 'issue_id': 1, 'pub_date': '2024-09-25', 'pub_date_card': {'ru': '25 сентября', 'en': 'September 25', 'zh': '9月25日'}, 'hash': 'cb6f2fff11f70974', 'data': {'categories': ['#reasoning', '#dataset', '#multilingual', '#training', '#data', '#dialogue', '#synthetic'], 'emoji': '🗣️', 'ru': {'title': 'DiaSynth: революция в создании диалоговых датасетов', 'desc': 'Статья представляет DiaSynth - фреймворк для синтетической генерации диалогов в различных предметных областях. Используя большую языковую модель (LLM) с рассуждениями по цепочке мыслей (CoT), DiaSynth создает контекстно-богатые диалоги, имитирующие естественное человеческое общение. Эксперименты показывают, что модели, дообученные на синтетических данных DiaSynth, превосходят базовые модели на 16.47%, а качество генерируемых данных улучшается с увеличением размера LLM. Результаты подтверждают потенциал DiaSynth как альтернативы традиционным методам сбора диалоговых данных.'}, 'en': {'title': 'DiaSynth: Bridging the Gap in Dialogue Data Generation', 'desc': "This paper presents DiaSynth, a framework for generating synthetic dialogue data to overcome the lack of domain-specific dialogue datasets. It utilizes a Large Language Model (LLM) with Chain of Thought (CoT) reasoning to create contextually rich dialogues that reflect natural human interactions. The generated dialogues incorporate simulated personas and diverse conversational characteristics, making them suitable for various applications. Experiments show that models fine-tuned on synthetic data outperform those trained on base models, demonstrating DiaSynth's effectiveness in producing high-quality dialogue data."}, 'zh': {'title': 'DiaSynth：合成对话生成的新方法', 'desc': '本论文介绍了一种名为DiaSynth的合成对话生成框架，旨在解决特定领域对话数据集稀缺的问题。该框架利用大型语言模型（LLM）和思维链（CoT）推理，动态生成高质量、上下文丰富的对话，涵盖多个领域。通过模拟角色、子主题和多样化的对话特征，DiaSynth能够生成更接近自然人类互动的对话。实验结果表明，使用合成数据微调的预训练语言模型在性能上优于基础模型，显示出DiaSynth作为传统数据收集方法的有力替代方案的潜力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.18747', 'title': 'Cottention: Linear Transformers With Cosine Attention', 'url': 'https://huggingface.co/papers/2409.18747', 'abstract': 'Attention mechanisms, particularly softmax attention, have been instrumental in the success of transformer-based models such as GPT. However, the quadratic memory complexity of softmax attention with respect to sequence length poses significant challenges for processing longer sequences. We introduce Cottention, a novel attention mechanism that replaces the softmax operation with cosine similarity. By leveraging the properties of cosine similarity and rearranging the attention equation, Cottention achieves native linear memory complexity with respect to sequence length, making it inherently more memory-efficient than softmax attention. We demonstrate that Cottention can be reformulated as a recurrent neural network (RNN) with a finite hidden state, allowing for constant memory usage during inference. We evaluate Cottention on both the bidirectional BERT and causal GPT tasks, demonstrating comparable performance to softmax attention while significantly reducing memory requirements. To ensure efficient computation, we develop a custom CUDA kernel for Cottention. Our results show that Cottention is a promising alternative to softmax attention, enabling the processing of longer sequences without sacrificing performance, due to its native linear memory complexity and ability to maintain a constant memory footprint during inference.', 'score': 15, 'issue_id': 1, 'pub_date': '2024-09-27', 'pub_date_card': {'ru': '27 сентября', 'en': 'September 27', 'zh': '9月27日'}, 'hash': '2df6bf8a5fefb135', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Cottention: Эффективное внимание для длинных последовательностей', 'desc': 'Представлен новый механизм внимания Cottention, заменяющий софтмакс косинусным сходством. Cottention имеет линейную сложность по памяти относительно длины последовательности, что делает его более эффективным, чем стандартное внимание. Его можно переформулировать как рекуррентную нейронную сеть с конечным скрытым состоянием, что позволяет использовать постоянный объем памяти при инференсе. Cottention показывает сопоставимую производительность с софтмакс-вниманием на задачах BERT и GPT при значительном снижении требований к памяти.'}, 'en': {'title': 'Cottention: Efficient Attention for Long Sequences', 'desc': 'This paper presents Cottention, a new attention mechanism that replaces the traditional softmax operation with cosine similarity to address the memory challenges of processing long sequences in transformer models. By doing so, Cottention achieves linear memory complexity, making it more efficient than softmax attention, which has quadratic complexity. The authors show that Cottention can be viewed as a recurrent neural network (RNN) with a fixed hidden state, allowing for constant memory usage during inference. Evaluations on BERT and GPT tasks indicate that Cottention performs comparably to softmax attention while significantly reducing memory requirements, supported by a custom CUDA kernel for efficient computation.'}, 'zh': {'title': 'Cottention：高效处理长序列的新选择', 'desc': '本文介绍了一种新的注意力机制Cottention，它用余弦相似度替代了传统的softmax操作。Cottention的内存复杂度与序列长度呈线性关系，这使得它在处理长序列时更加高效。我们将Cottention重新构造为具有有限隐藏状态的递归神经网络（RNN），在推理过程中实现了恒定的内存使用。实验结果表明，Cottention在性能上与softmax注意力相当，但显著降低了内存需求，适合处理更长的序列。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.20551', 'title': 'UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models', 'url': 'https://huggingface.co/papers/2409.20551', 'abstract': 'Previous studies on robotic manipulation are based on a limited understanding of the underlying 3D motion constraints and affordances. To address these challenges, we propose a comprehensive paradigm, termed UniAff, that integrates 3D object-centric manipulation and task understanding in a unified formulation. Specifically, we constructed a dataset labeled with manipulation-related key attributes, comprising 900 articulated objects from 19 categories and 600 tools from 12 categories. Furthermore, we leverage MLLMs to infer object-centric representations for manipulation tasks, including affordance recognition and reasoning about 3D motion constraints. Comprehensive experiments in both simulation and real-world settings indicate that UniAff significantly improves the generalization of robotic manipulation for tools and articulated objects. We hope that UniAff will serve as a general baseline for unified robotic manipulation tasks in the future. Images, videos, dataset, and code are published on the project website at:https://sites.google.com/view/uni-aff/home', 'score': 13, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '57856f760b2f14ce', 'data': {'categories': ['#reasoning', '#dataset', '#cv', '#graphs', '#open_source', '#robotics', '#3d'], 'emoji': '🤖', 'ru': {'title': 'UniAff: Единый подход к роботизированной манипуляции объектами', 'desc': 'UniAff - это новый подход к роботизированной манипуляции, объединяющий понимание 3D-ограничений движения и возможностей объектов. Исследователи создали датасет из 900 шарнирных объектов и 600 инструментов с ключевыми атрибутами для манипуляции. Используя мультимодальные языковые модели, система выводит объектно-ориентированные представления для задач манипуляции. Эксперименты показали значительное улучшение обобщающей способности роботов при работе с инструментами и шарнирными объектами.'}, 'en': {'title': 'UniAff: Unifying 3D Manipulation and Task Understanding for Robots', 'desc': 'This paper introduces a new framework called UniAff, which aims to enhance robotic manipulation by integrating 3D object understanding with task comprehension. The authors created a detailed dataset that includes 900 articulated objects and 600 tools, each labeled with key manipulation attributes. They utilize Multi-Label Learning Models (MLLMs) to derive object-centric representations that help robots recognize affordances and understand 3D motion constraints. Experimental results show that UniAff improves the ability of robots to generalize their manipulation skills across different tools and objects, establishing a new baseline for future research in this area.'}, 'zh': {'title': '统一机器人操作的新范式：UniAff', 'desc': '本研究提出了一种新的机器人操作范式，称为UniAff，旨在整合3D物体中心的操作和任务理解。我们构建了一个包含900个关节物体和600个工具的数据集，标注了与操作相关的关键属性。通过利用多模态大语言模型（MLLMs），我们能够推断出与操作任务相关的物体中心表示，包括可用性识别和3D运动约束推理。实验结果表明，UniAff显著提高了机器人在工具和关节物体操作中的泛化能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.20537', 'title': 'Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers', 'url': 'https://huggingface.co/papers/2409.20537', 'abstract': 'One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (https://liruiw.github.io/hpt/) for code and videos.', 'score': 12, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'e6dbd0e5f73f5c95', 'data': {'categories': ['#dataset', '#agi', '#training', '#optimization', '#transfer_learning', '#open_source', '#architecture', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Универсальное обучение роботов через гетерогенные трансформеры', 'desc': 'Эта статья представляет новый подход к обучению роботов - Heterogeneous Pre-trained Transformers (HPT). HPT предварительно обучает большую общую часть нейронной сети политики для создания универсального представления, независимого от конкретных задач и воплощений робота. Архитектура согласует разнородные входные данные от различных роботов в короткую последовательность токенов. Эксперименты показывают, что HPT превосходит базовые модели и улучшает производительность дообученной политики более чем на 20% на новых задачах.'}, 'en': {'title': 'Unlocking Versatility in Robotics with Heterogeneous Pre-training', 'desc': 'This paper addresses the challenge of training robotic models that can perform well across different types of robots and tasks, a problem known as heterogeneity. The authors introduce Heterogeneous Pre-trained Transformers (HPT), a method that pre-trains a shared neural network to create a flexible representation that works for various robot embodiments and tasks. By using a large amount of diverse data from real-world robots, simulations, and human videos, HPT learns to effectively map sensory inputs to control commands. The results show that HPT significantly improves the performance of fine-tuned policies on new tasks, outperforming existing methods by over 20%.'}, 'zh': {'title': '异质预训练，提升机器人学习能力！', 'desc': '本研究解决了训练通用机器人模型时的异质性问题。以往的机器人学习方法通常只针对特定任务和特定机器人收集数据，导致成本高且容易过拟合。我们提出了异质预训练变换器（HPT），通过在不同机器人和任务的数据上进行预训练，学习任务和机器人无关的共享表示。实验结果表明，HPT在多个基准测试中，针对未见任务的微调策略性能提升超过20%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19952', 'title': 'Image Copy Detection for Diffusion Models', 'url': 'https://huggingface.co/papers/2409.19952', 'abstract': 'Images produced by diffusion models are increasingly popular in digital artwork and visual marketing. However, such generated images might replicate content from existing ones and pose the challenge of content originality. Existing Image Copy Detection (ICD) models, though accurate in detecting hand-crafted replicas, overlook the challenge from diffusion models. This motivates us to introduce ICDiff, the first ICD specialized for diffusion models. To this end, we construct a Diffusion-Replication (D-Rep) dataset and correspondingly propose a novel deep embedding method. D-Rep uses a state-of-the-art diffusion model (Stable Diffusion V1.5) to generate 40, 000 image-replica pairs, which are manually annotated into 6 replication levels ranging from 0 (no replication) to 5 (total replication). Our method, PDF-Embedding, transforms the replication level of each image-replica pair into a probability density function (PDF) as the supervision signal. The intuition is that the probability of neighboring replication levels should be continuous and smooth. Experimental results show that PDF-Embedding surpasses protocol-driven methods and non-PDF choices on the D-Rep test set. Moreover, by utilizing PDF-Embedding, we find that the replication ratios of well-known diffusion models against an open-source gallery range from 10% to 20%.', 'score': 12, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'f2c605d982348d94', 'data': {'categories': ['#dataset', '#cv', '#data', '#benchmark', '#open_source', '#diffusion', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'ICDiff: Инновационный подход к обнаружению копий изображений в эпоху диффузионных моделей', 'desc': 'Статья представляет ICDiff - первую модель обнаружения копий изображений, специализированную для диффузионных моделей. Авторы создали датасет Diffusion-Replication (D-Rep) из 40 000 пар изображение-реплика, сгенерированных с помощью Stable Diffusion V1.5. Предложен новый метод глубокого встраивания PDF-Embedding, преобразующий уровень репликации в функцию плотности вероятности. Эксперименты показали, что PDF-Embedding превосходит другие методы на тестовом наборе D-Rep, а уровень репликации известных диффузионных моделей составляет 10-20%.'}, 'en': {'title': 'Detecting Replication in Diffusion-Generated Images', 'desc': 'This paper introduces ICDiff, a specialized Image Copy Detection (ICD) model designed to address the unique challenges posed by images generated through diffusion models. The authors create a new dataset called Diffusion-Replication (D-Rep), which includes 40,000 image-replica pairs annotated with varying levels of replication. They propose a novel deep embedding method, PDF-Embedding, which uses probability density functions to represent the replication levels, ensuring a smooth transition between levels. Experimental results demonstrate that PDF-Embedding outperforms existing methods, revealing significant replication rates of diffusion models in comparison to a public image gallery.'}, 'zh': {'title': '检测扩散模型生成图像的复制新方法', 'desc': '本论文介绍了一种新的图像复制检测模型ICDiff，专门针对扩散模型生成的图像。我们构建了一个名为D-Rep的数据集，包含40,000对图像复制样本，并将其标注为六个复制级别。我们提出的PDF-Embedding方法将每对图像复制的级别转化为概率密度函数，以此作为监督信号。实验结果表明，PDF-Embedding在检测扩散模型生成的图像复制方面表现优于传统方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19715', 'title': 'Coffee-Gym: An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code', 'url': 'https://huggingface.co/papers/2409.19715', 'abstract': "This paper presents Coffee-Gym, a comprehensive RL environment for training models that provide feedback on code editing. Coffee-Gym includes two major components: (1) Coffee, a dataset containing humans' code edit traces for coding questions and machine-written feedback for editing erroneous code; (2) CoffeeEval, a reward function that faithfully reflects the helpfulness of feedback by assessing the performance of the revised code in unit tests. With them, Coffee-Gym addresses the unavailability of high-quality datasets for training feedback models with RL, and provides more accurate rewards than the SOTA reward model (i.e., GPT-4). By applying Coffee-Gym, we elicit feedback models that outperform baselines in enhancing open-source code LLMs' code editing, making them comparable with closed-source LLMs. We make the dataset and the model checkpoint publicly available.", 'score': 8, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '01303362c974f480', 'data': {'categories': ['#dataset', '#training', '#rl', '#plp', '#benchmark', '#games', '#open_source', '#rlhf'], 'emoji': '☕', 'ru': {'title': 'Coffee-Gym: Революция в обучении моделей обратной связи для редактирования кода', 'desc': 'Coffee-Gym - это новая среда для обучения с подкреплением моделей, предоставляющих обратную связь по редактированию кода. Она включает в себя набор данных Coffee с историей редактирования кода людьми и автоматически сгенерированными отзывами, а также функцию вознаграждения CoffeeEval для оценки полезности обратной связи. Coffee-Gym решает проблему нехватки качественных данных для обучения моделей обратной связи с помощью RL и обеспечивает более точные вознаграждения, чем современные модели. Применение Coffee-Gym позволило создать модели обратной связи, превосходящие базовые в улучшении редактирования кода открытыми языковыми моделями.'}, 'en': {'title': 'Enhancing Code Editing Feedback with Coffee-Gym', 'desc': 'This paper introduces Coffee-Gym, a new reinforcement learning (RL) environment designed to improve code editing feedback models. It consists of two key parts: a dataset of human code edits and machine-generated feedback, and a reward function that evaluates the quality of this feedback based on unit test performance. Coffee-Gym aims to fill the gap in high-quality datasets for training feedback models, offering more precise rewards compared to existing models like GPT-4. The results show that models trained with Coffee-Gym significantly enhance the code editing capabilities of open-source language models, making them competitive with proprietary models.'}, 'zh': {'title': 'Coffee-Gym：提升代码编辑反馈的强化学习环境', 'desc': '本文介绍了Coffee-Gym，这是一个用于训练代码编辑反馈模型的综合强化学习环境。Coffee-Gym包含两个主要部分：Coffee数据集和CoffeeEval奖励函数。Coffee数据集包含人类的代码编辑轨迹和机器生成的反馈，而CoffeeEval则通过评估修订后代码在单元测试中的表现来反映反馈的有效性。通过使用Coffee-Gym，我们能够训练出在开源代码编辑方面优于基线模型的反馈模型，使其与闭源模型相媲美，并且我们将数据集和模型检查点公开。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19808', 'title': 'Can Models Learn Skill Composition from Examples?', 'url': 'https://huggingface.co/papers/2409.19808', 'abstract': 'As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization -- the capacity to combine learned skills in novel ways not encountered during training -- has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the SKILL-MIX evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified k-tuple of language skills. While small models struggled with composing even with k=3, larger models like GPT-4 performed reasonably well with k=5 and 6.   In this paper, we employ a setup akin to SKILL-MIX to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills -- including rhetorical, literary, reasoning, theory of mind, and common sense -- GPT-4 was used to generate text samples that exhibit random subsets of k skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of k, revealed the following findings: (1) Training on combinations of k=2 and 3 skills results in noticeable improvements in the ability to compose texts with k=4 and 5 skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills. This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.', 'score': 8, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '63faa63623faf67c', 'data': {'categories': ['#reasoning', '#agi', '#training', '#benchmark', '#alignment', '#small_models', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'Обучение языковых моделей композиционной генерализации на примерах', 'desc': 'Исследование посвящено композиционной генерализации в больших языковых моделях (LLM). Авторы предложили метод обучения моделей комбинированию различных языковых навыков на основе примеров, сгенерированных GPT-4. Эксперименты показали, что модели, обученные на комбинациях 2-3 навыков, могут успешно составлять тексты с 4-5 навыками. Также наблюдалось улучшение способности комбинировать ранее не встречавшиеся навыки.'}, 'en': {'title': 'Unlocking Compositional Generalization in Smaller Language Models', 'desc': 'This paper investigates how smaller language models can learn to combine different language skills to create coherent text, a process known as compositional generalization. The authors use a method called SKILL-MIX, where models are trained on combinations of language skills and then tested on their ability to use new, unseen skills. Results show that even smaller models can improve their performance on complex tasks by training on simpler combinations of skills. The findings suggest that using diverse and skill-rich training data can significantly boost the compositional abilities of these models.'}, 'zh': {'title': '提升模型组合能力的关键在于技能训练', 'desc': '随着大型语言模型（LLMs）的不断进步，它们在组合泛化方面的能力引起了广泛关注。组合泛化是指模型能够以新颖的方式结合学习到的技能，尤其是在训练数据之外的场景中。研究表明，通过使用多种语言技能的组合进行训练，可以显著提高模型在未见过的技能组合上的文本生成能力。该研究还建议，将富含技能的文本纳入训练中，可以大幅提升模型的组合能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19339', 'title': 'Visual Question Decomposition on Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2409.19339', 'abstract': "Question decomposition has emerged as an effective strategy for prompting Large Language Models (LLMs) to answer complex questions. However, while existing methods primarily focus on unimodal language models, the question decomposition capability of Multimodal Large Language Models (MLLMs) has yet to be explored. To this end, this paper explores visual question decomposition on MLLMs. Specifically, we introduce a systematic evaluation framework including a dataset and several evaluation criteria to assess the quality of the decomposed sub-questions, revealing that existing MLLMs struggle to produce high-quality sub-questions. To address this limitation, we propose a specific finetuning dataset, DecoVQA+, for enhancing the model's question decomposition capability. Aiming at enabling models to perform appropriate selective decomposition, we propose an efficient finetuning pipeline. The finetuning pipeline consists of our proposed dataset and a training objective for selective decomposition. Finetuned MLLMs demonstrate significant improvements in the quality of sub-questions and the policy of selective question decomposition. Additionally, the models also achieve higher accuracy with selective decomposition on VQA benchmark datasets.", 'score': 7, 'issue_id': 1, 'pub_date': '2024-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': 'a6b68fe261496ab7', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#optimization', '#benchmark', '#synthetic', '#multimodal'], 'emoji': '🧩', 'ru': {'title': 'Улучшение декомпозиции визуальных вопросов в мультимодальных ИИ', 'desc': 'Эта статья исследует возможности декомпозиции визуальных вопросов в мультимодальных больших языковых моделях (MLLM). Авторы представляют систематическую систему оценки, включающую набор данных и критерии для оценки качества декомпозированных подвопросов. Они предлагают специальный набор данных DecoVQA+ для улучшения способности модели к декомпозиции вопросов. Исследование также включает эффективный процесс дообучения, который значительно улучшает качество подвопросов и политику выборочной декомпозиции вопросов.'}, 'en': {'title': 'Enhancing Question Decomposition in Multimodal Models', 'desc': "This paper investigates how Multimodal Large Language Models (MLLMs) can break down complex questions into simpler sub-questions, a process known as question decomposition. The authors create a new evaluation framework and dataset, DecoVQA+, to measure the effectiveness of this decomposition in MLLMs, revealing that current models often produce low-quality sub-questions. To improve this, they propose a finetuning pipeline that enhances the models' ability to selectively decompose questions. The results show that finetuned MLLMs significantly improve the quality of sub-questions and perform better on visual question answering tasks."}, 'zh': {'title': '提升多模态模型的问题分解能力', 'desc': '本论文探讨了多模态大型语言模型（MLLMs）在视觉问题分解方面的能力。我们提出了一个系统的评估框架，包括数据集和评估标准，以评估分解子问题的质量。研究发现，现有的MLLMs在生成高质量子问题方面存在困难。为了解决这个问题，我们提出了一个特定的微调数据集DecoVQA+，并设计了一个高效的微调流程，以提高模型的问题分解能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19627', 'title': 'IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding', 'url': 'https://huggingface.co/papers/2409.19627', 'abstract': 'The audio watermarking technique embeds messages into audio and accurately extracts messages from the watermarked audio. Traditional methods develop algorithms based on expert experience to embed watermarks into the time-domain or transform-domain of signals. With the development of deep neural networks, deep learning-based neural audio watermarking has emerged. Compared to traditional algorithms, neural audio watermarking achieves better robustness by considering various attacks during training. However, current neural watermarking methods suffer from low capacity and unsatisfactory imperceptibility. Additionally, the issue of watermark locating, which is extremely important and even more pronounced in neural audio watermarking, has not been adequately studied. In this paper, we design a dual-embedding watermarking model for efficient locating. We also consider the impact of the attack layer on the invertible neural network in robustness training, improving the model to enhance both its reasonableness and stability. Experiments show that the proposed model, IDEAW, can withstand various attacks with higher capacity and more efficient locating ability compared to existing methods.', 'score': 1, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '40af324f96dd481e', 'data': {'categories': ['#audio', '#security', '#training', '#optimization', '#architecture'], 'emoji': '🎵', 'ru': {'title': 'Революция в аудио-водяных знаках: глубокое обучение на страже звука', 'desc': 'Эта статья представляет новый метод водяных знаков для аудио, использующий глубокое обучение. Авторы предлагают модель с двойным встраиванием для эффективного определения местоположения водяных знаков. Они также улучшают устойчивость модели, рассматривая влияние атакующего слоя на обратимую нейронную сеть. Эксперименты показывают, что предложенная модель IDEAW превосходит существующие методы по устойчивости к атакам, емкости и способности определения местоположения водяных знаков.'}, 'en': {'title': 'IDEAW: Enhancing Audio Watermarking with Dual-Embedding and Robustness', 'desc': 'This paper presents a novel dual-embedding watermarking model called IDEAW for audio watermarking using deep learning techniques. The model improves upon traditional methods by enhancing robustness against various attacks while also addressing the critical issue of watermark locating. By incorporating an attack layer into the training of an invertible neural network, the model achieves better stability and capacity for embedding messages. Experimental results demonstrate that IDEAW outperforms existing neural audio watermarking methods in terms of robustness, capacity, and locating efficiency.'}, 'zh': {'title': '双重嵌入模型提升音频水印的鲁棒性与定位能力', 'desc': '本文介绍了一种音频水印技术，能够将信息嵌入音频中并准确提取。传统方法依赖专家经验开发算法，而深度学习的出现使得神经音频水印技术得以发展。与传统算法相比，神经音频水印在训练过程中考虑了多种攻击，从而实现了更好的鲁棒性。然而，现有方法在容量和隐蔽性方面存在不足，且水印定位问题尚未得到充分研究。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19951', 'title': 'Law of the Weakest Link: Cross Capabilities of Large Language Models', 'url': 'https://huggingface.co/papers/2409.19951', 'abstract': 'The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which we term cross capabilities. To systematically explore this concept, we first define seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, we introduce CrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, we involve expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. Our findings reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the "Law of the Weakest Link," where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities a critical priority for future research to optimize performance in complex, multi-dimensional scenarios.', 'score': 53, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '8ea79f1e9cb3662c', 'data': {'categories': ['#reasoning', '#evaluation', '#training', '#interpretability', '#benchmark', '#architecture'], 'emoji': '🔗', 'ru': {'title': 'Кросс-возможности LLM: слабое звено определяет успех', 'desc': "Статья представляет новый подход к оценке больших языковых моделей (LLM), фокусируясь на пересечении нескольких способностей, названном 'кросс-возможностями'. Авторы разработали бенчмарк CrossEval, включающий 1400 аннотированных промптов для оценки семи индивидуальных и семи кросс-возможностей. Исследование выявило 'закон слабейшего звена', согласно которому производительность в задачах с кросс-возможностями ограничена наиболее слабой компонентой. Результаты подчеркивают необходимость улучшения слабейших возможностей LLM для оптимизации их работы в сложных многомерных сценариях."}, 'en': {'title': 'Unlocking the Power of Cross Capabilities in LLMs', 'desc': 'This paper discusses the limitations of Large Language Models (LLMs) when performing tasks that require multiple skills, which the authors call cross capabilities. They define seven individual capabilities and create pairs to form seven cross capabilities, supported by a detailed taxonomy. The authors introduce CrossEval, a benchmark with 1,400 prompts to evaluate these capabilities, using expert annotators to assess model responses. Their findings indicate that LLMs often perform poorly in cross-capability tasks, revealing that the weakest skill significantly limits overall performance, suggesting a need for future research to strengthen these weak areas.'}, 'zh': {'title': '提升交叉能力，优化模型表现', 'desc': '这篇论文探讨了大型语言模型（LLMs）在多种能力交叉应用中的表现。研究者定义了七种核心能力，并将其配对形成七种常见的交叉能力。通过构建CrossEval基准，评估了1400个人工标注的提示，发现当前的LLMs在交叉能力任务中表现不佳，常常受到最弱能力的限制。研究结果强调了识别和提升最弱能力的重要性，以优化模型在复杂任务中的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00531', 'title': 'TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices', 'url': 'https://huggingface.co/papers/2410.00531', 'abstract': "Large model inference is shifting from cloud to edge due to concerns about the privacy of user interaction data. However, edge devices often struggle with limited computing power, memory, and bandwidth, requiring collaboration across multiple devices to run and speed up LLM inference. Pipeline parallelism, the mainstream solution, is inefficient for single-user scenarios, while tensor parallelism struggles with frequent communications. In this paper, we argue that tensor parallelism can be more effective than pipeline on low-resource devices, and present a compute- and memory-efficient tensor parallel inference system, named TPI-LLM, to serve 70B-scale models. TPI-LLM keeps sensitive raw data local in the users' devices and introduces a sliding window memory scheduler to dynamically manage layer weights during inference, with disk I/O latency overlapped with the computation and communication. This allows larger models to run smoothly on memory-limited devices. We analyze the communication bottleneck and find that link latency, not bandwidth, emerges as the main issue, so a star-based allreduce algorithm is implemented. Through extensive experiments on both emulated and real testbeds, TPI-LLM demonstrated over 80% less time-to-first-token and token latency compared to Accelerate, and over 90% compared to Transformers and Galaxy, while cutting the peak memory footprint of Llama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models.", 'score': 28, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '993cc3dc03d1791f', 'data': {'categories': ['#optimization', '#small_models', '#low_resource', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Большие языковые модели на малых устройствах: эффективность и конфиденциальность', 'desc': 'В статье представлена система TPI-LLM для эффективного выполнения крупных языковых моделей на устройствах с ограниченными ресурсами. Она использует тензорный параллелизм и динамическое управление памятью для запуска 70-миллиардных моделей на edge-устройствах. TPI-LLM значительно снижает задержку и потребление памяти по сравнению с существующими решениями. Система обеспечивает конфиденциальность данных пользователя, сохраняя их локально на устройстве.'}, 'en': {'title': 'Efficient Tensor Parallelism for Edge Inference', 'desc': 'This paper discusses the shift of large model inference from cloud to edge devices due to privacy concerns. It highlights the challenges faced by edge devices, such as limited computing power and memory, and critiques existing methods like pipeline and tensor parallelism. The authors propose TPI-LLM, a tensor parallel inference system that efficiently manages memory and computation, allowing large models to run on low-resource devices. Their experiments show that TPI-LLM significantly reduces latency and memory usage compared to other systems, making it a viable solution for deploying large language models on edge devices.'}, 'zh': {'title': '边缘设备上的高效大模型推理', 'desc': '本论文讨论了大模型推理从云端转向边缘设备的原因，主要是为了保护用户隐私。由于边缘设备的计算能力、内存和带宽有限，多个设备之间的协作变得必要。我们提出了一种名为TPI-LLM的张量并行推理系统，能够在资源有限的设备上高效运行70B规模的模型。通过动态管理层权重和优化通信方式，TPI-LLM显著减少了推理时间和内存占用，提升了大模型在边缘设备上的运行效率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.17912', 'title': 'Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect', 'url': 'https://huggingface.co/papers/2409.17912', 'abstract': 'We introduce Atlas-Chat, the first-ever collection of large language models specifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as Darija, we construct our instruction dataset by consolidating existing Darija language resources, creating novel datasets both manually and synthetically, and translating English instructions with stringent quality control. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit superior ability in following Darija instructions and performing standard NLP tasks. Notably, our models outperform both state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13% performance boost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation suite for Darija covering both discriminative and generative tasks. Furthermore, we perform an experimental analysis of various fine-tuning strategies and base model choices to determine optimal configurations. All our resources are publicly accessible, and we believe our work offers comprehensive design methodologies of instruction-tuning for low-resource language variants, which are often neglected in favor of data-rich languages by contemporary LLMs.', 'score': 20, 'issue_id': 1, 'pub_date': '2024-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '99e46689e9b93364', 'data': {'categories': ['#dataset', '#multilingual', '#training', '#synthetic', '#machine_translation', '#data', '#optimization', '#benchmark', '#open_source', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Прорыв в обработке диалектного арабского языка с помощью специализированных языковых моделей', 'desc': 'Исследователи представили Atlas-Chat - первую коллекцию больших языковых моделей, специально разработанных для диалектного арабского языка, в частности для марокканского диалекта Дарижа. Они создали набор данных для обучения, объединив существующие ресурсы Дарижа, создав новые наборы данных вручную и синтетически, а также переведя английские инструкции с строгим контролем качества. Модели Atlas-Chat-9B и 2B, дообученные на этом наборе данных, демонстрируют превосходную способность следовать инструкциям на Дарижа и выполнять стандартные задачи обработки естественного языка. Исследователи также провели экспериментальный анализ различных стратегий дообучения и выбора базовых моделей для определения оптимальных конфигураций.'}, 'en': {'title': 'Empowering Dialectal Arabic with Atlas-Chat!', 'desc': 'Atlas-Chat is a groundbreaking collection of large language models tailored for dialectal Arabic, specifically Moroccan Arabic or Darija. The models, Atlas-Chat-9B and 2B, were fine-tuned using a carefully constructed dataset that includes both existing resources and newly created data, ensuring high-quality instruction following. These models demonstrate exceptional performance in natural language processing tasks, surpassing other advanced models like LLaMa and Jais by a significant margin. The research also explores various fine-tuning strategies, providing valuable insights for developing models for low-resource languages, which are often overlooked in the field of machine learning.'}, 'zh': {'title': 'Atlas-Chat：为摩洛哥阿拉伯语量身定制的语言模型', 'desc': '我们介绍了Atlas-Chat，这是首个专门为方言阿拉伯语开发的大型语言模型集合。我们重点关注摩洛哥阿拉伯语（Darija），通过整合现有的Darija语言资源，手动和合成创建新数据集，并严格控制翻译质量来构建指令数据集。经过微调的Atlas-Chat-9B和2B模型在遵循Darija指令和执行标准自然语言处理任务方面表现出色，超越了现有的阿拉伯语专用大型语言模型。我们的研究为低资源语言变体的指令调优提供了全面的设计方法论，填补了当代大型语言模型在这方面的空白。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19603', 'title': 'One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos', 'url': 'https://huggingface.co/papers/2409.19603', 'abstract': "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.", 'score': 17, 'issue_id': 1, 'pub_date': '2024-09-29', 'pub_date_card': {'ru': '29 сентября', 'en': 'September 29', 'zh': '9月29日'}, 'hash': '911a506d3af5683e', 'data': {'categories': ['#reasoning', '#video', '#graphs', '#benchmark', '#open_source', '#architecture', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'VideoLISA: Разумная сегментация видео по языковым инструкциям', 'desc': 'VideoLISA - это мультимодальная большая языковая модель для видео, предназначенная для сегментации объектов на основе языковых инструкций. Она объединяет возможности рассуждения больших языковых моделей с моделью Segment Anything для генерации согласованных во времени масок сегментации. VideoLISA использует стратегию разреженно-плотной выборки для баланса между временным контекстом и пространственными деталями. Модель также применяет подход One-Token-Seg-All со специальным токеном <TRK> для сегментации и отслеживания объектов в нескольких кадрах.'}, 'en': {'title': 'VideoLISA: Revolutionizing Video Segmentation with Language Instructions', 'desc': 'VideoLISA is a multimodal large language model specifically designed for video segmentation tasks that require reasoning based on language instructions. It combines the reasoning abilities of large language models with the Segment Anything Model to create consistent segmentation masks over time. Unlike traditional image-based methods, VideoLISA effectively handles the complexities of video data by using a Sparse Dense Sampling strategy, which optimizes both temporal and spatial information. The model also introduces a One-Token-Seg-All approach, allowing it to track and segment objects across multiple frames, demonstrating its effectiveness in complex video object segmentation tasks.'}, 'zh': {'title': '视频分割的新突破：VideoLISA', 'desc': 'VideoLISA是一种基于视频的多模态大语言模型，旨在解决视频中的语言指令推理分割问题。它结合了大语言模型的推理能力和世界知识，并通过Segment Anything Model增强，生成基于语言指令的时间一致性分割掩码。与现有的基于图像的方法相比，VideoLISA通过引入稀疏密集采样策略，平衡了时间上下文和空间细节，从而有效应对视频任务中的时间动态理解。该模型还提出了一种One-Token-Seg-All方法，利用特定设计的<TRK>标记，实现跨多个帧的对象分割和跟踪。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00890', 'title': 'Flex3D: Feed-Forward 3D Generation With Flexible Reconstruction Model And Input View Curation', 'url': 'https://huggingface.co/papers/2410.00890', 'abstract': 'Generating high-quality 3D content from text, single images, or sparse view images remains a challenging task with broad applications.Existing methods typically employ multi-view diffusion models to synthesize multi-view images, followed by a feed-forward process for 3D reconstruction. However, these approaches are often constrained by a small and fixed number of input views, limiting their ability to capture diverse viewpoints and, even worse, leading to suboptimal generation results if the synthesized views are of poor quality. To address these limitations, we propose Flex3D, a novel two-stage framework capable of leveraging an arbitrary number of high-quality input views. The first stage consists of a candidate view generation and curation pipeline. We employ a fine-tuned multi-view image diffusion model and a video diffusion model to generate a pool of candidate views, enabling a rich representation of the target 3D object. Subsequently, a view selection pipeline filters these views based on quality and consistency, ensuring that only the high-quality and reliable views are used for reconstruction. In the second stage, the curated views are fed into a Flexible Reconstruction Model (FlexRM), built upon a transformer architecture that can effectively process an arbitrary number of inputs. FlemRM directly outputs 3D Gaussian points leveraging a tri-plane representation, enabling efficient and detailed 3D generation. Through extensive exploration of design and training strategies, we optimize FlexRM to achieve superior performance in both reconstruction and generation tasks. Our results demonstrate that Flex3D achieves state-of-the-art performance, with a user study winning rate of over 92% in 3D generation tasks when compared to several of the latest feed-forward 3D generative models.', 'score': 17, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '8eb332defa865614', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#architecture', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Гибкая генерация 3D-контента с произвольным числом ракурсов', 'desc': 'Flex3D - это новая двухэтапная система для создания 3D-контента из текста или изображений. На первом этапе генерируется и отбирается набор высококачественных видов объекта с помощью диффузионных моделей. На втором этапе отобранные виды обрабатываются трансформерной моделью FlexRM, которая напрямую генерирует 3D-представление в виде гауссовых точек. Система может работать с произвольным количеством входных видов, что позволяет получать более детальные и качественные 3D-модели.'}, 'en': {'title': 'Flex3D: Revolutionizing 3D Content Generation with Flexible View Inputs', 'desc': 'The paper introduces Flex3D, a two-stage framework designed to generate high-quality 3D content from various input sources like text and images. It overcomes limitations of existing methods by allowing an arbitrary number of high-quality input views, enhancing the diversity and quality of the generated 3D representations. The first stage involves generating and curating candidate views using advanced diffusion models, while the second stage employs a Flexible Reconstruction Model (FlexRM) based on transformer architecture for efficient 3D reconstruction. Flex3D demonstrates superior performance in 3D generation tasks, achieving a winning rate of over 92% in user studies compared to other models.'}, 'zh': {'title': 'Flex3D：灵活高效的3D内容生成', 'desc': '本文提出了一种名为Flex3D的新框架，用于从文本、单幅图像或稀疏视图图像生成高质量的3D内容。该框架分为两个阶段：第一阶段生成候选视图并进行筛选，确保使用高质量和一致性的视图进行重建。第二阶段使用基于变换器架构的灵活重建模型（FlexRM），能够处理任意数量的输入，并直接输出3D高斯点。实验结果表明，Flex3D在3D生成任务中表现优异，用户研究的胜率超过92%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19946', 'title': 'Illustrious: an Open Advanced Illustration Model', 'url': 'https://huggingface.co/papers/2409.19946', 'abstract': 'In this work, we share the insights for achieving state-of-the-art quality in our text-to-image anime image generative model, called Illustrious. To achieve high resolution, dynamic color range images, and high restoration ability, we focus on three critical approaches for model improvement. First, we delve into the significance of the batch size and dropout control, which enables faster learning of controllable token based concept activations. Second, we increase the training resolution of images, affecting the accurate depiction of character anatomy in much higher resolution, extending its generation capability over 20MP with proper methods. Finally, we propose the refined multi-level captions, covering all tags and various natural language captions as a critical factor for model development. Through extensive analysis and experiments, Illustrious demonstrates state-of-the-art performance in terms of animation style, outperforming widely-used models in illustration domains, propelling easier customization and personalization with nature of open source. We plan to publicly release updated Illustrious model series sequentially as well as sustainable plans for improvements.', 'score': 13, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'ac67234852b7f9e1', 'data': {'categories': ['#cv', '#training', '#optimization', '#games', '#open_source', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Illustrious: прорыв в генерации аниме-изображений с помощью ИИ', 'desc': 'Статья описывает подходы к улучшению качества генеративной модели текст-в-изображение для аниме-иллюстраций под названием Illustrious. Авторы фокусируются на трех ключевых аспектах: оптимизации размера батча и контроле дропаута, увеличении разрешения обучающих изображений и использовании многоуровневых подписей. Модель демонстрирует передовые результаты в генерации аниме-стиля, превосходя широко используемые модели в области иллюстраций. Illustrious планируется выпустить в открытый доступ с дальнейшими улучшениями.'}, 'en': {'title': 'Illustrious: Elevating Anime Image Generation to New Heights!', 'desc': 'This paper presents the Illustrious model, a text-to-image generative model specifically designed for creating high-quality anime images. The authors emphasize three key strategies for enhancing model performance: optimizing batch size and dropout for improved learning, increasing training image resolution for better character anatomy representation, and utilizing refined multi-level captions for comprehensive training data. The results show that Illustrious achieves superior performance in animation style compared to existing models, allowing for greater customization and personalization. The authors also plan to release updates and improvements to the model in an open-source format.'}, 'zh': {'title': 'Illustrious：动漫生成模型的新高度', 'desc': '本文介绍了一种名为Illustrious的文本到图像动漫生成模型，旨在实现最先进的图像质量。我们通过优化批量大小和丢弃率控制，加快了可控标记概念激活的学习速度。其次，通过提高图像训练分辨率，模型能够更准确地描绘角色解剖结构，生成超过20MP的高分辨率图像。最后，我们提出了精细的多层次标题，涵盖所有标签和各种自然语言标题，作为模型发展的关键因素。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00337', 'title': 'SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs', 'url': 'https://huggingface.co/papers/2410.00337', 'abstract': 'The advancement of autonomous driving is increasingly reliant on high-quality annotated datasets, especially in the task of 3D occupancy prediction, where the occupancy labels require dense 3D annotation with significant human effort. In this paper, we propose SyntheOcc, which denotes a diffusion model that Synthesize photorealistic and geometric-controlled images by conditioning Occupancy labels in driving scenarios. This yields an unlimited amount of diverse, annotated, and controllable datasets for applications like training perception models and simulation. SyntheOcc addresses the critical challenge of how to efficiently encode 3D geometric information as conditional input to a 2D diffusion model. Our approach innovatively incorporates 3D semantic multi-plane images (MPIs) to provide comprehensive and spatially aligned 3D scene descriptions for conditioning. As a result, SyntheOcc can generate photorealistic multi-view images and videos that faithfully align with the given geometric labels (semantics in 3D voxel space). Extensive qualitative and quantitative evaluations of SyntheOcc on the nuScenes dataset prove its effectiveness in generating controllable occupancy datasets that serve as an effective data augmentation to perception models.', 'score': 10, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': 'a20c1db238f8e7e2', 'data': {'categories': ['#video', '#dataset', '#cv', '#training', '#graphs', '#diffusion', '#synthetic', '#3d'], 'emoji': '🚗', 'ru': {'title': 'Синтез реалистичных изображений для беспилотных автомобилей с помощью 3D меток', 'desc': 'SyntheOcc - это модель диффузии, которая синтезирует фотореалистичные изображения для автономного вождения, используя метки занятости в 3D пространстве. Модель решает проблему эффективного кодирования 3D геометрической информации в качестве условного входа для 2D модели диффузии. SyntheOcc использует семантические многоплоскостные изображения (MPI) для предоставления полного описания 3D сцены. Эксперименты на наборе данных nuScenes показывают эффективность SyntheOcc в генерации контролируемых наборов данных для улучшения моделей восприятия.'}, 'en': {'title': 'SyntheOcc: Revolutionizing 3D Occupancy Data Generation for Autonomous Driving', 'desc': 'This paper introduces SyntheOcc, a diffusion model designed to create photorealistic images conditioned on occupancy labels for autonomous driving scenarios. It addresses the challenge of encoding 3D geometric information into a 2D model by using 3D semantic multi-plane images (MPIs) for better spatial alignment. The generated images and videos are not only photorealistic but also accurately reflect the underlying 3D occupancy data. Evaluations on the nuScenes dataset demonstrate that SyntheOcc effectively produces diverse and controllable datasets, enhancing data augmentation for perception models.'}, 'zh': {'title': 'SyntheOcc：生成可控的3D占用数据集', 'desc': '本论文提出了一种名为SyntheOcc的扩散模型，用于合成高质量的3D占用预测数据集。该模型通过条件化占用标签，生成逼真的图像和几何控制图像，从而提供无限多样的标注数据集。SyntheOcc创新性地结合了3D语义多平面图像（MPI），为2D扩散模型提供了全面且空间对齐的3D场景描述。通过在nuScenes数据集上的广泛评估，证明了SyntheOcc在生成可控占用数据集方面的有效性，能够有效增强感知模型的训练。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00086', 'title': 'ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer', 'url': 'https://huggingface.co/papers/2410.00086', 'abstract': 'Diffusion models have emerged as a powerful generative technology and have been found to be applicable in various scenarios. Most existing foundational diffusion models are primarily designed for text-guided visual generation and do not support multi-modal conditions, which are essential for many visual editing tasks. This limitation prevents these foundational diffusion models from serving as a unified model in the field of visual generation, like GPT-4 in the natural language processing field. In this work, we propose ACE, an All-round Creator and Editor, which achieves comparable performance compared to those expert models in a wide range of visual generation tasks. To achieve this goal, we first introduce a unified condition format termed Long-context Condition Unit (LCU), and propose a novel Transformer-based diffusion model that uses LCU as input, aiming for joint training across various generation and editing tasks. Furthermore, we propose an efficient data collection approach to address the issue of the absence of available training data. It involves acquiring pairwise images with synthesis-based or clustering-based pipelines and supplying these pairs with accurate textual instructions by leveraging a fine-tuned multi-modal large language model. To comprehensively evaluate the performance of our model, we establish a benchmark of manually annotated pairs data across a variety of visual generation tasks. The extensive experimental results demonstrate the superiority of our model in visual generation fields. Thanks to the all-in-one capabilities of our model, we can easily build a multi-modal chat system that responds to any interactive request for image creation using a single model to serve as the backend, avoiding the cumbersome pipeline typically employed in visual agents. Code and models will be available on the project page: https://ali-vilab.github.io/ace-page/.', 'score': 10, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '21af441f6b636cb3', 'data': {'categories': ['#dataset', '#cv', '#long_context', '#training', '#data', '#benchmark', '#open_source', '#diffusion', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'ACE: универсальный инструмент для создания и редактирования изображений', 'desc': 'Исследователи представили ACE - универсальную модель для генерации и редактирования изображений. Модель использует унифицированный формат условий Long-context Condition Unit (LCU) и основана на архитектуре Transformer. Для обучения был разработан эффективный подход сбора данных с использованием мультимодальной языковой модели. ACE показывает высокую производительность в широком спектре задач визуальной генерации, что позволяет создать мультимодальную чат-систему на основе единой модели.'}, 'en': {'title': 'ACE: Unifying Visual Generation and Editing with Multi-Modal Diffusion Models', 'desc': "This paper introduces ACE, a novel diffusion model designed for multi-modal visual generation and editing tasks. Unlike existing models that focus mainly on text-guided image generation, ACE utilizes a Long-context Condition Unit (LCU) to unify various input conditions for improved performance. The authors also present an innovative data collection method to generate training pairs of images and textual instructions, enhancing the model's training process. Experimental results show that ACE outperforms existing models, making it a versatile tool for creating a multi-modal chat system for image generation."}, 'zh': {'title': 'ACE：全能的视觉生成与编辑模型', 'desc': '扩散模型作为一种强大的生成技术，已在多个场景中得到应用。现有的基础扩散模型主要用于文本引导的视觉生成，缺乏对多模态条件的支持，这限制了其在视觉编辑任务中的应用。为了解决这个问题，我们提出了ACE，一个全能的创作与编辑模型，能够在多种视觉生成任务中与专家模型相媲美。我们引入了统一的条件格式Long-context Condition Unit（LCU），并提出了一种基于Transformer的扩散模型，旨在实现多任务的联合训练。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00418', 'title': 'Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration', 'url': 'https://huggingface.co/papers/2410.00418', 'abstract': 'Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e.g., PSNR, SSIM) and by perceptual quality measures (e.g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality. To achieve this goal, current methods typically attempt to sample from the posterior distribution, or to optimize a weighted sum of a distortion loss (e.g., MSE) and a perceptual quality loss (e.g., GAN). Unlike previous works, this paper is concerned specifically with the optimal estimator that minimizes the MSE under a constraint of perfect perceptual index, namely where the distribution of the reconstructed images is equal to that of the ground-truth ones. A recent theoretical result shows that such an estimator can be constructed by optimally transporting the posterior mean prediction (MMSE estimate) to the distribution of the ground-truth images. Inspired by this result, we introduce Posterior-Mean Rectified Flow (PMRF), a simple yet highly effective algorithm that approximates this optimal estimator. In particular, PMRF first predicts the posterior mean, and then transports the result to a high-quality image using a rectified flow model that approximates the desired optimal transport map. We investigate the theoretical utility of PMRF and demonstrate that it consistently outperforms previous methods on a variety of image restoration tasks.', 'score': 9, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': 'cd355be6fba7c501', 'data': {'categories': ['#cv', '#math', '#optimization', '#benchmark', '#diffusion', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Оптимальное восстановление изображений с помощью выпрямленных потоков', 'desc': 'Эта статья представляет новый алгоритм для восстановления изображений под названием Posterior-Mean Rectified Flow (PMRF). PMRF сначала предсказывает среднее значение апостериорного распределения, а затем использует модель выпрямленного потока для переноса результата в высококачественное изображение. Этот метод основан на теоретическом результате, показывающем, что оптимальная оценка может быть получена путем оптимального транспорта среднего значения апостериорного распределения в распределение оригинальных изображений. PMRF превосходит существующие методы в различных задачах восстановления изображений.'}, 'en': {'title': 'Achieving Perfect Perception in Image Restoration with PMRF', 'desc': 'This paper presents a new approach to photo-realistic image restoration that focuses on minimizing mean squared error (MSE) while ensuring perceptual quality. The authors introduce a method called Posterior-Mean Rectified Flow (PMRF), which first predicts the posterior mean of the image and then optimally transports this prediction to match the distribution of high-quality images. This approach is based on a theoretical framework that allows for the construction of an optimal estimator under the constraint of perfect perceptual quality. The results show that PMRF outperforms existing algorithms across various image restoration tasks, demonstrating its effectiveness in achieving both low distortion and high perceptual quality.'}, 'zh': {'title': '最优图像恢复：后验均值校正流', 'desc': '本文提出了一种新的图像恢复算法，称为后验均值校正流（PMRF），旨在在保持感知质量的同时最小化均方误差（MSE）。与传统方法不同，PMRF通过优化后验均值预测，并将其传输到真实图像的分布上，从而实现最佳估计。研究表明，PMRF在多种图像恢复任务中表现优于现有方法，具有较高的有效性。该算法的理论基础是通过最优传输理论构建的，确保恢复图像的分布与真实图像一致。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.20018', 'title': 'Visual Context Window Extension: A New Perspective for Long Video Understanding', 'url': 'https://huggingface.co/papers/2409.20018', 'abstract': 'Large Multimodal Models (LMMs) have demonstrated impressive performance in short video understanding tasks but face great challenges when applied to long video understanding. In contrast, Large Language Models (LLMs) exhibit outstanding capabilities in modeling long texts. Existing work attempts to address this issue by introducing long video-text pairs during training. However, these approaches require substantial computational and data resources. In this paper, we tackle the challenge of long video understanding from the perspective of context windows, aiming to apply LMMs to long video tasks without retraining on long video datasets. We first conduct an in-depth analysis of why pretrained LMMs struggle to understand lengthy video content, identifying that discrepancies between visual and language modalities lead to different context windows for visual and language tokens, making it difficult to directly extend the visual tokens to match the language context window. Based on this, we propose to adapt LMMs for long video understanding tasks by extending the visual context window, eliminating the need for retraining on large scalelong video datasets. To further mitigate the significant memory consumption caused by long sequences, we introduce a progressive pooling inference strategy that selectively adjusts the spatial resolution of frame embeddings, reducing the number of visual tokens while retaining important spatial information. Across multiple long video understanding benchmarks, our method consistently improves the performance as the number of video frames increases. On the MLVU benchmark, our method outperforms GPT-4o, even though our model size is only 7B. Additionally, in the 256-frame setting, our method reduces memory usage by approximately 45% compared to the baseline, without introducing any performance loss.', 'score': 8, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '81b41650dd87a792', 'data': {'categories': ['#video', '#long_context', '#inference', '#optimization', '#benchmark', '#small_models', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Эффективное понимание длинных видео без переобучения', 'desc': 'Статья представляет новый подход к пониманию длинных видео с использованием мультимодальных моделей (LMM). Авторы предлагают расширить контекстное окно для визуальных токенов, чтобы улучшить обработку длинных видео без переобучения на больших наборах данных. Они также вводят стратегию прогрессивного пулинга для снижения потребления памяти при сохранении важной пространственной информации. Метод показывает улучшение производительности на нескольких бенчмарках для длинных видео, превосходя GPT-4o на MLVU, несмотря на меньший размер модели.'}, 'en': {'title': 'Enhancing Long Video Understanding with Efficient Context Adaptation', 'desc': 'This paper addresses the challenges faced by Large Multimodal Models (LMMs) in understanding long videos, which differ from their success in short video tasks. The authors analyze the issues arising from mismatched context windows between visual and language tokens, which hinder effective processing of lengthy video content. They propose a solution that extends the visual context window without the need for retraining on extensive long video datasets. Additionally, a progressive pooling inference strategy is introduced to reduce memory consumption while maintaining essential spatial information, leading to improved performance on long video understanding benchmarks.'}, 'zh': {'title': '提升长视频理解的多模态模型新策略', 'desc': '本文探讨了如何提高大型多模态模型（LMMs）在长视频理解任务中的表现。我们发现，视觉和语言模态之间的差异导致了不同的上下文窗口，使得直接扩展视觉标记以匹配语言上下文窗口变得困难。为了解决这个问题，我们提出了一种扩展视觉上下文窗口的方法，避免了对大型长视频数据集的重新训练。此外，我们还引入了一种渐进池化推理策略，以减少长序列带来的内存消耗，同时保留重要的空间信息。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.20563', 'title': 'DressRecon: Freeform 4D Human Reconstruction from Monocular Video', 'url': 'https://huggingface.co/papers/2409.20563', 'abstract': 'We present a method to reconstruct time-consistent human body models from monocular videos, focusing on extremely loose clothing or handheld object interactions. Prior work in human reconstruction is either limited to tight clothing with no object interactions, or requires calibrated multi-view captures or personalized template scans which are costly to collect at scale. Our key insight for high-quality yet flexible reconstruction is the careful combination of generic human priors about articulated body shape (learned from large-scale training data) with video-specific articulated "bag-of-bones" deformation (fit to a single video via test-time optimization). We accomplish this by learning a neural implicit model that disentangles body versus clothing deformations as separate motion model layers. To capture subtle geometry of clothing, we leverage image-based priors such as human body pose, surface normals, and optical flow during optimization. The resulting neural fields can be extracted into time-consistent meshes, or further optimized as explicit 3D Gaussians for high-fidelity interactive rendering. On datasets with highly challenging clothing deformations and object interactions, DressRecon yields higher-fidelity 3D reconstructions than prior art. Project page: https://jefftan969.github.io/dressrecon/', 'score': 7, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'aa9e4dffe4e6ad09', 'data': {'categories': ['#video', '#cv', '#training', '#graphs', '#optimization', '#3d'], 'emoji': '👕', 'ru': {'title': 'Реконструкция 3D-моделей людей в свободной одежде из видео', 'desc': "Представлен метод реконструкции согласованных во времени моделей человеческого тела из монокулярных видео, фокусирующийся на очень свободной одежде и взаимодействии с предметами. Ключевая идея заключается в комбинации общих человеческих приоров о форме тела с видеоспецифичной деформацией 'мешка костей'. Метод использует нейронную имплицитную модель, разделяющую деформации тела и одежды как отдельные слои модели движения. Для захвата тонкой геометрии одежды используются изображения-приоры, такие как поза тела, нормали поверхности и оптический поток."}, 'en': {'title': 'Revolutionizing Human Body Reconstruction from Single Videos', 'desc': 'This paper introduces a novel method for reconstructing human body models from single videos, particularly when subjects wear loose clothing or interact with objects. Unlike previous approaches that require multiple camera views or specific templates, this method combines learned human shape priors with video-specific deformations through test-time optimization. The authors utilize a neural implicit model that separates body and clothing movements, enhancing the accuracy of the reconstruction. By incorporating image-based priors like pose and optical flow, the method achieves high-quality, time-consistent 3D models, outperforming existing techniques in challenging scenarios.'}, 'zh': {'title': '从单目视频重建高保真人体模型', 'desc': '本文提出了一种从单目视频中重建时间一致的人体模型的方法，特别关注松散衣物或手持物体的交互。以往的人体重建工作通常局限于紧身衣物或不涉及物体交互，或者需要昂贵的多视角捕捉或个性化模板扫描。我们的方法结合了从大规模训练数据中学习的通用人体形状先验与视频特定的“骨骼袋”变形，通过测试时优化来实现高质量且灵活的重建。最终，我们的模型能够提取出时间一致的网格，或进一步优化为高保真度的3D高斯体，以实现交互式渲染。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00231', 'title': 'Helpful DoggyBot: Open-World Object Fetching using Legged Robots and Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.00231', 'abstract': "Learning-based methods have achieved strong performance for quadrupedal locomotion. However, several challenges prevent quadrupeds from learning helpful indoor skills that require interaction with environments and humans: lack of end-effectors for manipulation, limited semantic understanding using only simulation data, and low traversability and reachability in indoor environments. We present a system for quadrupedal mobile manipulation in indoor environments. It uses a front-mounted gripper for object manipulation, a low-level controller trained in simulation using egocentric depth for agile skills like climbing and whole-body tilting, and pre-trained vision-language models (VLMs) with a third-person fisheye and an egocentric RGB camera for semantic understanding and command generation. We evaluate our system in two unseen environments without any real-world data collection or training. Our system can zero-shot generalize to these environments and complete tasks, like following user's commands to fetch a randomly placed stuff toy after climbing over a queen-sized bed, with a 60% success rate. Project website: https://helpful-doggybot.github.io/", 'score': 6, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '341c4a5116bec7b6', 'data': {'categories': ['#cv', '#synthetic', '#graphs', '#rl', '#optimization', '#transfer_learning', '#games', '#robotics', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Умный робопёс: манипуляция и навигация в помещениях без дополнительного обучения', 'desc': 'Статья представляет систему для квадропедальной мобильной манипуляции в помещениях. Система использует фронтальный захват для манипуляции объектами и контроллер нижнего уровня, обученный в симуляции с использованием эгоцентрической глубины для ловких навыков. Для семантического понимания и генерации команд применяются предобученные визуально-языковые модели с камерами. Система демонстрирует способность к обобщению в незнакомой среде без дополнительного обучения на реальных данных.'}, 'en': {'title': 'Empowering Quadrupeds for Indoor Mobile Manipulation', 'desc': "This paper presents a novel system for quadrupedal robots to perform mobile manipulation tasks in indoor settings. The system integrates a front-mounted gripper for handling objects and employs a low-level controller trained in simulation to enable agile movements like climbing. It also utilizes pre-trained vision-language models to enhance semantic understanding and command execution. The results demonstrate the robot's ability to generalize to new environments and successfully complete tasks with a 60% success rate, showcasing its effectiveness in real-world scenarios without prior training data."}, 'zh': {'title': '四足机器人：室内操控的新突破', 'desc': '本论文提出了一种四足机器人在室内环境中进行移动操控的系统。该系统使用前置抓手进行物体操控，并通过模拟训练的低级控制器实现灵活的技能，如攀爬和全身倾斜。我们还结合了预训练的视觉-语言模型，以增强语义理解和指令生成能力。实验表明，该系统能够在未见过的环境中零-shot泛化，成功完成任务，成功率达到60%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00545', 'title': 'What the Harm? Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study', 'url': 'https://huggingface.co/papers/2410.00545', 'abstract': 'Gender bias in machine translation (MT) is recognized as an issue that can harm people and society. And yet, advancements in the field rarely involve people, the final MT users, or inform how they might be impacted by biased technologies. Current evaluations are often restricted to automatic methods, which offer an opaque estimate of what the downstream impact of gender disparities might be. We conduct an extensive human-centered study to examine if and to what extent bias in MT brings harms with tangible costs, such as quality of service gaps across women and men. To this aim, we collect behavioral data from 90 participants, who post-edited MT outputs to ensure correct gender translation. Across multiple datasets, languages, and types of users, our study shows that feminine post-editing demands significantly more technical and temporal effort, also corresponding to higher financial costs. Existing bias measurements, however, fail to reflect the found disparities. Our findings advocate for human-centered approaches that can inform the societal impact of bias.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '17621aa133820cd6', 'data': {'categories': ['#dataset', '#multilingual', '#machine_translation', '#ethics', '#data', '#benchmark'], 'emoji': '🚺', 'ru': {'title': 'Гендерное неравенство в машинном переводе: реальные последствия для пользователей', 'desc': 'Это исследование посвящено изучению гендерного смещения в машинном переводе и его влияния на пользователей. Авторы провели масштабное исследование с участием 90 человек, которые редактировали результаты машинного перевода для обеспечения корректного гендерного перевода. Результаты показали, что редактирование женского рода требует значительно больше технических и временных усилий, что также соответствует более высоким финансовым затратам. Исследование подчеркивает важность человеко-ориентированных подходов для оценки социального воздействия смещения в машинном обучении.'}, 'en': {'title': 'Unveiling Gender Bias: The Hidden Costs of Machine Translation', 'desc': 'This paper addresses the issue of gender bias in machine translation (MT) and its potential negative effects on users. The authors conducted a human-centered study involving 90 participants to analyze the impact of biased MT outputs on service quality for different genders. Their findings reveal that translating feminine terms requires more effort and incurs higher costs compared to masculine terms. The study highlights the inadequacy of current bias measurement methods and calls for approaches that consider the real-world implications of bias in MT.'}, 'zh': {'title': '关注机器翻译中的性别偏见', 'desc': '这篇论文探讨了机器翻译中的性别偏见问题，指出这种偏见可能对个人和社会造成伤害。研究表明，现有的评估方法主要依赖自动化手段，无法准确反映性别差异对用户的实际影响。通过对90名参与者的行为数据进行分析，发现女性的后编辑工作需要更多的技术和时间投入，导致更高的经济成本。论文呼吁采用以人为本的方法，以更好地理解偏见对社会的影响。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.18313', 'title': 'Embodied-RAG: General non-parametric Embodied Memory for Retrieval and Generation', 'url': 'https://huggingface.co/papers/2409.18313', 'abstract': "There is no limit to how much a robot might explore and learn, but all of that knowledge needs to be searchable and actionable. Within language research, retrieval augmented generation (RAG) has become the workhouse of large-scale non-parametric knowledge, however existing techniques do not directly transfer to the embodied domain, which is multimodal, data is highly correlated, and perception requires abstraction.   To address these challenges, we introduce Embodied-RAG, a framework that enhances the foundational model of an embodied agent with a non-parametric memory system capable of autonomously constructing hierarchical knowledge for both navigation and language generation. Embodied-RAG handles a full range of spatial and semantic resolutions across diverse environments and query types, whether for a specific object or a holistic description of ambiance. At its core, Embodied-RAG's memory is structured as a semantic forest, storing language descriptions at varying levels of detail. This hierarchical organization allows the system to efficiently generate context-sensitive outputs across different robotic platforms. We demonstrate that Embodied-RAG effectively bridges RAG to the robotics domain, successfully handling over 200 explanation and navigation queries across 19 environments, highlighting its promise for general-purpose non-parametric system for embodied agents.", 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '37095809c304fb89', 'data': {'categories': ['#reasoning', '#agi', '#rag', '#graphs', '#agents', '#transfer_learning', '#robotics', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Иерархическая память для интеллектуальных роботов', 'desc': 'Статья представляет Embodied-RAG - фреймворк для воплощённых агентов, который улучшает базовую модель с помощью непараметрической системы памяти. Эта система способна автономно создавать иерархические знания для навигации и генерации языка. Память Embodied-RAG структурирована как семантический лес, хранящий языковые описания на разных уровнях детализации. Фреймворк успешно обрабатывает более 200 запросов по объяснению и навигации в 19 различных средах.'}, 'en': {'title': 'Empowering Robots with Hierarchical Knowledge for Smart Navigation and Language Generation', 'desc': 'The paper introduces Embodied-RAG, a new framework that combines retrieval augmented generation (RAG) with embodied agents, which are robots that interact with the physical world. This framework allows robots to build a non-parametric memory system that organizes knowledge hierarchically, enabling them to navigate and generate language based on their environment. By structuring memory as a semantic forest, Embodied-RAG can efficiently respond to various queries about objects and surroundings. The results show that this approach successfully integrates RAG techniques into robotics, demonstrating its effectiveness across multiple environments and tasks.'}, 'zh': {'title': 'Embodied-RAG：机器人知识检索与生成的新框架', 'desc': '本文介绍了一种名为Embodied-RAG的框架，旨在解决机器人在多模态环境中知识检索和生成的问题。该框架通过非参数记忆系统增强了机器人基础模型，能够自主构建层次化知识，以支持导航和语言生成。Embodied-RAG的记忆结构为语义森林，能够存储不同细节层次的语言描述，从而高效生成上下文相关的输出。实验表明，Embodied-RAG成功处理了超过200个解释和导航查询，展示了其在机器人领域的广泛应用潜力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01044', 'title': 'RATIONALYST: Pre-training Process-Supervision for Improving Reasoning', 'url': 'https://huggingface.co/papers/2410.01044', 'abstract': 'The reasoning steps generated by LLMs might be incomplete, as they mimic logical leaps common in everyday communication found in their pre-training data: underlying rationales are frequently left implicit (unstated). To address this challenge, we introduce RATIONALYST, a model for process-supervision of reasoning based on pre-training on a vast collection of rationale annotations extracted from unlabeled data. We extract 79k rationales from web-scale unlabelled dataset (the Pile) and a combination of reasoning datasets with minimal human intervention. This web-scale pre-training for reasoning allows RATIONALYST to consistently generalize across diverse reasoning tasks, including mathematical, commonsense, scientific, and logical reasoning. Fine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by an average of 3.9% on 7 representative reasoning benchmarks. It also demonstrates superior performance compared to significantly larger verifiers like GPT-4 and similarly sized models fine-tuned on matching training sets.', 'score': 34, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '7443e92db63fd869', 'data': {'categories': ['#science', '#reasoning', '#dataset', '#multilingual', '#training', '#math', '#data', '#transfer_learning', '#benchmark', '#architecture', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'RATIONALYST: улучшение рассуждений в LLM с помощью предобучения на веб-данных', 'desc': 'В этой статье представлен RATIONALYST - новая модель для улучшения процесса рассуждений в больших языковых моделях (LLM). Модель предобучена на 79 тысячах обоснований, извлеченных из веб-данных и наборов данных для рассуждений. RATIONALYST демонстрирует улучшение точности на 3.9% в среднем на 7 эталонных тестах для различных типов рассуждений. Модель превосходит по производительности значительно более крупные верификаторы, такие как GPT-4.'}, 'en': {'title': 'RATIONALYST: Enhancing Reasoning with Explicit Rationale Learning', 'desc': 'RATIONALYST is a new model designed to improve reasoning in language models by addressing the issue of incomplete reasoning steps. It achieves this by pre-training on a large dataset of rationale annotations, which helps the model learn to make its reasoning more explicit. By extracting 79,000 rationales from a vast collection of unlabeled data, RATIONALYST can generalize effectively across various reasoning tasks, such as mathematical and commonsense reasoning. The model shows a notable improvement in accuracy, outperforming larger models like GPT-4 on several reasoning benchmarks.'}, 'zh': {'title': 'RATIONALYST：提升推理准确性的创新模型', 'desc': '本论文介绍了一种名为RATIONALYST的模型，旨在改善大型语言模型（LLMs）在推理过程中的不足。该模型通过在大量未标注数据中提取的79,000个推理理由进行预训练，从而实现了对推理过程的监督。RATIONALYST在多种推理任务上表现出色，包括数学、常识、科学和逻辑推理，平均提高了3.9%的准确率。与更大规模的验证模型如GPT-4相比，RATIONALYST在推理准确性上也表现出更优的性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01680', 'title': 'PHI-S: Distribution Balancing for Label-Free Multi-Teacher Distillation', 'url': 'https://huggingface.co/papers/2410.01680', 'abstract': 'Various visual foundation models have distinct strengths and weaknesses, both of which can be improved through heterogeneous multi-teacher knowledge distillation without labels, termed "agglomerative models." We build upon this body of work by studying the effect of the teachers\' activation statistics, particularly the impact of the loss function on the resulting student model quality. We explore a standard toolkit of statistical normalization techniques to better align the different distributions and assess their effects. Further, we examine the impact on downstream teacher-matching metrics, which motivates the use of Hadamard matrices. With these matrices, we demonstrate useful properties, showing how they can be used for isotropic standardization, where each dimension of a multivariate distribution is standardized using the same scale. We call this technique "PHI Standardization" (PHI-S) and empirically demonstrate that it produces the best student model across the suite of methods studied.', 'score': 32, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'dc301080cf253805', 'data': {'categories': ['#cv', '#training', '#optimization', '#transfer_learning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'PHI-стандартизация: новый подход к агломеративному обучению визуальных моделей', 'desc': "Статья исследует улучшение визуальных моделей с помощью агломеративного обучения без учителя. Авторы изучают влияние статистики активации учителей и функции потерь на качество модели-ученика. Рассматриваются различные методы статистической нормализации для лучшего выравнивания распределений. Предлагается новый метод 'PHI-стандартизации' на основе матриц Адамара, показывающий наилучшие результаты среди исследованных подходов."}, 'en': {'title': 'Enhancing Student Models with PHI Standardization', 'desc': "This paper explores how to improve visual foundation models using a technique called heterogeneous multi-teacher knowledge distillation without labels, known as agglomerative models. It focuses on the importance of the teachers' activation statistics and how different loss functions affect the quality of the student model. The authors investigate various statistical normalization techniques to align the distributions of the teachers' outputs and their impact on matching metrics. They introduce a new method called PHI Standardization (PHI-S), which standardizes multivariate distributions effectively, leading to superior student model performance."}, 'zh': {'title': '聚合模型：提升视觉模型的最佳实践', 'desc': '本文研究了不同视觉基础模型在无标签情况下通过异构多教师知识蒸馏的改进方法，称为“聚合模型”。我们重点分析了教师模型的激活统计特性，特别是损失函数对学生模型质量的影响。通过使用统计归一化技术，我们更好地对齐不同分布，并评估其效果。此外，我们引入了Hadamard矩阵，展示了其在各维度标准化中的有用特性，提出了“PHI标准化”（PHI-S）技术，并实验证明其在多种方法中产生了最佳的学生模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01215', 'title': 'From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging', 'url': 'https://huggingface.co/papers/2410.01215', 'abstract': 'While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.', 'score': 30, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'c2e7c70ae8f76f9b', 'data': {'categories': ['#reasoning', '#inference', '#interpretability', '#plp', '#optimization', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Иерархический отладчик кода нового поколения для ИИ-генерации', 'desc': 'Статья представляет новый подход к отладке кода, генерируемого большими языковыми моделями (LLM). Предложенный метод, названный Multi-Granularity Debugger (MGDebugger), декомпозирует проблемный код в иерархическую структуру подфункций и анализирует ошибки на разных уровнях детализации. MGDebugger использует LLM-симулированный исполнитель Python для точного определения ошибок. Эксперименты показали значительное улучшение точности генерации кода и высокую эффективность исправления ошибок различных типов и уровней сложности.'}, 'en': {'title': 'Hierarchical Debugging for Enhanced Code Generation Accuracy', 'desc': 'This paper presents the Multi-Granularity Debugger (MGDebugger), a novel hierarchical debugging system designed to improve the accuracy of code generated by large language models (LLMs). Unlike traditional debugging systems that treat code as a single unit, MGDebugger breaks down code into a tree structure of subfunctions, allowing for the identification and resolution of errors at various levels of granularity. The system employs an LLM-simulated Python executor to trace code execution and monitor variable states, enabling precise error detection. Experimental results show that MGDebugger significantly enhances debugging performance, achieving higher accuracy and repair success rates compared to existing methods.'}, 'zh': {'title': '多粒度调试，提升代码修复效率', 'desc': '本文介绍了一种新的代码调试系统，称为多粒度调试器（MGDebugger）。该系统通过将代码分解为层次树结构，能够在不同粒度上识别和解决错误，从低级语法错误到高级算法缺陷。MGDebugger采用自底向上的方法，逐个分析子函数并解决问题，确保调试过程的高效性。实验结果表明，MGDebugger在代码修复的准确性和成功率上均优于现有的调试系统。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01647', 'title': '3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection', 'url': 'https://huggingface.co/papers/2410.01647', 'abstract': 'Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and have been adapted for 3D Object Detection (3DOD), offering a promising approach to 3DOD through view-synthesis representation. However, NeRF faces inherent limitations: (i) limited representational capacity for 3DOD due to its implicit nature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that addresses these limitations. Inspired by these advantages, this paper introduces 3DGS into 3DOD for the first time, identifying two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and poor differentiation between objects and background, which hinders 3DOD; (ii) Excessive background blobs: 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D images, and propose an elegant and efficient solution by incorporating 2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and their background. To address the challenge (ii), we propose a Box-Focused Sampling strategy using 2D boxes to generate object probability distribution in 3D spaces, allowing effective probabilistic sampling in 3D to retain more object blobs and reduce noisy background blobs. Benefiting from our designs, our 3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det, achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.', 'score': 28, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '951512e0e25bc7da', 'data': {'categories': ['#dataset', '#cv', '#graphs', '#optimization', '#benchmark', '#3d'], 'emoji': '🕵️', 'ru': {'title': '3DGS-DET: Революция в 3D-детекции объектов', 'desc': 'Статья представляет новый подход к задаче 3D-детекции объектов, основанный на методе 3D Gaussian Splatting (3DGS). Авторы предлагают решение двух основных проблем: неоднозначное пространственное распределение гауссовых блобов и избыточное количество фоновых блобов. Для решения первой проблемы вводится 2D Boundary Guidance, улучшающее пространственное распределение блобов. Вторая проблема решается с помощью стратегии Box-Focused Sampling, которая позволяет эффективно выбирать блобы объектов и уменьшать количество шумовых фоновых блобов.'}, 'en': {'title': 'Enhancing 3D Object Detection with 3D Gaussian Splatting', 'desc': 'This paper presents a novel approach to 3D Object Detection (3DOD) by integrating 3D Gaussian Splatting (3DGS) with traditional methods. The authors identify two main challenges with 3DGS: unclear spatial distribution of Gaussian blobs and excessive background noise from 2D images. To improve the clarity of object differentiation, they introduce 2D Boundary Guidance, which enhances the spatial arrangement of Gaussian blobs. Additionally, they propose a Box-Focused Sampling strategy to effectively reduce background noise while retaining important object information, leading to significant performance improvements over existing methods.'}, 'zh': {'title': '3DGS：提升三维物体检测的新方法', 'desc': '神经辐射场（NeRF）在新视角合成和三维物体检测（3DOD）中得到了广泛应用，但存在一些固有的局限性，如隐式表示导致的表示能力有限和渲染速度慢。最近，三维高斯点云（3DGS）作为一种显式三维表示方法，解决了这些问题。本文首次将3DGS引入3DOD，提出了两个主要挑战：高斯斑点的空间分布模糊和背景斑点过多。为了解决这些挑战，本文提出了2D边界引导和基于盒子的采样策略，从而显著提高了物体与背景的区分度，并减少了噪声背景斑点。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01748', 'title': 'Not All LLM Reasoners Are Created Equal', 'url': 'https://huggingface.co/papers/2410.01748', 'abstract': 'We study the depth of grade-school math (GSM) problem-solving capabilities of LLMs. To this end, we evaluate their performance on pairs of existing math word problems together so that the answer to the second problem depends on correctly answering the first problem. Our findings reveal a significant reasoning gap in most LLMs, that is performance difference between solving the compositional pairs and solving each question independently. This gap is more pronounced in smaller, more cost-efficient, and math-specialized models. Moreover, instruction-tuning recipes and code generation have varying effects across LLM sizes, while finetuning on GSM can lead to task overfitting. Our analysis indicates that large reasoning gaps are not because of test-set leakage, but due to distraction from additional context and poor second-hop reasoning. Overall, LLMs exhibit systematic differences in their reasoning abilities, despite what their performance on standard benchmarks indicates.', 'score': 27, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '4179f096b2f31dbd', 'data': {'categories': ['#reasoning', '#leakage', '#training', '#math', '#optimization', '#benchmark', '#small_models'], 'emoji': '🧮', 'ru': {'title': 'Языковые модели спотыкаются на связанных математических задачах', 'desc': 'Исследователи изучают глубину способностей языковых моделей (LLM) решать математические задачи школьного уровня. Они оценивают производительность моделей на парах связанных задач, где ответ на вторую зависит от правильного решения первой. Результаты показывают значительный разрыв в рассуждениях у большинства LLM, особенно заметный в меньших, более экономичных и специализированных на математике моделях. Анализ указывает на то, что большие разрывы в рассуждениях связаны с отвлечением на дополнительный контекст и слабым рассуждением на втором шаге, а не с утечкой тестовых данных.'}, 'en': {'title': 'Uncovering Reasoning Gaps in LLMs for Math Problem Solving', 'desc': 'This paper investigates how well large language models (LLMs) can solve grade-school math problems, especially when the answer to one problem relies on the answer to another. The study finds that there is a notable reasoning gap, meaning LLMs perform worse when problems are connected compared to when they are solved separately. Smaller and more specialized models show an even larger gap in their reasoning abilities. The research suggests that this gap is not due to issues like test-set leakage, but rather because of distractions from extra information and difficulties in multi-step reasoning.'}, 'zh': {'title': '揭示大型语言模型的推理差距', 'desc': '我们研究了大型语言模型（LLMs）在解决小学数学问题（GSM）方面的能力。通过评估模型在一对数学应用题上的表现，我们发现大多数LLMs在解决组合问题时存在显著的推理差距。这个差距在较小、成本效益高且专注于数学的模型中更为明显。此外，指令调优和代码生成对不同规模的LLMs有不同的影响，而在GSM上进行微调可能导致任务过拟合。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01744', 'title': 'LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks', 'url': 'https://huggingface.co/papers/2410.01744', 'abstract': "Text-rich images, where text serves as the central visual element guiding the overall understanding, are prevalent in real-world applications, such as presentation slides, scanned documents, and webpage snapshots. Tasks involving multiple text-rich images are especially challenging, as they require not only understanding the content of individual images but reasoning about inter-relationships and logical flows across multiple visual inputs. Despite the importance of these scenarios, current multimodal large language models (MLLMs) struggle to handle such tasks due to two key challenges: (1) the scarcity of high-quality instruction tuning datasets for text-rich multi-image scenarios, and (2) the difficulty in balancing image resolution with visual feature sequence length. To address these challenges, we propose \\OurMethod, a MLLM designed specifically for handling vision-language tasks involving multiple text-rich images. First, we curated about one million high-quality multimodal instruction-tuning data, tailored to text-rich, multi-image scenarios. Second, we developed an adaptive high-resolution multi-image encoding module to dynamically optimize the allocation of visual sequence length based on the original aspect ratios and resolutions of the input images. Experiments across a wide range of benchmarks demonstrate our model's superior capabilities in text-rich, multi-image evaluations and competitive performance in general domain evaluations.", 'score': 24, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'f787d537107fa831', 'data': {'categories': ['#reasoning', '#dataset', '#cv', '#training', '#data', '#optimization', '#transfer_learning', '#benchmark', '#architecture', '#synthetic', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'Революция в обработке текстовых изображений: новая MLLM для многозадачного анализа', 'desc': 'Статья представляет новый мультимодальный языковой модель (MLLM) для обработки задач, связанных с несколькими изображениями, богатыми текстом. Авторы создали обширный набор данных для обучения модели на таких сценариях. Они также разработали адаптивный модуль кодирования изображений высокого разрешения для оптимизации обработки визуальной информации. Эксперименты показали превосходство предложенной модели в задачах с текстовыми изображениями и конкурентоспособность в общих задачах.'}, 'en': {'title': 'Empowering Multimodal Understanding with OurMethod', 'desc': 'This paper introduces a new multimodal large language model (MLLM) called \textit{OurMethod}, which is designed to effectively process and understand multiple text-rich images. The authors address two main challenges: the lack of quality datasets for training and the need to balance image resolution with the length of visual features. They created a dataset of about one million high-quality instruction-tuning examples specifically for text-rich, multi-image tasks. Additionally, they developed a novel encoding module that adapts to the resolutions of input images, leading to improved performance in evaluating text-rich, multi-image scenarios.'}, 'zh': {'title': '专为文本丰富图像设计的多模态模型', 'desc': '本文提出了一种新的多模态大语言模型（MLLM），专门用于处理包含多个文本丰富图像的视觉-语言任务。我们首先收集了约一百万个高质量的多模态指令调优数据，专门针对文本丰富的多图像场景。其次，我们开发了一种自适应高分辨率多图像编码模块，能够根据输入图像的原始纵横比和分辨率动态优化视觉序列长度的分配。实验结果表明，我们的模型在文本丰富的多图像评估中表现优越，并在一般领域评估中也具有竞争力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01463', 'title': 'Selective Aggregation for Low-Rank Adaptation in Federated Learning', 'url': 'https://huggingface.co/papers/2410.01463', 'abstract': 'We investigate LoRA in federated learning through the lens of the asymmetry analysis of the learned A and B matrices. In doing so, we uncover that A matrices are responsible for learning general knowledge, while B matrices focus on capturing client-specific knowledge. Based on this finding, we introduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two low-rank trainable matrices A and B to model the weight update, but only A matrices are shared with the server for aggregation. Moreover, we delve into the relationship between the learned A and B matrices in other LoRA variants, such as rsLoRA and VeRA, revealing a consistent pattern. Consequently, we extend our FedSA-LoRA method to these LoRA variants, resulting in FedSA-rsLoRA and FedSA-VeRA. In this way, we establish a general paradigm for integrating LoRA with FL, offering guidance for future work on subsequent LoRA variants combined with FL. Extensive experimental results on natural language understanding and generation tasks demonstrate the effectiveness of the proposed method.', 'score': 18, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '44e5fb0db92df32b', 'data': {'categories': ['#training', '#rl', '#optimization', '#transfer_learning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'LoRA в федеративном обучении: разделяй и властвуй', 'desc': 'Исследование применения метода LoRA в федеративном обучении выявило, что матрицы A отвечают за общие знания, а матрицы B - за специфические для клиента. На основе этого был разработан метод FedSA-LoRA, где только матрицы A передаются на сервер для агрегации. Аналогичная закономерность наблюдается и в других вариантах LoRA, что позволило создать FedSA-rsLoRA и FedSA-VeRA. Экспериментальные результаты на задачах обработки естественного языка подтверждают эффективность предложенного подхода.'}, 'en': {'title': 'Enhancing Federated Learning with Low-Rank Adaptation', 'desc': 'This paper explores the use of Low-Rank Adaptation (LoRA) in federated learning by analyzing the asymmetry of the learned matrices A and B. It finds that matrix A captures general knowledge applicable across clients, while matrix B focuses on client-specific information. The authors propose a new method called Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which shares only the A matrices with the server for aggregation, enhancing privacy and efficiency. They also extend this approach to other LoRA variants, establishing a comprehensive framework for integrating LoRA with federated learning, supported by strong experimental results in natural language tasks.'}, 'zh': {'title': '联邦学习中的低秩适应新范式', 'desc': '本文研究了在联邦学习中使用LoRA的方式，分析了学习到的A和B矩阵的不对称性。研究发现，A矩阵负责学习通用知识，而B矩阵则专注于捕捉客户端特定的知识。基于这一发现，提出了联邦共享低秩适应（FedSA-LoRA）方法，该方法使用两个低秩可训练矩阵A和B来建模权重更新，但仅共享A矩阵与服务器进行聚合。通过对其他LoRA变体（如rsLoRA和VeRA）中学习到的A和B矩阵的关系进行深入探讨，建立了将LoRA与联邦学习结合的一般范式，为未来的研究提供了指导。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01257', 'title': 'HelpSteer2-Preference: Complementing Ratings with Preferences', 'url': 'https://huggingface.co/papers/2410.01257', 'abstract': 'Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data. This is primarily because these approaches require data collected in different (but incompatible) formats, meaning that adequately matched data is not available in existing public datasets. To tackle this problem, we release preference annotations (designed for Bradley-Terry training) to complement existing ratings (designed for Regression style training) in the HelpSteer2 dataset. To improve data interpretability, preference annotations are accompanied with human-written justifications. Using this data, we conduct the first head-to-head comparison of Bradley-Terry and Regression models when adequately matched for data. Based on insights derived from such a comparison, we propose a novel approach to combine Bradley-Terry and Regression reward modeling. A Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. We also demonstrate the effectiveness of this reward model at aligning models to follow instructions in RLHF. We open-source this dataset (CC-BY-4.0 license) at https://huggingface.co/datasets/nvidia/HelpSteer2 and openly release the trained Reward Model at https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward', 'score': 18, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'b46023070ee2b9c9', 'data': {'categories': ['#dataset', '#training', '#interpretability', '#alignment', '#benchmark', '#open_source', '#rlhf'], 'emoji': '🏆', 'ru': {'title': 'Объединение лучшего из двух миров в обучении моделей вознаграждения', 'desc': 'Статья представляет сравнительный анализ двух популярных подходов к обучению моделей вознаграждения: стиль Брэдли-Терри и регрессионный стиль. Авторы выпустили новый набор данных HelpSteer2, содержащий аннотации предпочтений и рейтинги для справедливого сравнения этих методов. На основе результатов исследования предложен новый подход, объединяющий оба стиля обучения моделей вознаграждения. Модель Llama-3.1-70B-Instruct, обученная с использованием этого подхода, достигла наивысшего результата в 94.1 балла на бенчмарке RewardBench.'}, 'en': {'title': 'Bridging Reward Models: A New Approach for Better Alignment', 'desc': 'This paper discusses the importance of reward models in aligning machine learning models to follow instructions. It compares two popular training paradigms for these models: Bradley-Terry and Regression styles, highlighting the lack of compatible data for a fair comparison. To address this, the authors introduce preference annotations to the HelpSteer2 dataset, which allows for a direct comparison of the two approaches. They also propose a new method that combines both paradigms, resulting in a highly effective reward model that outperforms others in the field.'}, 'zh': {'title': '奖励模型的创新对比与结合', 'desc': '本论文探讨了奖励模型在指令对齐中的重要性，主要比较了Bradley-Terry风格和回归风格的训练方法。由于这两种方法需要不同格式的数据，导致现有公共数据集中缺乏适当匹配的数据。为了解决这个问题，我们在HelpSteer2数据集中发布了用于Bradley-Terry训练的偏好注释，并附上了人类撰写的理由，以提高数据的可解释性。通过对比实验，我们提出了一种新方法，将Bradley-Terry和回归奖励建模相结合，最终在RewardBench上取得了94.1的高分。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.20059', 'title': 'Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis', 'url': 'https://huggingface.co/papers/2409.20059', 'abstract': 'Neural metrics for machine translation (MT) evaluation have become increasingly prominent due to their superior correlation with human judgments compared to traditional lexical metrics. Researchers have therefore utilized neural metrics through quality-informed decoding strategies, achieving better results than likelihood-based methods. With the rise of Large Language Models (LLMs), preference-based alignment techniques have gained attention for their potential to enhance translation quality by optimizing model weights directly on preferences induced by quality estimators. This study focuses on Contrastive Preference Optimization (CPO) and conducts extensive experiments to evaluate the impact of preference-based alignment on translation quality. Our findings indicate that while CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data with regard to the alignment metric, it may lead to instability across downstream evaluation metrics, particularly between neural and lexical ones. Additionally, we demonstrate that relying solely on the base model for generating candidate translations achieves performance comparable to using multiple external systems, while ensuring better consistency across downstream metrics.', 'score': 15, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '700774cfa4699b68', 'data': {'categories': ['#multilingual', '#training', '#machine_translation', '#optimization', '#alignment', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'Оптимизация предпочтений для улучшения качества машинного перевода', 'desc': 'Это исследование посвящено применению нейронных метрик для оценки качества машинного перевода и использованию предпочтений для оптимизации моделей перевода. Авторы изучают метод Contrastive Preference Optimization (CPO) и сравнивают его с традиционным дообучением на размеченных данных. Результаты показывают, что CPO превосходит обычное дообучение по метрике выравнивания, но может приводить к нестабильности других метрик оценки. Также выявлено, что использование только базовой модели для генерации кандидатов перевода дает результаты, сопоставимые с применением нескольких внешних систем.'}, 'en': {'title': 'Enhancing Translation Quality with Contrastive Preference Optimization', 'desc': 'This paper discusses the use of neural metrics for evaluating machine translation (MT), which are more aligned with human judgment than traditional methods. It highlights the effectiveness of quality-informed decoding strategies that leverage these neural metrics, particularly in the context of Large Language Models (LLMs). The study introduces Contrastive Preference Optimization (CPO) as a technique to improve translation quality by directly optimizing model weights based on preferences from quality estimators. The results show that while CPO outperforms Supervised Fine-Tuning (SFT) on high-quality data, it can cause instability in evaluation metrics, and using the base model for generating translations can yield results similar to those from multiple external systems.'}, 'zh': {'title': '提升机器翻译质量的对比偏好优化', 'desc': '本研究探讨了神经度量在机器翻译评估中的应用，显示其与人类判断的相关性优于传统的词汇度量。研究者们通过质量信息解码策略利用神经度量，取得了比基于似然的方法更好的结果。随着大型语言模型的兴起，基于偏好的对齐技术受到关注，能够通过优化模型权重来提升翻译质量。我们的实验表明，尽管对比偏好优化（CPO）在高质量数据上优于监督微调（SFT），但在下游评估指标上可能导致不稳定性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01731', 'title': 'ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2410.01731', 'abstract': 'The practical use of text-to-image generation has evolved from simple, monolithic models to complex workflows that combine multiple specialized components. While workflow-based approaches can lead to improved image quality, crafting effective workflows requires significant expertise, owing to the large number of available components, their complex inter-dependence, and their dependence on the generation prompt. Here, we introduce the novel task of prompt-adaptive workflow generation, where the goal is to automatically tailor a workflow to each user prompt. We propose two LLM-based approaches to tackle this task: a tuning-based method that learns from user-preference data, and a training-free method that uses the LLM to select existing flows. Both approaches lead to improved image quality when compared to monolithic models or generic, prompt-independent workflows. Our work shows that prompt-dependent flow prediction offers a new pathway to improving text-to-image generation quality, complementing existing research directions in the field.', 'score': 15, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'e1174d1b695405ab', 'data': {'categories': ['#cv', '#training', '#alignment', '#diffusion', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Адаптивные рабочие процессы: новый путь к улучшению генерации изображений по тексту', 'desc': 'Статья представляет новую задачу генерации рабочих процессов, адаптированных под промпт, для улучшения качества генерации изображений по тексту. Авторы предлагают два подхода на основе больших языковых моделей: метод с дообучением на пользовательских предпочтениях и метод без дополнительного обучения для выбора существующих процессов. Оба подхода показывают улучшение качества изображений по сравнению с монолитными моделями и общими рабочими процессами. Исследование открывает новое направление для повышения качества генерации изображений по тексту.'}, 'en': {'title': 'Tailoring Workflows for Enhanced Image Generation', 'desc': 'This paper discusses advancements in text-to-image generation, moving from simple models to more complex workflows that utilize specialized components. It introduces the task of prompt-adaptive workflow generation, which aims to automatically customize workflows based on user prompts. The authors propose two methods using large language models (LLMs): one that learns from user preferences and another that selects existing workflows without additional training. Both methods enhance image quality compared to traditional models, highlighting the importance of adapting workflows to specific prompts for better results.'}, 'zh': {'title': '基于提示的工作流生成提升图像质量', 'desc': '本文介绍了一种新的任务——基于提示的工作流生成，旨在自动根据用户提示定制工作流。我们提出了两种基于大语言模型（LLM）的方法：一种是基于调优的方法，通过用户偏好数据进行学习；另一种是无训练的方法，利用LLM选择现有的工作流。这两种方法在图像质量上优于单一模型或通用的、与提示无关的工作流。我们的研究表明，基于提示的工作流预测为提高文本到图像生成的质量提供了一条新路径。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01036', 'title': 'MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages', 'url': 'https://huggingface.co/papers/2410.01036', 'abstract': 'The rise of foundation models (FMs), coupled with regulatory efforts addressing their risks and impacts, has sparked significant interest in open-source models. However, existing speech FMs (SFMs) fall short of full compliance with the open-source principles, even if claimed otherwise, as no existing SFM has model weights, code, and training data publicly available under open-source terms. In this work, we take the first step toward filling this gap by focusing on the 24 official languages of the European Union (EU). We collect suitable training data by surveying automatic speech recognition datasets and unlabeled speech corpora under open-source compliant licenses, for a total of 950k hours. Additionally, we release automatic transcripts for 441k hours of unlabeled data under the permissive CC-BY license, thereby facilitating the creation of open-source SFMs for the EU languages.', 'score': 14, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '9714f6cb6169fec1', 'data': {'categories': ['#audio', '#dataset', '#multilingual', '#data', '#open_source', '#low_resource'], 'emoji': '🗣️', 'ru': {'title': 'Открытые речевые модели для языков ЕС: первый шаг сделан', 'desc': 'Статья посвящена разработке открытых речевых фундаментальных моделей (SFM) для 24 официальных языков Европейского Союза. Авторы собрали 950 тысяч часов речевых данных с открытыми лицензиями для обучения моделей. Они также опубликовали автоматические транскрипции для 441 тысячи часов неразмеченных данных под лицензией CC-BY. Это первый шаг к созданию полностью открытых SFM, так как существующие модели не соответствуют всем принципам открытого исходного кода.'}, 'en': {'title': 'Building Open-Source Speech Models for EU Languages', 'desc': 'This paper addresses the limitations of existing speech foundation models (SFMs) in terms of open-source compliance. It highlights that no current SFM provides model weights, code, and training data that are fully accessible under open-source terms. The authors collect a substantial dataset of 950,000 hours of training data from various automatic speech recognition datasets and unlabeled speech corpora that comply with open-source licenses. They also release automatic transcripts for 441,000 hours of unlabeled data, promoting the development of open-source SFMs for the 24 official languages of the European Union.'}, 'zh': {'title': '推动欧盟语言的开源语音模型发展', 'desc': '本论文关注基础模型（FMs）在开源模型中的应用，特别是语音基础模型（SFMs）。目前，现有的语音基础模型未能完全遵循开源原则，因为没有公开的模型权重、代码和训练数据。我们针对欧盟的24种官方语言，收集了符合开源许可的自动语音识别数据集和未标记语音语料，总计达到950k小时。我们还发布了441k小时未标记数据的自动转录，促进了欧盟语言的开源语音基础模型的创建。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01769', 'title': 'Quantifying Generalization Complexity for Large Language Models', 'url': 'https://huggingface.co/papers/2410.01769', 'abstract': "While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold - referred to as critical complexity - where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.", 'score': 13, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'fc737f630759a34b', 'data': {'categories': ['#reasoning', '#training', '#interpretability', '#benchmark', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Scylla: раскрывая границы обобщения языковых моделей', 'desc': "Статья представляет Scylla - динамическую систему оценки, измеряющую способности больших языковых моделей (LLM) к обобщению. Scylla разделяет обобщение и запоминание, оценивая производительность модели на данных распределения обучения и вне его через 20 задач разной сложности. Исследование выявило нелинейную зависимость между сложностью задачи и разрывом в производительности, названную 'долиной обобщения'. Обнаружено, что с увеличением размера модели критическая сложность смещается в сторону более сложных задач рассуждения."}, 'en': {'title': 'Scylla: Unraveling Generalization in Large Language Models', 'desc': "This paper presents Scylla, a new evaluation framework designed to measure the generalization abilities of large language models (LLMs) while separating it from memorization. Scylla evaluates model performance on both in-distribution (ID) and out-of-distribution (OOD) data across various tasks of increasing complexity. The study identifies a phenomenon called the 'generalization valley,' which highlights a critical complexity threshold where models tend to rely more on memorization rather than generalization. Additionally, it shows that as LLMs grow in size, they can tackle more complex tasks before this reliance on memorization becomes problematic."}, 'zh': {'title': '揭示大型语言模型的泛化能力', 'desc': '本文介绍了一种名为Scylla的动态评估框架，用于定量测量大型语言模型（LLMs）的泛化能力。Scylla通过评估模型在分布内（ID）和分布外（OOD）数据上的表现，来区分泛化与记忆化。研究发现任务复杂性与ID和OOD数据之间的性能差距呈现非单调关系，称为泛化谷。随着模型规模的增加，关键复杂性向更高的任务复杂性移动，表明更大的模型在过度依赖记忆化之前能够处理更复杂的推理任务。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01691', 'title': 'FactAlign: Long-form Factuality Alignment of Large Language Models', 'url': 'https://huggingface.co/papers/2410.01691', 'abstract': "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign", 'score': 8, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '3b5151bf24c4fc15', 'data': {'categories': ['#dataset', '#hallucinations', '#long_context', '#training', '#alignment', '#open_source', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'FactAlign: повышение фактической точности языковых моделей без потери полезности', 'desc': 'Статья представляет FactAlign - новую систему для улучшения фактической точности длинных ответов языковых моделей. Авторы предлагают алгоритм fKTO, который оптимизирует ответы на уровне предложений, используя автоматическую оценку фактичности. Эксперименты показывают, что FactAlign значительно повышает фактическую точность ответов, сохраняя их полезность. Система также обучает модели предоставлять больше информации без потери точности.'}, 'en': {'title': 'Enhancing Factual Accuracy in Language Models with FactAlign', 'desc': 'This paper introduces FactAlign, a new framework aimed at improving the factual accuracy of long-form responses generated by large language models (LLMs). The authors highlight the problem of hallucination, where LLMs produce incorrect or misleading information, especially in extended outputs. FactAlign employs a fine-grained alignment algorithm called fKTO, which enhances the alignment process by using detailed factuality evaluations. Experimental results show that FactAlign not only boosts the factual accuracy of LLM responses but also maintains their overall helpfulness, leading to better performance in information retrieval tasks.'}, 'zh': {'title': '提升大型语言模型的事实准确性与有用性', 'desc': '本论文提出了一种新的对齐框架FactAlign，旨在提高大型语言模型（LLMs）在长文本响应中的事实准确性。我们引入了一种细粒度的句子级对齐算法fKTO，扩展了Kahneman-Tversky优化方法。FactAlign利用自动事实评估的最新进展，通过细粒度的事实评估来指导对齐过程。实验结果表明，FactAlign显著提高了LLM响应的事实准确性和有用性，同时保持了信息的丰富性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02197', 'title': 'General Preference Modeling with Preference Representations for Aligning Language Models', 'url': 'https://huggingface.co/papers/2410.02197', 'abstract': 'Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive preferences. Although supervised pair preference models (PairPM) can express general preferences, their implementation is highly ad-hoc and cannot guarantee a consistent preference probability of compared pairs. Additionally, they impose high computational costs due to their quadratic query complexity when comparing multiple responses. In this paper, we introduce preference representation learning, an approach that embeds responses into a latent space to capture intricate preference structures efficiently, achieving linear query complexity. Additionally, we propose preference score-based General Preference Optimization (GPO), which generalizes reward-based reinforcement learning from human feedback. Experimental results show that our General Preference representation model (GPM) outperforms the BT reward model on the RewardBench benchmark with a margin of up to 5.6% and effectively models cyclic preferences where any BT reward model behaves like a random guess. Furthermore, evaluations on downstream tasks such as AlpacaEval2.0 and MT-Bench, following the language model post-training with GPO and our general preference model, reveal substantial performance improvements with margins up to 9.3%. These findings indicate that our method may enhance the alignment of foundation models with nuanced human values. The code is available at https://github.com/general-preference/general-preference-model.', 'score': 7, 'issue_id': 1, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'eb2fb462dcfba826', 'data': {'categories': ['#training', '#optimization', '#alignment', '#benchmark', '#open_source', '#rlhf', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Новый подход к моделированию человеческих предпочтений для alignment ИИ', 'desc': 'Статья представляет новый подход к моделированию предпочтений человека для улучшения alignment крупных языковых моделей. Авторы вводят понятие preference representation learning, которое позволяет эффективно отображать ответы в латентное пространство для захвата сложных структур предпочтений. Также предлагается метод General Preference Optimization (GPO), обобщающий обучение с подкреплением на основе обратной связи от человека. Эксперименты показывают, что предложенная модель General Preference Model (GPM) превосходит традиционные подходы на нескольких бенчмарках, демонстрируя потенциал для улучшения соответствия языковых моделей человеческим ценностям.'}, 'en': {'title': 'Enhancing Model Alignment with Human Preferences through Efficient Learning', 'desc': 'This paper addresses the challenge of aligning foundation models with human preferences by introducing preference representation learning. Unlike traditional reward modeling methods, which struggle with intransitive preferences and high computational costs, this new approach efficiently captures complex preference structures in a latent space. The authors also present General Preference Optimization (GPO), which extends reinforcement learning from human feedback to improve preference modeling. Experimental results demonstrate that their General Preference representation model (GPM) significantly outperforms existing methods, particularly in handling cyclic preferences and enhancing model performance on various tasks.'}, 'zh': {'title': '提升模型与人类价值观的对齐能力', 'desc': '本文提出了一种新的偏好表示学习方法，旨在更好地捕捉人类的复杂偏好结构。与传统的奖励建模方法相比，该方法通过将响应嵌入到潜在空间中，显著提高了表达能力，并降低了计算复杂度。我们还提出了一种基于偏好分数的通用偏好优化方法，能够有效地从人类反馈中进行强化学习。实验结果表明，我们的通用偏好模型在多个基准测试中表现优于传统模型，能够更好地对齐基础模型与人类价值观。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01171', 'title': 'BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2410.01171', 'abstract': "Large language models excel at creative generation but continue to struggle with the issues of hallucination and bias. While retrieval-augmented generation (RAG) provides a framework for grounding LLMs' responses in accurate and up-to-date information, it still raises the question of bias: which sources should be selected for inclusion in the context? And how should their importance be weighted? In this paper, we study the challenge of cross-lingual RAG and present a dataset to investigate the robustness of existing systems at answering queries about geopolitical disputes, which exist at the intersection of linguistic, cultural, and political boundaries. Our dataset is sourced from Wikipedia pages containing information relevant to the given queries and we investigate the impact of including additional context, as well as the composition of this context in terms of language and source, on an LLM's response. Our results show that existing RAG systems continue to be challenged by cross-lingual use cases and suffer from a lack of consistency when they are provided with competing information in multiple languages. We present case studies to illustrate these issues and outline steps for future research to address these challenges. We make our dataset and code publicly available at https://github.com/manestay/bordIRlines.", 'score': 5, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '6a353ff492b330af', 'data': {'categories': ['#dataset', '#hallucinations', '#multilingual', '#rag', '#ethics', '#transfer_learning', '#open_source'], 'emoji': '🌍', 'ru': {'title': 'Преодоление языковых барьеров в retrieval-augmented generation', 'desc': 'Статья исследует проблему кросс-языкового retrieval-augmented generation (RAG) в контексте геополитических споров. Авторы представляют набор данных для изучения устойчивости существующих систем при ответе на запросы, находящиеся на пересечении языковых, культурных и политических границ. Исследование показывает, что современные RAG-системы по-прежнему сталкиваются с трудностями в кросс-языковых сценариях и страдают от отсутствия согласованности при предоставлении противоречивой информации на разных языках. Авторы предлагают направления для будущих исследований по решению этих проблем.'}, 'en': {'title': 'Enhancing LLMs: Tackling Bias and Hallucination in Cross-Lingual Contexts', 'desc': 'This paper addresses the limitations of large language models (LLMs) in generating accurate responses, particularly focusing on the issues of hallucination and bias. It explores retrieval-augmented generation (RAG) as a method to enhance LLMs by grounding their outputs in reliable information, while also questioning how to select and weigh sources effectively. The authors introduce a new dataset aimed at evaluating RAG systems in the context of geopolitical disputes, highlighting the challenges posed by cross-lingual queries. Their findings reveal that current RAG systems struggle with consistency and accuracy when faced with conflicting information across different languages, suggesting a need for further research in this area.'}, 'zh': {'title': '跨语言检索增强生成的挑战与机遇', 'desc': '这篇论文探讨了大型语言模型在生成创意内容时面临的幻觉和偏见问题。尽管检索增强生成（RAG）为大型语言模型的响应提供了准确和最新信息的框架，但在选择信息来源时仍然存在偏见问题。我们研究了跨语言RAG的挑战，并提出了一个数据集，以调查现有系统在回答地缘政治争端查询时的鲁棒性。研究结果表明，现有的RAG系统在处理多语言竞争信息时仍然面临挑战，缺乏一致性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.18111', 'title': 'E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding', 'url': 'https://huggingface.co/papers/2409.18111', 'abstract': 'Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding Benchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our fine-grained tasks, e.g., grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, E.T. Chat, together with an instruction-tuning dataset E.T. Instruct 164K tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-09-26', 'pub_date_card': {'ru': '26 сентября', 'en': 'September 26', 'zh': '9月26日'}, 'hash': '5135f0df381cd4c0', 'data': {'categories': ['#video', '#long_context', '#training', '#interpretability', '#benchmark', '#games', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'E.T. Bench: новый стандарт для оценки понимания видео на уровне событий', 'desc': 'Статья представляет новый бенчмарк E.T. Bench для оценки Video-LLM на уровне событий и с учетом времени. Бенчмарк включает 7.3 тысячи образцов по 12 задачам с использованием 7 тысяч видео из 8 доменов. Авторы провели оценку 8 Image-LLM и 12 Video-LLM моделей, выявив их ограничения в решении задач на уровне событий. Для улучшения результатов предложена базовая модель E.T. Chat и набор данных E.T. Instruct 164K для тонкой настройки.'}, 'en': {'title': 'E.T. Bench: Elevating Video Understanding to Event-Level Precision', 'desc': 'This paper discusses the development of E.T. Bench, a new benchmark designed for evaluating Video Large Language Models (Video-LLMs) in understanding events within videos. Unlike previous benchmarks that only assess video-level question-answering, E.T. Bench focuses on fine-grained event-level tasks across various scenarios. The benchmark includes a diverse set of tasks and a large dataset, allowing for comprehensive evaluation of model capabilities. Additionally, the authors introduce E.T. Chat, a baseline model that improves performance on these fine-grained tasks by addressing issues like short video context and inadequate training data.'}, 'zh': {'title': '提升视频理解的细粒度评估', 'desc': '最近，视频大型语言模型（Video-LLMs）在视频理解方面展现了巨大的潜力。为了验证这些模型的能力，研究者们提出了多个基准测试，但现有的基准仅通过视频级问答进行评估，缺乏对事件级的细致评估和任务多样性。为了解决这个问题，我们引入了E.T. Bench，这是一个大规模、高质量的开放式事件级视频理解基准，涵盖了12个任务和7.3K样本。我们还提出了E.T. Chat模型和针对细粒度事件理解的指令调优数据集E.T. Instruct 164K，展示了在多个场景中的优越性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01804', 'title': 'EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis', 'url': 'https://huggingface.co/papers/2410.01804', 'abstract': 'We present Exact Volumetric Ellipsoid Rendering (EVER), a method for real-time differentiable emission-only volume rendering. Unlike recent rasterization based approach by 3D Gaussian Splatting (3DGS), our primitive based representation allows for exact volume rendering, rather than alpha compositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does not suffer from popping artifacts and view dependent density, but still achieves frame rates of sim!30 FPS at 720p on an NVIDIA RTX4090. Since our approach is built upon ray tracing it enables effects such as defocus blur and camera distortion (e.g. such as from fisheye cameras), which are difficult to achieve by rasterization. We show that our method is more accurate with fewer blending issues than 3DGS and follow-up work on view-consistent rendering, especially on the challenging large-scale scenes from the Zip-NeRF dataset where it achieves sharpest results among real-time techniques.', 'score': 5, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '268be8bd1eb6316f', 'data': {'categories': ['#dataset', '#cv', '#3d'], 'emoji': '🔮', 'ru': {'title': 'EVER: Точный объемный рендеринг в реальном времени', 'desc': 'EVER (Exact Volumetric Ellipsoid Rendering) - это новый метод дифференцируемого рендеринга объемов в реальном времени. В отличие от 3D Gaussian Splatting, EVER обеспечивает точный рендеринг объемов без артефактов и зависимости плотности от ракурса. Метод достигает скорости около 30 кадров в секунду при разрешении 720p на NVIDIA RTX4090. EVER позволяет применять эффекты размытия и искажения камеры, демонстрируя более точные результаты на сложных сценах из датасета Zip-NeRF.'}, 'en': {'title': 'Achieving Real-Time Precision in Volume Rendering with EVER', 'desc': 'The paper introduces Exact Volumetric Ellipsoid Rendering (EVER), a novel technique for real-time volume rendering that focuses on emission-only scenarios. Unlike the 3D Gaussian Splatting (3DGS) method, which uses alpha compositing, EVER employs a primitive-based representation that ensures precise volume rendering without artifacts. This method achieves high frame rates of around 30 FPS at 720p resolution on advanced hardware, while also supporting complex effects like defocus blur and camera distortion. The results demonstrate that EVER outperforms 3DGS in accuracy and blending quality, particularly in large-scale scenes from the Zip-NeRF dataset.'}, 'zh': {'title': '实时精确体积渲染的新方法', 'desc': '我们提出了一种名为精确体积椭球渲染（EVER）的方法，用于实时可微分的仅发射体积渲染。与基于光栅化的3D高斯点云方法（3DGS）不同，我们的原始表示允许进行精确的体积渲染，而不是对3D高斯广告牌进行透明合成。因此，我们的方法没有出现3DGS中的弹跳伪影和视角依赖密度问题，同时在NVIDIA RTX4090上仍能以720p的分辨率达到每秒30帧的帧率。由于我们的方法基于光线追踪，它能够实现模糊和相机畸变等效果，这些效果在光栅化中难以实现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00296', 'title': 'VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data', 'url': 'https://huggingface.co/papers/2410.00296', 'abstract': 'Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications. Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data. To address the issue, we introduce VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, we present an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, our framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods. Disclaimer: This paper may contain offensive examples; reader discretion is advised.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': 'eb03a7f1b6890abe', 'data': {'categories': ['#cv', '#security', '#training', '#data', '#synthetic', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'VLMGuard: защита визуально-языковых моделей от вредоносных промптов', 'desc': 'В статье представлен VLMGuard - новый подход к обнаружению вредоносных промптов для визуально-языковых моделей (VLM). Авторы предлагают использовать неразмеченные пользовательские запросы для обучения классификатора, способного отличать безопасные промпты от вредоносных. Ключевая особенность метода - автоматическая оценка вредоносности промптов без необходимости ручной разметки данных. Эксперименты показывают, что VLMGuard значительно превосходит существующие методы обнаружения вредоносных промптов для VLM.'}, 'en': {'title': 'VLMGuard: Safeguarding Vision-Language Models from Malicious Prompts', 'desc': 'This paper introduces VLMGuard, a framework designed to detect malicious prompts in vision-language models (VLMs) without needing extensive labeled data. It utilizes unlabeled user prompts, which are common in real-world applications, to differentiate between benign and malicious inputs. By implementing an automated maliciousness estimation score, VLMGuard can effectively train a binary classifier to enhance the reliability of VLM outputs. The results demonstrate that VLMGuard significantly outperforms existing methods in detecting adversarial prompts, ensuring safer use of VLMs in various applications.'}, 'zh': {'title': 'VLMGuard：提升视觉语言模型的安全性', 'desc': '视觉语言模型（VLMs）在理解视觉和文本信息的上下文中至关重要。然而，它们对恶意输入的脆弱性带来了重大风险，影响了输出的可靠性。为了解决这一问题，我们提出了VLMGuard，一个新颖的学习框架，利用未标记的用户提示来检测恶意提示。我们的框架通过自动化的恶意性估计评分，能够在未标记的数据中区分良性和恶意样本，从而训练出一个二元提示分类器。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00316', 'title': 'EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control', 'url': 'https://huggingface.co/papers/2410.00316', 'abstract': 'While recent advances in Text-to-Speech (TTS) technology produce natural and expressive speech, they lack the option for users to select emotion and control intensity. We propose EmoKnob, a framework that allows fine-grained emotion control in speech synthesis with few-shot demonstrative samples of arbitrary emotion. Our framework leverages the expressive speaker representation space made possible by recent advances in foundation voice cloning models. Based on the few-shot capability of our emotion control framework, we propose two methods to apply emotion control on emotions described by open-ended text, enabling an intuitive interface for controlling a diverse array of nuanced emotions. To facilitate a more systematic emotional speech synthesis field, we introduce a set of evaluation metrics designed to rigorously assess the faithfulness and recognizability of emotion control frameworks. Through objective and subjective evaluations, we show that our emotion control framework effectively embeds emotions into speech and surpasses emotion expressiveness of commercial TTS services.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '3b1f67773dd59956', 'data': {'categories': ['#audio', '#dataset', '#benchmark', '#open_source', '#synthetic'], 'emoji': '🗣️', 'ru': {'title': 'Тонкая настройка эмоций в синтезированной речи', 'desc': 'Исследователи представили EmoKnob - фреймворк для точного контроля эмоций в синтезе речи с использованием малого числа демонстрационных образцов. Система использует пространство представлений выразительных голосов, созданное современными моделями клонирования голоса. Предложены два метода для применения эмоционального контроля на основе текстовых описаний эмоций. Авторы также ввели набор метрик для оценки точности и узнаваемости систем эмоционального контроля в синтезе речи.'}, 'en': {'title': 'EmoKnob: Fine-Grained Emotion Control in Speech Synthesis', 'desc': 'This paper introduces EmoKnob, a novel framework for enhancing Text-to-Speech (TTS) systems by allowing users to control emotions and their intensity in synthesized speech. It utilizes few-shot learning techniques to enable emotion control based on limited examples, leveraging advanced voice cloning models for expressive speaker representation. The framework includes methods for interpreting open-ended text descriptions of emotions, providing a user-friendly interface for nuanced emotional expression. Additionally, the authors propose new evaluation metrics to assess the effectiveness of emotion control in TTS, demonstrating that EmoKnob outperforms existing commercial TTS services in emotional expressiveness.'}, 'zh': {'title': '情感控制，语音合成的新突破', 'desc': '本文提出了一种名为EmoKnob的框架，旨在改善文本到语音（TTS）技术中的情感控制。该框架允许用户通过少量示例样本来精细调节合成语音的情感和强度。我们利用了基础语音克隆模型的进展，构建了一个富有表现力的说话者表示空间。通过引入一套评估指标，我们系统地评估了情感控制的有效性，结果表明该框架在情感表达上超越了现有的商业TTS服务。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01723', 'title': 'HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration', 'url': 'https://huggingface.co/papers/2410.01723', 'abstract': 'Diffusion Transformers (DiTs) have gained prominence for outstanding scalability and extraordinary performance in generative tasks. However, their considerable inference costs impede practical deployment. The feature cache mechanism, which involves storing and retrieving redundant computations across timesteps, holds promise for reducing per-step inference time in diffusion models. Most existing caching methods for DiT are manually designed. Although the learning-based approach attempts to optimize strategies adaptively, it suffers from discrepancies between training and inference, which hampers both the performance and acceleration ratio. Upon detailed analysis, we pinpoint that these discrepancies primarily stem from two aspects: (1) Prior Timestep Disregard, where training ignores the effect of cache usage at earlier timesteps, and (2) Objective Mismatch, where the training target (align predicted noise in each timestep) deviates from the goal of inference (generate the high-quality image). To alleviate these discrepancies, we propose HarmoniCa, a novel method that Harmonizes training and inference with a novel learning-based Caching framework built upon Step-Wise Denoising Training (SDT) and Image Error Proxy-Guided Objective (IEPO). Compared to the traditional training paradigm, the newly proposed SDT maintains the continuity of the denoising process, enabling the model to leverage information from prior timesteps during training, similar to the way it operates during inference. Furthermore, we design IEPO, which integrates an efficient proxy mechanism to approximate the final image error caused by reusing the cached feature. Therefore, IEPO helps balance final image quality and cache utilization, resolving the issue of training that only considers the impact of cache usage on the predicted output at each timestep.', 'score': 4, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '1347f88bc3c0da94', 'data': {'categories': ['#training', '#inference', '#optimization', '#diffusion', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'Гармонизация обучения и вывода для эффективного кэширования в диффузионных моделях', 'desc': 'Статья представляет новый метод HarmoniCa для оптимизации кэширования в диффузионных трансформерах. Авторы выявили несоответствия между обучением и выводом в существующих подходах к кэшированию. Предложенный метод включает пошаговое обучение шумоподавлению (SDT) и целевую функцию, основанную на прокси-оценке ошибки изображения (IEPO). HarmoniCa позволяет эффективно балансировать качество итогового изображения и использование кэша, улучшая производительность и ускорение диффузионных моделей.'}, 'en': {'title': 'HarmoniCa: Bridging Training and Inference for Efficient Diffusion Transformers', 'desc': 'This paper introduces HarmoniCa, a new method designed to improve the efficiency of Diffusion Transformers (DiTs) in generative tasks by addressing the discrepancies between training and inference. The authors identify two main issues: the neglect of prior timestep effects during training and the mismatch between training objectives and inference goals. HarmoniCa employs a Step-Wise Denoising Training (SDT) approach to ensure that the model learns to utilize cached features effectively, mirroring its inference behavior. Additionally, the Image Error Proxy-Guided Objective (IEPO) is introduced to optimize the balance between image quality and cache usage, enhancing the overall performance of DiTs during deployment.'}, 'zh': {'title': 'HarmoniCa：提升扩散模型推理效率的创新方法', 'desc': '扩散变换器（DiTs）在生成任务中表现出色，但其推理成本较高，限制了实际应用。我们提出了一种新的方法HarmoniCa，通过一种基于学习的缓存框架来协调训练和推理过程，解决了训练和推理之间的差异。该方法采用逐步去噪训练（SDT）和图像误差代理引导目标（IEPO），使模型在训练时能够利用之前时间步的信息。通过这种方式，HarmoniCa提高了推理效率，同时保持了生成图像的高质量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.20325', 'title': 'Old Optimizer, New Norm: An Anthology', 'url': 'https://huggingface.co/papers/2409.20325', 'abstract': 'Deep learning optimizers are often motivated through a mix of convex and approximate second-order theory. We select three such methods -- Adam, Shampoo and Prodigy -- and argue that each method can instead be understood as a squarely first-order method without convexity assumptions. In fact, after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm. By generalizing this observation, we chart a new design space for training algorithms. Different operator norms should be assigned to different tensors based on the role that the tensor plays within the network. For example, while linear and embedding layers may have the same weight space of R^{mtimes n}, these layers play different roles and should be assigned different norms. We hope that this idea of carefully metrizing the neural architecture might lead to more stable, scalable and indeed faster training.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'cf41fa7b2190f430', 'data': {'categories': ['#math', '#training', '#optimization', '#architecture'], 'emoji': '🧭', 'ru': {'title': 'Новый взгляд на оптимизацию нейросетей: метрика важнее, чем мы думали', 'desc': 'Статья рассматривает популярные оптимизаторы глубокого обучения, такие как Adam, Shampoo и Prodigy, с новой точки зрения. Авторы утверждают, что эти методы можно интерпретировать как методы первого порядка без предположений о выпуклости. Они показывают, что каждый метод эквивалентен градиентному спуску с определенной нормой. На основе этого наблюдения предлагается новый подход к разработке алгоритмов обучения, где различным тензорам присваиваются разные операторные нормы в зависимости от их роли в нейронной сети.'}, 'en': {'title': 'Reimagining Optimizers: Tailoring Norms for Neural Network Efficiency', 'desc': 'This paper explores deep learning optimizers, specifically Adam, Shampoo, and Prodigy, and reinterprets them as first-order methods without relying on convexity. The authors demonstrate that by disabling exponential moving averages, these optimizers can be viewed as steepest descent methods under specific norms. They propose a novel approach to designing training algorithms by assigning different operator norms to tensors based on their roles in the neural network. This careful metrization of the architecture aims to enhance the stability, scalability, and speed of the training process.'}, 'zh': {'title': '优化器的新视角：根据角色分配范数', 'desc': '深度学习优化器通常基于凸性和近似二阶理论进行设计。本文选择了三种优化方法——Adam、Shampoo和Prodigy，并提出它们可以被理解为不依赖于凸性假设的一阶方法。通过关闭指数移动平均，这些方法实际上等同于在特定范数下的最陡下降法。我们建议根据张量在网络中的角色，为不同的张量分配不同的算子范数，以期实现更稳定、更可扩展和更快速的训练。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01440', 'title': 'Closed-loop Long-horizon Robotic Planning via Equilibrium Sequence Modeling', 'url': 'https://huggingface.co/papers/2410.01440', 'abstract': 'In the endeavor to make autonomous robots take actions, task planning is a major challenge that requires translating high-level task descriptions into long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate a self-refining scheme that iteratively refines a draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in a simple supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with better scaling for inference computation. Code is available at https://github.com/Singularity0104/equilibrium-planner.', 'score': 3, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '903dc847118efa24', 'data': {'categories': ['#reasoning', '#inference', '#rl', '#optimization', '#benchmark', '#open_source', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Самоулучшающееся планирование для автономных роботов', 'desc': 'В этой статье представлен новый подход к планированию действий для автономных роботов с использованием самоулучшающейся схемы. Метод итеративно уточняет черновой план до достижения равновесия, что позволяет обучать планировщики путем простого контролируемого обучения. Предложена процедура моделирования вложенных равновесных последовательностей для эффективного планирования с обратной связью. Метод показал улучшенную производительность на бенчмарке VirtualHome-Env с лучшим масштабированием вычислений при выводе.'}, 'en': {'title': 'Self-Refining Planning for Autonomous Robots', 'desc': 'This paper addresses the challenge of task planning in autonomous robots, which involves converting high-level tasks into detailed action sequences. The authors propose a self-refining planning approach that iteratively improves an initial draft plan until it stabilizes at an optimal solution. This method can be trained using supervised learning without needing extra verification systems or reward models, making it simpler to implement. Additionally, they introduce a nested equilibrium sequence modeling technique that enhances planning efficiency by utilizing feedback from the environment or an internal model.'}, 'zh': {'title': '自我精炼：提升机器人任务规划的智能', 'desc': '本文探讨了自主机器人任务规划的挑战，特别是将高层任务描述转化为长时间的行动序列。尽管语言模型代理有了进展，但它们在规划时仍容易出错，且前瞻性有限。为了解决这些问题，本文提出了一种自我精炼的方案，通过迭代优化草拟计划，直到达到平衡状态。该方法可以从分析的角度进行端到端优化，无需额外的验证器或奖励模型，且在VirtualHome-Env基准测试中表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01481', 'title': 'SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios', 'url': 'https://huggingface.co/papers/2410.01481', 'abstract': 'The systematic evaluation of speech separation and enhancement models under moving sound source conditions typically requires extensive data comprising diverse scenarios. However, real-world datasets often contain insufficient data to meet the training and evaluation requirements of models. Although synthetic datasets offer a larger volume of data, their acoustic simulations lack realism. Consequently, neither real-world nor synthetic datasets effectively fulfill practical needs. To address these issues, we introduce SonicSim, a synthetic toolkit de-designed to generate highly customizable data for moving sound sources. SonicSim is developed based on the embodied AI simulation platform, Habitat-sim, supporting multi-level adjustments, including scene-level, microphone-level, and source-level, thereby generating more diverse synthetic data. Leveraging SonicSim, we constructed a moving sound source benchmark dataset, SonicSet, using the Librispeech, the Freesound Dataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the Matterport3D to evaluate speech separation and enhancement models. Additionally, to validate the differences between synthetic data and real-world data, we randomly selected 5 hours of raw data without reverberation from the SonicSet validation set to record a real-world speech separation dataset, which was then compared with the corresponding synthetic datasets. Similarly, we utilized the real-world speech enhancement dataset RealMAN to validate the acoustic gap between other synthetic datasets and the SonicSet dataset for speech enhancement. The results indicate that the synthetic data generated by SonicSim can effectively generalize to real-world scenarios. Demo and code are publicly available at https://cslikai.cn/SonicSim/.', 'score': 2, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'd284601ce4d1a07d', 'data': {'categories': ['#audio', '#dataset', '#synthetic', '#data', '#benchmark', '#open_source', '#robotics', '#3d'], 'emoji': '🔊', 'ru': {'title': 'SonicSim: Реалистичная симуляция движущихся источников звука для задач обработки речи', 'desc': 'SonicSim - это инструментарий для создания синтетических данных для задач разделения и улучшения речи в условиях движущихся источников звука. Он основан на платформе Habitat-sim и позволяет настраивать множество параметров на уровне сцены, микрофонов и источников звука. С помощью SonicSim был создан бенчмарк-датасет SonicSet для оценки моделей обработки речи. Эксперименты показали, что синтетические данные, сгенерированные SonicSim, хорошо обобщаются на реальные сценарии.'}, 'en': {'title': 'SonicSim: Bridging the Gap in Speech Separation Datasets', 'desc': 'This paper presents SonicSim, a synthetic toolkit designed to create customizable datasets for evaluating speech separation and enhancement models in dynamic environments. Traditional datasets often lack the necessary diversity and realism, which SonicSim addresses by allowing multi-level adjustments for various sound source conditions. The authors constructed a benchmark dataset called SonicSet, which combines data from multiple sources to facilitate comprehensive model evaluation. Results show that the synthetic data generated by SonicSim can effectively generalize to real-world applications, bridging the gap between synthetic and real-world datasets.'}, 'zh': {'title': 'SonicSim：为移动声源生成真实感合成数据的工具', 'desc': '本论文介绍了一种名为SonicSim的合成工具包，用于生成可定制的移动声源数据。该工具包基于Habitat-sim平台，支持多层次的调整，能够生成更丰富的合成数据。通过SonicSim，我们构建了一个移动声源基准数据集SonicSet，用于评估语音分离和增强模型。研究结果表明，SonicSim生成的合成数据能够有效地推广到真实场景中。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01518', 'title': 'InfiniPot: Infinite Context Processing on Memory-Constrained LLMs', 'url': 'https://huggingface.co/papers/2410.01518', 'abstract': 'Handling long input contexts remains a significant challenge for Large Language Models (LLMs), particularly in resource-constrained environments such as mobile devices. Our work aims to address this limitation by introducing InfiniPot, a novel KV cache control framework designed to enable pre-trained LLMs to manage extensive sequences within fixed memory constraints efficiently, without requiring additional training. InfiniPot leverages Continual Context Distillation (CCD), an iterative process that compresses and retains essential information through novel importance metrics, effectively maintaining critical data even without access to future context. Our comprehensive evaluations indicate that InfiniPot significantly outperforms models trained for long contexts in various NLP tasks, establishing its efficacy and versatility. This work represents a substantial advancement toward making LLMs applicable to a broader range of real-world scenarios.', 'score': 2, 'issue_id': 1, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'a13831078cca1658', 'data': {'categories': ['#long_context', '#training', '#optimization', '#inference'], 'emoji': '🔄', 'ru': {'title': 'Эффективная обработка длинного контекста в LLM без переобучения', 'desc': 'InfiniPot - это новая система управления кэшем ключ-значение для больших языковых моделей (LLM). Она позволяет обрабатывать длинные последовательности в условиях ограниченной памяти без дополнительного обучения. InfiniPot использует метод непрерывной дистилляции контекста (CCD) для сжатия и сохранения важной информации. Система превосходит модели, обученные для работы с длинным контекстом, в различных задачах обработки естественного языка.'}, 'en': {'title': 'InfiniPot: Efficient Long Context Management for LLMs', 'desc': 'This paper presents InfiniPot, a new framework that helps Large Language Models (LLMs) handle long input sequences efficiently, especially on devices with limited resources. It uses a technique called Continual Context Distillation (CCD) to compress important information and keep it accessible without needing extra training. By focusing on key data, InfiniPot allows LLMs to perform better in natural language processing tasks compared to models specifically trained for long contexts. This advancement makes it easier to use LLMs in various real-world applications where memory is a constraint.'}, 'zh': {'title': 'InfiniPot：高效管理长上下文的创新框架', 'desc': '处理长输入上下文对大型语言模型（LLMs）仍然是一个重大挑战，尤其是在资源受限的环境中，如移动设备。我们的研究提出了InfiniPot，这是一种新颖的KV缓存控制框架，旨在使预训练的LLMs能够在固定内存限制内高效管理大量序列，而无需额外训练。InfiniPot利用持续上下文蒸馏（CCD），通过新颖的重要性度量迭代压缩和保留关键信息，即使在没有未来上下文的情况下也能有效维护重要数据。我们的综合评估表明，InfiniPot在各种自然语言处理任务中显著优于为长上下文训练的模型，证明了其有效性和多功能性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02740', 'title': 'Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models', 'url': 'https://huggingface.co/papers/2410.02740', 'abstract': 'Recent advancements in multimodal models highlight the value of rewritten captions for improving performance, yet key challenges remain. For example, while synthetic captions often provide superior quality and image-text alignment, it is not clear whether they can fully replace AltTexts: the role of synthetic captions and their interaction with original web-crawled AltTexts in pre-training is still not well understood. Moreover, different multimodal foundation models may have unique preferences for specific caption formats, but efforts to identify the optimal captions for each model remain limited. In this work, we propose a novel, controllable, and scalable captioning pipeline designed to generate diverse caption formats tailored to various multimodal models. By examining Short Synthetic Captions (SSC) towards Dense Synthetic Captions (DSC+) as case studies, we systematically explore their effects and interactions with AltTexts across models such as CLIP, multimodal LLMs, and diffusion models. Our findings reveal that a hybrid approach that keeps both synthetic captions and AltTexts can outperform the use of synthetic captions alone, improving both alignment and performance, with each model demonstrating preferences for particular caption formats. This comprehensive analysis provides valuable insights into optimizing captioning strategies, thereby advancing the pre-training of multimodal foundation models.', 'score': 52, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '81cb4e856bb4642c', 'data': {'categories': ['#training', '#data', '#optimization', '#diffusion', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Оптимизация подписей для мультимодальных моделей: гибридный подход побеждает', 'desc': 'Статья исследует влияние синтетических подписей к изображениям на производительность мультимодальных моделей машинного обучения. Авторы предлагают новый метод генерации разнообразных форматов подписей, адаптированных под различные модели. Исследование показывает, что гибридный подход, сочетающий синтетические подписи и оригинальные AltText, превосходит использование только синтетических подписей. Результаты демонстрируют, что каждая модель имеет предпочтения к определенным форматам подписей, что важно для оптимизации стратегий предобучения мультимодальных моделей.'}, 'en': {'title': 'Optimizing Multimodal Models with Hybrid Captioning Strategies', 'desc': 'This paper discusses the importance of using rewritten captions to enhance the performance of multimodal models, which combine text and images. It highlights the challenges of understanding how synthetic captions interact with original AltTexts during pre-training. The authors introduce a new captioning pipeline that generates various caption formats tailored to different models, such as CLIP and multimodal LLMs. Their research shows that using both synthetic captions and AltTexts together leads to better model performance and alignment than using synthetic captions alone.'}, 'zh': {'title': '优化多模态模型的标题生成策略', 'desc': '最近多模态模型的进展显示，重写的标题对提高性能有重要价值，但仍面临一些挑战。例如，虽然合成标题通常提供更好的质量和图像-文本对齐，但尚不清楚它们是否可以完全替代AltTexts。不同的多模态基础模型可能对特定的标题格式有独特的偏好，但识别每个模型的最佳标题的努力仍然有限。我们的研究提出了一种新颖、可控且可扩展的标题生成管道，旨在为各种多模态模型生成多样化的标题格式。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02367', 'title': 'SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration', 'url': 'https://huggingface.co/papers/2410.02367', 'abstract': 'The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1 times and 2.7 times, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation.', 'score': 45, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '9c0f4a698ce3fbd7', 'data': {'categories': ['#video', '#cv', '#inference', '#optimization', '#architecture'], 'emoji': '⚡', 'ru': {'title': 'SageAttention: Революция в квантизации механизма внимания', 'desc': 'В статье представлен метод SageAttention для эффективной квантизации механизма внимания в трансформерах. Авторы анализируют возможность квантизации внимания и предлагают подход, превосходящий существующие методы по скорости и точности. SageAttention демонстрирует значительное ускорение по сравнению с FlashAttention2 и xformers, сохраняя высокую точность. Эксперименты подтверждают эффективность метода для различных моделей обработки языка, генерации изображений и видео.'}, 'en': {'title': 'Revolutionizing Attention: SageAttention for Efficient Quantization', 'desc': 'This paper addresses the computational challenges of the attention mechanism in transformer models, which has a complexity of O(N^2). It highlights that while quantization techniques have been effective for linear layers, they have not been adequately applied to attention mechanisms. The authors introduce SageAttention, a novel quantization method specifically designed for attention, which significantly improves operational efficiency. Experimental results demonstrate that SageAttention not only accelerates processing speed but also maintains high accuracy across various applications, including language processing and image generation.'}, 'zh': {'title': '高效量化注意力机制的SageAttention', 'desc': '本文探讨了变换器架构中的注意力机制，指出其计算复杂度为O(N^2)，在处理长序列时成为主要的时间消耗部分。尽管量化技术在加速模型推理方面有效，但现有方法主要集中在优化线性层。为此，本文详细分析了注意力机制中量化的可行性，并提出了一种高效且准确的量化方法SageAttention。实验结果表明，SageAttention在操作每秒（OPS）方面优于FlashAttention2和xformers，且在多种模型上几乎没有损失端到端指标。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02073', 'title': 'Depth Pro: Sharp Monocular Metric Depth in Less Than a Second', 'url': 'https://huggingface.co/papers/2410.02073', 'abstract': 'We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro', 'score': 40, 'issue_id': 10, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'b8db8c8f9dcb5574', 'data': {'categories': ['#dataset', '#cv', '#training', '#benchmark', '#open_source', '#architecture', '#synthetic'], 'emoji': '🔍', 'ru': {'title': 'Революция в оценке глубины: быстро, точно и без дополнительных данных', 'desc': 'Авторы представляют фундаментальную модель для zero-shot оценки метрической глубины по монокулярным изображениям. Модель Depth Pro синтезирует высокодетализированные карты глубины с беспрецедентной четкостью и высокочастотными деталями. Предсказания модели метрические, с абсолютным масштабом, и не требуют дополнительных метаданных, таких как параметры камеры. Модель работает быстро, создавая карту глубины размером 2.25 мегапикселя за 0.3 секунды на стандартном GPU.'}, 'en': {'title': 'Depth Pro: Fast and Accurate Depth Estimation Without Metadata', 'desc': 'This paper introduces Depth Pro, a foundation model designed for zero-shot metric monocular depth estimation. It generates high-resolution depth maps that are sharp and detailed, without needing camera metadata for scale. The model operates quickly, producing a 2.25-megapixel depth map in just 0.3 seconds on a standard GPU. Key innovations include a multi-scale vision transformer for dense predictions and a training method that effectively combines real and synthetic data to enhance accuracy and boundary detail.'}, 'zh': {'title': 'Depth Pro：快速高效的单目深度估计模型', 'desc': '我们提出了一种用于零-shot度量单目深度估计的基础模型，称为Depth Pro。该模型能够合成高分辨率的深度图，具有无与伦比的清晰度和高频细节。它的预测是度量的，具有绝对尺度，不依赖于相机内参等元数据。Depth Pro在标准GPU上以0.3秒的速度生成2.25百万像素的深度图，展现了其高效性和准确性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02757', 'title': 'Loong: Generating Minute-level Long Videos with Autoregressive Language Models', 'url': 'https://huggingface.co/papers/2410.02757', 'abstract': 'It is desirable but challenging to generate content-rich long videos in the scale of minutes. Autoregressive large language models (LLMs) have achieved great success in generating coherent and long sequences of tokens in the domain of natural language processing, while the exploration of autoregressive LLMs for video generation is limited to generating short videos of several seconds. In this work, we conduct a deep analysis of the challenges that prevent autoregressive LLM-based video generators from generating long videos. Based on the observations and analysis, we propose Loong, a new autoregressive LLM-based video generator that can generate minute-long videos. Specifically, we model the text tokens and video tokens as a unified sequence for autoregressive LLMs and train the model from scratch. We propose progressive short-to-long training with a loss re-weighting scheme to mitigate the loss imbalance problem for long video training. We further investigate inference strategies, including video token re-encoding and sampling strategies, to diminish error accumulation during inference. Our proposed Loong can be trained on 10-second videos and be extended to generate minute-level long videos conditioned on text prompts, as demonstrated by the results. More samples are available at: https://epiphqny.github.io/Loong-video.', 'score': 36, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '8003554c44032bf4', 'data': {'categories': ['#video', '#long_context', '#training', '#inference', '#optimization', '#diffusion', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Loong: прорыв в генерации длинных видео с помощью LLM', 'desc': 'Статья представляет новый генератор видео Loong, основанный на авторегрессивных языковых моделях (LLM). Авторы анализируют проблемы генерации длинных видео и предлагают решения, включая прогрессивное обучение от коротких к длинным видео и переоценку видеотокенов. Модель Loong обучается на 10-секундных видео, но способна генерировать минутные ролики по текстовым запросам. Это значительный прогресс в области генерации длинных видео с помощью LLM.'}, 'en': {'title': 'Unlocking Long Video Generation with Loong!', 'desc': 'This paper addresses the challenge of generating long videos using autoregressive large language models (LLMs), which have been successful in natural language processing but struggle with video generation. The authors introduce Loong, a novel video generator that treats text and video tokens as a unified sequence, allowing for the creation of minute-long videos. They implement a progressive training approach that gradually increases video length while addressing loss imbalance, and explore various inference strategies to reduce errors. The results show that Loong can effectively generate longer videos based on text prompts, marking a significant advancement in video generation technology.'}, 'zh': {'title': '生成分钟级长视频的新突破', 'desc': '本论文探讨了使用自回归大型语言模型（LLMs）生成长视频的挑战。尽管LLMs在自然语言处理领域取得了成功，但在视频生成方面，现有方法仅能生成几秒钟的短视频。我们提出了一种新的自回归LLM视频生成器Loong，能够生成分钟级长视频。通过将文本和视频标记建模为统一序列，并采用逐步短到长的训练策略，我们有效解决了长视频训练中的损失不平衡问题。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02713', 'title': 'Video Instruction Tuning With Synthetic Data', 'url': 'https://huggingface.co/papers/2410.02713', 'abstract': 'The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.', 'score': 36, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'f4ccb1c1c9671dde', 'data': {'categories': ['#video', '#dataset', '#training', '#data', '#benchmark', '#games', '#open_source', '#synthetic', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Синтетические данные открывают новые возможности для видео-ИИ', 'desc': 'Исследователи представили новый подход к созданию мультимодальных моделей для работы с видео. Они создали синтетический набор данных LLaVA-Video-178K для обучения выполнению инструкций в видео. На основе этого набора данных была разработана модель LLaVA-Video, показавшая высокие результаты в различных видео-ориентированных задачах. Авторы планируют опубликовать набор данных, процесс его генерации и чекпоинты модели.'}, 'en': {'title': 'Synthetic Data for Superior Video Understanding', 'desc': "This paper presents a solution to the challenge of gathering high-quality data for training video large multimodal models (LMMs). The authors introduce a synthetic dataset called LLaVA-Video-178K, designed for video instruction-following tasks, which includes detailed captioning and question-answering. By utilizing this dataset alongside existing visual instruction tuning data, they develop a new video LMM named LLaVA-Video. Their experiments show that LLaVA-Video performs well on various video benchmarks, demonstrating the dataset's effectiveness in enhancing model training."}, 'zh': {'title': '合成数据集助力视频多模态模型发展', 'desc': '本论文提出了一种新方法，解决了视频大规模多模态模型（LMMs）在获取高质量原始数据时的困难。我们创建了一个专门用于视频指令跟随的高质量合成数据集，称为LLaVA-Video-178K。该数据集包含详细的字幕、开放式问答和多项选择问答等关键任务。通过在这个数据集上训练，并结合现有的视觉指令调优数据，我们推出了新的视频LMM——LLaVA-Video，并在多个视频基准测试中表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02712', 'title': 'LLaVA-Critic: Learning to Evaluate Multimodal Models', 'url': 'https://huggingface.co/papers/2410.02712', 'abstract': "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation criteria and scenarios. Our experiments demonstrate the model's effectiveness in two key areas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation scores, performing on par with or surpassing GPT models on multiple evaluation benchmarks; and (2) Preference Learning, where it generates reward signals for preference learning, enhancing model alignment capabilities. This work underscores the potential of open-source LMMs in self-critique and evaluation, setting the stage for future research into scalable, superhuman alignment feedback mechanisms for LMMs.", 'score': 34, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '529e51b7f382eb97', 'data': {'categories': ['#dataset', '#training', '#interpretability', '#benchmark', '#alignment', '#open_source', '#rlhf', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'LLaVA-Critic: открытая мультимодальная модель для самокритики и оценки ИИ', 'desc': 'Статья представляет LLaVA-Critic - первую открытую большую мультимодальную модель (LMM) для оценки различных мультимодальных задач. Модель обучена на высококачественном наборе данных с инструкциями для критики. LLaVA-Critic показывает эффективность в роли судьи, предоставляя надежные оценки, и в обучении с подкреплением, генерируя сигналы вознаграждения. Это исследование открывает перспективы для создания масштабируемых механизмов обратной связи для выравнивания LMM.'}, 'en': {'title': 'LLaVA-Critic: The Future of Multimodal Evaluation', 'desc': 'LLaVA-Critic is a groundbreaking open-source large multimodal model (LMM) that serves as a generalist evaluator for various multimodal tasks. It is trained on a comprehensive dataset that includes diverse evaluation criteria, allowing it to assess performance effectively. The model excels in two main areas: providing reliable evaluation scores comparable to advanced GPT models and generating reward signals for preference learning to improve model alignment. This research highlights the promise of open-source LMMs in self-evaluation and sets a foundation for future advancements in alignment feedback mechanisms.'}, 'zh': {'title': 'LLaVA-Critic：开源多模态模型的评估新纪元', 'desc': 'LLaVA-Critic是首个开源的大型多模态模型，旨在作为通用评估者，评估多种多模态任务的表现。该模型通过高质量的批评指令跟随数据集进行训练，涵盖了多样的评估标准和场景。实验表明，LLaVA-Critic在两个关键领域表现出色：作为评判者提供可靠的评估分数，并在偏好学习中生成奖励信号，增强模型的对齐能力。此项工作展示了开源多模态模型在自我批评和评估中的潜力，为未来可扩展的超人类对齐反馈机制研究奠定了基础。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02724', 'title': 'Large Language Models as Markov Chains', 'url': 'https://huggingface.co/papers/2410.02724', 'abstract': 'Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size T and context window of size K and Markov chains defined on a finite state space of size O(T^K). We derive several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. We then prove pre-training and in-context generalization bounds and show how the drawn equivalence allows us to enrich their interpretation. Finally, we illustrate our theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice.', 'score': 31, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'faad779778001c6f', 'data': {'categories': ['#reasoning', '#training', '#math', '#optimization', '#interpretability', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая тайны эффективности больших языковых моделей через призму цепей Маркова', 'desc': 'Статья исследует теоретические основы эффективности больших языковых моделей (LLM). Авторы проводят аналогию между авторегрессионными языковыми моделями и цепями Маркова на конечном пространстве состояний. Они анализируют существование стационарного распределения, скорость сходимости и влияние температуры на эти характеристики. Исследователи также доказывают границы предобучения и обобщения в контексте, подкрепляя теоретические выводы экспериментами на современных LLM.'}, 'en': {'title': 'Unraveling the Power of Large Language Models through Markov Chains', 'desc': 'This paper explores the theoretical foundations of large language models (LLMs) by establishing a connection between autoregressive language models and Markov chains. It reveals that LLMs can be understood through the lens of Markov chains with a specific state space, which helps explain their performance and convergence properties. The authors derive important results regarding the stationary distribution of these chains and how temperature affects convergence speed. Additionally, they provide generalization bounds and validate their theoretical insights through experiments on contemporary LLMs, demonstrating the practical implications of their findings.'}, 'zh': {'title': '揭示大型语言模型的理论基础', 'desc': '本文探讨了大型语言模型（LLMs）在自然语言处理任务中的表现及其理论基础。我们将通用自回归语言模型与有限状态空间上的马尔可夫链建立了等价关系。研究发现，马尔可夫链的平稳分布能够捕捉LLMs的推理能力，并分析了温度对收敛速度的影响。通过实验验证了我们的理论保证，展示了LLMs在实际应用中的行为。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02746', 'title': 'Contrastive Localized Language-Image Pre-Training', 'url': 'https://huggingface.co/papers/2410.02746', 'abstract': 'Contrastive Language-Image Pre-training (CLIP) has been a celebrated method for training vision encoders to generate image/text representations facilitating various applications. Recently, CLIP has been widely adopted as the vision backbone of multimodal large language models (MLLMs) to connect image inputs for language interactions. The success of CLIP as a vision-language foundation model relies on aligning web-crawled noisy text annotations at image levels. Nevertheless, such criteria may become insufficient for downstream tasks in need of fine-grained vision representations, especially when region-level understanding is demanding for MLLMs. In this paper, we improve the localization capability of CLIP with several advances. We propose a pre-training method called Contrastive Localized Language-Image Pre-training (CLOC) by complementing CLIP with region-text contrastive loss and modules. We formulate a new concept, promptable embeddings, of which the encoder produces image embeddings easy to transform into region representations given spatial hints. To support large-scale pre-training, we design a visually-enriched and spatially-localized captioning framework to effectively generate region-text pseudo-labels at scale. By scaling up to billions of annotated images, CLOC enables high-quality regional embeddings for image region recognition and retrieval tasks, and can be a drop-in replacement of CLIP to enhance MLLMs, especially on referring and grounding tasks.', 'score': 31, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'cc226eaa6867dda5', 'data': {'categories': ['#cv', '#training', '#graphs', '#optimization', '#transfer_learning', '#alignment', '#architecture', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'CLOC: Улучшение локализации в предобучении визуально-языковых моделей', 'desc': "Статья представляет новый метод предварительного обучения под названием CLOC (Contrastive Localized Language-Image Pre-training), который улучшает возможности локализации модели CLIP. CLOC дополняет CLIP контрастивной потерей на уровне регионов и текста, а также вводит концепцию 'promptable embeddings'. Для поддержки масштабного предобучения авторы разработали фреймворк для генерации псевдо-меток регион-текст. CLOC демонстрирует улучшенные результаты в задачах распознавания и поиска регионов изображений, а также может заменить CLIP в мультимодальных больших языковых моделях."}, 'en': {'title': 'Enhancing Image Understanding with CLOC for Multimodal Models', 'desc': 'This paper introduces Contrastive Localized Language-Image Pre-training (CLOC), an enhancement of the CLIP model that improves its ability to understand images at a finer level. CLOC incorporates region-text contrastive loss and new modules to better align image regions with corresponding text descriptions. The authors propose a novel concept called promptable embeddings, which allows the model to easily convert image embeddings into detailed region representations using spatial hints. By leveraging a large-scale, visually-enriched captioning framework, CLOC generates high-quality regional embeddings, making it suitable for tasks that require precise image localization and retrieval in multimodal large language models.'}, 'zh': {'title': '提升CLIP的区域理解能力', 'desc': '对比语言-图像预训练（CLIP）是一种用于训练视觉编码器的方法，能够生成图像和文本的表示，广泛应用于多模态大语言模型（MLLMs）。本文提出了一种改进的预训练方法，称为对比局部化语言-图像预训练（CLOC），通过区域-文本对比损失增强CLIP的定位能力。我们引入了可提示嵌入的概念，使得编码器能够根据空间提示轻松转换为区域表示。CLOC通过生成大规模的区域-文本伪标签，提升了图像区域识别和检索任务的质量，能够有效替代CLIP，特别是在引用和定位任务中。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02416', 'title': 'Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models', 'url': 'https://huggingface.co/papers/2410.02416', 'abstract': 'Classifier-free guidance (CFG) is crucial for improving both generation quality and alignment between the input condition and final output in diffusion models. While a high guidance scale is generally required to enhance these aspects, it also causes oversaturation and unrealistic artifacts. In this paper, we revisit the CFG update rule and introduce modifications to address this issue. We first decompose the update term in CFG into parallel and orthogonal components with respect to the conditional model prediction and observe that the parallel component primarily causes oversaturation, while the orthogonal component enhances image quality. Accordingly, we propose down-weighting the parallel component to achieve high-quality generations without oversaturation. Additionally, we draw a connection between CFG and gradient ascent and introduce a new rescaling and momentum method for the CFG update rule based on this insight. Our approach, termed adaptive projected guidance (APG), retains the quality-boosting advantages of CFG while enabling the use of higher guidance scales without oversaturation. APG is easy to implement and introduces practically no additional computational overhead to the sampling process. Through extensive experiments, we demonstrate that APG is compatible with various conditional diffusion models and samplers, leading to improved FID, recall, and saturation scores while maintaining precision comparable to CFG, making our method a superior plug-and-play alternative to standard classifier-free guidance.', 'score': 25, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'd40106a5a7b0cb85', 'data': {'categories': ['#cv', '#training', '#optimization', '#diffusion', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Улучшение качества генерации изображений без пересыщения', 'desc': 'Статья представляет новый метод адаптивного проецируемого руководства (APG) для улучшения качества генерации изображений в диффузионных моделях. Авторы предлагают модификацию классического метода руководства без классификатора (CFG), разделяя обновление на параллельные и ортогональные компоненты. APG снижает вес параллельного компонента, что позволяет использовать более высокие масштабы руководства без пересыщения. Метод легко реализуется, практически не увеличивает вычислительные затраты и демонстрирует улучшение показателей FID и recall при сохранении точности.'}, 'en': {'title': 'Enhancing Diffusion Models with Adaptive Projected Guidance', 'desc': 'This paper focuses on improving classifier-free guidance (CFG) in diffusion models, which is essential for generating high-quality outputs that align well with input conditions. The authors identify that while a high guidance scale enhances generation quality, it can also lead to oversaturation and unrealistic artifacts. They propose a new method called adaptive projected guidance (APG) that modifies the CFG update rule by down-weighting the oversaturating component, allowing for better image quality without the negative effects of high guidance scales. Extensive experiments show that APG outperforms standard CFG in various metrics, making it a practical and efficient alternative for enhancing diffusion model outputs.'}, 'zh': {'title': '自适应投影引导：提升生成质量的无分类器引导新方法', 'desc': '本文探讨了无分类器引导（CFG）在扩散模型中的重要性，特别是在生成质量和输入条件与最终输出之间的对齐方面。我们发现，CFG的更新规则可以分解为平行和正交两个部分，其中平行部分会导致过饱和现象，而正交部分则提升图像质量。为了解决过饱和问题，我们提出了一种新的自适应投影引导（APG）方法，通过降低平行部分的权重来实现高质量生成，同时允许使用更高的引导比例。实验结果表明，APG在多种条件扩散模型和采样器中表现出色，提升了FID、召回率和饱和度分数，同时保持了与CFG相当的精度。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01679', 'title': 'VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment', 'url': 'https://huggingface.co/papers/2410.01679', 'abstract': "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative.", 'score': 22, 'issue_id': 10, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '70e8bd770c7d370b', 'data': {'categories': ['#reasoning', '#training', '#math', '#rl', '#optimization', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'VinePPO: Эффективное обучение языковых моделей для сложных рассуждений', 'desc': 'Статья представляет новый подход к обучению больших языковых моделей (LLM) для решения сложных задач рассуждения. Авторы выявили недостатки в существующем методе Proximal Policy Optimization (PPO) при работе с задачами, требующими многоступенчатых рассуждений. Они предложили альтернативный метод VinePPO, который использует оценки Монте-Карло вместо сетей значений для более точного присвоения вознаграждений. VinePPO показал превосходные результаты на наборах данных MATH и GSM8K, требуя меньше вычислительных ресурсов.'}, 'en': {'title': 'VinePPO: A Better Way to Assign Credit in Complex Reasoning Tasks', 'desc': 'This paper discusses the challenges of credit assignment in reinforcement learning (RL) for large language models (LLMs) when performing complex reasoning tasks. It highlights the limitations of using value networks in the Proximal Policy Optimization (PPO) algorithm, which often leads to inaccurate predictions and poor performance. The authors introduce VinePPO, a new method that utilizes unbiased Monte Carlo estimates to improve credit assignment without relying on large value networks. Their experiments show that VinePPO significantly outperforms PPO and other baselines, achieving better results with fewer updates and less computation time.'}, 'zh': {'title': 'VinePPO：提升大型语言模型推理性能的新方法', 'desc': '大型语言模型（LLMs）在复杂推理任务中越来越多地被应用，这些任务需要在获得奖励之前执行多个复杂步骤。正确地分配这些步骤的信用对于提高模型性能至关重要。我们提出了一种新的方法VinePPO，它利用语言环境的灵活性来计算无偏的蒙特卡洛估计，避免了大型价值网络的需求。我们的实验表明，VinePPO在MATH和GSM8K数据集上表现优于传统的PPO算法，且所需的梯度更新次数和时间显著减少。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02678', 'title': 'Distilling an End-to-End Voice Assistant Without Instruction Training Data', 'url': 'https://huggingface.co/papers/2410.02678', 'abstract': 'Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to models ``forgetting" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using >100x less training compute.', 'score': 22, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'c0f8d0752bf51e22', 'data': {'categories': ['#audio', '#multilingual', '#training', '#machine_translation', '#transfer_learning', '#alignment', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'Эффективное обучение речевых ассистентов без размеченных данных', 'desc': 'В статье представлен новый подход к обучению речевых моделей больших языковых моделей (LLM) без использования размеченных данных. Авторы предлагают метод, основанный на самообучении, используя ответы текстовой LLM на транскрипты речи в качестве целевых данных. Разработанная модель DiVA (Distilled Voice Assistant) демонстрирует способность к обобщению в задачах устного ответа на вопросы, классификации и перевода. Результаты показывают, что DiVA лучше соответствует предпочтениям пользователей, превосходя современные модели при значительно меньших вычислительных затратах на обучение.'}, 'en': {'title': 'Revolutionizing Voice Assistants with Self-Supervised Learning', 'desc': 'This paper introduces a new approach for training Speech Large Language Models (LLMs) that combines audio and text processing more effectively. Instead of relying on supervised finetuning with instruction data, the proposed method uses self-supervision by leveraging responses from a text-only LLM to improve speech understanding. The resulting model, called Distilled Voice Assistant (DiVA), demonstrates strong performance in tasks like Spoken Question Answering, Classification, and Translation. Notably, DiVA achieves a higher user satisfaction rate while requiring significantly less computational resources compared to existing models.'}, 'zh': {'title': '创新语音助手：自我监督的语音大型语言模型', 'desc': '本论文提出了一种新的训练语音大型语言模型（LLMs）的方法，旨在解决传统语音助手在音频和文本建模中信息丢失的问题。我们的方法利用文本-only LLM对转录文本的响应作为自我监督，而无需使用标注数据。实验结果表明，我们的Distilled Voice Assistant（DiVA）在口语问答、分类和翻译任务中表现出色，并且在用户偏好上优于现有的最先进模型。值得注意的是，DiVA在训练计算资源上节省了超过100倍。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19291', 'title': 'CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling', 'url': 'https://huggingface.co/papers/2409.19291', 'abstract': 'In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. This deficiency significantly limits the ability of a single CLIP model to handle images rich in visual detail. In this work, we propose a simple yet effective model-agnostic strategy, Diversified Multiplet Upcycling (DMU), for CLIP. DMU efficiently fine-tunes a series of CLIP models that capture different feature spaces, from a dense pre-trained CLIP checkpoint, sharing parameters except for the Feed-Forward Network (FFN). These models can then be transformed into a CLIP-MoE with a larger model capacity, leading to significantly enhanced performance with minimal computational overhead. To the best of our knowledge, Diversified Multiplet Upcycling is the first approach to introduce sparsely activated MoE into CLIP foundation models. Extensive experiments demonstrate the significant performance of CLIP-MoE across various zero-shot retrieval, zero-shot image classification tasks, and downstream Multimodal Large Language Model (MLLM) benchmarks by serving as a vision encoder. Furthermore, Diversified Multiplet Upcycling enables the conversion of any dense CLIP model into CLIP-MoEs, which can seamlessly replace CLIP in a plug-and-play manner without requiring further adaptation in downstream frameworks. Through Diversified Multiplet Upcycling, we aim to provide valuable insights for future research on developing more efficient and effective multimodal learning systems.', 'score': 18, 'issue_id': 10, 'pub_date': '2024-09-28', 'pub_date_card': {'ru': '28 сентября', 'en': 'September 28', 'zh': '9月28日'}, 'hash': 'c79e62159ed005c9', 'data': {'categories': ['#cv', '#training', '#optimization', '#transfer_learning', '#benchmark', '#architecture', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'Повышение эффективности CLIP с помощью разнообразного мультиплетного апсайклинга', 'desc': 'В статье представлен новый подход к улучшению модели CLIP под названием Diversified Multiplet Upcycling (DMU). DMU позволяет создать серию моделей CLIP, захватывающих различные пространства признаков, путем тонкой настройки плотной предобученной модели CLIP. Эти модели затем преобразуются в CLIP-MoE с большей емкостью модели, что значительно повышает производительность при минимальных вычислительных затратах. Эксперименты показывают значительное улучшение результатов в задачах нулевого обучения и мультимодальных языковых моделях.'}, 'en': {'title': 'Enhancing CLIP with Diversified Multiplet Upcycling for Better Multimodal Learning', 'desc': 'This paper introduces Diversified Multiplet Upcycling (DMU), a novel strategy to enhance the performance of Contrastive Language-Image Pre-training (CLIP) models. DMU fine-tunes multiple CLIP models that focus on different feature spaces while sharing most parameters, except for the Feed-Forward Network (FFN). This approach transforms these models into a CLIP-Mixture of Experts (MoE), which increases model capacity and improves performance on tasks like zero-shot retrieval and image classification. The method allows for easy integration of CLIP-MoEs into existing systems, paving the way for more efficient multimodal learning.'}, 'zh': {'title': '多样化多重升级：提升CLIP模型性能的新策略', 'desc': '近年来，对比语言-图像预训练（CLIP）已成为多模态智能的基石。然而，研究发现CLIP编码过程中存在显著的信息损失，且CLIP往往只能捕捉输入的粗粒度特征。这一缺陷限制了单一CLIP模型处理视觉细节丰富的图像的能力。我们提出了一种简单而有效的模型无关策略——多样化多重升级（DMU），通过高效微调一系列捕捉不同特征空间的CLIP模型，显著提升了性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02525', 'title': 'Contextual Document Embeddings', 'url': 'https://huggingface.co/papers/2410.02525', 'abstract': 'Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. We propose two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. We achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes. Our method can be applied to improve performance on any contrastive learning dataset and any biencoder.', 'score': 16, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'b45b19a592899862', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Контекст имеет значение: новый подход к эмбеддингам документов', 'desc': 'Статья предлагает новый подход к созданию контекстуализированных векторных представлений документов для нейронного поиска. Авторы утверждают, что традиционные методы создания эмбеддингов документов не учитывают контекст их использования. Они предлагают два метода: новую контрастивную функцию потерь, учитывающую соседние документы, и архитектуру, кодирующую информацию о соседях в представление документа. Результаты показывают улучшение производительности по сравнению с биэнкодерами, особенно на данных вне обучающей выборки.'}, 'en': {'title': 'Context Matters: Enhancing Document Embeddings with Contextualization', 'desc': 'This paper discusses the importance of dense document embeddings in neural retrieval systems. The authors argue that traditional embeddings do not consider the context provided by neighboring documents, which can limit their effectiveness. They propose two new methods for creating contextualized document embeddings that incorporate information from related documents. Their approach shows significant improvements in retrieval performance, especially in challenging scenarios, and sets new benchmarks without relying on complex training techniques.'}, 'zh': {'title': '上下文化文档嵌入，提升检索性能！', 'desc': '本文探讨了密集文档嵌入在神经检索中的重要性。我们认为，现有的嵌入方法虽然有效，但在特定检索任务中缺乏上下文信息。为此，我们提出了两种方法来生成上下文化的文档嵌入，分别是通过对比学习目标和新的上下文化架构。实验结果表明，这两种方法在多个设置中均优于传统的双编码器，尤其在域外任务中表现更为突出。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02749', 'title': 'Training Language Models on Synthetic Edit Sequences Improves Code Synthesis', 'url': 'https://huggingface.co/papers/2410.02749', 'abstract': 'Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already scarce, high-quality edit data is even scarcer. To fill this gap, we develop a synthetic data generation algorithm called LintSeq. This algorithm refactors existing code into a sequence of code edits by using a linter to procedurally sample across the error-free insertions that can be used to sequentially write programs. It outputs edit sequences as text strings consisting of consecutive program diffs. To test LintSeq, we use it to refactor a dataset of instruction + program pairs into instruction + program-diff-sequence tuples. Then, we instruction finetune a series of smaller LLMs ranging from 2.6B to 14B parameters on both the re-factored and original versions of this dataset, comparing zero-shot performance on code synthesis benchmarks. We show that during repeated sampling, edit sequence finetuned models produce more diverse programs than baselines. This results in better inference-time scaling for benchmark coverage as a function of samples, i.e. the fraction of problems "pass@k" solved by any attempt given "k" tries. For example, on HumanEval pass@50, small LLMs finetuned on synthetic edit sequences are competitive with GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%) in absolute score. Finally, we also pretrain our own tiny LMs for code understanding. We show that finetuning tiny models on synthetic code edits results in state-of-the-art code synthesis for the on-device model class. Our 150M parameter edit sequence LM matches or outperforms code models with twice as many parameters, both with and without repeated sampling, including Codex and AlphaCode.', 'score': 12, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '07238a3d10e1aa04', 'data': {'categories': ['#dataset', '#training', '#data', '#plp', '#benchmark', '#open_source', '#small_models', '#synthetic'], 'emoji': '✏️', 'ru': {'title': 'LintSeq: Улучшение генерации кода через обучение на синтетических последовательностях правок', 'desc': 'Статья представляет новый алгоритм LintSeq для генерации синтетических данных редактирования кода. Этот метод преобразует существующие программы в последовательности правок, используя линтер для пошагового создания кода. Исследователи применили LintSeq для обучения небольших языковых моделей и сравнили их производительность с базовыми моделями на задачах синтеза кода. Результаты показывают, что модели, обученные на синтетических последовательностях правок, генерируют более разнообразные программы и достигают лучших показателей при многократном сэмплировании.'}, 'en': {'title': 'Enhancing Code Synthesis with Synthetic Edit Sequences', 'desc': 'This paper introduces LintSeq, a synthetic data generation algorithm designed to create high-quality edit sequences for code synthesis. By utilizing a linter, LintSeq refactors existing code into a series of error-free edits, producing text strings that represent program diffs. The authors demonstrate that fine-tuning smaller language models on these synthetic edit sequences leads to improved performance in generating diverse programs compared to traditional training methods. Notably, their approach allows smaller models to achieve competitive results against larger models like GPT-4, showcasing the effectiveness of using synthetic edit data for code synthesis tasks.'}, 'zh': {'title': '用LintSeq填补代码编辑数据的空白', 'desc': '本文提出了一种名为LintSeq的合成数据生成算法，旨在解决高质量代码编辑数据稀缺的问题。该算法通过使用linter对现有代码进行重构，生成一系列代码编辑序列，以便于程序的逐步编写。研究表明，经过LintSeq处理的小型语言模型在代码合成基准测试中表现优异，能够生成更具多样性的程序。最终，作者的150M参数模型在代码理解和合成方面的表现超过了许多参数更多的模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01782', 'title': 'Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models', 'url': 'https://huggingface.co/papers/2410.01782', 'abstract': 'Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/', 'score': 10, 'issue_id': 10, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'dbbea1382f0a84e5', 'data': {'categories': ['#reasoning', '#rag', '#inference', '#transfer_learning', '#open_source', '#small_models', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Open-RAG: Повышение интеллекта открытых языковых моделей', 'desc': 'Open-RAG - это новая система для улучшения способности рассуждения у моделей с открытым исходным кодом в контексте RAG (Retrieval-Augmented Generation). Она преобразует плотную языковую модель в разреженную смесь экспертов (MoE), способную решать сложные задачи рассуждения. Open-RAG обучает модель работать с отвлекающими факторами и использует латентное обучение для динамического выбора экспертов. Система также предлагает гибридный метод адаптивного поиска для оптимизации производительности.'}, 'en': {'title': 'Enhancing Reasoning in RAG with Open-RAG Framework', 'desc': 'This paper presents Open-RAG, a new framework that improves the reasoning abilities of Retrieval-Augmented Generation (RAG) models using open-source Large Language Models (LLMs). Open-RAG transforms a dense LLM into a sparse mixture of experts (MoE) model, allowing it to tackle complex reasoning tasks more effectively. The framework is designed to help the model distinguish between relevant and misleading information, enhancing its ability to provide accurate responses. Additionally, it introduces a hybrid adaptive retrieval method to optimize the balance between performance and speed during inference.'}, 'zh': {'title': 'Open-RAG：提升开源LLM推理能力的创新框架', 'desc': '本文介绍了一种新框架Open-RAG，旨在提高开源大型语言模型（LLMs）在检索增强生成（RAG）中的推理能力。该框架将任意稠密LLM转变为一种参数高效的稀疏专家混合模型（MoE），能够处理复杂的推理任务，包括单跳和多跳查询。Open-RAG通过训练模型识别相关但具有误导性的干扰信息，从而有效整合外部知识，提供更准确和上下文相关的回答。此外，本文还提出了一种混合自适应检索方法，以平衡性能提升与推理速度之间的权衡。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02115', 'title': 'L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?', 'url': 'https://huggingface.co/papers/2410.02115', 'abstract': 'Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization. As the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM outputs is insufficient, as it is quite challenging for humans to verify the results from the extremely lengthy context. Yet, although some efforts have been made to assess whether LCMs respond truly based on the context, these works either are limited to specific tasks or heavily rely on external evaluation resources like GPT-4.In this work, we introduce L-CiteEval, a comprehensive multi-task benchmark for long-context understanding with citations, aiming to evaluate both the understanding capability and faithfulness of LCMs. L-CiteEval covers 11 tasks from diverse domains, spanning context lengths from 8K to 48K, and provides a fully automated evaluation suite. Through testing with 11 cutting-edge closed-source and open-source LCMs, we find that although these models show minor differences in their generated results, open-source models substantially trail behind their closed-source counterparts in terms of citation accuracy and recall. This suggests that current open-source LCMs are prone to responding based on their inherent knowledge rather than the given context, posing a significant risk to the user experience in practical applications. We also evaluate the RAG approach and observe that RAG can significantly improve the faithfulness of LCMs, albeit with a slight decrease in the generation quality. Furthermore, we discover a correlation between the attention mechanisms of LCMs and the citation generation process.', 'score': 10, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '0d1ec9ec865dc20b', 'data': {'categories': ['#long_context', '#rag', '#interpretability', '#benchmark', '#alignment', '#open_source', '#architecture'], 'emoji': '📏', 'ru': {'title': 'L-CiteEval: Новый стандарт оценки достоверности моделей с длинным контекстом', 'desc': 'Статья представляет L-CiteEval - комплексный многозадачный бенчмарк для оценки понимания длинного контекста и достоверности моделей LCM (Long-context models). Бенчмарк охватывает 11 задач из разных областей с длиной контекста от 8К до 48К токенов и предоставляет полностью автоматизированный набор для оценки. Результаты тестирования 11 современных LCM показали, что модели с открытым исходным кодом существенно отстают от закрытых моделей по точности и полноте цитирования. Исследование также выявило, что подход RAG может значительно повысить достоверность LCM, хотя и с небольшим снижением качества генерации.'}, 'en': {'title': 'Evaluating Long-Context Models: Faithfulness and Understanding with L-CiteEval', 'desc': 'This paper presents L-CiteEval, a new benchmark designed to evaluate long-context models (LCMs) on their understanding and faithfulness when handling extensive text. It includes 11 diverse tasks with context lengths ranging from 8K to 48K, allowing for a comprehensive assessment of LCM performance. The study reveals that while closed-source LCMs outperform open-source models in citation accuracy and recall, the latter often rely on their pre-existing knowledge rather than the provided context. Additionally, the research highlights the effectiveness of the RAG approach in enhancing the faithfulness of LCM outputs, despite a slight trade-off in generation quality.'}, 'zh': {'title': '提升长上下文模型的理解与真实性', 'desc': '长上下文模型（LCMs）在处理长文本任务方面取得了显著进展，尤其是在文档摘要方面。随着对生成结果的真实性的重视，仅仅保证输出的准确性已不足以满足需求，因为人类很难验证来自极长上下文的结果。为此，我们提出了L-CiteEval，这是一个全面的多任务基准，旨在评估LCMs的理解能力和真实性。我们的研究表明，开源模型在引用准确性和召回率方面明显落后于闭源模型，这表明当前的开源LCMs更倾向于基于固有知识作答，而非依赖于给定的上下文。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02762', 'title': 'Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations', 'url': 'https://huggingface.co/papers/2410.02762', 'abstract': "We investigate the internal representations of vision-language models (VLMs) to address hallucinations, a persistent challenge despite advances in model size and training. We project VLMs' internal image representations to their language vocabulary and observe more confident output probabilities on real objects than hallucinated objects. We additionally use these output probabilities to spatially localize real objects. Building on this approach, we introduce a knowledge erasure algorithm that removes hallucinations by linearly orthogonalizing image features with respect to hallucinated object features. We show that targeted edits to a model's latent representations can reduce hallucinations by up to 25.7% on the COCO2014 dataset while preserving performance. Our findings demonstrate how a deeper understanding of VLMs' latent representations can enhance reliability and enable novel capabilities, such as zero-shot segmentation.", 'score': 9, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '8ecc2787bf47eaca', 'data': {'categories': ['#dataset', '#cv', '#hallucinations', '#interpretability', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Борьба с галлюцинациями в VLM через анализ скрытых представлений', 'desc': 'Исследователи изучают внутренние представления моделей компьютерного зрения и обработки естественного языка (VLM) для решения проблемы галлюцинаций. Они проецируют внутренние представления изображений на языковой словарь модели и наблюдают более уверенные вероятности для реальных объектов по сравнению с галлюцинациями. На основе этого подхода авторы разрабатывают алгоритм удаления знаний, который устраняет галлюцинации путем линейной ортогонализации признаков изображения относительно признаков галлюцинируемых объектов. Результаты показывают, что целенаправленные изменения в скрытых представлениях модели могут снизить галлюцинации до 25.7% на наборе данных COCO2014 при сохранении производительности.'}, 'en': {'title': 'Enhancing VLM Reliability by Reducing Hallucinations', 'desc': "This paper explores how vision-language models (VLMs) represent images and words to tackle the issue of hallucinations, which are incorrect outputs generated by the models. The authors find that VLMs are more confident in identifying real objects compared to hallucinated ones, and they use this insight to improve object localization. They propose a knowledge erasure algorithm that modifies the model's internal features to reduce hallucinations while maintaining overall performance. Their results indicate that by refining the latent representations of VLMs, hallucinations can be decreased significantly, leading to more reliable model outputs and new functionalities like zero-shot segmentation."}, 'zh': {'title': '提升视觉-语言模型的可靠性与能力', 'desc': '我们研究了视觉-语言模型（VLMs）的内部表示，以解决幻觉问题，这在模型规模和训练进展的情况下仍然是一个持续的挑战。我们将VLMs的内部图像表示投影到其语言词汇上，观察到真实物体的输出概率比幻觉物体更有信心。我们还利用这些输出概率对真实物体进行空间定位。基于此方法，我们引入了一种知识消除算法，通过线性正交化图像特征与幻觉物体特征来消除幻觉。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02052', 'title': 'Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning', 'url': 'https://huggingface.co/papers/2410.02052', 'abstract': "Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, we introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, we improve the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, our GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning.", 'score': 9, 'issue_id': 10, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '026dc6add144373f', 'data': {'categories': ['#reasoning', '#cv', '#training', '#agi', '#rl', '#optimization', '#agents', '#benchmark', '#transfer_learning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'R-MCTS: Повышение эффективности ИИ-агентов через рефлексивный поиск и самообучение', 'desc': 'Статья представляет новый алгоритм Reflective Monte Carlo Tree Search (R-MCTS), который улучшает способность ИИ-агентов исследовать пространство решений в реальном времени. R-MCTS расширяет традиционный MCTS, добавляя контрастную рефлексию и многоагентные дебаты для более эффективного поиска и оценки состояний. Исследователи также применили самообучение для дополнительной настройки модели GPT-4o, используя траектории, сгенерированные R-MCTS. Результаты показывают значительное улучшение производительности на бенчмарке VisualWebArena по сравнению с предыдущими методами.'}, 'en': {'title': 'Enhancing AI Decision-Making with R-MCTS', 'desc': 'This paper presents Reflective Monte Carlo Tree Search (R-MCTS), a new algorithm that improves the decision-making abilities of AI agents, particularly those using vision-language models like GPT-4o. R-MCTS enhances traditional Monte Carlo Tree Search by integrating contrastive reflection for learning from past experiences and employing multi-agent debate for better state evaluations. The authors demonstrate that their approach leads to significant performance improvements on the VisualWebArena benchmark, achieving up to 30% better results compared to previous models. Additionally, the fine-tuned GPT-4o retains most of the R-MCTS performance while being more efficient in terms of computational resources.'}, 'zh': {'title': '提升智能体决策能力的新方法', 'desc': '自主智能体在自动化复杂的多步骤决策任务中展现出显著潜力。然而，即使是最先进的视觉-语言模型（VLMs），如GPT-4o，在复杂的网络环境和长远规划任务中仍然无法达到人类水平的表现。为了解决这些问题，我们提出了一种新颖的测试时算法——反思蒙特卡洛树搜索（R-MCTS），旨在增强AI智能体的决策空间探索能力。通过对传统MCTS的扩展，R-MCTS结合了对比反思和多智能体辩论，显著提高了智能体的搜索效率和状态评估能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02458', 'title': 'MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation', 'url': 'https://huggingface.co/papers/2410.02458', 'abstract': 'Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentation by integrating pre-trained LLM transformer blocks. Our approach, which incorporates a frozen LLM transformer block into the encoder of a ViT-based model, leads to substantial improvements in segmentation performance across various medical imaging modalities. We propose a Hybrid Attention Mechanism that combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales. The enhanced model shows significant performance gains, including an average Dice score increase from 0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index. These results demonstrate the effectiveness of LLM-based transformers in refining medical image segmentation, highlighting their potential to significantly boost model accuracy and robustness. The source code and our implementation are available at: https://bit.ly/3zf2CVs', 'score': 9, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '28682265fba39b78', 'data': {'categories': ['#science', '#cv', '#healthcare', '#open_source', '#architecture'], 'emoji': '🩻', 'ru': {'title': 'Улучшение сегментации медицинских изображений с помощью LLM-трансформеров', 'desc': 'Это исследование посвящено улучшению сегментации медицинских изображений с помощью интеграции предобученных блоков трансформеров из больших языковых моделей (LLM) в Vision Transformers (ViT). Авторы предлагают гибридный механизм внимания, сочетающий глобальное и локальное обучение признаков, а также блок мультимасштабного слияния. Результаты показывают значительное улучшение производительности сегментации для различных модальностей медицинской визуализации, включая повышение среднего показателя Dice с 0.74 до 0.79. Это исследование демонстрирует потенциал LLM-трансформеров для повышения точности и надежности моделей сегментации медицинских изображений.'}, 'en': {'title': 'Boosting Medical Image Segmentation with LLMs and Vision Transformers', 'desc': 'This paper investigates the use of Large Language Models (LLMs) to improve medical image segmentation, which is essential for accurate diagnostics. The authors enhance Vision Transformers (ViTs) by integrating pre-trained LLM transformer blocks, resulting in better segmentation performance. They introduce a Hybrid Attention Mechanism that effectively combines global and local feature learning, along with a Multi-Scale Fusion Block to aggregate features from different scales. The proposed model shows significant improvements in key metrics, demonstrating the potential of LLMs to enhance the accuracy and robustness of medical image segmentation tasks.'}, 'zh': {'title': '利用大型语言模型提升医学图像分割性能', 'desc': '本研究探讨了如何利用大型语言模型（LLMs）来提升医学图像分割的性能。我们将预训练的LLM变换器模块集成到视觉变换器（ViT）模型的编码器中，从而显著提高了不同医学成像模式下的分割效果。通过提出混合注意力机制，结合全局和局部特征学习，以及多尺度融合模块，我们实现了特征的有效聚合。实验结果显示，模型的Dice分数从0.74提高到0.79，准确率、精确率和Jaccard指数也有显著提升，证明了LLM变换器在医学图像分割中的有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02103', 'title': 'MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis', 'url': 'https://huggingface.co/papers/2410.02103', 'abstract': 'Recent works in volume rendering, e.g. NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries. To solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions: 1) We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants. 2) Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions. 3) Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views. 4) By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically. As a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.', 'score': 8, 'issue_id': 10, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'eac41a9d952f603f', 'data': {'categories': ['#cv', '#training', '#graphs', '#optimization', '#3d'], 'emoji': '🎭', 'ru': {'title': 'Многоракурсная оптимизация для улучшения 3D Gaussian Splatting', 'desc': 'Эта статья представляет новый метод оптимизации для 3D Gaussian Splatting (3DGS), улучшающий рендеринг и геометрическую точность. Авторы предлагают многоракурсную стратегию обучения вместо традиционного одноракурсного подхода, что помогает избежать переобучения на отдельных ракурсах. Они также вводят схему кросс-внутренней регуляризации, стратегию кросс-лучевого уплотнения и многоракурсное аугментированное уплотнение. Эти инновации повышают общую точность реконструкции в различных сценариях и вариантах гауссовых моделей.'}, 'en': {'title': 'Enhancing 3D Gaussian Splatting with Multi-View Optimization', 'desc': 'This paper presents a new optimization method for 3D Gaussian Splatting (3DGS) to enhance rendering quality and efficiency. The authors shift from a single-view to a multi-view training strategy, which helps prevent overfitting and improves the accuracy of novel-view synthesis. They introduce a cross-intrinsic guidance scheme for a more effective training process and a cross-ray densification strategy to increase the density of Gaussian kernels in critical areas. Finally, they propose a multi-view augmented densification approach to ensure sufficient Gaussian representation, leading to better 3D reconstruction accuracy.'}, 'zh': {'title': '多视图优化提升3D渲染精度', 'desc': '本文提出了一种新的3D高斯点渲染优化方法，旨在解决传统单视图训练导致的过拟合问题。通过引入多视图训练策略，优化3D高斯属性，从而提高了在不同场景下的整体准确性。我们还提出了交叉内在引导方案和交叉光线稠密化策略，以增强训练过程中的细节表现。最终，采用多视图增强稠密化策略，显著提高了重建精度。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02763', 'title': 'Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos', 'url': 'https://huggingface.co/papers/2410.02763', 'abstract': 'There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understanding long-form videos. However, is this really the case? Our studies indicate that LMMs still lack many fundamental reasoning capabilities even when dealing with short videos. We introduce Vinoground, a temporal counterfactual LMM evaluation benchmark encompassing 1000 short and natural video-caption pairs. We demonstrate that existing LMMs severely struggle to distinguish temporal differences between different actions and object transformations. For example, the best model GPT-4o only obtains ~50% on our text and video scores, showing a large gap compared to the human baseline of ~90%. All open-source multimodal models and CLIP-based models perform much worse, producing mostly random chance performance. Through this work, we shed light onto the fact that temporal reasoning in short videos is a problem yet to be fully solved. The dataset and evaluation code are available at https://vinoground.github.io.', 'score': 7, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '3b6bf220f7ee6708', 'data': {'categories': ['#reasoning', '#video', '#dataset', '#benchmark', '#open_source', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Большие мультимодальные модели все еще не понимают время в коротких видео', 'desc': 'Статья посвящена оценке способности больших мультимодальных моделей (LMM) к пониманию коротких видео. Авторы представляют новый бенчмарк Vinoground, состоящий из 1000 пар видео-подпись для оценки темпорального рассуждения моделей. Результаты показывают, что даже лучшие LMM, такие как GPT-4o, значительно уступают людям в этой задаче, достигая лишь 50% точности. Исследование демонстрирует, что проблема темпорального рассуждения в коротких видео все еще далека от полного решения.'}, 'en': {'title': 'Unveiling the Gaps in Video Comprehension: The Vinoground Challenge', 'desc': 'This paper discusses the limitations of large multimodal models (LMMs) in understanding short videos, despite recent claims of their effectiveness. The authors introduce Vinoground, a benchmark designed to evaluate LMMs on their ability to reason about temporal aspects in video content. Their findings reveal that even the best-performing model, GPT-4o, only achieves around 50% accuracy in distinguishing temporal differences, significantly lower than the human baseline of approximately 90%. This highlights that the challenge of temporal reasoning in video comprehension remains unresolved, indicating a need for further research and development in this area.'}, 'zh': {'title': '短视频理解中的时间推理挑战', 'desc': '近年来，现代大型多模态模型（LMMs）在短视频理解方面取得了一定进展，但在理解长视频时仍面临更复杂的挑战。我们的研究表明，即使在处理短视频时，LMMs仍然缺乏基本的推理能力。我们引入了Vinoground，这是一个包含1000对短视频和自然语言描述的时间反事实评估基准。结果显示，现有的LMMs在区分不同动作和物体变化的时间差异方面表现不佳，表明短视频中的时间推理问题尚未完全解决。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02056', 'title': 'Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data', 'url': 'https://huggingface.co/papers/2410.02056', 'abstract': 'We present Synthio, a novel approach for augmenting small-scale audio classification datasets with synthetic data. Our goal is to improve audio classification accuracy with limited labeled data. Traditional data augmentation techniques, which apply artificial transformations (e.g., adding random noise or masking segments), struggle to create data that captures the true diversity present in real-world audios. To address this shortcoming, we propose to augment the dataset with synthetic audio generated from text-to-audio (T2A) diffusion models. However, synthesizing effective augmentations is challenging because not only should the generated data be acoustically consistent with the underlying small-scale dataset, but they should also have sufficient compositional diversity. To overcome the first challenge, we align the generations of the T2A model with the small-scale dataset using preference optimization. This ensures that the acoustic characteristics of the generated data remain consistent with the small-scale dataset. To address the second challenge, we propose a novel caption generation technique that leverages the reasoning capabilities of Large Language Models to (1) generate diverse and meaningful audio captions and (2) iteratively refine their quality. The generated captions are then used to prompt the aligned T2A model. We extensively evaluate Synthio on ten datasets and four simulated limited-data settings. Results indicate our method consistently outperforms all baselines by 0.1%-39% using a T2A model trained only on weakly-captioned AudioSet.', 'score': 6, 'issue_id': 10, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'e4975d472a070194', 'data': {'categories': ['#reasoning', '#audio', '#dataset', '#training', '#data', '#alignment', '#diffusion', '#synthetic', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'Синтетическое аудио для улучшения классификации при ограниченных данных', 'desc': 'Synthio - это новый подход к расширению небольших наборов данных для классификации аудио с помощью синтетических данных. Метод использует модели диффузии текст-в-аудио (T2A) для генерации дополнительных аудиозаписей. Для обеспечения акустической согласованности с исходным набором данных применяется оптимизация предпочтений. Разнообразие генерируемых образцов достигается с помощью языковых моделей, которые создают и уточняют разнообразные описания аудио.'}, 'en': {'title': 'Enhancing Audio Classification with Synthetic Data', 'desc': 'Synthio is a new method designed to enhance small audio classification datasets by adding synthetic data. It aims to improve classification accuracy when there is limited labeled data available. Unlike traditional augmentation techniques that may not capture real-world audio diversity, Synthio uses text-to-audio diffusion models to generate more representative synthetic audio. The approach includes aligning generated audio with existing data and using advanced caption generation to ensure both acoustic consistency and compositional diversity.'}, 'zh': {'title': 'Synthio：用合成数据提升音频分类准确性', 'desc': '本文介绍了一种名为Synthio的新方法，用于通过合成数据增强小规模音频分类数据集。我们的目标是提高在有限标记数据下的音频分类准确性。传统的数据增强技术难以生成真实世界音频的多样性，因此我们采用文本到音频（T2A）扩散模型生成合成音频来解决这个问题。通过优化生成偏好，我们确保生成的数据在声学特性上与小规模数据集一致，同时利用大型语言模型生成多样化的音频描述，以提高合成数据的质量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01335', 'title': 'Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models', 'url': 'https://huggingface.co/papers/2410.01335', 'abstract': 'Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate "experts" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.', 'score': 5, 'issue_id': 10, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '5828b5e66fffb240', 'data': {'categories': ['#reasoning', '#multilingual', '#training', '#math', '#interpretability', '#transfer_learning', '#benchmark', '#architecture', '#low_resource'], 'emoji': '🧠', 'ru': {'title': 'Слияние моделей для переноса математических навыков между языками', 'desc': 'Эта статья представляет методологию слияния моделей для улучшения математических рассуждений на языках, для которых нет специализированных данных. Авторы обучают отдельные модели на математических задачах на английском и на общих инструкциях на целевом языке. Затем они заменяют верхние и нижние слои трансформера математической модели слоями языковой модели. Этот метод превосходит другие подходы на 10% в четырех основных языках на бенчмарке MGSM.'}, 'en': {'title': 'Enhancing Math Performance in Non-English LLMs through Layer Swapping', 'desc': "This paper introduces a novel approach to model merging, specifically for Large Language Models (LLMs) that need to perform mathematical reasoning in non-English languages. The authors propose a method where two separate 'expert' models are fine-tuned: one on math data in English and the other on general instruction data in the target language. By swapping the transformer layers between these experts, they enhance the math performance of the model in the target language without requiring additional training data. The results show a significant improvement in performance on the math benchmark, demonstrating the effectiveness of this layer swapping technique for cross-lingual transfer and modular model solutions."}, 'zh': {'title': '跨语言模型合并，提升数学推理能力', 'desc': '本文提出了一种模型合并方法，旨在解决在非英语语言中微调大型语言模型（LLMs）的困难。我们通过将数学能力与语言能力结合，促进跨语言转移，尤其是在缺乏特定任务数据的情况下。我们从同一预训练模型开始，分别在英语的数学指令数据和目标语言的通用指令数据上微调不同的“专家”。通过直接替换数学专家的顶部和底部变换器层，我们的合并模型在数学基准测试中表现优于单独的专家和其他合并方法，提升了10%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02536', 'title': 'Intelligence at the Edge of Chaos', 'url': 'https://huggingface.co/papers/2410.02536', 'abstract': "We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the capabilities of models trained to predict these rules. Our study focuses on elementary cellular automata (ECA), simple yet powerful one-dimensional systems that generate behaviors ranging from trivial to highly complex. By training distinct Large Language Models (LLMs) on different ECAs, we evaluated the relationship between the complexity of the rules' behavior and the intelligence exhibited by the LLMs, as reflected in their performance on downstream tasks. Our findings reveal that rules with higher complexity lead to models exhibiting greater intelligence, as demonstrated by their performance on reasoning and chess move prediction tasks. Both uniform and periodic systems, and often also highly chaotic systems, resulted in poorer downstream performance, highlighting a sweet spot of complexity conducive to intelligence. We conjecture that intelligence arises from the ability to predict complexity and that creating intelligence may require only exposure to complexity.", 'score': 5, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'b962196b43ec4ddd', 'data': {'categories': ['#reasoning', '#agi', '#training', '#rl', '#games', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Сложность порождает интеллект: уроки клеточных автоматов', 'desc': 'В этой работе исследуется связь между сложностью правил клеточных автоматов и интеллектуальными способностями обученных на них языковых моделей. Авторы обнаружили, что правила с более высокой сложностью приводят к моделям, демонстрирующим большие интеллектуальные способности на задачах рассуждения и предсказания шахматных ходов. Простые однородные и периодические системы, а также сильно хаотичные системы давали худшие результаты. Исследователи предполагают, что интеллект возникает из способности предсказывать сложность.'}, 'en': {'title': 'Complexity Fuels Intelligence in AI Models', 'desc': 'This paper investigates how the complexity of rule-based systems affects the intelligence of models trained to predict these rules. It specifically examines elementary cellular automata (ECA), which are simple one-dimensional systems that can produce a wide range of behaviors. By training Large Language Models (LLMs) on various ECAs, the study finds that more complex rules lead to better performance in tasks requiring reasoning and prediction. The results suggest that there is an optimal level of complexity that enhances model intelligence, indicating that exposure to complexity may be key to developing intelligent systems.'}, 'zh': {'title': '复杂性与智能的关系', 'desc': '本研究探讨了人工系统中智能行为的出现，重点分析了基于规则的系统复杂性如何影响模型的预测能力。我们使用简单而强大的初等元胞自动机（ECA）作为研究对象，这些系统能够生成从简单到复杂的多种行为。通过对不同ECA训练大型语言模型（LLM），我们评估了规则行为的复杂性与LLM表现出的智能之间的关系。研究结果表明，复杂性较高的规则使得模型在推理和国际象棋走法预测任务中表现更好，暗示智能的产生与对复杂性的预测能力密切相关。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00255', 'title': 'Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning', 'url': 'https://huggingface.co/papers/2410.00255', 'abstract': "Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model's discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model's generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning. Notably, we achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\% improvement in the captioning task (Scan2Cap).", 'score': 5, 'issue_id': 10, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': '537bc5070d162d76', 'data': {'categories': ['#security', '#training', '#agi', '#data', '#optimization', '#agents', '#benchmark', '#architecture', '#synthetic', '#multimodal', '#3d'], 'emoji': '🤖', 'ru': {'title': 'Robin3D: Революция в понимании 3D-инструкций искусственным интеллектом', 'desc': 'В этой статье представлен Robin3D - мощная трехмерная языковая модель (3DLLM), обученная на масштабном наборе данных для выполнения инструкций. Данные были сгенерированы с помощью нового механизма RIG, который создает как состязательные, так и разнообразные инструкции для улучшения дискриминативной способности и обобщения модели. Robin3D использует улучшенные методы пространственного понимания и связывания объектов. Модель превосходит предыдущие методы на пяти широко используемых бенчмарках мультимодального обучения в 3D, без необходимости дообучения под конкретные задачи.'}, 'en': {'title': 'Robin3D: Elevating 3D Language Models with Robust Instruction Data', 'desc': 'This paper presents Robin3D, an advanced 3D Large Language Model (3DLLM) designed to improve instruction-following capabilities in 3D environments. The model is trained using a novel data generation engine called Robust Instruction Generation (RIG), which creates high-quality instruction data, including adversarial and diverse samples. By incorporating techniques like Relation-Augmented Projector and ID-Feature Bonding, Robin3D enhances its spatial understanding and object grounding abilities. The results show significant performance improvements over previous models on multiple 3D multimodal learning benchmarks, demonstrating its effectiveness without requiring task-specific fine-tuning.'}, 'zh': {'title': 'Robin3D：提升3D语言模型的指令理解能力', 'desc': '本文介绍了一种新的3D大型语言模型Robin3D，该模型通过我们的新数据引擎RIG生成的大规模指令跟随数据进行训练。RIG生成了两种关键的指令数据：对抗性指令跟随数据和多样性指令跟随数据，以增强模型的区分能力和泛化能力。Robin3D在处理复杂指令时，采用了关系增强投影器和ID特征绑定技术，提升了空间理解和物体引用能力。实验结果表明，Robin3D在多个3D多模态学习基准测试中表现优异，显著提高了模型的性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02426', 'title': 'Learning the Latent Rules of a Game from Data: A Chess Story', 'url': 'https://huggingface.co/papers/2410.02426', 'abstract': 'We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process. Inspired by Stefan Zweig\'s novella "Schachnovelle," also known as "The Royal Game" in English, we show that 28M and 125M parameter pretrained foundational small language models (SLMs) can be instruction fine-tuned with 1,000-to-1,000,000 examples to learn the rules of chess, propose legal moves, and accurately solve chess problems. We also explore the impact of successive language model fine-tuning epochs on improved outcomes and demonstrate reductions in model hallucinations by increasing the number of instruction fine-tuning examples.', 'score': 5, 'issue_id': 10, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '8713f059b61a607f', 'data': {'categories': ['#reasoning', '#hallucinations', '#training', '#games', '#small_models'], 'emoji': '♟️', 'ru': {'title': 'Малые языковые модели осваивают шахматы через обучение на примерах', 'desc': "Исследователи показали, что малые языковые модели (SLM) с миллионами параметров способны изучать скрытые правила процессов на основе связанных с ними данных. Эксперимент, вдохновленный новеллой Стефана Цвейга 'Шахматная новелла', продемонстрировал, что модели с 28 и 125 миллионами параметров могут обучиться правилам шахмат, предлагать легальные ходы и решать шахматные задачи после файнтюнинга на 1000-1000000 примерах. Изучено влияние последовательных эпох обучения на улучшение результатов. Также показано, как увеличение числа обучающих примеров снижает галлюцинации модели."}, 'en': {'title': 'Small Models, Big Moves: Learning Chess with Language Models', 'desc': 'This paper shows that small pretrained generative language models, with millions of parameters, can effectively learn the underlying rules of a specific process, such as chess, from provided data. By fine-tuning these models with varying amounts of examples, they can generate legal chess moves and solve chess problems accurately. The study highlights the importance of the number of fine-tuning epochs, showing that more training leads to better performance. Additionally, it finds that increasing the number of examples reduces the occurrence of model hallucinations, improving the reliability of the generated outputs.'}, 'zh': {'title': '小型语言模型的潜力：学习国际象棋规则', 'desc': '这篇论文展示了小型预训练生成语言模型如何从与特定过程相关的数据中学习潜在规则。研究表明，具有2800万和1.25亿参数的小型语言模型可以通过指令微调，利用1000到1000000个示例学习国际象棋的规则，提出合法的走法，并准确解决棋局问题。论文还探讨了连续微调周期对模型性能的影响，并通过增加微调示例的数量来减少模型的幻觉现象。总的来说，这项研究表明小型语言模型在特定任务上的有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01946', 'title': 'SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics', 'url': 'https://huggingface.co/papers/2410.01946', 'abstract': 'Prompt-based fine-tuning has become an essential method for eliciting information encoded in pre-trained language models for a variety of tasks, including text classification. For multi-class classification tasks, prompt-based fine-tuning under low-resource scenarios has resulted in performance levels comparable to those of fully fine-tuning methods. Previous studies have used crafted prompt templates and verbalizers, mapping from the label terms space to the class space, to solve the classification problem as a masked language modeling task. However, cross-domain and fine-grained prompt-based fine-tuning with an automatically enriched verbalizer remains unexplored, mainly due to the difficulty and costs of manually selecting domain label terms for the verbalizer, which requires humans with domain expertise. To address this challenge, we introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks. To this end, we select semantically correlated and domain-specific label terms within the context of scientific literature for verbalizer augmentation. Furthermore, we propose a new verbalization strategy that uses correlation scores as additional weights to enhance the prediction performance of the language model during model tuning. Our method outperforms state-of-the-art, prompt-based fine-tuning methods on scientific text classification tasks under few and zero-shot settings, especially in classifying fine-grained and emerging scientific topics.', 'score': 4, 'issue_id': 10, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '6a0f04b3d6aec2b3', 'data': {'categories': ['#science', '#multilingual', '#training', '#data', '#transfer_learning', '#low_resource'], 'emoji': '🧪', 'ru': {'title': 'Автоматическое обогащение вербализатора для точной классификации научных текстов', 'desc': 'Статья представляет SciPrompt - фреймворк для автоматического извлечения терминов, связанных с научными темами, для задач классификации текста с ограниченными ресурсами. Авторы предлагают новую стратегию вербализации, использующую оценки корреляции в качестве дополнительных весов для улучшения производительности языковой модели. Метод превосходит современные методы тонкой настройки на основе промптов при классификации научных текстов в условиях малого количества примеров и нулевого обучения. Особенно эффективен при классификации детальных и новых научных тем.'}, 'en': {'title': 'Automating Domain-Specific Labeling for Better Text Classification', 'desc': "This paper presents SciPrompt, a framework that enhances prompt-based fine-tuning for low-resource multi-class text classification tasks, particularly in scientific domains. It addresses the challenge of manually selecting domain-specific label terms for verbalizers by automatically retrieving relevant terms from scientific literature. The proposed method uses correlation scores to weight these terms, improving the model's prediction performance during tuning. SciPrompt demonstrates superior results compared to existing prompt-based methods, especially for fine-grained and emerging scientific topics in few and zero-shot scenarios."}, 'zh': {'title': 'SciPrompt：自动化科学文本分类的新方法', 'desc': '本文介绍了一种名为SciPrompt的框架，旨在自动检索与科学主题相关的术语，以应对低资源文本分类任务。通过选择语义相关且特定领域的标签术语，SciPrompt增强了提示模板的效果。我们提出了一种新的表述策略，利用相关性得分作为额外权重，提高语言模型的预测性能。实验结果表明，该方法在少样本和零样本设置下，尤其在细粒度和新兴科学主题的分类任务中，优于现有的提示基础微调方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.00907', 'title': 'Addition is All You Need for Energy-efficient Language Models', 'url': 'https://huggingface.co/papers/2410.00907', 'abstract': 'Large neural networks spend most computation on floating point tensor multiplications. In this work, we find that a floating point multiplier can be approximated by one integer adder with high precision. We propose the linear-complexity multiplication L-Mul algorithm that approximates floating point number multiplication with integer addition operations. The new algorithm costs significantly less computation resource than 8-bit floating point multiplication but achieves higher precision. Compared to 8-bit floating point multiplications, the proposed method achieves higher precision but consumes significantly less bit-level computation. Since multiplying floating point numbers requires substantially higher energy compared to integer addition operations, applying the L-Mul operation in tensor processing hardware can potentially reduce 95% energy cost by element-wise floating point tensor multiplications and 80% energy cost of dot products. We calculated the theoretical error expectation of L-Mul, and evaluated the algorithm on a wide range of textual, visual, and symbolic tasks, including natural language understanding, structural reasoning, mathematics, and commonsense question answering. Our numerical analysis experiments agree with the theoretical error estimation, which indicates that L-Mul with 4-bit mantissa achieves comparable precision as float8_e4m3 multiplications, and L-Mul with 3-bit mantissa outperforms float8_e5m2. Evaluation results on popular benchmarks show that directly applying L-Mul to the attention mechanism is almost lossless. We further show that replacing all floating point multiplications with 3-bit mantissa L-Mul in a transformer model achieves equivalent precision as using float8_e4m3 as accumulation precision in both fine-tuning and inference.', 'score': 143, 'issue_id': 1, 'pub_date': '2024-10-01', 'pub_date_card': {'ru': '1 октября', 'en': 'October 1', 'zh': '10月1日'}, 'hash': '2e806ebbdf3cc22d', 'data': {'categories': ['#math', '#inference', '#optimization', '#benchmark', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'L-Mul: революция в эффективности нейронных вычислений', 'desc': 'Статья представляет новый алгоритм L-Mul для аппроксимации умножения чисел с плавающей запятой с помощью операций целочисленного сложения. Этот метод потребляет значительно меньше вычислительных ресурсов, чем 8-битное умножение с плавающей запятой, но обеспечивает более высокую точность. Применение L-Mul в оборудовании для тензорной обработки потенциально может снизить энергозатраты на 95% для поэлементных умножений тензоров с плавающей запятой и на 80% для скалярных произведений. Эксперименты показали, что прямое применение L-Mul к механизму внимания практически не приводит к потерям точности.'}, 'en': {'title': 'Efficient Precision: Revolutionizing Neural Computation with L-Mul', 'desc': 'This paper introduces the L-Mul algorithm, which approximates floating point multiplications using integer additions, significantly reducing computational resources while maintaining high precision. The method is particularly energy-efficient, potentially reducing energy costs by up to 95% for element-wise tensor multiplications. Theoretical and experimental evaluations demonstrate that L-Mul with a 4-bit mantissa achieves precision comparable to traditional 8-bit floating point operations. Applying L-Mul in transformer models shows almost no loss in precision, making it a promising alternative for efficient neural network computations.'}, 'zh': {'title': '用整数加法革新浮点数乘法', 'desc': '这篇论文提出了一种新的算法L-Mul，可以用整数加法来近似浮点数乘法，从而大大减少计算资源的消耗。与传统的8位浮点数乘法相比，L-Mul不仅计算精度更高，而且能显著降低能耗。实验表明，L-Mul在多种任务中表现出色，尤其是在自然语言理解和常识问答等领域。通过将L-Mul应用于注意力机制，几乎没有精度损失，甚至在某些情况下优于传统方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03027', 'title': 'MLP-KAN: Unifying Deep Representation and Function Learning', 'url': 'https://huggingface.co/papers/2410.03027', 'abstract': 'Recent advancements in both representation learning and function learning have demonstrated substantial promise across diverse domains of artificial intelligence. However, the effective integration of these paradigms poses a significant challenge, particularly in cases where users must manually decide whether to apply a representation learning or function learning model based on dataset characteristics. To address this issue, we introduce MLP-KAN, a unified method designed to eliminate the need for manual model selection. By integrating Multi-Layer Perceptrons (MLPs) for representation learning and Kolmogorov-Arnold Networks (KANs) for function learning within a Mixture-of-Experts (MoE) architecture, MLP-KAN dynamically adapts to the specific characteristics of the task at hand, ensuring optimal performance. Embedded within a transformer-based framework, our work achieves remarkable results on four widely-used datasets across diverse domains. Extensive experimental evaluation demonstrates its superior versatility, delivering competitive performance across both deep representation and function learning tasks. These findings highlight the potential of MLP-KAN to simplify the model selection process, offering a comprehensive, adaptable solution across various domains. Our code and weights are available at https://github.com/DLYuanGod/MLP-KAN.', 'score': 28, 'issue_id': 23, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '2ee6d0e9eb9836a4', 'data': {'categories': ['#dataset', '#training', '#optimization', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'MLP-KAN: Универсальная модель для автоматического выбора между обучением представлений и функций', 'desc': 'Исследователи представили MLP-KAN - унифицированный метод, объединяющий многослойные перцептроны (MLP) для обучения представлений и сети Колмогорова-Арнольда (KAN) для обучения функций в архитектуре Mixture-of-Experts. Этот подход устраняет необходимость ручного выбора модели, динамически адаптируясь к характеристикам задачи. MLP-KAN встроен в трансформер-подобную архитектуру и показывает превосходные результаты на четырех широко используемых наборах данных из разных областей. Экспериментальная оценка демонстрирует универсальность метода и конкурентоспособную производительность как в задачах глубокого обучения представлений, так и в задачах обучения функций.'}, 'en': {'title': 'MLP-KAN: Simplifying Model Selection in AI', 'desc': 'This paper presents MLP-KAN, a novel approach that combines representation learning and function learning to simplify model selection in machine learning tasks. By utilizing Multi-Layer Perceptrons (MLPs) for representation and Kolmogorov-Arnold Networks (KANs) for function learning, MLP-KAN adapts to the specific needs of different datasets without requiring manual intervention. The method is implemented within a Mixture-of-Experts (MoE) architecture and is embedded in a transformer-based framework, showcasing its effectiveness across various domains. Experimental results indicate that MLP-KAN achieves competitive performance on multiple datasets, highlighting its versatility and potential to streamline the model selection process.'}, 'zh': {'title': '简化模型选择，提升学习效率！', 'desc': '本文介绍了一种名为MLP-KAN的统一方法，旨在简化机器学习中的模型选择过程。该方法结合了多层感知机（MLP）用于表示学习和Kolmogorov-Arnold网络（KAN）用于函数学习，采用混合专家（MoE）架构。MLP-KAN能够根据任务的特征动态调整，确保最佳性能。通过在变换器框架中嵌入，实验结果显示其在多个数据集上表现出色，展现了其在深度表示和函数学习任务中的优越性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03017', 'title': 'Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise', 'url': 'https://huggingface.co/papers/2410.03017', 'abstract': "Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. We introduce Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, we find that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p<0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. We find that Tutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students.", 'score': 25, 'issue_id': 1, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '72b9d001759c0a5d', 'data': {'categories': ['#science', '#agi', '#training', '#healthcare', '#ethics', '#agents', '#alignment'], 'emoji': '🤖👨\u200d🏫', 'ru': {'title': 'Tutor CoPilot: ИИ-помощник для демократизации качественного образования', 'desc': 'Исследование представляет систему Tutor CoPilot, которая использует языковые модели для поддержки репетиторов в реальном времени. В рандомизированном контролируемом исследовании с участием 900 репетиторов и 1800 учеников из недостаточно обслуживаемых сообществ было обнаружено, что ученики, работавшие с репетиторами, использующими Tutor CoPilot, на 4 процентных пункта чаще осваивали темы. Анализ 550 000+ сообщений показал, что репетиторы с доступом к Tutor CoPilot чаще использовали качественные педагогические стратегии. Исследование демонстрирует, как системы человек-ИИ могут масштабировать экспертные знания в реальных областях и сделать качественное образование доступным для всех учащихся.'}, 'en': {'title': 'Scaling Expertise: AI-Powered Tutoring for All', 'desc': 'The paper introduces Tutor CoPilot, a Human-AI system designed to provide expert-like guidance to tutors, enhancing the quality of education for students, especially in under-served communities. In a large-scale trial involving 900 tutors and 1,800 students, the system improved student mastery of topics by 4 percentage points, with the most significant gains seen in students taught by lower-rated tutors. The study highlights the cost-effectiveness of Tutor CoPilot, costing only $20 per tutor annually, and its ability to encourage tutors to use effective teaching strategies. Despite some issues with grade-level appropriateness, Tutor CoPilot demonstrates the potential of AI to scale expertise and improve educational outcomes.'}, 'zh': {'title': 'Tutor CoPilot：让优质教育触手可及', 'desc': '这篇论文介绍了一种名为Tutor CoPilot的创新人机协作系统，旨在通过模拟专家思维为辅导员提供类似专家的指导。研究表明，使用Tutor CoPilot的辅导员能更有效地帮助学生掌握知识，尤其是那些评分较低的辅导员，其学生的知识掌握率提高了9个百分点。Tutor CoPilot每年每位辅导员的成本仅为20美元，并且能促使辅导员采用更高质量的教学策略。尽管Tutor CoPilot在某些情况下会生成不适合年级的建议，但总体上它展示了人机协作系统在教育领域扩展专业知识的潜力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02613', 'title': 'NL-Eye: Abductive NLI for Images', 'url': 'https://huggingface.co/papers/2410.02613', 'abstract': "Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050 images) spanning diverse reasoning categories: physical, functional, logical, emotional, cultural, and social. The data curation process involved two steps - writing textual descriptions and generating images using text-to-image models, both requiring substantial human involvement to ensure high-quality and challenging scenes. Our experiments show that VLMs struggle significantly on NL-Eye, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality. This demonstrates a deficiency in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a crucial step toward developing VLMs capable of robust multimodal reasoning for real-world applications, including accident-prevention bots and generated video verification.", 'score': 22, 'issue_id': 4, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '315844ac62249b1b', 'data': {'categories': ['#reasoning', '#cv', '#data', '#interpretability', '#benchmark', '#synthetic', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'NL-Eye: раскрывая пробелы в визуальном мышлении ИИ', 'desc': 'Статья представляет NL-Eye - новый бенчмарк для оценки способностей визуальных языковых моделей (VLM) к абдуктивному рассуждению. NL-Eye адаптирует задачу абдуктивного естественно-языкового вывода к визуальной области, требуя от моделей оценивать правдоподобность гипотетических изображений на основе исходного изображения. Эксперименты показывают, что современные VLM значительно уступают людям в этой задаче, часто демонстрируя результаты на уровне случайного угадывания. NL-Eye представляет важный шаг к разработке VLM, способных к надежному мультимодальному рассуждению для реальных приложений.'}, 'en': {'title': 'Bridging the Gap: Enhancing VLMs for Real-World Reasoning', 'desc': 'The paper introduces NL-Eye, a benchmark designed to test the visual abductive reasoning skills of Visual Language Models (VLMs). It adapts the Natural Language Inference task to the visual domain, requiring models to assess the plausibility of images based on a given premise. The study finds that current VLMs struggle with this task, often performing no better than random chance, while humans excel. This highlights a significant gap in the reasoning abilities of VLMs, suggesting the need for further development to enable real-world applications like accident prevention.'}, 'zh': {'title': '提升视觉语言模型的推理能力：NL-Eye的挑战', 'desc': '这篇论文探讨了视觉语言模型（VLM）在推理因果关系方面的能力。研究团队开发了一个名为NL-Eye的基准，用于评估VLM的视觉溯因推理能力。实验结果显示，当前的VLM在NL-Eye任务中表现不佳，常常只能达到随机基线水平，而人类在可行性预测和解释质量上表现优异。这表明现代VLM在溯因推理能力上存在不足。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02703', 'title': 'Selective Attention Improves Transformer', 'url': 'https://huggingface.co/papers/2410.02703', 'abstract': "Unneeded elements in the attention's context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention improves language modeling performance in a variety of model sizes and context lengths. For example, a range of transformers trained with the language modeling objective on C4 with selective attention perform equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attention's context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity.", 'score': 22, 'issue_id': 1, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'fd0e4fdfc820e20f', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization', '#small_models', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Избирательное внимание: повышение эффективности трансформеров без дополнительных параметров', 'desc': 'Статья представляет концепцию избирательного внимания (Selective Attention) для улучшения механизма внимания в трансформерах. Этот подход уменьшает внимание к ненужным элементам в контексте, что приводит к улучшению производительности языкового моделирования. Избирательное внимание позволяет уменьшить размер буфера контекста внимания, значительно снижая требования к памяти и вычислительным ресурсам при инференсе. Эксперименты показывают, что трансформеры с избирательным вниманием могут достичь такой же производительности, как и стандартные модели с вдвое большим количеством параметров.'}, 'en': {'title': 'Selective Attention: Streamlining Efficiency in Language Models', 'desc': "The paper introduces Selective Attention, a modification to the standard attention mechanism in machine learning models that filters out unnecessary elements, enhancing performance. This approach improves language modeling across various model sizes and context lengths, making models as effective as those with twice the number of attention heads and parameters. Selective Attention also significantly reduces memory and computational requirements during inference by decreasing the size of the attention's context buffer. This innovation allows models to maintain the same level of accuracy while using substantially less memory, making them more efficient."}, 'zh': {'title': '选择性注意力：提升性能，减少资源消耗', 'desc': '这篇论文介绍了一种名为选择性注意力的新方法，可以减少注意力机制中不必要元素的影响。通过这种方法，语言模型的性能在不同模型大小和上下文长度下都得到了提升。选择性注意力使得模型在相同的验证困惑度下，显著减少了内存和计算需求。实验表明，使用选择性注意力的变压器模型在注意力模块中需要的内存比传统方法少得多。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01699', 'title': 'Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding', 'url': 'https://huggingface.co/papers/2410.01699', 'abstract': 'The current large auto-regressive models can generate high-quality, high-resolution images, but these models require hundreds or even thousands of steps of next-token prediction during inference, resulting in substantial time consumption. In existing studies, Jacobi decoding, an iterative parallel decoding algorithm, has been used to accelerate the auto-regressive generation and can be executed without training. However, the Jacobi decoding relies on a deterministic criterion to determine the convergence of iterations. Thus, it works for greedy decoding but is incompatible with sampling-based decoding which is crucial for visual quality and diversity in the current auto-regressive text-to-image generation. In this paper, we propose a training-free probabilistic parallel decoding algorithm, Speculative Jacobi Decoding (SJD), to accelerate auto-regressive text-to-image generation. By introducing a probabilistic convergence criterion, our SJD accelerates the inference of auto-regressive text-to-image generation while maintaining the randomness in sampling-based token decoding and allowing the model to generate diverse images. Specifically, SJD facilitates the model to predict multiple tokens at each step and accepts tokens based on the probabilistic criterion, enabling the model to generate images with fewer steps than the conventional next-token-prediction paradigm. We also investigate the token initialization strategies that leverage the spatial locality of visual data to further improve the acceleration ratio under specific scenarios. We conduct experiments for our proposed SJD on multiple auto-regressive text-to-image generation models, showing the effectiveness of model acceleration without sacrificing the visual quality.', 'score': 17, 'issue_id': 7, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'e4be41b1e418b1b8', 'data': {'categories': ['#cv', '#training', '#inference', '#optimization', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Ускорение генерации изображений без потери качества', 'desc': 'Статья представляет новый алгоритм параллельного декодирования - Speculative Jacobi Decoding (SJD) для ускорения авторегрессивной генерации изображений по тексту. SJD использует вероятностный критерий сходимости, что позволяет сохранить случайность при выборке токенов и разнообразие генерируемых изображений. Алгоритм предсказывает несколько токенов за шаг и принимает их на основе вероятностного критерия, что уменьшает количество шагов генерации. Эксперименты показали эффективность SJD в ускорении моделей без потери качества изображений.'}, 'en': {'title': '"Faster Images, Same Quality: Meet Speculative Jacobi Decoding!"', 'desc': 'The paper introduces Speculative Jacobi Decoding (SJD), a new algorithm to speed up the process of generating images from text using large auto-regressive models. Unlike previous methods, SJD uses a probabilistic approach to decide when to stop iterating, which allows it to work well with sampling-based decoding that is important for creating diverse and high-quality images. This method lets the model predict several tokens at once, reducing the number of steps needed to generate an image. Experiments show that SJD can make the image generation process faster without losing the quality of the images.'}, 'zh': {'title': '推测雅可比解码：加速自回归图像生成的新方法', 'desc': '当前的大型自回归模型可以生成高质量、高分辨率的图像，但需要大量的步骤进行推理，耗时较长。现有的雅可比解码算法可以加速自回归生成，但不适用于基于采样的解码。本文提出了一种新的推测雅可比解码算法，通过引入概率收敛标准，加速生成过程并保持图像的多样性。实验表明，该算法在不影响视觉质量的情况下有效提高了模型的生成速度。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2409.19989', 'title': 'RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models', 'url': 'https://huggingface.co/papers/2409.19989', 'abstract': 'Text-to-texture generation has recently attracted increasing attention, but existing methods often suffer from the problems of view inconsistencies, apparent seams, and misalignment between textures and the underlying mesh. In this paper, we propose a robust text-to-texture method for generating consistent and seamless textures that are well aligned with the mesh. Our method leverages state-of-the-art 2D diffusion models, including SDXL and multiple ControlNets, to capture structural features and intricate details in the generated textures. The method also employs a symmetrical view synthesis strategy combined with regional prompts for enhancing view consistency. Additionally, it introduces novel texture blending and soft-inpainting techniques, which significantly reduce the seam regions. Extensive experiments demonstrate that our method outperforms existing state-of-the-art methods.', 'score': 17, 'issue_id': 2, 'pub_date': '2024-09-30', 'pub_date_card': {'ru': '30 сентября', 'en': 'September 30', 'zh': '9月30日'}, 'hash': 'd0d2a72fcf8c7f62', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#architecture', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Бесшовные текстуры из текста: новый уровень реализма в 3D', 'desc': 'Предложен новый метод генерации текстур на основе текстовых описаний, решающий проблемы несогласованности видов, видимых швов и несоответствия текстур и трехмерных моделей. Метод использует современные 2D диффузионные модели, включая SDXL и несколько ControlNet, для захвата структурных особенностей и мелких деталей текстур. Применяется стратегия симметричного синтеза видов с региональными промптами для улучшения согласованности. Также введены новые техники смешивания текстур и мягкой инпайнтинга для уменьшения швов.'}, 'en': {'title': 'Seamless Textures, Perfectly Aligned: A New Era in Text-to-Texture Generation', 'desc': 'The paper introduces a new method for generating textures from text descriptions that align well with 3D meshes, addressing common issues like view inconsistencies and seams. It uses advanced 2D diffusion models, such as SDXL and ControlNets, to capture detailed structural features in textures. The approach includes a symmetrical view synthesis strategy and regional prompts to improve view consistency. Novel techniques like texture blending and soft-inpainting are employed to minimize seam regions, showing superior performance over existing methods.'}, 'zh': {'title': '无缝对齐：革新文本到纹理生成', 'desc': '这篇论文提出了一种新的文本到纹理生成方法，解决了现有方法中视图不一致、明显接缝和纹理与网格不对齐的问题。该方法利用最先进的2D扩散模型，如SDXL和多个ControlNets，来捕捉生成纹理中的结构特征和复杂细节。通过对称视图合成策略和区域提示，增强了视图一致性。此外，引入了新的纹理混合和软修补技术，显著减少了接缝区域。实验结果表明，该方法优于现有的最先进方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02362', 'title': 'A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond', 'url': 'https://huggingface.co/papers/2410.02362', 'abstract': 'Mamba, a special case of the State Space Model, is gaining popularity as an alternative to template-based deep learning approaches in medical image analysis. While transformers are powerful architectures, they have drawbacks, including quadratic computational complexity and an inability to address long-range dependencies efficiently. This limitation affects the analysis of large and complex datasets in medical imaging, where there are many spatial and temporal relationships. In contrast, Mamba offers benefits that make it well-suited for medical image analysis. It has linear time complexity, which is a significant improvement over transformers. Mamba processes longer sequences without attention mechanisms, enabling faster inference and requiring less memory. Mamba also demonstrates strong performance in merging multimodal data, improving diagnosis accuracy and patient outcomes. The organization of this paper allows readers to appreciate the capabilities of Mamba in medical imaging step by step. We begin by defining core concepts of SSMs and models, including S4, S5, and S6, followed by an exploration of Mamba architectures such as pure Mamba, U-Net variants, and hybrid models with convolutional neural networks, transformers, and Graph Neural Networks. We also cover Mamba optimizations, techniques and adaptations, scanning, datasets, applications, experimental results, and conclude with its challenges and future directions in medical imaging. This review aims to demonstrate the transformative potential of Mamba in overcoming existing barriers within medical imaging while paving the way for innovative advancements in the field. A comprehensive list of Mamba architectures applied in the medical field, reviewed in this work, is available at Github.', 'score': 16, 'issue_id': 3, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'dd4bc5254b2493c9', 'data': {'categories': ['#survey', '#dataset', '#cv', '#healthcare', '#graphs', '#optimization', '#open_source', '#architecture', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'Mamba: Революция в анализе медицинских изображений', 'desc': 'Эта статья рассматривает применение модели Mamba в анализе медицинских изображений. Mamba, являясь частным случаем модели пространства состояний (State Space Model), предлагает линейную временную сложность и эффективную обработку длинных последовательностей без механизмов внимания. В отличие от трансформеров, Mamba способна лучше справляться с долгосрочными зависимостями и объединением мультимодальных данных. Статья подробно описывает архитектуры Mamba, их оптимизации и применения в медицинской визуализации.'}, 'en': {'title': 'Mamba: Revolutionizing Medical Imaging with Efficiency and Precision', 'desc': 'Mamba is a type of State Space Model that offers a more efficient alternative to traditional deep learning methods like transformers in medical image analysis. Unlike transformers, which struggle with high computational demands and long-range dependencies, Mamba operates with linear time complexity, making it faster and more memory-efficient. It excels in processing long sequences without attention mechanisms and effectively merges multimodal data, enhancing diagnostic accuracy. The paper details various Mamba architectures and optimizations, highlighting its potential to revolutionize medical imaging by addressing current limitations.'}, 'zh': {'title': 'Mamba：医学图像分析的新突破', 'desc': 'Mamba是一种特殊的状态空间模型，在医学图像分析中逐渐成为模板化深度学习方法的替代方案。与变压器模型相比，Mamba具有线性时间复杂度，能够更高效地处理长序列数据，减少内存需求。Mamba在多模态数据融合中表现出色，提高了诊断准确性和患者治疗效果。本文详细介绍了Mamba在医学成像中的应用，包括其架构、优化技术及未来发展方向。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02760', 'title': 'Erasing Conceptual Knowledge from Language Models', 'url': 'https://huggingface.co/papers/2410.02760', 'abstract': "Concept erasure in language models has traditionally lacked a comprehensive evaluation framework, leading to incomplete assessments of effectiveness of erasure methods. We propose an evaluation paradigm centered on three critical criteria: innocence (complete knowledge removal), seamlessness (maintaining conditional fluent generation), and specificity (preserving unrelated task performance). Our evaluation metrics naturally motivate the development of Erasure of Language Memory (ELM), a new method designed to address all three dimensions. ELM employs targeted low-rank updates to alter output distributions for erased concepts while preserving overall model capabilities including fluency when prompted for an erased concept. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across our proposed metrics, including near-random scores on erased topic assessments, generation fluency, maintained accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info", 'score': 12, 'issue_id': 1, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '018d36385dbb4dea', 'data': {'categories': ['#dataset', '#security', '#interpretability', '#optimization', '#benchmark', '#open_source', '#architecture'], 'emoji': '🧹', 'ru': {'title': 'ELM: Новый подход к избирательному стиранию знаний в языковых моделях', 'desc': 'Статья представляет новый метод стирания концепций в языковых моделях - Erasure of Language Memory (ELM). Авторы предлагают комплексную систему оценки эффективности стирания, основанную на трех критериях: полнота удаления знаний, сохранение условной генерации и специфичность. ELM использует целевые обновления низкого ранга для изменения выходных распределений стираемых концепций, сохраняя при этом общие возможности модели.'}, 'en': {'title': "Erase with Grace: ELM's Breakthrough in Concept Erasure", 'desc': "The paper introduces a new evaluation framework for concept erasure in language models, focusing on innocence, seamlessness, and specificity. It presents Erasure of Language Memory (ELM), a method that uses low-rank updates to effectively erase concepts while maintaining the model's fluency and performance on unrelated tasks. ELM is tested on various domains, showing superior results in erasing specific topics without affecting other capabilities. The method also demonstrates robustness against adversarial attacks, ensuring reliable performance."}, 'zh': {'title': '语言模型中的概念消除新标准', 'desc': '这篇论文提出了一种新的评估框架，用于评估语言模型中概念消除的效果。该框架基于三个关键标准：无害性（完全知识移除）、无缝性（保持条件流畅生成）和特异性（保留无关任务性能）。研究者开发了一种名为语言记忆消除（ELM）的方法，通过低秩更新来改变被消除概念的输出分布，同时保持模型的整体能力。实验表明，ELM在生物安全、网络安全和文学领域的消除任务中表现优异。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01999', 'title': 'CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs', 'url': 'https://huggingface.co/papers/2410.01999', 'abstract': "Recent advancements in Code Large Language Models (CodeLLMs) have predominantly focused on open-ended code generation tasks, often neglecting the critical aspect of code understanding and comprehension. To bridge this gap, we present CodeMMLU, a comprehensive multiple-choice question-answer benchmark designed to evaluate the depth of software and code understanding in LLMs. CodeMMLU includes over 10,000 questions sourced from diverse domains, encompassing tasks such as code analysis, defect detection, and software engineering principles across multiple programming languages. Unlike traditional benchmarks, CodeMMLU assesses models's ability to reason about code rather than merely generate it, providing deeper insights into their grasp of complex software concepts and systems. Our extensive evaluation reveals that even state-of-the-art models face significant challenges with CodeMMLU, highlighting deficiencies in comprehension beyond code generation. By underscoring the crucial relationship between code understanding and effective generation, CodeMMLU serves as a vital resource for advancing AI-assisted software development, ultimately aiming to create more reliable and capable coding assistants.", 'score': 10, 'issue_id': 9, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'a61e37ff42c5cd79', 'data': {'categories': ['#reasoning', '#multilingual', '#plp', '#benchmark', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'CodeMMLU: Переосмысление оценки понимания кода в ИИ', 'desc': 'CodeMMLU - новый бенчмарк для оценки понимания кода языковыми моделями. Он содержит более 10 000 вопросов с множественным выбором из различных областей программирования. В отличие от традиционных бенчмарков, CodeMMLU оценивает способность моделей рассуждать о коде, а не просто генерировать его. Результаты показывают, что даже современные модели сталкиваются со значительными трудностями при решении задач CodeMMLU.'}, 'en': {'title': '"From Code Generation to Code Comprehension: Elevating AI\'s Understanding"', 'desc': "The paper introduces CodeMMLU, a benchmark designed to test the code understanding capabilities of large language models (LLMs) through multiple-choice questions. It focuses on evaluating models' ability to comprehend and reason about code, rather than just generating it. The benchmark includes over 10,000 questions from various domains, highlighting the challenges even advanced models face in understanding complex software concepts. CodeMMLU aims to improve AI-assisted software development by emphasizing the importance of code comprehension in creating reliable coding assistants."}, 'zh': {'title': 'CodeMMLU：提升代码理解，超越生成', 'desc': '近年来，代码大语言模型（CodeLLMs）主要关注于开放式代码生成任务，往往忽视了代码理解的重要性。为了解决这一问题，我们提出了CodeMMLU，这是一个全面的多项选择题基准，用于评估大语言模型对软件和代码的理解深度。CodeMMLU包含超过10,000个问题，涵盖代码分析、缺陷检测和软件工程原理等任务。我们的评估显示，即使是最先进的模型在CodeMMLU上也面临重大挑战，强调了理解代码与生成代码之间的重要关系。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01273', 'title': 'CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction', 'url': 'https://huggingface.co/papers/2410.01273', 'abstract': 'Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications.', 'score': 8, 'issue_id': 4, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': 'db64c057695506e6', 'data': {'categories': ['#reasoning', '#dataset', '#cv', '#training', '#rl', '#optimization', '#transfer_learning', '#games', '#robotics', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'CANVAS: навигация роботов с человеческим здравым смыслом', 'desc': 'CANVAS - это новая система для навигации роботов, использующая визуальные и языковые инструкции. Она основана на имитационном обучении, позволяющем роботу учиться на примерах человеческого поведения. Авторы создали датасет COMMAND с аннотированными человеком результатами навигации для обучения систем в симулированных средах. CANVAS превосходит базовую систему ROS NavStack во всех тестовых сценариях, особенно при работе с зашумленными инструкциями.'}, 'en': {'title': 'Guiding Robots with Human-Like Understanding', 'desc': 'The paper introduces CANVAS, a framework that helps robots navigate using both visual and linguistic instructions, making them more aligned with human expectations. It uses imitation learning to teach robots how to interpret abstract human commands by learning from human navigation behavior. The researchers created a dataset called COMMAND to train these systems, showing that CANVAS performs better than traditional rule-based systems, especially in challenging environments. The framework also demonstrates strong Sim2Real transfer, meaning it works well in real-world scenarios after being trained in simulations.'}, 'zh': {'title': 'CANVAS：让机器人导航更贴近人类常识', 'desc': '这篇论文介绍了一个名为CANVAS的新框架，它结合视觉和语言指令来实现常识感知的导航。通过模仿学习，机器人可以从人类的导航行为中学习。实验表明，CANVAS在所有环境中都优于传统的基于规则的系统，尤其是在噪声指令下表现出色。在真实世界的应用中，CANVAS展示了从模拟环境到现实环境的出色转移能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03103', 'title': 'Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for Code Generation with Lookahead Planning', 'url': 'https://huggingface.co/papers/2410.03103', 'abstract': 'Fill-in-the-Middle (FIM) has become integral to code language models, enabling generation of missing code given both left and right contexts. However, the current FIM training paradigm, which reorders original training sequences and then performs regular next-token prediction (NTP), often leads to models struggling to generate content that aligns smoothly with the surrounding context. Crucially, while existing works rely on rule-based post-processing to circumvent this weakness, such methods are not practically usable in open-domain code completion tasks as they depend on restrictive, dataset-specific assumptions (e.g., generating the same number of lines as in the ground truth). Moreover, model performance on FIM tasks deteriorates significantly without these unrealistic assumptions.   We hypothesize that NTP alone is insufficient for models to learn effective planning conditioned on the distant right context, a critical factor for successful code infilling. To overcome this, we propose Horizon-Length Prediction (HLP), a novel training objective that teaches models to predict the number of remaining middle tokens (i.e., horizon length) at each step. HLP advances FIM with lookahead planning, enabling models to inherently learn infilling boundaries for arbitrary left and right contexts without relying on dataset-specific post-processing. Our evaluation across different models and sizes shows that HLP significantly improves FIM performance by up to 24% relatively on diverse benchmarks, across file-level and repository-level, and without resorting to unrealistic post-processing methods. Furthermore, the enhanced planning capability gained through HLP boosts model performance on code reasoning. Importantly, HLP only incurs negligible training overhead and no additional inference cost, ensuring its practicality for real-world scenarios.', 'score': 6, 'issue_id': 7, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '6d595cf08f21593b', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#plp', '#benchmark', '#architecture'], 'emoji': '🧩', 'ru': {'title': 'HLP: Умное заполнение пропусков в коде с прогнозированием горизонта', 'desc': 'Статья представляет новый метод обучения моделей для заполнения пропусков в коде - Horizon-Length Prediction (HLP). В отличие от стандартного подхода Fill-in-the-Middle (FIM), HLP учит модель предсказывать количество оставшихся токенов, что улучшает планирование и согласованность с контекстом. Эксперименты показывают, что HLP значительно повышает производительность на различных бенчмарках без необходимости в постобработке. Метод также улучшает способности модели к рассуждению о коде, при этом не требуя существенных дополнительных ресурсов.'}, 'en': {'title': '"Horizon-Length Prediction: Elevating Code Completion with Smarter Planning"', 'desc': "The paper discusses a new approach to improve code language models' ability to fill in missing code segments, called Fill-in-the-Middle (FIM). Traditional methods struggle because they rely on reordering sequences and predicting the next token, which doesn't always align well with the surrounding code context. The authors propose a new training method called Horizon-Length Prediction (HLP), which helps models predict how many tokens are needed to complete the middle section, improving their planning and infilling capabilities. This method significantly enhances model performance without needing complex post-processing, making it more practical for real-world applications."}, 'zh': {'title': '视界长度预测：提升代码填充的未来', 'desc': '这篇论文讨论了代码语言模型中的填充中间部分（FIM）问题，指出现有的训练方法在生成与上下文平滑衔接的内容时存在困难。作者提出了一种新的训练目标，称为视界长度预测（HLP），以改善模型在FIM任务中的表现。HLP通过预测剩余中间标记的数量，帮助模型在不依赖数据集特定后处理的情况下学习填充边界。实验结果表明，HLP显著提高了模型在多种基准测试中的表现，同时对训练和推理的开销影响很小。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03535', 'title': 'NRGBoost: Energy-Based Generative Boosted Trees', 'url': 'https://huggingface.co/papers/2410.03535', 'abstract': 'Despite the rise to dominance of deep learning in unstructured data domains, tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular algorithms with a focus on explicitly modeling the data density (up to a normalization constant), thus enabling other applications besides sampling. As our main contribution we propose an energy-based generative boosting algorithm that is analogous to the second order boosting implemented in popular packages like XGBoost. We show that, despite producing a generative model capable of handling inference tasks over any input variable, our proposed algorithm can achieve similar discriminative performance to GBDT on a number of real world tabular datasets, outperforming alternative generative approaches. At the same time, we show that it is also competitive with neural network based models for sampling.', 'score': 6, 'issue_id': 4, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'e71078964f133d48', 'data': {'categories': ['#training', '#optimization', '#data', '#architecture', '#synthetic'], 'emoji': '🌳', 'ru': {'title': 'Генеративный бустинг: мощь деревьев решений в мире генеративных моделей', 'desc': 'Статья исследует генеративные расширения популярных древовидных алгоритмов, таких как Random Forests и Gradient Boosted Decision Trees, для работы с табличными данными. Авторы предлагают энергетический генеративный алгоритм бустинга, аналогичный бустингу второго порядка в XGBoost. Новый метод способен моделировать плотность данных и выполнять различные задачи вывода, сохраняя при этом высокую дискриминативную производительность. Эксперименты показывают, что предложенный подход конкурентоспособен как с традиционными древовидными методами, так и с нейросетевыми моделями для генерации выборок.'}, 'en': {'title': 'Boosting Trees Beyond Boundaries: Generative Power Unleashed', 'desc': 'This paper explores how tree-based methods like Random Forests and Gradient Boosted Decision Trees can be extended to generative models, which can model data density and perform tasks beyond just classification or regression. The authors introduce an energy-based generative boosting algorithm that mirrors the second-order boosting used in popular tools like XGBoost. Their proposed method not only matches the discriminative performance of traditional GBDT on real-world tabular data but also competes well with neural networks in generating samples. This approach offers a versatile model that can handle both inference and sampling tasks effectively.'}, 'zh': {'title': '树模型的生成式新突破：能量提升算法', 'desc': '这篇论文探讨了如何将随机森林和梯度提升决策树等树模型扩展为生成模型，重点在于显式建模数据密度。作者提出了一种基于能量的生成提升算法，与XGBoost中的二阶提升类似。实验表明，该算法在处理推理任务时，能够在多个真实数据集上达到与GBDT相似的判别性能，并优于其他生成方法。同时，它在采样任务中也能与神经网络模型竞争。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02241', 'title': 'MIGA: Mixture-of-Experts with Group Aggregation for Stock Market Prediction', 'url': 'https://huggingface.co/papers/2410.02241', 'abstract': 'Stock market prediction has remained an extremely challenging problem for many decades owing to its inherent high volatility and low information noisy ratio. Existing solutions based on machine learning or deep learning demonstrate superior performance by employing a single model trained on the entire stock dataset to generate predictions across all types of stocks. However, due to the significant variations in stock styles and market trends, a single end-to-end model struggles to fully capture the differences in these stylized stock features, leading to relatively inaccurate predictions for all types of stocks. In this paper, we present MIGA, a novel Mixture of Expert with Group Aggregation framework designed to generate specialized predictions for stocks with different styles by dynamically switching between distinct style experts. To promote collaboration among different experts in MIGA, we propose a novel inner group attention architecture, enabling experts within the same group to share information and thereby enhancing the overall performance of all experts. As a result, MIGA significantly outperforms other end-to-end models on three Chinese Stock Index benchmarks including CSI300, CSI500, and CSI1000. Notably, MIGA-Conv reaches 24 % excess annual return on CSI300 benchmark, surpassing the previous state-of-the-art model by 8% absolute. Furthermore, we conduct a comprehensive analysis of mixture of experts for stock market prediction, providing valuable insights for future research.', 'score': 6, 'issue_id': 1, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '9cfda67b2beb4586', 'data': {'categories': ['#survey', '#multilingual', '#optimization', '#benchmark', '#architecture'], 'emoji': '📈', 'ru': {'title': 'MIGA: Прорыв в прогнозировании фондового рынка с помощью специализированных экспертов', 'desc': 'MIGA - это новый фреймворк для прогнозирования фондового рынка, основанный на смеси экспертов с групповой агрегацией. В отличие от существующих моделей, MIGA использует специализированных экспертов для разных стилей акций, что позволяет более точно учитывать особенности различных типов ценных бумаг. Фреймворк включает новую архитектуру внутригрупповой аттенции, улучшающую взаимодействие между экспертами. MIGA значительно превосходит другие модели на китайских фондовых индексах, достигая 24% годовой доходности на индексе CSI300.'}, 'en': {'title': '"MIGA: Tailored Expertise for Smarter Stock Predictions"', 'desc': 'The paper introduces MIGA, a new framework for stock market prediction that uses a Mixture of Expert with Group Aggregation approach. Unlike traditional models that apply a single model to all stocks, MIGA dynamically switches between specialized experts tailored to different stock styles. This method enhances prediction accuracy by allowing experts to share information through an inner group attention mechanism. MIGA demonstrates superior performance on Chinese Stock Index benchmarks, achieving a notable 24% excess annual return on the CSI300 benchmark.'}, 'zh': {'title': 'MIGA：股票预测的风格专家混合框架', 'desc': '股票市场预测一直是一个具有挑战性的问题，因为市场的高波动性和信息噪声比低。传统的机器学习和深度学习方法通常使用单一模型来预测所有类型的股票，但由于股票风格和市场趋势的差异，这种方法的准确性有限。本文提出了一种新的专家混合框架MIGA，通过动态切换不同风格的专家来生成针对不同风格股票的预测。MIGA在多个中国股票指数基准上表现优异，尤其是在CSI300基准上实现了24%的超额年回报率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03051', 'title': 'AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark', 'url': 'https://huggingface.co/papers/2410.03051', 'abstract': 'Video detailed captioning is a key task which aims to generate comprehensive and coherent textual descriptions of video content, benefiting both video understanding and generation. In this paper, we propose AuroraCap, a video captioner based on a large multimodal model. We follow the simplest architecture design without additional parameters for temporal modeling. To address the overhead caused by lengthy video sequences, we implement the token merging strategy, reducing the number of input visual tokens. Surprisingly, we found that this strategy results in little performance loss. AuroraCap shows superior performance on various video and image captioning benchmarks, for example, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include simple descriptions, consisting of a few dozen words, which limits research in this field. Therefore, we develop VDC, a video detailed captioning benchmark with over one thousand carefully annotated structured captions. In addition, we propose a new LLM-assisted metric VDCscore for bettering evaluation, which adopts a divide-and-conquer strategy to transform long caption evaluation into multiple short question-answer pairs. With the help of human Elo ranking, our experiments show that this benchmark better correlates with human judgments of video detailed captioning quality.', 'score': 3, 'issue_id': 23, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'ad420143a7fcbc93', 'data': {'categories': ['#video', '#survey', '#dataset', '#interpretability', '#optimization', '#benchmark', '#games', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'AuroraCap: прорыв в детальном описании видео', 'desc': 'Статья представляет AuroraCap - модель для детального описания видео, основанную на большой мультимодальной архитектуре. Авторы применили стратегию объединения токенов для уменьшения вычислительной сложности, что неожиданно не привело к значительной потере качества. AuroraCap превзошла существующие модели на ряде бенчмарков по описанию видео и изображений. Исследователи также создали новый датасет VDC и метрику VDCscore для лучшей оценки качества детальных описаний видео.'}, 'en': {'title': 'AuroraCap: Simplifying Video Captioning with High Performance', 'desc': 'This paper introduces AuroraCap, a video captioning model that generates detailed descriptions of video content using a large multimodal architecture. The authors simplify the model design by avoiding complex temporal modeling and instead use a token merging strategy to handle long video sequences efficiently. Despite reducing the number of visual tokens, AuroraCap maintains high performance, outperforming other models like GPT-4V and Gemini-1.5 Pro on various benchmarks. Additionally, the paper presents a new benchmark, VDC, for detailed video captioning, along with a novel evaluation metric, VDCscore, which improves the assessment of caption quality by correlating better with human evaluations.'}, 'zh': {'title': 'AuroraCap：视频描述的新突破', 'desc': '视频详细描述是一个重要任务，旨在生成全面且连贯的视频内容文本描述，有助于视频理解和生成。本文提出了AuroraCap，这是一种基于大型多模态模型的视频描述生成器。我们采用了最简单的架构设计，没有额外的时间建模参数，并通过实现令牌合并策略来减少输入视觉令牌的数量，意外地发现这一策略对性能影响很小。AuroraCap在多个视频和图像描述基准测试中表现优越，并且我们开发了VDC基准，以提供超过一千个精心注释的结构化描述，推动该领域的研究。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03645', 'title': 'GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs', 'url': 'https://huggingface.co/papers/2410.03645', 'abstract': 'Robotic simulation today remains challenging to scale up due to the human efforts required to create diverse simulation tasks and scenes. Simulation-trained policies also face scalability issues as many sim-to-real methods focus on a single task. To address these challenges, this work proposes GenSim2, a scalable framework that leverages coding LLMs with multi-modal and reasoning capabilities for complex and realistic simulation task creation, including long-horizon tasks with articulated objects. To automatically generate demonstration data for these tasks at scale, we propose planning and RL solvers that generalize within object categories. The pipeline can generate data for up to 100 articulated tasks with 200 objects and reduce the required human efforts. To utilize such data, we propose an effective multi-task language-conditioned policy architecture, dubbed proprioceptive point-cloud transformer (PPT), that learns from the generated demonstrations and exhibits strong sim-to-real zero-shot transfer. Combining the proposed pipeline and the policy architecture, we show a promising usage of GenSim2 that the generated data can be used for zero-shot transfer or co-train with real-world collected data, which enhances the policy performance by 20% compared with training exclusively on limited real data.', 'score': 2, 'issue_id': 7, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'f956848db7fdb9bc', 'data': {'categories': ['#reasoning', '#training', '#synthetic', '#rl', '#optimization', '#transfer_learning', '#games', '#architecture', '#robotics', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'GenSim2: ИИ-помощник для масштабного обучения роботов', 'desc': 'GenSim2 - это масштабируемая система для создания сложных симуляций роботов с использованием языковых моделей. Она автоматически генерирует разнообразные задачи и сцены, включая длительные последовательности действий с шарнирными объектами. Система использует планировщики и алгоритмы обучения с подкреплением для создания демонстрационных данных. Предложенная архитектура политики на основе трансформеров позволяет обучаться на сгенерированных данных и демонстрирует хороший перенос из симуляции в реальность.'}, 'en': {'title': 'Scaling Robotic Simulations with GenSim2: Automate, Learn, Transfer!', 'desc': 'The paper introduces GenSim2, a framework designed to automate the creation of complex simulation tasks using coding language models with multi-modal and reasoning capabilities. It addresses the scalability issues in robotic simulations by generating demonstration data for numerous tasks, reducing the need for human input. The framework includes a multi-task language-conditioned policy architecture called proprioceptive point-cloud transformer (PPT), which effectively learns from these demonstrations and shows strong sim-to-real zero-shot transfer capabilities. By combining GenSim2 with real-world data, the approach enhances policy performance significantly, demonstrating a 20% improvement over training with limited real data alone.'}, 'zh': {'title': 'GenSim2：自动化生成复杂模拟任务的未来', 'desc': '这篇论文提出了一种名为GenSim2的框架，旨在通过利用多模态和推理能力的编码大语言模型来创建复杂的模拟任务。该框架能够自动生成多达100个任务的数据，减少了人力投入。为了利用这些数据，研究者设计了一种多任务语言条件策略架构，称为本体点云变换器（PPT），可以实现从模拟到真实环境的零样本迁移。结合生成的数据和策略架构，GenSim2展示了在增强策略性能方面的潜力，比仅使用有限的真实数据训练提高了20%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05258', 'title': 'Differential Transformer', 'url': 'https://huggingface.co/papers/2410.05258', 'abstract': 'Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.', 'score': 165, 'issue_id': 14, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'fae9004e0f12e4fc', 'data': {'categories': ['#reasoning', '#long_context', '#multilingual', '#training', '#hallucinations', '#rag', '#diffusion', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Diff Transformer: точнее фокусируемся на важном, отсекая шум', 'desc': 'Статья представляет новую архитектуру Diff Transformer, которая улучшает внимание к релевантному контексту и подавляет шум. Механизм дифференциального внимания вычисляет оценки внимания как разницу между двумя отдельными картами внимания softmax. Эксперименты показывают, что Diff Transformer превосходит обычный Transformer в различных задачах обработки естественного языка. Модель демонстрирует преимущества в моделировании длинного контекста, извлечении ключевой информации, снижении галлюцинаций и обучении в контексте.'}, 'en': {'title': 'Diff Transformer: Sharpening Focus, Reducing Noise in Language Models', 'desc': 'The paper introduces Diff Transformer, a new model that improves attention mechanisms by focusing more on relevant information and reducing noise. It uses a differential attention mechanism that calculates attention scores by subtracting two softmax attention maps, leading to clearer and more focused attention patterns. This approach enhances performance in tasks like long-context modeling and information retrieval, and it reduces issues like hallucination in text generation. Diff Transformer also shows improved robustness in in-context learning, making it a promising advancement for large language models.'}, 'zh': {'title': 'Diff Transformer：更专注的注意力机制', 'desc': 'Transformer模型有时会过多关注不相关的上下文。Diff Transformer通过差分注意力机制，增强对相关上下文的关注，同时消除噪声。实验表明，Diff Transformer在语言建模中表现优于传统Transformer，尤其在长上下文建模和信息检索等应用中效果显著。它还能减少幻觉现象，提高上下文学习的准确性和鲁棒性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02884', 'title': 'LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2410.02884', 'abstract': 'This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.', 'score': 48, 'issue_id': 16, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'cc4c09dfca59a8d4', 'data': {'categories': ['#reasoning', '#math', '#optimization', '#benchmark', '#rlhf', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'LLaMA-Berry: Прорыв в математическом мышлении искусственного интеллекта', 'desc': 'LLaMA-Berry - это передовая система для решения математических задач, улучшающая способности больших языковых моделей к математическим рассуждениям. Она объединяет метод Монте-Карло для поиска по дереву (MCTS) с итеративным самоулучшением для оптимизации пути рассуждений. Система использует модель парных вознаграждений для глобальной оценки различных путей решения. LLaMA-Berry демонстрирует превосходную производительность по сравнению с существующими методами, особенно на сложных олимпиадных задачах.'}, 'en': {'title': 'LLaMA-Berry: Revolutionizing Math Reasoning in AI', 'desc': 'The paper introduces LLaMA-Berry, a framework designed to improve the mathematical reasoning skills of Large Language Models by integrating Monte Carlo Tree Search with a method called Self-Refine. This combination allows for a more efficient exploration of possible solutions by overcoming the limitations of traditional search algorithms. The framework also uses a Pairwise Preference Reward Model to evaluate and rank different solution paths, inspired by Reinforcement Learning from Human Feedback. Tested on various benchmarks, LLaMA-Berry demonstrates superior performance in solving complex mathematical problems compared to existing methods.'}, 'zh': {'title': 'LLaMA-Berry：提升大型语言模型数学推理的新框架', 'desc': '这篇论文介绍了一种名为LLaMA-Berry的高级数学问题解决框架，用于增强大型语言模型的数学推理能力。该框架结合了蒙特卡洛树搜索（MCTS）和迭代自我优化，以优化推理路径，并利用成对奖励模型对不同路径进行全局评估。通过利用大型语言模型的自我批评和重写能力，SR-MCTS克服了传统逐步和贪婪搜索算法的低效和局限性。成对偏好奖励模型（PPRM）则通过增强的博尔达计数方法，将解决方案的成对偏好合成为全局排名分数，以找到更好的答案。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02707', 'title': 'LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations', 'url': 'https://huggingface.co/papers/2410.02707', 'abstract': 'Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs\' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs\' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model\'s internal perspective, which can guide future research on enhancing error analysis and mitigation.', 'score': 47, 'issue_id': 15, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'd3da276d1028f171', 'data': {'categories': ['#reasoning', '#hallucinations', '#training', '#interpretability', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Раскрытие тайн внутренних представлений LLM для улучшения обнаружения ошибок', 'desc': 'Исследование показывает, что внутренние представления больших языковых моделей (LLM) содержат больше информации о достоверности генерируемых данных, чем считалось ранее. Авторы обнаружили, что эта информация сконцентрирована в определенных токенах, что позволяет значительно улучшить обнаружение ошибок. Однако детекторы ошибок не обобщаются между датасетами, что указывает на отсутствие универсального кодирования достоверности. Исследование также выявило, что LLM могут внутренне кодировать правильный ответ, но при этом генерировать неверный.'}, 'en': {'title': 'Unlocking the Hidden Truth: Enhancing Error Detection in LLMs', 'desc': "This paper explores how large language models (LLMs) encode information about the truthfulness of their outputs, which can be used to detect errors like hallucinations. The study finds that truthfulness information is concentrated in specific tokens, improving error detection, but these detectors don't generalize well across different datasets. It also shows that internal representations can predict the types of errors a model might make, helping to create specific strategies to reduce these errors. Additionally, the research highlights a gap between what LLMs internally know and what they output, as they might encode the correct answer but still produce an incorrect one."}, 'zh': {'title': '揭示大型语言模型内部的真实性编码', 'desc': '大型语言模型（LLMs）常常会产生错误，包括事实不准确、偏见和推理失败，这些统称为“幻觉”。研究表明，LLMs的内部状态包含关于其输出真实性的信息，这些信息可以用来检测错误。我们发现，真实性信息集中在特定的标记上，利用这一特性可以显著提高错误检测性能。然而，这种错误检测器在不同数据集上无法泛化，说明真实性编码不是普遍的，而是多方面的。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04364', 'title': "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide", 'url': 'https://huggingface.co/papers/2410.04364', 'abstract': "Text-to-image (T2I) diffusion models have revolutionized visual content creation, but extending these capabilities to text-to-video (T2V) generation remains a challenge, particularly in preserving temporal consistency. Existing methods that aim to improve consistency often cause trade-offs such as reduced imaging quality and impractical computational time. To address these issues we introduce VideoGuide, a novel framework that enhances the temporal consistency of pretrained T2V models without the need for additional training or fine-tuning. Instead, VideoGuide leverages any pretrained video diffusion model (VDM) or itself as a guide during the early stages of inference, improving temporal quality by interpolating the guiding model's denoised samples into the sampling model's denoising process. The proposed method brings about significant improvement in temporal consistency and image fidelity, providing a cost-effective and practical solution that synergizes the strengths of various video diffusion models. Furthermore, we demonstrate prior distillation, revealing that base models can achieve enhanced text coherence by utilizing the superior data prior of the guiding model through the proposed method. Project Page: http://videoguide2025.github.io/", 'score': 26, 'issue_id': 13, 'pub_date': '2024-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': 'fd005deb7206d4fe', 'data': {'categories': ['#video', '#inference', '#optimization', '#diffusion', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'VideoGuide: повышение качества генерации видео без переобучения', 'desc': 'VideoGuide - это новая система, которая улучшает временную согласованность предобученных моделей генерации видео по тексту без дополнительного обучения. Она использует предобученную модель диффузии видео в качестве гида на ранних этапах вывода, интерполируя ее образцы в процесс шумоподавления основной модели. Это значительно повышает временную согласованность и качество изображения, объединяя сильные стороны различных моделей генерации видео. Метод также демонстрирует дистилляцию приора, позволяя базовым моделям улучшить текстовую согласованность.'}, 'en': {'title': 'VideoGuide: Elevating T2V Consistency Without Compromise', 'desc': 'The paper introduces VideoGuide, a framework designed to improve the temporal consistency of text-to-video (T2V) generation models without additional training. VideoGuide uses a guiding model to enhance the denoising process of a sampling model, leading to better temporal quality and image fidelity. This approach effectively balances the trade-offs between consistency and computational efficiency. Additionally, the method allows base models to enhance text coherence by leveraging the data prior of the guiding model.'}, 'zh': {'title': 'VideoGuide：提升文本到视频生成的一致性与质量', 'desc': '这篇论文介绍了一种名为VideoGuide的新框架，用于提高文本到视频生成模型的时间一致性。现有方法在提高一致性时常常导致图像质量下降或计算时间过长，而VideoGuide无需额外训练或微调即可解决这些问题。它通过在推理的早期阶段利用预训练的视频扩散模型作为指导，改善时间质量。该方法显著提高了时间一致性和图像保真度，是一种经济实用的解决方案。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02675', 'title': 'FAN: Fourier Analysis Networks', 'url': 'https://huggingface.co/papers/2410.02675', 'abstract': 'Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they tend to memorize the periodic data rather than genuinely understanding the underlying principles of periodicity. However, periodicity is a crucial trait in various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, a novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena. By introducing Fourier Series, the periodicity is naturally integrated into the structure and computational processes of the neural network, thus achieving a more accurate expression and prediction of periodic patterns. As a promising substitute to multi-layer perceptron (MLP), FAN can seamlessly replace MLP in various models with fewer parameters and FLOPs. Through extensive experiments, we demonstrate the effectiveness of FAN in modeling and reasoning about periodic functions, and the superiority and generalizability of FAN across a range of real-world tasks, including symbolic formula representation, time series forecasting, and language modeling.', 'score': 24, 'issue_id': 12, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '9138501c89f38809', 'data': {'categories': ['#reasoning', '#training', '#math', '#optimization', '#small_models', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'FAN: Революция в моделировании периодичности нейронными сетями', 'desc': 'Статья представляет новую архитектуру нейронной сети под названием FAN, основанную на анализе Фурье. FAN эффективно моделирует и обрабатывает периодические явления, преодолевая ограничения традиционных нейронных сетей в этой области. Авторы демонстрируют превосходство FAN над многослойным перцептроном (MLP) в различных задачах, включая представление символьных формул и прогнозирование временных рядов. FAN интегрирует периодичность в структуру сети, обеспечивая более точное выражение и предсказание периодических паттернов.'}, 'en': {'title': 'FAN: Revolutionizing Periodic Pattern Recognition with Fourier Analysis', 'desc': 'The paper identifies a limitation in traditional neural networks like MLPs and Transformers, which tend to memorize rather than understand periodic data. To address this, the authors introduce FAN, a new network architecture that uses Fourier Analysis to better model periodic phenomena. FAN integrates Fourier Series into its structure, allowing it to predict periodic patterns more accurately with fewer parameters. Experiments show that FAN outperforms traditional models in tasks like time series forecasting and language modeling.'}, 'zh': {'title': 'FAN：用傅里叶分析重塑周期性建模', 'desc': '这篇论文指出，尽管神经网络如MLP和Transformer取得了显著成功，但在处理周期性数据时存在潜在缺陷，倾向于记忆而非理解周期性原理。周期性在推理和泛化中至关重要，影响自然和工程系统的可预测性。为此，作者提出了一种基于傅里叶分析的新型网络架构FAN，能够更有效地建模和推理周期现象。实验表明，FAN在多种实际任务中表现出色，具有更少的参数和计算量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05080', 'title': 'ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery', 'url': 'https://huggingface.co/papers/2410.05080', 'abstract': 'The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about the true capabilities of such agents. In this work, we argue that for an agent to fully automate scientific discovery, it must be able to complete all essential tasks in the workflow. Thus, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. These results underscore the limited capacities of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.', 'score': 19, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'e1f883a68716278f', 'data': {'categories': ['#science', '#data', '#plp', '#agents', '#benchmark', '#open_source', '#synthetic'], 'emoji': '🧪', 'ru': {'title': 'Реальная оценка возможностей ИИ-агентов в науке', 'desc': 'В статье представлен новый бенчмарк ScienceAgentBench для оценки языковых агентов в научных исследованиях. Авторы извлекли 102 задачи из 44 рецензируемых публикаций в четырех дисциплинах и привлекли экспертов для их валидации. Бенчмарк оценивает способность агентов генерировать Python-код для решения научных задач. Результаты показывают, что даже лучший агент решает только 32.4% задач самостоятельно, что указывает на ограниченные возможности современных языковых моделей в научной сфере.'}, 'en': {'title': 'Benchmarking the Future: Evaluating Language Agents in Scientific Discovery', 'desc': "The paper discusses the development of language model-based agents aimed at automating scientific discovery, highlighting the need for these agents to handle all tasks in a scientific workflow. To evaluate these agents, the authors introduce ScienceAgentBench, a benchmark derived from 102 tasks across four scientific disciplines, validated by experts. The benchmark assesses the agents' ability to generate Python programs for these tasks, using various evaluation metrics to ensure scientific accuracy and relevance. Results show that even the best-performing language agents can only solve a limited percentage of tasks, indicating their current limitations in fully automating scientific research."}, 'zh': {'title': '科学发现自动化：语言代理的挑战与机遇', 'desc': '这篇论文讨论了基于大型语言模型（LLM）的语言代理在科学发现中的应用。作者认为，要实现科学发现的全自动化，代理必须能够完成工作流程中的所有关键任务。为此，他们提出了一个名为ScienceAgentBench的新基准，用于评估语言代理在数据驱动的科学发现中的表现。研究结果表明，目前的语言代理在生成代码方面能力有限，尚无法实现科学研究的端到端自动化。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04534', 'title': 'UniMuMo: Unified Text, Music and Motion Generation', 'url': 'https://huggingface.co/papers/2410.04534', 'abstract': 'We introduce UniMuMo, a unified multimodal model capable of taking arbitrary text, music, and motion data as input conditions to generate outputs across all three modalities. To address the lack of time-synchronized data, we align unpaired music and motion data based on rhythmic patterns to leverage existing large-scale music-only and motion-only datasets. By converting music, motion, and text into token-based representation, our model bridges these modalities through a unified encoder-decoder transformer architecture. To support multiple generation tasks within a single framework, we introduce several architectural improvements. We propose encoding motion with a music codebook, mapping motion into the same feature space as music. We introduce a music-motion parallel generation scheme that unifies all music and motion generation tasks into a single transformer decoder architecture with a single training task of music-motion joint generation. Moreover, the model is designed by fine-tuning existing pre-trained single-modality models, significantly reducing computational demands. Extensive experiments demonstrate that UniMuMo achieves competitive results on all unidirectional generation benchmarks across music, motion, and text modalities. Quantitative results are available in the https://hanyangclarence.github.io/unimumo_demo/{project page}.', 'score': 18, 'issue_id': 14, 'pub_date': '2024-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '4bfc87467c16c280', 'data': {'categories': ['#audio', '#video', '#dataset', '#cv', '#training', '#graphs', '#data', '#optimization', '#transfer_learning', '#benchmark', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'UniMuMo: единая модель для генерации текста, музыки и движений', 'desc': 'UniMuMo - это унифицированная мультимодальная модель, способная генерировать текст, музыку и движения на основе входных данных в этих модальностях. Модель использует выравнивание неспаренных музыкальных и двигательных данных на основе ритмических паттернов. Архитектура UniMuMo основана на трансформере типа энкодер-декодер с токенизированным представлением всех модальностей. Модель достигает конкурентоспособных результатов на различных бенчмарках по однонаправленной генерации для музыки, движений и текста.'}, 'en': {'title': 'UniMuMo: Harmonizing Text, Music, and Motion in One Model', 'desc': 'The paper introduces UniMuMo, a model that can generate text, music, and motion from any of these inputs. It solves the problem of unpaired data by aligning music and motion based on rhythm, using large datasets. The model uses a unified transformer architecture to convert these inputs into tokens, allowing them to be processed together. By fine-tuning existing models, it efficiently handles multiple tasks, showing strong performance across different generation benchmarks.'}, 'zh': {'title': 'UniMuMo：跨越文本、音乐与动作的多模态生成', 'desc': '这篇论文介绍了一个名为UniMuMo的统一多模态模型，可以根据任意的文本、音乐和动作数据生成相应的输出。为了克服时间同步数据的缺乏，研究人员通过节奏模式对未配对的音乐和动作数据进行对齐。模型通过将音乐、动作和文本转换为基于token的表示，使用统一的编码器-解码器Transformer架构连接这些模态。通过对现有的单模态预训练模型进行微调，显著降低了计算需求，并在多种生成任务中取得了竞争性结果。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05229', 'title': 'GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models', 'url': 'https://huggingface.co/papers/2410.05229', 'abstract': "Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.", 'score': 17, 'issue_id': 18, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '96f3985414b99bf9', 'data': {'categories': ['#reasoning', '#training', '#math', '#benchmark', '#open_source', '#synthetic'], 'emoji': '🧮', 'ru': {'title': 'Разоблачение иллюзии математического мышления у языковых моделей', 'desc': 'Статья посвящена оценке способностей больших языковых моделей (LLM) к математическим рассуждениям. Авторы представляют новый бенчмарк GSM-Symbolic, созданный на основе символических шаблонов для более контролируемой оценки. Исследование показывает, что производительность LLM значительно снижается при изменении числовых значений в вопросах и увеличении количества условий. Авторы предполагают, что современные LLM не способны к подлинным логическим рассуждениям, а лишь воспроизводят шаги рассуждений из обучающих данных.'}, 'en': {'title': 'Unveiling the Limits of LLMs in Math Reasoning', 'desc': "This paper explores the mathematical reasoning abilities of Large Language Models (LLMs) using a new benchmark called GSM-Symbolic. The study reveals that LLMs struggle with genuine logical reasoning, as their performance drops significantly when questions are slightly altered. The research highlights that LLMs often rely on patterns from their training data rather than true reasoning, especially when questions become more complex. The findings suggest that current metrics may overestimate LLMs' reasoning capabilities, emphasizing the need for more reliable evaluation methods."}, 'zh': {'title': '揭示大型语言模型数学推理的局限性', 'desc': '这篇论文研究了大型语言模型在数学推理方面的能力，特别是使用GSM8K基准进行评估。作者提出了一个新的评估基准GSM-Symbolic，通过符号模板生成多样化的问题集，以更好地评估模型的推理能力。研究发现，模型在面对同一问题的不同实例时表现出显著的差异，尤其是在问题中仅改变数值时，性能会下降。论文指出，当前的模型可能无法进行真正的逻辑推理，而是依赖于训练数据中的推理步骤。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05046', 'title': 'Named Clinical Entity Recognition Benchmark', 'url': 'https://huggingface.co/papers/2410.05046', 'abstract': 'This technical report introduces a Named Clinical Entity Recognition Benchmark for evaluating language models in healthcare, addressing the crucial natural language processing (NLP) task of extracting structured information from clinical narratives to support applications like automated coding, clinical trial cohort identification, and clinical decision support.   The leaderboard provides a standardized platform for assessing diverse language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities across multiple medical domains. A curated collection of openly available clinical datasets is utilized, encompassing entities such as diseases, symptoms, medications, procedures, and laboratory measurements. Importantly, these entities are standardized according to the Observational Medical Outcomes Partnership (OMOP) Common Data Model, ensuring consistency and interoperability across different healthcare systems and datasets, and a comprehensive evaluation of model performance. Performance of models is primarily assessed using the F1-score, and it is complemented by various assessment modes to provide comprehensive insights into model performance. The report also includes a brief analysis of models evaluated to date, highlighting observed trends and limitations.   By establishing this benchmarking framework, the leaderboard aims to promote transparency, facilitate comparative analyses, and drive innovation in clinical entity recognition tasks, addressing the need for robust evaluation methods in healthcare NLP.', 'score': 17, 'issue_id': 17, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '13152fb25949544e', 'data': {'categories': ['#science', '#dataset', '#healthcare', '#benchmark', '#open_source'], 'emoji': '🏥', 'ru': {'title': 'Новый стандарт оценки языковых моделей в медицинском NLP', 'desc': 'Этот технический отчет представляет новый бенчмарк для оценки языковых моделей в распознавании именованных клинических сущностей. Бенчмарк использует стандартизированные наборы данных для оценки способности моделей идентифицировать и классифицировать клинические сущности в различных медицинских областях. Основной метрикой оценки является F1-мера, дополненная другими режимами оценки для комплексного анализа производительности моделей. Целью бенчмарка является продвижение прозрачности, сравнительного анализа и инноваций в задачах распознавания клинических сущностей.'}, 'en': {'title': 'Benchmarking Healthcare NLP: Setting the Standard for Clinical Entity Recognition', 'desc': 'This paper introduces a benchmark for evaluating language models in healthcare, focusing on the task of extracting structured information from clinical narratives. It provides a standardized platform to assess various language models, including encoder and decoder architectures, on their ability to identify and classify clinical entities like diseases and medications. The benchmark uses datasets standardized by the OMOP Common Data Model to ensure consistency across healthcare systems. Model performance is primarily measured using the F1-score, with additional assessment modes to offer comprehensive insights.'}, 'zh': {'title': '推动医疗领域语言模型的创新与透明', 'desc': '这篇技术报告介绍了一种用于评估语言模型在医疗领域表现的命名临床实体识别基准。该基准通过标准化平台评估不同语言模型在识别和分类多种医学领域临床实体的能力。使用的临床数据集包括疾病、症状、药物、程序和实验室测量等实体，并根据OMOP通用数据模型进行标准化。通过这种基准框架，排行榜旨在促进透明度、便于比较分析，并推动临床实体识别任务中的创新。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05243', 'title': 'Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents', 'url': 'https://huggingface.co/papers/2410.05243', 'abstract': 'Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly take pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.', 'score': 16, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'c91b020a4bdfc21e', 'data': {'categories': ['#dataset', '#cv', '#graphs', '#agents', '#benchmark', '#games', '#open_source', '#architecture', '#synthetic', '#multimodal'], 'emoji': '👁️', 'ru': {'title': 'Визуальное восприятие - ключ к эффективным GUI-агентам', 'desc': 'Статья представляет новый подход к созданию агентов графического пользовательского интерфейса (GUI) с использованием мультимодальных больших языковых моделей. Авторы предлагают модель UGround, которая обучается на визуальном восприятии интерфейса без использования текстовых представлений. Эксперименты показывают, что UGround превосходит существующие модели визуальной привязки для GUI-агентов на 20%. Результаты демонстрируют перспективность создания агентов, навигирующих в цифровом мире подобно людям.'}, 'en': {'title': 'Seeing is Believing: Revolutionizing GUI Agents with Visual Grounding', 'desc': "This paper explores the development of GUI agents that operate using visual perception, similar to how humans interact with digital interfaces. By focusing on visual grounding models, the authors aim to improve the accuracy of mapping GUI elements to their coordinates, enhancing the agents' effectiveness. They introduce UGround, a model trained on a large dataset of GUI elements, which significantly outperforms existing models by relying solely on visual data. The study demonstrates that GUI agents can achieve superior performance without the need for text-based inputs, suggesting a promising future for visually-grounded digital navigation."}, 'zh': {'title': '像人类一样导航数字世界的GUI代理', 'desc': '这篇论文介绍了一种新的图形用户界面（GUI）代理方法，利用多模态大语言模型（MLLMs）来增强其在真实世界应用中的能力。传统的GUI代理主要依赖于文本表示，但这种方法常常带来噪声和不完整性。作者提出了一种类似人类的GUI代理方法，完全通过视觉感知环境，并在像素级别进行操作。通过使用合成数据和LLaVA架构的轻微调整，他们训练出了一种名为UGround的视觉定位模型，显著提升了GUI代理的性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03825', 'title': 'MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion', 'url': 'https://huggingface.co/papers/2410.03825', 'abstract': "Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.", 'score': 16, 'issue_id': 14, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '0327c1225649bc76', 'data': {'categories': ['#video', '#cv', '#training', '#optimization', '#transfer_learning', '#synthetic', '#3d'], 'emoji': '🎥', 'ru': {'title': 'MonST3R: Революция в оценке геометрии динамических сцен', 'desc': 'MonST3R - это новый подход к оценке геометрии динамических сцен в компьютерном зрении. Вместо сложных многоступенчатых систем, он напрямую оценивает геометрию для каждого кадра, адаптируя представление DUST3R для динамических сцен. Несмотря на проблему нехватки данных для обучения, авторы смогли обучить модель, используя дообучение и стратегический подбор датасетов. MonST3R показывает высокую производительность в задачах оценки глубины видео и позы камеры, а также обещающие результаты в 4D-реконструкции.'}, 'en': {'title': 'Simplifying Dynamic Scene Geometry with MonST3R', 'desc': 'The paper introduces Motion DUSt3R (MonST3R), a new method for estimating geometry in dynamic scenes, where objects move and change shape over time. Unlike traditional methods that break the problem into smaller tasks, MonST3R directly estimates geometry for each moment, simplifying the process and reducing errors. The authors tackle the challenge of limited training data by fine-tuning the model on selected datasets, allowing it to handle dynamic scenes effectively. MonST3R not only improves video depth and camera pose estimation but also shows potential for efficient 4D reconstruction.'}, 'zh': {'title': '动态场景几何估计的新突破：MonST3R', 'desc': '这篇论文提出了一种新的方法，称为Motion DUSt3R（MonST3R），用于从动态场景中直接估计每个时间步的几何结构。传统方法通常将问题分解为多个子任务，导致系统复杂且容易出错，而MonST3R通过简单估计每个时间步的点图来解决这一问题。尽管缺乏合适的训练数据是一个挑战，但通过将问题视为微调任务并选择合适的数据集进行训练，模型能够在没有明确运动表示的情况下处理动态场景。实验结果表明，MonST3R在视频深度和相机姿态估计等任务中表现出色，优于以往的方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04734', 'title': 'TLDR: Token-Level Detective Reward Model for Large Vision Language Models', 'url': 'https://huggingface.co/papers/2410.04734', 'abstract': 'Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one binary feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose a Token-Level Detective Reward Model (TLDR) to provide fine-grained annotations to each text token. We first introduce a perturbation-based method to generate synthetic hard negatives and their token-level labels to train TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as a hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation by 3 times to acquire a broader range of high-quality vision language data.', 'score': 16, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '8c4b74044ea31d8d', 'data': {'categories': ['#hallucinations', '#training', '#alignment', '#rlhf', '#synthetic', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Точность на уровне токенов: новый подход к мультимодальным языковым моделям', 'desc': "В статье представлена модель вознаграждения на уровне токенов (TLDR) для мультимодальных языковых моделей. TLDR обеспечивает детальную аннотацию каждого текстового токена, что позволяет лучше учитывать как текстовую, так и визуальную информацию. Метод использует синтетические 'сложные отрицательные примеры' для обучения модели. TLDR может применяться для самокоррекции генераций моделей и оценки галлюцинаций, а также ускоряет процесс аннотации данных в 3 раза."}, 'en': {'title': '"Token-Level Precision: Elevating Multimodal Models with TLDR"', 'desc': "The paper introduces a Token-Level Detective Reward Model (TLDR) to enhance the feedback mechanism in multimodal large language models. Unlike traditional reward models that provide binary feedback, TLDR offers detailed annotations for each text token, improving the model's understanding of both text and images. The authors use a perturbation-based method to create challenging examples and train the TLDR model, which helps models self-correct and evaluate hallucinations. Additionally, TLDR models can accelerate human annotation processes, making it three times faster to gather high-quality data."}, 'zh': {'title': '细粒度奖励模型：提升多模态语言模型的关键', 'desc': '这篇论文提出了一种新的奖励模型，称为Token-Level Detective Reward Model（TLDR），用于改进多模态大语言模型。传统的奖励模型只给文本整体一个二元反馈，而TLDR模型则对每个文本标记提供细粒度的注释。通过引入基于扰动的方法，生成合成的困难负样本及其标记，来训练TLDR模型。实验表明，TLDR模型不仅能帮助现有模型自我纠正生成内容，还能作为幻觉评估工具，并能将人工注释速度提高三倍。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05167', 'title': 'Presto! Distilling Steps and Layers for Accelerating Music Generation', 'url': 'https://huggingface.co/papers/2410.05167', 'abstract': 'Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than comparable SOTA) -- the fastest high-quality TTM to our knowledge. Sound examples can be found at https://presto-music.github.io/web/.', 'score': 15, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'f332523f23a7857e', 'data': {'categories': ['#audio', '#training', '#inference', '#optimization', '#diffusion'], 'emoji': '🎵', 'ru': {'title': 'Ускорение генерации музыки по тексту с помощью дистилляции диффузионных моделей', 'desc': 'Статья представляет Presto! - новый подход к ускорению генерации музыки на основе текста с использованием диффузионных трансформеров. Авторы разработали метод дистилляции на основе согласования распределений (DMD) для уменьшения количества шагов сэмплирования. Они также улучшили метод послойной дистилляции для снижения вычислительных затрат на каждом шаге. Комбинация этих методов позволяет генерировать высококачественную музыку в 10-18 раз быстрее базовой модели.'}, 'en': {'title': 'Presto! - Fast-Track Your Text-to-Music Creations', 'desc': 'The paper introduces Presto!, a method to speed up text-to-music generation using diffusion transformers by reducing both the number of sampling steps and the cost per step. It employs a novel score-based distribution matching distillation (DMD) method, which is the first GAN-based distillation approach for text-to-music models. Additionally, it enhances a recent layer distillation method to better preserve hidden state variance, improving learning efficiency. The combined approach significantly accelerates the model, achieving high-quality outputs with greater diversity and up to 18 times faster than previous state-of-the-art methods.'}, 'zh': {'title': 'Presto!：快速高质量的文本到音乐生成', 'desc': '这篇论文介绍了一种名为Presto!的方法，用于加速基于扩散的文本到音乐生成。通过减少采样步骤和每步的计算成本，Presto!提高了生成效率。研究者开发了一种新的基于得分的分布匹配蒸馏方法和改进的层蒸馏方法，以更好地保持隐藏状态的方差。最终，结合这两种蒸馏方法，Presto!实现了高质量输出的快速生成，比现有技术快15倍。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04698', 'title': 'MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2410.04698', 'abstract': "Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark.", 'score': 13, 'issue_id': 15, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '1e15804dd57fc363', 'data': {'categories': ['#reasoning', '#long_context', '#math', '#benchmark', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'MathHay: Новый вызов для математических способностей языковых моделей', 'desc': 'Статья представляет новый бенчмарк MathHay для оценки способностей больших языковых моделей (LLM) к математическим рассуждениям в контексте длинных текстов. В отличие от существующих бенчмарков, MathHay требует от моделей не только поиска информации, но и сложных математических вычислений. Эксперименты с восемью ведущими LLM показали, что даже лучшая модель Gemini-1.5-Pro-002 достигает точности всего 51.26% на текстах длиной 128 тысяч токенов. Результаты указывают на значительный потенциал для улучшения способностей LLM в области длинных математических рассуждений.'}, 'en': {'title': 'Pushing the Limits: Math Reasoning in Long Contexts', 'desc': 'The paper introduces MathHay, a new benchmark specifically designed to evaluate the mathematical reasoning abilities of large language models (LLMs) over long contexts. Unlike previous benchmarks, MathHay requires models to not only retrieve information but also perform complex mathematical reasoning. Experiments conducted on eight leading LLMs reveal that even the best model, Gemini-1.5-Pro-002, achieves only 51.26% accuracy at 128K tokens, indicating challenges in this area. This study highlights the need for further advancements in LLMs to improve their mathematical reasoning capabilities in long-context scenarios.'}, 'zh': {'title': 'MathHay：挑战大型语言模型的数学推理极限', 'desc': '这篇论文介绍了一种名为MathHay的自动化基准，用于评估大型语言模型在长文本中的数学推理能力。与之前的基准不同，MathHay不仅要求模型具备信息检索能力，还需要复杂的数学推理能力。研究表明，即使是表现最好的模型在长文本数学推理中也仅能达到51.26%的准确率。这表明在MathHay基准上还有很大的改进空间。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04932', 'title': 'OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction', 'url': 'https://huggingface.co/papers/2410.04932', 'abstract': 'We present OmniBooth, an image generation framework that enables spatial control with instance-level multi-modal customization. For all instances, the multimodal instruction can be described through text prompts or image references. Given a set of user-defined masks and associated text or image guidance, our objective is to generate an image, where multiple objects are positioned at specified coordinates and their attributes are precisely aligned with the corresponding guidance. This approach significantly expands the scope of text-to-image generation, and elevates it to a more versatile and practical dimension in controllability. In this paper, our core contribution lies in the proposed latent control signals, a high-dimensional spatial feature that provides a unified representation to integrate the spatial, textual, and image conditions seamlessly. The text condition extends ControlNet to provide instance-level open-vocabulary generation. The image condition further enables fine-grained control with personalized identity. In practice, our method empowers users with more flexibility in controllable generation, as users can choose multi-modal conditions from text or images as needed. Furthermore, thorough experiments demonstrate our enhanced performance in image synthesis fidelity and alignment across different tasks and datasets. Project page: https://len-li.github.io/omnibooth-web/', 'score': 9, 'issue_id': 14, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '20ada6e6ba364412', 'data': {'categories': ['#cv', '#interpretability', '#optimization', '#diffusion', '#architecture', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Управляемое творчество: новый уровень генерации изображений', 'desc': 'OmniBooth - это фреймворк для генерации изображений с пространственным контролем и мультимодальной кастомизацией на уровне отдельных объектов. Система позволяет задавать расположение и атрибуты объектов с помощью текстовых запросов или изображений-образцов. Ключевым элементом является предложенный латентный сигнал управления - многомерный пространственный признак, объединяющий пространственные, текстовые и визуальные условия. OmniBooth расширяет возможности текст-в-изображение генерации, предоставляя пользователям гибкий инструмент для контролируемого создания изображений.'}, 'en': {'title': 'OmniBooth: Mastering Image Generation with Precision and Flexibility', 'desc': 'OmniBooth is a framework for generating images where users can control the placement and attributes of objects using text prompts or image references. It introduces latent control signals, which are high-dimensional features that integrate spatial, textual, and image conditions for seamless image generation. This method enhances the flexibility of text-to-image generation by allowing instance-level customization and open-vocabulary generation. Experiments show that OmniBooth improves image synthesis quality and alignment across various tasks and datasets.'}, 'zh': {'title': 'OmniBooth：实现图像生成的多模态空间控制', 'desc': 'OmniBooth 是一个图像生成框架，允许用户通过文本提示或图像参考来进行空间控制和实例级多模态定制。用户可以定义遮罩和相关的文本或图像指导，以生成在指定坐标上放置多个对象的图像，并精确对齐其属性。该方法通过提出潜在控制信号，将空间、文本和图像条件无缝整合，扩展了文本到图像生成的范围。实验表明，OmniBooth 在图像合成的保真度和对齐性上表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05262', 'title': 'TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles', 'url': 'https://huggingface.co/papers/2410.05262', 'abstract': 'As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic interactions with users. Moreover, these benchmarks often depend on specific background knowledge, complicating the measurement of a model\'s logical reasoning capabilities. Other dynamic evaluation methods based on strong models or manual efforts may introduce biases and incur high costs and time demands, hindering large-scale application. To address these issues, we propose TurtleBench. TurtleBench collects real user guesses from our online Turtle Soup Puzzle platform that we developed. This approach allows for the relatively dynamic generation of evaluation datasets, mitigating the risk of model cheating while aligning assessments more closely with genuine user needs for reasoning capabilities, thus enhancing the reliability of evaluations. TurtleBench includes 1,532 user guesses along with the correctness of guesses after annotation. Using this dataset, we thoroughly evaluated nine of the most advanced LLMs available today. Notably, the OpenAI o1 series models did not achieve leading results in these evaluations. We propose several hypotheses for further research, such as "the latent reasoning of o1 utilizes trivial Chain-of-Thought (CoT) techniques" and "increasing CoT length not only provides reasoning benefits but also incurs noise costs."', 'score': 9, 'issue_id': 13, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '6d3f3633d606dec9', 'data': {'categories': ['#reasoning', '#dataset', '#evaluation', '#data', '#benchmark', '#architecture'], 'emoji': '🐢', 'ru': {'title': 'TurtleBench: Динамическая оценка LLM на основе реальных пользовательских догадок', 'desc': 'TurtleBench - новый метод оценки больших языковых моделей (LLM), основанный на реальных догадках пользователей в игре Turtle Soup Puzzle. Этот подход позволяет динамически генерировать наборы данных для оценки, снижая риск обмана со стороны моделей и более точно отражая потребности пользователей в способностях рассуждения. В исследовании оценивались девять передовых LLM, причем модели серии OpenAI o1 не показали лидирующих результатов. Авторы выдвигают гипотезы для дальнейших исследований, включая использование тривиальных методов цепочки рассуждений (Chain-of-Thought) в латентных рассуждениях o1.'}, 'en': {'title': 'TurtleBench: Revolutionizing LLM Evaluation with Real User Interactions', 'desc': "The paper introduces TurtleBench, a novel evaluation framework for Large Language Models (LLMs) that uses real user interactions from an online puzzle platform to create dynamic datasets. This method addresses the limitations of static benchmarks and reduces biases and costs associated with manual evaluations. TurtleBench's dataset includes over 1,500 user guesses, providing a more authentic measure of a model's reasoning capabilities. The study reveals that some advanced models, like OpenAI's o1 series, may not perform as well as expected, suggesting areas for further research into reasoning techniques and their trade-offs."}, 'zh': {'title': 'TurtleBench：动态评估大型语言模型的新方法', 'desc': '随着大型语言模型（LLMs）的应用扩大，对可靠评估的需求也在增加。现有的评估基准主要依赖于静态数据集，难以评估模型在与用户动态交互中的表现。TurtleBench通过收集真实用户在Turtle Soup Puzzle平台上的猜测，动态生成评估数据集，降低了模型作弊的风险。通过这种方法，我们对九个先进的LLMs进行了评估，发现OpenAI o1系列模型未能取得领先结果。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03617', 'title': 'What Matters for Model Merging at Scale?', 'url': 'https://huggingface.co/papers/2410.03617', 'abstract': "Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.", 'score': 8, 'issue_id': 18, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '1890d06170bcbcb8', 'data': {'categories': ['#training', '#optimization', '#transfer_learning', '#benchmark', '#small_models', '#architecture'], 'emoji': '🔀', 'ru': {'title': 'Объединение моделей в масштабе: больше, лучше, эффективнее', 'desc': 'Статья исследует масштабируемое объединение моделей машинного обучения, анализируя влияние размера модели, качества базовой модели и количества экспертных моделей на производительность объединенной модели. Авторы проводят эксперименты с объединением полностью дообученных моделей размером от 1 до 64 миллиардов параметров, используя четыре популярных метода объединения. Результаты показывают, что объединение более эффективно при использовании сильных базовых моделей и более крупных моделей, а также улучшает способности к обобщению. Исследование предоставляет ценные выводы о свойствах и ограничениях масштабного объединения моделей.'}, 'en': {'title': 'Unlocking the Power of Model Merging at Scale', 'desc': 'This paper explores the concept of model merging, which combines multiple expert models into a single, more capable model, focusing on large-scale applications. The study evaluates the effectiveness of merging models of varying sizes, from 1 billion to 64 billion parameters, using four different methods. Key findings include that merging is more successful with strong base models, larger models facilitate easier merging, and merging improves generalization capabilities. The research provides insights into the benefits and limitations of model merging, aiming to guide future studies in this area.'}, 'zh': {'title': '大规模模型合并：提升泛化能力的新途径', 'desc': '这篇论文研究了如何将多个专家模型合并成一个更强大的单一模型。研究发现，合并效果在专家模型来自强大的基础模型时更好，尤其是这些基础模型在零样本任务中表现良好。大规模模型更容易进行合并，并且合并后模型的泛化能力通常优于多任务训练的模型。不同的合并方法在大规模下表现相似，这为未来的大规模模型合并研究提供了参考。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05057', 'title': 'SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification', 'url': 'https://huggingface.co/papers/2410.05057', 'abstract': 'Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT, the first large-scale benchmark of curation strategies for image classification.   In order to generate baseline methods for the SELECT benchmark, we create a new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K to date. Our dataset extends ImageNet with 5 new training-data shifts, each approximately the size of ImageNet-1K itself, and each assembled using a distinct curation strategy. We evaluate our data curation baselines in two ways: (i) using each training-data shift to train identical image classification models from scratch (ii) using the data itself to fit a pretrained self-supervised representation.   Our findings show interesting trends, particularly pertaining to recent methods for data curation such as synthetic data generation and lookup based on CLIP embeddings. We show that although these strategies are highly competitive for certain tasks, the curation strategy used to assemble the original ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark can illuminate the path for new methods to further reduce the gap. We release our checkpoints, code, documentation, and a link to our dataset at https://github.com/jimmyxu123/SELECT.', 'score': 7, 'issue_id': 18, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '830db899ae55d1d4', 'data': {'categories': ['#dataset', '#cv', '#data', '#benchmark', '#open_source', '#synthetic'], 'emoji': '🔍', 'ru': {'title': 'Систематическая оценка стратегий курирования данных для компьютерного зрения', 'desc': 'Статья представляет новый эталонный тест SELECT для оценки стратегий курирования данных в задачах классификации изображений. Авторы создали расширенный набор данных ImageNet++, включающий 5 новых сдвигов обучающих данных. Проведена оценка различных стратегий курирования, включая синтетическую генерацию данных и поиск на основе CLIP-эмбеддингов. Результаты показывают, что оригинальная стратегия курирования ImageNet-1K остается золотым стандартом.'}, 'en': {'title': 'SELECT: Benchmarking the Future of Data Curation', 'desc': 'The paper addresses the challenge of data curation, which involves collecting and organizing samples into datasets that enhance machine learning efficiency. It introduces SELECT, a benchmark for evaluating different data curation strategies, using a new dataset called ImageNet++. This dataset expands on ImageNet-1K with five new data shifts, each created using a unique curation method. The study finds that while new curation methods like synthetic data generation show promise, the original ImageNet-1K curation strategy remains the most effective.'}, 'zh': {'title': '数据整理新基准：SELECT的诞生', 'desc': '这篇论文探讨了如何收集和组织样本以支持高效学习的数据整理问题。作者引入了SELECT，这是第一个用于图像分类的大规模数据整理策略基准。通过创建新的数据集ImageNet++，他们评估了不同的数据整理策略。研究发现，尽管新方法在某些任务中表现出色，但原始ImageNet-1K的数据整理策略仍然是标准。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03187', 'title': 'Autonomous Character-Scene Interaction Synthesis from Text Instruction', 'url': 'https://huggingface.co/papers/2410.03187', 'abstract': 'Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and human-object interaction, presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.', 'score': 7, 'issue_id': 15, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '233eb6997f48b10d', 'data': {'categories': ['#dataset', '#cv', '#training', '#rl', '#games', '#diffusion', '#synthetic', '#multimodal', '#3d'], 'emoji': '🤖', 'ru': {'title': 'ИИ оживляет персонажей в 3D по текстовым командам', 'desc': 'Статья представляет новую систему для синтеза сложных движений человека в 3D-средах на основе текстовых инструкций и целевого положения. Авторы используют авторегрессионную диффузионную модель для генерации последовательных сегментов движения и автономный планировщик для предсказания переходов между этапами действий. Для интеграции движений с окружением предложено специальное представление сцены, учитывающее локальное восприятие. Также создан обширный датасет из 16 часов захваченных движений в 120 indoor-сценах для обучения модели.'}, 'en': {'title': 'Animating Life: From Text to Motion in 3D Worlds', 'desc': "This paper introduces a new framework for creating 3D human motions in complex environments using a single text instruction and goal location. It uses an auto-regressive diffusion model to generate motion segments and an autonomous scheduler to manage transitions between action stages. The approach includes a scene representation that accounts for local perception at both the start and goal locations, enhancing motion integration. A large motion-captured dataset with language annotations supports the model's training, showing effective results in producing realistic, multi-stage motions."}, 'zh': {'title': '从文本到动作：自动化多阶段场景感知运动合成', 'desc': '这篇论文提出了一种新的框架，可以从单一的文本指令和目标位置合成多阶段的场景感知交互动作。该方法使用自回归扩散模型来合成下一个动作片段，并通过自主调度器预测每个动作阶段的过渡。为了确保合成的动作与环境无缝集成，论文提出了一种场景表示方法，考虑了起始和目标位置的局部感知。实验结果表明，该方法能够生成高质量的多阶段动作，与环境和文本条件高度一致。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03160', 'title': 'Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach', 'url': 'https://huggingface.co/papers/2410.03160', 'abstract': "Diffusion models have revolutionized image generation, and their extension to video generation has shown promise. However, current video diffusion models~(VDMs) rely on a scalar timestep variable applied at the clip level, which limits their ability to model complex temporal dependencies needed for various tasks like image-to-video generation. To address this limitation, we propose a frame-aware video diffusion model~(FVDM), which introduces a novel vectorized timestep variable~(VTV). Unlike conventional VDMs, our approach allows each frame to follow an independent noise schedule, enhancing the model's capacity to capture fine-grained temporal dependencies. FVDM's flexibility is demonstrated across multiple tasks, including standard video generation, image-to-video generation, video interpolation, and long video synthesis. Through a diverse set of VTV configurations, we achieve superior quality in generated videos, overcoming challenges such as catastrophic forgetting during fine-tuning and limited generalizability in zero-shot methods.Our empirical evaluations show that FVDM outperforms state-of-the-art methods in video generation quality, while also excelling in extended tasks. By addressing fundamental shortcomings in existing VDMs, FVDM sets a new paradigm in video synthesis, offering a robust framework with significant implications for generative modeling and multimedia applications.", 'score': 4, 'issue_id': 22, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '9b55e7feeb00d83b', 'data': {'categories': ['#video', '#training', '#optimization', '#diffusion', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации видео: покадровая диффузионная модель', 'desc': 'Статья представляет новый подход к генерации видео с использованием диффузионных моделей. Авторы предлагают модель FVDM (Frame-aware Video Diffusion Model), которая вводит векторизованную переменную временного шага (VTV) для каждого кадра. Это позволяет модели лучше улавливать сложные временные зависимости в видео. FVDM демонстрирует превосходные результаты в различных задачах, включая генерацию видео из изображений и интерполяцию видео.'}, 'en': {'title': 'Frame-Aware Diffusion: Revolutionizing Video Generation', 'desc': 'The paper introduces a new approach to video generation using diffusion models, called the frame-aware video diffusion model (FVDM). Unlike traditional models that use a single timestep for an entire video clip, FVDM uses a vectorized timestep variable, allowing each frame to have its own noise schedule. This innovation enables the model to better capture complex temporal dependencies, improving the quality of generated videos across various tasks. The results show that FVDM outperforms existing methods, offering a more flexible and robust framework for video synthesis.'}, 'zh': {'title': '帧感知扩散模型：视频生成的新范式', 'desc': '扩散模型在图像生成中取得了革命性进展，并在视频生成中展现了潜力。然而，现有的视频扩散模型在处理复杂的时间依赖性时存在局限性。为了解决这个问题，我们提出了一种帧感知视频扩散模型，通过引入矢量化时间步变量来增强模型的时间依赖性捕捉能力。实验结果表明，该模型在视频生成质量上优于现有方法，并在多项任务中表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05255', 'title': 'SePPO: Semi-Policy Preference Optimization for Diffusion Alignment', 'url': 'https://huggingface.co/papers/2410.05255', 'abstract': 'Reinforcement learning from human feedback (RLHF) methods are emerging as a way to fine-tune diffusion models (DMs) for visual generation. However, commonly used on-policy strategies are limited by the generalization capability of the reward model, while off-policy approaches require large amounts of difficult-to-obtain paired human-annotated data, particularly in visual generation tasks. To address the limitations of both on- and off-policy RLHF, we propose a preference optimization method that aligns DMs with preferences without relying on reward models or paired human-annotated data. Specifically, we introduce a Semi-Policy Preference Optimization (SePPO) method. SePPO leverages previous checkpoints as reference models while using them to generate on-policy reference samples, which replace "losing images" in preference pairs. This approach allows us to optimize using only off-policy "winning images." Furthermore, we design a strategy for reference model selection that expands the exploration in the policy space. Notably, we do not simply treat reference samples as negative examples for learning. Instead, we design an anchor-based criterion to assess whether the reference samples are likely to be winning or losing images, allowing the model to selectively learn from the generated reference samples. This approach mitigates performance degradation caused by the uncertainty in reference sample quality. We validate SePPO across both text-to-image and text-to-video benchmarks. SePPO surpasses all previous approaches on the text-to-image benchmarks and also demonstrates outstanding performance on the text-to-video benchmarks. Code will be released in https://github.com/DwanZhang-AI/SePPO.', 'score': 4, 'issue_id': 19, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '1202d13e6d3d4583', 'data': {'categories': ['#video', '#cv', '#training', '#diffusion', '#rl', '#optimization', '#benchmark', '#alignment', '#open_source', '#rlhf'], 'emoji': '🖼️', 'ru': {'title': 'Оптимизация предпочтений без размеченных данных для генеративных моделей', 'desc': "Статья представляет новый метод оптимизации предпочтений для диффузионных моделей в задачах генерации изображений и видео. Метод SePPO (Semi-Policy Preference Optimization) использует предыдущие чекпоинты модели в качестве референсных и генерирует референсные примеры для замены 'проигрышных' изображений в парах предпочтений. Авторы разработали стратегию выбора референсных моделей и критерий на основе якорей для оценки качества референсных примеров. SePPO превзошел существующие подходы на бенчмарках text-to-image и text-to-video."}, 'en': {'title': 'Revolutionizing Visual Generation with SePPO: Smarter Learning from Human Preferences', 'desc': 'The paper introduces a new method called Semi-Policy Preference Optimization (SePPO) to improve diffusion models for visual generation using reinforcement learning from human feedback. Unlike traditional methods that rely heavily on reward models or large datasets of human-annotated images, SePPO uses previous model checkpoints to generate reference samples, optimizing only with "winning images." This approach allows the model to learn more effectively by using an anchor-based criterion to evaluate the quality of reference samples, avoiding performance issues from uncertain data. The method shows superior results in both text-to-image and text-to-video generation tasks, outperforming existing techniques.'}, 'zh': {'title': 'SePPO：无需奖励模型的视觉生成优化', 'desc': '这篇论文介绍了一种新的方法来优化视觉生成模型，称为半策略偏好优化（SePPO）。传统的强化学习方法需要大量的人类标注数据，而SePPO通过使用之前的模型检查点作为参考，减少了对这些数据的依赖。SePPO通过选择性地学习生成的参考样本，避免了因样本质量不确定性导致的性能下降。实验结果表明，SePPO在文本到图像和文本到视频的基准测试中表现优异。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03959', 'title': 'Grounding Language in Multi-Perspective Referential Communication', 'url': 'https://huggingface.co/papers/2410.03959', 'abstract': "We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them. We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model.", 'score': 3, 'issue_id': 12, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '7cc5107d8cc84062', 'data': {'categories': ['#dataset', '#cv', '#training', '#agents', '#games', '#open_source'], 'emoji': '👥', 'ru': {'title': 'Улучшение коммуникации ИИ-агентов через учет перспективы собеседника', 'desc': 'Статья представляет новую задачу и набор данных для генерации и понимания референциальных выражений в многоагентных средах. Два агента в общей сцене должны учитывать визуальную перспективу друг друга для создания и понимания ссылок на объекты и их пространственные отношения. Авторы собрали датасет из 2970 человеческих референциальных выражений с оценками понимания. Эксперименты показали, что производительность моделей в генерации и понимании ссылок отстает от человеческих пар, но обучение с обратной связью об успешности коммуникации улучшило результаты.'}, 'en': {'title': 'Bridging Perspectives: Enhancing AI Communication in Shared Spaces', 'desc': "This paper introduces a new task and dataset for generating and understanding referring expressions in environments where two agents must consider each other's visual perspectives. The dataset includes 2,970 human-written expressions and human comprehension judgments, used to evaluate automated models in both generating and understanding references. The study finds that current models perform worse than human pairs in these tasks. However, training an open-weight speaker model with feedback on communicative success improves its performance significantly, even surpassing the best proprietary model."}, 'zh': {'title': '多智能体环境中的指代表达生成与理解', 'desc': '这篇论文介绍了一项任务和数据集，用于在多智能体环境中生成和理解指代表达。在这个任务中，两个智能体需要考虑彼此不同的视觉视角，以生成和理解场景中物体的指代和空间关系。研究收集了2970个人类编写的指代表达，并评估了自动化模型在与人类搭档时作为说话者和听者的表现，发现模型在生成和理解指代方面的表现落后于人类。最后，通过训练一个开放权重的说话者模型，结合听者的沟通成功证据，模型的沟通成功率从58.9%提高到69.3%，甚至超过了最强的专有模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03960', 'title': 'SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation', 'url': 'https://huggingface.co/papers/2410.03960', 'abstract': "LLM inference for popular enterprise use cases, such as summarization, RAG, and code-generation, typically observes orders of magnitude longer prompt lengths than generation lengths. This characteristic leads to high cost of prefill and increased response latency. In this paper, we present SwiftKV, a novel model transformation and distillation procedure specifically designed to reduce the time and cost of processing prompt tokens while preserving high quality of generated tokens. SwiftKV combines three key mechanisms: i) SingleInputKV, which prefills later layers' KV cache using a much earlier layer's output, allowing prompt tokens to skip much of the model computation, ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the memory footprint and support larger batch size for higher throughput, and iii) a knowledge-preserving distillation procedure that can adapt existing LLMs for SwiftKV with minimal accuracy impact and low compute and data requirement. For Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50% and the memory requirement of the KV cache by 62.5% while incurring minimum quality degradation across a wide range of tasks. In the end-to-end inference serving using an optimized vLLM implementation, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100 GPUs.", 'score': 1, 'issue_id': 21, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'f79582d6875d464c', 'data': {'categories': ['#training', '#inference', '#optimization', '#transfer_learning', '#small_models', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'SwiftKV: Революция в ускорении инференса LLM', 'desc': 'SwiftKV - это новый метод трансформации и дистилляции моделей, направленный на ускорение обработки токенов промпта в LLM. Он включает в себя три ключевых механизма: SingleInputKV, AcrossKV и процедуру дистилляции с сохранением знаний. SwiftKV снижает вычислительные требования предварительного заполнения на 50% и требования к памяти KV-кэша на 62,5% для моделей Llama-3.1. В результате достигается двукратное увеличение общей пропускной способности и снижение времени на выходной токен на 60%.'}, 'en': {'title': 'SwiftKV: Speeding Up LLMs Without Sacrificing Quality', 'desc': 'The paper introduces SwiftKV, a method to make large language models (LLMs) more efficient by reducing the time and cost of processing prompt tokens. It uses three main techniques: SingleInputKV, which allows skipping some model computations; AcrossKV, which reduces memory use by merging caches; and a distillation process that adapts existing models with minimal accuracy loss. SwiftKV significantly cuts down on computational and memory needs while maintaining output quality, achieving faster processing speeds and higher throughput. This approach is particularly effective for models like Llama-3.1, enhancing their performance in enterprise applications.'}, 'zh': {'title': 'SwiftKV：高效处理提示令牌的创新方案', 'desc': 'SwiftKV 是一种新型模型转换和蒸馏方法，旨在降低处理提示令牌的时间和成本，同时保持生成令牌的高质量。它通过三个关键机制实现：SingleInputKV 允许提示令牌跳过大部分模型计算，AcrossKV 合并相邻层的 KV 缓存以减少内存占用，并通过知识保留蒸馏程序适应现有 LLM。SwiftKV 在 Llama-3.1-8B 和 70B 上减少了 50% 的预填充计算需求和 62.5% 的 KV 缓存内存需求。最终，SwiftKV 在优化的 vLLM 实现中实现了高达 2 倍的总吞吐量和 60% 的每个输出令牌时间降低。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04199', 'title': 'LongGenBench: Long-context Generation Benchmark', 'url': 'https://huggingface.co/papers/2410.04199', 'abstract': 'Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which allows for flexible configurations of customized generation context lengths. LongGenBench advances beyond traditional benchmarks by redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of performance degradation, with the Gemini-1.5-Flash model showing the least degradation among API accessed models, and the Qwen2 series exhibiting the least degradation in LongGenBench among open source models.', 'score': 17, 'issue_id': 25, 'pub_date': '2024-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '12ea9c1effd189a8', 'data': {'categories': ['#long_context', '#benchmark', '#open_source', '#architecture', '#synthetic'], 'emoji': '📏', 'ru': {'title': 'LongGenBench: новый рубеж в оценке генерации длинных текстов', 'desc': 'LongGenBench - это новый синтетический бенчмарк для оценки способности языковых моделей к генерации длинных контекстов. В отличие от существующих тестов, фокусирующихся на поиске информации, LongGenBench требует от моделей создания целостных длинных ответов. Исследование показало, что все модели, как API-доступные, так и с открытым исходным кодом, демонстрируют снижение производительности при работе с длинными контекстами. Модели Gemini-1.5-Flash и Qwen2 показали наименьшую деградацию среди API-моделей и моделей с открытым кодом соответственно.'}, 'en': {'title': '"LongGenBench: Elevating LLMs to New Contextual Heights"', 'desc': "The paper introduces LongGenBench, a new benchmark designed to evaluate the long-context generation capabilities of Large Language Models (LLMs). Unlike existing benchmarks that focus on retrieval tasks, LongGenBench assesses how well LLMs can generate coherent and contextually accurate text over long passages. The study finds that many models, both API-based and open source, show significant performance drops in long-context generation, with some models like Gemini-1.5-Flash and Qwen2 series performing better than others. This benchmark aims to fill the gap in evaluating LLMs' ability to handle extended text generation, providing a more comprehensive understanding of their capabilities."}, 'zh': {'title': '突破长文本生成的评估瓶颈', 'desc': '这篇论文介绍了一种新的基准测试，名为LongGenBench，用于评估大型语言模型在长文本生成中的表现。传统的基准测试主要关注信息检索能力，而LongGenBench则专注于生成连贯且上下文准确的长文本。研究发现，不同的模型在长文本生成中表现不同，其中Gemini-1.5-Flash和Qwen2系列模型的性能下降最小。通过这种新的测试方法，可以更全面地评估模型的长文本生成能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04717', 'title': '$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction Diversity on Generalization', 'url': 'https://huggingface.co/papers/2410.04717', 'abstract': "Understanding and accurately following instructions is critical for large language models (LLMs) to be effective across diverse tasks. In this work, we rigorously examine the key factors that enable models to generalize to unseen instructions, providing insights to guide the collection of data for instruction-tuning. Through controlled experiments, inspired by the Turing-complete Markov algorithm, we demonstrate that such generalization only emerges when training data is diversified enough across semantic domains. Our findings also reveal that merely diversifying within limited domains fails to ensure robust generalization. In contrast, cross-domain data diversification, even under constrained data budgets, significantly enhances a model's adaptability. We further extend our analysis to real-world scenarios, including fine-tuning of $textbf{specialist} and textbf{generalist}$ models. In both cases, we demonstrate that 1) better performance can be achieved by increasing the diversity of an established dataset while keeping the data size constant, and 2) when scaling up the data, diversifying the semantics of instructions is more effective than simply increasing the quantity of similar data. Our research provides important insights for dataset collation, particularly when optimizing model performance by expanding training data for both specialist and generalist scenarios. We show that careful consideration of data diversification is key: training specialist models with data extending beyond their core domain leads to significant performance improvements, while generalist models benefit from diverse data mixtures that enhance their overall instruction-following capabilities across a wide range of applications. Our results highlight the critical role of strategic diversification and offer clear guidelines for improving data quality.", 'score': 17, 'issue_id': 25, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '30a0e92854b70969', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#data', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Разнообразие данных - ключ к обобщению навыков языковых моделей', 'desc': 'Статья исследует факторы, позволяющие языковым моделям обобщать навыки на новые инструкции. Авторы проводят эксперименты, демонстрирующие, что такое обобщение возникает только при достаточном разнообразии обучающих данных по семантическим доменам. Исследование показывает, что диверсификация данных между доменами значительно повышает адаптивность модели. Результаты применимы как для специализированных, так и для универсальных моделей, подчеркивая важность стратегической диверсификации данных при обучении.'}, 'en': {'title': '"Diversity Drives Understanding: Enhancing LLMs with Cross-Domain Data"', 'desc': 'This paper explores how large language models (LLMs) can better understand and follow instructions by examining the factors that help them generalize to new tasks. The study shows that models perform better when trained on data that is diverse across different semantic domains, rather than just within a single domain. By diversifying the types of instructions in the training data, even with limited data, models become more adaptable and effective. The research provides guidelines for improving model performance by strategically diversifying training data, benefiting both specialist and generalist models.'}, 'zh': {'title': '数据多样化：提升模型泛化能力的关键', 'desc': '这篇论文探讨了如何让大型语言模型更好地理解和执行未见过的指令。研究表明，只有在训练数据在语义领域上足够多样化时，模型才能实现良好的泛化。通过实验，作者发现跨领域的数据多样化，即使在数据量有限的情况下，也能显著提高模型的适应性。论文还指出，增加数据的语义多样性比单纯增加相似数据的数量更有效。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.01912', 'title': 'A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation', 'url': 'https://huggingface.co/papers/2410.01912', 'abstract': "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, model depth, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.", 'score': 13, 'issue_id': 28, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '63855d0547d5d08f', 'data': {'categories': ['#science', '#dataset', '#cv', '#optimization', '#open_source', '#architecture', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'DnD-Transformer: Новый взгляд на авторегрессивную генерацию изображений', 'desc': 'Статья представляет новую архитектуру модели под названием 2-Dimensional Autoregression (DnD) Transformer для генерации изображений. DnD-Transformer решает проблему потери информации при векторном квантовании, вводя новое направление авторегрессии - глубину модели. По сравнению с традиционными методами, DnD-Transformer генерирует изображения более высокого качества при той же длине последовательности. Модель также демонстрирует способность генерировать изображения с текстом и графическими элементами, показывая понимание комбинированных модальностей.'}, 'en': {'title': '"DnD-Transformer: Elevating Image Generation with Depth and Direction"', 'desc': 'The paper introduces the DnD-Transformer, a new model architecture for autoregressive image generation that addresses the information loss bottleneck of vector-quantization. By adding a new autoregression direction, the model predicts more codes for an image, improving image quality without increasing model size. Unlike previous models, the DnD-Transformer can generate images with complex text and graphical elements, showcasing a unique understanding of combined modalities. This advancement suggests a new level of vision-language intelligence in image generation models.'}, 'zh': {'title': 'DnD-Transformer：突破图像生成的自回归新视角', 'desc': '这篇论文提出了一种新的模型架构，称为二维自回归（DnD）Transformer，以解决矢量量化自回归图像生成中的信息损失瓶颈。DnD-Transformer通过引入新的自回归方向和模型深度，预测图像的更多编码。与传统的一维自回归和类似的二维图像分解方法相比，DnD-Transformer能够在相同的模型大小和序列长度下生成更高质量的图像。实验表明，DnD-Transformer不仅能生成自然图像，还能在自监督的情况下生成包含丰富文本和图形元素的图像，展示了对这些组合模态的理解。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05193', 'title': 'RevisEval: Improving LLM-as-a-Judge via Response-Adapted References', 'url': 'https://huggingface.co/papers/2410.05193', 'abstract': "With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing the text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance.", 'score': 12, 'issue_id': 25, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'dcb3bbd44f2e2a94', 'data': {'categories': ['#inference', '#ethics', '#optimization', '#interpretability', '#benchmark', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'RevisEval: Революция в оценке качества генерации текста', 'desc': 'RevisEval - это новая парадигма оценки генерации текста, использующая адаптированные под ответ эталоны. Метод задействует возможности больших языковых моделей для пересмотра ответа и создания релевантного эталона. Эксперименты показывают, что RevisEval превосходит традиционные методы оценки без эталона и с эталоном, использующие LLM в качестве судьи. Более того, адаптированные эталоны улучшают классические метрики текста, такие как BLEU и BERTScore.'}, 'en': {'title': 'RevisEval: Revolutionizing Text Evaluation with Adaptive References', 'desc': "The paper introduces RevisEval, a new method for evaluating text generation by using response-adapted references. This approach leverages large language models to revise responses, creating more relevant references for evaluation. RevisEval has been shown to outperform traditional evaluation methods, improving metrics like BLEU and BERTScore. The study also highlights RevisEval's ability to reduce bias and enhance evaluation reliability."}, 'zh': {'title': 'RevisEval：通过响应适应的参考提升文本评估', 'desc': '这篇论文介绍了一种新的文本生成评估方法，称为RevisEval，它通过响应适应的参考来提高评估的准确性。RevisEval利用大型语言模型的文本修订能力，将生成的文本进行适应性修订，然后将修订后的文本作为参考进行评估。实验表明，RevisEval在自然语言生成任务中优于传统的无参考和基于参考的评估方法。更重要的是，RevisEval的参考可以提升经典文本指标的表现，如BLEU和BERTScore，并且在某些情况下甚至可以媲美LLM-as-a-Judge。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03864', 'title': 'DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search', 'url': 'https://huggingface.co/papers/2410.03864', 'abstract': 'Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called "reasoning actions"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason dynamically via optimal reasoning trajectory search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.', 'score': 10, 'issue_id': 34, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'c9a263348ca9157d', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'DOTS: Динамическое рассуждение для языковых моделей через оптимальный поиск траекторий', 'desc': 'Статья представляет DOTS - подход, позволяющий языковым моделям динамически рассуждать путем поиска оптимальной траектории рассуждений. Метод включает определение атомарных модулей действий рассуждения, поиск оптимальной траектории действий для каждого вопроса и обучение языковой модели планированию траекторий рассуждений для новых вопросов. Эксперименты показывают, что DOTS превосходит статические методы рассуждения и стандартный подход с инструкциями. Анализ выявляет, что метод позволяет языковым моделям адаптировать вычисления в зависимости от сложности задачи.'}, 'en': {'title': "Dynamic Reasoning: Tailoring AI's Thought Process for Better Problem Solving", 'desc': "This paper introduces DOTS, a method to enhance large language models' reasoning by dynamically tailoring reasoning strategies to each question. Unlike previous static approaches, DOTS searches for the best reasoning path for each question, using atomic reasoning modules. The method involves training a language model to plan reasoning paths for new questions, either by using an external planner or by enhancing the model's internal planning capabilities. Experiments show that DOTS improves performance across various reasoning tasks by allowing models to allocate more resources to complex problems."}, 'zh': {'title': 'DOTS：动态推理路径搜索，提升语言模型推理能力', 'desc': '这篇论文提出了一种名为DOTS的方法，旨在提升大型语言模型（LLM）的推理能力。与以往使用静态推理策略不同，DOTS通过动态搜索最佳推理路径，根据每个问题的特性和LLM的能力进行调整。该方法包括定义基本推理动作模块、为每个问题寻找最佳推理路径，以及利用这些路径训练LLM以应对新问题。实验结果表明，DOTS在多个推理任务中表现优于传统方法，能够根据问题难度调整计算深度。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04343', 'title': 'Inference Scaling for Long-Context Retrieval Augmented Generation', 'url': 'https://huggingface.co/papers/2410.04343', 'abstract': "The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.", 'score': 9, 'issue_id': 48, 'pub_date': '2024-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '1f6403a22be1233e', 'data': {'categories': ['#reasoning', '#long_context', '#rag', '#inference', '#optimization', '#benchmark'], 'emoji': '🚀', 'ru': {'title': 'Оптимизация вычислений для RAG: больше - не всегда лучше, важно как', 'desc': 'Статья исследует масштабирование вычислений для вывода в контексте генерации с использованием извлечения информации (RAG) для больших языковых моделей. Авторы изучают две стратегии: обучение в контексте и итеративный промптинг. Они обнаруживают, что оптимальное распределение вычислительных ресурсов приводит к почти линейному росту производительности RAG. На основе этих наблюдений разрабатывается модель для предсказания оптимальных параметров вывода при различных вычислительных ограничениях.'}, 'en': {'title': 'Unlocking Performance: Optimal Inference Scaling for RAG in LLMs', 'desc': 'This paper explores how to improve the performance of retrieval augmented generation (RAG) in large language models (LLMs) by scaling inference computation. It highlights that simply increasing the amount of external knowledge does not always lead to better results unless it is effectively utilized. The authors investigate two strategies: in-context learning and iterative prompting, which allow for more flexible use of computational resources during inference. Their findings show that with optimal allocation of inference resources, RAG performance can significantly improve, demonstrating a nearly linear relationship between increased computation and performance gains.'}, 'zh': {'title': '推理计算扩展，提升RAG性能！', 'desc': '本文探讨了推理计算的扩展如何提升检索增强生成（RAG）模型的性能。我们提出了两种推理扩展策略：上下文学习和迭代提示，这些策略可以灵活地增加测试时的计算量。研究表明，合理配置推理计算时，RAG的性能几乎呈线性提升。我们还开发了计算分配模型，以预测在不同计算约束下的最佳推理参数，从而优化RAG的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04081', 'title': '$ε$-VAE: Denoising as Visual Decoding', 'url': 'https://huggingface.co/papers/2410.04081', 'abstract': 'In generative modeling, tokenization simplifies complex data into compact, structured representations, creating a more efficient, learnable space. For high-dimensional visual data, it reduces redundancy and emphasizes key features for high-quality generation. Current visual tokenization methods rely on a traditional autoencoder framework, where the encoder compresses data into latent representations, and the decoder reconstructs the original input. In this work, we offer a new perspective by proposing denoising as decoding, shifting from single-step reconstruction to iterative refinement. Specifically, we replace the decoder with a diffusion process that iteratively refines noise to recover the original image, guided by the latents provided by the encoder. We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID), comparing it to state-of-the-art autoencoding approach. We hope this work offers new insights into integrating iterative generation and autoencoding for improved compression and generation.', 'score': 7, 'issue_id': 48, 'pub_date': '2024-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '2006fe3703285810', 'data': {'categories': ['#diffusion', '#optimization', '#architecture', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Диффузия как декодер: новый взгляд на визуальную токенизацию', 'desc': 'Статья предлагает новый подход к визуальной токенизации в генеративном моделировании. Вместо традиционной структуры автоэнкодера, авторы используют процесс диффузии в качестве декодера. Этот метод позволяет итеративно улучшать изображение, начиная с шума и руководствуясь латентными представлениями от энкодера. Исследователи оценивают качество реконструкции и генерации, сравнивая свой подход с современными методами автокодирования.'}, 'en': {'title': 'Iterative Denoising: A New Era in Generative Modeling', 'desc': 'This paper presents a novel approach to generative modeling by introducing a denoising process as a method of decoding. Instead of the traditional single-step reconstruction used in autoencoders, the authors propose an iterative refinement technique that utilizes a diffusion process to enhance image recovery. By leveraging latent representations from the encoder, the model progressively reduces noise to generate high-quality images. The effectiveness of this method is evaluated through metrics that assess both reconstruction fidelity and generation quality, showing potential improvements over existing autoencoding techniques.'}, 'zh': {'title': '迭代去噪，提升生成质量', 'desc': '在生成建模中，标记化将复杂数据简化为紧凑的结构化表示，从而创建更高效的可学习空间。对于高维视觉数据，它减少了冗余并强调关键特征，以实现高质量的生成。当前的视觉标记化方法依赖于传统的自编码器框架，其中编码器将数据压缩为潜在表示，解码器重建原始输入。我们提出了一种新的视角，通过将去噪作为解码，转变为迭代精炼的过程，以提高压缩和生成的效果。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02705', 'title': 'ControlAR: Controllable Image Generation with Autoregressive Models', 'url': 'https://huggingface.co/papers/2410.02705', 'abstract': "Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls short in generation quality compared to ControlNet and suffers from inefficiency. To this end, we introduce ControlAR, an efficient and effective framework for integrating spatial controls into autoregressive image generation models. Firstly, we explore control encoding for AR models and propose a lightweight control encoder to transform spatial inputs (e.g., canny edges or depth maps) into control tokens. Then ControlAR exploits the conditional decoding method to generate the next image token conditioned on the per-token fusion between control and image tokens, similar to positional encodings. Compared to prefilling tokens, using conditional decoding significantly strengthens the control capability of AR models but also maintains the model's efficiency. Furthermore, the proposed ControlAR surprisingly empowers AR models with arbitrary-resolution image generation via conditional decoding and specific controls. Extensive experiments can demonstrate the controllability of the proposed ControlAR for the autoregressive control-to-image generation across diverse inputs, including edges, depths, and segmentation masks. Furthermore, both quantitative and qualitative results indicate that ControlAR surpasses previous state-of-the-art controllable diffusion models, e.g., ControlNet++. Code, models, and demo will soon be available at https://github.com/hustvl/ControlAR.", 'score': 7, 'issue_id': 28, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '455b112b5c885449', 'data': {'categories': ['#cv', '#benchmark', '#games', '#open_source', '#diffusion', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'ControlAR: Революция в управляемой генерации изображений', 'desc': 'ControlAR - это новый эффективный метод для интеграции пространственного контроля в авторегрессионные модели генерации изображений. Он использует легковесный энкодер для преобразования пространственных входных данных в контрольные токены. ControlAR применяет условное декодирование для генерации следующего токена изображения на основе послойного слияния контрольных и изображенческих токенов. Этот подход позволяет генерировать изображения произвольного разрешения и превосходит предыдущие модели управляемой диффузии по качеству и эффективности.'}, 'en': {'title': 'ControlAR: Elevating Image Generation with Precision and Efficiency', 'desc': 'The paper introduces ControlAR, a new framework for autoregressive (AR) models that enhances control-to-image generation by integrating spatial controls. ControlAR uses a lightweight control encoder to convert spatial inputs like edges or depth maps into control tokens, which are then used in conditional decoding to improve image generation quality. This method allows AR models to generate images with better control and efficiency compared to previous methods like ControlNet. Experiments show that ControlAR not only improves controllability but also supports arbitrary-resolution image generation, outperforming existing diffusion models.'}, 'zh': {'title': 'ControlAR：自回归图像生成的新控制力', 'desc': '这篇论文介绍了一种名为ControlAR的新框架，用于在自回归图像生成模型中集成空间控制。通过引入轻量级控制编码器，将空间输入转换为控制标记，并使用条件解码方法生成图像标记。与预填充标记的方法相比，条件解码显著增强了自回归模型的控制能力，同时保持了模型的效率。实验表明，ControlAR在多种输入条件下的可控性优于现有的可控扩散模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04422', 'title': 'Hyper-multi-step: The Truth Behind Difficult Long-context Tasks', 'url': 'https://huggingface.co/papers/2410.04422', 'abstract': 'Long-context language models (LCLM), characterized by their extensive context window, is becoming increasingly popular. Meanwhile, many long-context benchmarks present challenging tasks that even the most advanced LCLMs struggle to complete. However, the underlying sources of various challenging long-context tasks have seldom been studied. To bridge this gap, we conduct experiments to indicate their difficulty stems primarily from two basic issues: "multi-matching retrieval," which requires the simultaneous retrieval of multiple items, and "logic-based retrieval," which necessitates logical judgment within retrieval criteria. These two problems, while seemingly straightforward, actually exceed the capabilities of LCLMs because they are proven to be hyper-multi-step (demanding numerous steps to solve) in nature. This finding could explain why LLMs struggle with more advanced long-context tasks, providing a more accurate perspective for rethinking solutions for them.', 'score': 7, 'issue_id': 28, 'pub_date': '2024-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '6077c1d0003654bc', 'data': {'categories': ['#reasoning', '#long_context', '#rag', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие корней сложности: почему языковые модели спотыкаются на длинном контексте', 'desc': 'Исследование посвящено языковым моделям с длинным контекстом (LCLM) и их трудностям в решении сложных задач. Авторы выявили две основные проблемы: многократное сопоставление при извлечении информации и логическое извлечение данных. Эти задачи оказываются гипермногоступенчатыми, что превышает возможности современных LCLM. Результаты исследования объясняют, почему языковые модели испытывают трудности с продвинутыми задачами на длинном контексте.'}, 'en': {'title': 'Unraveling the Complexity of Long-Context Language Models', 'desc': "The paper explores why long-context language models (LCLMs) struggle with certain complex tasks. It identifies two main challenges: 'multi-matching retrieval,' which involves retrieving multiple items at once, and 'logic-based retrieval,' which requires logical reasoning. These tasks are hyper-multi-step, meaning they need many steps to solve, which exceeds the current capabilities of LCLMs. Understanding these challenges helps in rethinking how to improve LCLMs for better performance on advanced tasks."}, 'zh': {'title': '揭示长上下文任务的隐藏挑战', 'desc': '这篇论文研究了长上下文语言模型（LCLM）在处理复杂任务时遇到的困难。研究发现，这些困难主要来自于两个基本问题：多匹配检索和基于逻辑的检索。尽管这些问题看似简单，但实际上需要超多步骤才能解决，超出了LCLM的能力范围。这一发现为重新思考解决方案提供了更准确的视角。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05076', 'title': 'TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention', 'url': 'https://huggingface.co/papers/2410.05076', 'abstract': 'Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.', 'score': 6, 'issue_id': 33, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '299609c59ee293c9', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization', '#architecture'], 'emoji': '🌊', 'ru': {'title': 'TidalDecode: Эффективное декодирование LLM с сохранением позиции разреженного внимания', 'desc': 'Статья представляет TidalDecode - алгоритм для быстрого и точного декодирования больших языковых моделей (LLM) с помощью разреженного внимания с сохранением позиции. TidalDecode использует пространственную когерентность токенов, выбранных существующими методами разреженного внимания, и вводит несколько слоев выбора токенов с полным вниманием. Это позволяет значительно снизить накладные расходы на выбор токенов для разреженного внимания без ущерба для качества генерируемых результатов. Оценка на различных LLM и задачах показывает, что TidalDecode соответствует производительности методов с полным вниманием, уменьшая задержку декодирования LLM до 2,1 раза.'}, 'en': {'title': '"TidalDecode: Streamlining LLM Decoding with Smart Sparse Attention"', 'desc': 'The paper addresses the memory constraints in large language models (LLMs) caused by the expanding key-value cache size during the decoding phase. It critiques existing sparse attention mechanisms for not effectively identifying relevant tokens and ignoring spatial coherence across Transformer layers. The authors propose TidalDecode, an algorithm that uses position persistent sparse attention to improve token selection efficiency without compromising performance. TidalDecode significantly reduces decoding latency while maintaining high-quality results, as demonstrated in various LLM tasks.'}, 'zh': {'title': 'TidalDecode：提升大语言模型解码效率的新方法', 'desc': '这篇论文介绍了一种名为TidalDecode的新算法，用于提高大语言模型的解码效率。TidalDecode通过位置持久稀疏注意力机制，解决了现有稀疏注意力方法在选择相关词元时的不足。它在少数层中使用全注意力来识别高注意力分数的词元，而其他层则使用预选词元进行稀疏注意力。实验表明，TidalDecode在保持生成质量的同时，将解码延迟减少了最多2.1倍。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03290', 'title': 'Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models', 'url': 'https://huggingface.co/papers/2410.03290', 'abstract': "Video Large Language Models (Video-LLMs) have demonstrated remarkable capabilities in coarse-grained video understanding, however, they struggle with fine-grained temporal grounding. In this paper, we introduce Grounded-VideoLLM, a novel Video-LLM adept at perceiving and reasoning over specific video moments in a fine-grained manner. We identify that current Video-LLMs have limitations for fine-grained video understanding since they lack effective temporal modeling and timestamp representation. In light of this, we sharpen our model by incorporating (1) an additional temporal stream to encode the relationships between frames and (2) discrete temporal tokens enriched with specific time knowledge to represent timestamps. To optimize the training of Grounded-VideoLLM, we employ a multi-stage training scheme, beginning with simple video-captioning tasks and progressively introducing video temporal grounding tasks of increasing complexity. To further enhance Grounded-VideoLLM's temporal reasoning capability, we also curate a grounded VideoQA dataset by an automatic annotation pipeline. Extensive experiments demonstrate that Grounded-VideoLLM not only excels in fine-grained grounding tasks such as temporal sentence grounding, dense video captioning, and grounded VideoQA, but also shows great potential as a versatile video assistant for general video understanding.", 'score': 6, 'issue_id': 29, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '6bcaebbfbd863fb6', 'data': {'categories': ['#reasoning', '#video', '#dataset', '#training', '#graphs', '#optimization', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Грануляция времени: новый уровень понимания видео для ИИ', 'desc': 'Grounded-VideoLLM - это новая модель Video-LLM, способная к детальному пониманию видео и временной локализации. Модель использует дополнительный временной поток для кодирования связей между кадрами и дискретные временные токены для представления временных меток. Обучение проводится поэтапно, начиная с простых задач описания видео и постепенно усложняясь до задач временной локализации. Эксперименты показывают эффективность Grounded-VideoLLM в задачах временной локализации предложений, плотного описания видео и ответов на вопросы с привязкой ко времени.'}, 'en': {'title': "Mastering Time: Grounded-VideoLLM's Leap in Video Understanding", 'desc': "The paper introduces Grounded-VideoLLM, a new model designed to improve fine-grained temporal understanding in videos. It addresses the limitations of existing Video-LLMs by adding a temporal stream to better capture frame relationships and using discrete temporal tokens for precise timestamp representation. The model is trained using a multi-stage approach, starting with simple tasks and gradually tackling more complex temporal grounding challenges. Additionally, a specialized dataset is created to enhance the model's temporal reasoning, resulting in superior performance in tasks like temporal sentence grounding and dense video captioning."}, 'zh': {'title': '细粒度视频理解的新突破', 'desc': '现有的视频大语言模型在理解视频的细节时存在困难，因为它们缺乏有效的时间建模和时间戳表示。为了解决这个问题，我们提出了一种新模型，Grounded-VideoLLM，通过增加时间流和离散时间标记来增强时间感知能力。我们采用多阶段训练策略，从简单的视频字幕任务开始，逐步引入复杂的视频时间定位任务。实验表明，Grounded-VideoLLM在细粒度任务中表现出色，并有潜力成为通用的视频助手。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02743', 'title': 'MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions', 'url': 'https://huggingface.co/papers/2410.02743', 'abstract': 'Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to successful outcomes. This hinders learning efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions -- sequences of tokens or higher-level language constructs -- into the learning process. By operating at this higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30% in text summarization and code generation, 18% in dialogue, and 8% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in terms of training time and continues to outperform it with further training. We will make our code and data publicly available at https://github.com/ernie-research/MA-RLHF .', 'score': 5, 'issue_id': 26, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '2db1c25c0ecd97fd', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#rl', '#plp', '#alignment', '#open_source', '#rlhf'], 'emoji': '🚀', 'ru': {'title': 'Ускоренное обучение языковых моделей с помощью макродействий', 'desc': 'Статья представляет новый подход к обучению с подкреплением на основе обратной связи от человека (RLHF) для больших языковых моделей. Предложенный метод MA-RLHF использует макродействия - последовательности токенов или высокоуровневые языковые конструкции - для улучшения процесса обучения. Это позволяет решить проблему назначения кредита для длинных последовательностей и повысить эффективность обучения. Эксперименты показывают значительное улучшение производительности по сравнению со стандартным RLHF на различных задачах обработки естественного языка.'}, 'en': {'title': '"Boosting Learning Efficiency with Macro Actions in RLHF"', 'desc': 'The paper introduces MA-RLHF, a new framework for reinforcement learning from human feedback that uses macro actions to improve learning efficiency. By focusing on sequences of tokens or higher-level constructs, the approach addresses the credit assignment problem, making it easier to link actions to rewards. This method enhances the stability of policy gradient estimates and speeds up training without adding computational complexity. Experiments show significant performance improvements in tasks like text summarization and dialogue generation compared to traditional RLHF methods.'}, 'zh': {'title': '通过宏观动作提升强化学习效率', 'desc': '这篇论文介绍了一种新的强化学习方法，称为MA-RLHF，通过引入宏观动作来提高学习效率。宏观动作是指一系列的词或更高层次的语言结构，这样可以缩短动作与奖励之间的时间距离。通过这种方法，模型能够更快、更准确地进行信用分配，从而提高学习效率。实验结果表明，这种方法在文本摘要、对话生成、问答和程序合成等任务中表现优异，训练时间也大大缩短。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03399', 'title': 'EBES: Easy Benchmarking for Event Sequences', 'url': 'https://huggingface.co/papers/2410.03399', 'abstract': 'Event sequences, characterized by irregular sampling intervals and a mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standardized benchmarks for evaluating their performance on event sequences. This complicates result comparison across different papers due to varying evaluation protocols, potentially misleading progress in this field. We introduce EBES, a comprehensive benchmarking tool with standardized evaluation scenarios and protocols, focusing on regression and classification problems with sequence-level targets. Our library simplifies benchmarking, dataset addition, and method integration through a unified interface. It includes a novel synthetic dataset and provides preprocessed real-world datasets, including the largest publicly available banking dataset. Our results provide an in-depth analysis of datasets, identifying some as unsuitable for model comparison. We investigate the importance of modeling temporal and sequential components, as well as the robustness and scaling properties of the models. These findings highlight potential directions for future research. Our benchmark aim is to facilitate reproducible research, expediting progress and increasing real-world impacts.', 'score': 4, 'issue_id': 30, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'c88e217d2ab5a78c', 'data': {'categories': ['#survey', '#dataset', '#healthcare', '#graphs', '#data', '#optimization', '#benchmark', '#open_source', '#synthetic'], 'emoji': '📊', 'ru': {'title': 'EBES: Стандартизация оценки моделей для последовательностей событий', 'desc': 'Статья представляет EBES - инструмент для стандартизированного бенчмаркинга моделей, работающих с последовательностями событий. EBES фокусируется на задачах регрессии и классификации с целевыми переменными на уровне последовательности. Инструмент включает синтетический датасет и предобработанные реальные данные, в том числе крупнейший публично доступный банковский датасет. Авторы анализируют важность моделирования временных и последовательных компонентов, а также исследуют робастность и масштабируемость моделей.'}, 'en': {'title': 'Standardizing Event Sequence Evaluation with EBES', 'desc': 'The paper introduces EBES, a benchmarking tool designed to standardize the evaluation of machine learning models on event sequences, which are common in fields like healthcare and finance. EBES provides a unified interface for benchmarking, adding datasets, and integrating methods, making it easier to compare results across different studies. It includes both synthetic and real-world datasets, offering insights into which datasets are suitable for model comparison. The tool aims to promote reproducible research and accelerate progress in understanding the temporal and sequential aspects of event sequence data.'}, 'zh': {'title': 'EBES：事件序列数据评估的标准化工具', 'desc': '这篇论文介绍了一种名为EBES的工具，用于标准化事件序列数据的评估。事件序列数据在医疗、金融等领域很常见，但缺乏统一的评估标准。EBES提供了一个统一的接口，简化了基准测试和方法集成。研究结果强调了时间和序列建模的重要性，并为未来研究指明了方向。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05993', 'title': 'Aria: An Open Multimodal Native Mixture-of-Experts Model', 'url': 'https://huggingface.co/papers/2410.05993', 'abstract': 'Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.', 'score': 107, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'a04f813494758df0', 'data': {'categories': ['#long_context', '#training', '#open_source', '#small_models', '#architecture', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Aria: открытая мультимодальная модель ИИ с производительностью мирового класса', 'desc': 'Статья представляет Aria - открытую мультимодальную модель искусственного интеллекта. Модель использует архитектуру смеси экспертов и имеет 3,9 млрд и 3,5 млрд активируемых параметров для визуальных и текстовых токенов соответственно. Aria превосходит Pixtral-12B и Llama3.2-11B, конкурируя с лучшими проприетарными моделями в различных мультимодальных задачах. Модель обучается по 4-этапному конвейеру, постепенно приобретая сильные способности в понимании языка, мультимодальном понимании, работе с длинным контекстом и следовании инструкциям.'}, 'en': {'title': 'Aria: Open-Source Multimodal Mastery', 'desc': 'The paper introduces Aria, an open-source multimodal native AI model designed to integrate diverse types of information for a comprehensive understanding of real-world data. Aria is a mixture-of-expert model with billions of parameters, enabling it to excel in tasks involving language, multimodal data, and coding. It surpasses existing models like Pixtral-12B and Llama3.2-11B in performance and is competitive with top proprietary models. The model is pre-trained using a four-stage pipeline to enhance its capabilities, and its open-source nature allows for easy adoption and adaptation in various applications.'}, 'zh': {'title': 'Aria：开源多模态模型的卓越表现', 'desc': 'Aria是一个开源的多模态原生模型，旨在整合多种信息来源以提供全面的理解。它在视觉和文本任务中表现出色，超过了许多专有模型。Aria通过四阶段的预训练流程，从零开始逐步增强其语言理解和多模态理解能力。我们开放了Aria的模型权重和代码库，方便在实际应用中进行采用和适应。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05254', 'title': 'GLEE: A Unified Framework and Benchmark for Language-based Economic Environments', 'url': 'https://huggingface.co/papers/2410.05254', 'abstract': "Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic human behavior? Do they tend to reach an efficient and fair outcome? What is the role of natural language in the strategic interaction? How do characteristics of the economic environment influence these dynamics? These questions become crucial concerning the economic and societal implications of integrating LLM-based agents into real-world data-driven systems, such as online retail platforms and recommender systems. While the ML community has been exploring the potential of LLMs in such multi-agent setups, varying assumptions, design choices and evaluation criteria across studies make it difficult to draw robust and meaningful conclusions. To address this, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of LLM vs. LLM interactions across numerous game configurations and an additional dataset of human vs. LLM interactions. Through extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of LLM-based agents to human players in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents.", 'score': 80, 'issue_id': 41, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'ec706b594e8a3b49', 'data': {'categories': ['#dataset', '#agi', '#rl', '#ethics', '#agents', '#benchmark', '#games', '#open_source', '#multimodal'], 'emoji': '🎲', 'ru': {'title': 'LLM в экономических играх: новый бенчмарк для оценки рациональности и эффективности', 'desc': 'Статья представляет новый бенчмарк для исследования поведения больших языковых моделей (LLM) в экономических и стратегических взаимодействиях. Авторы разработали фреймворк для симуляции и анализа игр между двумя участниками, основанных на естественном языке. Исследование включает сравнение поведения LLM-агентов с человеческими игроками в различных экономических контекстах. Результаты позволяют оценить индивидуальную и коллективную эффективность агентов, а также влияние экономических характеристик среды на их поведение.'}, 'en': {'title': 'Standardizing LLMs in Economic Games: A New Benchmark for Fair Play', 'desc': 'This paper explores the behavior of Large Language Models (LLMs) in economic and strategic interactions, focusing on their rationality, mimicry of human behavior, and ability to achieve fair outcomes. The authors introduce a benchmark for standardizing research on two-player, sequential, language-based games, inspired by economic literature. They develop an open-source framework to simulate interactions and collect datasets of LLM vs. LLM and human vs. LLM interactions. The study aims to compare LLM behavior to human players, evaluate performance, and understand the impact of economic environments on agent behavior.'}, 'zh': {'title': '标准化LLM在经济互动中的表现评估', 'desc': '这篇论文探讨了大型语言模型（LLM）在经济和战略互动中的表现，尤其是在自然语言交流中。研究者们提出了一个基准，用于标准化两人顺序语言游戏的研究，以便更好地评估LLM的表现。通过开发开源框架和收集数据集，研究者能够比较LLM与人类玩家在不同经济环境中的行为。实验结果显示，这种框架可以有效评估LLM在个体和集体表现上的表现，并量化经济环境特征对代理行为的影响。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07113', 'title': 'Personalized Visual Instruction Tuning', 'url': 'https://huggingface.co/papers/2410.07113', 'abstract': 'Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as "face blindness". Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeting at specific individuals. This deficiency hinders the application of MLLMs in personalized settings, such as tailored visual assistants on mobile devices, or domestic robots that need to recognize members of the family. In this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel data curation and training framework designed to enable MLLMs to identify target individuals within an image and engage in personalized and coherent dialogues. Our approach involves the development of a sophisticated pipeline that autonomously generates training data containing personalized conversations. This pipeline leverages the capabilities of various visual experts, image generation models, and (multi-modal) large language models. To evaluate the personalized potential of MLLMs, we present a benchmark called P-Bench, which encompasses various question types with different levels of difficulty. The experiments demonstrate a substantial personalized performance enhancement after fine-tuning with our curated dataset.', 'score': 69, 'issue_id': 39, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'c923f3711f603f2c', 'data': {'categories': ['#cv', '#training', '#data', '#interpretability', '#optimization', '#benchmark', '#synthetic', '#multimodal'], 'emoji': '👤', 'ru': {'title': "Преодоление 'слепоты' мультимодальных моделей: персонализированное общение с изображениями", 'desc': 'Статья представляет новый подход к обучению мультимодальных языковых моделей (MLLM) для персонализированного общения. Авторы предлагают метод PVIT (Personalized Visual Instruction Tuning), который позволяет моделям идентифицировать конкретных людей на изображениях и вести персонализированный диалог. Для оценки эффективности метода разработан бенчмарк P-Bench с различными типами вопросов. Эксперименты показывают значительное улучшение персонализированной производительности после файнтюнинга на специально подготовленном датасете.'}, 'en': {'title': "Breaking the 'Face Blindness' Barrier in Multimodal Models", 'desc': "The paper addresses the limitation of multimodal large language models (MLLMs) in recognizing and engaging with specific individuals, a problem termed as 'face blindness'. To overcome this, the authors propose Personalized Visual Instruction Tuning (PVIT), a framework that curates and trains MLLMs to identify individuals in images and conduct personalized dialogues. The framework uses a pipeline that autonomously generates training data by integrating visual experts, image generation models, and large language models. The effectiveness of this approach is validated through a benchmark called P-Bench, showing improved personalized performance after fine-tuning."}, 'zh': {'title': '打破“面盲症”：个性化视觉对话新突破', 'desc': '近年来，多模态大语言模型（MLLMs）取得了显著进展，但存在一个显著的局限，即“面盲症”，无法进行针对特定个体的个性化对话。为了解决这个问题，本文提出了一种名为个性化视觉指令调优（PVIT）的新框架，旨在使MLLMs能够识别图像中的目标个体并进行个性化对话。我们开发了一条复杂的流水线，自动生成包含个性化对话的训练数据，并利用视觉专家、图像生成模型和多模态大语言模型的能力。实验结果表明，经过我们精心策划的数据集微调后，MLLMs的个性化性能显著提升。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07073', 'title': 'Pixtral 12B', 'url': 'https://huggingface.co/papers/2410.07073', 'abstract': 'We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license.', 'score': 59, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '336b26133f29e630', 'data': {'categories': ['#survey', '#long_context', '#benchmark', '#open_source', '#small_models', '#architecture', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Pixtral-12B: Революция в мультимодальном искусственном интеллекте', 'desc': 'Pixtral-12B - это мультимодальная языковая модель с 12 миллиардами параметров, способная понимать как изображения, так и текст. Она превосходит более крупные модели по различным мультимодальным показателям, при этом не уступая в задачах обработки естественного языка. Pixtral использует новый энкодер изображений, обученный с нуля, что позволяет обрабатывать изображения в их естественном разрешении и соотношении сторон. Модель может обрабатывать любое количество изображений в контекстном окне из 128 тысяч токенов.'}, 'en': {'title': '"Pixtral-12B: Small but Mighty in Multimodal Mastery!"', 'desc': 'Pixtral-12B is a powerful multimodal language model with 12 billion parameters, designed to understand both images and text effectively. It achieves top performance on various benchmarks, outperforming larger models while maintaining strong natural language capabilities. The model features a new vision encoder that processes images at their natural resolution, offering flexibility in token usage and handling up to 128K tokens in its context window. Additionally, Pixtral-12B introduces an open-source benchmark for evaluating vision-language models, providing tools for standardized assessments.'}, 'zh': {'title': 'Pixtral-12B：小体积，大能量的多模态语言模型', 'desc': 'Pixtral-12B 是一个拥有 120 亿参数的多模态语言模型，能够理解自然图像和文档，并在多种多模态基准测试中表现出色。与许多开源模型不同，Pixtral 在不牺牲自然语言性能的情况下，在多模态任务中也表现优异。它使用全新训练的视觉编码器，可以以自然分辨率和纵横比处理图像，灵活使用 128K 令牌的长上下文窗口处理任意数量的图像。Pixtral-12B 的性能超过了许多同类大小的开源模型，并且在体积上比一些更大的模型小 7 倍。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05363', 'title': 'Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation', 'url': 'https://huggingface.co/papers/2410.05363', 'abstract': "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive Physics Generation Benchmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/OpenGVLab/PhyGenBench", 'score': 44, 'issue_id': 37, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '17a92a30d25bd139', 'data': {'categories': ['#science', '#reasoning', '#video', '#cv', '#training', '#agi', '#diffusion', '#open_source', '#graphs', '#interpretability', '#optimization', '#benchmark', '#games', '#alignment', '#rlhf', '#architecture', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'PhyGenBench: оценка физического здравого смысла в моделях текст-в-видео', 'desc': 'Статья представляет PhyGenBench - комплексный бенчмарк для оценки корректности физического здравого смысла в моделях текст-в-видео (T2V). Бенчмарк включает 160 промптов, охватывающих 27 физических законов в четырех фундаментальных областях. Авторы также предлагают PhyGenEval - новую структуру оценки, использующую передовые модели компьютерного зрения и большие языковые модели. Результаты показывают, что текущие T2V модели испытывают трудности с генерацией видео, соответствующих физическому здравому смыслу.'}, 'en': {'title': 'Building Smarter Simulations: Teaching AI Intuitive Physics', 'desc': "The paper introduces PhyGenBench, a benchmark designed to evaluate the ability of text-to-video models to understand and represent intuitive physics. It includes 160 prompts based on 27 physical laws to test models' grasp of physical commonsense. Alongside, PhyGenEval is proposed as a framework for assessing these models using advanced vision-language and large language models. The study finds that current models struggle with physical commonsense, suggesting that improvements are needed beyond just scaling or prompt engineering."}, 'zh': {'title': '超越娱乐：提升模型的物理常识理解', 'desc': '这篇论文介绍了一种名为PhyGenBench的物理生成基准，用于评估文本到视频模型在物理常识方面的表现。研究发现，现有模型在生成符合物理常识的视频方面存在困难，简单地扩大模型规模或使用提示工程技术并不能完全解决这些问题。为了更好地评估模型的物理常识理解能力，作者还提出了一种新的评估框架PhyGenEval。通过这些工具，研究人员希望推动社区在模型中优先学习物理常识，而不仅仅是娱乐应用。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07171', 'title': 'IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2410.07171', 'abstract': 'Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: https://github.com/YangLing0818/IterComp', 'score': 41, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'aca07d3bd7236f38', 'data': {'categories': ['#dataset', '#cv', '#training', '#diffusion', '#optimization', '#benchmark', '#open_source', '#rlhf'], 'emoji': '🧩', 'ru': {'title': 'IterComp: Объединяя силы моделей для идеальной композиции изображений', 'desc': 'IterComp - это новый фреймворк для улучшения композиционной генерации изображений по тексту. Он агрегирует предпочтения нескольких моделей диффузии и использует итеративное обучение с обратной связью. IterComp оценивает модели по трем ключевым метрикам композиции и создает набор данных для обучения моделей вознаграждения. Метод показывает превосходство над современными подходами, особенно в композиции объектов нескольких категорий и сложном семантическом выравнивании.'}, 'en': {'title': 'IterComp: Uniting Model Strengths for Superior Image Composition', 'desc': 'The paper introduces IterComp, a framework designed to improve text-to-image generation by combining the strengths of multiple diffusion models. It evaluates models based on their ability to handle attribute binding, spatial, and non-spatial relationships, creating a dataset to train reward models. IterComp uses an iterative feedback learning approach to refine both the base diffusion model and reward models, enhancing compositionality over time. The results show that IterComp outperforms existing methods in generating complex images with multiple objects and semantic alignment.'}, 'zh': {'title': 'IterComp：提升组合生成的新框架', 'desc': '本文介绍了一种名为IterComp的新框架，用于改进文本到图像生成中的组合能力。IterComp通过聚合多个模型的组合偏好，并采用迭代反馈学习方法来增强组合生成。我们评估了六个开源扩散模型的三个关键组合指标：属性绑定、空间关系和非空间关系。实验结果表明，IterComp在多类别对象组合和复杂语义对齐方面优于现有方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06885', 'title': 'F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching', 'url': 'https://huggingface.co/papers/2410.06885', 'abstract': "This paper introduces F5-TTS, a fully non-autoregressive text-to-speech system based on flow matching with Diffusion Transformer (DiT). Without requiring complex designs such as duration model, text encoder, and phoneme alignment, the text input is simply padded with filler tokens to the same length as input speech, and then the denoising is performed for speech generation, which was originally proved feasible by E2 TTS. However, the original design of E2 TTS makes it hard to follow due to its slow convergence and low robustness. To address these issues, we first model the input with ConvNeXt to refine the text representation, making it easy to align with the speech. We further propose an inference-time Sway Sampling strategy, which significantly improves our model's performance and efficiency. This sampling strategy for flow step can be easily applied to existing flow matching based models without retraining. Our design allows faster training and achieves an inference RTF of 0.15, which is greatly improved compared to state-of-the-art diffusion-based TTS models. Trained on a public 100K hours multilingual dataset, our Fairytaler Fakes Fluent and Faithful speech with Flow matching (F5-TTS) exhibits highly natural and expressive zero-shot ability, seamless code-switching capability, and speed control efficiency. Demo samples can be found at https://SWivid.github.io/F5-TTS. We release all code and checkpoints to promote community development.", 'score': 40, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '753a8935b426d722', 'data': {'categories': ['#audio', '#multilingual', '#training', '#inference', '#open_source', '#diffusion', '#architecture', '#synthetic'], 'emoji': '🗣️', 'ru': {'title': 'F5-TTS: Революция в синтезе речи без авторегрессии', 'desc': 'F5-TTS - это неавторегрессивная система синтеза речи, основанная на сопоставлении потоков с использованием Diffusion Transformer. В отличие от сложных моделей, F5-TTS просто дополняет текстовый ввод до длины речи и выполняет шумоподавление для генерации. Система использует ConvNeXt для улучшения представления текста и стратегию Sway Sampling для повышения производительности. F5-TTS демонстрирует высокую естественность речи, возможность переключения кодов и контроль скорости.'}, 'en': {'title': 'F5-TTS: Simplifying Speech Synthesis with Flow Matching and Diffusion Transformers', 'desc': 'The paper presents F5-TTS, a novel text-to-speech system that simplifies the process by using flow matching with a Diffusion Transformer, eliminating the need for complex components like duration models and phoneme alignment. By padding text inputs to match the length of speech inputs, the system performs denoising to generate speech, improving upon the slow convergence and low robustness of previous models. The introduction of ConvNeXt refines text representation, and a new Sway Sampling strategy enhances performance and efficiency, achieving faster training and inference times. F5-TTS demonstrates natural and expressive speech capabilities, including zero-shot ability and seamless code-switching, trained on a large multilingual dataset.'}, 'zh': {'title': 'F5-TTS：流畅自然的文本到语音新突破', 'desc': '这篇论文介绍了一种名为F5-TTS的全非自回归文本到语音系统，基于流匹配和扩散变压器。与传统方法不同，它不需要复杂的设计，如时长模型、文本编码器和音素对齐，而是通过填充标记来简化输入。为了提高模型的性能和效率，作者提出了一种推理时的Sway采样策略，并使用ConvNeXt来优化文本表示。该系统在多语言数据集上训练，展示了自然流畅的语音生成能力，并且代码已公开以促进社区发展。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05954', 'title': 'Pyramidal Flow Matching for Efficient Video Generative Modeling', 'url': 'https://huggingface.co/papers/2410.05954', 'abstract': 'Video generation requires modeling a vast spatiotemporal space, which demands significant computational resources and data usage. To reduce the complexity, the prevailing approaches employ a cascaded architecture to avoid direct training with full resolution. Despite reducing computational demands, the separate optimization of each sub-stage hinders knowledge sharing and sacrifices flexibility. This work introduces a unified pyramidal flow matching algorithm. It reinterprets the original denoising trajectory as a series of pyramid stages, where only the final stage operates at the full resolution, thereby enabling more efficient video generative modeling. Through our sophisticated design, the flows of different pyramid stages can be interlinked to maintain continuity. Moreover, we craft autoregressive video generation with a temporal pyramid to compress the full-resolution history. The entire framework can be optimized in an end-to-end manner and with a single unified Diffusion Transformer (DiT). Extensive experiments demonstrate that our method supports generating high-quality 5-second (up to 10-second) videos at 768p resolution and 24 FPS within 20.7k A100 GPU training hours. All code and models will be open-sourced at https://pyramid-flow.github.io.', 'score': 37, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '365906882dc00532', 'data': {'categories': ['#video', '#training', '#optimization', '#open_source', '#diffusion', '#architecture'], 'emoji': '🎞️', 'ru': {'title': 'Пирамидальное сопоставление потоков: новый подход к эффективной генерации видео', 'desc': 'Статья представляет новый алгоритм пирамидального сопоставления потоков для генерации видео. Этот метод переосмысливает траекторию шумоподавления как серию пирамидальных этапов, где только финальный этап работает в полном разрешении. Подход позволяет оптимизировать весь процесс end-to-end с использованием единого Diffusion Transformer (DiT). Эксперименты показывают, что метод способен генерировать высококачественные видео длительностью 5-10 секунд с разрешением 768p и частотой 24 кадра в секунду.'}, 'en': {'title': '"Efficient Video Generation with Pyramidal Flow Matching"', 'desc': 'This paper presents a novel approach to video generation by introducing a unified pyramidal flow matching algorithm. The method reinterprets the denoising process as a series of pyramid stages, optimizing only the final stage at full resolution to enhance efficiency. By linking flows across pyramid stages, the approach maintains continuity and supports autoregressive video generation with a temporal pyramid. The framework is optimized end-to-end using a single Diffusion Transformer, achieving high-quality video generation with reduced computational demands.'}, 'zh': {'title': '金字塔流匹配：高效视频生成的新方法', 'desc': '这篇论文提出了一种新的金字塔流匹配算法，用于更高效的视频生成建模。通过将去噪轨迹重新解释为一系列金字塔阶段，只有最后一个阶段在全分辨率下操作，从而减少计算复杂性。该方法通过时间金字塔压缩全分辨率历史，实现自回归视频生成。整个框架可以通过单一的统一扩散变压器进行端到端优化。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07167', 'title': 'Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate', 'url': 'https://huggingface.co/papers/2410.07167', 'abstract': 'We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality without the costly supervised fine-tuning stage is under-explored. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when aligning a well-trained LLM with a new modality. Due to the lack of proper metrics, the research of LVLMs in the critical pre-training stage is hindered greatly, including the training data choice, efficient module design, etc. In this paper, we propose evaluating the pre-training quality from the inter-modal distribution distance perspective and present MIR, the Modality Integration Rate, which is 1) Effective to represent the pre-training quality and show a positive relation with the benchmark performance after supervised fine-tuning. 2) Robust toward different training/evaluation data. 3) Generalize across training configurations and architecture choices. We conduct a series of pre-training experiments to explore the effectiveness of MIR and observe satisfactory results that MIR is indicative about training data selection, training strategy schedule, and model architecture design to get better pre-training results. We hope MIR could be a helpful metric for building capable LVLMs and inspire the following research about modality alignment in different areas. Our code is at: https://github.com/shikiw/Modality-Integration-Rate.', 'score': 36, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '8e1f683abf35b291', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#open_source', '#architecture', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'MIR: новый способ оценки качества мультимодальных моделей', 'desc': 'В статье представлен новый метрический показатель Modality Integration Rate (MIR) для оценки качества предварительного обучения мультимодальных моделей компьютерного зрения и обработки естественного языка (Large Vision Language Models, LVLMs). MIR позволяет эффективно оценивать качество обучения без необходимости дорогостоящей процедуры дообучения с учителем. Метрика основана на измерении межмодального расстояния распределений и демонстрирует положительную корреляцию с результатами тестирования после дообучения. MIR показывает устойчивость к различным конфигурациям обучения и архитектурам моделей.'}, 'en': {'title': '"MIR: A New Lens for Multi-Modal Model Mastery"', 'desc': 'The paper introduces the Modality Integration Rate (MIR), a new metric designed to evaluate the quality of multi-modal pre-training in Large Vision Language Models (LVLMs). Traditional metrics like loss and perplexity are not effective for assessing how well a language model aligns with new modalities during pre-training. MIR measures the inter-modal distribution distance, providing a more accurate indication of pre-training quality and its potential performance after fine-tuning. The authors demonstrate that MIR is effective, robust, and generalizable across different training setups, aiding in better data selection and model design.'}, 'zh': {'title': '模态整合率：提升多模态预训练质量的新指标', 'desc': '这篇论文介绍了一种新的指标，称为模态整合率（MIR），用于评估大型视觉语言模型（LVLMs）的多模态预训练质量。传统的预训练指标如损失和困惑度在与新模态对齐时效果不佳，而MIR可以有效表示预训练质量，并与监督微调后的基准性能呈正相关。MIR在不同的训练和评估数据上表现出稳健性，并且可以在不同的训练配置和架构选择中推广。通过一系列实验，作者发现MIR在训练数据选择、训练策略安排和模型架构设计中具有指导意义。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06373', 'title': 'Unveiling the Backbone-Optimizer Coupling Bias in Visual Representation Learning', 'url': 'https://huggingface.co/papers/2410.06373', 'abstract': 'This paper delves into the interplay between vision backbones and optimizers, unvealing an inter-dependent phenomenon termed \\textbf{backbone-optimizer coupling bias} (BOCB). We observe that canonical CNNs, such as VGG and ResNet, exhibit a marked co-dependency with SGD families, while recent architectures like ViTs and ConvNeXt share a tight coupling with the adaptive learning rate ones. We further show that BOCB can be introduced by both optimizers and certain backbone designs and may significantly impact the pre-training and downstream fine-tuning of vision models. Through in-depth empirical analysis, we summarize takeaways on recommended optimizers and insights into robust vision backbone architectures. We hope this work can inspire the community to question long-held assumptions on backbones and optimizers, stimulate further explorations, and thereby contribute to more robust vision systems. The source code and models are publicly available at https://bocb-ai.github.io/.', 'score': 34, 'issue_id': 37, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'bf22e906e7d30eea', 'data': {'categories': ['#cv', '#training', '#optimization', '#open_source', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая тайны взаимосвязи архитектур и оптимизаторов в компьютерном зрении', 'desc': "Статья исследует взаимосвязь между архитектурами нейронных сетей для компьютерного зрения и оптимизаторами, выявляя феномен 'смещения связи между архитектурой и оптимизатором' (BOCB). Авторы обнаружили, что классические CNN лучше работают с семейством SGD-оптимизаторов, а современные архитектуры, такие как ViT и ConvNeXt, тесно связаны с адаптивными оптимизаторами. Исследование показывает, что BOCB может влиять на предварительное обучение и дообучение моделей компьютерного зрения. На основе эмпирического анализа авторы дают рекомендации по выбору оптимизаторов и созданию устойчивых архитектур нейронных сетей для задач компьютерного зрения."}, 'en': {'title': 'Rethink Your Backbone: Optimizer Matters!', 'desc': "This paper explores how different vision model architectures, known as backbones, interact with various optimizers, revealing a phenomenon called backbone-optimizer coupling bias (BOCB). It finds that traditional convolutional neural networks (CNNs) like VGG and ResNet work best with stochastic gradient descent (SGD) optimizers, while newer models like Vision Transformers (ViTs) and ConvNeXt are more compatible with adaptive learning rate optimizers. The study shows that both the choice of optimizer and the design of the backbone can introduce BOCB, affecting the model's performance during pre-training and fine-tuning. The authors provide recommendations for choosing optimizers and insights into designing robust vision backbones, encouraging the community to rethink established practices."}, 'zh': {'title': '重新思考视觉骨干与优化器的耦合关系', 'desc': '这篇论文研究了视觉骨干网络和优化器之间的相互作用，提出了一个称为骨干-优化器耦合偏差（BOCB）的现象。研究发现，传统的卷积神经网络（如VGG和ResNet）与SGD优化器家族有明显的依赖关系，而新型架构如ViTs和ConvNeXt则与自适应学习率优化器紧密耦合。BOCB可以由优化器和某些骨干设计引入，并可能显著影响视觉模型的预训练和下游微调。通过深入的实证分析，论文总结了推荐的优化器和稳健的视觉骨干架构的见解。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05355', 'title': 'Falcon Mamba: The First Competitive Attention-free 7B Language Model', 'url': 'https://huggingface.co/papers/2410.05355', 'abstract': 'In this technical report, we present Falcon Mamba 7B, a new base large language model based on the novel Mamba architecture. Falcon Mamba 7B is trained on 5.8 trillion tokens with carefully selected data mixtures. As a pure Mamba-based model, Falcon Mamba 7B surpasses leading open-weight models based on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is on par with Gemma 7B and outperforms models with different architecture designs, such as RecurrentGemma 9B and RWKV-v6 Finch 7B/14B. Currently, Falcon Mamba 7B is the best-performing Mamba model in the literature at this scale, surpassing both existing Mamba and hybrid Mamba-Transformer models, according to the Open LLM Leaderboard. Due to its architecture, Falcon Mamba 7B is significantly faster at inference and requires substantially less memory for long sequence generation. Despite recent studies suggesting that hybrid Mamba-Transformer models outperform pure architecture designs, we demonstrate that even the pure Mamba design can achieve similar, or even superior results compared to the Transformer and hybrid designs. We make the weights of our implementation of Falcon Mamba 7B publicly available on https://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license.', 'score': 27, 'issue_id': 40, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'c700bbc81473edd9', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization', '#open_source', '#architecture'], 'emoji': '🦅', 'ru': {'title': 'Falcon Mamba 7B: Прорыв в эффективности языковых моделей', 'desc': 'Falcon Mamba 7B - это новая базовая языковая модель, основанная на архитектуре Mamba. Она обучена на 5,8 триллионах токенов и превосходит ведущие модели на основе трансформеров, такие как Mistral 7B и Llama3.1 8B. Falcon Mamba 7B демонстрирует лучшую производительность среди моделей Mamba своего масштаба и значительно быстрее работает при инференсе. Модель показывает, что чистая архитектура Mamba может достигать результатов, сравнимых или превосходящих гибридные модели Mamba-Transformer.'}, 'en': {'title': 'Falcon Mamba 7B: Redefining Efficiency and Performance in Language Models', 'desc': 'Falcon Mamba 7B is a new large language model that uses the innovative Mamba architecture, trained on a vast dataset of 5.8 trillion tokens. It outperforms other leading models like Mistral 7B and Llama3.1 8B, and is comparable to Gemma 7B, showcasing the strength of the pure Mamba design. The model is efficient, requiring less memory and offering faster inference for long sequences, challenging the notion that hybrid models are superior. The weights for Falcon Mamba 7B are available for public use, promoting further research and development in the field.'}, 'zh': {'title': '纯 Mamba 架构的力量：Falcon Mamba 7B 的卓越表现', 'desc': 'Falcon Mamba 7B 是一种基于全新 Mamba 架构的大型语言模型，经过 5.8 万亿个标记的训练。与其他基于 Transformer 的模型相比，它在性能上有显著提升，并且在推理速度和内存使用上更具优势。尽管混合架构通常被认为更优，但 Falcon Mamba 7B 证明了纯 Mamba 架构也能达到甚至超越混合架构的效果。该模型的权重已在 Hugging Face 上公开，供研究人员使用。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07177', 'title': 'MM-Ego: Towards Building Egocentric Multimodal LLMs', 'url': 'https://huggingface.co/papers/2410.07177', 'abstract': 'This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding. To achieve this goal, we work on three fronts. First, as there is a lack of QA data for egocentric video understanding, we develop a data engine that efficiently generates 7M high-quality QA samples for egocentric videos ranging from 30 seconds to one hour long, based on human-annotated data. This is currently the largest egocentric QA dataset. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models\' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we propose a specialized multimodal architecture featuring a novel "Memory Pointer Prompting" mechanism. This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding.', 'score': 20, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '46a93631041dba07', 'data': {'categories': ['#video', '#survey', '#dataset', '#cv', '#long_context', '#ethics', '#benchmark', '#architecture', '#synthetic', '#multimodal'], 'emoji': '👀', 'ru': {'title': 'MM-Ego: Мультимодальная модель для глубокого понимания эгоцентрического видео', 'desc': "Исследование направлено на создание мультимодальной модели для понимания эгоцентрического видео. Авторы разработали генератор данных, создающий 7 миллионов высококачественных пар вопрос-ответ для эгоцентрических видео различной длительности. Они также представили новый бенчмарк для оценки способности моделей распознавать и запоминать визуальные детали в видео разной длины. Предложенная авторами архитектура включает механизм 'Memory Pointer Prompting', который позволяет модели эффективнее понимать длинные видео."}, 'en': {'title': 'Unlocking the Secrets of Egocentric Videos with MM-Ego', 'desc': 'This paper presents the development of a multimodal foundation model called MM-Ego for understanding egocentric videos. The researchers created a large dataset of 7 million QA samples to address the lack of data in this area, and introduced a challenging benchmark to test the model\'s ability to recognize and remember visual details. They also developed a new evaluation method to reduce language bias and proposed a novel architecture with a "Memory Pointer Prompting" mechanism to improve video comprehension. The result is a powerful model that enhances the understanding of long and complex egocentric video content.'}, 'zh': {'title': 'MM-Ego：自我中心视频理解的多模态基础模型', 'desc': '这项研究旨在构建一个多模态基础模型，用于理解以自我为中心的视频。首先，我们开发了一个数据引擎，生成了700万高质量的问答样本，这是目前最大的自我中心问答数据集。其次，我们提供了一个具有挑战性的基准测试，包含629个视频和7026个问题，并引入了一种新的去偏评估方法。最后，我们提出了一种专门的多模态架构，包含“记忆指针提示”机制，以更好地理解长视频内容。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06244', 'title': 'Story-Adapter: A Training-free Iterative Framework for Long Story Visualization', 'url': 'https://huggingface.co/papers/2410.06244', 'abstract': 'Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring computational feasibility remain challenging, especially in long story visualization (i.e., up to 100 frames). In this work, we propose a training-free and computationally efficient framework, termed Story-Adapter, to enhance the generative capability of long stories. Specifically, we propose an iterative paradigm to refine each generated image, leveraging both the text prompt and all generated images from the previous iteration. Central to our framework is a training-free global reference cross-attention module, which aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings. This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, resulting in more precise and fine-grained interactions. Extensive experiments validate the superiority of Story-Adapter in improving both semantic consistency and generative capability for fine-grained interactions, particularly in long story scenarios. The project page and associated code can be accessed via https://jwmao1.github.io/storyadapter .', 'score': 19, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '8289ac05d8c302b2', 'data': {'categories': ['#cv', '#long_context', '#training', '#open_source', '#diffusion', '#architecture', '#story_generation'], 'emoji': '🎞️', 'ru': {'title': 'Story-Adapter: Новый подход к визуализации длинных историй', 'desc': 'В статье представлен фреймворк Story-Adapter для улучшения генерации изображений на основе длинных историй. Он использует итеративный подход с глобальным модулем кросс-внимания для сохранения семантической согласованности. Story-Adapter не требует дополнительного обучения и эффективен с вычислительной точки зрения. Эксперименты подтверждают превосходство метода в улучшении согласованности и детализации генерируемых изображений, особенно для длинных историй.'}, 'en': {'title': 'Enhancing Storytelling with Smarter Image Generation', 'desc': 'The paper introduces Story-Adapter, a framework designed to improve the generation of coherent images from long narratives using text-to-image models. It addresses challenges like maintaining semantic consistency and generating detailed interactions without extensive computational demands. The framework uses an iterative process with a global reference cross-attention module to refine images by incorporating text prompts and previously generated images. Experiments show that Story-Adapter enhances both the consistency and quality of images in long story visualizations.'}, 'zh': {'title': 'Story-Adapter：提升长篇故事图像生成的新方法', 'desc': '这篇论文介绍了一种名为Story-Adapter的框架，用于提升长篇故事的图像生成能力。该框架无需训练，且计算效率高，通过迭代的方式优化每一帧图像。核心技术是一个无需训练的全局参考交叉注意力模块，能够在保持语义一致性的同时降低计算成本。实验结果表明，Story-Adapter在长篇故事的语义一致性和细粒度交互生成能力上表现优异。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07170', 'title': 'One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation', 'url': 'https://huggingface.co/papers/2410.07170', 'abstract': 'Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are usually initialized at random with a uniform rank distribution across model weights. Recent works focus on weight-driven initialization or learning of adaptive ranks during training. Both approaches have only been investigated in isolation, resulting in slow convergence or a uniform rank distribution, in turn leading to sub-optimal performance. We propose to enhance LoRA by initializing the new weights in a data-driven manner by computing singular value decomposition on minibatches of activation vectors. Then, we initialize the LoRA matrices with the obtained right-singular vectors and re-distribute ranks among all weight matrices to explain the maximal amount of variance and continue the standard LoRA fine-tuning procedure. This results in our new method Explained Variance Adaptation (EVA). We apply EVA to a variety of fine-tuning tasks ranging from language generation and understanding to image classification and reinforcement learning. EVA exhibits faster convergence than competitors and attains the highest average score across a multitude of tasks per domain.', 'score': 15, 'issue_id': 39, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'd987cfa2ee91a2b5', 'data': {'categories': ['#cv', '#training', '#rl', '#optimization', '#transfer_learning'], 'emoji': '🔬', 'ru': {'title': 'EVA: Эффективная тонкая настройка фундаментальных моделей', 'desc': 'Статья представляет новый метод тонкой настройки фундаментальных моделей под названием EVA (Explained Variance Adaptation). Этот метод улучшает традиционный подход LoRA, инициализируя новые веса на основе данных с помощью сингулярного разложения. EVA перераспределяет ранги между матрицами весов для объяснения максимального количества дисперсии. Метод показывает более быструю сходимость и лучшие результаты на различных задачах, включая генерацию текста, понимание языка, классификацию изображений и обучение с подкреплением.'}, 'en': {'title': 'EVA: Boosting Model Fine-Tuning with Data-Driven Weight Initialization', 'desc': 'The paper introduces a new method called Explained Variance Adaptation (EVA) to improve the fine-tuning of foundation models. EVA enhances the low-rank adaptation (LoRA) technique by using data-driven initialization of weights through singular value decomposition. This approach allows for a more efficient distribution of ranks across weight matrices, leading to faster convergence and better performance. EVA is tested on various tasks, showing superior results compared to existing methods.'}, 'zh': {'title': '解释方差适应：提升基础模型微调效率的新方法', 'desc': '这篇论文介绍了一种改进的低秩适应方法，称为解释方差适应（EVA），用于微调基础模型。EVA通过对激活向量的小批量进行奇异值分解，以数据驱动的方式初始化新的权重矩阵。然后，使用获得的右奇异向量初始化LoRA矩阵，并重新分配所有权重矩阵的秩，以解释最大量的方差。实验表明，EVA在多种任务中比其他方法收敛更快，并取得了更高的平均分数。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06961', 'title': 'Self-Boosting Large Language Models with Synthetic Preference Data', 'url': 'https://huggingface.co/papers/2410.06961', 'abstract': 'Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvement of LLMs. We introduce SynPO, a self-boosting paradigm that leverages synthetic preference data for model alignment. SynPO employs an iterative mechanism wherein a self-prompt generator creates diverse prompts, and a response improver refines model responses progressively. This approach trains LLMs to autonomously learn the generative rewards for their own outputs and eliminates the need for large-scale annotation of prompts and human preferences. After four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements in instruction-following abilities, achieving over 22.1% win rate improvements on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the well-recognized Open LLM leaderboard.', 'score': 15, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'cbb5c59488cb0db9', 'data': {'categories': ['#training', '#alignment', '#open_source', '#rlhf', '#small_models', '#synthetic'], 'emoji': '🔄', 'ru': {'title': 'SynPO: самоусиление языковых моделей без ручной разметки', 'desc': 'В статье представлен метод SynPO для улучшения языковых моделей без использования ручной разметки. SynPO использует итеративный механизм, где генератор создает разнообразные промпты, а улучшатель ответов постепенно совершенствует ответы модели. После четырех итераций SynPO, модели Llama3-8B и Mistral-7B показали значительное улучшение в способности следовать инструкциям и общей производительности. Этот подход позволяет языковым моделям автономно обучаться генеративным наградам для своих собственных выходных данных.'}, 'en': {'title': 'Empowering LLMs with Self-Improving Synthetic Data', 'desc': "The paper introduces SynPO, a method that uses synthetic data to align Large Language Models (LLMs) with human preferences, making them more honest, harmless, and helpful. SynPO works by generating prompts and refining responses iteratively, allowing models to learn and improve without extensive human input. This self-boosting approach significantly enhances the performance of LLMs like Llama3-8B and Mistral-7B, as shown by their improved scores in various evaluations. Overall, SynPO reduces the need for large-scale human annotation while boosting the models' ability to follow instructions and perform diverse tasks."}, 'zh': {'title': 'SynPO：用合成数据自我提升语言模型', 'desc': '这篇论文介绍了一种名为SynPO的自我提升方法，用于改进大型语言模型的对齐能力。SynPO通过生成合成偏好数据，减少了对大规模人工标注的依赖。该方法使用自我提示生成器和响应改进器，逐步优化模型的输出。经过多次迭代，模型在任务执行能力上有显著提升。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05677', 'title': 'T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design', 'url': 'https://huggingface.co/papers/2410.05677', 'abstract': 'In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench, with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling.', 'score': 14, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'e30f39e021f2ee4b', 'data': {'categories': ['#video', '#dataset', '#training', '#diffusion', '#optimization', '#benchmark', '#rlhf'], 'emoji': '🎬', 'ru': {'title': 'Революция в генерации видео: T2V-Turbo-v2 превосходит конкурентов', 'desc': 'В статье представлен метод T2V-Turbo-v2 для улучшения модели преобразования текста в видео на основе диффузии. Авторы предлагают дистиллировать модель согласованности из предобученной T2V модели, используя различные сигналы обучения. Метод включает интеграцию высококачественных данных, обратную связь от модели вознаграждения и условное руководство в процесс дистилляции согласованности. Эмпирически T2V-Turbo-v2 устанавливает новый state-of-the-art результат на бенчмарке VBench с общим счетом 85.13.'}, 'en': {'title': 'Turbocharge Your Text-to-Video Models with T2V-Turbo-v2!', 'desc': "The paper presents T2V-Turbo-v2, a method to improve text-to-video models by distilling a consistency model from a pretrained version. It uses high-quality data, feedback from reward models, and conditional guidance to enhance the model's performance. The study emphasizes the importance of using tailored datasets and diverse reward models to improve visual quality and text-video alignment. The approach also explores conditional guidance strategies, particularly focusing on motion guidance, to improve the motion quality of generated videos, achieving state-of-the-art results on VBench."}, 'zh': {'title': 'T2V-Turbo-v2：提升文本到视频生成的新突破', 'desc': '这篇论文介绍了一种名为T2V-Turbo-v2的新方法，通过从预训练的文本到视频模型中提取一致性模型来增强扩散模型。该方法结合了高质量训练数据、奖励模型反馈和条件引导等多种监督信号，显著提高了视觉质量和文本视频对齐。研究表明，针对特定学习目标定制数据集和从多样化奖励模型中学习是提升模型性能的关键。通过设计有效的能量函数，改进了运动引导策略，显著提升了生成视频的运动质量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07002', 'title': 'CursorCore: Assist Programming through Aligning Anything', 'url': 'https://huggingface.co/papers/2410.07002', 'abstract': 'Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at https://github.com/TechxGenus/CursorCore.', 'score': 13, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '70a480b72654e749', 'data': {'categories': ['#dataset', '#training', '#data', '#plp', '#benchmark', '#games', '#open_source', '#small_models', '#synthetic'], 'emoji': '💻', 'ru': {'title': 'Интеллектуальный помощник программиста: новый подход к автоматизации кодирования', 'desc': 'Статья представляет новую систему для помощи в программировании, использующую большие языковые модели. Авторы разработали фреймворк, интегрирующий историю кодирования, текущий код и инструкции пользователя. Они также создали новый бенчмарк APEval для оценки моделей и pipeline для генерации обучающих данных. Результаты показывают, что их модель CursorCore превосходит аналоги сопоставимого размера.'}, 'en': {'title': 'Revolutionizing Coding Assistance with Integrated Conversational Frameworks', 'desc': 'The paper introduces a new conversational framework for programming assistance that integrates coding history, current code, and user instructions to improve automation in tasks like code completion and editing. It presents a benchmark called APEval to evaluate how well models align with different information types and the quality of their outputs. A data generation pipeline, Programming-Instruct, is developed to synthesize training data from sources like GitHub, resulting in 219K samples used to fine-tune models. The resulting CursorCore models outperform others of similar size, enhancing coding assistants with features like inline chat and automated editing.'}, 'zh': {'title': 'CursorCore：统一编程助手的未来', 'desc': '这篇论文提出了一种新的对话框架，可以全面整合编程过程中的各种信息来源，如编码历史、当前代码和用户指令。为了评估模型在编程辅助任务中的表现，研究者引入了一个新的基准测试APEval。通过一个名为Programming-Instruct的数据生成管道，研究者从GitHub等多种来源合成训练数据，并生成了219K样本。最终，开发的CursorCore系列模型在性能上优于其他同类模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05591', 'title': 'TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation', 'url': 'https://huggingface.co/papers/2410.05591', 'abstract': "Despite significant advancements in customizing text-to-image and video generation models, generating images and videos that effectively integrate multiple personalized concepts remains a challenging task. To address this, we present TweedieMix, a novel method for composing customized diffusion models during the inference phase. By analyzing the properties of reverse diffusion sampling, our approach divides the sampling process into two stages. During the initial steps, we apply a multiple object-aware sampling technique to ensure the inclusion of the desired target objects. In the later steps, we blend the appearances of the custom concepts in the de-noised image space using Tweedie's formula. Our results demonstrate that TweedieMix can generate multiple personalized concepts with higher fidelity than existing methods. Moreover, our framework can be effortlessly extended to image-to-video diffusion models, enabling the generation of videos that feature multiple personalized concepts. Results and source code are in our anonymous project page.", 'score': 13, 'issue_id': 39, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '1d4bfe307ff6d748', 'data': {'categories': ['#video', '#cv', '#inference', '#open_source', '#diffusion', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'TweedieMix: Смешивание персонализированных концепций в генерации изображений и видео', 'desc': 'TweedieMix - это новый метод для объединения персонализированных диффузионных моделей на этапе вывода. Он разделяет процесс сэмплирования на два этапа: сначала применяется техника сэмплирования с учетом нескольких объектов, а затем используется формула Твиди для смешивания внешнего вида персонализированных концепций. Метод позволяет генерировать изображения и видео с несколькими персонализированными концепциями более качественно, чем существующие подходы. TweedieMix также легко расширяется на диффузионные модели для преобразования изображений в видео.'}, 'en': {'title': 'TweedieMix: Mastering Multi-Concept Image and Video Generation', 'desc': "The paper introduces TweedieMix, a new method for generating images and videos that combine multiple personalized concepts using diffusion models. It divides the reverse diffusion sampling process into two stages: the first ensures the inclusion of target objects, and the second blends custom concepts using Tweedie's formula. This approach results in higher fidelity images and videos compared to existing methods. Additionally, TweedieMix can be easily adapted for image-to-video diffusion models, enhancing its versatility."}, 'zh': {'title': 'TweedieMix：多概念图像生成的新突破', 'desc': '这篇论文介绍了一种名为TweedieMix的新方法，用于在推理阶段组合定制的扩散模型。通过分析反向扩散采样的特性，该方法将采样过程分为两个阶段。在初始阶段，应用多对象感知采样技术以确保目标对象的包含。在后期阶段，使用Tweedie公式在去噪图像空间中融合定制概念的外观。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05651', 'title': 'ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler', 'url': 'https://huggingface.co/papers/2410.05651', 'abstract': 'Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start & end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation.', 'score': 13, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'd350fdc5aa2b2edc', 'data': {'categories': ['#video', '#inference', '#optimization', '#diffusion', '#architecture'], 'emoji': '🎞️', 'ru': {'title': 'Революционный подход к интерполяции видео с использованием двунаправленной выборки', 'desc': 'Статья представляет новую стратегию двунаправленной выборки для улучшения интерполяции ключевых кадров в моделях диффузии изображение-видео. Авторы предлагают последовательную выборку вдоль прямого и обратного путей, обусловленную начальным и конечным кадрами, что обеспечивает более согласованную генерацию промежуточных кадров. Метод включает в себя передовые техники направленной генерации, такие как CFG++ и DDS, для дальнейшего улучшения процесса интерполяции. Результаты демонстрируют высокую эффективность и качество генерации видео между ключевыми кадрами.'}, 'en': {'title': 'Smooth Transitions: Revolutionizing Video Interpolation with Bidirectional Sampling', 'desc': 'This paper introduces a new approach to improve video generation by focusing on keyframe interpolation using diffusion models. The authors propose a bidirectional sampling strategy that effectively addresses off-manifold issues without needing extensive re-noising. By conditioning on both start and end frames, the method ensures smoother and more coherent generation of intermediate frames. The integration of advanced guidance techniques, CFG++ and DDS, further enhances the quality and efficiency of the interpolation process, achieving state-of-the-art results.'}, 'zh': {'title': '双向采样：提升关键帧插值的新策略', 'desc': '这篇论文介绍了一种新的双向采样策略，用于改进图像到视频的生成模型，特别是在关键帧插值方面。传统方法在处理两个条件帧（起始和结束帧）时，常出现离开流形的问题，导致生成的中间帧出现瑕疵。新方法通过在前向和后向路径上进行顺序采样，确保生成的中间帧更加连贯和符合流形。结合先进的指导技术CFG++和DDS，该方法在关键帧插值上达到了最先进的性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02465', 'title': 'Response Tuning: Aligning Large Language Models without Instruction', 'url': 'https://huggingface.co/papers/2410.02465', 'abstract': 'Instruction tuning-supervised fine-tuning using instruction-response pairs-is a foundational step in transitioning pre-trained Large Language Models (LLMs) into helpful and safe chat assistants. Our hypothesis is that establishing an adequate output space can enable such a transition given the capabilities inherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT), which eliminates the instruction-conditioning step in instruction tuning and solely focuses on response space supervision. Our experiments demonstrate that RT models, trained only using responses, can effectively respond to a wide range of instructions and exhibit helpfulness comparable to that of their instruction-tuned counterparts. Furthermore, we observe that controlling the training response distribution can significantly improve their user preference or elicit target behaviors such as refusing assistance for unsafe queries. Our findings illuminate the role of establishing an adequate output space in alignment, highlighting the potential of the extensive inherent capabilities of pre-trained LLMs.', 'score': 12, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '0682a2364e4a8840', 'data': {'categories': ['#training', '#optimization', '#architecture', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Response Tuning: эффективное обучение языковых моделей без инструкций', 'desc': 'Исследователи предлагают метод Response Tuning (RT) для обучения языковых моделей без использования инструкций. RT фокусируется только на пространстве ответов, позволяя моделям эффективно реагировать на различные запросы. Эксперименты показывают, что модели RT могут быть такими же полезными, как и модели, обученные с инструкциями. Контроль распределения ответов при обучении может улучшить предпочтения пользователей и желаемое поведение моделей.'}, 'en': {'title': 'Response Tuning: Unlocking LLM Potential with Focused Supervision', 'desc': 'The paper explores a method called Response Tuning (RT) to improve the performance of Large Language Models (LLMs) as chat assistants. Instead of using instruction-response pairs, RT focuses solely on supervising the response space, which helps the models respond effectively to various instructions. The study shows that RT-trained models can be as helpful as those trained with traditional instruction tuning. Additionally, by controlling the response distribution during training, these models can be guided to exhibit desired behaviors, such as refusing unsafe requests.'}, 'zh': {'title': '响应调优：释放预训练模型的潜力', 'desc': '这篇论文探讨了如何将预训练的大型语言模型转变为有用且安全的聊天助手。研究提出了一种名为响应调优的方法，专注于对模型输出的监督，而不是指令调优。实验表明，仅通过响应训练的模型可以有效地处理各种指令，并且表现出与指令调优模型相当的帮助性。研究还发现，通过控制训练响应的分布，可以显著提高用户偏好，并在处理不安全查询时表现出拒绝协助的行为。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05295', 'title': 'AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs', 'url': 'https://huggingface.co/papers/2410.05295', 'abstract': 'In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that can automatically discover as many jailbreak strategies as possible from scratch, without any human intervention or predefined scopes (e.g., specified candidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo can significantly outperform baseline methods, achieving a 74.3% higher average attack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an 88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a unified framework that can incorporate existing human-designed jailbreak strategies in a plug-and-play manner. By integrating human-designed strategies, AutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on GPT-4-1106-turbo.', 'score': 12, 'issue_id': 39, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'bc2003c7b895855f', 'data': {'categories': ['#security', '#optimization', '#agents', '#benchmark', '#architecture'], 'emoji': '🔓', 'ru': {'title': 'AutoDAN-Turbo: Автоматический взлом ИИ без участия человека', 'desc': 'AutoDAN-Turbo - это метод автоматического обнаружения уязвимостей в системах искусственного интеллекта без вмешательства человека. Он значительно превосходит базовые методы, достигая на 74.3% более высокого среднего показателя успешности атак на публичных бенчмарках. На модели GPT-4-1106-turbo AutoDAN-Turbo достигает 88.5% успешности атак. Кроме того, AutoDAN-Turbo может интегрировать существующие стратегии взлома, разработанные людьми, повышая успешность атак до 93.4% на GPT-4-1106-turbo.'}, 'en': {'title': 'AutoDAN-Turbo: Unleashing Automated AI Vulnerability Discovery', 'desc': 'AutoDAN-Turbo is a machine learning framework designed to automatically discover jailbreak strategies for AI models without human input. It significantly improves attack success rates, outperforming existing methods by 74.3% on average. The framework is versatile, allowing the integration of human-designed strategies to further enhance its effectiveness, achieving a 93.4% success rate on GPT-4-1106-turbo. This approach demonstrates the potential of automated systems in identifying vulnerabilities in AI models.'}, 'zh': {'title': 'AutoDAN-Turbo：自动化破解策略的革新', 'desc': '这篇论文介绍了一种名为AutoDAN-Turbo的黑箱破解方法，它可以在没有任何人为干预或预定义范围的情况下，从头开始自动发现尽可能多的破解策略。AutoDAN-Turbo在公共基准测试中表现优异，平均攻击成功率比基线方法高出74.3%。特别是，它在GPT-4-1106-turbo上达到了88.5%的攻击成功率。通过整合现有的人为设计的破解策略，AutoDAN-Turbo的攻击成功率甚至可以提高到93.4%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06166', 'title': 'Temporal Reasoning Transfer from Text to Video', 'url': 'https://huggingface.co/papers/2410.06166', 'abstract': "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships. While previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy. Surprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks. Building on this discovery, we introduce the Textual Temporal reasoning Transfer (T3). T3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. Remarkably, without using any video data, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples. Additionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. Further analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains.", 'score': 12, 'issue_id': 37, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '5cd9bf212c19151b', 'data': {'categories': ['#science', '#reasoning', '#video', '#training', '#transfer_learning', '#benchmark', '#small_models', '#architecture'], 'emoji': '⏳', 'ru': {'title': 'Улучшение временного понимания видео-LLM через текстовый перенос', 'desc': 'Исследователи обнаружили, что видео-модели большого языка (Video LLMs) испытывают трудности с пониманием временных отношений не из-за неэффективного кодирования визуальных входных данных, а из-за ограничений базовой языковой модели в понимании временных концепций. Для решения этой проблемы был разработан метод Textual Temporal reasoning Transfer (T3), который синтезирует задачи временного рассуждения в текстовом формате из существующих наборов данных изображений и текста. Применение T3 позволило значительно улучшить временное понимание модели LongVA-7B без использования видеоданных. Результаты показали сильную корреляцию между производительностью в текстовых и видео задачах временного рассуждения.'}, 'en': {'title': 'Unlocking Temporal Understanding in Video LLMs with Textual Insights', 'desc': "The paper explores the limitations of Video Large Language Models (Video LLMs) in understanding temporal changes and relationships in videos. It identifies that the main issue lies not in the video data itself but in the LLM's inherent difficulty with temporal concepts, as shown by poor performance on text-based temporal tasks. To address this, the authors introduce Textual Temporal reasoning Transfer (T3), which uses text-based tasks to improve temporal reasoning without video data. This approach significantly enhances the model's performance on video benchmarks, demonstrating the effectiveness of transferring temporal reasoning skills from text to video."}, 'zh': {'title': '从文本到视频：时间推理能力的跨域转移', 'desc': '这篇论文研究了视频大语言模型（Video LLMs）在理解时间变化和推理时间关系方面的挑战。研究发现，问题的关键在于底层语言模型对时间概念的理解困难，而不是视频表示本身的信息不足。为了解决这个问题，作者提出了一种名为文本时间推理转移（T3）的方法，通过从现有的图文数据集中合成多样的纯文本时间推理任务来提升模型的时间理解能力。结果表明，T3方法在不使用视频数据的情况下显著提高了模型在复杂时间场景下的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07155', 'title': 'Trans4D: Realistic Geometry-Aware Transition for Compositional Text-to-4D Synthesis', 'url': 'https://huggingface.co/papers/2410.07155', 'abstract': 'Recent advances in diffusion models have demonstrated exceptional capabilities in image and video generation, further improving the effectiveness of 4D synthesis. Existing 4D generation methods can generate high-quality 4D objects or scenes based on user-friendly conditions, benefiting the gaming and video industries. However, these methods struggle to synthesize significant object deformation of complex 4D transitions and interactions within scenes. To address this challenge, we propose Trans4D, a novel text-to-4D synthesis framework that enables realistic complex scene transitions. Specifically, we first use multi-modal large language models (MLLMs) to produce a physic-aware scene description for 4D scene initialization and effective transition timing planning. Then we propose a geometry-aware 4D transition network to realize a complex scene-level 4D transition based on the plan, which involves expressive geometrical object deformation. Extensive experiments demonstrate that Trans4D consistently outperforms existing state-of-the-art methods in generating 4D scenes with accurate and high-quality transitions, validating its effectiveness. Code: https://github.com/YangLing0818/Trans4D', 'score': 11, 'issue_id': 60, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'b0fe7f6e783e4cb0', 'data': {'categories': ['#video', '#cv', '#games', '#open_source', '#diffusion', '#architecture', '#multimodal', '#3d'], 'emoji': '🌀', 'ru': {'title': 'Реалистичные 4D-переходы на основе текста', 'desc': 'Статья представляет Trans4D - новую систему для синтеза сложных 4D-сцен на основе текстового описания. Авторы используют мультимодальные языковые модели для создания физически корректного описания сцены и планирования переходов. Затем применяется специальная нейросеть для реализации сложных переходов между 4D-сценами с учетом геометрии объектов. Эксперименты показывают, что Trans4D превосходит существующие методы в генерации качественных и точных 4D-переходов.'}, 'en': {'title': 'Trans4D: Revolutionizing 4D Scene Generation with Realistic Transitions', 'desc': 'This paper introduces Trans4D, a new framework for generating 4D scenes that can handle complex transitions and object deformations. It leverages multi-modal large language models (MLLMs) to create detailed scene descriptions and plan transition timings effectively. The framework includes a geometry-aware 4D transition network that allows for realistic and expressive changes in object shapes during scene transitions. Experimental results show that Trans4D outperforms existing methods, providing high-quality and accurate 4D scene generation.'}, 'zh': {'title': 'Trans4D：实现真实复杂场景过渡的创新框架', 'desc': '最近，扩散模型在图像和视频生成方面取得了显著进展，进一步提升了4D合成的效果。现有的4D生成方法能够根据用户友好的条件生成高质量的4D对象或场景，惠及游戏和视频行业。然而，这些方法在合成复杂4D过渡和场景内的交互时，面临显著物体变形的挑战。为了解决这个问题，我们提出了Trans4D，一个新颖的文本到4D合成框架，能够实现真实的复杂场景过渡。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06084', 'title': 'Diversity-Rewarded CFG Distillation', 'url': 'https://huggingface.co/papers/2410.06084', 'abstract': 'Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at https://google-research.github.io/seanet/musiclm/diverse_music/.', 'score': 10, 'issue_id': 42, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '99610ace8463ca57', 'data': {'categories': ['#audio', '#training', '#inference', '#rl', '#optimization', '#games', '#diffusion'], 'emoji': '🎵', 'ru': {'title': 'Повышение качества и разнообразия генеративных моделей музыки', 'desc': 'Статья представляет новый метод обучения генеративных моделей для музыки, называемый diversity-rewarded CFG distillation. Этот подход объединяет дистилляцию Classifier-Free Guidance (CFG) с обучением с подкреплением для поощрения разнообразия. Авторы применяют метод к модели MusicLM для генерации музыки по текстовому описанию. Результаты показывают улучшение качества и разнообразия генерируемой музыки по сравнению с базовой моделью с CFG.'}, 'en': {'title': 'Boosting Creativity: Diverse Outputs Without Extra Cost', 'desc': 'This paper introduces a new method called diversity-rewarded CFG distillation to improve generative models used in creative fields like music generation. The approach combines a distillation objective, which helps the model mimic CFG-augmented predictions, with a reinforcement learning objective that rewards diversity in outputs. By finetuning the model, it achieves high-quality and diverse outputs without increasing inference costs. The method also allows for weight-based model merging, enabling control over the quality-diversity trade-off and enhancing performance.'}, 'zh': {'title': '多样性奖励蒸馏：提升生成模型的质量与多样性', 'desc': '这篇论文介绍了一种新的微调方法，称为多样性奖励的CFG蒸馏，旨在解决CFG在生成内容时的局限性。通过优化蒸馏目标和强化学习目标，该方法在不增加推理成本的情况下，提升了生成内容的质量和多样性。研究人员在MusicLM模型上进行了实验，结果显示这种方法在质量和多样性方面优于传统的CFG。通过权重插值策略，可以在部署时灵活调整生成内容的质量与多样性之间的平衡。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06241', 'title': 'BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way', 'url': 'https://huggingface.co/papers/2410.06241', 'abstract': 'The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resulting in near-static video. In this work, we have identified a correlation between the disparity of temporal attention maps across different blocks and the occurrence of temporal inconsistencies. Additionally, we have observed that the energy contained within the temporal attention maps is directly related to the magnitude of motion amplitude in the generated videos. Based on these observations, we present BroadWay, a training-free method to improve the quality of text-to-video generation without introducing additional parameters, augmenting memory or sampling time. Specifically, BroadWay is composed of two principal components: 1) Temporal Self-Guidance improves the structural plausibility and temporal consistency of generated videos by reducing the disparity between the temporal attention maps across various decoder blocks. 2) Fourier-based Motion Enhancement enhances the magnitude and richness of motion by amplifying the energy of the map. Extensive experiments demonstrate that BroadWay significantly improves the quality of text-to-video generation with negligible additional cost.', 'score': 10, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'b38bb081870f60d1', 'data': {'categories': ['#video', '#training', '#inference', '#optimization', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'BroadWay: Повышение качества генерации видео без дополнительного обучения', 'desc': 'Статья представляет метод BroadWay для улучшения качества генерации видео из текста без дополнительного обучения или параметров. Метод основан на наблюдении связи между различиями во временных картах внимания и временной несогласованностью в видео. BroadWay включает два компонента: Temporal Self-Guidance для улучшения правдоподобности и согласованности, и Fourier-based Motion Enhancement для усиления движения. Эксперименты показывают значительное улучшение качества генерации видео с минимальными дополнительными затратами.'}, 'en': {'title': 'BroadWay: Elevating Text-to-Video Quality Without Extra Costs', 'desc': 'The paper introduces BroadWay, a method to enhance text-to-video generation by addressing common issues like structural implausibility and temporal inconsistency. It identifies that disparities in temporal attention maps lead to these inconsistencies and that the energy in these maps correlates with motion amplitude. BroadWay uses Temporal Self-Guidance to align attention maps for better video consistency and Fourier-based Motion Enhancement to boost motion richness. This approach improves video quality without adding extra parameters or increasing computational costs.'}, 'zh': {'title': 'BroadWay：提升文本到视频生成质量的新方法', 'desc': '这篇论文研究了文本到视频生成模型中常见的问题，如结构不合理和时间不一致。研究发现，不同块的时间注意力图之间的差异与时间不一致有关，而注意力图中的能量与视频中的运动幅度有关。基于这些观察，提出了一种名为BroadWay的方法，通过减少注意力图的差异和增强能量来提高视频质量。实验表明，BroadWay在不增加额外成本的情况下显著提升了视频生成的效果。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06458', 'title': 'LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints', 'url': 'https://huggingface.co/papers/2410.06458', 'abstract': 'Instruction following is a key capability for LLMs. However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post "in a funny tone" with "no hashtag"). Despite this, most evaluations focus solely on synthetic data. To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs\' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants. We also investigate model-based evaluation as a cost-effective alternative to human annotation for this task. Our findings reveal that even the proprietary GPT-4 model fails to meet at least one constraint on over 21% of instructions, highlighting the limitations of state-of-the-art models. To address the performance gap between open-source and proprietary models, we propose the Decompose, Critique and Refine (DeCRIM) self-correction pipeline, which enhances LLMs\' ability to follow constraints. DeCRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM\'s response needs refinement. Our results show that DeCRIM improves Mistral\'s performance by 7.3% on RealInstruct and 8.0% on IFEval even with weak feedback. Moreover, we demonstrate that with strong feedback, open-source LLMs with DeCRIM can outperform GPT-4 on both benchmarks.', 'score': 8, 'issue_id': 43, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'f8cc2a4349e584a6', 'data': {'categories': ['#dataset', '#training', '#alignment', '#benchmark', '#open_source', '#architecture', '#synthetic'], 'emoji': '🎯', 'ru': {'title': 'Преодоление ограничений в следовании инструкциям для языковых моделей', 'desc': 'Статья представляет RealInstruct - первый бенчмарк для оценки способности языковых моделей следовать реальным многоограниченным инструкциям. Исследование показывает, что даже GPT-4 не соблюдает хотя бы одно ограничение в более чем 21% инструкций. Авторы предлагают pipeline DeCRIM для улучшения способности моделей следовать ограничениям. Результаты демонстрируют, что DeCRIM повышает производительность Mistral на 7.3% в RealInstruct и 8.0% в IFEval.'}, 'en': {'title': 'Mastering Multi-Constraint Instructions with DeCRIM', 'desc': 'The paper introduces RealInstruct, a benchmark designed to evaluate large language models (LLMs) on their ability to follow real-world instructions with multiple constraints. It highlights that even advanced models like GPT-4 struggle with such tasks, failing to meet at least one constraint in over 21% of cases. To improve performance, the authors propose the Decompose, Critique, and Refine (DeCRIM) pipeline, which breaks down instructions into constraints and uses a Critic model to refine responses. The study shows that DeCRIM significantly enhances the performance of open-source models, even surpassing GPT-4 with strong feedback.'}, 'zh': {'title': '突破多重约束：提升LLM的指令执行能力', 'desc': '这篇论文介绍了一个名为RealInstruct的新基准，用于评估大型语言模型（LLM）在处理真实世界多重约束指令时的能力。研究发现，即使是先进的GPT-4模型，在21%以上的指令中也未能满足至少一个约束。为了解决这一问题，作者提出了DeCRIM自我修正流程，通过分解指令和使用批评模型来提高LLM的表现。结果显示，DeCRIM显著提升了开源模型的性能，甚至在某些情况下超过了GPT-4。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02428', 'title': 'Collective Critics for Creative Story Generation', 'url': 'https://huggingface.co/papers/2410.02428', 'abstract': "Generating a long story of several thousand words with narrative coherence using Large Language Models (LLMs) has been a challenging task. Previous research has addressed this challenge by proposing different frameworks that create a story plan and generate a long story based on that plan. However, these frameworks have been mainly focusing on maintaining narrative coherence in stories, often overlooking creativity in story planning and the expressiveness of the stories generated from those plans, which are desirable properties to captivate readers' interest. In this paper, we propose Collective Critics for Creative Story Generation framework (CritiCS), which is composed of plan refining stage (CrPlan) and story generation stage (CrText), to integrate a collective revision mechanism that promotes those properties into long-form story generation process. Specifically, in each stage, a group of LLM critics and one leader collaborate to incrementally refine drafts of plan and story throughout multiple rounds. Extensive human evaluation shows that the CritiCS can significantly enhance story creativity and reader engagement, while also maintaining narrative coherence. Furthermore, the design of the framework allows active participation from human writers in any role within the critique process, enabling interactive human-machine collaboration in story writing.", 'score': 8, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': 'fd2a584ee28c9b94', 'data': {'categories': ['#training', '#alignment', '#architecture', '#story_generation', '#multimodal'], 'emoji': '📚', 'ru': {'title': 'CritiCS: Коллективный подход к созданию креативных историй с помощью ИИ', 'desc': 'Статья представляет новый подход к генерации длинных историй с помощью больших языковых моделей, называемый CritiCS. Этот метод использует коллективный механизм критики для улучшения креативности и выразительности генерируемых историй. CritiCS состоит из двух этапов: уточнения плана (CrPlan) и генерации текста (CrText), где группа ИИ-критиков и лидер сотрудничают для постепенного улучшения черновиков. Оценка показала, что CritiCS значительно повышает креативность историй и вовлеченность читателей, сохраняя при этом связность повествования.'}, 'en': {'title': 'CritiCS: Elevating Storytelling with Creative Collaboration', 'desc': 'The paper introduces a new framework called CritiCS for generating long stories using Large Language Models (LLMs). CritiCS consists of two stages: CrPlan for refining story plans and CrText for generating the story, both involving a group of LLM critics and a leader to enhance creativity and expressiveness. This approach not only maintains narrative coherence but also significantly boosts creativity and reader engagement, as shown by extensive human evaluations. Additionally, the framework supports interactive collaboration between human writers and the machine, allowing humans to participate actively in the critique process.'}, 'zh': {'title': 'CritiCS：提升故事创造性与连贯性的创新框架', 'desc': '这篇论文提出了一种新的框架CritiCS，用于生成具有创造性和叙述连贯性的长篇故事。CritiCS框架包括计划优化阶段（CrPlan）和故事生成阶段（CrText），通过集体修订机制提高故事的创造性和吸引力。在每个阶段，多个大型语言模型的评论者和一个领导者合作，逐步完善计划和故事草稿。人类评估表明，CritiCS框架显著提升了故事的创造性和读者的参与度，同时保持了叙述的连贯性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.02503', 'title': 'Mixed-Session Conversation with Egocentric Memory', 'url': 'https://huggingface.co/papers/2410.02503', 'abstract': "Recently introduced dialogue systems have demonstrated high usability. However, they still fall short of reflecting real-world conversation scenarios. Current dialogue systems exhibit an inability to replicate the dynamic, continuous, long-term interactions involving multiple partners. This shortfall arises because there have been limited efforts to account for both aspects of real-world dialogues: deeply layered interactions over the long-term dialogue and widely expanded conversation networks involving multiple participants. As the effort to incorporate these aspects combined, we introduce Mixed-Session Conversation, a dialogue system designed to construct conversations with various partners in a multi-session dialogue setup. We propose a new dataset called MiSC to implement this system. The dialogue episodes of MiSC consist of 6 consecutive sessions, with four speakers (one main speaker and three partners) appearing in each episode. Also, we propose a new dialogue model with a novel memory management mechanism, called Egocentric Memory Enhanced Mixed-Session Conversation Agent (EMMA). EMMA collects and retains memories from the main speaker's perspective during conversations with partners, enabling seamless continuity in subsequent interactions. Extensive human evaluations validate that the dialogues in MiSC demonstrate a seamless conversational flow, even when conversation partners change in each session. EMMA trained with MiSC is also evaluated to maintain high memorability without contradiction throughout the entire conversation.", 'score': 8, 'issue_id': 42, 'pub_date': '2024-10-03', 'pub_date_card': {'ru': '3 октября', 'en': 'October 3', 'zh': '10月3日'}, 'hash': '87a67ed9acbc74a2', 'data': {'categories': ['#dataset', '#long_context', '#dialogue_systems', '#agents', '#architecture', '#synthetic'], 'emoji': '🗣️', 'ru': {'title': 'Революция в диалоговых системах: непрерывное общение с множеством партнеров', 'desc': 'Статья представляет новую систему диалогов под названием Mixed-Session Conversation, которая способна вести разговоры с несколькими партнерами в многосессионном формате. Авторы предлагают набор данных MiSC, состоящий из 6 последовательных сессий с участием четырех собеседников. Также представлена модель EMMA с новым механизмом управления памятью, которая собирает и сохраняет воспоминания с точки зрения главного говорящего. Оценки показывают, что диалоги в MiSC демонстрируют непрерывный разговорный поток даже при смене собеседников, а EMMA сохраняет высокую запоминаемость без противоречий.'}, 'en': {'title': 'Revolutionizing Dialogue Systems with Multi-Session Dynamics', 'desc': 'The paper introduces a new dialogue system called Mixed-Session Conversation (MiSC) designed to handle dynamic, long-term interactions with multiple partners. It addresses the limitations of current systems by incorporating deeply layered interactions and expanded conversation networks. A novel dataset, MiSC, is used to train the system, featuring dialogue episodes with multiple sessions and speakers. The proposed model, EMMA, uses an innovative memory management mechanism to ensure continuity and coherence in conversations, validated through extensive human evaluations.'}, 'zh': {'title': '多方长时间互动的对话新纪元', 'desc': '这篇论文介绍了一种新的对话系统，称为混合会话对话系统，旨在模拟真实世界中的多方长时间互动。研究者提出了一个新的数据集MiSC，用于支持该系统的实现，其中每个对话包含6个连续会话和四个参与者。为了增强对话的连续性，论文中还介绍了一种新的对话模型EMMA，它通过从主说话者的视角收集和管理记忆来实现。通过广泛的人类评估，证明MiSC中的对话即使在会话伙伴变化时也能保持流畅，EMMA在整个对话中保持高记忆性且无矛盾。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06555', 'title': 'ING-VP: MLLMs cannot Play Easy Vision-based Games Yet', 'url': 'https://huggingface.co/papers/2410.06555', 'abstract': "As multimodal large language models (MLLMs) continue to demonstrate increasingly competitive performance across a broad spectrum of tasks, more intricate and comprehensive benchmarks have been developed to assess these cutting-edge models. These benchmarks introduce new challenges to core capabilities such as perception, reasoning, and planning. However, existing multimodal benchmarks fall short in providing a focused evaluation of multi-step planning based on spatial relationships in images. To bridge this gap, we present ING-VP, the first INteractive Game-based Vision Planning benchmark, specifically designed to evaluate the spatial imagination and multi-step reasoning abilities of MLLMs. ING-VP features 6 distinct games, encompassing 300 levels, each with 6 unique configurations. A single model engages in over 60,000 rounds of interaction. The benchmark framework allows for multiple comparison settings, including image-text vs. text-only inputs, single-step vs. multi-step reasoning, and with-history vs. without-history conditions, offering valuable insights into the model's capabilities. We evaluated numerous state-of-the-art MLLMs, with the highest-performing model, Claude-3.5 Sonnet, achieving an average accuracy of only 3.37%, far below the anticipated standard. This work aims to provide a specialized evaluation framework to drive advancements in MLLMs' capacity for complex spatial reasoning and planning. The code is publicly available at https://github.com/Thisisus7/ING-VP.git.", 'score': 8, 'issue_id': 41, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'dabc7be39f163aa7', 'data': {'categories': ['#reasoning', '#cv', '#rl', '#benchmark', '#games', '#open_source', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'ING-VP: Новый рубеж в оценке пространственного мышления ИИ', 'desc': 'Представлен новый бенчмарк ING-VP для оценки пространственного воображения и многошагового рассуждения мультимодальных языковых моделей. Бенчмарк содержит 6 игр с 300 уровнями и 6 конфигурациями каждый, что позволяет провести более 60 000 раундов взаимодействия. ING-VP оценивает модели в различных условиях, включая сравнение входных данных изображение-текст и только текст. Лучшая модель Claude-3.5 Sonnet показала точность всего 3.37%, что подчеркивает сложность задачи.'}, 'en': {'title': 'Pushing Boundaries: Evaluating Spatial Reasoning in MLLMs with ING-VP', 'desc': 'The paper introduces ING-VP, a new benchmark designed to test the spatial reasoning and multi-step planning abilities of multimodal large language models (MLLMs). It features interactive games that challenge models to understand and plan based on spatial relationships in images. The benchmark allows for various comparison settings, such as image-text versus text-only inputs, to evaluate different aspects of model performance. Despite testing several advanced models, the highest accuracy achieved was only 3.37%, highlighting the difficulty of the tasks and the need for further advancements in this area.'}, 'zh': {'title': '推动多模态模型的空间推理新高度', 'desc': '这篇论文介绍了一种新的基准测试，名为ING-VP，用于评估多模态大语言模型（MLLMs）的空间想象和多步推理能力。ING-VP通过6个不同的游戏和300个关卡，测试模型在图像中的空间关系上的多步规划能力。研究发现，即使是最先进的模型在这个基准测试中的表现也远低于预期，表明当前模型在复杂空间推理和规划方面还有很大提升空间。这个工作旨在推动MLLMs在复杂任务中的能力发展。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07160', 'title': 'TextToon: Real-Time Text Toonify Head Avatar from Single Video', 'url': 'https://huggingface.co/papers/2410.07160', 'abstract': 'We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing related works heavily rely on multi-view modeling to recover geometry via texture embeddings, presented in a static manner, leading to control limitations. The multi-view video input also makes it difficult to deploy these models in real-world applications. To address these issues, we adopt a conditional embedding Tri-plane to learn realistic and stylized facial representations in a Gaussian deformation field. Additionally, we expand the stylization capabilities of 3D Gaussian Splatting by introducing an adaptive pixel-translation neural network and leveraging patch-aware contrastive learning to achieve high-quality images. To push our work into consumer applications, we develop a real-time system that can operate at 48 FPS on a GPU machine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate the efficacy of our approach in generating textual avatars over existing methods in terms of quality and real-time animation. Please refer to our project page for more details: https://songluchuan.github.io/TextToon/.', 'score': 8, 'issue_id': 40, 'pub_date': '2024-09-23', 'pub_date_card': {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'}, 'hash': '425ed10e0d4ec2cd', 'data': {'categories': ['#video', '#cv', '#inference', '#optimization', '#games', '#architecture', '#synthetic', '#multimodal', '#3d'], 'emoji': '🎭', 'ru': {'title': 'TextToon: Создание управляемых мультяшных аватаров по видео и тексту', 'desc': 'TextToon - это метод создания управляемого мультяшного аватара на основе короткого видео и текстового описания стиля. Он использует условное встраивание Tri-plane для реалистичного и стилизованного представления лица в гауссовом поле деформации. Модель улучшает возможности стилизации 3D Gaussian Splatting с помощью адаптивной нейронной сети пиксельного преобразования и контрастного обучения с учетом патчей. TextToon работает в режиме реального времени и превосходит существующие методы по качеству и анимации.'}, 'en': {'title': '"Animate Your Avatar: Real-Time Toonification from Text and Video"', 'desc': 'TextToon is a novel method for creating animated avatars from a single video and a text description of the desired style. Unlike previous methods that require multiple camera angles and result in static models, TextToon uses a conditional embedding Tri-plane to create dynamic and stylized facial representations. The approach enhances 3D Gaussian Splatting with an adaptive pixel-translation neural network and patch-aware contrastive learning, resulting in high-quality, real-time animations. The system is efficient, running at 48 FPS on GPUs and 15-18 FPS on mobile devices, making it suitable for consumer applications.'}, 'zh': {'title': 'TextToon：实时生成可驱动卡通化头像的创新方法', 'desc': '这篇论文介绍了一种名为TextToon的方法，可以生成可驱动的卡通化头像。通过一个短的单目视频序列和关于头像风格的书面指令，模型能够生成高保真度的卡通化头像，并能通过另一个视频实时驱动。与现有方法不同，TextToon不依赖多视角建模，而是采用条件嵌入三平面来学习真实和风格化的面部表示。通过引入自适应像素翻译神经网络和利用补丁感知对比学习，TextToon实现了高质量的图像生成。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06172', 'title': 'Multimodal Situational Safety', 'url': 'https://huggingface.co/papers/2410.06172', 'abstract': 'Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.', 'score': 8, 'issue_id': 39, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '9fee460887b3cb38', 'data': {'categories': ['#reasoning', '#dataset', '#security', '#ethics', '#agents', '#benchmark', '#alignment', '#open_source', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Новый рубеж в безопасности ИИ: оценка ситуационной осведомленности мультимодальных моделей', 'desc': 'Статья представляет первый анализ проблемы многомодальной ситуационной безопасности в контексте мультимодальных больших языковых моделей (MLLM). Авторы разработали набор данных MSSBench для оценки способности MLLM учитывать безопасность в различных ситуациях. Исследование показало, что существующие MLLM испытывают трудности с комплексным решением задач ситуационной безопасности. Предложенные мультиагентные подходы демонстрируют улучшение безопасности по сравнению с исходными ответами MLLM.'}, 'en': {'title': 'Ensuring Safety in Multimodal AI: A New Benchmark for Contextual Understanding', 'desc': 'This paper introduces the concept of Multimodal Situational Safety, focusing on how safety considerations change based on the context in which a Multimodal Large Language Model (MLLM) operates. The authors present a new benchmark, MSSBench, to evaluate how well MLLMs can assess safety by analyzing 1,820 language query-image pairs. The study finds that current MLLMs struggle with understanding and responding to situational safety challenges, indicating a need for further research in this area. To address these challenges, the authors propose multi-agent pipelines that improve safety performance over standard MLLM responses.'}, 'zh': {'title': '多模态情境安全：MLLMs的新挑战', 'desc': '多模态大语言模型（MLLMs）正在迅速发展，展现出作为多模态助手的强大能力，但也带来了安全性问题。本文首次提出并分析了一种新的安全挑战，称为多模态情境安全，研究了用户或代理在特定情境下的安全考量。为了评估这种能力，我们开发了多模态情境安全基准（MSSBench），用于评估当前MLLMs的情境安全性能。研究发现，现有的MLLMs在处理这种复杂的安全问题时存在困难，尤其是在指令跟随的情况下。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05643', 'title': 'TRACE: Temporal Grounding Video LLM via Causal Event Modeling', 'url': 'https://huggingface.co/papers/2410.05643', 'abstract': "Video Temporal Grounding (VTG) is a crucial capability for video understanding models and plays a vital role in downstream tasks such as video browsing and editing. To effectively handle various tasks simultaneously and enable zero-shot prediction, there is a growing trend in employing video LLMs for VTG tasks. However, current video LLM-based methods rely exclusively on natural language generation, lacking the ability to model the clear structure inherent in videos, which restricts their effectiveness in tackling VTG tasks. To address this issue, this paper first formally introduces causal event modeling framework, which represents videos as sequences of events, and predict the current event using previous events, video inputs, and textural instructions. Each event consists of three components: timestamps, salient scores, and textual captions. We then propose a novel task-interleaved video LLM called TRACE to effectively implement the causal event modeling framework in practice. The TRACE processes visual frames, timestamps, salient scores, and text as distinct tasks, employing various encoders and decoding heads for each. Task tokens are arranged in an interleaved sequence according to the causal event modeling framework's formulation. Extensive experiments on various VTG tasks and datasets demonstrate the superior performance of TRACE compared to state-of-the-art video LLMs. Our model and code are available at https://github.com/gyxxyg/TRACE.", 'score': 8, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '7241bf4a739cd624', 'data': {'categories': ['#reasoning', '#video', '#dataset', '#games', '#open_source', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'TRACE: Революция в понимании структуры видео с помощью каузального моделирования событий', 'desc': 'Статья представляет новый подход к задаче временной локализации в видео (Video Temporal Grounding). Авторы вводят концепцию каузального моделирования событий, представляя видео как последовательность событий. Они предлагают модель TRACE - мультизадачную видео-LLM, которая обрабатывает визуальные кадры, временные метки, оценки значимости и текст как отдельные задачи. Эксперименты показывают превосходство TRACE над современными видео-LLM в различных задачах VTG.'}, 'en': {'title': 'TRACE: Structuring Videos for Superior Understanding', 'desc': 'The paper introduces a new approach to Video Temporal Grounding (VTG) by using a causal event modeling framework, which structures videos as sequences of events. This framework allows for more effective video understanding by predicting current events based on previous ones, using timestamps, salient scores, and textual captions. The proposed model, TRACE, processes these components as distinct tasks with specialized encoders and decoders, improving the handling of VTG tasks. Experiments show that TRACE outperforms existing video language models, enhancing video browsing and editing capabilities.'}, 'zh': {'title': 'TRACE：革新视频时间定位的因果事件建模', 'desc': '这篇论文介绍了一种新的因果事件建模框架，用于视频时间定位任务。该框架将视频表示为事件序列，通过先前事件、视频输入和文本指令来预测当前事件。论文提出了一种名为TRACE的新型视频大语言模型，能够有效实现因果事件建模。实验结果表明，TRACE在多种视频时间定位任务中表现优于现有的最先进模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07064', 'title': 'Data Selection via Optimal Control for Language Models', 'url': 'https://huggingface.co/papers/2410.07064', 'abstract': "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. We formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics. Based on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. In our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes. Moreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws. PDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/data_selection.", 'score': 8, 'issue_id': 37, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'f8459aee9413c793', 'data': {'categories': ['#training', '#optimization', '#data', '#open_source', '#small_models'], 'emoji': '🎯', 'ru': {'title': 'Оптимизация выбора данных для эффективного предобучения языковых моделей', 'desc': 'Эта работа исследует выбор высококачественных данных для предобучения из массивных корпусов с целью улучшения возможностей языковых моделей. Авторы формулируют выбор данных как обобщенную задачу оптимального управления, решаемую с помощью принципа максимума Понтрягина. На основе теоретических результатов они представляют фреймворк PDS для приближенного оптимального выбора данных. Эксперименты показывают, что PDS ускоряет обучение моделей и повышает их эффективность на различных задачах, а также улучшает использование данных при ограниченных ресурсах.'}, 'en': {'title': '"Smart Data, Smarter Models: Optimizing Pre-training for Better AI"', 'desc': "This paper explores how to choose the best pre-training data from large datasets to improve language models (LMs) for future tasks. The authors use a mathematical approach called Pontryagin's Maximum Principle to find the best data selection strategy, which they call PMP-based Data Selection (PDS). Experiments show that PDS helps LMs learn faster and perform better on various tasks, even with very large models. Additionally, PDS makes better use of limited data, reducing the need for large datasets by almost half."}, 'zh': {'title': '通过PMP优化数据选择，提升语言模型性能', 'desc': '这项研究探讨了如何从大量语料库中选择高质量的预训练数据，以增强语言模型在下游任务中的能力。我们将数据选择问题表述为一个广义的最优控制问题，并通过庞特里亚金最大值原理（PMP）来解决，得出描述最佳数据选择与语言模型训练动态关系的必要条件。基于这些理论结果，我们引入了PMP数据选择（PDS）框架，通过解决PMP条件来近似最佳数据选择。在实验中，PDS选择的数据加速了语言模型的学习，并在各种下游任务中持续提升其性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05791', 'title': 'FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance', 'url': 'https://huggingface.co/papers/2410.05791', 'abstract': 'Piano playing requires agile, precise, and coordinated hand control that stretches the limits of dexterity. Hand motion models with the sophistication to accurately recreate piano playing have a wide range of applications in character animation, embodied AI, biomechanics, and VR/AR. In this paper, we construct a first-of-its-kind large-scale dataset that contains approximately 10 hours of 3D hand motion and audio from 15 elite-level pianists playing 153 pieces of classical music. To capture natural performances, we designed a markerless setup in which motions are reconstructed from multi-view videos using state-of-the-art pose estimation models. The motion data is further refined via inverse kinematics using the high-resolution MIDI key-pressing data obtained from sensors in a specialized Yamaha Disklavier piano. Leveraging the collected dataset, we developed a pipeline that can synthesize physically-plausible hand motions for musical scores outside of the dataset. Our approach employs a combination of imitation learning and reinforcement learning to obtain policies for physics-based bimanual control involving the interaction between hands and piano keys. To solve the sampling efficiency problem with the large motion dataset, we use a diffusion model to generate natural reference motions, which provide high-level trajectory and fingering (finger order and placement) information. However, the generated reference motion alone does not provide sufficient accuracy for piano performance modeling. We then further augmented the data by using musical similarity to retrieve similar motions from the captured dataset to boost the precision of the RL policy. With the proposed method, our model generates natural, dexterous motions that generalize to music from outside the training dataset.', 'score': 7, 'issue_id': 40, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'cba54e31692e353c', 'data': {'categories': ['#video', '#audio', '#dataset', '#synthetic', '#graphs', '#rl', '#games', '#diffusion', '#robotics', '#multimodal', '#3d'], 'emoji': '🎹', 'ru': {'title': 'Виртуозное воссоздание игры на пианино с помощью ИИ', 'desc': 'Эта статья представляет новый набор данных, содержащий 10 часов 3D-движений рук и аудио от 15 пианистов высокого уровня, играющих классическую музыку. Авторы разработали систему безмаркерного захвата движений с использованием мультиракурсной съемки и современных моделей оценки позы. На основе собранных данных создан конвейер для синтеза реалистичных движений рук при игре на пианино, используя имитационное обучение и обучение с подкреплением. Модель генерирует естественные и точные движения, которые обобщаются на музыку вне обучающего набора.'}, 'en': {'title': 'Mastering Piano with AI: Crafting Realistic Hand Motions', 'desc': 'The paper introduces a novel dataset capturing 3D hand motions and audio from elite pianists, using advanced pose estimation and inverse kinematics to ensure accuracy. This dataset is used to develop a model that synthesizes realistic hand movements for piano playing, employing imitation and reinforcement learning for bimanual control. A diffusion model enhances sampling efficiency by generating reference motions, which are further refined using musical similarity to improve precision. The resulting model can produce natural and dexterous hand motions for musical pieces not included in the training data.'}, 'zh': {'title': '钢琴演奏的手部动作建模新突破', 'desc': '这篇论文介绍了一种新的大规模数据集，包含15位顶级钢琴家演奏153首古典音乐的3D手部动作和音频数据。研究人员使用无标记的多视角视频和先进的姿态估计模型来重建自然的演奏动作，并通过逆运动学和高分辨率MIDI数据进行精细化处理。通过模仿学习和强化学习相结合的方法，研究人员开发了一种合成物理合理的手部动作的流程。为了提高采样效率，他们使用扩散模型生成自然的参考动作，并通过音乐相似性增强数据，提升强化学习策略的精度。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05664', 'title': 'Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning', 'url': 'https://huggingface.co/papers/2410.05664', 'abstract': 'As text-to-image diffusion models become advanced enough for commercial applications, there is also increasing concern about their potential for malicious and harmful use. Model unlearning has been proposed to mitigate the concerns by removing undesired and potentially harmful information from the pre-trained model. So far, the success of unlearning is mainly measured by whether the unlearned model can generate a target concept while maintaining image quality. However, unlearning is typically tested under limited scenarios, and the side effects of unlearning have barely been studied in the current literature. In this work, we thoroughly analyze unlearning under various scenarios with five key aspects. Our investigation reveals that every method has side effects or limitations, especially in more complex and realistic situations. By releasing our comprehensive evaluation framework with the source codes and artifacts, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.', 'score': 7, 'issue_id': 38, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '9630c1ce042de601', 'data': {'categories': ['#cv', '#security', '#training', '#optimization', '#benchmark', '#open_source', '#diffusion'], 'emoji': '🧠', 'ru': {'title': 'Разобучение генеративных моделей: проблемы и перспективы', 'desc': 'Статья посвящена исследованию методов разобучения моделей генерации изображений для удаления нежелательной информации. Авторы проанализировали существующие подходы к разобучению по пяти ключевым аспектам в различных сценариях. Исследование выявило, что все методы имеют побочные эффекты или ограничения, особенно в сложных реалистичных ситуациях. Авторы представили комплексную систему оценки с открытым исходным кодом для стимулирования дальнейших исследований в этой области.'}, 'en': {'title': "Unlearning: Cleaning Up AI's Act", 'desc': "This paper explores the concept of model unlearning in text-to-image diffusion models, which aims to remove harmful or unwanted information from pre-trained models. The authors highlight that while unlearning is often evaluated based on the model's ability to generate specific concepts without losing image quality, it is usually tested in limited scenarios. They conduct a thorough analysis of unlearning across various situations, identifying that all current methods have side effects or limitations, particularly in complex environments. By providing a comprehensive evaluation framework, the authors aim to encourage further research to develop more reliable unlearning techniques."}, 'zh': {'title': '探索模型遗忘：消除不良信息的挑战与机遇', 'desc': '随着文本到图像扩散模型的进步，它们在商业应用中的潜力越来越大，但也引发了对其可能被恶意使用的担忧。模型遗忘技术被提出以消除预训练模型中不需要的和潜在有害的信息。本文深入分析了在不同场景下的遗忘效果，发现每种方法都有其副作用或局限性。我们发布了一个全面的评估框架，希望能激发更多关于可靠和有效的遗忘方法的研究。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04223', 'title': 'Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning', 'url': 'https://huggingface.co/papers/2410.04223', 'abstract': 'While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.', 'score': 7, 'issue_id': 38, 'pub_date': '2024-10-05', 'pub_date_card': {'ru': '5 октября', 'en': 'October 5', 'zh': '10月5日'}, 'hash': '9c153ad16c36d327', 'data': {'categories': ['#science', '#reasoning', '#dataset', '#graphs', '#rl', '#optimization', '#benchmark', '#diffusion', '#architecture', '#multimodal'], 'emoji': '🧪', 'ru': {'title': 'Llamole: прорыв в генерации молекул с помощью мультимодальных языковых моделей', 'desc': 'Llamole - это первая мультимодальная языковая модель, способная генерировать текст и графы молекул. Она объединяет базовую языковую модель с Graph Diffusion Transformer и графовыми нейронными сетями для многоусловной генерации молекул и вывода реакций. Llamole также интегрирует A* поиск с функциями стоимости на основе языковой модели для эффективного ретросинтетического планирования. Модель значительно превосходит 14 адаптированных языковых моделей по 12 метрикам для контролируемого дизайна молекул и ретросинтетического планирования.'}, 'en': {'title': 'Llamole: Bridging Text and Graphs for Smarter Molecular Design', 'desc': 'The paper introduces Llamole, a groundbreaking multimodal large language model that can generate both text and graphs, specifically designed for applications in materials and drug design. Llamole combines a base language model with advanced graph processing techniques like the Graph Diffusion Transformer and Graph Neural Networks to handle complex molecular generation and reaction inference. It also uses A* search with language model-based cost functions to improve retrosynthetic planning, making it more efficient. Extensive experiments show that Llamole outperforms other adapted language models in controllable molecular design and retrosynthetic planning, demonstrating its potential in these fields.'}, 'zh': {'title': 'Llamole：突破图形生成的多模态大语言模型', 'desc': '大语言模型（LLM）在处理图形数据时面临挑战，限制了其在材料和药物设计中的应用。为了解决这个问题，我们引入了Llamole，这是第一个能够生成文本和图形的多模态LLM。Llamole结合了图扩散变压器和图神经网络，实现了分子逆向设计和反应推断。实验表明，Llamole在可控分子设计和逆合成规划方面显著优于其他模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06462', 'title': 'Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders', 'url': 'https://huggingface.co/papers/2410.06462', 'abstract': 'The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic strings, previous work on math solutions that embed harmful prompts demonstrate that the guardrails may differ between expert contexts. These loopholes would appear in mixture of expert\'s models when the context of the question changes and may offer fewer malicious training examples to filter toxic comments or recommended offensive actions. The present work demonstrates that foundational models may refuse to propose destructive actions correctly when prompted overtly but may unfortunately drop their guard when presented with a sudden change of context, like solving a computer programming challenge. We show empirical examples with trojan-hosting repositories like GitHub, NPM, NuGet, and popular content delivery networks (CDN) like jsDelivr which amplify the attack surface. In the LLM\'s directives to be helpful, example recommendations propose application programming interface (API) endpoints which a determined domain-squatter could acquire and setup attack mobile infrastructure that triggers from the naively copied code. We compare this attack to previous work on context-shifting and contrast the attack surface as a novel version of "living off the land" attacks in the malware literature. In the latter case, foundational language models can hijack otherwise innocent user prompts to recommend actions that violate their owners\' safety policies when posed directly without the accompanying coding support request.', 'score': 7, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'a2ceecf54aefe4d6', 'data': {'categories': ['#hallucinations', '#plp', '#security', '#architecture'], 'emoji': '🕵️', 'ru': {'title': 'Неожиданные уязвимости LLM при рекомендации кода', 'desc': "Данное исследование посвящено изучению потенциальных уязвимостей больших языковых моделей (LLM) в контексте рекомендаций кода. Авторы демонстрируют, что при резком изменении контекста, например, при решении задачи программирования, LLM могут снизить свою защиту и предложить потенциально вредоносный код. Эксперименты показывают, как модели могут рекомендовать API-эндпоинты, которые злоумышленники могут использовать для атак. Исследование сравнивает этот тип атаки с предыдущими работами по смене контекста и рассматривает его как новую версию атак типа 'living off the land' в литературе о вредоносном ПО."}, 'en': {'title': 'Exploiting Context: The Hidden Vulnerability in AI Code Recommendations', 'desc': "This paper explores how adversaries can exploit large language models (LLMs) to introduce malicious code into popular code repositories by manipulating context. It highlights that while LLMs from companies like OpenAI and Google have safeguards against harmful outputs, these can be bypassed when the context of a request changes, such as during programming challenges. The study provides examples of how repositories like GitHub and content delivery networks can be used to spread malicious code through seemingly innocent API recommendations. The research compares this method to 'living off the land' attacks, where LLMs inadvertently suggest harmful actions by misinterpreting user prompts."}, 'zh': {'title': '基础模型的上下文漏洞：编程挑战中的隐患', 'desc': '这项研究探讨了在流行代码库中引入恶意代码的对抗性潜力，特别是通过复制代码或虚构的AI建议。尽管OpenAI、Google和Anthropic的基础大语言模型（LLM）对有害行为和有毒字符串有防护，但在数学解决方案中嵌入有害提示的工作表明，这些防护措施在专家环境中可能有所不同。研究表明，当问题的上下文发生变化时，基础模型可能会放松警惕，尤其是在解决编程挑战时。通过实证例子展示了在GitHub、NPM等平台上，恶意代码如何利用这些漏洞进行攻击。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07071', 'title': 'Retrieval-Augmented Decision Transformer: External Memory for In-context RL', 'url': 'https://huggingface.co/papers/2410.07071', 'abstract': "In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid-worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments.", 'score': 6, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '0815ef9a074b159d', 'data': {'categories': ['#reasoning', '#dataset', '#rag', '#rl', '#transfer_learning', '#games', '#open_source', '#robotics'], 'emoji': '🧠', 'ru': {'title': 'RA-DT: Эффективное обучение с подкреплением в контексте с помощью извлечения релевантного опыта', 'desc': 'Статья представляет новый метод обучения с подкреплением в контексте - Retrieval-Augmented Decision Transformer (RA-DT). RA-DT использует внешнюю память для хранения прошлого опыта и извлекает только релевантные текущей ситуации под-траектории. Метод не требует обучения компонента извлечения и может быть независимым от домена. RA-DT превосходит базовые модели на сеточных мирах, используя лишь часть длины их контекста.'}, 'en': {'title': 'Revolutionizing In-Context Learning with RA-DT: Efficient, Domain-Agnostic, and Powerful', 'desc': 'The paper introduces the Retrieval-Augmented Decision Transformer (RA-DT), a novel approach in Reinforcement Learning that enhances in-context learning by using an external memory to store and retrieve relevant sub-trajectories. Unlike previous methods that require entire episodes, RA-DT efficiently handles complex environments with long episodes and sparse rewards by focusing only on pertinent past experiences. The retrieval mechanism in RA-DT is domain-agnostic and does not need training, making it versatile across different tasks. Evaluations show that RA-DT outperforms existing methods in grid-worlds and other environments, highlighting its potential to overcome current limitations in in-context RL.'}, 'zh': {'title': '检索增强：强化学习中的新突破', 'desc': '这篇论文介绍了一种新的方法，称为检索增强决策变换器（RA-DT），用于在强化学习中实现上下文学习。RA-DT通过外部记忆机制存储过去的经验，并检索与当前情况相关的子轨迹，从而解决了传统方法在复杂环境中长时间段和稀疏奖励的问题。RA-DT在网格世界、机器人模拟和程序生成的视频游戏中表现优异，超越了基线方法。研究还揭示了当前上下文强化学习方法在复杂环境中的局限性，并讨论了未来的研究方向。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07095', 'title': 'MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering', 'url': 'https://huggingface.co/papers/2410.07095', 'abstract': "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI's o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents.", 'score': 6, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '103d4f2b02c940a9', 'data': {'categories': ['#dataset', '#training', '#agents', '#benchmark', '#games', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'MLE-bench: измеряем инженерные навыки ИИ в машинном обучении', 'desc': 'MLE-bench - это новый бенчмарк для оценки способностей ИИ-агентов в области инженерии машинного обучения. Он включает 75 соревнований с Kaggle, охватывающих различные аспекты ML-инженерии. Авторы установили базовые показатели человеческой производительности и протестировали несколько языковых моделей, используя открытые фреймворки для агентов. Лучший результат показала модель OpenAI o1-preview с AIDE, достигнув уровня бронзовой медали Kaggle в 16.9% соревнований.'}, 'en': {'title': "Testing AI's Engineering Prowess with MLE-bench", 'desc': "MLE-bench is a new benchmark designed to evaluate how well AI agents can perform machine learning engineering tasks. It includes 75 competitions from Kaggle that test skills like model training, dataset preparation, and experiment execution. The benchmark uses human performance baselines from Kaggle leaderboards and evaluates AI models, finding that OpenAI's o1-preview with AIDE scaffolding performs at a bronze medal level in 16.9% of tasks. The study also explores how resource scaling affects AI performance and the influence of pre-training data contamination, with the benchmark code available for public use."}, 'zh': {'title': '评估AI在机器学习工程中的表现', 'desc': '我们推出了MLE-bench，这是一个用于评估AI代理在机器学习工程中表现的基准。我们从Kaggle上精选了75个与ML工程相关的竞赛，涵盖了训练模型、准备数据集和运行实验等实际技能。通过使用Kaggle的公开排行榜，我们为每个竞赛建立了人类基线。我们发现，使用OpenAI的o1-preview与AIDE框架的组合在16.9%的竞赛中达到了Kaggle铜牌水平。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06985', 'title': 'Jointly Generating Multi-view Consistent PBR Textures using Collaborative Control', 'url': 'https://huggingface.co/papers/2410.06985', 'abstract': 'Multi-view consistency remains a challenge for image diffusion models. Even within the Text-to-Texture problem, where perfect geometric correspondences are known a priori, many methods fail to yield aligned predictions across views, necessitating non-trivial fusion methods to incorporate the results onto the original mesh. We explore this issue for a Collaborative Control workflow specifically in PBR Text-to-Texture. Collaborative Control directly models PBR image probability distributions, including normal bump maps; to our knowledge, the only diffusion model to directly output full PBR stacks. We discuss the design decisions involved in making this model multi-view consistent, and demonstrate the effectiveness of our approach in ablation studies, as well as practical applications.', 'score': 5, 'issue_id': 41, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '0007650863977d09', 'data': {'categories': ['#diffusion', '#architecture', '#cv', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Согласованность многоракурсных представлений в диффузионных моделях для PBR текстурирования', 'desc': 'Статья посвящена проблеме согласованности представлений в моделях диффузии изображений. Авторы исследуют этот вопрос в контексте рабочего процесса Collaborative Control для задачи Text-to-Texture с PBR (Physically Based Rendering). Модель Collaborative Control напрямую моделирует распределения вероятностей PBR-изображений, включая карты нормалей. В работе обсуждаются решения по обеспечению согласованности между различными ракурсами и демонстрируется эффективность предложенного подхода.'}, 'en': {'title': 'Achieving Multi-View Harmony in PBR Text-to-Texture Models', 'desc': 'The paper addresses the challenge of achieving multi-view consistency in image diffusion models, particularly in the Text-to-Texture problem where geometric correspondences are known. It introduces a Collaborative Control workflow that directly models PBR image probability distributions, including normal bump maps, making it unique in its ability to output full PBR stacks. The authors discuss the design choices that enhance multi-view consistency and validate their approach through ablation studies and practical applications. This work aims to improve the alignment of predictions across views, reducing the need for complex fusion methods.'}, 'zh': {'title': '实现多视图一致性的PBR图像扩散模型', 'desc': '这篇论文探讨了图像扩散模型在多视图一致性方面的挑战，特别是在文本到纹理问题中。即使在几何对应关系已知的情况下，许多方法仍无法在不同视图中生成对齐的预测结果。我们研究了在PBR文本到纹理的协作控制工作流程中解决这一问题的方法。通过直接建模PBR图像概率分布，我们的模型能够直接输出完整的PBR堆栈，并在消融研究和实际应用中展示了其有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06845', 'title': 'MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders', 'url': 'https://huggingface.co/papers/2410.06845', 'abstract': 'Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main', 'score': 5, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'd89e94812dda796d', 'data': {'categories': ['#science', '#multilingual', '#training', '#healthcare', '#benchmark', '#open_source', '#architecture', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'MentalArena: ИИ-терапевт нового поколения для персонализированной диагностики', 'desc': 'Статья представляет MentalArena - фреймворк для обучения языковых моделей в области психического здоровья путем генерации персонализированных данных. Авторы разработали Symptom Encoder для моделирования поведения пациентов и Symptom Decoder для управления диалогом между пациентом и терапевтом. Модели, обученные с помощью MentalArena на GPT-3.5 и Llama-3-8b, превзошли другие современные модели на различных тестах по биомедицине и психическому здоровью. Исследование направлено на улучшение персонализированной диагностики и лечения психических расстройств.'}, 'en': {'title': 'Revolutionizing Mental Health Care with AI-Powered Personalization', 'desc': 'The paper introduces MentalArena, a self-play framework designed to train language models for diagnosing and treating mental health disorders by generating personalized data. It uses a Symptom Encoder to simulate realistic patient behavior and cognition, and a Symptom Decoder to manage dialogue by comparing diagnosed and encoded symptoms. The framework was tested against six benchmarks and outperformed advanced models like GPT-4o, showing its effectiveness in personalized mental health care. This work aims to inspire further research in personalized treatment models while addressing privacy concerns in mental health data.'}, 'zh': {'title': 'MentalArena：个性化心理健康诊疗的未来', 'desc': '这篇论文介绍了一个名为MentalArena的自我对弈框架，用于训练语言模型以生成特定领域的个性化数据，从而改善心理健康诊断和治疗。为了模拟真实的心理健康患者，研究者设计了症状编码器，从认知和行为角度进行模拟。为了解决患者与治疗师互动中的意图偏差，提出了症状解码器，以动态管理对话。实验结果表明，经过GPT-3.5和Llama-3-8b微调的模型在多项基准测试中表现优于其他先进模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06949', 'title': 'Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach', 'url': 'https://huggingface.co/papers/2410.06949', 'abstract': 'In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability.', 'score': 5, 'issue_id': 37, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '7b96c448f8692316', 'data': {'categories': ['#optimization', '#plp', '#agents', '#open_source', '#architecture'], 'emoji': '🛡️', 'ru': {'title': 'Seeker: Улучшение обработки исключений с помощью LLM и мультиагентного подхода', 'desc': 'Статья посвящена проблеме обработки исключений в разработке программного обеспечения и предлагает решение с использованием больших языковых моделей (LLM). Авторы выявили три ключевые проблемы: нечувствительное обнаружение хрупкого кода, неточный захват типов исключений и искаженные решения по обработке. Предложена система Seeker - мультиагентный фреймворк, вдохновленный стратегиями экспертов-разработчиков для обработки исключений. Seeker использует агенты для помощи LLM в более эффективном обнаружении, захвате и разрешении исключений.'}, 'en': {'title': '"Seeker: Revolutionizing Exception Handling with AI"', 'desc': 'The paper addresses the challenge of poor exception handling in software development, which affects code robustness and reliability. It highlights the use of large language models (LLMs) to improve exception handling by identifying common issues like insensitive detection, inaccurate capture, and distorted handling of exceptions. The authors propose a multi-agent framework called Seeker, which includes agents like Scanner and Handler to assist LLMs in better managing exceptions. This study is the first to systematically explore using LLMs for enhancing exception handling, offering insights for improving software quality.'}, 'zh': {'title': '用LLMs提升代码异常处理的可靠性', 'desc': '在软件开发中，不当或缺失的异常处理会影响代码的稳健性和可靠性。我们研究了使用大型语言模型（LLMs）来改善代码中的异常处理。通过分析，我们发现了三个主要问题：脆弱代码的检测不敏感、异常类型捕获不准确以及处理方案失真。为了解决这些问题，我们提出了一个名为Seeker的多代理框架，帮助LLMs更有效地检测、捕获和解决异常。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05160', 'title': 'VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks', 'url': 'https://huggingface.co/papers/2410.05160', 'abstract': "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite their importance. In this work, we aim to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks (i.e. classification, visual question answering, multimodal retrieval, and visual grounding) and 36 datasets, including 20 training and 16 evaluation datasets, and (2) VLM2Vec (Vision-Language Model -> Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model via training on MMEB. Unlike previous models such as CLIP and BLIP, VLM2Vec can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. We build a series of VLM2Vec models on Phi-3.5-V and evaluate them on MMEB's evaluation split. Our results show that \\model achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB.", 'score': 4, 'issue_id': 46, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '36a094d66cdd6a3e', 'data': {'categories': ['#training', '#graphs', '#optimization', '#transfer_learning', '#benchmark', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🔀', 'ru': {'title': 'Универсальные мультимодальные эмбеддинги: новый подход и бенчмарк', 'desc': 'Статья представляет новый подход к созданию универсальных мультимодальных эмбеддингов. Авторы предлагают MMEB - масштабный бенчмарк для оценки мультимодальных эмбеддингов, охватывающий 36 датасетов и 4 мета-задачи. Также представлен VLM2Vec - фреймворк для преобразования любой современной vision-language модели в модель эмбеддингов путем обучения на MMEB. Результаты показывают, что VLM2Vec достигает улучшения на 10-20% по сравнению с существующими мультимодальными моделями эмбеддингов.'}, 'en': {'title': 'VLM2Vec: Bridging Vision and Language for Universal Embeddings', 'desc': 'This paper introduces a new approach to creating universal multimodal embedding models that can handle a variety of tasks. The authors present MMEB, a comprehensive benchmark covering multiple tasks and datasets, and VLM2Vec, a framework that transforms vision-language models into versatile embedding models. VLM2Vec stands out by generating fixed-dimensional vectors from any combination of images and text, outperforming existing models like CLIP and BLIP. The results demonstrate significant improvements in performance across different datasets, highlighting the potential of VLM2Vec in advancing multimodal embeddings.'}, 'zh': {'title': '通用嵌入模型：跨任务的多模态处理', 'desc': '这篇论文探讨了构建通用嵌入模型的可能性，这种模型可以处理多种下游任务。作者提出了一个名为MMEB的大规模多模态嵌入基准，涵盖了四个元任务和36个数据集。论文还介绍了VLM2Vec，一种对比训练框架，可以将任何先进的视觉-语言模型转化为嵌入模型。实验结果表明，VLM2Vec在MMEB的评估中比现有的多模态嵌入模型提高了10%到20%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06524', 'title': 'Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA', 'url': 'https://huggingface.co/papers/2410.06524', 'abstract': 'Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving.', 'score': 4, 'issue_id': 38, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '5eec66236f57298a', 'data': {'categories': ['#science', '#reasoning', '#math', '#rag', '#agents', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Сравнение когнитивных способностей человека и ИИ: новый взгляд на обработку естественного языка', 'desc': 'CAIMIRA - новая система оценки способностей к решению задач у людей и ИИ в области обработки естественного языка. Исследование, основанное на анализе более 300 000 ответов от ~70 систем ИИ и 155 людей, выявило различия в профессиональных навыках в разных областях знаний. Люди превосходят ИИ в абдуктивном и концептуальном мышлении, в то время как современные языковые модели лучше справляются с извлечением информации и фактологическими рассуждениями. Результаты подчеркивают необходимость разработки задач, требующих более сложного мышления и применения знаний в различных контекстах.'}, 'en': {'title': "Bridging the Gap: Enhancing AI's Cognitive Abilities", 'desc': 'The paper introduces CAIMIRA, a framework using item response theory to evaluate the problem-solving skills of AI and human question-answering agents. By analyzing responses from both AI systems and humans, the study reveals that humans excel in knowledge-grounded abductive and conceptual reasoning. In contrast, AI systems like GPT-4 and LLaMA perform better in tasks involving targeted information retrieval and fact-based reasoning. The research suggests that future AI development should focus on enhancing higher-order reasoning and nuanced linguistic interpretation to better mimic human cognitive abilities.'}, 'zh': {'title': 'AI与人类：问答能力的较量', 'desc': '这篇论文研究了大型语言模型在自然语言处理任务中的表现，特别是与人类的比较。研究引入了一个名为CAIMIRA的新框架，基于项目反应理论，用于定量评估和比较人类和AI系统在问答任务中的解题能力。通过分析大量AI系统和人类的回答，发现人类在知识基础的推理和概念推理上表现更好，而先进的语言模型在信息检索和事实推理上更胜一筹。研究结果表明，未来的问答任务需要更关注挑战高阶推理和科学思维的问题，以推动AI更好地模拟或补充人类的认知能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05873', 'title': 'MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment', 'url': 'https://huggingface.co/papers/2410.05873', 'abstract': 'English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves a statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code: https://github.com/cisnlp/Mexa.', 'score': 3, 'issue_id': 45, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '98dd8fcc39110307', 'data': {'categories': ['#dataset', '#multilingual', '#transfer_learning', '#benchmark', '#open_source', '#architecture'], 'emoji': '🌐', 'ru': {'title': 'MEXA: Новый способ оценки многоязычности LLM через параллельные предложения', 'desc': 'MEXA - это новый метод оценки многоязычных возможностей предобученных англоцентричных языковых моделей (LLM), использующий параллельные предложения. Метод основан на том, что англоцентричные LLM используют английский как своего рода язык-посредник в промежуточных слоях. MEXA вычисляет выравнивание между английским и другими языками, чтобы оценить перенос понимания языка с английского на другие языки. Результаты показывают высокую корреляцию (0,90) с существующими задачами на нескольких моделях и наборах данных.'}, 'en': {'title': 'Unlocking Multilingual Potential in English-Centric Models', 'desc': 'The paper introduces MEXA, a method to evaluate the multilingual capabilities of English-centric large language models (LLMs) using parallel sentences. MEXA leverages the use of English as a pivot language in LLMs to assess language understanding transfer to non-English languages. The method shows a high correlation with established multilingual benchmarks, indicating its reliability in estimating model performance across languages. This approach provides insights into the multilingual potential and internal mechanisms of LLMs.'}, 'zh': {'title': 'MEXA：揭示英语中心语言模型的多语言潜力', 'desc': '这篇论文介绍了一种名为MEXA的方法，用于评估以英语为中心的大型语言模型的多语言能力。MEXA利用平行句子来计算英语与非英语语言之间的对齐，从而评估语言理解的转移效果。研究表明，MEXA在默认设置下，与三个已建立的下游任务的平均皮尔逊相关系数达到0.90，显示出其在多语言能力评估中的可靠性。通过这种方法，可以更清晰地了解这些模型的多语言潜力和内部工作机制。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07062', 'title': 'TinyEmo: Scaling down Emotional Reasoning via Metric Projection', 'url': 'https://huggingface.co/papers/2410.07062', 'abstract': 'This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the language model allowing for more efficient training and inference, (3) a multi-modal large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated framework for bias detection. TinyEmo is able to perform emotion classification and emotional reasoning, all while using substantially fewer parameters than comparable models. This efficiency allows us to freely incorporate more diverse emotional datasets, enabling strong performance on classification tasks, with our smallest model (700M parameters) outperforming larger state-of-the-art models based on general-purpose MM-LLMs with over 7B parameters. Additionally, the Metric Projector allows for interpretability and indirect bias detection in large models without additional training, offering an approach to understand and improve AI systems.   We release code, models, and dataset at https://github.com/ggcr/TinyEmo', 'score': 3, 'issue_id': 40, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '354409ec230f329b', 'data': {'categories': ['#reasoning', '#audio', '#video', '#dataset', '#cv', '#training', '#ethics', '#data', '#interpretability', '#optimization', '#benchmark', '#open_source', '#small_models', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'TinyEmo: Эффективный анализ эмоций с меньшими ресурсами', 'desc': 'TinyEmo - это семейство небольших мультимодальных языковых моделей для эмоционального анализа и классификации. Модель использует синтетический набор данных для обучения и настройки, а также включает Metric Projector для эффективной классификации. TinyEmo способна выполнять классификацию эмоций и эмоциональные рассуждения, используя значительно меньше параметров, чем сопоставимые модели. Кроме того, модель предлагает подход к пониманию и улучшению систем ИИ через интерпретируемость и косвенное обнаружение предвзятости.'}, 'en': {'title': 'TinyEmo: Small Models, Big Emotional Insights', 'desc': 'TinyEmo is a new set of small language models designed to understand and classify emotions efficiently. It uses a special dataset to train the models and a unique tool called the Metric Projector to make training faster and easier. These models are smaller but still perform better than larger models, thanks to their efficient design. They also help detect biases in AI systems, making them more reliable and fair.'}, 'zh': {'title': 'TinyEmo：小模型，大情感', 'desc': '这篇论文介绍了TinyEmo，一种用于情感推理和分类的小型多模态语言模型。TinyEmo使用合成情感指令数据集进行预训练和微调，并通过度量投影器提高训练和推理效率。该模型在使用更少参数的情况下，能够在情感分类任务中表现优异，甚至超过了一些更大规模的模型。此外，度量投影器还提供了模型可解释性和偏差检测的能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07112', 'title': 'VHELM: A Holistic Evaluation of Vision Language Models', 'url': 'https://huggingface.co/papers/2410.07112', 'abstract': 'Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: visual perception, knowledge, reasoning, bias, fairness, multilinguality, robustness, toxicity, and safety. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website (https://crfm.stanford.edu/helm/vhelm/v2.0.1). VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.', 'score': 2, 'issue_id': 47, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'f9db5fd76663d260', 'data': {'categories': ['#reasoning', '#survey', '#cv', '#security', '#multilingual', '#inference', '#ethics', '#benchmark', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'VHELM: Комплексная оценка мультимодальных моделей по 9 ключевым аспектам', 'desc': 'VHELM - это расширение фреймворка HELM для комплексной оценки моделей компьютерного зрения и языка (VLM). Он охватывает 9 аспектов, включая визуальное восприятие, знания, рассуждения, предвзятость, справедливость, мультиязычность, надежность, токсичность и безопасность. VHELM стандартизирует параметры вывода, методы промптинга и метрики оценки для обеспечения справедливых сравнений между моделями. Исследование выявило интересные результаты, например, что модели, ориентированные на эффективность, значительно хуже справляются с тестами на предвзятость.'}, 'en': {'title': '"VHELM: A New Standard for Fair and Comprehensive VLM Evaluation"', 'desc': 'The paper introduces VHELM, a comprehensive framework for evaluating vision-language models (VLMs) across nine critical aspects, including fairness, multilinguality, and toxicity. By standardizing evaluation procedures and metrics, VHELM allows for fair comparisons between different models. The initial evaluation of 22 VLMs reveals that efficiency-focused models perform worse on bias benchmarks compared to their full counterparts. VHELM aims to be an evolving benchmark, continuously incorporating new datasets and models to provide a holistic view of VLM capabilities.'}, 'zh': {'title': 'VHELM：全面评估视觉语言模型的新标准', 'desc': '这篇论文提出了一种新的评估框架VHELM，用于全面评估视觉语言模型（VLMs）的多方面能力。VHELM通过整合多个数据集，涵盖视觉感知、知识、推理、公平性、多语言性、鲁棒性、毒性和安全性等九个方面。该框架标准化了推理参数、提示方法和评估指标，使得模型之间的比较更加公平。初步评估显示，注重效率的模型在偏见测试中表现不佳，但在其他方面没有明显差距。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07145', 'title': 'Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling', 'url': 'https://huggingface.co/papers/2410.07145', 'abstract': "One essential advantage of recurrent neural networks (RNNs) over transformer-based language models is their linear computational complexity concerning the sequence length, which makes them much faster in handling long sequences during inference. However, most publicly available RNNs (e.g., Mamba and RWKV) are trained on sequences with less than 10K tokens, and their effectiveness in longer contexts remains largely unsatisfying so far. In this paper, we study the cause of the inability to process long context for RNNs and suggest critical mitigations. We examine two practical concerns when applying state-of-the-art RNNs to long contexts: (1) the inability to extrapolate to inputs longer than the training length and (2) the upper bound of memory capacity. Addressing the first concern, we first investigate *state collapse* (SC), a phenomenon that causes severe performance degradation on sequence lengths not encountered during training. With controlled experiments, we attribute this to overfitting due to the recurrent state being overparameterized for the training length. For the second concern, we train a series of Mamba-2 models on long documents to empirically estimate the recurrent state capacity in language modeling and passkey retrieval. Then, three SC mitigation methods are proposed to improve Mamba-2's length generalizability, allowing the model to process more than 1M tokens without SC. We also find that the recurrent state capacity in passkey retrieval scales exponentially to the state size, and we empirically train a Mamba-2 370M with near-perfect passkey retrieval accuracy on 256K context length. This suggests a promising future for RNN-based long-context modeling.", 'score': 2, 'issue_id': 46, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '955fb73ec92505eb', 'data': {'categories': ['#reasoning', '#long_context', '#training', '#inference', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'RNN покоряют длинные последовательности: прорыв в обработке контекста в миллион токенов', 'desc': "В статье исследуются причины неспособности рекуррентных нейронных сетей (RNN) обрабатывать длинные последовательности и предлагаются способы решения этой проблемы. Авторы выявляют явление 'схлопывания состояния' (state collapse), вызывающее резкое падение производительности на последовательностях длиннее тренировочных. Предложены три метода для смягчения этого эффекта, позволяющие модели Mamba-2 обрабатывать более миллиона токенов без схлопывания. Также эмпирически показано, что емкость рекуррентного состояния для задачи извлечения паролей масштабируется экспоненциально с размером состояния."}, 'en': {'title': 'Unlocking RNNs for Long Sequences: Overcoming State Collapse', 'desc': "Recurrent neural networks (RNNs) are faster than transformer models for long sequences due to their linear computational complexity, but they struggle with sequences longer than those they were trained on. This paper identifies two main issues: the inability to handle longer sequences than trained on, and limited memory capacity. The authors explore 'state collapse,' where performance drops for longer sequences, and attribute it to overfitting. They propose three methods to improve RNNs' ability to handle longer sequences, showing that RNNs can be effective for long-context tasks with proper adjustments."}, 'zh': {'title': 'RNN长序列处理的未来潜力', 'desc': '递归神经网络（RNN）在处理长序列时比基于Transformer的语言模型更快，因为它们的计算复杂度与序列长度呈线性关系。然而，大多数公开的RNN在处理超过1万标记的长序列时效果不佳。本文研究了RNN在长上下文处理中的不足，并提出了关键的改进措施。通过实验，我们发现过拟合和状态崩溃是主要问题，并提出了三种方法来提高Mamba-2模型的长度泛化能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06468', 'title': 'Does Spatial Cognition Emerge in Frontier Models?', 'url': 'https://huggingface.co/papers/2410.06468', 'abstract': 'Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-scale reasoning about object shapes and layouts, and cognitive infrastructure such as spatial attention and memory. For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models. Results suggest that contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on a number of classic tests of animal cognition.', 'score': 2, 'issue_id': 42, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'b568df3a7211e7fa', 'data': {'categories': ['#science', '#reasoning', '#cv', '#benchmark', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'SPACE: новый рубеж в оценке пространственного интеллекта ИИ', 'desc': 'SPACE - это новый бенчмарк для оценки пространственного мышления у передовых моделей искусственного интеллекта. Он основан на многолетних исследованиях в когнитивной науке и оценивает способности к крупномасштабному картографированию, мелкомасштабному рассуждению о формах и расположении объектов, а также когнитивную инфраструктуру, такую как пространственное внимание и память. Бенчмарк позволяет тестировать как языковые, так и мультимодальные модели. Результаты показывают, что современные модели значительно уступают животным в пространственном интеллекте.'}, 'en': {'title': "Bridging the Gap: Evaluating AI's Spatial Smarts", 'desc': "The paper introduces SPACE, a benchmark designed to assess spatial cognition in advanced machine learning models. It draws from cognitive science to evaluate models' abilities in large-scale mapping, object shape reasoning, and spatial attention and memory. The benchmark uses both text and images to test large language and multimodal models. Findings indicate that current models struggle with spatial tasks, performing poorly compared to animals."}, 'zh': {'title': 'SPACE：评估模型空间认知的新基准', 'desc': '这篇论文介绍了一个名为SPACE的基准，用于系统地评估前沿模型的空间认知能力。该基准基于认知科学的多年研究，评估模型在大规模地图构建、小规模物体形状和布局推理以及空间注意力和记忆等方面的能力。通过文本和图像的平行展示，SPACE可以同时评估大型语言模型和多模态模型。结果表明，当前的前沿模型在空间智能方面仍不如动物，在许多经典的动物认知测试中表现接近随机水平。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07484', 'title': 'WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents', 'url': 'https://huggingface.co/papers/2410.07484', 'abstract': 'Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment\'s dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such "world alignment" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent "WALL-E" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E\'s reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8-20 fewer replanning rounds and only 60-80% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations.', 'score': 48, 'issue_id': 59, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '6e6aca942958f302', 'data': {'categories': ['#reasoning', '#agi', '#rl', '#agents', '#alignment', '#games', '#architecture', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'LLM как модель мира: эффективное обучение правилам для согласования с окружающей средой', 'desc': 'Статья исследует возможность использования больших языковых моделей (LLM) в качестве мощных моделей мира для агентов на основе модели. Авторы предлагают нейросимволический подход для обучения правил, позволяющих согласовать предсказания LLM с динамикой конкретной среды. Разработанный агент WALL-E использует модельно-прогнозирующее управление (MPC) для оптимизации действий. В экспериментах в Minecraft и ALFWorld WALL-E демонстрирует более высокую эффективность по сравнению с существующими методами.'}, 'en': {'title': 'Aligning LLMs with the World: A New Era of Efficient Exploration', 'desc': 'The paper explores the potential of large language models (LLMs) to serve as effective world models for model-based agents by aligning them with specific environment dynamics through rule learning. This alignment is achieved using a neurosymbolic approach that involves inducing, updating, and pruning rules based on comparisons between agent-explored trajectories and LLM predictions. The resulting model, which combines the LLM with learned rules, enhances exploration and learning efficiency, as demonstrated by the WALL-E agent in open-world challenges like Minecraft and ALFWorld. WALL-E outperforms existing methods by achieving higher success rates with fewer resources, such as replanning time and tokens, required for reasoning.'}, 'zh': {'title': '大型语言模型：智能体世界模型的新可能', 'desc': '这篇论文探讨了大型语言模型（LLM）是否可以直接作为基于模型的智能体的强大世界模型。研究发现，通过规则学习，可以有效地将LLM与其部署环境对齐，从而弥合知识差距。作者提出了一种神经符号方法，通过比较智能体探索轨迹和世界模型预测来学习规则。最终的世界模型由LLM和学习到的规则组成，在Minecraft和ALFWorld等开放世界挑战中表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08196', 'title': 'MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code', 'url': 'https://huggingface.co/papers/2410.08196', 'abstract': 'Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline. The code is released at https://github.com/mathllm/MathCoder2 .', 'score': 44, 'issue_id': 52, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '0ef94d1fd5147144', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#math', '#data', '#plp', '#open_source', '#synthetic'], 'emoji': '🧮', 'ru': {'title': 'Улучшение математических способностей языковых моделей с помощью кода', 'desc': 'Статья представляет новый метод генерации математического кода с соответствующими шагами рассуждений для дальнейшего предобучения языковых моделей. Авторы создали высококачественный датасет MathCode-Pile объемом 19.2 млрд токенов, включающий математические веб-данные, код с использованием математических библиотек, учебники и синтетические данные. Метод включает извлечение выражений LaTeX, необходимых условий и результатов из собранных данных, а затем генерацию соответствующего кода. Обучение популярных базовых моделей на этом корпусе значительно улучшило их математические способности.'}, 'en': {'title': '"Code Your Way to Better Math Reasoning!"', 'desc': 'This paper presents a new method for improving the mathematical reasoning skills of large language models by generating mathematical code with reasoning steps. The authors create a high-quality dataset called MathCode-Pile, which includes math-related web data, code, textbooks, and synthetic data. They extract LaTeX expressions and conditions from this dataset to generate code that accurately reflects mathematical reasoning. Training models with this dataset significantly enhances their mathematical abilities, resulting in the MathCoder2 family of models.'}, 'zh': {'title': '代码与推理：提升语言模型的数学能力', 'desc': '这篇论文介绍了一种新的方法，通过生成数学代码和相应的推理步骤来增强大语言模型的数学推理能力。研究人员构建了一个高质量的数学预训练数据集，结合了数学相关的网络数据、使用数学包的代码、数学教科书和合成数据。通过提取LaTeX表达式、条件和结果，生成相应的代码来准确捕捉数学推理过程。最终，使用这个数据集训练的模型显著提高了数学能力，形成了MathCoder2模型家族。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03450', 'title': 'MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents', 'url': 'https://huggingface.co/papers/2410.03450', 'abstract': "MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MLLM as ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritize them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs' summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All benchmark task sets and simulator code modifications for action and observation spaces will be released.", 'score': 32, 'issue_id': 51, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': 'db11faae830e2807', 'data': {'categories': ['#reasoning', '#training', '#open_source', '#optimization', '#agents', '#benchmark', '#games', '#transfer_learning', '#architecture', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'MART: Улучшение воплощенных агентов через тонкую настройку мультимодальных ретриверов', 'desc': 'Статья представляет новый метод MART для улучшения работы воплощенных агентов с использованием мультимодальных языковых моделей (MLLM). MART использует данные взаимодействия для тонкой настройки MLLM-ретривера на основе обучения с предпочтениями, что позволяет эффективно оценивать и приоритезировать траектории для новых задач. Также вводится механизм абстракции траекторий, использующий возможности MLLM по суммаризации для лучшего понимания ключевых этапов. Эксперименты показывают значительное улучшение успешности выполнения задач в новых сценах по сравнению с базовыми методами.'}, 'en': {'title': "Revolutionizing Task Success: MART's Smart Trajectory Retrieval", 'desc': 'This paper introduces a new method called MLLM as ReTriever (MART) to improve the performance of embodied agents in complex tasks. MART fine-tunes a machine learning language model (MLLM) to retrieve task-relevant trajectory data by focusing on the effectiveness of the trajectories rather than just surface-level similarities. The method also uses Trajectory Abstraction to summarize trajectories with fewer tokens, helping agents understand key milestones. Experimental results show that MART significantly enhances task success rates in new environments compared to existing methods.'}, 'zh': {'title': '通过MART方法提升具身智能体的任务成功率', 'desc': '这篇论文提出了一种新方法，称为MLLM作为检索器（MART），通过利用交互数据来微调MLLM检索器，以提高具身智能体在复杂任务中的表现。传统的检索方法主要关注轨迹的表面相似性，而忽略了其在特定任务中的有效性。MART方法通过偏好学习来优化检索器，使其能够优先考虑未见任务中轨迹的有效性。实验结果表明，该方法在不同环境中显著提高了任务成功率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05265', 'title': 'PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs', 'url': 'https://huggingface.co/papers/2410.05265', 'abstract': 'Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot models by 1.2x to 1.3x. Our code is available at https://github.com/ChenMnZ/PrefixQuant.', 'score': 29, 'issue_id': 51, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'a083aa646dcae575', 'data': {'categories': ['#open_source', '#optimization', '#architecture', '#inference'], 'emoji': '🗜️', 'ru': {'title': 'PrefixQuant: Эффективное статическое квантование LLM без потери качества', 'desc': 'PrefixQuant - это новый метод квантования больших языковых моделей (LLM), который изолирует выбросы на уровне токенов офлайн без переобучения. Он префиксирует часто встречающиеся токены-выбросы в KV-кэше, что позволяет применять эффективное статическое квантование на уровне тензоров. PrefixQuant превосходит существующие методы динамического квантования по производительности и скорости вывода. Метод демонстрирует значительные улучшения в перплексии и точности на различных задачах по сравнению с предыдущими подходами.'}, 'en': {'title': 'PrefixQuant: Revolutionizing LLM Quantization with Token-Wise Precision', 'desc': 'The paper introduces PrefixQuant, a new method for quantizing large language models that focuses on token-wise outliers, which are often overlooked by existing methods. PrefixQuant works by identifying and managing high-frequency outlier tokens offline, thus avoiding the need for expensive per-token dynamic quantization during inference. This approach allows for efficient per-tensor static quantization, which not only improves memory efficiency and inference speed but also enhances model performance. The results show that PrefixQuant significantly outperforms previous methods in terms of perplexity and accuracy while being faster than traditional FP16 models.'}, 'zh': {'title': 'PrefixQuant：高效静态量化的新突破', 'desc': '量化对于部署大型语言模型至关重要，因为它可以提高内存效率和推理速度。现有的激活量化方法主要解决通道级异常值的问题，往往忽略了令牌级异常值，从而依赖于昂贵的每令牌动态量化。PrefixQuant是一种新技术，可以在不重新训练的情况下离线隔离异常令牌。它通过在KV缓存中前缀高频异常令牌，防止推理过程中生成异常令牌，从而简化量化过程。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07869', 'title': 'Benchmarking Agentic Workflow Generation', 'url': 'https://huggingface.co/papers/2410.07869', 'abstract': "Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorFBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorFEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference. Code and dataset will be available at https://github.com/zjunlp/WorFBench.", 'score': 25, 'issue_id': 54, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'ce1da2a3c48bcb17', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#graphs', '#inference', '#agents', '#benchmark', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'WorFBench: новый стандарт оценки способностей ЯМ в генерации рабочих процессов', 'desc': 'Статья представляет WorFBench - новый бенчмарк для оценки способности языковых моделей генерировать рабочие процессы. Авторы также предлагают WorFEval - протокол оценки, использующий алгоритмы сопоставления подпоследовательностей и подграфов. Исследование выявило значительный разрыв между способностями моделей в планировании последовательностей и графов. Результаты показывают, что сгенерированные рабочие процессы могут улучшить выполнение последующих задач.'}, 'en': {'title': 'Bridging the Workflow Gap in Large Language Models', 'desc': "The paper introduces WorFBench, a benchmark designed to evaluate the workflow generation capabilities of Large Language Models (LLMs) across diverse scenarios and complex graph structures. It also presents WorFEval, an evaluation protocol that uses subsequence and subgraph matching to assess the accuracy of these workflows. The study reveals a significant gap in the sequence and graph planning abilities of LLMs, including a 15% gap in GPT-4's performance. Additionally, the research shows that the generated workflows can improve the efficiency and effectiveness of downstream tasks."}, 'zh': {'title': 'WorFBench：提升LLM工作流程生成能力的基准', 'desc': '大型语言模型（LLMs）在处理推理和规划任务方面表现出色，能够将复杂问题分解为可执行的工作流程。现有的工作流程评估框架存在场景覆盖有限、结构简单和评估标准松散等问题。为此，我们引入了WorFBench，一个统一的工作流程生成基准，具有多方面的场景和复杂的图形工作流程结构。通过对不同类型的LLMs进行评估，我们发现序列规划能力和图形规划能力之间存在显著差距，甚至GPT-4也存在约15%的差距。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08164', 'title': 'Agent S: An Open Agentic Framework that Uses Computers Like a Human', 'url': 'https://huggingface.co/papers/2410.08164', 'abstract': 'We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S.', 'score': 24, 'issue_id': 52, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'ccbad559d2898387', 'data': {'categories': ['#reasoning', '#graphs', '#agents', '#benchmark', '#transfer_learning', '#open_source', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Agent S: Автономный помощник для управления компьютером', 'desc': 'Agent S - это открытая агентная система, предназначенная для автономного взаимодействия с компьютерами через графический интерфейс пользователя. Система решает три ключевые проблемы автоматизации компьютерных задач: получение специфических знаний, планирование долгосрочных задач и работа с динамическими интерфейсами. Agent S использует иерархическое планирование, усиленное опытом, и специальный интерфейс Agent-Computer Interface для улучшения взаимодействия с GUI на основе мультимодальных больших языковых моделей.'}, 'en': {'title': 'Agent S: Revolutionizing Task Automation with Intelligent GUI Interaction', 'desc': 'Agent S is a framework designed to automate complex tasks on computers by interacting with their graphical interfaces. It tackles challenges like acquiring specific knowledge, planning long tasks, and managing changing interfaces using experience-augmented hierarchical planning. This approach combines learning from external searches and internal experiences to improve task execution. The framework shows significant improvements in task success rates and generalizes well across different operating systems.'}, 'zh': {'title': 'Agent S：自动化人机交互的未来', 'desc': 'Agent S 是一个开放的代理框架，旨在通过图形用户界面实现计算机的自主交互，自动化复杂的多步骤任务。它通过引入经验增强的层次规划，解决了获取领域知识、长任务规划和处理动态界面等挑战。Agent S 使用多模态大语言模型来提升 GUI 代理的推理和控制能力。在 OSWorld 基准测试中，Agent S 的成功率比基线高出 9.37%，并在 WindowsAgentArena 基准中展示了广泛的通用性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08159', 'title': 'DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2410.08159', 'abstract': 'Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process that gradually adds noise to the input. We argue that the Markovian property limits the models ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model with the same architecture as standard language models. DART does not rely on image quantization, enabling more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis.', 'score': 23, 'issue_id': 57, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '40e6937b715fa538', 'data': {'categories': ['#cv', '#optimization', '#transfer_learning', '#benchmark', '#diffusion', '#architecture', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'DART: Объединение авторегрессии и диффузии для эффективной генерации изображений', 'desc': 'DART - это новый подход к генерации изображений, объединяющий авторегрессионные модели и диффузию в рамках немарковского процесса. Модель использует трансформер для итеративного шумоподавления патчей изображения как в пространственном, так и в спектральном измерении. DART не требует квантизации изображений и может обучаться одновременно на текстовых и визуальных данных. Эксперименты показывают конкурентоспособность DART в задачах генерации изображений по классу и текстовому описанию.'}, 'en': {'title': 'DART: Revolutionizing Image Generation with Unified AR and Diffusion', 'desc': 'The paper introduces DART, a new model for generating images that combines autoregressive and diffusion techniques in a non-Markovian framework. Unlike traditional diffusion models, DART does not rely on a step-by-step noise addition process, allowing it to use the generation path more efficiently. By using a transformer-based architecture similar to language models, DART can denoise image patches without needing image quantization, improving image quality and flexibility. The model also supports training with both text and image data, achieving strong results in generating images from text descriptions.'}, 'zh': {'title': 'DART：突破扩散模型的高效图像生成新标杆', 'desc': '扩散模型是目前视觉生成的主流方法，但其马尔可夫性质限制了模型充分利用生成轨迹，导致训练和推理效率低下。本文提出了一种名为DART的模型，它结合了自回归和扩散方法，采用非马尔可夫框架。DART通过自回归模型在空间和光谱上迭代去噪图像块，不依赖图像量化，从而提高了图像建模的效果。DART还可以在统一模型中同时训练文本和图像数据，展示了在类条件和文本到图像生成任务中的竞争力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08207', 'title': 'DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models', 'url': 'https://huggingface.co/papers/2410.08207', 'abstract': 'Discrete diffusion models have achieved success in tasks like image generation and masked language modeling but face limitations in controlled content editing. We introduce DICE (Discrete Inversion for Controllable Editing), the first approach to enable precise inversion for discrete diffusion models, including multinomial diffusion and masked generative models. By recording noise sequences and masking patterns during the reverse diffusion process, DICE enables accurate reconstruction and flexible editing of discrete data without the need for predefined masks or attention manipulation. We demonstrate the effectiveness of DICE across both image and text domains, evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results show that DICE preserves high data fidelity while enhancing editing capabilities, offering new opportunities for fine-grained content manipulation in discrete spaces. For project webpage, see https://hexiaoxiao-cs.github.io/DICE/.', 'score': 18, 'issue_id': 50, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '49494862b22f09dc', 'data': {'categories': ['#cv', '#training', '#diffusion', '#architecture', '#synthetic'], 'emoji': '🎛️', 'ru': {'title': 'DICE: Точная инверсия для гибкого редактирования дискретных данных', 'desc': 'DICE (Discrete Inversion for Controllable Editing) - это новый подход к точной инверсии для дискретных диффузионных моделей. Он позволяет осуществлять точную реконструкцию и гибкое редактирование дискретных данных без необходимости в предопределенных масках или манипуляциях с вниманием. DICE работает с моделями как в области изображений, так и текста, включая VQ-Diffusion, Paella и RoBERTa. Метод сохраняет высокую точность данных при улучшении возможностей редактирования.'}, 'en': {'title': 'DICE: Precision Editing in Discrete Diffusion Models', 'desc': 'The paper introduces DICE, a novel method for precise content editing in discrete diffusion models, which are used in tasks like image generation and language modeling. DICE records noise sequences and masking patterns during the reverse diffusion process, allowing for accurate data reconstruction and flexible editing without predefined masks. This approach is tested on models like VQ-Diffusion and RoBERTa, showing that it maintains high data fidelity while improving editing capabilities. DICE opens up new possibilities for detailed content manipulation in discrete data spaces.'}, 'zh': {'title': 'DICE：离散扩散模型的精确可控编辑', 'desc': '这篇论文介绍了一种名为DICE的新方法，用于在离散扩散模型中实现可控编辑。DICE通过记录噪声序列和掩码模式，实现了对离散数据的精确重建和灵活编辑。与传统方法不同，DICE不需要预定义的掩码或注意力操作。实验表明，DICE在图像和文本领域都能保持高数据保真度，同时增强编辑能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04751', 'title': 'Intriguing Properties of Large Language and Vision Models', 'url': 'https://huggingface.co/papers/2410.04751', 'abstract': "Recently, large language and vision models (LLVMs) have received significant attention and development efforts due to their remarkable generalization performance across a wide range of tasks requiring perception and cognitive abilities. A key factor behind their success is their simple architecture, which consists of a vision encoder, a projector, and a large language model (LLM). Despite their achievements in advanced reasoning tasks, their performance on fundamental perception-related tasks (e.g., MMVP) remains surprisingly low. This discrepancy raises the question of how LLVMs truly perceive images and exploit the advantages of the vision encoder. To address this, we systematically investigate this question regarding several aspects: permutation invariance, robustness, math reasoning, alignment preserving and importance, by evaluating the most common LLVM's families (i.e., LLaVA) across 10 evaluation benchmarks. Our extensive experiments reveal several intriguing properties of current LLVMs: (1) they internally process the image in a global manner, even when the order of visual patch sequences is randomly permuted; (2) they are sometimes able to solve math problems without fully perceiving detailed numerical information; (3) the cross-modal alignment is overfitted to complex reasoning tasks, thereby, causing them to lose some of the original perceptual capabilities of their vision encoder; (4) the representation space in the lower layers (<25%) plays a crucial role in determining performance and enhancing visual understanding. Lastly, based on the above observations, we suggest potential future directions for building better LLVMs and constructing more challenging evaluation benchmarks.", 'score': 16, 'issue_id': 52, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '4712ab5ada7bb4c9', 'data': {'categories': ['#reasoning', '#cv', '#math', '#interpretability', '#benchmark', '#alignment', '#architecture', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая тайны восприятия в крупных языково-визуальных моделях', 'desc': 'В статье исследуются крупные языковые и визуальные модели (LLVM) с точки зрения их восприятия изображений. Авторы проводят систематический анализ различных аспектов работы LLVM, включая инвариантность к перестановкам, устойчивость, математические рассуждения и сохранение выравнивания. Эксперименты показывают, что LLVM обрабатывают изображения глобально, могут решать математические задачи без полного восприятия числовой информации, и что нижние слои модели играют ключевую роль в определении производительности. На основе полученных результатов предлагаются направления для улучшения LLVM и создания более сложных тестовых наборов.'}, 'en': {'title': 'Bridging the Gap: Enhancing Perception in Large Language and Vision Models', 'desc': 'The paper explores the performance of large language and vision models (LLVMs) in perception-related tasks, revealing a gap between their advanced reasoning abilities and basic perceptual skills. It highlights that LLVMs process images globally, sometimes solving math problems without detailed perception, and that their cross-modal alignment may compromise original perceptual capabilities. The study finds that the lower layers of these models are crucial for visual understanding, suggesting that improvements in these areas could enhance performance. The authors propose future research directions to develop more effective LLVMs and create more challenging evaluation benchmarks.'}, 'zh': {'title': '探索大型语言与视觉模型的感知奥秘', 'desc': '大型语言与视觉模型（LLVMs）在许多需要感知和认知能力的任务中表现出色，但在基本感知任务上的表现却不尽如人意。研究发现，这些模型在处理图像时具有全局处理的特性，即使视觉片段顺序被打乱，它们仍能保持一定的理解能力。此外，它们在解决数学问题时，有时不需要完全感知详细的数据信息。研究还指出，模型的跨模态对齐过度适应复杂推理任务，导致其视觉编码器的原始感知能力有所下降。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07303', 'title': 'Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow', 'url': 'https://huggingface.co/papers/2410.07303', 'abstract': 'Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components include: 1) using the diffusion form of flow-matching, 2) employing boldsymbol v-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using a pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is a specific case of flow-matching models. The more critical training target is to achieve a first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our method on Stable Diffusion v1-5 and Stable Diffusion XL. Our method not only greatly simplifies the training procedure of rectified flow-based previous works (e.g., InstaFlow) but also achieves superior performance with even lower training cost. Our code is available at https://github.com/G-U-N/Rectified-Diffusion.', 'score': 16, 'issue_id': 51, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '08c053399c56f831', 'data': {'categories': ['#cv', '#training', '#optimization', '#open_source', '#diffusion', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Ускорение диффузионных моделей: новый взгляд на ректификацию', 'desc': 'Статья представляет новый метод под названием Rectified Diffusion, который улучшает скорость генерации изображений в диффузионных моделях. Авторы утверждают, что успех ректификации основан на использовании предобученной диффузионной модели для получения сопоставленных пар шума и образцов. Метод обобщает пространство проектирования и область применения ректификации на более широкую категорию диффузионных моделей. Rectified Diffusion упрощает процедуру обучения и достигает лучшей производительности при меньших затратах на обучение.'}, 'en': {'title': 'Speed Up and Simplify: Revolutionizing Image Generation with Rectified Diffusion', 'desc': "The paper discusses how diffusion models, which are used for generating images, can be slow because they require solving complex equations. The authors propose a new method called Rectified Diffusion, which simplifies the process by using a pretrained model to match noise with samples, making the training faster and more efficient. They argue that previous methods focused too much on making the path straight, but the real goal should be to create a path that naturally fits the model's needs. Their approach not only speeds up the process but also improves the quality of the generated images while reducing training costs."}, 'zh': {'title': '校正扩散：简化训练，提升性能', 'desc': '扩散模型在视觉生成方面有很大提升，但生成速度较慢。本文提出了一种称为“校正扩散”的方法，通过使用预训练的扩散模型来匹配噪声和样本对，从而简化训练过程。我们发现，直线化并不是校正的必要目标，关键在于实现一阶近似的ODE路径。我们的方法在稳定扩散模型上验证，表现优异且训练成本更低。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06154', 'title': 'GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models', 'url': 'https://huggingface.co/papers/2410.06154', 'abstract': 'In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Langugage Models (VLMs) to enhance downstream vision tasks. Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). These prompts are ranked according to a purity measure obtained through a fitness function. In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of text prompts preferred by the downstream VLM. Furthermore, we also explicitly steer the LLM generation process in each optimization step by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step. This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models -- showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models.', 'score': 15, 'issue_id': 54, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '1b375d9257a67241', 'data': {'categories': ['#dataset', '#cv', '#optimization', '#benchmark', '#vision', '#architecture'], 'emoji': '🔮', 'ru': {'title': 'GLOV: LLM как оптимизатор для улучшения задач компьютерного зрения', 'desc': 'Предложен новый метод GLOV, позволяющий использовать большие языковые модели (LLM) в качестве неявных оптимизаторов для визуально-языковых моделей (VLM) с целью улучшения задач компьютерного зрения. GLOV использует мета-промпты для LLM с описанием целевой задачи, запрашивая подходящие промпты для VLM. Эти промпты ранжируются по мере чистоты, полученной с помощью функции пригодности. Метод также включает явное управление процессом генерации LLM путем добавления вектора смещения, основанного на предыдущих решениях.'}, 'en': {'title': 'GLOV: Boosting Vision-Language Models with Smart Language Prompts', 'desc': "The paper introduces GLOV, a method that uses Large Language Models (LLMs) as implicit optimizers to improve Vision-Language Models (VLMs) for vision tasks. GLOV works by generating and ranking prompts for VLMs, using a fitness function to measure their effectiveness. The method also adjusts the LLM's output by incorporating an offset vector, which guides the language generation towards what the VLM prefers. This approach significantly boosts the performance of VLMs on various datasets, achieving up to 57.5% improvement in recognition tasks."}, 'zh': {'title': 'GLOV：提升视觉任务性能的隐式优化器', 'desc': '这篇论文提出了一种新方法GLOV，使大型语言模型（LLM）可以作为视觉语言模型（VLM）的隐式优化器，以增强视觉任务。GLOV通过元提示向LLM提供任务描述，并查询适合的VLM提示，这些提示根据纯度测量进行排名。在每个优化步骤中，排名的提示作为上下文示例输入LLM，以帮助其了解VLM偏好的文本提示类型。此外，通过在每个优化步骤中添加偏移向量来引导LLM生成过程，从而提高下游视觉任务的性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08151', 'title': 'Progressive Autoregressive Video Diffusion Models', 'url': 'https://huggingface.co/papers/2410.08151', 'abstract': 'Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. In this work, we show that existing models can be naturally extended to autoregressive video diffusion models without changing the architectures. Our key idea is to assign the latent frames with progressively increasing noise levels rather than a single noise level, which allows for fine-grained condition among the latents and large overlaps between the attention windows. Such progressive video denoising allows our models to autoregressively generate video frames without quality degradation or abrupt scene changes. We present state-of-the-art results on long video generation at 1 minute (1440 frames at 24 FPS). Videos from this paper are available at https://desaixie.github.io/pa-vdm/.', 'score': 15, 'issue_id': 51, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '599024cc2a308da1', 'data': {'categories': ['#video', '#long_context', '#training', '#diffusion', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Прорыв в генерации длинных видео: минута качественного контента', 'desc': 'Статья представляет новый подход к генерации длинных видео с использованием авторегрессивных моделей диффузии. Авторы предлагают применять прогрессивно увеличивающиеся уровни шума к латентным кадрам, что позволяет создавать более тонкие связи между ними. Этот метод позволяет генерировать видео длительностью до 1 минуты (1440 кадров при 24 FPS) без ухудшения качества или резких изменений сцены. Результаты демонстрируют передовой уровень в области генерации длинных видео.'}, 'en': {'title': 'Breaking Barriers: Extending Video Diffusion to One Minute', 'desc': 'The paper introduces a method to extend current video diffusion models to generate longer videos by using an autoregressive approach. By assigning progressively increasing noise levels to latent frames, the model can maintain high-quality video generation without abrupt scene changes. This technique allows for fine-grained conditioning among latent frames and large overlaps in attention windows, enabling the generation of videos up to one minute long. The results demonstrate state-of-the-art performance in long video generation, overcoming previous limitations of short clip production.'}, 'zh': {'title': '逐步去噪：长视频生成的新突破', 'desc': '这篇论文探讨了如何将现有的视频扩散模型扩展为自回归视频扩散模型，而无需改变其架构。作者提出了一种关键方法，即为潜在帧分配逐步增加的噪声水平，而不是单一的噪声水平。这种逐步的视频去噪方法使得模型能够自回归地生成视频帧，而不会出现质量下降或场景突变。实验结果表明，该方法在长视频生成方面达到了最先进的水平，能够生成长达一分钟的视频。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05603', 'title': 'Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition', 'url': 'https://huggingface.co/papers/2410.05603', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term "task superposition". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of "LLMs as superposition of simulators", and raise questions about the mechanisms enabling simultaneous task execution.', 'score': 11, 'issue_id': 57, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'a6c2dccffed8ffb4', 'data': {'categories': ['#reasoning', '#training', '#inference', '#interpretability', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Суперпозиция задач: скрытая сила языковых моделей', 'desc': "Исследование демонстрирует удивительную способность больших языковых моделей (LLM) выполнять несколько вычислительно различных задач обучения в контексте одновременно, названную 'суперпозицией задач'. Это явление наблюдается в различных семействах LLM и масштабах, даже если модель обучена решать задачи по одной. Теоретически показано, что эта способность находится в пределах выразительной мощности трансформеров. Исследование также раскрывает, как LLM внутренне составляют векторы задач во время суперпозиции."}, 'en': {'title': 'Unleashing the Power of Task Superposition in LLMs', 'desc': "This paper investigates a unique ability of Large Language Models (LLMs) called 'task superposition', where they can handle multiple distinct in-context learning tasks at once during a single inference. The authors provide evidence that this capability is present across different LLM families and sizes, even when models are trained to learn one task at a time. They explain that this ability is possible due to the expressive power of transformers, and they explore how LLMs internally manage task vectors during superposition. The study also finds that larger models can perform more tasks simultaneously and better adjust their output, offering new insights into the potential of LLMs."}, 'zh': {'title': '探索大型语言模型的任务叠加能力', 'desc': '大型语言模型（LLMs）在上下文学习中表现出色，能够在一次推理中同时执行多种不同的任务，这种能力被称为“任务叠加”。研究表明，即使模型被训练为一次只学习一个任务，这种现象仍然会出现。理论上，这种能力在变压器的表达能力范围内。更大的模型可以同时解决更多的任务，并更好地校准其输出分布。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05210', 'title': 'Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality', 'url': 'https://huggingface.co/papers/2410.05210', 'abstract': "In this paper, we propose a new method to enhance compositional understanding in pre-trained vision and language models (VLMs) without sacrificing performance in zero-shot multi-modal tasks. Traditional fine-tuning approaches often improve compositional reasoning at the cost of degrading multi-modal capabilities, primarily due to the use of global hard negative (HN) loss, which contrasts global representations of images and texts. This global HN loss pushes HN texts that are highly similar to the original ones, damaging the model's multi-modal representations. To overcome this limitation, we propose Fine-grained Selective Calibrated CLIP (FSC-CLIP), which integrates local hard negative loss and selective calibrated regularization. These innovations provide fine-grained negative supervision while preserving the model's representational integrity. Our extensive evaluations across diverse benchmarks for both compositionality and multi-modal tasks show that FSC-CLIP not only achieves compositionality on par with state-of-the-art models but also retains strong multi-modal capabilities. Code is available at: https://github.com/ytaek-oh/fsc-clip.", 'score': 10, 'issue_id': 50, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': 'aec4ac6f174a5a13', 'data': {'categories': ['#reasoning', '#cv', '#training', '#optimization', '#benchmark', '#open_source', '#architecture', '#multimodal'], 'emoji': '🧩', 'ru': {'title': 'Улучшение композиционного понимания без потери мультимодальных возможностей', 'desc': 'В статье предлагается новый метод улучшения композиционного понимания в предобученных моделях зрения и языка (VLM) без ущерба для производительности в мультимодальных задачах с нулевым обучением. Авторы представляют Fine-grained Selective Calibrated CLIP (FSC-CLIP), который интегрирует локальную потерю жестких негативных примеров и селективную калиброванную регуляризацию. FSC-CLIP обеспечивает детальный негативный надзор, сохраняя целостность представлений модели. Результаты оценки на различных бенчмарках показывают, что FSC-CLIP достигает композиционности на уровне современных моделей, сохраняя при этом сильные мультимодальные возможности.'}, 'en': {'title': 'Balancing Act: Enhancing Compositional Understanding Without Compromise', 'desc': "The paper introduces FSC-CLIP, a method to improve how vision and language models understand compositions without losing their ability to handle multiple types of data at once. Traditional methods often harm the model's ability to work with both images and text by using a global hard negative loss, which can confuse the model. FSC-CLIP uses a local hard negative loss and selective calibrated regularization to provide more precise guidance, helping the model learn better without losing its multi-modal skills. Tests show that FSC-CLIP matches top models in understanding compositions while keeping its multi-modal strengths intact."}, 'zh': {'title': 'FSC-CLIP：提升组合理解，不损多模态表现', 'desc': '这篇论文提出了一种新方法，增强预训练视觉和语言模型的组合理解能力，同时不影响零样本多模态任务的表现。传统的微调方法通常通过使用全局硬负样本损失来提高组合推理，但这会损害多模态能力。为了解决这个问题，我们提出了细粒度选择性校准CLIP（FSC-CLIP），结合了局部硬负样本损失和选择性校准正则化。这些创新提供了细粒度的负监督，同时保持了模型的表示完整性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06508', 'title': 'Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning', 'url': 'https://huggingface.co/papers/2410.06508', 'abstract': 'Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO have enabled LLMs to distill high-quality behaviors from MCTS, improving their reasoning performance. However, existing distillation methods underutilize the rich trajectory information generated by MCTS, limiting the potential for improvements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel pairwise training framework that enables LLMs to self-improve through MCTS behavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via two key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from child nodes sharing the same parent in the search tree, providing step-level information for more effective MCTS behavior distillation. (2) AlphaLLM-CPL introduces curriculum preference learning, dynamically adjusting the training sequence of trajectory pairs in each offline training epoch to prioritize critical learning steps and mitigate overfitting. Experimental results on mathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly outperforms previous MCTS behavior distillation methods, substantially boosting the reasoning capabilities of LLMs.', 'score': 9, 'issue_id': 51, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'e3c203504e6848e3', 'data': {'categories': ['#reasoning', '#training', '#math', '#rl', '#optimization', '#games'], 'emoji': '🧠', 'ru': {'title': 'AlphaLLM-CPL: Усиление рассуждений LLM через дистилляцию MCTS', 'desc': 'AlphaLLM-CPL - это новый метод обучения языковых моделей (LLM) с использованием поиска по дереву Монте-Карло (MCTS). Он улучшает рассуждения LLM, эффективно извлекая информацию из траекторий MCTS. AlphaLLM-CPL использует попарное обучение на основе узлов дерева поиска и куррикулярное обучение предпочтениям. Эксперименты показывают значительное улучшение способностей LLM к математическим рассуждениям по сравнению с предыдущими методами.'}, 'en': {'title': 'Boosting LLM Reasoning with Smart MCTS Learning', 'desc': 'This paper introduces AlphaLLM-CPL, a new training framework that helps large language models (LLMs) improve their reasoning skills using Monte Carlo Tree Search (MCTS). Unlike previous methods, AlphaLLM-CPL makes better use of the detailed path information from MCTS by creating stepwise trajectory pairs, which helps in more effective learning. It also uses a technique called curriculum preference learning to adjust the order of learning steps, focusing on the most important ones and avoiding overfitting. Tests on math problems show that AlphaLLM-CPL greatly enhances the reasoning abilities of LLMs compared to older methods.'}, 'zh': {'title': 'AlphaLLM-CPL：通过MCTS行为蒸馏自我提升LLM推理能力', 'desc': '这篇论文介绍了一种新的训练框架AlphaLLM-CPL，用于通过蒙特卡罗树搜索（MCTS）行为蒸馏来提升大型语言模型（LLM）的推理能力。AlphaLLM-CPL通过构建来自同一父节点的子节点的逐步轨迹对，提供了更有效的MCTS行为蒸馏。它还引入了课程偏好学习，动态调整轨迹对的训练顺序，以优先考虑关键学习步骤并减少过拟合。实验结果表明，AlphaLLM-CPL在数学推理任务中显著优于现有的MCTS行为蒸馏方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05248', 'title': 'SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe', 'url': 'https://huggingface.co/papers/2410.05248', 'abstract': "To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets' intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. In this paper, we propose SFTMix, a novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, we argue that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies a Mixup-based regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across a wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMix's design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications.", 'score': 8, 'issue_id': 52, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '960aef6ca0d9e59b', 'data': {'categories': ['#dataset', '#training', '#healthcare', '#optimization', '#data', '#alignment'], 'emoji': '🔀', 'ru': {'title': 'SFTMix: улучшение инструктивной настройки LLM без дорогостоящей фильтрации данных', 'desc': 'SFTMix - это новый метод для улучшения процесса инструктивной настройки больших языковых моделей (LLM). Он использует динамику обучения для идентификации примеров с разными уровнями уверенности модели. Затем применяется регуляризация на основе Mixup для снижения переобучения на уверенных примерах и улучшения обучения на менее уверенных. SFTMix превосходит стандартный метод предсказания следующего токена на различных задачах без необходимости в тщательно отобранных наборах данных.'}, 'en': {'title': 'SFTMix: Elevating Instruction-Tuning Without Expensive Datasets', 'desc': 'The paper introduces SFTMix, a new method to improve instruction-tuning in large language models without relying on expensive, high-quality datasets. SFTMix uses training dynamics to identify examples with varying confidence levels and applies a Mixup-based regularization to balance learning. This approach helps prevent overfitting on confident examples and enhances learning on less confident ones, leading to better performance across various tasks. The method is shown to be adaptable to different models and scalable to any dataset size, making it a versatile tool in natural language processing.'}, 'zh': {'title': 'SFTMix：无需精心策划数据集的指令微调新方法', 'desc': '这篇论文提出了一种名为SFTMix的新方法，用于提升大语言模型的指令微调性能。传统方法依赖高质量的数据集，而SFTMix通过利用数据集的内在特性，减少了对精心策划数据集的依赖。SFTMix通过识别不同信心水平的例子，并应用一种基于Mixup的正则化方法，来改善模型在不确定例子上的学习。实验表明，SFTMix在多种任务中表现优异，具有良好的适应性和可扩展性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08049', 'title': 'Scaling Up Your Kernels: Large Kernel Design in ConvNets towards Universal Representations', 'url': 'https://huggingface.co/papers/2410.08049', 'abstract': 'This paper proposes the paradigm of large convolutional kernels in designing modern Convolutional Neural Networks (ConvNets). We establish that employing a few large kernels, instead of stacking multiple smaller ones, can be a superior design strategy. Our work introduces a set of architecture design guidelines for large-kernel ConvNets that optimize their efficiency and performance. We propose the UniRepLKNet architecture, which offers systematical architecture design principles specifically crafted for large-kernel ConvNets, emphasizing their unique ability to capture extensive spatial information without deep layer stacking. This results in a model that not only surpasses its predecessors with an ImageNet accuracy of 88.0%, an ADE20K mIoU of 55.6%, and a COCO box AP of 56.4% but also demonstrates impressive scalability and performance on various modalities such as time-series forecasting, audio, point cloud, and video recognition. These results indicate the universal modeling abilities of large-kernel ConvNets with faster inference speed compared with vision transformers. Our findings reveal that large-kernel ConvNets possess larger effective receptive fields and a higher shape bias, moving away from the texture bias typical of smaller-kernel CNNs. All codes and models are publicly available at https://github.com/AILab-CVC/UniRepLKNet promoting further research and development in the community.', 'score': 8, 'issue_id': 51, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '8c8b1af09bf67bbe', 'data': {'categories': ['#video', '#audio', '#cv', '#inference', '#optimization', '#open_source', '#architecture', '#3d'], 'emoji': '🔍', 'ru': {'title': 'Большие сверточные ядра - ключ к универсальным и эффективным ConvNets', 'desc': 'Статья предлагает новую парадигму использования больших сверточных ядер в современных сверточных нейронных сетях (ConvNets). Авторы утверждают, что применение нескольких больших ядер вместо стека из множества мелких может быть более эффективной стратегией проектирования. Они представляют архитектуру UniRepLKNet, которая обеспечивает систематические принципы проектирования для ConvNets с большими ядрами, подчеркивая их уникальную способность захватывать обширную пространственную информацию без глубокого наслоения. Результаты показывают превосходство этого подхода в различных задачах компьютерного зрения и других модальностях, демонстрируя универсальные возможности моделирования ConvNets с большими ядрами при более высокой скорости вывода по сравнению с vision transformers.'}, 'en': {'title': '"Think Big: Revolutionizing ConvNets with Large Kernels"', 'desc': 'This paper introduces a new approach to designing Convolutional Neural Networks (ConvNets) by using large convolutional kernels instead of many smaller ones. The authors present the UniRepLKNet architecture, which is optimized for capturing extensive spatial information efficiently. Their model achieves high accuracy and performance across various tasks, outperforming traditional models and vision transformers. The research highlights the advantages of large-kernel ConvNets, such as larger receptive fields and a shift from texture to shape bias, offering faster inference speeds.'}, 'zh': {'title': '大卷积核：现代卷积神经网络的新方向', 'desc': '这篇论文提出了在设计现代卷积神经网络时使用大卷积核的范式。研究表明，使用少量大卷积核比堆叠多个小卷积核更优。我们介绍了一套针对大卷积核网络的架构设计指南，优化其效率和性能。UniRepLKNet架构展示了大卷积核网络在捕捉广泛空间信息方面的独特能力，并在多个领域表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07041', 'title': 'Emergent properties with repeated examples', 'url': 'https://huggingface.co/papers/2410.07041', 'abstract': 'We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.', 'score': 8, 'issue_id': 50, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'eac06638db21722d', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#math', '#optimization'], 'emoji': '🔁', 'ru': {'title': 'Повторение - мать учения для трансформеров', 'desc': 'Исследование показывает, что трансформеры лучше обучаются на меньших наборах данных с повторяющимися примерами, чем на больших наборах с уникальными примерами. Эксперименты проводились на трех математических задачах: нахождение наибольшего общего делителя, модульное умножение и собственные значения матриц. Авторы демонстрируют эффективность метода двухэтапного обучения, сочетающего повторное использование малого подмножества примеров с обычной выборкой из остального набора данных. Результаты подчеркивают, что преимущества повторения могут превосходить преимущества разнообразия данных в глубоком обучении.'}, 'en': {'title': 'Repetition Over Diversity: A New Path to Better Learning', 'desc': 'This paper explores how transformers perform when trained with repeated examples versus a larger variety of single-use examples. It finds that using a smaller set of repeated examples can lead to better performance than using a larger, more diverse dataset. The study introduces a two-set training method, which combines repeated examples with normal sampling, resulting in faster learning and improved outcomes. This research provides insights into the balance between memorization and generalization in deep learning.'}, 'zh': {'title': '重复训练：超越数据多样性的力量', 'desc': '这篇论文研究了在算法生成的数据集上，Transformer模型的性能与训练样本重复次数之间的关系。在最大公约数、模乘法和矩阵特征值三个数学问题上，研究表明在固定训练步数下，使用较小重复样本集训练的模型优于使用较大单次样本集的模型。研究还展示了双集训练法，即在正常采样的基础上重复使用一小部分随机样本，可以加快学习速度并提高性能。这表明重复的好处可能超过数据多样性的好处。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08115', 'title': 'Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System', 'url': 'https://huggingface.co/papers/2410.08115', 'abstract': "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optima shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS (https://chenweize1998.github.io/optima-project-page).", 'score': 7, 'issue_id': 50, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'b7cacda3f030e1bd', 'data': {'categories': ['#reasoning', '#training', '#inference', '#rl', '#optimization', '#agents', '#games', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'Optima: революция в эффективности многоагентных систем на основе LLM', 'desc': 'Optima - новая система, улучшающая эффективность коммуникации и решения задач в многоагентных системах на основе больших языковых моделей. Она использует итеративный подход генерации, ранжирования, отбора и обучения с функцией вознаграждения, балансирующей производительность, токен-эффективность и читаемость. Система интегрирует техники, вдохновленные Monte Carlo Tree Search, для генерации данных обучения. Optima демонстрирует значительные улучшения по сравнению с одноагентными базовыми линиями и обычными многоагентными системами на основе Llama 3 8B.'}, 'en': {'title': 'Optima: Revolutionizing Multi-Agent Communication and Efficiency', 'desc': 'The paper introduces Optima, a framework designed to improve communication efficiency and task effectiveness in multi-agent systems using large language models. Optima uses a generate, rank, select, and train approach with a reward function to balance task performance and communication readability. It explores reinforcement learning techniques like Supervised Fine-Tuning and Direct Preference Optimization, integrating Monte Carlo Tree Search methods to enhance data generation. The framework demonstrates significant performance improvements in multi-agent tasks, achieving better results with fewer resources compared to traditional methods.'}, 'zh': {'title': 'Optima：提升多智能体系统效率的新框架', 'desc': '这篇论文介绍了一种名为Optima的新框架，旨在提高基于大型语言模型的多智能体系统的通信效率和任务效果。Optima通过生成、排序、选择和训练的迭代过程，结合奖励函数来平衡任务表现、令牌效率和通信可读性。研究中使用了多种强化学习算法，包括监督微调和直接偏好优化，并结合蒙特卡罗树搜索技术来生成数据。实验结果表明，Optima在信息不对称问答和复杂推理等任务中表现出显著的性能提升，展示了其在大规模、多智能体系统中的潜力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07137', 'title': 'Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates', 'url': 'https://huggingface.co/papers/2410.07137', 'abstract': 'Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a "null model" that always outputs a constant response (irrelevant to input instructions) can cheat automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench. Moreover, the crafted cheating outputs are transferable because we assume that the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are private and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.', 'score': 6, 'issue_id': 56, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'fdf529020c19324c', 'data': {'categories': ['#security', '#inference', '#ethics', '#benchmark', '#open_source'], 'emoji': '🕵️', 'ru': {'title': "Осторожно: даже 'нулевая модель' может обмануть бенчмарки ЯМ!", 'desc': "Статья рассматривает проблему обмана автоматических бенчмарков для оценки языковых моделей. Авторы демонстрируют, как даже 'нулевая модель', всегда выдающая один и тот же ответ, может достичь высоких результатов на популярных бенчмарках. Эксперименты показывают, что такие трюки могут быть использованы для неэтичного повышения рейтинга моделей. Исследование подчеркивает необходимость разработки механизмов защиты от обмана для обеспечения надежности автоматических бенчмарков."}, 'en': {'title': 'Unmasking the Illusion: Ensuring Fair Play in LLM Benchmarks', 'desc': "The paper discusses how automatic benchmarks for evaluating language models, like AlpacaEval 2.0, can be manipulated to falsely boost a model's performance. It reveals that even a simple model that outputs the same response regardless of input can achieve high scores on these benchmarks. This highlights the vulnerability of current evaluation systems to gaming tactics, which can lead to misleading promotional claims about a model's capabilities. The authors emphasize the need for developing robust anti-cheating mechanisms to ensure the reliability of automatic benchmarks."}, 'zh': {'title': '揭示自动化评估基准的漏洞：呼唤反作弊机制', 'desc': '这篇论文讨论了自动化语言模型评估基准的漏洞，这些基准如AlpacaEval 2.0等，虽然成本低且可扩展，但容易被操控。研究发现，即使是简单的“空模型”也能通过固定输出来欺骗这些基准，获得高胜率。论文强调了需要开发反作弊机制，以确保评估的可靠性。研究结果表明，现有的基准测试可能无法准确反映模型的真实性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06293', 'title': 'Accelerated Preference Optimization for Large Language Model Alignment', 'url': 'https://huggingface.co/papers/2410.06293', 'abstract': "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal tool for aligning large language models (LLMs) with human preferences. Direct Preference Optimization (DPO), one of the most popular approaches, formulates RLHF as a policy optimization problem without explicitly estimating the reward function. It overcomes the stability and efficiency issues of two-step approaches, which typically involve first estimating the reward function and then optimizing the policy via proximal policy optimization (PPO). Since RLHF is essentially an optimization problem, and it is well-known that momentum techniques can accelerate optimization both theoretically and empirically, a natural question arises: Can RLHF be accelerated by momentum? This paper answers this question in the affirmative. In detail, we first show that the iterative preference optimization method can be viewed as a proximal point method. Based on this observation, we propose a general Accelerated Preference Optimization (APO) framework, which unifies many existing preference optimization algorithms and employs Nesterov's momentum technique to speed up the alignment of LLMs. Theoretically, we demonstrate that APO can achieve a faster convergence rate than the standard iterative preference optimization methods, including DPO and Self-Play Preference Optimization (SPPO). Empirically, we show the superiority of APO over DPO, iterative DPO, and other strong baselines for RLHF on the AlpacaEval 2.0 benchmark.", 'score': 4, 'issue_id': 105, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': 'b5313b248ee2e065', 'data': {'categories': ['#training', '#math', '#rl', '#optimization', '#benchmark', '#alignment', '#rlhf'], 'emoji': '🚀', 'ru': {'title': 'Ускоряем обучение языковых моделей с помощью импульса', 'desc': 'Статья представляет новый метод Accelerated Preference Optimization (APO) для обучения больших языковых моделей с учетом предпочтений человека. APO использует технику импульса Нестерова для ускорения процесса выравнивания моделей. Теоретически доказано, что APO имеет более быструю скорость сходимости по сравнению со стандартными методами итеративной оптимизации предпочтений. Эмпирические эксперименты на бенчмарке AlpacaEval 2.0 подтверждают превосходство APO над существующими подходами.'}, 'en': {'title': 'Accelerating Alignment: Momentum in RLHF', 'desc': "This paper introduces Accelerated Preference Optimization (APO), a new framework that enhances Reinforcement Learning from Human Feedback (RLHF) for aligning large language models with human preferences. It builds on Direct Preference Optimization (DPO) by applying Nesterov's momentum technique to improve the speed and efficiency of the optimization process. The authors demonstrate that APO achieves a faster convergence rate compared to traditional methods like DPO and Self-Play Preference Optimization (SPPO). Empirical results show that APO outperforms these existing approaches on the AlpacaEval 2.0 benchmark, confirming its effectiveness in optimizing language model alignment."}, 'zh': {'title': '加速偏好优化：提升RLHF效率的关键', 'desc': '强化学习中的人类反馈（RLHF）是对齐大型语言模型（LLM）与人类偏好的重要工具。直接偏好优化（DPO）是一种流行的方法，它将RLHF视为一个策略优化问题，而不需要明确估计奖励函数。本文提出了一种加速偏好优化（APO）框架，利用Nesterov动量技术加速LLM的对齐过程，并证明其收敛速度优于传统的迭代偏好优化方法。通过实验证明，APO在AlpacaEval 2.0基准测试中表现优于DPO和其他强基线。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05269', 'title': 'Data Advisor: Dynamic Data Curation for Safety Alignment of Large Language Models', 'url': 'https://huggingface.co/papers/2410.05269', 'abstract': 'Data is a crucial element in large language model (LLM) alignment. Recent studies have explored using LLMs for efficient data collection. However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints. To address these problems, we propose Data Advisor, an enhanced LLM-based method for generating data that takes into account the characteristics of the desired dataset. Starting from a set of pre-defined principles in hand, Data Advisor monitors the status of the generated data, identifies weaknesses in the current dataset, and advises the next iteration of data generation accordingly. Data Advisor can be easily integrated into existing data generation methods to enhance data quality and coverage. Experiments on safety alignment of three representative LLMs (i.e., Mistral, Llama2, and Falcon) demonstrate the effectiveness of Data Advisor in enhancing model safety against various fine-grained safety issues without sacrificing model utility.', 'score': 3, 'issue_id': 105, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '583137a3293a5c8c', 'data': {'categories': ['#training', '#data', '#synthetic', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Data Advisor: умный помощник для создания качественных данных в обучении ИИ', 'desc': 'Статья представляет метод Data Advisor для улучшения качества данных, генерируемых большими языковыми моделями (LLM) для их настройки. Data Advisor анализирует текущий набор данных, выявляет слабые места и даёт рекомендации для следующей итерации генерации. Этот метод легко интегрируется в существующие подходы к генерации данных. Эксперименты показали эффективность Data Advisor в повышении безопасности моделей без ущерба для их полезности.'}, 'en': {'title': 'Enhancing Data Quality in LLMs with Data Advisor', 'desc': 'This paper introduces Data Advisor, a novel method designed to improve the quality of data generated by large language models (LLMs). It addresses common issues such as low-quality data points and gaps in representation by monitoring the generated data and identifying its weaknesses. By following a set of predefined principles, Data Advisor provides guidance for subsequent data generation iterations, ensuring better alignment with the desired dataset characteristics. Experiments show that integrating Data Advisor enhances the safety of LLMs while maintaining their overall utility.'}, 'zh': {'title': '数据顾问：提升LLM数据质量的利器', 'desc': '数据在大型语言模型（LLM）对齐中至关重要。最近的研究探讨了使用LLM进行高效数据收集，但LLM生成的数据常常存在质量问题，如某些方面的代表性不足或缺失。为了解决这些问题，我们提出了数据顾问（Data Advisor），这是一种基于LLM的增强方法，能够根据所需数据集的特征生成数据。数据顾问可以轻松集成到现有的数据生成方法中，以提高数据质量和覆盖率，并在实验中证明了其在增强模型安全性方面的有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.05629', 'title': 'Vector-ICL: In-context Learning with Continuous Vector Representations', 'url': 'https://huggingface.co/papers/2410.05629', 'abstract': "Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.", 'score': 3, 'issue_id': 58, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '0f95824fcb35b2e3', 'data': {'categories': ['#science', '#video', '#audio', '#cv', '#training', '#graphs', '#data', '#optimization', '#transfer_learning', '#architecture', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'LLM осваивают векторный мир: от текста к универсальным данным', 'desc': 'В статье исследуется возможность расширения способностей больших языковых моделей (LLM) к обучению в контексте на векторные данные из различных доменов. Авторы предлагают метод Vector-ICL, который позволяет LLM эффективно обрабатывать и учиться на проецированных векторах. Эксперименты показывают, что Vector-ICL часто превосходит как few-shot ICL, так и специализированные модели в различных задачах и модальностях. Результаты указывают на потенциал LLM в обработке векторных представлений за пределами традиционных токен-ориентированных парадигм.'}, 'en': {'title': 'Expanding LLM Horizons: From Text to Vectors', 'desc': "This paper investigates the ability of large language models (LLMs) to perform in-context learning (ICL) on continuous vectors from various domains. By using lightweight projectors to align input data with the LLM's embedding space, the study introduces Vector-ICL, where LLMs can effectively learn from these projected vectors. The research shows that pretraining projectors with general language modeling objectives is crucial for enabling Vector-ICL, and task-specific finetuning further improves its performance. Experiments demonstrate that Vector-ICL often outperforms few-shot ICL and domain-specific models across multiple tasks and modalities, suggesting LLMs' potential to handle vector data beyond traditional text tokens."}, 'zh': {'title': '向量上下文学习：超越传统的语言模型', 'desc': '大型语言模型（LLMs）在文本数据上展示了出色的上下文学习能力。我们研究这些能力是否可以扩展到来自不同领域的连续向量，这些向量是通过黑箱预训练编码器获得的。通过轻量级投影器将输入数据与LLM的嵌入空间对齐，我们发现LLM可以有效地处理和学习这些投影向量，这被称为向量上下文学习（Vector-ICL）。实验表明，预训练投影器与一般语言建模目标可以实现Vector-ICL，而特定任务的微调进一步提高了性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07707', 'title': 'MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2410.07707', 'abstract': 'Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results. Project page: https://ruijiezhu94.github.io/MotionGS_page', 'score': 3, 'issue_id': 53, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'f7341783586984de', 'data': {'categories': ['#cv', '#graphs', '#optimization', '#architecture', '#3d'], 'emoji': '🎥', 'ru': {'title': 'MotionGS: Реконструкция динамических сцен с помощью деформируемых 3D-гауссианов', 'desc': 'MotionGS - это новый фреймворк для деформируемого 3D-сплаттинга гауссианов, который использует явные ограничения движения объектов для улучшения реконструкции динамических сцен. Система включает модуль декомпозиции оптического потока, разделяющий его на поток камеры и поток движения объектов. Также предложен модуль уточнения положения камеры для оптимизации 3D-гауссианов и поз камеры. Эксперименты показывают превосходство MotionGS над современными методами в задаче реконструкции монокулярных динамических сцен.'}, 'en': {'title': 'MotionGS: Mastering Dynamic 3D Scenes with Motion-Aware Gaussians', 'desc': 'The paper introduces MotionGS, a new framework for reconstructing dynamic 3D scenes using Gaussian splatting. It addresses the challenge of optimizing object motion by incorporating explicit motion priors through an optical flow decoupling module. This module separates camera movement from object motion, allowing for more accurate deformation of 3D Gaussians. The framework also includes a camera pose refinement module to improve the accuracy of camera poses, resulting in superior performance compared to existing methods.'}, 'zh': {'title': 'MotionGS：动态场景重建的新突破', 'desc': '动态场景重建一直是3D视觉领域的长期挑战。最近，3D高斯点的出现为这个问题提供了新的见解。我们提出了一种新的可变形3D高斯点框架MotionGS，通过引入光流解耦模块来指导3D高斯的变形。实验表明，MotionGS在单目动态场景中优于现有方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.03437', 'title': 'Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs', 'url': 'https://huggingface.co/papers/2410.03437', 'abstract': 'Solving time-dependent parametric partial differential equations (PDEs) is challenging, as models must adapt to variations in parameters such as coefficients, forcing terms, and boundary conditions. Data-driven neural solvers either train on data sampled from the PDE parameters distribution in the hope that the model generalizes to new instances or rely on gradient-based adaptation and meta-learning to implicitly encode the dynamics from observations. This often comes with increased inference complexity. Inspired by the in-context learning capabilities of large language models (LLMs), we introduce Zebra, a novel generative auto-regressive transformer designed to solve parametric PDEs without requiring gradient adaptation at inference. By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context trajectories or preceding states. This approach enables Zebra to flexibly handle arbitrarily sized context inputs and supports uncertainty quantification through the sampling of multiple solution trajectories. We evaluate Zebra across a variety of challenging PDE scenarios, demonstrating its adaptability, robustness, and superior performance compared to existing approaches.', 'score': 2, 'issue_id': 58, 'pub_date': '2024-10-04', 'pub_date_card': {'ru': '4 октября', 'en': 'October 4', 'zh': '10月4日'}, 'hash': '49b78243ef220e9c', 'data': {'categories': ['#reasoning', '#math', '#inference', '#optimization', '#transfer_learning', '#diffusion', '#architecture'], 'emoji': '🦓', 'ru': {'title': 'Zebra: решение ДУЧП с помощью обучения в контексте', 'desc': 'Zebra - это новый генеративный авторегрессионный трансформер для решения параметрических дифференциальных уравнений в частных производных (ДУЧП). В отличие от существующих подходов, Zebra не требует градиентной адаптации во время вывода, а использует обучение в контексте, вдохновленное возможностями больших языковых моделей. Модель динамически адаптируется к новым задачам, обусловливая входные последовательности, включающие контекстные траектории или предыдущие состояния. Zebra демонстрирует превосходную производительность по сравнению с существующими подходами в различных сценариях ДУЧП.'}, 'en': {'title': 'Zebra: Transforming PDE Solutions with In-Context Learning', 'desc': 'The paper introduces Zebra, a new generative auto-regressive transformer model designed to solve time-dependent parametric partial differential equations (PDEs) without needing gradient adaptation during inference. Zebra leverages in-context learning, inspired by large language models, to dynamically adapt to new tasks by conditioning on input sequences that include context trajectories. This approach allows Zebra to handle varying context sizes and provides uncertainty quantification by sampling multiple solution trajectories. The model is evaluated on various challenging PDE scenarios, showing its adaptability, robustness, and superior performance compared to existing methods.'}, 'zh': {'title': 'Zebra：无需梯度调整的PDE解决方案', 'desc': '这篇论文介绍了一种名为Zebra的新型生成自回归Transformer，用于解决参数化偏微分方程（PDEs）。Zebra通过在预训练和推理时利用上下文信息，动态适应新任务，而无需在推理时进行梯度调整。它能够灵活处理任意大小的上下文输入，并通过采样多个解轨迹来支持不确定性量化。实验表明，Zebra在各种复杂的PDE场景中表现出色，具有很强的适应性和鲁棒性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.04808', 'title': 'LPZero: Language Model Zero-cost Proxy Search from Zero', 'url': 'https://huggingface.co/papers/2410.04808', 'abstract': "In spite of the outstanding performance, Neural Architecture Search (NAS) is criticized for massive computation. Recently, Zero-shot NAS has emerged as a promising approach by exploiting Zero-cost (ZC) proxies, which markedly reduce computational demands. Despite this, existing ZC proxies heavily rely on expert knowledge and incur significant trial-and-error costs. Particularly in NLP tasks, most existing ZC proxies fail to surpass the performance of the naive baseline. To address these challenges, we introduce a novel framework, LPZero, which is the first to automatically design ZC proxies for various tasks, achieving higher ranking consistency than human-designed proxies. Specifically, we model the ZC proxy as a symbolic equation and incorporate a unified proxy search space that encompasses existing ZC proxies, which are composed of a predefined set of mathematical symbols. To heuristically search for the best ZC proxy, LPZero incorporates genetic programming to find the optimal symbolic composition. We propose a Rule-based Pruning Strategy (RPS), which preemptively eliminates unpromising proxies, thereby mitigating the risk of proxy degradation. Extensive experiments on FlexiBERT, GPT-2, and LLaMA-7B demonstrate LPZero's superior ranking ability and performance on downstream tasks compared to current approaches.", 'score': 2, 'issue_id': 56, 'pub_date': '2024-10-07', 'pub_date_card': {'ru': '7 октября', 'en': 'October 7', 'zh': '10月7日'}, 'hash': '58e4dff9a00bb4f7', 'data': {'categories': ['#training', '#math', '#optimization', '#architecture'], 'emoji': '🧬', 'ru': {'title': 'LPZero: Автоматическое проектирование прокси с нулевой стоимостью для эффективного нейроархитектурного поиска', 'desc': 'LPZero - это новый фреймворк для автоматического проектирования прокси с нулевой стоимостью (ZC) для различных задач нейроархитектурного поиска (NAS). Он моделирует ZC-прокси как символическое уравнение и включает единое пространство поиска прокси. LPZero использует генетическое программирование для поиска оптимальной символической композиции и предлагает стратегию правил-основанной обрезки (RPS) для устранения бесперспективных прокси. Эксперименты на FlexiBERT, GPT-2 и LLaMA-7B показывают превосходные способности ранжирования и производительность LPZero по сравнению с существующими подходами.'}, 'en': {'title': '"LPZero: Automating Efficiency in Neural Architecture Search"', 'desc': 'The paper introduces LPZero, a novel framework for Zero-shot Neural Architecture Search (NAS) that reduces computational demands by automatically designing Zero-cost (ZC) proxies. Unlike existing methods that rely heavily on expert knowledge, LPZero uses genetic programming to explore a unified search space of symbolic equations, improving ranking consistency across tasks. The framework includes a Rule-based Pruning Strategy (RPS) to eliminate ineffective proxies early, enhancing efficiency and performance. Experiments show that LPZero outperforms current methods in ranking and downstream tasks, particularly in natural language processing applications.'}, 'zh': {'title': 'LPZero：自动化零成本代理设计的突破', 'desc': '这篇论文介绍了一种新的框架LPZero，用于自动设计零成本代理（ZC proxies），以减少神经架构搜索（NAS）的计算需求。LPZero通过将ZC代理建模为符号方程，并结合统一的代理搜索空间，来提高代理的排名一致性。该框架使用遗传编程来寻找最佳的符号组合，并通过规则剪枝策略（RPS）提前淘汰不理想的代理。实验表明，LPZero在FlexiBERT、GPT-2和LLaMA-7B等任务中表现优于现有方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08565', 'title': 'Baichuan-Omni Technical Report', 'url': 'https://huggingface.co/papers/2410.08565', 'abstract': 'The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.', 'score': 82, 'issue_id': 90, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '635ebaf87323c35f', 'data': {'categories': ['#training', '#benchmark', '#alignment', '#open_source', '#small_models', '#architecture', '#multimodal'], 'emoji': '🌐', 'ru': {'title': 'Baichuan-Omni: открытая мультимодальная модель для обработки текста, изображений, видео и аудио', 'desc': 'Статья представляет Baichuan-Omni - первую открытую мультимодальную языковую модель на 7 миллиардов параметров. Модель способна одновременно обрабатывать изображения, видео, аудио и текст, обеспечивая продвинутый интерактивный опыт. Авторы предлагают эффективную схему мультимодального обучения в два этапа: мультимодальное выравнивание и мультизадачная донастройка. Baichuan-Omni демонстрирует высокие результаты на различных мультимодальных бенчмарках.'}, 'en': {'title': 'Baichuan-Omni: Bridging Multimodal Gaps in Open-Source AI', 'desc': 'The paper introduces Baichuan-Omni, an open-source 7 billion parameter Multimodal Large Language Model (MLLM) capable of processing images, videos, audio, and text simultaneously. It employs a two-stage training process involving multimodal alignment and multitask fine-tuning to enhance its ability to handle diverse data types. The model demonstrates strong performance across various benchmarks, showcasing its potential as a competitive tool for multimodal understanding. This work aims to provide a robust baseline for the open-source community to further develop real-time multimodal interaction capabilities.'}, 'zh': {'title': 'Baichuan-Omni：开源多模态大语言模型的先锋', 'desc': '这篇论文介绍了Baichuan-Omni，这是第一个开源的7B多模态大语言模型，能够同时处理图像、视频、音频和文本。通过两阶段的多模态对齐和多任务微调训练方法，该模型在多模态交互体验和性能上表现出色。Baichuan-Omni展示了在多种多模态基准测试中的强大性能。我们希望这一贡献能成为开源社区在多模态理解和实时交互方面的竞争基准。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08261', 'title': 'Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2410.08261', 'abstract': "Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing 1024 times 1024 resolution images.", 'score': 49, 'issue_id': 91, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '3b6012d644e53308', 'data': {'categories': ['#cv', '#training', '#data', '#optimization', '#alignment', '#open_source', '#diffusion', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Meissonic: Революция в генерации изображений с MIM', 'desc': 'Meissonic - это новая модель для генерации изображений по текстовому описанию, основанная на подходе masked image modeling (MIM). Она сочетает архитектурные инновации, продвинутые стратегии позиционного кодирования и оптимизированные условия сэмплирования для улучшения производительности и эффективности MIM. Meissonic использует качественные обучающие данные, микро-условия на основе оценок предпочтений человека и слои сжатия признаков для повышения качества и разрешения изображений. Модель демонстрирует результаты на уровне или превосходящие современные диффузионные модели вроде SDXL.'}, 'en': {'title': 'Meissonic: Redefining Text-to-Image Synthesis with Efficiency and Quality', 'desc': 'The paper introduces Meissonic, a new approach to text-to-image generation that improves upon existing diffusion models like Stable Diffusion. Unlike autoregressive models, Meissonic uses non-autoregressive masked image modeling, which is more efficient and faster. The model incorporates advanced architectural designs, positional encoding, and optimized sampling to enhance image quality and resolution. Extensive testing shows that Meissonic not only matches but often surpasses current state-of-the-art models in generating high-resolution images.'}, 'zh': {'title': 'Meissonic：引领文本到图像生成的新标准', 'desc': '这篇论文介绍了一种名为Meissonic的新模型，它在非自回归的掩码图像建模中取得了显著进展。通过引入一系列架构创新、先进的位置编码策略和优化的采样条件，Meissonic大大提高了性能和效率。该模型利用高质量的训练数据和人类偏好评分的微条件，结合特征压缩层，提升了图像的清晰度和分辨率。实验结果表明，Meissonic在生成高质量、高分辨率图像方面不仅能与现有模型媲美，甚至在某些方面超越了它们。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08815', 'title': 'StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization', 'url': 'https://huggingface.co/papers/2410.08815', 'abstract': 'Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation. In this paper, motivated by the cognitive theories that humans convert raw information into various structured knowledge when tackling knowledge-intensive reasoning, we proposes a new framework, StructRAG, which can identify the optimal structure type for the task at hand, reconstruct original documents into this structured format, and infer answers based on the resulting structure. Extensive experiments across various knowledge-intensive tasks show that StructRAG achieves state-of-the-art performance, particularly excelling in challenging scenarios, demonstrating its potential as an effective solution for enhancing LLMs in complex real-world applications.', 'score': 39, 'issue_id': 92, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '0b6586df53e81f2e', 'data': {'categories': ['#reasoning', '#optimization', '#architecture', '#rag'], 'emoji': '🧠', 'ru': {'title': 'StructRAG: структурированное знание для эффективного рассуждения', 'desc': 'StructRAG - новый фреймворк для улучшения работы больших языковых моделей в задачах, требующих интенсивного использования знаний. Он преодолевает ограничения существующих методов RAG, структурируя исходную информацию оптимальным для конкретной задачи образом. StructRAG идентифицирует наиболее подходящий тип структуры, реконструирует исходные документы в этот формат и делает выводы на основе полученной структуры. Эксперименты показывают, что StructRAG достигает передовых результатов, особенно в сложных сценариях.'}, 'en': {'title': 'StructRAG: Structuring Knowledge for Superior Reasoning', 'desc': 'The paper introduces StructRAG, a novel framework designed to improve retrieval-augmented generation (RAG) methods for large language models (LLMs) in knowledge-intensive reasoning tasks. Traditional RAG methods often struggle because the necessary information is scattered and noisy, making it hard to perform accurate reasoning. StructRAG addresses this by converting raw information into structured knowledge, allowing for better identification and reasoning of key information. Experiments show that StructRAG significantly enhances performance in complex tasks, setting a new standard for LLMs in real-world applications.'}, 'zh': {'title': 'StructRAG：结构化知识提升大语言模型', 'desc': '这篇论文介绍了一种名为StructRAG的新框架，用于改进大语言模型在知识密集型任务中的表现。现有的检索增强生成方法在处理这些任务时常常难以准确识别关键信息，因为信息分散且噪声较多。StructRAG通过将原始信息转换为结构化知识，帮助模型更好地进行全局推理。实验表明，StructRAG在多种复杂任务中表现优异，尤其在具有挑战性的场景中展现了其潜力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06456', 'title': 'From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning', 'url': 'https://huggingface.co/papers/2410.06456', 'abstract': 'Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at https://github.com/baiyang4/VITask.', 'score': 36, 'issue_id': 91, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'b9de6611a2cfd8cb', 'data': {'categories': ['#cv', '#training', '#healthcare', '#optimization', '#transfer_learning', '#open_source', '#architecture', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'VITask: Преодоление разрыва между предобучением и тонкой настройкой в визуально-языковых моделях', 'desc': 'VITask - новый фреймворк для улучшения адаптации крупных визуально-языковых моделей (VLM) к конкретным задачам. Он использует три стратегии: примерное подсказывание, выравнивание распределения ответов и контрастную настройку ответов. VITask интегрирует специализированные модели для конкретных задач, чтобы улучшить производительность VLM. Эксперименты на 12 наборах данных медицинской диагностики показали превосходство VITask над обычными VLM и специализированными моделями.'}, 'en': {'title': '"VITask: Bridging the Gap for Task-Specific Vision Language Models"', 'desc': 'The paper introduces VITask, a framework designed to improve the task-specific performance of large vision language models (VLMs) by integrating task-specific models (TSMs). VITask uses exemplar prompting, response distribution alignment, and contrastive response tuning to enhance adaptability and accuracy in specific tasks. These strategies help VLMs learn from task-specific features and improve their response accuracy without relying on TSMs during inference. Experiments demonstrate that VITask outperforms traditional VLMs and TSMs, especially in medical diagnosis tasks, by effectively combining features from both models.'}, 'zh': {'title': 'VITask：提升视觉语言模型任务适应性的创新框架', 'desc': '这篇论文介绍了一种名为VITask的新框架，旨在提高大型视觉语言模型（VLMs）在特定任务上的适应性。VITask通过整合任务特定模型（TSMs）来实现这一目标，并采用了示例提示、响应分布对齐和对比响应调优三种策略。实验表明，VITask在12个医学诊断数据集上表现优异，能够有效结合VLMs和TSMs的优势。此外，VITask还具有灵活的TSM集成和对不完整指令的鲁棒性，成为任务特定VLM调优的高效解决方案。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08102', 'title': 'Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining', 'url': 'https://huggingface.co/papers/2410.08102', 'abstract': 'Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain of 10.5% across multiple language model benchmarks compared to the state-of-the-art methods.', 'score': 19, 'issue_id': 93, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'e6e97c0fdfa09a15', 'data': {'categories': ['#training', '#optimization', '#data', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Многоагентный подход к оптимизации данных для LLM', 'desc': 'Представлен новый механизм многоагентного совместного отбора данных для предобучения больших языковых моделей (LLM). Каждый метод отбора данных выступает в роли независимого агента, а консоль агентов динамически интегрирует информацию от всех агентов в процессе обучения LLM. Экспериментальные результаты показывают значительное повышение эффективности использования данных и ускорение сходимости при обучении LLM. Предложенный подход достигает среднего прироста производительности в 10.5% по сравнению с современными методами на нескольких эталонных тестах языковых моделей.'}, 'en': {'title': 'Collaborative Agents: Boosting Language Model Training Efficiency', 'desc': 'The paper introduces a new way to choose data for training large language models more efficiently by using a multi-agent system. Each data selection method acts as an independent agent, and a central console combines their insights to optimize the training process. This approach helps the model learn faster and better, showing a 10.5% improvement in performance over existing methods. The study highlights the importance of collaboration between different data selection strategies to enhance the pretraining of language models.'}, 'zh': {'title': '多智能体协作：提升大语言模型预训练效率的新方法', 'desc': '本文提出了一种多智能体协作的数据选择机制，以提高大语言模型的预训练效率。在这个框架中，每种数据选择方法都作为一个独立的智能体，通过一个控制台动态整合所有智能体的信息。实验结果表明，这种方法显著提高了数据效率，加速了大语言模型的训练收敛速度。与现有最先进的方法相比，我们的方法在多个语言模型基准上平均性能提升了10.5%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07133', 'title': 'EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.07133', 'abstract': 'Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.', 'score': 18, 'issue_id': 90, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'c49f4ef8183585ee', 'data': {'categories': ['#cv', '#training', '#data', '#transfer_learning', '#open_source', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Эволюционное обучение генеративных моделей на публичных данных', 'desc': 'Статья представляет EvolveDirector - фреймворк для обучения модели генерации изображений по тексту, сопоставимой с продвинутыми моделями, но используя только общедоступные ресурсы. Авторы используют API существующих моделей для получения пар текст-изображение и обучения базовой модели. Для уменьшения необходимого объема данных применяются предобученные мультимодальные модели, которые оценивают и корректируют датасет в процессе обучения. Результатом стала модель Edgen, превосходящая по возможностям исходные продвинутые модели.'}, 'en': {'title': 'EvolveDirector: Democratizing Advanced Text-to-Image Generation', 'desc': "The paper introduces EvolveDirector, a framework designed to train a text-to-image generation model using publicly available resources by interacting with advanced models through their APIs. It addresses the challenge of high costs and resource demands by employing pre-trained vision-language models to guide the training process, reducing the need for large datasets. The framework dynamically refines the training data through operations like discrimination and mutation, enhancing the model's learning efficiency. The resulting model, Edgen, not only approximates but also surpasses the capabilities of existing advanced models, demonstrating the potential of this innovative approach."}, 'zh': {'title': 'EvolveDirector：用公共资源实现高级生成能力', 'desc': 'EvolveDirector 是一个框架，通过与高级模型的公共 API 交互，获取文本-图像数据对来训练基础模型。实验表明，使用高级模型生成的数据训练的模型可以接近其生成能力，但需要大量样本。为解决这一问题，利用预训练的大型视觉语言模型（VLM）来指导基础模型的演化，显著减少所需数据量。最终训练的模型 Edgen 超越了这些高级模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07656', 'title': 'Mechanistic Permutability: Match Features Across Layers', 'url': 'https://huggingface.co/papers/2410.07656', 'abstract': 'Understanding how features evolve across layers in deep neural networks is a fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual layers, aligning these features across layers has remained an open problem. In this paper, we introduce SAE Match, a novel, data-free method for aligning SAE features across different layers of a neural network. Our approach involves matching features by minimizing the mean squared error between the folded parameters of SAEs, a technique that incorporates activation thresholds into the encoder and decoder weights to account for differences in feature scales. Through extensive experiments on the Gemma 2 language model, we demonstrate that our method effectively captures feature evolution across layers, improving feature matching quality. We also show that features persist over several layers and that our approach can approximate hidden states across layers. Our work advances the understanding of feature dynamics in neural networks and provides a new tool for mechanistic interpretability studies.', 'score': 16, 'issue_id': 96, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '6a4c86357dc6667d', 'data': {'categories': ['#training', '#optimization', '#interpretability', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'SAE Match: Новый взгляд на эволюцию признаков в глубоких нейронных сетях', 'desc': 'Статья представляет новый метод SAE Match для выравнивания признаков, извлеченных разреженными автоэнкодерами (SAE), между слоями нейронной сети. Метод минимизирует среднеквадратичную ошибку между свернутыми параметрами SAE, учитывая пороги активации. Эксперименты на языковой модели Gemma 2 показывают эффективность метода в отслеживании эволюции признаков. Исследование также демонстрирует, что признаки сохраняются на протяжении нескольких слоев.'}, 'en': {'title': 'Aligning Features Across Layers: A New Approach to Neural Network Interpretability', 'desc': 'This paper introduces SAE Match, a new method to align features across layers in deep neural networks using Sparse Autoencoders. The technique minimizes the mean squared error between folded parameters, incorporating activation thresholds to handle different feature scales. Experiments on the Gemma 2 language model show that SAE Match effectively tracks feature evolution and improves feature matching quality. This work enhances our understanding of feature dynamics and offers a tool for mechanistic interpretability in neural networks.'}, 'zh': {'title': '揭示神经网络特征演变的奥秘', 'desc': '这篇论文探讨了深度神经网络中特征在各层之间的演变，特别是多义性和特征叠加的问题。作者提出了一种名为SAE Match的新方法，通过最小化稀疏自编码器参数的均方误差来对齐不同层的特征。该方法在Gemma 2语言模型上的实验表明，它能有效捕捉特征在各层的演变，提高特征匹配质量。研究结果有助于理解神经网络中的特征动态，并为机械解释性研究提供了新工具。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07035', 'title': 'PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness', 'url': 'https://huggingface.co/papers/2410.07035', 'abstract': "Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations. We identify this issue as stemming from a lack of positional awareness and propose novel approaches--PositionID Prompting and PositionID Fine-Tuning--to address it. These methods enhance the model's ability to continuously monitor and manage text length during generation. Additionally, we introduce PositionID CP Prompting to enable LLMs to perform copy and paste operations accurately. Furthermore, we develop two benchmarks for evaluating length control and copy-paste abilities. Our experiments demonstrate that our methods significantly improve the model's adherence to length constraints and copy-paste accuracy without compromising response quality.", 'score': 16, 'issue_id': 91, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '20096ea1f7372f0f', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#benchmark', '#architecture'], 'emoji': '📏', 'ru': {'title': 'Точный контроль длины текста в LLM с помощью позиционной осведомленности', 'desc': 'Исследователи предлагают новые методы для улучшения контроля длины текста в больших языковых моделях (LLM). Они вводят PositionID Prompting и PositionID Fine-Tuning для повышения позиционной осведомленности моделей. Также представлен метод PositionID CP Prompting для выполнения операций копирования и вставки. Разработаны два новых бенчмарка для оценки контроля длины и возможностей копирования-вставки.'}, 'en': {'title': 'Mastering Length: Enhancing LLMs with PositionID Techniques', 'desc': 'Large Language Models (LLMs) are powerful but struggle with controlling the length of their outputs due to token-level operations and insufficient training on length-specific data. This paper identifies the problem as a lack of positional awareness and introduces PositionID Prompting and PositionID Fine-Tuning to improve length management. Additionally, PositionID CP Prompting is developed to enhance copy-paste accuracy. The proposed methods are tested with new benchmarks, showing significant improvements in length control and copy-paste tasks without affecting the quality of responses.'}, 'zh': {'title': '提升大型语言模型的长度控制能力', 'desc': '大型语言模型在许多领域表现出色，但在控制文本长度方面仍然存在挑战。我们发现问题的根源在于模型缺乏位置感知能力，因此提出了PositionID提示和微调方法来解决。通过这些方法，模型可以更好地监控和管理生成文本的长度。此外，我们还引入了PositionID CP提示来提高复制粘贴操作的准确性。实验结果表明，这些方法显著提高了模型在长度控制和复制粘贴方面的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09008', 'title': 'SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights', 'url': 'https://huggingface.co/papers/2410.09008', 'abstract': "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm", 'score': 16, 'issue_id': 90, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '5c5ecb064656bbe6', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#benchmark', '#open_source', '#rlhf', '#small_models', '#architecture'], 'emoji': '🧮', 'ru': {'title': "SuperCorrect: Подход 'учитель-ученик' для улучшения математических рассуждений малых языковых моделей", 'desc': 'SuperCorrect - это новый двухэтапный фреймворк для улучшения математических рассуждений маленьких языковых моделей. Он использует большую модель-учителя для руководства и коррекции процессов рассуждения и рефлексии меньшей модели-ученика. На первом этапе извлекаются иерархические шаблоны мыслей из модели-учителя. На втором этапе применяется кросс-модельная оптимизация прямых предпочтений для улучшения способностей модели-ученика к самокоррекции.'}, 'en': {'title': 'SuperCorrect: Elevating Small Models with Big Guidance', 'desc': "The paper introduces SuperCorrect, a two-stage framework designed to improve the reasoning and self-correction abilities of smaller language models by using a large teacher model for guidance. In the first stage, the teacher model provides hierarchical templates to help the student model develop more detailed reasoning processes. The second stage involves cross-model collaborative direct preference optimization, where the student model learns to identify and correct errors by following the teacher's correction patterns. This approach significantly enhances the performance of smaller models, as demonstrated by the SuperCorrect-7B model's superior results on mathematical reasoning benchmarks compared to other models of similar size."}, 'zh': {'title': 'SuperCorrect：小模型的推理与纠错新突破', 'desc': '大型语言模型在推理任务中表现出色，但小型模型在复杂数学推理上仍有困难。反思方法试图解决这个问题，但在独立检测推理错误上仍有挑战。SuperCorrect框架通过大模型指导小模型，提升其推理和自我纠正能力。实验表明，SuperCorrect在多个基准测试中表现优异，超越了之前的方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09009', 'title': 'Semantic Score Distillation Sampling for Compositional Text-to-3D Generation', 'url': 'https://huggingface.co/papers/2410.09009', 'abstract': 'Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D', 'score': 13, 'issue_id': 90, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '2c0887a19dd7fec9', 'data': {'categories': ['#cv', '#optimization', '#open_source', '#diffusion', '#architecture', '#3d'], 'emoji': '🎨', 'ru': {'title': 'SemanticSDS: Точный контроль над генерацией сложных 3D-сцен', 'desc': 'В статье представлен новый подход к генерации 3D-контента на основе текстовых описаний - Semantic Score Distillation Sampling (SemanticSDS). Метод использует семантические эмбеддинги для улучшения выразительности и точности композиционной генерации 3D-объектов. SemanticSDS трансформирует эмбеддинги в семантическую карту, которая направляет процесс оптимизации для каждого региона изображения. Эксперименты показывают, что предложенный подход позволяет создавать сложные 3D-сцены и объекты высокого качества.'}, 'en': {'title': 'Unlocking 3D Creativity: SemanticSDS for Detailed Text-to-3D Generation', 'desc': 'The paper addresses the challenge of generating high-quality 3D assets from text descriptions by introducing a new method called Semantic Score Distillation Sampling (SemanticSDS). This approach enhances the expressiveness and accuracy of text-to-3D generation by using semantic embeddings that ensure consistency across different views and distinguish between various objects. These embeddings are used to create a semantic map that guides a region-specific optimization process, allowing for precise and detailed 3D scene generation. The method significantly improves the quality of 3D content, especially for complex scenes, by leveraging existing pre-trained diffusion models with explicit semantic guidance.'}, 'zh': {'title': '语义引导，提升3D生成精度', 'desc': '这篇论文探讨了如何从文本描述生成高质量的3D资产，这是计算机图形学和视觉研究中的一个重要挑战。由于3D数据稀缺，现有方法利用预训练的2D扩散先验，通过得分蒸馏采样（SDS）进行优化。为了提高复杂3D场景的生成能力，作者提出了一种新的语义得分蒸馏采样（SemanticSDS）方法。该方法通过引入新的语义嵌入，能够在不同渲染视图中保持一致性，并清晰区分不同对象和部分，从而实现更精确的优化和组合生成。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08391', 'title': 'KV Prediction for Improved Time to First Token', 'url': 'https://huggingface.co/papers/2410.08391', 'abstract': "Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the ``time to first token'', or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of 15%-50% across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to 30% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction .", 'score': 11, 'issue_id': 91, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '447ad32759dde03e', 'data': {'categories': ['#inference', '#optimization', '#plp', '#benchmark', '#open_source', '#small_models'], 'emoji': '⚡', 'ru': {'title': 'KV Prediction: молниеносный старт для языковых моделей', 'desc': 'Статья представляет новый метод под названием KV Prediction для ускорения вывода трансформерных языковых моделей. Метод использует вспомогательную модель для аппроксимации KV-кэша основной модели, что значительно сокращает время до первого токена (TTFT). Авторы демонстрируют улучшение точности на 15-50% на наборе данных TriviaQA и до 30% на HumanEval при фиксированном бюджете FLOPS для TTFT. Эксперименты на CPU Apple M2 Pro подтверждают ускорение TTFT на реальном оборудовании.'}, 'en': {'title': 'Speed Up Your AI: Faster First Outputs with KV Prediction', 'desc': 'The paper introduces a method called KV Prediction to reduce the time to first token (TTFT) in transformer-based language models. This method uses a small auxiliary model to approximate the key-value (KV) cache, which speeds up the initial output generation without repeatedly querying the auxiliary model. The approach achieves a balance between efficiency and accuracy, showing significant improvements in tasks like TriviaQA and HumanEval. The method also demonstrates hardware speedup on an Apple M2 Pro CPU, enhancing user experience by reducing latency.'}, 'zh': {'title': 'KV预测：加速生成的创新方法', 'desc': '这篇论文介绍了一种名为KV预测的新方法，用于减少预训练模型生成第一个输出的时间。通过使用一个小型辅助模型来近似生成KV缓存，从而加快生成速度。实验表明，该方法在效率和准确性之间达到了帕累托最优的平衡。在TriviaQA和HumanEval等任务中，方法表现出显著的准确性提升。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06264', 'title': 'Think While You Generate: Discrete Diffusion with Planned Denoising', 'url': 'https://huggingface.co/papers/2410.06264', 'abstract': 'Discrete diffusion has achieved state-of-the-art performance, outperforming or approaching autoregressive models on standard benchmarks. In this work, we introduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework that separates the generation process into two models: a planner and a denoiser. At inference time, the planner selects which positions to denoise next by identifying the most corrupted positions in need of denoising, including both initially corrupted and those requiring additional refinement. This plan-and-denoise approach enables more efficient reconstruction during generation by iteratively identifying and denoising corruptions in the optimal order. DDPD outperforms traditional denoiser-only mask diffusion methods, achieving superior results on language modeling benchmarks such as text8, OpenWebText, and token-based generation on ImageNet 256 times 256. Notably, in language modeling, DDPD significantly reduces the performance gap between diffusion-based and autoregressive methods in terms of generative perplexity. Code is available at https://github.com/liusulin/DDPD.', 'score': 9, 'issue_id': 120, 'pub_date': '2024-10-08', 'pub_date_card': {'ru': '8 октября', 'en': 'October 8', 'zh': '10月8日'}, 'hash': '2e52dacdaf42baa1', 'data': {'categories': ['#inference', '#benchmark', '#open_source', '#diffusion', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Плановое шумоподавление для улучшения дискретной диффузии', 'desc': 'В статье представлен новый подход к дискретной диффузии под названием DDPD. Эта модель разделяет процесс генерации на две части: планировщик и шумоподавитель. Планировщик выбирает, какие позиции нужно обработать в первую очередь, определяя наиболее искаженные участки. Такой подход позволяет более эффективно восстанавливать данные во время генерации и превосходит традиционные методы диффузии с маскированием.'}, 'en': {'title': 'Efficient Data Generation with Planned Denoising', 'desc': 'This paper presents a new method called Discrete Diffusion with Planned Denoising (DDPD) that enhances the process of generating data. DDPD uses two models: a planner that decides which parts of the data need fixing and a denoiser that corrects those parts. By focusing on the most corrupted areas first, DDPD improves the efficiency of data reconstruction. The results show that DDPD performs better than traditional methods on various language modeling tasks, narrowing the gap between diffusion models and autoregressive models.'}, 'zh': {'title': '计划去噪，提升生成效率！', 'desc': '离散扩散模型在标准基准测试中表现出色，超越或接近自回归模型。本文提出了一种新的框架——计划去噪的离散扩散（DDPD），将生成过程分为两个模型：规划者和去噪器。在推理时，规划者通过识别最需要去噪的位置来选择下一个去噪的目标，从而实现更高效的重建。DDPD在语言建模基准测试中表现优异，显著缩小了扩散模型与自回归方法之间的性能差距。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08168', 'title': 'ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion', 'url': 'https://huggingface.co/papers/2410.08168', 'abstract': 'We present ZeroComp, an effective zero-shot 3D object compositing approach that does not require paired composite-scene images during training. Our method leverages ControlNet to condition from intrinsic images and combines it with a Stable Diffusion model to utilize its scene priors, together operating as an effective rendering engine. During training, ZeroComp uses intrinsic images based on geometry, albedo, and masked shading, all without the need for paired images of scenes with and without composite objects. Once trained, it seamlessly integrates virtual 3D objects into scenes, adjusting shading to create realistic composites. We developed a high-quality evaluation dataset and demonstrate that ZeroComp outperforms methods using explicit lighting estimations and generative techniques in quantitative and human perception benchmarks. Additionally, ZeroComp extends to real and outdoor image compositing, even when trained solely on synthetic indoor data, showcasing its effectiveness in image compositing.', 'score': 7, 'issue_id': 98, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '8857b485f78603ed', 'data': {'categories': ['#dataset', '#cv', '#benchmark', '#diffusion', '#architecture', '#synthetic', '#3d'], 'emoji': '🎭', 'ru': {'title': 'Реалистичная композиция 3D-объектов без парных данных', 'desc': 'ZeroComp — это эффективный подход к композиции 3D-объектов с нулевым обучением, не требующий парных изображений сцен во время тренировки. Метод использует ControlNet для обусловливания по внутренним изображениям и комбинирует его с моделью Stable Diffusion для использования её априорных знаний о сценах. ZeroComp обучается на внутренних изображениях, основанных на геометрии, альбедо и маскированном затенении. После обучения метод может реалистично интегрировать виртуальные 3D-объекты в сцены, корректируя затенение.'}, 'en': {'title': 'ZeroComp: Mastering 3D Object Integration Without Paired Data', 'desc': 'ZeroComp is a novel method for integrating 3D objects into images without needing paired training data. It uses ControlNet to guide the process with intrinsic images and combines it with Stable Diffusion to leverage scene knowledge. The approach focuses on geometry, albedo, and masked shading to create realistic composites without explicit lighting data. ZeroComp excels in both synthetic and real-world scenarios, outperforming traditional methods in quality and perception tests.'}, 'zh': {'title': 'ZeroComp：无需配对图像的3D对象合成新突破', 'desc': 'ZeroComp是一种无需配对图像的零样本3D对象合成方法。它利用ControlNet从内在图像中获取条件，并结合稳定扩散模型来使用场景先验知识。训练时，ZeroComp使用基于几何、反照率和遮罩阴影的内在图像，无需配对的场景图像。经过训练后，它可以无缝地将虚拟3D对象整合到场景中，调整阴影以创建逼真的合成效果。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07536', 'title': 'I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow', 'url': 'https://huggingface.co/papers/2410.07536', 'abstract': "Rectified Flow Transformers (RFTs) offer superior training and inference efficiency, making them likely the most viable direction for scaling up diffusion models. However, progress in generation resolution has been relatively slow due to data quality and training costs. Tuning-free resolution extrapolation presents an alternative, but current methods often reduce generative stability, limiting practical application. In this paper, we review existing resolution extrapolation methods and introduce the I-Max framework to maximize the resolution potential of Text-to-Image RFTs. I-Max features: (i) a novel Projected Flow strategy for stable extrapolation and (ii) an advanced inference toolkit for generalizing model knowledge to higher resolutions. Experiments with Lumina-Next-2K and Flux.1-dev demonstrate I-Max's ability to enhance stability in resolution extrapolation and show that it can bring image detail emergence and artifact correction, confirming the practical value of tuning-free resolution extrapolation.", 'score': 5, 'issue_id': 99, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '2d11f82cc2214114', 'data': {'categories': ['#survey', '#cv', '#training', '#inference', '#optimization', '#diffusion', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'I-Max: Революция в экстраполяции разрешения для генерации изображений', 'desc': 'Статья представляет новый фреймворк I-Max для улучшения экстраполяции разрешения в трансформерах с выпрямленным потоком (RFT) для генерации изображений по тексту. I-Max включает стратегию Projected Flow для стабильной экстраполяции и продвинутый инструментарий для вывода, позволяющий обобщать знания модели на более высокие разрешения. Эксперименты показывают, что I-Max повышает стабильность экстраполяции разрешения и улучшает детализацию изображений. Авторы утверждают, что этот подход демонстрирует практическую ценность экстраполяции разрешения без дополнительного обучения.'}, 'en': {'title': '"Boosting Image Resolution: The I-Max Revolution in RFTs"', 'desc': 'The paper introduces Rectified Flow Transformers (RFTs) as a promising approach for improving the efficiency of training and inference in diffusion models. It highlights the challenges of increasing generation resolution due to data quality and training costs, and critiques current tuning-free resolution extrapolation methods for their instability. The authors propose the I-Max framework, which includes a Projected Flow strategy and an advanced inference toolkit, to enhance the resolution potential of Text-to-Image RFTs. Experiments demonstrate that I-Max improves stability and detail in image generation, making tuning-free resolution extrapolation more practical.'}, 'zh': {'title': '提升分辨率的I-Max框架：稳定且无需调参', 'desc': '这篇论文介绍了一种新的方法来提高文本到图像生成模型的分辨率，称为I-Max框架。I-Max框架通过引入投影流策略，解决了现有方法在分辨率外推时的不稳定性问题。实验表明，I-Max不仅能提高生成图像的细节和修正瑕疵，还能在不需要调参的情况下实现高分辨率生成。该方法在Lumina-Next-2K和Flux.1-dev上的实验验证了其实际应用价值。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09038', 'title': 'SimpleStrat: Diversifying Language Model Generation with Stratification', 'url': 'https://huggingface.co/papers/2410.09038', 'abstract': "Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, we show not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. We propose , an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata. To measure diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the output distribution and uniform distribution over valid ground truth answers. As computing probability per response/solution for proprietary models is infeasible, we measure recall on ground truth solutions. Our evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3.", 'score': 4, 'issue_id': 102, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '06e284b0650ef57d', 'data': {'categories': ['#dataset', '#inference', '#optimization', '#architecture', '#synthetic'], 'emoji': '🎭', 'ru': {'title': 'SimpleStrat: новый подход к разнообразной генерации текста', 'desc': 'В статье представлен новый метод генерации разнообразных ответов от больших языковых моделей (LLM) под названием SimpleStrat. Авторы показывают, что традиционный подход увеличения температуры для повышения разнообразия имеет недостатки. SimpleStrat использует саму языковую модель для разделения пространства на страты, из которых затем выбирается случайная страта для генерации ответа. Для оценки разнообразия был создан датасет CoverageQA с недоопределенными вопросами, имеющими несколько равновероятных ответов. Эксперименты показали, что SimpleStrat достигает более высокого recall и меньшего KL-расхождения по сравнению с базовыми методами.'}, 'en': {'title': 'SimpleStrat: Boosting Diversity Without Sacrificing Quality', 'desc': 'The paper discusses a new method for generating diverse responses from large language models, which is important for tasks like planning and synthetic data generation. Traditional methods increase the temperature to boost diversity, but this can lower the quality of responses. The authors propose a novel approach called SimpleStrat, which uses the model to divide the response space into different sections and randomly selects from these sections to generate diverse outputs. They introduce a new dataset, CoverageQA, to measure diversity and show that SimpleStrat improves recall and reduces KL Divergence compared to other models.'}, 'zh': {'title': '用语言模型自身提升响应多样性', 'desc': '这篇论文探讨了如何从大型语言模型中生成多样化的响应，这对于规划、搜索和合成数据生成等应用至关重要。传统方法通过增加温度来提高多样性，但这可能导致生成质量下降。作者提出了一种新方法，利用语言模型自身将空间划分为不同的层次，并从中随机选择一个层次进行采样。通过引入CoverageQA数据集，作者证明了这种方法在多样性和召回率上优于现有方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09045', 'title': 'MiRAGeNews: Multimodal Realistic AI-Generated News Detection', 'url': 'https://huggingface.co/papers/2410.09045', 'abstract': 'The proliferation of inflammatory or misleading "fake" news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -- is particularly potent and dangerous. To combat the spread of AI-generated fake news, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. We find that our dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. We release our code and data to aid future work on detecting AI-generated content.', 'score': 4, 'issue_id': 101, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '0fc6dec55a36d111', 'data': {'categories': ['#dataset', '#cv', '#security', '#ethics', '#open_source', '#multimodal'], 'emoji': '🕵️', 'ru': {'title': 'MiRAGeNews: Борьба с дезинформацией в эпоху ИИ-генерации', 'desc': 'Статья представляет набор данных MiRAGeNews, состоящий из 12 500 пар изображений и подписей, как реальных, так и сгенерированных ИИ. Авторы обнаружили, что этот датасет представляет значительную сложность как для людей (60% F1-меры), так и для современных мультимодальных языковых моделей (<24% F1-меры). На основе этого набора данных был обучен мультимодальный детектор MiRAGe, который превосходит современные базовые модели на 5,1% по F1-мере при работе с парами изображение-подпись из сторонних генераторов изображений и новостных издателей. Код и данные опубликованы для содействия будущим исследованиям по обнаружению контента, сгенерированного ИИ.'}, 'en': {'title': 'Detecting the Mirage: Battling AI-Generated Fake News', 'desc': 'The paper addresses the growing issue of AI-generated fake news by introducing the MiRAGeNews Dataset, which includes 12,500 image-caption pairs from both real and AI-generated sources. This dataset is challenging for both humans and advanced AI models to accurately classify, highlighting the difficulty in detecting AI-generated content. The authors developed a multi-modal detector called MiRAGe, which outperforms existing models by 5.1% in F-1 score on unseen data. By releasing their dataset and code, they aim to support further research in identifying AI-generated news content.'}, 'zh': {'title': 'MiRAGeNews：打击AI生成虚假新闻的利器', 'desc': '近年来，虚假新闻的传播变得越来越普遍，而使用人工智能工具生成逼真图像也变得更加容易。为了应对这种结合了AI生成的虚假新闻的威胁，我们提出了MiRAGeNews数据集，其中包含12,500对高质量的真实和AI生成的图像-标题对。我们的研究发现，这个数据集对人类和最先进的多模态大语言模型都构成了显著挑战。通过使用该数据集，我们训练了一个多模态检测器MiRAGe，在检测跨领域图像生成器和新闻发布者的图像-标题对时，性能提高了5.1%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09037', 'title': 'Mentor-KD: Making Small Language Models Better Multi-step Reasoners', 'url': 'https://huggingface.co/papers/2410.09037', 'abstract': "Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.", 'score': 4, 'issue_id': 97, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '2a269c59131978ed', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization', '#transfer_learning', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Mentor-KD: Эффективная передача навыков рассуждения от больших языковых моделей к меньшим', 'desc': 'Статья представляет новый метод под названием Mentor-KD для эффективной дистилляции способности к многошаговым рассуждениям от больших языковых моделей (LLM) к меньшим моделям. Метод использует промежуточную модель-наставника для расширения набора аннотаций Chain-of-Thought и предоставления мягких меток для модели-ученика. Авторы решают проблемы, связанные с качеством данных и предоставлением мягких меток при дистилляции рассуждений. Эксперименты подтверждают эффективность Mentor-KD для различных моделей и сложных задач рассуждения.'}, 'en': {'title': 'Mentor-KD: Elevating Small Models with Big Reasoning Skills', 'desc': 'The paper introduces Mentor-KD, a method to improve the transfer of reasoning skills from large language models (LLMs) to smaller models. It addresses challenges in knowledge distillation, such as the quality of data and the provision of soft labels, by using an intermediate-sized mentor model. This mentor model generates additional Chain-of-Thought annotations and soft labels to enhance the learning process of the student model. Experiments show that Mentor-KD effectively enhances the reasoning capabilities of smaller models across different tasks.'}, 'zh': {'title': 'Mentor-KD：提升小模型推理能力的新方法', 'desc': '这篇论文探讨了如何通过知识蒸馏技术将大型语言模型的多步推理能力传递给较小的模型。研究中提出了一种名为Mentor-KD的方法，通过使用中等大小的任务特定模型来增强数据质量和提供软标签。Mentor-KD有效地解决了以往方法中数据质量和软标签不足的问题。实验结果表明，Mentor-KD在多种模型和复杂推理任务中表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07331', 'title': 'DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.07331', 'abstract': 'We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.', 'score': 4, 'issue_id': 95, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'c2d3d40ef1bad864', 'data': {'categories': ['#reasoning', '#data', '#plp', '#agents', '#benchmark', '#games', '#open_source'], 'emoji': '📊', 'ru': {'title': 'DA-Code: Новый рубеж в оценке ИИ для анализа данных', 'desc': 'DA-Code - это новый бенчмарк для оценки способностей языковых моделей в задачах генерации кода для анализа данных. Он включает сложные задачи, требующие продвинутых навыков кодирования, основан на реальных разнообразных данных и охватывает широкий спектр задач по обработке и анализу данных. Бенчмарк создан в контролируемой исполняемой среде, имитирующей реальные сценарии анализа данных. Эксперименты показали, что даже лучшие современные языковые модели достигают точности лишь 30.5% на этом бенчмарке.'}, 'en': {'title': 'Pushing LLMs to Master Data Science Challenges', 'desc': 'DA-Code is a new benchmark designed to test large language models (LLMs) on complex data science tasks that require advanced coding skills. It includes challenging tasks based on real-world data, focusing on data wrangling and analytics. The benchmark requires models to use sophisticated programming languages to process data and find solutions. Initial tests show that even the best current models only achieve 30.5% accuracy, indicating significant potential for improvement.'}, 'zh': {'title': 'DA-Code：挑战大型语言模型的数据科学任务', 'desc': 'DA-Code 是一个专门为评估大型语言模型在基于代理的数据科学任务上的代码生成基准。这个基准包含三个核心元素：首先，DA-Code 中的任务具有挑战性，需要高级编码技能。其次，DA-Code 的例子基于真实多样的数据，涵盖复杂的数据处理和分析任务。最后，模型需要使用复杂的数据科学编程语言来解决任务。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08193', 'title': 'GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment', 'url': 'https://huggingface.co/papers/2410.08193', 'abstract': 'Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, we introduce GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, we demonstrate that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining.', 'score': 3, 'issue_id': 102, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'ebce9bee397ae5dd', 'data': {'categories': ['#training', '#inference', '#rl', '#optimization', '#alignment', '#rlhf'], 'emoji': '🎯', 'ru': {'title': 'GenARM: Эффективное тестовое выравнивание LLM без переобучения', 'desc': 'Статья представляет GenARM - новый подход к тестовому выравниванию больших языковых моделей (LLM) с использованием авторегрессивных моделей вознаграждения. Этот метод позволяет эффективно направлять замороженные LLM к желаемому распределению без переобучения. Экспериментальные результаты показывают, что GenARM превосходит существующие методы тестового выравнивания и соответствует производительности методов обучения. Подход также обеспечивает эффективное руководство от слабого к сильному и поддерживает многоцелевое выравнивание для удовлетворения разнообразных предпочтений пользователей.'}, 'en': {'title': 'Efficiently Aligning Language Models with GenARM: No Retraining Needed!', 'desc': 'The paper introduces GenARM, a novel method for aligning large language models (LLMs) with human preferences at test time without retraining. GenARM uses an Autoregressive Reward Model to predict next-token rewards, making it suitable for autoregressive text generation. This approach allows frozen LLMs to be guided efficiently, matching the performance of traditional training-time methods while avoiding high retraining costs. GenARM also supports multi-objective alignment, enabling real-time adjustments to cater to diverse user preferences.'}, 'zh': {'title': 'GenARM：高效对齐大型语言模型的新方法', 'desc': '大型语言模型（LLMs）功能强大，但需要与人类偏好对齐。传统方法在训练时微调模型，但成本高且需多次训练。GenARM是一种测试时对齐方法，使用自回归奖励模型来预测下一个词的奖励。实验表明，GenARM在效率和效果上优于现有方法，并支持多目标对齐。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08612', 'title': 'Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting', 'url': 'https://huggingface.co/papers/2410.08612', 'abstract': 'Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time, to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism.', 'score': 1, 'issue_id': 104, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '8000e8d6f78ec110', 'data': {'categories': ['#science', '#audio', '#dataset', '#cv', '#inference', '#data', '#diffusion', '#architecture', '#synthetic', '#multimodal'], 'emoji': '🌊', 'ru': {'title': 'Революция в синтезе сонарных изображений с помощью ИИ', 'desc': 'Статья представляет новый фреймворк Synth-SONAR для синтеза сонарных изображений, используя диффузионные модели и GPT-промптинг. Ключевые особенности включают интеграцию методов генеративного ИИ для инжекции стиля, двухуровневую иерархию диффузионных моделей для синтеза изображений разной детализации, а также использование визуальных языковых моделей и GPT-промптинга для генерации текстовых описаний. Synth-SONAR позволяет создавать разнообразные и реалистичные сонарные изображения на основе текстовых запросов, что является первым применением GPT-промптинга в области сонарных изображений.'}, 'en': {'title': '"Revolutionizing Sonar Imagery with AI: Synth-SONAR\'s Breakthrough"', 'desc': 'The paper introduces Synth-SONAR, a novel framework for generating synthetic sonar images using diffusion models and GPT prompting. By integrating Generative AI techniques with existing data, it creates a large and diverse sonar data corpus. The framework employs a dual text-conditioning model to produce high-quality, varied sonar images from textual descriptions. This approach marks the first use of GPT-prompting in sonar imagery, achieving state-of-the-art results in dataset quality and diversity.'}, 'zh': {'title': 'Synth-SONAR：用GPT提示革新声呐图像合成', 'desc': '这篇论文介绍了一种新的声呐图像合成框架，名为Synth-SONAR，利用扩散模型和GPT提示技术。该框架通过生成式AI风格注入技术和公开的真实/模拟数据，创建了一个大型声呐数据集。它采用双重文本条件的声呐扩散模型层次结构，生成高质量和多样化的粗粒度和细粒度声呐图像。该方法首次在声呐图像生成中应用GPT提示，显著提高了合成声呐数据集的多样性和真实性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09732', 'title': 'LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models', 'url': 'https://huggingface.co/papers/2410.09732', 'abstract': 'With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/', 'score': 54, 'issue_id': 107, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'ec9d4127c5348f2d', 'data': {'categories': ['#reasoning', '#video', '#audio', '#cv', '#interpretability', '#benchmark', '#open_source', '#synthetic', '#multimodal', '#3d'], 'emoji': '🕵️', 'ru': {'title': 'LOKI: Мультимодальный детектив для искусственного интеллекта', 'desc': 'LOKI - это новый бенчмарк для оценки способности больших мультимодальных моделей (LMM) обнаруживать синтетические данные в различных модальностях. Он включает в себя 18 тысяч вопросов по видео, изображениям, 3D, тексту и аудио, разделенных на 26 подкатегорий с четкими уровнями сложности. LOKI позволяет проводить комплексный анализ LMM через задачи грубой классификации, выбора из нескольких вариантов, а также выявления и объяснения аномалий. Авторы оценили 28 моделей LMM на этом бенчмарке, выявив их потенциал и ограничения в обнаружении синтетических данных.'}, 'en': {'title': 'LOKI: Unmasking Synthetic Data with Multimodal Models', 'desc': "The paper introduces LOKI, a benchmark designed to evaluate large multimodal models (LMMs) in detecting synthetic data across various modalities like video, image, and text. LOKI includes 18,000 questions that test the models' ability to distinguish real from synthetic data, providing insights into their perception and reasoning skills. The benchmark features different types of questions, such as multiple-choice and anomaly detection, to thoroughly assess the models' capabilities. The study evaluates 28 models, revealing both their potential and limitations in synthetic data detection."}, 'zh': {'title': 'LOKI：多模态合成数据检测的新基准', 'desc': '随着AI生成内容的快速发展，未来的互联网可能会充斥着合成数据，使得辨别真实和可信的多模态数据变得越来越困难。为了应对这一挑战，我们引入了LOKI，一个用于评估大型多模态模型（LMMs）检测合成数据能力的新基准。LOKI涵盖视频、图像、3D、文本和音频等多种模态，提供了18K个精心设计的问题，帮助全面分析LMMs的能力。我们对22个开源LMMs和6个闭源模型进行了评估，揭示了它们作为合成数据检测器的潜力和一些能力发展的局限性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10139', 'title': 'MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.10139', 'abstract': 'Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.', 'score': 50, 'issue_id': 107, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '6dc4d9e284dd99ca', 'data': {'categories': ['#science', '#dataset', '#training', '#math', '#healthcare', '#graphs', '#interpretability', '#plp', '#benchmark', '#open_source', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'MMIE: Новый стандарт для оценки чередующихся мультимодальных моделей', 'desc': 'MMIE - это новый масштабный бенчмарк для оценки моделей с чередующимся мультимодальным пониманием и генерацией (LVLMs). Он содержит 20 000 тщательно отобранных мультимодальных запросов из 12 областей знаний. MMIE поддерживает как чередующиеся входные данные, так и выходные, предлагая различные форматы вопросов. Авторы также предлагают автоматизированную метрику оценки, основанную на модели, дообученной на размеченных человеком данных.'}, 'en': {'title': 'MMIE: Elevating Multimodal Model Evaluation', 'desc': 'The paper introduces MMIE, a large-scale benchmark designed to evaluate the ability of Large Vision-Language Models (LVLMs) to understand and generate both images and text in mixed sequences. MMIE includes 20,000 multimodal queries across various fields like mathematics and arts, using both multiple-choice and open-ended questions to test different skills. The authors also propose a new automated evaluation metric that uses a scoring model fine-tuned with human data to reduce bias and improve accuracy. Experiments show that while current LVLMs have room for improvement, MMIE provides a comprehensive framework for assessing their capabilities.'}, 'zh': {'title': '推动多模态理解与生成的新时代', 'desc': '这篇论文介绍了一种新的评估基准MMIE，用于评估大型视觉语言模型（LVLMs）的多模态理解和生成能力。MMIE包含2万条精心设计的多模态查询，涵盖数学、编程、物理等多个领域，支持交错输入和输出。研究者还提出了一种可靠的自动化评估指标，利用经过人类标注数据微调的评分模型来减少偏差，提高评估准确性。实验结果表明，即使是最好的模型也有很大的改进空间，MMIE将推动交错LVLMs的发展。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10306', 'title': 'Animate-X: Universal Character Image Animation with Enhanced Motion Representation', 'url': 'https://huggingface.co/papers/2410.10306', 'abstract': 'Character image animation, which generates high-quality videos from a reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used in industries like gaming and entertainment. Our in-depth analysis suggests to attribute this limitation to their insufficient modeling of motion, which is unable to comprehend the movement pattern of the driving video, thus imposing a pose sequence rigidly onto the target character. To this end, this paper proposes Animate-X, a universal animation framework based on LDM for various character types (collectively named X), including anthropomorphic characters. To enhance motion representation, we introduce the Pose Indicator, which captures comprehensive motion pattern from the driving video through both implicit and explicit manner. The former leverages CLIP visual features of a driving video to extract its gist of motion, like the overall movement pattern and temporal relations among motions, while the latter strengthens the generalization of LDM by simulating possible inputs in advance that may arise during inference. Moreover, we introduce a new Animated Anthropomorphic Benchmark (A^2Bench) to evaluate the performance of Animate-X on universal and widely applicable animation images. Extensive experiments demonstrate the superiority and effectiveness of Animate-X compared to state-of-the-art methods.', 'score': 49, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'da7093c8b8de76a8', 'data': {'categories': ['#video', '#cv', '#graphs', '#benchmark', '#games', '#diffusion', '#architecture'], 'emoji': '🎭', 'ru': {'title': 'Универсальная анимация персонажей с улучшенным представлением движений', 'desc': 'Статья представляет Animate-X - универсальную систему анимации персонажей на основе LDM, включая антропоморфных. Авторы вводят Pose Indicator для улучшенного представления движений, используя как неявные, так и явные методы. Система использует визуальные признаки CLIP для извлечения сути движения из управляющего видео. Также представлен новый бенчмарк A^2Bench для оценки производительности на универсальных анимационных изображениях.'}, 'en': {'title': 'Animate-X: Bringing Characters to Life with Universal Animation', 'desc': "The paper introduces Animate-X, a novel animation framework designed to generate high-quality videos from reference images and pose sequences, applicable to a wide range of character types, including anthropomorphic ones. Unlike existing methods that struggle with non-human figures, Animate-X uses a Pose Indicator to capture motion patterns from driving videos, enhancing motion representation. This is achieved through both implicit methods, using CLIP visual features, and explicit methods, simulating potential inputs to improve generalization. The framework's effectiveness is validated using a new benchmark, A^2Bench, showing superior performance over current state-of-the-art techniques."}, 'zh': {'title': 'Animate-X：适用于各种角色的通用动画框架', 'desc': '这篇论文介绍了一种名为Animate-X的动画框架，可以从参考图像和目标姿势序列生成高质量视频。与现有方法不同，Animate-X适用于各种角色类型，包括拟人化角色。为了增强动作表示，作者引入了姿势指示器，通过隐式和显式方式捕捉驱动视频的运动模式。此外，论文还提出了一个新的基准A^2Bench，用于评估Animate-X在动画图像上的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09584', 'title': 'Toward General Instruction-Following Alignment for Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2410.09584', 'abstract': 'Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io.', 'score': 45, 'issue_id': 107, 'pub_date': '2024-10-12', 'pub_date_card': {'ru': '12 октября', 'en': 'October 12', 'zh': '10月12日'}, 'hash': 'd7ccc55af0bc4ea5', 'data': {'categories': ['#dataset', '#multilingual', '#rag', '#inference', '#data', '#benchmark', '#alignment', '#open_source', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'VIF-RAG: Новый подход к обучению RAG-систем следованию инструкциям', 'desc': 'VIF-RAG - это первый автоматизированный и масштабируемый конвейер для улучшения следования инструкциям в системах RAG. Авторы создали набор атомарных инструкций, правила их комбинирования и верификации, а также модели для перефразирования инструкций. На основе этого был сгенерирован большой датасет VIF-RAG-QA и создан бенчмарк FollowRAG для оценки RAG-систем. Эксперименты показали, что VIF-RAG значительно улучшает способность языковых моделей следовать инструкциям в различных сценариях.'}, 'en': {'title': 'Enhancing RAG Systems with VIF-RAG: A New Era of Instruction-Following Alignment', 'desc': 'The paper introduces VIF-RAG, a novel system designed to improve instruction-following alignment in Retrieval-Augmented Generation (RAG) systems. It uses a minimal set of manually crafted instructions and combination rules to create complex instructions, which are then verified using a Python executor. The system generates a high-quality dataset, VIF-RAG-QA, and introduces the FollowRAG Benchmark to evaluate instruction-following capabilities across various categories. The results show that VIF-RAG significantly enhances the performance of Large Language Models in following instructions within RAG contexts.'}, 'zh': {'title': '提升RAG系统的指令遵循能力', 'desc': '这篇论文提出了一种名为VIF-RAG的新方法，用于提高检索增强生成（RAG）系统中的指令遵循能力。研究人员通过手动设计少量基础指令，并开发组合规则来生成复杂指令，从而创建了一个高质量的数据集。通过监督模型和Python执行器，他们实现了指令重写和质量验证的自动化。最终，VIF-RAG显著提升了大语言模型在多种指令约束下的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10563', 'title': 'MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks', 'url': 'https://huggingface.co/papers/2410.10563', 'abstract': 'We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MMBench, and MMT-Bench), we embrace a wide range of output formats like numbers, phrases, code, \\LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. Unlike existing benchmarks, MEGA-Bench offers a fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate a wide variety of frontier vision-language models on MEGA-Bench to understand their capabilities across these dimensions.', 'score': 36, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '11a6963fbbeabd49', 'data': {'categories': ['#survey', '#optimization', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'MEGA-Bench: Масштабная оценка мультимодальных моделей на реальных задачах', 'desc': 'MEGA-Bench - это новый набор данных для оценки мультимодальных моделей, включающий более 500 реальных задач. Он охватывает широкий спектр форматов вывода и использует более 40 метрик для оценки. MEGA-Bench предоставляет подробный отчет о возможностях моделей по различным измерениям. Авторы провели оценку различных современных мультимодальных моделей на этом наборе данных.'}, 'en': {'title': 'MEGA-Bench: Unleashing Multimodal Evaluation Power', 'desc': 'MEGA-Bench is a comprehensive evaluation suite designed to test machine learning models on over 500 real-world multimodal tasks. It focuses on providing high-quality, diverse data samples to ensure accurate and cost-effective model evaluation. Unlike traditional benchmarks, MEGA-Bench supports a variety of output formats, such as numbers, phrases, and JSON, and uses over 40 metrics to assess model performance. This approach allows for a detailed analysis of model capabilities across different dimensions, offering users a deeper understanding of model strengths and weaknesses.'}, 'zh': {'title': 'MEGA-Bench：多模态任务的全面评估', 'desc': 'MEGA-Bench 是一个多模态评估套件，涵盖了超过 500 个真实世界任务，旨在优化高质量数据样本的多样性和丰富性。我们收集了 505 个任务和 8000 多个样本，支持多种输出格式，如数字、短语、代码等。为了评估这些任务，我们开发了 40 多种指标，提供细致的能力报告。通过 MEGA-Bench，我们可以深入了解视觉语言模型在不同维度上的能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10792', 'title': 'Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations', 'url': 'https://huggingface.co/papers/2410.10792', 'abstract': 'Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.', 'score': 26, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '064d3a6fd8a2ab06', 'data': {'categories': ['#cv', '#rl', '#optimization', '#diffusion', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная инверсия и редактирование изображений с помощью выпрямленных потоков', 'desc': 'Статья представляет новый метод инверсии и редактирования изображений с использованием стохастических эквивалентов моделей выпрямленного потока (Rectified Flow). Авторы предлагают инверсию RF с помощью динамического оптимального управления, полученного через линейный квадратичный регулятор. Метод обеспечивает высокую производительность в задачах инверсии и редактирования изображений без дополнительного обучения. Проведенные крупномасштабные оценки подтверждают преимущество метода перед существующими подходами.'}, 'en': {'title': 'Revolutionizing Image Inversion and Editing with Rectified Flows', 'desc': 'This paper explores how to reverse the process of generating images from noise, using a method called Rectified Flows (RFs), which is less explored compared to popular Diffusion Models (DMs). The authors propose a new way to invert images back to noise using a technique called dynamic optimal control, which is more efficient than current methods that require extra training or complex optimization. They also introduce a new tool for editing images, which allows for better performance in tasks like turning sketches into images or changing parts of an image. The results show that their method is preferred by users, offering a promising alternative to existing techniques.'}, 'zh': {'title': '逆转生成模型：从图像到噪声的创新之路', 'desc': '这篇论文研究了如何将生成模型生成的图像逆转回结构化的噪声，以便进行恢复和编辑。作者提出了一种基于动态最优控制的逆转方法，使用线性二次调节器来实现。该方法在零样本逆转和编辑任务中表现出色，优于现有的方法。大规模的人类评估显示，用户更偏好这种新方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07985', 'title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'url': 'https://huggingface.co/papers/2410.07985', 'abstract': "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.", 'score': 26, 'issue_id': 107, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '3d68a72c482a94bf', 'data': {'categories': ['#science', '#reasoning', '#dataset', '#math', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'Новый вызов для ИИ: олимпиадная математика как тест на интеллект', 'desc': 'Статья представляет новый эталонный набор данных для оценки математических способностей больших языковых моделей на уровне олимпиад. Набор данных содержит 4428 задач олимпиадного уровня с тщательной человеческой аннотацией, разделенных на 33 поддомена и 10 уровней сложности. Эксперименты показали, что даже самые продвинутые модели, такие как OpenAI o1-mini и OpenAI o1-preview, испытывают трудности с решением сложных олимпиадных задач, достигая точности 60.54% и 52.55% соответственно. Это исследование выявляет значительные проблемы в области математических рассуждений на олимпиадном уровне для современных языковых моделей.'}, 'en': {'title': 'Pushing the Limits: Challenging LLMs with Olympiad-Level Math', 'desc': "The paper introduces a new benchmark designed to test large language models (LLMs) on Olympiad-level mathematical reasoning, as existing benchmarks are no longer challenging enough. This new dataset includes 4428 competition-level problems, categorized into over 33 sub-domains and 10 difficulty levels, providing a comprehensive assessment of LLMs' capabilities. The study reveals that even advanced models like OpenAI o1-mini and OpenAI o1-preview struggle with these problems, achieving only 60.54% and 52.55% accuracy, respectively. This highlights the need for further advancements in LLMs to tackle complex mathematical reasoning tasks."}, 'zh': {'title': '挑战大型语言模型的奥林匹克数学推理能力', 'desc': '近年来，大型语言模型在数学推理能力上取得了显著突破。然而，现有的基准测试如GSM8K或MATH已经被高精度解决，显示出它们对这些模型的挑战性不足。为此，我们提出了一个专门设计的综合性挑战基准，用于评估大型语言模型在奥林匹克水平的数学推理能力。实验结果表明，即使是最先进的模型在面对奥林匹克级别的数学问题时仍然面临巨大挑战。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10783', 'title': 'LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content', 'url': 'https://huggingface.co/papers/2410.10783', 'abstract': 'The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here.', 'score': 25, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '14085b9b484efc2d', 'data': {'categories': ['#science', '#dataset', '#cv', '#training', '#benchmark', '#open_source', '#synthetic', '#multimodal'], 'emoji': '📊', 'ru': {'title': 'LiveXiv: Эволюционирующий бенчмарк для оценки мультимодальных моделей', 'desc': 'LiveXiv - это масштабируемый развивающийся бенчмарк, основанный на научных статьях ArXiv. Он автоматически генерирует пары вопрос-ответ по визуальному контенту (VQA) из рукописей, используя графики, диаграммы и таблицы. Бенчмарк предлагает эффективный подход к оценке производительности моделей, снижая общие затраты на оценку. LiveXiv позволяет оценить истинные способности мультимодальных моделей, избегая загрязнения тестовых данных.'}, 'en': {'title': 'LiveXiv: Unveiling True Model Capabilities with Dynamic Benchmarks', 'desc': 'The paper introduces LiveXiv, a dynamic benchmark designed to evaluate multi-modal models using scientific papers from ArXiv, ensuring no test data contamination. It automatically generates visual question-answer pairs from the content of these papers, like graphs and tables, without human intervention. The authors propose an efficient evaluation method that reduces costs by assessing only a subset of models, yet accurately estimates performance across all models. The benchmark reveals the true capabilities of large multi-modal models, with minimal performance variance between automatic and manually verified results.'}, 'zh': {'title': 'LiveXiv：科学论文驱动的动态基准测试', 'desc': '这篇论文介绍了一种名为LiveXiv的新方法，用于评估多模态模型的能力。通过从科学ArXiv论文中自动生成视觉问答对，LiveXiv避免了测试数据污染的问题。该方法不需要人工参与，利用论文中的图表和表格等多模态内容。研究表明，这种方法不仅降低了评估成本，还能准确反映模型的真实能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10774', 'title': 'Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention', 'url': 'https://huggingface.co/papers/2410.10774', 'abstract': 'In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce Cavia, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: https://ir1d.github.io/Cavia/', 'score': 23, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '95f9847ff43aeaf2', 'data': {'categories': ['#video', '#cv', '#data', '#diffusion', '#architecture', '#synthetic', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Cavia: Революция в генерации видео с контролем камеры', 'desc': 'Cavia - это новая система для генерации видео с контролируемой камерой и возможностью создания нескольких ракурсов. Она расширяет модули пространственного и временного внимания, улучшая согласованность ракурсов и временную согласованность. Cavia позволяет пользователю точно указывать движение камеры, одновременно получая движение объектов. Эксперименты показывают, что Cavia превосходит современные методы по геометрической согласованности и визуальному качеству.'}, 'en': {'title': 'Cavia: Mastering Camera Control in Image-to-Video Generation', 'desc': 'The paper introduces Cavia, a new framework for generating videos from images with improved 3D consistency and camera control. Cavia enhances video generation by using view-integrated attention modules, which ensure both viewpoint and temporal consistency across frames. This approach allows for precise camera motion control and can be trained with a variety of data sources, including static, synthetic, and real-world videos. Experiments show that Cavia outperforms existing methods in maintaining geometric consistency and high perceptual quality in the generated videos.'}, 'zh': {'title': 'Cavia：实现相机可控的多视角视频生成', 'desc': '近年来，图像到视频生成技术取得了显著突破，但生成帧的三维一致性和相机可控性仍未解决。Cavia是一个新框架，能够将输入图像转换为多个时空一致的视频，并允许用户精确控制相机运动。该框架通过将空间和时间注意力模块扩展为视图集成注意力模块，提高了视点和时间一致性。实验表明，Cavia在几何一致性和感知质量方面超越了现有方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10594', 'title': 'VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents', 'url': 'https://huggingface.co/papers/2410.10594', 'abstract': 'Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play crucial roles in real-world multi-modality documents. In this paper, we introduce VisRAG, which tackles this issue by establishing a vision-language model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the document to obtain text, the document is directly embedded using a VLM as an image and then retrieved to enhance the generation of a VLM. Compared to traditional text-based RAG, VisRAG maximizes the retention and utilization of the data information in the original documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in VisRAG and explore a variety of generation methods. Experiments demonstrate that VisRAG outperforms traditional RAG in both the retrieval and generation stages, achieving a 25--39\\% end-to-end performance gain over traditional text-based RAG pipeline. Further analysis reveals that VisRAG is effective in utilizing training data and demonstrates strong generalization capability, positioning it as a promising solution for RAG on multi-modality documents. Our code and data are available at https://github.com/openbmb/visrag .', 'score': 22, 'issue_id': 118, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '21d912d0f36db525', 'data': {'categories': ['#cv', '#rag', '#graphs', '#data', '#games', '#open_source', '#synthetic', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'VisRAG: Мультимодальный RAG для работы с документами как с изображениями', 'desc': 'VisRAG - это новый подход к извлечению и генерации информации (RAG), основанный на использовании мультимодальных моделей (VLM). В отличие от традиционных текстовых RAG-систем, VisRAG работает напрямую с изображениями документов, сохраняя всю визуальную информацию. Эксперименты показали, что VisRAG превосходит текстовые RAG-системы на 25-39% при работе с мультимодальными документами. Авторы предоставили код и данные в открытом доступе для дальнейших исследований.'}, 'en': {'title': 'VisRAG: Bridging Text and Vision for Smarter Document Understanding', 'desc': "The paper introduces VisRAG, a novel approach that enhances retrieval-augmented generation by incorporating visual information into the process. Unlike traditional RAG systems that rely solely on text, VisRAG uses a vision-language model to embed documents as images, preserving more information from the original source. This method significantly improves the performance of both retrieval and generation tasks, showing a 25-39% increase over text-based RAG systems. The study highlights VisRAG's strong generalization capabilities and its potential as a robust solution for handling multi-modality documents."}, 'zh': {'title': 'VisRAG：多模态文档处理的新突破', 'desc': '这篇论文介绍了一种名为VisRAG的新技术，它通过视觉-语言模型（VLM）来增强检索增强生成（RAG）系统的能力。VisRAG直接将文档作为图像嵌入，而不是先解析成文本，从而最大限度地保留和利用原始文档中的信息。实验表明，VisRAG在检索和生成阶段的表现优于传统的基于文本的RAG系统，性能提升达25%到39%。这种方法在多模态文档处理中展示了强大的泛化能力，是一种有前景的解决方案。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10818', 'title': 'TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models', 'url': 'https://huggingface.co/papers/2410.10818', 'abstract': "Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In this paper, we introduce TemporalBench, a new benchmark dedicated to evaluating fine-grained temporal understanding in videos. TemporalBench consists of ~10K video question-answer pairs, derived from ~2K high-quality human annotations detailing the temporal dynamics in video clips. As a result, our benchmark provides a unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc. Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models. Results show that state-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating a significant gap (~30%) between humans and AI in temporal understanding. Furthermore, we notice a critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find a centralized description as a cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias. We hope that TemporalBench can foster research on improving models' temporal reasoning capabilities. Both dataset and evaluation code will be made available.", 'score': 14, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '96570f7d74bf91e1', 'data': {'categories': ['#reasoning', '#video', '#dataset', '#benchmark', '#games', '#open_source', '#multimodal'], 'emoji': '⏳', 'ru': {'title': 'TemporalBench: Новый стандарт для оценки понимания временной динамики в видео', 'desc': 'TemporalBench - это новый бенчмарк для оценки детального понимания временной динамики в видео. Он состоит из примерно 10 тысяч пар вопрос-ответ по видео, основанных на высококачественных аннотациях временной динамики в видеоклипах. Бенчмарк позволяет оценивать различные аспекты временного понимания, такие как частота действий, величина движения, порядок событий, а также применим к различным задачам и моделям. Результаты показывают значительный разрыв между людьми и ИИ в понимании временной динамики видео.'}, 'en': {'title': 'TemporalBench: Bridging the Gap in Video Temporal Understanding', 'desc': 'The paper introduces TemporalBench, a new benchmark designed to evaluate fine-grained temporal understanding in videos, addressing the limitations of existing video benchmarks that lack detailed temporal annotations. TemporalBench includes around 10,000 video question-answer pairs based on high-quality human annotations, providing a unique platform for assessing temporal reasoning abilities like action frequency and event order. The benchmark reveals a significant gap in temporal understanding between humans and AI, with state-of-the-art models achieving only 38.5% accuracy. The authors propose a new evaluation metric, Multiple Binary Accuracy (MBA), to address biases in multi-choice question answering, aiming to enhance research in temporal reasoning capabilities of AI models.'}, 'zh': {'title': '提升视频时间理解的新基准：TemporalBench', 'desc': '这篇论文介绍了一个名为TemporalBench的新基准，用于评估视频中细粒度的时间理解能力。TemporalBench包含约1万个视频问答对，基于约2000个高质量的人类注释，详细描述了视频片段中的时间动态。研究表明，现有的顶尖模型在TemporalBench上的问答准确率仅为38.5%，显示出人类与AI在时间理解上的显著差距。论文还提出了一种新的评估方法，称为多重二元准确率（MBA），以纠正多选题中的偏差。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09335', 'title': 'Rethinking Data Selection at Scale: Random Selection is Almost All You Need', 'url': 'https://huggingface.co/papers/2410.09335', 'abstract': 'Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even exceeding those obtained using the entire dataset. However, most existing data selection techniques are designed for small-scale data pools, which fail to meet the demands of real-world SFT scenarios. In this paper, we replicated several self-scoring methods those that do not rely on external model assistance on two million scale datasets, and found that nearly all methods struggled to significantly outperform random selection when dealing with such large-scale data pools. Moreover, our comparisons suggest that, during SFT, diversity in data selection is more critical than simply focusing on high quality data. We also analyzed the limitations of several current approaches, explaining why they perform poorly on large-scale datasets and why they are unsuitable for such contexts. Finally, we found that filtering data by token length offers a stable and efficient method for improving results. This approach, particularly when training on long text data, proves highly beneficial for relatively weaker base models, such as Llama3.', 'score': 14, 'issue_id': 107, 'pub_date': '2024-10-12', 'pub_date_card': {'ru': '12 октября', 'en': 'October 12', 'zh': '10月12日'}, 'hash': 'b683d10c4851f00f', 'data': {'categories': ['#long_context', '#training', '#optimization', '#data', '#alignment', '#small_models'], 'emoji': '🔍', 'ru': {'title': 'Эффективный отбор данных для обучения больших языковых моделей', 'desc': 'Статья посвящена проблеме выбора данных для обучения больших языковых моделей (LLM) с помощью контролируемой доводки (SFT). Авторы провели репликацию нескольких методов самооценки на наборах данных из двух миллионов примеров и обнаружили, что большинство методов не превосходят случайный выбор. Исследование показало, что разнообразие данных важнее их качества при SFT. Авторы также предложили простой, но эффективный метод фильтрации данных по длине токенов, который особенно полезен для более слабых базовых моделей.'}, 'en': {'title': 'Diversity Over Quality: Rethinking Data Selection for LLM Fine-Tuning', 'desc': 'The paper explores the challenges of selecting a representative subset of data for supervised fine-tuning of Large Language Models (LLMs). It finds that most existing data selection methods, which work well on small datasets, struggle with large-scale datasets, often performing no better than random selection. The study highlights the importance of diversity in data selection over merely focusing on high-quality data. Additionally, it suggests that filtering data by token length can enhance the performance of weaker models during fine-tuning.'}, 'zh': {'title': '多样性胜于质量：大规模数据集中的监督微调策略', 'desc': '这篇论文探讨了监督微调（SFT）在大语言模型（LLM）中的重要性，尤其是在选择训练数据子集时。研究发现，现有的数据选择方法在大规模数据集上表现不佳，随机选择反而效果更好。论文指出，在SFT中，数据选择的多样性比单纯追求高质量数据更为重要。通过分析，作者发现通过筛选数据的词元长度可以稳定提高结果，尤其对较弱的基础模型如Llama3效果显著。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10630', 'title': 'Thinking LLMs: General Instruction Following with Thought Generation', 'url': 'https://huggingface.co/papers/2410.10630', 'abstract': 'LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks.', 'score': 9, 'issue_id': 118, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'ce8e4d06cae69001', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization', '#alignment', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Обучение ИИ думать перед ответом', 'desc': 'Статья представляет новый метод обучения языковых моделей (LLM) способности к явному мышлению перед ответом на вопросы. Авторы предлагают итеративную процедуру поиска и оптимизации, которая исследует пространство возможных генераций мыслей без использования дополнительных человеческих данных. Метод использует модель-судью для оценки ответов и оптимизацию на основе предпочтений. Результаты показывают улучшение производительности на наборах данных AlpacaEval и Arena-Hard, а также преимущества мышления в различных категориях задач.'}, 'en': {'title': 'Teaching LLMs to Think Before They Speak', 'desc': "The paper introduces a new training method for large language models (LLMs) to enhance their ability to think explicitly before answering questions. This method involves an iterative search and optimization process that allows the model to explore and learn from different thought processes without needing additional human data. The approach uses a judge model to score thought candidates and optimizes them through preference optimization, leading to improved performance across various tasks. The results show that this thinking ability enhances the model's performance not only in reasoning tasks but also in areas like marketing and health."}, 'zh': {'title': '让AI学会思考：无需人类数据的智能训练方法', 'desc': '这篇论文提出了一种新的训练方法，使得大型语言模型（LLM）在回答问题前能够进行明确的思考。通过迭代搜索和优化过程，模型可以在没有额外人类数据的情况下学习如何思考。每个指令的思考候选项通过一个评判模型进行评分，并通过偏好优化进行优化。实验结果表明，这种方法在复杂推理任务和非推理任务（如市场营销、健康和常识）上都表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10813', 'title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'url': 'https://huggingface.co/papers/2410.10813', 'abstract': 'Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages. Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope. Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI.', 'score': 9, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'b100ebbfd8b25c2e', 'data': {'categories': ['#reasoning', '#long_context', '#training', '#rag', '#optimization', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'LongMemEval: Новый стандарт оценки долговременной памяти чат-ассистентов', 'desc': 'Статья представляет LongMemEval - комплексный бенчмарк для оценки долгосрочной памяти чат-ассистентов на основе больших языковых моделей (LLM). Авторы выделяют пять ключевых способностей долгосрочной памяти и создают набор из 500 вопросов для их тестирования. Исследование показывает значительное снижение точности существующих систем при длительном взаимодействии. Предлагается унифицированная структура для улучшения долгосрочной памяти, включающая оптимизации индексирования, извлечения и чтения информации.'}, 'en': {'title': 'Enhancing Chat Assistants with Long-Term Memory Mastery', 'desc': 'The paper introduces LongMemEval, a benchmark to test the long-term memory abilities of chat assistants, focusing on five key areas like information extraction and temporal reasoning. It highlights the challenges faced by current systems, which show a significant drop in accuracy when dealing with sustained interactions. The authors propose a framework with innovative memory designs, such as session decomposition and time-aware query expansion, to enhance memory recall and question answering. These improvements aim to advance the personalization and reliability of conversational AI systems.'}, 'zh': {'title': '提升聊天助手的长期记忆能力', 'desc': '这篇论文介绍了一种名为LongMemEval的基准，用于评估聊天助手的五种核心长期记忆能力。研究发现，现有的商业聊天助手在长时间交互中记忆信息的准确率下降了30%。作者提出了一种统一框架，通过优化索引、检索和读取阶段来提升记忆性能。实验结果表明，这些优化显著提高了记忆召回率和问题回答能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06634', 'title': 'Tree of Problems: Improving structured problem solving with compositionality', 'url': 'https://huggingface.co/papers/2410.06634', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: https://github.com/ArmelRandy/tree-of-problems.', 'score': 8, 'issue_id': 109, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '0f052b591f948ce8', 'data': {'categories': ['#reasoning', '#training', '#architecture', '#open_source'], 'emoji': '🌳', 'ru': {'title': 'Древо проблем: новый метод для решения сложных задач с помощью ИИ', 'desc': 'В статье предлагается новый метод под названием Tree of Problems (ToP) для решения сложных задач с помощью больших языковых моделей. ToP является упрощенной версией метода Tree of Thoughts (ToT) и предназначен для задач, которые можно разделить на идентичные подзадачи. Авторы провели эмпирические исследования, показавшие превосходство ToP над методами ToT и Graph of Thoughts (GoT). Кроме того, ToP показал лучшие результаты, чем Chain-of-Thought (CoT) промптинг на сложных задачах рассуждения.'}, 'en': {'title': 'Simplifying Complexity: Tree of Problems Revolutionizes Reasoning', 'desc': "The paper introduces a new method called Tree of Problems (ToP) to improve the performance of Large Language Models (LLMs) on complex reasoning tasks. ToP simplifies the Tree of Thoughts (ToT) approach by breaking down complex problems into identical subtasks, which enhances the model's ability to solve them. The authors demonstrate that ToP outperforms both ToT and Graph of Thoughts (GoT), as well as Chain-of-Thought (CoT) prompting, in handling intricate reasoning challenges. This advancement suggests a promising direction for enhancing LLMs' problem-solving capabilities."}, 'zh': {'title': '问题树：简化复杂任务的有效方法', 'desc': '大型语言模型在多种任务中表现出色，尤其是在上下文学习中。对于需要逐步推理的复杂任务，链式思维提示结合自我一致性取得了显著效果。然而，一些任务对大型语言模型来说仍然很难解决。本文提出了问题树（ToP），一种更简单的思维树版本，实验结果表明其在复杂推理任务中优于思维树和思维图。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09733', 'title': 'MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.09733', 'abstract': "The advent of large Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling more sophisticated and accurate integration of visual and textual information across various tasks, including image and video captioning, visual question answering, and cross-modal retrieval. Despite VLMs' superior capabilities, researchers lack a comprehensive understanding of their compositionality -- the ability to understand and produce novel combinations of known visual and textual components. Prior benchmarks provide only a relatively rough compositionality evaluation from the perspectives of objects, relations, and attributes while neglecting deeper reasoning about object interactions, counting, and complex compositions. However, compositionality is a critical ability that facilitates coherent reasoning and understanding across modalities for VLMs. To address this limitation, we propose MMCOMPOSITION, a novel human-annotated benchmark for comprehensively and accurately evaluating VLMs' compositionality. Our proposed benchmark serves as a complement to these earlier works. With MMCOMPOSITION, we can quantify and explore the compositionality of the mainstream VLMs. Surprisingly, we find GPT-4o's compositionality inferior to the best open-source model, and we analyze the underlying reasons. Our experimental analysis reveals the limitations of VLMs in fine-grained compositional perception and reasoning, and points to areas for improvement in VLM design and training. Resources available at: https://hanghuacs.github.io/MMComposition/", 'score': 7, 'issue_id': 116, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'fb02158c865832bc', 'data': {'categories': ['#reasoning', '#cv', '#training', '#graphs', '#interpretability', '#benchmark', '#open_source', '#architecture', '#multimodal'], 'emoji': '🧩', 'ru': {'title': 'MMCOMPOSITION: новый взгляд на композиционность мультимодальных моделей', 'desc': 'MMCOMPOSITION - это новый бенчмарк для оценки композиционности крупных мультимодальных языковых моделей (VLM). Он позволяет более точно оценить способность моделей понимать и создавать новые комбинации известных визуальных и текстовых компонентов. Бенчмарк включает в себя тестирование глубокого рассуждения о взаимодействии объектов, подсчете и сложных композициях. Анализ с помощью MMCOMPOSITION выявил ограничения существующих VLM в точном композиционном восприятии и рассуждении.'}, 'en': {'title': 'Unlocking the Compositional Power of Vision-Language Models', 'desc': 'The paper introduces MMCOMPOSITION, a new benchmark designed to evaluate the compositionality of Vision-Language Models (VLMs), which is their ability to understand and generate new combinations of visual and textual elements. This benchmark addresses the shortcomings of previous evaluations by focusing on deeper reasoning tasks such as object interactions and complex compositions. The study reveals that some open-source models outperform GPT-4o in compositionality, highlighting areas where VLMs need improvement. The findings suggest that enhancing compositional perception and reasoning is crucial for advancing VLM capabilities.'}, 'zh': {'title': '探索视觉语言模型的组合能力', 'desc': '大型视觉语言模型（VLMs）在多模态理解上取得了显著进展，但其组合能力仍需深入研究。组合能力是指理解和生成已知视觉和文本元素的新组合的能力。为此，研究者提出了一个名为MMCOMPOSITION的新基准，用于全面评估VLMs的组合能力。实验结果显示，GPT-4o的组合能力不如某些开源模型，这揭示了VLMs在细粒度组合感知和推理上的局限性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10803', 'title': 'Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies', 'url': 'https://huggingface.co/papers/2410.10803', 'abstract': 'Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in 3D visuomotor policies, such as the 3D Diffusion Policy (DP3), have shown promise in extending these capabilities to wilder environments. However, 3D visuomotor policies often rely on camera calibration and point-cloud segmentation, which present challenges for deployment on mobile robots like humanoids. In this work, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D visuomotor policy that eliminates these constraints by leveraging egocentric 3D visual representations. We demonstrate that iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Videos are available at: https://humanoid-manipulation.github.io', 'score': 6, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '753b160b8fdc53e0', 'data': {'categories': ['#cv', '#synthetic', '#games', '#diffusion', '#architecture', '#robotics', '#3d'], 'emoji': '🤖', 'ru': {'title': 'Революция в автономной манипуляции гуманоидных роботов', 'desc': 'Статья представляет новый подход к автономной манипуляции гуманоидных роботов - Improved 3D Diffusion Policy (iDP3). Этот метод использует эгоцентрические 3D визуальные представления, что позволяет избежать ограничений, связанных с калибровкой камеры и сегментацией облака точек. iDP3 демонстрирует способность полноразмерного гуманоидного робота автономно выполнять задачи в различных реальных сценариях, используя только данные, собранные в лабораторных условиях. Это значительный шаг вперед в области автономной манипуляции гуманоидных роботов в разнообразных средах.'}, 'en': {'title': 'Breaking Barriers: Humanoids in the Real World', 'desc': 'The paper introduces the Improved 3D Diffusion Policy (iDP3), a new approach in 3D visuomotor policies for humanoid robots. iDP3 overcomes the limitations of previous methods by using egocentric 3D visual representations, eliminating the need for camera calibration and point-cloud segmentation. This advancement allows humanoid robots to autonomously perform tasks in various real-world environments using only lab-collected data. The research demonstrates significant progress in enabling humanoid robots to operate independently in diverse settings.'}, 'zh': {'title': '突破限制：类人机器人自主操作的新策略', 'desc': '这篇论文介绍了一种新的3D视觉运动策略，称为改进的3D扩散策略（iDP3），它可以让类人机器人在多种真实场景中自主操作。传统的3D视觉运动策略依赖于相机校准和点云分割，这对移动机器人来说是个挑战。iDP3通过利用自我中心的3D视觉表示，消除了这些限制。实验表明，iDP3使得全尺寸类人机器人仅凭实验室收集的数据就能在多样化的环境中执行技能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10819', 'title': 'DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads', 'url': 'https://huggingface.co/papers/2410.10819', 'abstract': "Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.", 'score': 5, 'issue_id': 132, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '689fba0f95046e53', 'data': {'categories': ['#long_context', '#training', '#inference', '#optimization', '#open_source', '#architecture', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'DuoAttention: эффективная обработка длинных контекстов в LLM', 'desc': 'Статья представляет DuoAttention - новый метод для оптимизации работы больших языковых моделей (LLM) с длинным контекстом. Авторы обнаружили, что только часть механизмов внимания (так называемые Retrieval Heads) критически важны для обработки длинных контекстов. Основываясь на этом, DuoAttention применяет полный KV-кэш только к Retrieval Heads, а для остальных (Streaming Heads) использует облегченный кэш фиксированной длины. Этот подход значительно снижает потребление памяти и ускоряет работу LLM без ущерба для способности обрабатывать длинные контексты.'}, 'en': {'title': 'Optimizing Long-Context LLMs with DuoAttention', 'desc': "This paper addresses the challenges of deploying long-context large language models (LLMs) by introducing a new framework called DuoAttention. It identifies that only certain attention heads, known as Retrieval Heads, are essential for processing long contexts, while others, called Streaming Heads, can operate with reduced memory. DuoAttention optimizes memory usage by applying a full Key-Value (KV) cache to Retrieval Heads and a lightweight cache to Streaming Heads, significantly improving efficiency. The proposed method achieves substantial reductions in memory and latency during inference, while maintaining the model's long-context capabilities with minimal accuracy loss."}, 'zh': {'title': 'DuoAttention：优化长上下文处理的创新框架', 'desc': '本文提出了一种名为DuoAttention的框架，旨在解决长上下文大语言模型（LLM）在计算和内存方面的挑战。我们发现，只有一部分注意力头（称为检索头）对处理长上下文至关重要，而其他注意力头（称为流式头）则主要关注最近的标记，不需要全注意力机制。DuoAttention通过对检索头应用完整的KV缓存，而对流式头使用轻量级的固定长度KV缓存，从而显著减少了LLM的解码和预填充内存及延迟，同时保持了长上下文的能力。我们的实验表明，该方法在内存和速度上都有显著提升，且对准确性影响最小。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09223', 'title': 'The Same But Different: Structural Similarities and Differences in Multilingual Language Modeling', 'url': 'https://huggingface.co/papers/2410.09223', 'abstract': 'We employ new tools from mechanistic interpretability in order to ask whether the internal structure of large language models (LLMs) shows correspondence to the linguistic structures which underlie the languages on which they are trained. In particular, we ask (1) when two languages employ the same morphosyntactic processes, do LLMs handle them using shared internal circuitry? and (2) when two languages require different morphosyntactic processes, do LLMs handle them using different internal circuitry? Using English and Chinese multilingual and monolingual models, we analyze the internal circuitry involved in two tasks. We find evidence that models employ the same circuit to handle the same syntactic process independently of the language in which it occurs, and that this is the case even for monolingual models trained completely independently. Moreover, we show that multilingual models employ language-specific components (attention heads and feed-forward networks) when needed to handle linguistic processes (e.g., morphological marking) that only exist in some languages. Together, our results provide new insights into how LLMs trade off between exploiting common structures and preserving linguistic differences when tasked with modeling multiple languages simultaneously.', 'score': 5, 'issue_id': 119, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '025fd1d188bbd7f3', 'data': {'categories': ['#multilingual', '#graphs', '#interpretability', '#transfer_learning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Универсальность и специфичность языковых структур в LLM', 'desc': 'Исследователи применили методы механистической интерпретации для анализа внутренней структуры больших языковых моделей (LLM) и их соответствия лингвистическим структурам языков обучения. Они изучали, используют ли LLM общие внутренние схемы для обработки схожих морфосинтаксических процессов в разных языках, и различные схемы для обработки различающихся процессов. Анализ английских и китайских моно- и мультиязычных моделей показал, что модели используют одинаковые схемы для одинаковых синтаксических процессов независимо от языка, даже в случае независимо обученных моноязычных моделей. Кроме того, мультиязычные модели используют специфичные для языка компоненты при необходимости обработки лингвистических процессов, существующих только в некоторых языках.'}, 'en': {'title': 'Unveiling the Linguistic Brain of AI: Shared Circuits and Unique Paths', 'desc': 'This paper explores how large language models (LLMs) internally process linguistic structures from different languages. It investigates whether LLMs use the same internal mechanisms for similar morphosyntactic processes across languages and different mechanisms for distinct processes. The study uses English and Chinese models to show that LLMs often use the same circuitry for similar syntactic tasks, even in monolingual models. Additionally, it finds that multilingual models can adapt by using language-specific components for unique linguistic features.'}, 'zh': {'title': '探索LLM内部结构与语言结构的对应关系', 'desc': '这篇论文研究了大型语言模型（LLM）的内部结构是否与其训练语言的语言结构相对应。研究发现，当两种语言使用相同的形态句法过程时，模型会使用相同的内部电路来处理，即使是完全独立训练的单语模型也是如此。此外，多语言模型在处理某些语言特有的语言过程时，会使用特定的语言组件。研究结果揭示了LLM在同时建模多种语言时，如何在利用共同结构和保留语言差异之间进行权衡。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07752', 'title': 'TVBench: Redesigning Video-Language Evaluation', 'url': 'https://huggingface.co/papers/2410.07752', 'abstract': 'Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.', 'score': 5, 'issue_id': 107, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'dd1e54c62f09078e', 'data': {'categories': ['#video', '#cv', '#interpretability', '#benchmark', '#games', '#open_source', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Новый взгляд на оценку видео-языковых моделей: важность временного анализа', 'desc': 'В статье рассматриваются проблемы существующих бенчмарков для оценки моделей видео-языкового понимания. Авторы выявили, что многие задачи можно решить без глубокого анализа временной информации в видео. Предложен новый бенчмарк TVBench, требующий высокого уровня понимания временных аспектов. Результаты показывают, что большинство современных видео-языковых моделей не справляются с TVBench, демонстрируя результаты на уровне случайного угадывания.'}, 'en': {'title': 'Rethinking Video Understanding: Beyond Static Frames', 'desc': "The paper discusses the limitations of current video-language benchmarks, which often don't require true temporal reasoning to solve. It identifies three main issues: tasks can be solved with static frame information, overly informative text, and reliance on world knowledge. The authors propose a new benchmark, TVBench, which demands genuine temporal understanding for video question-answering. They find that most state-of-the-art models perform poorly on TVBench, highlighting the need for improved temporal reasoning in video models."}, 'zh': {'title': '提升视频理解：从静态到动态的挑战', 'desc': '这篇论文探讨了视频语言模型的评估挑战，指出现有的基准测试存在问题。研究发现，许多任务可以通过单帧静态信息或文本信息解决，而不需要真正的视觉推理。为此，作者提出了一个新的基准测试TVBench，强调时间理解的重要性。结果显示，许多先进的模型在TVBench上的表现与随机水平相当，只有Gemini-Pro和Tarsier表现优异。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09637', 'title': "ReLU's Revival: On the Entropic Overload in Normalization-Free Large Language Models", 'url': 'https://huggingface.co/papers/2410.09637', 'abstract': "LayerNorm is a critical component in modern large language models (LLMs) for stabilizing training and ensuring smooth optimization. However, it introduces significant challenges in mechanistic interpretability, outlier feature suppression, faithful signal propagation, and computational and communication complexity of private inference. This work explores desirable activation functions in normalization-free decoder-only LLMs. Contrary to the conventional preference for the GELU in transformer-based models, our empirical findings demonstrate an {\\em opposite trend} -- ReLU significantly outperforms GELU in LayerNorm-free models, leading to an {\\bf 8.2\\%} perplexity improvement. We discover a key issue with GELU, where early layers experience entropic overload, leading to the under-utilization of the representational capacity of attention heads. This highlights that smoother activations like GELU are {\\em ill-suited} for LayerNorm-free architectures, whereas ReLU's geometrical properties -- specialization in input space and intra-class selectivity -- lead to improved learning dynamics and better information retention in the absence of LayerNorm. This study offers key insights for optimizing transformer architectures where LayerNorm introduces significant challenges.", 'score': 3, 'issue_id': 132, 'pub_date': '2024-10-12', 'pub_date_card': {'ru': '12 октября', 'en': 'October 12', 'zh': '10月12日'}, 'hash': 'da83f65a63281815', 'data': {'categories': ['#training', '#optimization', '#interpretability', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'ReLU побеждает GELU в трансформерах без нормализации', 'desc': 'Это исследование рассматривает эффективность различных функций активации в языковых моделях без нормализации слоёв. Результаты показывают, что ReLU значительно превосходит GELU в таких моделях, улучшая перплексию на 8.2%. Авторы обнаружили, что GELU приводит к перегрузке ранних слоёв энтропией, что снижает эффективность использования внимания. Геометрические свойства ReLU, такие как специализация в пространстве входных данных и внутриклассовая селективность, обеспечивают лучшую динамику обучения в отсутствие LayerNorm.'}, 'en': {'title': 'ReLU Revolutionizes LayerNorm-Free Language Models!', 'desc': "This paper investigates the role of activation functions in large language models (LLMs) that do not use LayerNorm. It finds that the ReLU activation function outperforms the commonly used GELU, resulting in an 8.2% improvement in perplexity. The authors identify a problem with GELU, where early layers become overloaded with entropy, which limits the effectiveness of attention heads. The study suggests that ReLU's properties enhance learning dynamics and information retention in models without LayerNorm, providing valuable insights for optimizing transformer architectures."}, 'zh': {'title': 'ReLU优于GELU：无LayerNorm模型的新发现', 'desc': '本文探讨了在无LayerNorm的解码器模型中，激活函数的选择对模型性能的影响。研究发现，与传统上在变换器模型中偏好使用GELU不同，ReLU在无LayerNorm模型中表现更佳，提升了8.2%的困惑度。GELU在早期层中出现熵过载，导致注意力头的表示能力未被充分利用。研究表明，ReLU的几何特性更适合无LayerNorm架构，从而改善了学习动态和信息保留。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11758', 'title': 'Latent Action Pretraining from Videos', 'url': 'https://huggingface.co/papers/2410.11758', 'abstract': 'We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model.', 'score': 2, 'issue_id': 132, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '541dfe85d4ca3878', 'data': {'categories': ['#video', '#training', '#synthetic', '#transfer_learning', '#architecture', '#robotics', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Предобучение роботов на видео без разметки действий', 'desc': 'Авторы представляют LAPA - метод предобучения моделей зрения-языка-действия (VLA) без использования размеченных данных о действиях робота. Они обучают модель квантования действий для извлечения дискретных латентных действий из видео, затем предобучают латентную VLA-модель для предсказания этих действий по наблюдениям и описаниям задач. Наконец, модель дообучается на небольшом наборе данных о манипуляциях робота для сопоставления латентных и реальных действий. Эксперименты показывают, что метод превосходит существующие подходы и позволяет использовать масштабные видеоданные для обучения робототехнических моделей.'}, 'en': {'title': 'Unlocking Action Learning Without Labels', 'desc': 'The paper presents Latent Action Pretraining for general Action models (LAPA), an innovative unsupervised approach for pretraining Vision-Language-Action (VLA) models without needing labeled robot actions. By utilizing internet-scale videos, LAPA circumvents the limitations of traditional methods that rely on human-collected action labels. The process involves training an action quantization model to identify discrete latent actions from video frames, followed by pretraining a VLA model to predict these actions based on observations and task descriptions. The results show that LAPA not only surpasses existing techniques in robot manipulation but also excels in tasks requiring language understanding and generalization to new scenarios.'}, 'zh': {'title': '利用潜在动作预训练提升机器人操作能力', 'desc': '我们提出了一种用于通用动作模型的潜在动作预训练方法（LAPA），这是一种无监督的预训练方法，旨在处理视觉-语言-动作（VLA）模型，而无需真实的机器人动作标签。现有的VLA模型通常依赖于人类操作员收集的动作标签，这限制了数据来源和规模。我们的方法通过学习互联网规模的视频数据，首先训练一个动作量化模型，然后预训练一个潜在的VLA模型，最后在小规模的机器人操作数据上进行微调。实验结果表明，我们的方法在机器人操作策略的训练上显著优于现有技术，并在需要语言条件、对未见物体的泛化以及对未见指令的语义泛化的实际操作任务中表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10814', 'title': 'Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free', 'url': 'https://huggingface.co/papers/2410.10814', 'abstract': 'While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.', 'score': 48, 'issue_id': 124, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'c2ddb801d2228a78', 'data': {'categories': ['#dataset', '#agi', '#interpretability', '#optimization', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'MoE LLM: Скрытый потенциал эмбеддингов без дополнительной настройки', 'desc': 'Исследование показывает, что маршрутизаторы экспертов в моделях Mixture-of-Experts (MoE) LLM могут служить готовой моделью для создания эмбеддингов с многообещающей производительностью на разнообразных задачах, не требуя дополнительной настройки. Анализ демонстрирует, что веса маршрутизации MoE (RW) дополняют скрытое состояние (HS) LLM, широко используемое для эмбеддингов. Предложенный метод MoEE, объединяющий RW и HS, показывает лучшую производительность, чем использование каждого по отдельности. Эксперименты проводились на 6 задачах эмбеддинга с 20 наборами данных из Massive Text Embedding Benchmark (MTEB).'}, 'en': {'title': 'Unlocking the Embedding Power of MoE LLMs: No Finetuning Needed!', 'desc': 'The paper investigates the potential of Mixture-of-Experts (MoE) large language models (LLMs) as effective embedding models without additional finetuning. It reveals that the expert routers in MoE LLMs can be used directly for embedding tasks, showing strong performance across various tasks. The study finds that the routing weights (RW) in MoE models complement the hidden states (HS) of LLMs, offering robustness to prompt variations and focusing on high-level semantics. By combining RW and HS, the proposed MoEE model achieves superior results, as demonstrated on multiple datasets from the Massive Text Embedding Benchmark (MTEB).'}, 'zh': {'title': '专家混合：提升语言模型嵌入能力的新路径', 'desc': '大型语言模型（LLM）在生成任务上表现出色，但其仅解码架构限制了其作为嵌入模型的潜力。我们研究了专家混合（MoE）LLM，发现其专家路由器无需微调即可在多种嵌入任务中表现良好。MoE的路由权重（RW）与隐藏状态（HS）互补，且对提示选择更具鲁棒性。我们提出的MoEE结合了RW和HS，显著提升了嵌入任务的性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09342', 'title': 'LLM$\\times$MapReduce: Simplified Long-Sequence Processing using Large Language Models', 'url': 'https://huggingface.co/papers/2410.09342', 'abstract': 'Enlarging the context window of large language models (LLMs) has become a crucial research area, particularly for applications involving extremely long texts. In this work, we propose a novel training-free framework for processing long texts, utilizing a divide-and-conquer strategy to achieve comprehensive document understanding. The proposed LLMtimesMapReduce framework splits the entire document into several chunks for LLMs to read and then aggregates the intermediate answers to produce the final output. The main challenge for divide-and-conquer long text processing frameworks lies in the risk of losing essential long-range information when splitting the document, which can lead the model to produce incomplete or incorrect answers based on the segmented texts. Disrupted long-range information can be classified into two categories: inter-chunk dependency and inter-chunk conflict. We design a structured information protocol to better cope with inter-chunk dependency and an in-context confidence calibration mechanism to resolve inter-chunk conflicts. Experimental results demonstrate that LLMtimesMapReduce can outperform representative open-source and commercial long-context LLMs, and is applicable to several different models.', 'score': 37, 'issue_id': 125, 'pub_date': '2024-10-12', 'pub_date_card': {'ru': '12 октября', 'en': 'October 12', 'zh': '10月12日'}, 'hash': '214a55e4bcf12eb6', 'data': {'categories': ['#open_source', '#training', '#architecture', '#long_context'], 'emoji': '📄', 'ru': {'title': 'Эффективная обработка длинных текстов без переобучения LLM', 'desc': 'Статья представляет новый подход к обработке длинных текстов с помощью больших языковых моделей (LLM). Авторы предлагают фреймворк LLM×MapReduce, который разделяет документ на части для обработки, а затем объединяет промежуточные результаты. Для решения проблем потери информации при разделении текста разработаны протокол структурированной информации и механизм калибровки уверенности в контексте. Эксперименты показывают, что LLM×MapReduce превосходит существующие модели с длинным контекстом и применим к различным LLM.'}, 'en': {'title': 'Divide and Conquer: Mastering Long Texts with LLMtimesMapReduce', 'desc': "The paper introduces a new method called LLMtimesMapReduce to help large language models (LLMs) understand very long texts without needing extra training. It uses a divide-and-conquer approach, breaking the text into smaller parts for the LLMs to process, and then combines the results to form a complete understanding. The main challenge is ensuring that important information isn't lost when the text is split, which the authors address with a structured information protocol and a confidence calibration mechanism. Tests show that this method works better than other similar tools and can be used with different LLMs."}, 'zh': {'title': '分而治之：无训练框架处理超长文本', 'desc': '这篇论文提出了一种新的无训练框架，称为LLMtimesMapReduce，用于处理超长文本。该框架通过分而治之的方法，将文档分成多个部分供大语言模型（LLM）阅读，然后汇总中间答案以生成最终输出。主要挑战在于分割文档时可能丢失重要的长距离信息，导致模型基于分段文本产生不完整或错误的答案。实验结果表明，LLMtimesMapReduce在处理长文本方面优于其他开源和商业长上下文LLM。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10626', 'title': 'Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts', 'url': 'https://huggingface.co/papers/2410.10626', 'abstract': 'Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.', 'score': 37, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'e8c698bf76856366', 'data': {'categories': ['#dataset', '#multilingual', '#healthcare', '#interpretability', '#optimization', '#transfer_learning', '#architecture', '#low_resource'], 'emoji': '🏥', 'ru': {'title': 'Многоязычные медицинские LLM: преодоление языковых барьеров в здравоохранении', 'desc': 'Статья посвящена адаптации больших языковых моделей (LLM) для медицинских целей в различных языках. Авторы создают качественный медицинский датасет и исследуют внутренние механизмы многоязычных LLM с использованием модульности Mixture of Experts (MoE). Они предлагают новый метод маршрутизации MoE с языково-специфичными экспертами и кросс-языковой маршрутизацией. На основе анализа информационного потока разработана архитектура Post-MoE, применяющая разреженную маршрутизацию только в поздних слоях модели.'}, 'en': {'title': 'Breaking Language Barriers in Medical AI', 'desc': "The paper addresses the challenge of adapting medical Large Language Models (LLMs) to low-resource languages by constructing a high-quality medical dataset and analyzing its quality. It introduces a novel Mixture of Experts (MoE) routing method that uses language-specific experts and cross-lingual routing to enhance the model's generalization capabilities. Inspired by circuit theory, the authors propose the Post-MoE architecture, which applies sparse routing in later layers to improve language-specific performance while maintaining interpretability. The approach allows the model to scale efficiently to 50 languages by using language family experts, leveraging linguistic priors without increasing parameters."}, 'zh': {'title': '跨语言医疗模型：突破语言障碍的创新之路', 'desc': '这篇论文探讨了如何将大型语言模型（LLM）适应本地语言，以减少获取医疗服务的障碍。研究中，作者首先构建了一个高质量的医疗数据集，并进行了分析以确保其质量。通过使用专家混合（MoE）模块化方法，研究了多语言LLM的内部信息流，提出了一种新的MoE路由方法。实验结果表明，这种方法提高了多语言模型的泛化能力，同时保持了解释性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2406.15786', 'title': 'What Matters in Transformers? Not All Attention is Needed', 'url': 'https://huggingface.co/papers/2406.15786', 'abstract': 'While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: https://github.com/Shwai-He/LLM-Drop.', 'score': 27, 'issue_id': 121, 'pub_date': '2024-06-22', 'pub_date_card': {'ru': '22 июня', 'en': 'June 22', 'zh': '6月22日'}, 'hash': 'fdf872015ec393aa', 'data': {'categories': ['#training', '#inference', '#optimization', '#open_source', '#architecture'], 'emoji': '✂️', 'ru': {'title': 'Оптимизация LLM: меньше слоев, та же мощность', 'desc': 'Это исследование посвящено проблеме избыточности в архитектуре больших языковых моделей (LLM) на базе трансформеров. Авторы обнаружили, что значительная часть слоев внимания (attention layers) обладает высокой схожестью и может быть удалена без существенной потери производительности. Например, модель Llama-2-70B достигла ускорения на 48.4% при снижении производительности всего на 2.4% после удаления половины слоев внимания. Исследователи также предложили метод совместного удаления слоев внимания и MLP, что позволяет еще более агрессивно сокращать архитектуру модели.'}, 'en': {'title': 'Prune to Perform: Streamlining Transformers for Efficiency', 'desc': "This paper explores the redundancy in Transformer-based large language models, focusing on the MLP and Attention layers. The authors found that many attention layers are highly similar and can be pruned without significantly affecting performance, as demonstrated by a 48.4% speedup in Llama-2-70B with minimal performance loss. They also propose a method to jointly prune Attention and MLP layers, achieving substantial efficiency gains while maintaining most of the model's performance. This research offers insights into optimizing Transformer architectures for more efficient real-world applications."}, 'zh': {'title': '优化Transformer：减少冗余，提升效率', 'desc': '这篇论文研究了在Transformer模型中不同模块的冗余问题，特别是注意力层和MLP层。研究发现，尽管注意力层在Transformer中非常重要，但许多层的相似度过高，可以被剪枝而不影响性能。通过剪枝注意力层，模型的速度可以显著提高，而性能损失很小。此外，论文还提出了一种同时剪枝注意力层和MLP层的方法，进一步提高了模型的效率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11779', 'title': 'MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation', 'url': 'https://huggingface.co/papers/2410.11779', 'abstract': 'Multimodal Large Language Models (MLLMs) frequently exhibit hallucination phenomena, but the underlying reasons remain poorly understood. In this paper, we present an empirical analysis and find that, although MLLMs incorrectly generate the objects in the final output, they are actually able to recognize visual objects in the preceding layers. We speculate that this may be due to the strong knowledge priors of the language model suppressing the visual information, leading to hallucinations. Motivated by this, we propose a novel dynamic correction decoding method for MLLMs (DeCo), which adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. Note that DeCo is model agnostic and can be seamlessly incorporated with various classic decoding strategies and applied to different MLLMs. We evaluate DeCo on widely-used benchmarks, demonstrating that it can reduce hallucination rates by a large margin compared to baselines, highlighting its potential to mitigate hallucinations. Code is available at https://github.com/zjunlp/DeCo.', 'score': 24, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '25a9176442c65ae3', 'data': {'categories': ['#hallucinations', '#training', '#interpretability', '#benchmark', '#open_source', '#architecture', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Борьба с галлюцинациями в мультимодальных моделях', 'desc': 'Статья исследует проблему галлюцинаций в мультимодальных больших языковых моделях (MLLM). Авторы обнаружили, что MLLM способны распознавать визуальные объекты в промежуточных слоях, но в итоговом выводе могут их некорректно генерировать. Предполагается, что это связано с подавлением визуальной информации сильными языковыми приорами модели. Для решения проблемы предложен метод декодирования DeCo, адаптивно интегрирующий знания из предыдущих слоев в финальный слой.'}, 'en': {'title': 'DeCo: Correcting Hallucinations in Multimodal Models', 'desc': "This paper investigates why Multimodal Large Language Models (MLLMs) often produce hallucinations, where they generate incorrect outputs. The authors discover that while MLLMs can recognize visual objects in earlier layers, the language model's strong knowledge priors may suppress this visual information, causing hallucinations. To address this, they introduce a new method called dynamic correction decoding (DeCo), which adjusts the integration of knowledge in the final output layer. DeCo is versatile and can be used with different models and decoding strategies, significantly reducing hallucination rates in tests."}, 'zh': {'title': '动态校正解码：减少多模态模型幻觉的创新方法', 'desc': '多模态大语言模型（MLLMs）常常会出现幻觉现象，但其原因尚不明确。本文通过实证分析发现，尽管MLLMs在最终输出中错误生成对象，但它们实际上能够在前面的层中识别视觉对象。我们推测这可能是由于语言模型的强知识先验抑制了视觉信息，导致幻觉现象。为此，我们提出了一种新的动态校正解码方法（DeCo），可以自适应地选择合适的前置层，并将知识按比例整合到最终层中，以调整输出。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10816', 'title': 'LVD-2M: A Long-take Video Dataset with Temporally Dense Captions', 'url': 'https://huggingface.co/papers/2410.10816', 'abstract': 'The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe our work will significantly contribute to future research in long video generation.', 'score': 19, 'issue_id': 123, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'b7f807e5ff9e1614', 'data': {'categories': ['#video', '#dataset', '#long_context', '#training', '#data', '#synthetic'], 'emoji': '🎥', 'ru': {'title': 'LVD-2M: Новый стандарт для обучения генерации длинных видео', 'desc': 'Статья представляет новый датасет LVD-2M для обучения моделей генерации длинных видео. Датасет содержит 2 миллиона видео длительностью более 10 секунд, снятых одним кадром и сопровождаемых плотными временными подписями. Авторы разработали методику отбора качественных видео и создания аннотаций. Эффективность датасета подтверждена экспериментами по дообучению моделей генерации видео.'}, 'en': {'title': '"Unlocking the Future of Long Video Generation"', 'desc': 'The paper discusses the importance of high-quality datasets for training video generation models, especially for long videos. It introduces a new dataset, LVD-2M, which includes 2 million long-take videos, each over 10 seconds, with diverse content and detailed captions. The authors developed a pipeline to select these videos based on specific quality metrics and to generate temporally dense captions. This dataset aims to advance research in generating long videos with dynamic motions.'}, 'zh': {'title': '推动长视频生成研究的新数据集', 'desc': '这篇论文讨论了视频生成模型的训练数据集质量对模型效果的重要性。作者指出，现有的视频生成模型大多基于短视频片段，而长视频生成的研究受到高质量长视频缺乏的限制。为此，作者提出了一种新的数据集选择流程，专注于长视频的质量评估和密集字幕生成。最终，他们创建了一个名为LVD-2M的数据集，包含200万段长视频，并验证了其在长视频生成中的有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11710', 'title': 'MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.11710', 'abstract': 'Large Language Models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the "multi-granularity" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench. Code and data will be released at https: //github.com/MTU-Bench-Team/MTU-Bench.git.', 'score': 18, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '44b55db6ab50c3d4', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#data', '#benchmark', '#open_source'], 'emoji': '🛠️', 'ru': {'title': 'Многогранная оценка навыков LLM в использовании инструментов', 'desc': 'В этой статье представлен новый бенчмарк MTU-Bench для оценки навыков использования инструментов большими языковыми моделями (LLM). MTU-Bench охватывает пять сценариев использования инструментов, от простых до сложных и нестандартных задач. Оценка производится без использования API GPT или человеческой экспертизы, что снижает затраты. Авторы также предлагают набор данных MTU-Instruct для улучшения способностей LLM в использовании инструментов.'}, 'en': {'title': 'MTU-Bench: Elevating LLM Tool-Use Evaluation', 'desc': "The paper introduces MTU-Bench, a new benchmark designed to evaluate large language models (LLMs) on their ability to use tools effectively. MTU-Bench addresses the limitations of existing datasets by providing a multi-granularity approach, covering various tool-use scenarios such as single or multiple tools and turns, as well as out-of-distribution tasks. Unlike previous benchmarks, MTU-Bench relies solely on prediction results and ground truth for evaluation, avoiding costly GPT or human assessments. Additionally, the authors present MTU-Instruct, a dataset aimed at enhancing LLMs' tool-use capabilities, demonstrating its effectiveness through comprehensive experiments."}, 'zh': {'title': 'MTU-Bench：提升语言模型工具使用能力的新基准', 'desc': '大型语言模型在推理和决策能力上有了显著提升，并能与用户进行自然对话。现有的工具使用基准数据集存在评估场景不足和评估成本高的问题。为了解决这些问题，本文提出了一个多粒度工具使用基准MTU-Bench，涵盖五种工具使用场景。实验结果表明，MTU-Bench在提升语言模型工具使用能力方面效果显著。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11795', 'title': 'Efficient Diffusion Models: A Comprehensive Survey from Principles to Practices', 'url': 'https://huggingface.co/papers/2410.11795', 'abstract': 'As one of the most popular and sought-after generative models in the recent years, diffusion models have sparked the interests of many researchers and steadily shown excellent advantage in various generative tasks such as image synthesis, video generation, molecule design, 3D scene rendering and multimodal generation, relying on their dense theoretical principles and reliable application practices. The remarkable success of these recent efforts on diffusion models comes largely from progressive design principles and efficient architecture, training, inference, and deployment methodologies. However, there has not been a comprehensive and in-depth review to summarize these principles and practices to help the rapid understanding and application of diffusion models. In this survey, we provide a new efficiency-oriented perspective on these existing efforts, which mainly focuses on the profound principles and efficient practices in architecture designs, model training, fast inference and reliable deployment, to guide further theoretical research, algorithm migration and model application for new scenarios in a reader-friendly way. https://github.com/ponyzym/Efficient-DMs-Survey', 'score': 16, 'issue_id': 124, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': 'e1a8ba0c4193aa99', 'data': {'categories': ['#video', '#survey', '#training', '#inference', '#diffusion', '#architecture', '#multimodal', '#3d'], 'emoji': '🌀', 'ru': {'title': 'Эффективные диффузионные модели: от теории к практике', 'desc': 'Эта статья представляет собой обзор диффузионных моделей, одного из самых популярных и востребованных типов генеративных моделей последних лет. Авторы предоставляют комплексный анализ принципов работы и эффективных практик применения диффузионных моделей в различных задачах, таких как синтез изображений, генерация видео и молекулярный дизайн. Особое внимание уделяется архитектуре моделей, методам обучения, быстрому выводу и надежному развертыванию. Статья призвана помочь исследователям и практикам быстро понять и применить диффузионные модели в новых сценариях.'}, 'en': {'title': 'Unlocking the Power of Diffusion Models: Efficiency in Generative Tasks', 'desc': 'Diffusion models have become a popular choice for generative tasks like image and video creation due to their strong theoretical foundations and practical applications. This paper reviews the design principles and methodologies that make diffusion models efficient and effective. It aims to provide a comprehensive understanding of these models to facilitate their application in new scenarios. The survey focuses on architecture design, model training, fast inference, and reliable deployment to guide future research and development.'}, 'zh': {'title': '扩散模型：高效生成的未来', 'desc': '扩散模型是近年来非常受欢迎的生成模型，广泛应用于图像合成、视频生成等任务。它们的成功主要归功于渐进的设计原则和高效的架构、训练、推理及部署方法。本文综述了这些原则和实践，提供了一个以效率为导向的新视角。通过这种方式，帮助读者更好地理解和应用扩散模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11096', 'title': 'SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI', 'url': 'https://huggingface.co/papers/2410.11096', 'abstract': "Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI. These risks are primarily reflected in two areas: a model potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness). While these benchmarks have made significant strides, there remain opportunities for further improvement. For instance, many current benchmarks tend to focus more on a model ability to provide attack suggestions rather than its capacity to generate executable attacks. Additionally, most benchmarks rely heavily on static evaluation metrics, which may not be as precise as dynamic metrics such as passing test cases. Conversely, expert-verified benchmarks, while offering high-quality data, often operate at a smaller scale. To address these gaps, we develop SecCodePLT, a unified and comprehensive evaluation platform for code GenAIs' risks. For insecure code, we introduce a new methodology for data creation that combines experts with automatic generation. Our methodology ensures the data quality while enabling large-scale generation. We also associate samples with test cases to conduct code-related dynamic evaluation. For cyberattack helpfulness, we set up a real environment and construct samples to prompt a model to generate actual attacks, along with dynamic metrics in our environment. We conduct extensive experiments and show that SecCodePLT outperforms the state-of-the-art (SOTA) benchmark CyberSecEval in security relevance. Furthermore, it better identifies the security risks of SOTA models in insecure coding and cyberattack helpfulness. Finally, we apply SecCodePLT to the SOTA code agent, Cursor, and, for the first time, identify non-trivial security risks in this advanced coding agent.", 'score': 12, 'issue_id': 121, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'f1b69c45d56f76d8', 'data': {'categories': ['#security', '#data', '#plp', '#agents', '#benchmark', '#optimization', '#synthetic'], 'emoji': '🛡️', 'ru': {'title': 'SecCodePLT: Новый стандарт оценки безопасности AI-кодогенераторов', 'desc': 'Статья представляет SecCodePLT - новую платформу для оценки рисков безопасности генеративных моделей AI для кода. Авторы разработали методологию создания данных, сочетающую экспертную оценку и автоматическую генерацию, а также внедрили динамические метрики оценки. SecCodePLT позволяет оценивать как потенциал моделей генерировать небезопасный код, так и их полезность для кибератак. Эксперименты показали превосходство SecCodePLT над существующими бенчмарками в выявлении рисков безопасности современных моделей кодогенерации.'}, 'en': {'title': 'SecCodePLT: Elevating Code GenAI Security Evaluation', 'desc': "The paper introduces SecCodePLT, a new platform designed to evaluate the security risks of Code GenAI models more effectively. It addresses the limitations of existing benchmarks by combining expert input with automatic data generation to ensure high-quality, large-scale data for insecure code evaluation. The platform also uses dynamic evaluation metrics, such as test cases, to assess the models' ability to generate executable attacks in a real environment. Extensive experiments demonstrate that SecCodePLT surpasses current benchmarks in identifying security risks, revealing significant vulnerabilities in advanced coding agents like Cursor."}, 'zh': {'title': 'SecCodePLT：提升代码生成AI安全评估的新平台', 'desc': '这篇论文介绍了一个名为SecCodePLT的新平台，用于评估代码生成AI的安全风险。SecCodePLT通过结合专家和自动生成的方法，创建高质量且大规模的数据集，并使用动态评估方法来检测不安全代码和网络攻击的生成能力。与现有的基准相比，SecCodePLT在安全相关性上表现更好，并能更准确地识别最先进模型的安全风险。研究还首次在先进的代码代理Cursor中发现了显著的安全风险。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09704', 'title': 'EchoPrime: A Multi-Video View-Informed Vision-Language Model for Comprehensive Echocardiography Interpretation', 'url': 'https://huggingface.co/papers/2410.09704', 'abstract': 'Echocardiography is the most widely used cardiac imaging modality, capturing ultrasound video data to assess cardiac structure and function. Artificial intelligence (AI) in echocardiography has the potential to streamline manual tasks and improve reproducibility and precision. However, most echocardiography AI models are single-view, single-task systems that do not synthesize complementary information from multiple views captured during a full exam, and thus lead to limited performance and scope of applications. To address this problem, we introduce EchoPrime, a multi-view, view-informed, video-based vision-language foundation model trained on over 12 million video-report pairs. EchoPrime uses contrastive learning to train a unified embedding model for all standard views in a comprehensive echocardiogram study with representation of both rare and common diseases and diagnoses. EchoPrime then utilizes view-classification and a view-informed anatomic attention model to weight video-specific interpretations that accurately maps the relationship between echocardiographic views and anatomical structures. With retrieval-augmented interpretation, EchoPrime integrates information from all echocardiogram videos in a comprehensive study and performs holistic comprehensive clinical echocardiography interpretation. In datasets from two independent healthcare systems, EchoPrime achieves state-of-the art performance on 23 diverse benchmarks of cardiac form and function, surpassing the performance of both task-specific approaches and prior foundation models. Following rigorous clinical evaluation, EchoPrime can assist physicians in the automated preliminary assessment of comprehensive echocardiography.', 'score': 11, 'issue_id': 127, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '5bf065e3ea6b9f7a', 'data': {'categories': ['#science', '#video', '#dataset', '#cv', '#healthcare', '#optimization', '#transfer_learning', '#benchmark', '#architecture', '#multimodal'], 'emoji': '❤️', 'ru': {'title': 'EchoPrime: революция в автоматизированной интерпретации эхокардиограмм', 'desc': 'EchoPrime - это многозадачная модель искусственного интеллекта для анализа эхокардиограмм, обученная на более чем 12 миллионах пар видео-отчетов. Она использует контрастное обучение для создания единой модели вложений для всех стандартных проекций в комплексном эхокардиографическом исследовании. EchoPrime применяет классификацию проекций и анатомическую модель внимания для точной интерпретации взаимосвязи между эхокардиографическими проекциями и анатомическими структурами. Модель превосходит существующие подходы в 23 различных задачах оценки формы и функции сердца, что было подтверждено в двух независимых медицинских системах.'}, 'en': {'title': 'EchoPrime: Revolutionizing Heart Imaging with Multi-View AI', 'desc': 'EchoPrime is a new AI model designed to improve echocardiography by integrating information from multiple ultrasound views, unlike traditional single-view models. It uses contrastive learning to create a unified representation of echocardiogram data, capturing both common and rare cardiac conditions. The model employs view-classification and anatomic attention to accurately interpret the relationship between different views and heart structures. EchoPrime has demonstrated superior performance across various benchmarks, offering a more comprehensive and precise tool for cardiac assessment.'}, 'zh': {'title': 'EchoPrime：多视角心脏超声AI的革新', 'desc': 'EchoPrime 是一个多视角、视图知情的视频语言基础模型，旨在解决传统单视角AI模型的局限性。它通过对比学习训练一个统一的嵌入模型，能够处理标准心脏超声检查中的所有视图。EchoPrime 利用视图分类和解剖注意模型，准确映射超声视图与解剖结构之间的关系。经过严格的临床评估，EchoPrime 在心脏形态和功能的23个基准测试中表现优异，能够帮助医生进行自动化的初步评估。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11805', 'title': 'NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of Large Language Models', 'url': 'https://huggingface.co/papers/2410.11805', 'abstract': 'Large language models (LLMs) combined with tool learning have gained impressive results in real-world applications. During tool learning, LLMs may call multiple tools in nested orders, where the latter tool call may take the former response as its input parameters. However, current research on the nested tool learning capabilities is still under-explored, since the existing benchmarks lack of relevant data instances. To address this problem, we introduce NesTools to bridge the current gap in comprehensive nested tool learning evaluations. NesTools comprises a novel automatic data generation method to construct large-scale nested tool calls with different nesting structures. With manual review and refinement, the dataset is in high quality and closely aligned with real-world scenarios. Therefore, NesTools can serve as a new benchmark to evaluate the nested tool learning abilities of LLMs. We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.', 'score': 11, 'issue_id': 127, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '9823a7c3dbc76e44', 'data': {'categories': ['#dataset', '#training', '#rag', '#rl', '#optimization', '#benchmark', '#synthetic'], 'emoji': '🛠️', 'ru': {'title': 'NesTools: новый стандарт для оценки вложенного обучения инструментам в LLM', 'desc': 'Статья представляет новый датасет NesTools для оценки способностей больших языковых моделей (LLM) к обучению вложенному использованию инструментов. NesTools включает метод автоматической генерации данных для создания крупномасштабных вложенных вызовов инструментов с различными структурами вложенности. Датасет прошел ручную проверку и доработку, что обеспечивает его высокое качество и соответствие реальным сценариям. Эксперименты на 22 LLM с использованием NesTools показали, что современные модели все еще испытывают трудности со сложными задачами вложенного обучения инструментам.'}, 'en': {'title': "NesTools: Elevating LLMs' Nested Tool Learning Evaluation", 'desc': "The paper introduces NesTools, a new benchmark designed to evaluate the nested tool learning capabilities of large language models (LLMs). NesTools uses an innovative automatic data generation method to create complex nested tool call scenarios, which are then manually reviewed to ensure high quality and real-world relevance. The study highlights that current LLMs struggle with these complex tasks, as demonstrated through extensive experiments on 22 different models. This work aims to fill the gap in existing research by providing a comprehensive dataset for assessing LLMs' ability to handle nested tool learning."}, 'zh': {'title': 'NesTools：提升嵌套工具学习的全新基准', 'desc': '这篇论文介绍了一种名为NesTools的新方法，用于评估大型语言模型（LLMs）的嵌套工具学习能力。NesTools通过自动生成大量不同嵌套结构的工具调用数据，填补了现有基准中缺乏相关数据实例的空白。经过人工审核和改进，数据集质量高且与现实场景紧密结合。实验表明，当前的LLMs在处理复杂的嵌套工具学习任务时仍然存在困难。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10934', 'title': 'Agent-as-a-Judge: Evaluate Agents with Agents', 'url': 'https://huggingface.co/papers/2410.10934', 'abstract': 'Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes -- ignoring the step-by-step nature of agentic systems, or require excessive manual labour. To address this, we introduce the Agent-as-a-Judge framework, wherein agentic systems are used to evaluate agentic systems. This is an organic extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving process. We apply the Agent-as-a-Judge to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we present DevAI, a new benchmark of 55 realistic automated AI development tasks. It includes rich manual annotations, like a total of 365 hierarchical user requirements. We benchmark three of the popular agentic systems using Agent-as-a-Judge and find it dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that Agent-as-a-Judge marks a concrete step forward for modern agentic systems -- by providing rich and reliable reward signals necessary for dynamic and scalable self-improvement.', 'score': 10, 'issue_id': 127, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'a25e89cb7be0bed4', 'data': {'categories': ['#interpretability', '#plp', '#agents', '#benchmark', '#optimization', '#alignment', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Агент судит агента: новый подход к оценке ИИ-систем', 'desc': 'Статья представляет новый подход к оценке агентных систем искусственного интеллекта - Agent-as-a-Judge. Этот метод использует агентные системы для оценки других агентных систем, что позволяет получать промежуточную обратную связь на протяжении всего процесса решения задачи. Авторы применяют Agent-as-a-Judge к задаче генерации кода и представляют новый бенчмарк DevAI, содержащий 55 реалистичных задач по автоматизированной разработке ИИ. Результаты показывают, что Agent-as-a-Judge значительно превосходит LLM-as-a-Judge и сопоставим с оценкой человека.'}, 'en': {'title': '"Agent-as-a-Judge: Revolutionizing Evaluation for Agentic Systems"', 'desc': 'The paper introduces the Agent-as-a-Judge framework, which uses agentic systems to evaluate other agentic systems, providing intermediate feedback throughout the task-solving process. This approach is applied to code generation and is tested using a new benchmark called DevAI, which includes 55 realistic AI development tasks with detailed annotations. The framework is shown to outperform existing evaluation methods like LLM-as-a-Judge and matches human evaluation reliability. Overall, Agent-as-a-Judge offers a significant advancement in providing dynamic and scalable feedback for agentic systems.'}, 'zh': {'title': '代理即评判者：现代代理系统评估的新方向', 'desc': '传统的评估方法对代理系统来说不够完善，因为它们要么只关注最终结果而忽略了代理系统的逐步特性，要么需要大量的人工劳动。为了解决这个问题，我们提出了“代理即评判者”框架，利用代理系统来评估代理系统。这种方法是“LLM即评判者”框架的自然扩展，加入了代理特性，能够在整个任务解决过程中提供中间反馈。我们应用“代理即评判者”于代码生成任务，并引入了新的基准DevAI，证明其在评估中表现优于传统方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11419', 'title': 'GS^3: Efficient Relighting with Triple Gaussian Splatting', 'url': 'https://huggingface.co/papers/2410.11419', 'abstract': 'We present a spatial and angular Gaussian based representation and a triple splatting process, for real-time, high-quality novel lighting-and-view synthesis from multi-view point-lit input images. To describe complex appearance, we employ a Lambertian plus a mixture of angular Gaussians as an effective reflectance function for each spatial Gaussian. To generate self-shadow, we splat all spatial Gaussians towards the light source to obtain shadow values, which are further refined by a small multi-layer perceptron. To compensate for other effects like global illumination, another network is trained to compute and add a per-spatial-Gaussian RGB tuple. The effectiveness of our representation is demonstrated on 30 samples with a wide variation in geometry (from solid to fluffy) and appearance (from translucent to anisotropic), as well as using different forms of input data, including rendered images of synthetic/reconstructed objects, photographs captured with a handheld camera and a flash, or from a professional lightstage. We achieve a training time of 40-70 minutes and a rendering speed of 90 fps on a single commodity GPU. Our results compare favorably with state-of-the-art techniques in terms of quality/performance. Our code and data are publicly available at https://GSrelight.github.io/.', 'score': 10, 'issue_id': 121, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '8aa8e4a555a54086', 'data': {'categories': ['#dataset', '#cv', '#training', '#open_source', '#architecture', '#synthetic', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Реалистичный рендеринг с динамическим освещением на основе гауссианов', 'desc': 'Статья представляет новый метод синтеза изображений с изменением освещения и ракурса в реальном времени. Авторы используют представление на основе пространственных и угловых гауссианов, а также процесс тройного сплаттинга. Для описания сложных визуальных эффектов применяется функция отражения, состоящая из ламбертовской составляющей и смеси угловых гауссианов. Метод демонстрирует высокое качество результатов на разнообразных объектах при быстром обучении и рендеринге.'}, 'en': {'title': '"Illuminate and Transform: Real-Time View Synthesis with Gaussian Magic"', 'desc': 'This paper introduces a novel method for creating high-quality images with new lighting and viewpoints using a spatial and angular Gaussian representation combined with a triple splatting process. The approach models complex appearances by using a Lambertian and angular Gaussian mixture reflectance function for each spatial Gaussian. To handle self-shadowing, spatial Gaussians are projected towards the light source, with shadow values refined by a small neural network, while another network adjusts for global illumination effects. The method is tested on diverse samples and achieves fast training and rendering times, outperforming existing techniques in quality and performance.'}, 'zh': {'title': '实时高质量光照合成的新方法', 'desc': '这篇论文介绍了一种基于空间和角度高斯的表示方法，以及三重喷溅过程，用于从多视点光照输入图像中实时生成高质量的新光照和视图合成。为了描述复杂的外观，作者使用了朗伯反射加上角度高斯混合作为每个空间高斯的有效反射函数。为了生成自阴影，所有空间高斯被喷溅到光源方向以获得阴影值，并通过一个小型多层感知器进一步优化。为了补偿全局光照等其他效果，另一个网络被训练来计算并添加每个空间高斯的RGB值。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09754', 'title': 'SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning', 'url': 'https://huggingface.co/papers/2410.09754', 'abstract': "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments.", 'score': 7, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '04b27b8dbf829651', 'data': {'categories': ['#cv', '#training', '#rl', '#optimization', '#games', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'SimBa: Простота и масштабируемость для глубокого обучения с подкреплением', 'desc': 'Статья представляет архитектуру SimBa для масштабирования параметров в глубоком обучении с подкреплением (RL). SimBa включает нормализацию наблюдений, резидуальный блок прямой связи и нормализацию слоев для внедрения смещения к простоте. Эксперименты показывают, что SimBa улучшает эффективность выборки для различных алгоритмов RL, включая off-policy, on-policy и unsupervised методы. Интеграция SimBa в алгоритм SAC позволяет достичь или превзойти современные методы RL на различных средах с высокой вычислительной эффективностью.'}, 'en': {'title': '"SimBa: Scaling Simplicity in Deep Reinforcement Learning"', 'desc': 'The paper introduces SimBa, a new architecture for deep reinforcement learning (RL) that scales up network parameters while maintaining simplicity to avoid overfitting. SimBa includes an observation normalization layer, a residual feedforward block, and a layer normalization to enhance model performance. By using SimBa, various deep RL algorithms show improved sample efficiency and computational effectiveness. The architecture proves its versatility by matching or surpassing state-of-the-art methods across different environments and tasks.'}, 'zh': {'title': 'SimBa：通过简单性偏差提升深度强化学习', 'desc': '近年来，计算机视觉和自然语言处理领域通过增加网络参数数量取得了显著进展，尽管传统理论认为更大的网络容易过拟合。这些大型网络通过引入简单性偏差的组件来避免过拟合，指导模型朝向简单且可推广的解决方案。在深度强化学习中，网络设计和扩展的研究较少。SimBa架构通过注入简单性偏差来扩展深度强化学习中的参数，提高了多种深度强化学习算法的样本效率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08001', 'title': 'Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2410.08001', 'abstract': 'The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for a wide range of applications. Inspired by these observations, we introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language-action (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing a specialist policy with merely 20M trainable parameters. It maintains strong performance with 5% of demonstration data only, and enables a 3.8 times higher control frequency in real-world deployment. Code would be made publicly available. Our project page is hosted at: https://opendrivelab.com/RoboDual/', 'score': 4, 'issue_id': 122, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'f9a57a673dfbc6f6', 'data': {'categories': ['#reasoning', '#training', '#agi', '#inference', '#games', '#open_source', '#diffusion', '#small_models', '#architecture', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'RoboDual: Объединение сильных сторон обобщенной и специализированной политик для универсальных роботов', 'desc': 'RoboDual - это синергетическая двойная система, объединяющая преимущества обобщенной и специализированной политик для роботов. Обобщенная политика на основе vision-language-action обеспечивает высокоуровневое понимание задач, а специализированная политика на основе диффузионного трансформера осуществляет точные многошаговые действия. RoboDual демонстрирует значительное улучшение производительности по сравнению с OpenVLA как в реальных, так и в симулированных средах. Система сохраняет высокую эффективность даже при использовании всего 5% демонстрационных данных и обеспечивает более высокую частоту управления при развертывании в реальном мире.'}, 'en': {'title': 'RoboDual: Merging Generalist Flexibility with Specialist Precision', 'desc': 'The paper introduces RoboDual, a dual-system approach that combines the strengths of both generalist and specialist policies for robotic systems. The generalist policy uses a vision-language-action model to provide high-level task understanding, while the specialist policy, based on a diffusion transformer, focuses on efficient multi-step action rollouts. RoboDual significantly improves performance in real-world settings and specific benchmarks by using a specialist policy with a small number of trainable parameters. This system achieves high efficiency and adaptability, even with limited demonstration data, and enhances control frequency in practical applications.'}, 'zh': {'title': 'RoboDual：通用与专业策略的完美结合', 'desc': 'RoboDual 是一个结合通用策略和专业策略的双系统，旨在提高机器人在多变环境中的适应性。通用策略利用大规模跨体数据，提供高层次的任务理解，而专业策略则专注于特定任务的高效执行。RoboDual 通过引入基于扩散变压器的专业策略，在真实环境中实现了显著的性能提升。该系统在仅使用少量示范数据的情况下，仍能保持高效的控制频率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.06593', 'title': 'Towards Natural Image Matting in the Wild via Real-Scenario Prior', 'url': 'https://huggingface.co/papers/2410.06593', 'abstract': 'Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. Specifically, the construction of our COCO-Matting includes accessory fusion and mask-to-matte, which selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 38,251 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. We open-source our code, models, and dataset at https://github.com/XiaRho/SEMat.', 'score': 2, 'issue_id': 126, 'pub_date': '2024-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '656804d454a782ca', 'data': {'categories': ['#dataset', '#cv', '#training', '#graphs', '#open_source', '#architecture', '#synthetic'], 'emoji': '✂️', 'ru': {'title': 'Революция в интерактивном маттинге: от синтетики к реальности', 'desc': 'В статье представлен новый датасет COCO-Matting для задачи интерактивного маттинга изображений, основанный на реальных сложных сценах из COCO. Авторы предлагают архитектуру SEMat, которая улучшает существующие методы на основе SAM, используя feature-aligned transformer и matte-aligned decoder. Введены новые функции потерь для обучения модели, включая регуляризацию и trimap loss. Эксперименты на семи различных датасетах показывают превосходство предложенного метода в задаче интерактивного маттинга естественных изображений.'}, 'en': {'title': 'Revolutionizing Image Matting with Real-World Data and Advanced Architectures', 'desc': 'The paper introduces a new dataset called COCO-Matting, which enhances the generalization of matting models to complex real-world scenes by using real images from the COCO dataset. It addresses the limitations of models trained on synthetic data by converting semantic segmentation masks into matting labels. The authors propose a novel model, SEMat, which includes a feature-aligned transformer and a matte-aligned decoder to improve edge and transparency feature extraction. Their approach, validated through extensive experiments, shows superior performance in interactive image matting, leveraging both new training objectives and architecture improvements.'}, 'zh': {'title': 'SEMat：提升交互式抠图的精度与泛化能力', 'desc': '这篇论文提出了一种新的抠图数据集COCO-Matting，基于COCO数据集构建，旨在解决合成数据训练模型在复杂场景中泛化能力不足的问题。COCO-Matting通过从COCO中选择真实世界的复杂图像，并将语义分割掩码转换为抠图标签，包含了38,251个人物实例级别的alpha mattes。为了充分利用预训练的SAM模型，作者提出了SEMat，通过特征对齐的transformer和抠图对齐的解码器来提取细粒度的边缘和透明度特征，并将粗略的掩码转换为高精度的抠图。实验结果表明，SEMat在七个不同的数据集上表现优异，证明了其在交互式自然图像抠图中的有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09745', 'title': 'Empirical Study of Mutual Reinforcement Effect and Application in Few-shot Text Classification Tasks via Prompt', 'url': 'https://huggingface.co/papers/2410.09745', 'abstract': "The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ empirical experiment to observe and substantiate the MRE theory. Our experiments on 21 MRE mix datasets revealed the presence of MRE in the model and its impact. Specifically, we conducted compare experiments use fine-tune. The results of findings from comparison experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our final experiment, the F1-score significantly surpassed the baseline in 18 out of 21 MRE Mix datasets, further validating the notion that word-level information enhances the language model's comprehension of the text as a whole.", 'score': 2, 'issue_id': 125, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '9e1731e2bbd0b081', 'data': {'categories': ['#reasoning', '#dataset', '#training', '#optimization', '#interpretability'], 'emoji': '🔄', 'ru': {'title': 'Взаимное усиление классификации слов и текста улучшает понимание языка', 'desc': 'Статья исследует взаимоусиливающий эффект (MRE) между классификацией на уровне слов и текста в задачах классификации текста. Авторы проводят эмпирические эксперименты на 21 наборе данных MRE Mix для подтверждения теории MRE. Результаты сравнительных экспериментов с использованием тонкой настройки подтверждают существование MRE. Применение MRE к обучению с подсказками, используя информацию на уровне слов в качестве вербализатора, значительно улучшило F1-меру на 18 из 21 набора данных.'}, 'en': {'title': 'Boosting Text Understanding: The Power of Word-Level Insights', 'desc': "The paper explores the Mutual Reinforcement Effect (MRE), which suggests that word-level and text-level classifications can improve each other's performance in text classification tasks. Through empirical experiments on 21 datasets, the study demonstrates the presence and impact of MRE, showing that word-level information can enhance text-level predictions. The researchers also apply MRE to prompt learning, using word-level data to improve the model's text-level classification accuracy. The experiments show a significant improvement in F1-scores, confirming that word-level insights can enhance overall text comprehension by the model."}, 'zh': {'title': '词级与文本级分类的相互增强效应', 'desc': '这篇论文研究了词级和文本级分类在文本分类任务中的相互增强效应。通过实验证明了这种效应的存在，并展示了其对模型性能的影响。研究还将这种效应应用于提示学习，利用词级信息来增强文本级分类标签的预测。最终实验结果显示，在21个数据集中的18个，F1分数显著超过基线，验证了词级信息对语言模型理解文本整体的提升作用。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11619', 'title': 'MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval', 'url': 'https://huggingface.co/papers/2410.11619', 'abstract': 'Efficiently retrieving and synthesizing information from large-scale multimodal collections has become a critical challenge. However, existing video retrieval datasets suffer from scope limitations, primarily focusing on matching descriptive but vague queries with small collections of professionally edited, English-centric videos. To address this gap, we introduce MultiVENT 2.0, a large-scale, multilingual event-centric video retrieval benchmark featuring a collection of more than 218,000 news videos and 3,906 queries targeting specific world events. These queries specifically target information found in the visual content, audio, embedded text, and text metadata of the videos, requiring systems leverage all these sources to succeed at the task. Preliminary results show that state-of-the-art vision-language models struggle significantly with this task, and while alternative approaches show promise, they are still insufficient to adequately address this problem. These findings underscore the need for more robust multimodal retrieval systems, as effective video retrieval is a crucial step towards multimodal content understanding and generation tasks.', 'score': 0, 'issue_id': 130, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': 'dd603cadf5349b0c', 'data': {'categories': ['#science', '#video', '#survey', '#dataset', '#multilingual', '#graphs', '#benchmark', '#games', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'MultiVENT 2.0: Новый вызов в мире мультимодального поиска видео', 'desc': 'MultiVENT 2.0 - это новый крупномасштабный многоязычный бенчмарк для поиска видео, ориентированный на события. Он включает более 218 000 новостных видео и 3 906 запросов, нацеленных на конкретные мировые события. Бенчмарк требует от систем использования визуального контента, аудио, встроенного текста и текстовых метаданных видео. Предварительные результаты показывают, что современные модели компьютерного зрения и обработки естественного языка значительно уступают в этой задаче, подчеркивая необходимость более надежных мультимодальных систем поиска.'}, 'en': {'title': 'Unlocking the Power of Multimodal Video Retrieval', 'desc': 'The paper introduces MultiVENT 2.0, a new benchmark for video retrieval that focuses on multilingual and event-centric content, addressing the limitations of existing datasets. It includes over 218,000 news videos and 3,906 queries that require systems to utilize visual, audio, and text data effectively. Initial tests reveal that current vision-language models struggle with this complex task, highlighting the need for more advanced multimodal retrieval systems. This research emphasizes the importance of improving video retrieval to enhance multimodal content understanding and generation.'}, 'zh': {'title': '突破多模态视频检索的瓶颈', 'desc': '这篇论文介绍了一个名为MultiVENT 2.0的大规模多语言事件中心视频检索基准，旨在解决现有视频检索数据集范围有限的问题。该基准包含超过218,000个新闻视频和3,906个针对特定世界事件的查询，要求系统利用视频的视觉内容、音频、嵌入文本和文本元数据来完成任务。初步结果显示，最先进的视觉语言模型在这个任务上表现不佳，而其他方法虽然有潜力，但仍不足以解决问题。这表明需要更强大的多模态检索系统，以实现对多模态内容的理解和生成。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11623', 'title': 'VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI', 'url': 'https://huggingface.co/papers/2410.11623', 'abstract': 'Recent advancements in Multi-modal Large Language Models (MLLMs) have opened new avenues for applications in Embodied AI. Building on previous work, EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating egocentric video understanding capabilities. To bridge the gap between MLLMs and low-level control in Embodied AI, we design four key interrelated tasks: video question-answering, hierarchy planning, visual grounding and reward modeling. To minimize manual annotation costs, we develop an automatic data generation pipeline based on the Ego4D dataset, leveraging the prior knowledge and multimodal capabilities of GPT-4o. Three human annotators then filter the generated data to ensure diversity and quality, resulting in the VidEgoThink benchmark. We conduct extensive experiments with three types of models: API-based MLLMs, open-source image-based MLLMs, and open-source video-based MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform poorly across all tasks related to egocentric video understanding. These findings suggest that foundation models still require significant advancements to be effectively applied to first-person scenarios in Embodied AI. In conclusion, VidEgoThink reflects a research trend towards employing MLLMs for egocentric vision, akin to human capabilities, enabling active observation and interaction in the complex real-world environments.', 'score': 46, 'issue_id': 137, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '67310ef96a11f09c', 'data': {'categories': ['#science', '#synthetic', '#benchmark', '#graphs', '#video', '#multimodal', '#data', '#robotics', '#open_source', '#games'], 'emoji': '👁️', 'ru': {'title': 'VidEgoThink: новый рубеж в понимании эгоцентрического видео для воплощенного ИИ', 'desc': 'Статья представляет VidEgoThink - комплексный бенчмарк для оценки возможностей понимания эгоцентрического видео мультимодальными большими языковыми моделями (MLLM). Авторы разработали четыре ключевые взаимосвязанные задачи: ответы на вопросы по видео, иерархическое планирование, визуальная локализация и моделирование вознаграждений. Данные для бенчмарка были сгенерированы автоматически на основе датасета Ego4D с использованием GPT-4o и отфильтрованы аннотаторами. Эксперименты показали, что современные MLLM, включая GPT-4o, плохо справляются с задачами понимания эгоцентрического видео, что указывает на необходимость дальнейших исследований в этой области.'}, 'en': {'title': 'Pioneering Egocentric Video Understanding with VidEgoThink', 'desc': 'The paper introduces VidEgoThink, a benchmark designed to evaluate how well multi-modal large language models (MLLMs) understand egocentric videos, which are videos captured from a first-person perspective. It focuses on four tasks: video question-answering, hierarchy planning, visual grounding, and reward modeling, to connect MLLMs with low-level control in Embodied AI. An automatic data generation pipeline is used to create diverse and high-quality data, which is then filtered by human annotators. The study finds that current MLLMs, including GPT-4o, struggle with these tasks, indicating that more advancements are needed for these models to effectively handle first-person video understanding in real-world scenarios.'}, 'zh': {'title': 'VidEgoThink：迈向具身AI的多模态大语言模型', 'desc': '这篇论文介绍了VidEgoThink，这是一个用于评估自我中心视频理解能力的基准。研究设计了四个关键任务：视频问答、层次规划、视觉定位和奖励建模，以连接多模态大语言模型与具身人工智能的低级控制。通过自动数据生成管道和人工筛选，确保数据的多样性和质量。实验结果表明，现有的多模态大语言模型在自我中心视频理解任务上表现不佳，表明基础模型在具身人工智能中的应用仍需重大进展。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12381', 'title': 'HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks', 'url': 'https://huggingface.co/papers/2410.12381', 'abstract': "Coding tasks have been valuable for evaluating Large Language Models (LLMs), as they demand the comprehension of high-level instructions, complex reasoning, and the implementation of functional programs -- core capabilities for advancing Artificial General Intelligence. Despite the progress in Large Multimodal Models (LMMs), which extend LLMs with visual perception and understanding capabilities, there remains a notable lack of coding benchmarks that rigorously assess these models, particularly in tasks that emphasize visual reasoning. To address this gap, we introduce HumanEval-V, a novel and lightweight benchmark specifically designed to evaluate LMMs' visual understanding and reasoning capabilities through code generation. HumanEval-V includes 108 carefully crafted, entry-level Python coding tasks derived from platforms like CodeForces and Stack Overflow. Each task is adapted by modifying the context and algorithmic patterns of the original problems, with visual elements redrawn to ensure distinction from the source, preventing potential data leakage. LMMs are required to complete the code solution based on the provided visual context and a predefined Python function signature outlining the task requirements. Every task is equipped with meticulously handcrafted test cases to ensure a thorough and reliable evaluation of model-generated solutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering significant challenges. Proprietary models like GPT-4o achieve only 13% pass@1 and 36.4% pass@10, while open-weight models with 70B parameters score below 4% pass@1. Ablation studies further reveal the limitations of current LMMs in vision reasoning and coding capabilities. These results underscore key areas for future research to enhance LMMs' capabilities. We have open-sourced our code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.", 'score': 41, 'issue_id': 135, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '7693dd92c7f5b4e2', 'data': {'categories': ['#reasoning', '#leakage', '#benchmark', '#cv', '#graphs', '#agi', '#plp', '#multimodal', '#open_source', '#games'], 'emoji': '👁️\u200d🗨️', 'ru': {'title': 'HumanEval-V: Новый рубеж в оценке визуально-кодовых способностей AI', 'desc': 'Статья представляет новый бенчмарк HumanEval-V для оценки способностей больших мультимодальных моделей (LMM) к визуальному пониманию и рассуждению через генерацию кода. Бенчмарк содержит 108 задач по программированию на Python, адаптированных с визуальным контекстом. Авторы провели оценку 19 современных LMM, выявив значительные трудности даже для лучших моделей. Результаты показывают ограничения текущих LMM в области визуального рассуждения и кодирования, обозначая направления для будущих исследований.'}, 'en': {'title': 'Enhancing LMMs: Bridging the Visual Reasoning Gap in Code Generation', 'desc': "The paper introduces HumanEval-V, a new benchmark designed to evaluate Large Multimodal Models (LMMs) on their ability to understand and reason with visual information through coding tasks. It highlights the gap in existing benchmarks that fail to rigorously test LMMs' visual reasoning capabilities, especially in the context of code generation. The benchmark consists of 108 Python coding tasks, each adapted with unique visual elements to prevent data leakage, and includes handcrafted test cases for thorough evaluation. Results from testing 19 state-of-the-art LMMs reveal significant challenges in their visual reasoning and coding abilities, pointing to areas for future research."}, 'zh': {'title': '提升LMMs的视觉推理与编程能力', 'desc': '这篇论文介绍了一种新的基准测试工具HumanEval-V，用于评估大型多模态模型（LMMs）的视觉理解和推理能力。HumanEval-V包含108个经过精心设计的Python编程任务，这些任务结合了视觉元素，以测试模型在视觉推理和代码生成方面的能力。研究发现，当前的LMMs在这些任务中表现不佳，揭示了它们在视觉推理和编程能力上的局限性。论文强调了未来研究的关键领域，以提高LMMs的能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12787', 'title': 'The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio', 'url': 'https://huggingface.co/papers/2410.12787', 'abstract': 'Recent advancements in large multimodal models (LMMs) have significantly enhanced performance across diverse tasks, with ongoing efforts to further integrate additional modalities such as video and audio. However, most existing LMMs remain vulnerable to hallucinations, the discrepancy between the factual multimodal input and the generated textual output, which has limited their applicability in various real-world scenarios. This paper presents the first systematic investigation of hallucinations in LMMs involving the three most common modalities: language, visual, and audio. Our study reveals two key contributors to hallucinations: overreliance on unimodal priors and spurious inter-modality correlations. To address these challenges, we introduce the benchmark The Curse of Multi-Modalities (CMM), which comprehensively evaluates hallucinations in LMMs, providing a detailed analysis of their underlying issues. Our findings highlight key vulnerabilities, including imbalances in modality integration and biases from training data, underscoring the need for balanced cross-modal learning and enhanced hallucination mitigation strategies. Based on our observations and findings, we suggest potential research directions that could enhance the reliability of LMMs.', 'score': 30, 'issue_id': 135, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'e40b5fd56a795812', 'data': {'categories': ['#hallucinations', '#benchmark', '#multimodal', '#interpretability', '#ethics', '#training'], 'emoji': '🌈', 'ru': {'title': 'Борьба с галлюцинациями в мультимодальных моделях', 'desc': 'Статья представляет первое систематическое исследование галлюцинаций в крупных мультимодальных моделях (LMM), охватывающее три основные модальности: язык, визуальную и аудио. Авторы выявили две ключевые причины галлюцинаций: чрезмерная опора на одномодальные приоры и ложные межмодальные корреляции. Для оценки галлюцинаций в LMM был создан бенчмарк The Curse of Multi-Modalities (CMM). Исследование подчеркивает необходимость сбалансированного кросс-модального обучения и улучшенных стратегий по снижению галлюцинаций.'}, 'en': {'title': 'Taming Hallucinations in Multimodal Models', 'desc': "This paper explores the issue of hallucinations in large multimodal models (LMMs), which occur when the model's output doesn't match the input from different modalities like language, visuals, and audio. The study identifies two main causes of these hallucinations: relying too much on single-modality information and incorrect connections between different modalities. To tackle these problems, the authors introduce a new benchmark called The Curse of Multi-Modalities (CMM) to evaluate and analyze these hallucinations. The research suggests that improving the balance in how different modalities are integrated and addressing biases in training data can help reduce hallucinations in LMMs."}, 'zh': {'title': '破解多模态模型的幻觉挑战', 'desc': '这篇论文研究了大型多模态模型（LMMs）在处理语言、视觉和音频三种常见模态时出现的幻觉问题。研究发现，幻觉主要由对单一模态的过度依赖和模态间的虚假相关性引起。为了解决这些问题，作者提出了一个名为“多模态诅咒”的基准，用于全面评估LMMs中的幻觉现象。研究结果强调了模态整合的不平衡和训练数据的偏差，建议未来的研究应关注跨模态学习的平衡和幻觉的减轻策略。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12628', 'title': 'DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception', 'url': 'https://huggingface.co/papers/2410.12628', 'abstract': 'Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods relying solely on visual features offer faster processing speeds at the expense of accuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel approach that enhances accuracy while maintaining speed advantages through document-specific optimizations in both pre-training and model design. For robust document pre-training, we introduce the Mesh-candidate BestFit algorithm, which frames document synthesis as a two-dimensional bin packing problem, generating the large-scale, diverse DocSynth-300K dataset. Pre-training on the resulting DocSynth-300K dataset significantly improves fine-tuning performance across various document types. In terms of model optimization, we propose a Global-to-Local Controllable Receptive Module that is capable of better handling multi-scale variations of document elements. Furthermore, to validate performance across different document types, we introduce a complex and challenging benchmark named DocStructBench. Extensive experiments on downstream datasets demonstrate that DocLayout-YOLO excels in both speed and accuracy. Code, data, and models are available at https://github.com/opendatalab/DocLayout-YOLO.', 'score': 25, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '0c81afcfe57d6c2f', 'data': {'categories': ['#synthetic', '#benchmark', '#cv', '#optimization', '#data', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '📄', 'ru': {'title': 'DocLayout-YOLO: Быстрый и точный анализ макета документов', 'desc': 'DocLayout-YOLO - это новый подход к анализу макета документов, оптимизирующий соотношение скорости и точности. Метод использует алгоритм Mesh-candidate BestFit для создания синтетического набора данных DocSynth-300K, что улучшает предобучение модели. В архитектуру внедрен модуль Global-to-Local Controllable Receptive для лучшей обработки элементов документа разного масштаба. Авторы также представляют новый бенчмарк DocStructBench для оценки производительности на различных типах документов.'}, 'en': {'title': 'DocLayout-YOLO: Fast and Accurate Document Layout Analysis', 'desc': 'The paper introduces DocLayout-YOLO, a new method for document layout analysis that balances speed and accuracy by using document-specific optimizations. It employs a unique pre-training strategy with the Mesh-candidate BestFit algorithm to create a diverse dataset called DocSynth-300K, enhancing model performance. The model also features a Global-to-Local Controllable Receptive Module to handle different document element scales effectively. Extensive testing shows that DocLayout-YOLO performs well in both speed and accuracy across various document types.'}, 'zh': {'title': 'DocLayout-YOLO：速度与准确性的完美结合', 'desc': '这篇论文介绍了一种新的文档布局分析方法，称为DocLayout-YOLO，它在提高准确性的同时保持了速度优势。通过引入Mesh-candidate BestFit算法，作者创建了一个名为DocSynth-300K的大规模数据集，用于增强文档的预训练效果。为了优化模型，论文提出了一种全局到局部的可控感受模块，以更好地处理文档元素的多尺度变化。实验结果表明，DocLayout-YOLO在速度和准确性方面都表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12409', 'title': 'Revealing the Barriers of Language Agents in Planning', 'url': 'https://huggingface.co/papers/2410.12409', 'abstract': 'Autonomous planning has been an ongoing pursuit since the inception of artificial intelligence. Based on curated problem solvers, early planning agents could deliver precise solutions for specific tasks but lacked generalization. The emergence of large language models (LLMs) and their powerful reasoning capabilities has reignited interest in autonomous planning by automatically generating reasonable solutions for given tasks. However, prior research and our experiments show that current language agents still lack human-level planning abilities. Even the state-of-the-art reasoning model, OpenAI o1, achieves only 15.6% on one of the complex real-world planning benchmarks. This highlights a critical question: What hinders language agents from achieving human-level planning? Although existing studies have highlighted weak performance in agent planning, the deeper underlying issues and the mechanisms and limitations of the strategies proposed to address them remain insufficiently understood. In this work, we apply the feature attribution study and identify two key factors that hinder agent planning: the limited role of constraints and the diminishing influence of questions. We also find that although current strategies help mitigate these challenges, they do not fully resolve them, indicating that agents still have a long way to go before reaching human-level intelligence.', 'score': 23, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'faf0a6d62d983811', 'data': {'categories': ['#reasoning', '#rl', '#benchmark', '#agi', '#interpretability', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Преодолевая барьеры: путь к человеческому уровню планирования для ИИ', 'desc': 'Статья исследует проблемы автономного планирования в контексте языковых моделей. Авторы выявляют два ключевых фактора, препятствующих эффективному планированию агентов: ограниченную роль ограничений и уменьшающееся влияние вопросов. Исследование показывает, что даже современные модели, такие как OpenAI o1, достигают лишь 15.6% успеха на сложных задачах планирования реального мира. Несмотря на существующие стратегии улучшения, авторы заключают, что языковые агенты все еще далеки от человеческого уровня планирования.'}, 'en': {'title': 'Bridging the Gap: Enhancing Language Models for Human-Level Planning', 'desc': 'This paper explores the limitations of current language models in achieving human-level planning abilities. It identifies two main factors hindering progress: the limited role of constraints and the diminishing influence of questions. Despite existing strategies to address these issues, they are not fully effective, suggesting that language agents need further development. The study uses feature attribution to analyze these challenges and highlights the gap between current capabilities and human-level intelligence.'}, 'zh': {'title': '揭示语言模型规划能力的瓶颈', 'desc': '这篇论文探讨了自主规划在人工智能中的发展历程，指出当前语言模型在规划能力上仍然不及人类水平。研究发现，限制条件的有限作用和问题影响力的减弱是阻碍语言代理规划能力的两个关键因素。尽管现有策略在一定程度上缓解了这些问题，但并未彻底解决，表明代理距离达到人类智能还有很长的路要走。通过特征归因研究，作者揭示了这些深层次问题的机制和局限性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12613', 'title': 'Exploring Model Kinship for Merging Large Language Models', 'url': 'https://huggingface.co/papers/2410.12613', 'abstract': 'Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-k Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Code is available at https://github.com/zjunlp/ModelKinship.', 'score': 19, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '5154e5ff2c4f7709', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🧬', 'ru': {'title': 'Родство моделей: ключ к эффективному слиянию LLM', 'desc': "Статья посвящена новой стратегии объединения моделей для улучшения больших языковых моделей (LLM). Авторы вводят понятие 'родства моделей', аналогичное биологической эволюции, и исследуют его связь с производительностью после слияния. На основе этого предлагается стратегия 'Top-k Жадного Слияния с Учетом Родства Моделей', которая показывает лучшие результаты на контрольных наборах данных. Исследование демонстрирует, как учет родства моделей помогает избежать локальных оптимумов при эволюции моделей."}, 'en': {'title': '"Model Kinship: Guiding the Evolution of Language Models"', 'desc': "This paper explores the concept of model merging to improve the performance of Large Language Models (LLMs). It introduces 'model kinship,' a measure of similarity between models, which can predict the effectiveness of merging two models. The authors propose a new strategy called Top-k Greedy Merging with Model Kinship, which uses this similarity measure to enhance model performance on benchmark datasets. This approach helps avoid local optima, improving the overall efficiency and capability of LLMs."}, 'zh': {'title': '模型亲缘关系：提升大语言模型合并性能的新策略', 'desc': '这篇论文探讨了大语言模型（LLMs）合并的性能提升和原则。研究引入了模型亲缘关系的概念，类似于生物进化，来分析模型合并后的性能提升。通过实证分析，发现模型亲缘关系与合并后的性能提升有一定关系，这可以指导我们选择合适的候选模型。基于此，提出了一种新的模型合并策略：基于模型亲缘关系的Top-k贪心合并策略，能够在基准数据集上获得更好的性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10672', 'title': 'Large Language Model Evaluation via Matrix Nuclear-Norm', 'url': 'https://huggingface.co/papers/2410.10672', 'abstract': "As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \\( L_{1,2}-norm \\) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm.", 'score': 18, 'issue_id': 137, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'dbc484f05cdfefb9', 'data': {'categories': ['#small_models', '#benchmark', '#optimization', '#math', '#training', '#open_source'], 'emoji': '🧮', 'ru': {'title': 'Эффективная оценка LLM: быстрее, масштабируемее, точнее', 'desc': 'Статья представляет новую метрику оценки больших языковых моделей (LLM) - Matrix Nuclear-Norm. Эта метрика позволяет эффективно оценивать способность моделей сжимать информацию и уменьшать избыточность. По сравнению с традиционной метрикой Matrix Entropy, новый подход имеет меньшую вычислительную сложность O(n^2) вместо O(n^3). Тесты показали, что Matrix Nuclear-Norm работает в 8-24 раза быстрее для больших моделей, сохраняя при этом надежность оценки.'}, 'en': {'title': 'Speeding Up LLM Evaluation with Matrix Nuclear-Norm', 'desc': 'The paper introduces the Matrix Nuclear-Norm as a new metric for evaluating large language models (LLMs) in terms of their ability to compress information and reduce redundancy. This metric offers a more computationally efficient alternative to traditional methods like Matrix Entropy, reducing time complexity from O(n^3) to O(n^2) by eliminating the need for Singular Value Decomposition (SVD). By using the L_{1,2}-norm to approximate the nuclear norm, the approach maintains accuracy while significantly speeding up the evaluation process, especially for large models like CEREBRAS-GPT and Pythia. The Matrix Nuclear-Norm is validated as a reliable and scalable tool for assessing LLM performance, balancing accuracy with computational efficiency.'}, 'zh': {'title': '矩阵核范数：高效评估大型语言模型的新工具', 'desc': '随着大型语言模型的发展，评估其信息压缩能力和减少冗余的有效指标变得至关重要。传统的矩阵熵指标虽然有用，但由于奇异值分解的复杂性，对大规模模型的计算负担较重。我们提出了矩阵核范数作为新的评估指标，不仅能量化模型的数据压缩能力，还能通过凸近似矩阵秩来捕捉预测的可辨识性和多样性。通过使用 \\( L_{1,2}-norm \\) 进一步近似核范数，我们显著降低了计算复杂度，并在不需要奇异值分解的情况下提高了评估速度。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11081', 'title': 'Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models', 'url': 'https://huggingface.co/papers/2410.11081', 'abstract': 'Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%.', 'score': 16, 'issue_id': 138, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '0c5455c7c793e07e', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Стабильное обучение непрерывных моделей консистентности для быстрой генерации изображений', 'desc': 'Статья представляет новый подход к обучению непрерывных по времени моделей консистентности (CMs) для генеративных моделей диффузии. Авторы предлагают унифицированную теоретическую основу, объясняющую причины нестабильности обучения предыдущих подходов. Они вводят улучшения в параметризацию процесса диффузии, архитектуру нейронной сети и целевые функции обучения. Эти изменения позволили обучить модель с 1.5 миллиардами параметров на ImageNet 512x512, достигнув показателей FID, близких к лучшим существующим моделям диффузии.'}, 'en': {'title': 'Revolutionizing Consistency Models: Faster, Better, and Scalable', 'desc': 'The paper introduces a new approach to improve consistency models (CMs), which are a type of generative model used for creating data like images. Traditional CMs often face issues due to discretized timesteps, leading to errors and extra parameters. The authors propose a unified framework to address these issues by enhancing the diffusion process, network design, and training methods. Their improved model can handle large-scale data and achieves competitive performance with fewer sampling steps, as shown by their impressive FID scores on various datasets.'}, 'zh': {'title': '突破一致性模型的训练瓶颈', 'desc': '这篇论文介绍了一种新的连续时间一致性模型（CMs），用于生成模型的快速采样。传统的CMs使用离散时间步长，容易引入误差和不稳定性。作者提出了一种简化的理论框架，统一了扩散模型和CMs的参数化，解决了训练不稳定的问题。通过改进扩散过程的参数化、网络架构和训练目标，成功在大规模数据集上训练了CMs，并取得了优异的生成效果。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11817', 'title': 'Improving Long-Text Alignment for Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2410.11817', 'abstract': 'The rapid advancement of text-to-image (T2I) diffusion models has enabled them to generate unprecedented results from given texts. However, as text inputs become longer, existing encoding methods like CLIP face limitations, and aligning the generated images with long texts becomes challenging. To tackle these issues, we propose LongAlign, which includes a segment-level encoding method for processing long texts and a decomposed preference optimization method for effective alignment training. For segment-level encoding, long texts are divided into multiple segments and processed separately. This method overcomes the maximum input length limits of pretrained encoding models. For preference optimization, we provide decomposed CLIP-based preference models to fine-tune diffusion models. Specifically, to utilize CLIP-based preference models for T2I alignment, we delve into their scoring mechanisms and find that the preference scores can be decomposed into two components: a text-relevant part that measures T2I alignment and a text-irrelevant part that assesses other visual aspects of human preference. Additionally, we find that the text-irrelevant part contributes to a common overfitting problem during fine-tuning. To address this, we propose a reweighting strategy that assigns different weights to these two components, thereby reducing overfitting and enhancing alignment. After fine-tuning 512 times 512 Stable Diffusion (SD) v1.5 for about 20 hours using our method, the fine-tuned SD outperforms stronger foundation models in T2I alignment, such as PixArt-alpha and Kandinsky v2.2. The code is available at https://github.com/luping-liu/LongAlign.', 'score': 14, 'issue_id': 134, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': 'c9ba30d5d0f611d5', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#open_source', '#architecture', '#long_context'], 'emoji': '🖼️', 'ru': {'title': 'LongAlign: Превосходя ограничения в генерации изображений по длинным текстам', 'desc': 'Статья представляет метод LongAlign для улучшения генерации изображений по длинным текстовым описаниям. Авторы предлагают сегментированное кодирование длинных текстов и декомпозицию оптимизации предпочтений для более эффективного обучения моделей. Метод преодолевает ограничения существующих подходов, таких как CLIP, и позволяет лучше согласовывать генерируемые изображения с длинными текстами. После дообучения модель Stable Diffusion с помощью LongAlign превзошла более сильные базовые модели в задаче генерации изображений по тексту.'}, 'en': {'title': '"Long Texts, Perfect Images: The LongAlign Revolution"', 'desc': 'The paper introduces LongAlign, a novel approach to improve text-to-image (T2I) diffusion models when dealing with long text inputs. It addresses the limitations of existing encoding methods like CLIP by segmenting long texts and optimizing alignment through decomposed preference models. By dividing texts into segments, LongAlign overcomes input length restrictions, and its reweighting strategy reduces overfitting during fine-tuning. The method significantly enhances T2I alignment, outperforming other models like PixArt-alpha and Kandinsky v2.2.'}, 'zh': {'title': '长文本对齐新突破：LongAlign方法', 'desc': '这篇论文介绍了一种名为LongAlign的新方法，用于解决文本到图像生成模型在处理长文本时的对齐问题。LongAlign通过将长文本分段编码，克服了预训练编码模型的输入长度限制。为了优化对齐效果，论文提出了一种分解的偏好优化方法，利用CLIP模型的偏好评分机制。通过重新加权策略，LongAlign有效减少了过拟合问题，并在对齐性能上超越了其他基础模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12405', 'title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'url': 'https://huggingface.co/papers/2410.12405', 'abstract': 'Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations. To address these shortcomings, we introduce ProSA, a framework designed to evaluate and comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity metric, PromptSensiScore, and leverages decoding confidence to elucidate underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness. We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoning-oriented tasks. Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness. We believe this work will serve as a helpful tool in studying prompt sensitivity of LLMs. The project is released at: https://github.com/open-compass/ProSA .', 'score': 13, 'issue_id': 134, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'd2f8e1c0dcdf19fa', 'data': {'categories': ['#reasoning', '#benchmark', '#interpretability', '#training', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'ProSA: Новый подход к оценке чувствительности языковых моделей к промптам', 'desc': 'Авторы представляют ProSA - фреймворк для оценки и понимания чувствительности больших языковых моделей (LLM) к промптам. Они вводят новую метрику PromptSensiScore и используют уверенность декодирования для объяснения механизмов работы LLM. Исследование показывает, что чувствительность к промптам варьируется между датасетами и моделями, причем более крупные модели демонстрируют большую устойчивость. Авторы обнаружили, что примеры few-shot могут снизить проблему чувствительности, а субъективные оценки также подвержены влиянию промптов.'}, 'en': {'title': 'Mastering the Art of Prompting: Enhancing LLM Robustness', 'desc': 'Large language models (LLMs) can perform well on many tasks, but their success depends a lot on how they are prompted. This paper introduces ProSA, a framework to study how sensitive LLMs are to different prompts using a new metric called PromptSensiScore. The study shows that bigger models handle prompt changes better, and using few-shot examples can help reduce sensitivity. It also finds that when models are more confident, they are less affected by changes in prompts.'}, 'zh': {'title': '理解和提升大型语言模型的提示词鲁棒性', 'desc': '大型语言模型在不同任务中表现出色，但对提示词的敏感性很高，这影响了评估的准确性和用户满意度。为了解决这个问题，研究人员提出了ProSA框架，用于评估和理解提示词敏感性。研究发现，提示词敏感性在不同数据集和模型中波动，较大的模型表现出更强的鲁棒性。通过使用少量示例可以减轻这种敏感性问题，并且模型的高置信度与提示词的鲁棒性增加有关。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08968', 'title': 'Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements', 'url': 'https://huggingface.co/papers/2410.08968', 'abstract': 'The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addition, users may have diverse safety needs, making a model with static safety standards too restrictive to be useful, as well as too costly to be re-aligned.   We propose Controllable Safety Alignment (CoSA), a framework designed to adapt models to diverse safety requirements without re-training. Instead of aligning a fixed model, we align models to follow safety configs -- free-form natural language descriptions of the desired safety behaviors -- that are provided as part of the system prompt. To adjust model safety behavior, authorized users only need to modify such safety configs at inference time. To enable that, we propose CoSAlign, a data-centric method for aligning LLMs to easily adapt to diverse safety configs. Furthermore, we devise a novel controllability evaluation protocol that considers both helpfulness and configured safety, summarizing them into CoSA-Score, and construct CoSApien, a human-authored benchmark that consists of real-world LLM use cases with diverse safety requirements and corresponding evaluation prompts.   We show that CoSAlign leads to substantial gains of controllability over strong baselines including in-context alignment. Our framework encourages better representation and adaptation to pluralistic human values in LLMs, and thereby increasing their practicality.', 'score': 12, 'issue_id': 140, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': 'b81f79d5fa0e574a', 'data': {'categories': ['#benchmark', '#multilingual', '#inference', '#ethics', '#data', '#training', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Гибкая настройка безопасности языковых моделей без переобучения', 'desc': "Статья представляет новый подход к обеспечению безопасности больших языковых моделей (LLM) под названием Controllable Safety Alignment (CoSA). В отличие от традиционного подхода 'один размер для всех', CoSA позволяет адаптировать модели к различным требованиям безопасности без переобучения. Авторы предлагают метод CoSAlign для настройки LLM на следование конфигурациям безопасности, задаваемым в виде текстовых описаний. Также разработан новый протокол оценки контролируемости и создан бенчмарк CoSApien с реальными сценариями использования LLM."}, 'en': {'title': 'Flexible Safety for Diverse Needs: CoSA Framework for LLMs', 'desc': "The paper introduces Controllable Safety Alignment (CoSA), a framework that allows large language models (LLMs) to adapt to diverse safety requirements without needing to be retrained. Instead of using a fixed safety standard, CoSA uses safety configurations, which are natural language descriptions of desired safety behaviors, that can be modified by authorized users at inference time. The authors propose CoSAlign, a data-centric method that enhances the model's ability to adjust to these safety configurations, and introduce a new evaluation protocol called CoSA-Score to measure both helpfulness and safety. The framework aims to better represent and adapt to varying human values, making LLMs more practical and flexible in real-world applications."}, 'zh': {'title': '灵活适应多元安全需求的语言模型', 'desc': '目前的大型语言模型安全对齐方法通常采用一刀切的方式，拒绝与任何被认为不安全的内容互动。这种方法缺乏灵活性，难以适应不同文化和地区的社会规范。我们提出了一种名为可控安全对齐（CoSA）的框架，通过在系统提示中提供安全配置来调整模型的安全行为，而无需重新训练。CoSAlign方法使得模型能够轻松适应多样化的安全需求，提高了模型的实用性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.07722', 'title': 'DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities', 'url': 'https://huggingface.co/papers/2410.07722', 'abstract': "Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained transformers, which often split entities into nonsensical fragments. Splitting entities can reduce retrieval accuracy and limits the model's ability to incorporate up-to-date world knowledge not included in the training data. In this work, we enhance the LSR vocabulary with Wikipedia concepts and entities, enabling the model to resolve ambiguities more effectively and stay current with evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo) head, which leverages existing entity embeddings and an entity retrieval component that identifies entities relevant to a query or document. We use the DyVo head to generate entity weights, which are then merged with word piece weights to create joint representations for efficient indexing and retrieval using an inverted index. In experiments across three entity-rich document ranking datasets, the resulting DyVo model substantially outperforms state-of-the-art baselines.", 'score': 12, 'issue_id': 139, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'aff6e5f440cea35e', 'data': {'categories': ['#rag', '#reasoning', '#graphs', '#dataset', '#transfer_learning', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Динамический словарь для точного поиска информации', 'desc': 'Статья представляет новый подход к улучшению моделей разреженного поиска (LSR) путем обогащения их словаря концепциями и сущностями из Википедии. Авторы предлагают использовать динамическую головную часть модели (DyVo), которая объединяет существующие вложения сущностей с компонентом поиска сущностей. Это позволяет модели более эффективно разрешать неоднозначности и оставаться в курсе меняющихся знаний. Эксперименты на трех наборах данных по ранжированию документов, богатых сущностями, показывают значительное превосходство предложенной модели DyVo над современными базовыми моделями.'}, 'en': {'title': 'Boosting Retrieval with Dynamic Vocabulary', 'desc': 'The paper introduces a method to improve Learned Sparse Retrieval (LSR) models by enhancing their vocabulary with Wikipedia concepts and entities. This approach helps the model better understand and retrieve information by resolving ambiguities and incorporating current world knowledge. A key innovation is the Dynamic Vocabulary (DyVo) head, which combines entity embeddings with word piece weights for more accurate indexing and retrieval. Experiments show that this method significantly outperforms existing models in entity-rich document ranking tasks.'}, 'zh': {'title': '动态词汇头：提升实体检索的准确性', 'desc': '这篇论文介绍了一种改进的稀疏检索模型，称为动态词汇头（DyVo），它通过结合维基百科的概念和实体来增强模型的词汇表。传统的模型在处理实体时常常将其分割成无意义的片段，影响了检索的准确性。DyVo头利用现有的实体嵌入和实体检索组件来识别与查询或文档相关的实体，并生成实体权重。实验结果表明，DyVo模型在多个实体丰富的文档排名数据集上显著优于现有的最先进基线。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.08584', 'title': 'ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression', 'url': 'https://huggingface.co/papers/2410.08584', 'abstract': 'The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs that resolves both computation and memory bottlenecks through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform attention mechanism solely on those important tokens to accelerate the prefill phase. To mitigate the memory bottleneck in the decoding phase, we employ mixed-precision quantization to the KV cache, where high-bit quantization is used for caches of important tokens, while low-bit quantization is applied to those of less importance. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.6times and reduce GPU memory usage by 50.0%, with a minimal accuracy reduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively enhancing the generation efficiency of LVLMs.', 'score': 11, 'issue_id': 136, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': 'faf1b859b79c97a2', 'data': {'categories': ['#benchmark', '#cv', '#inference', '#video', '#optimization', '#graphs', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'ZipVL: Ускорение LVLM без потери качества', 'desc': 'ZipVL - это эффективный фреймворк для вывода крупных моделей зрения и языка (LVLM). Он решает проблемы вычислительной и памятной эффективности с помощью динамической стратегии распределения важных токенов. Фреймворк ускоряет фазу предзаполнения, выполняя механизм внимания только на важных токенах. Для уменьшения использования памяти при декодировании применяется смешанная квантизация кэша ключей и значений.'}, 'en': {'title': 'ZipVL: Streamlining Vision-Language Models for Speed and Efficiency', 'desc': 'The paper introduces ZipVL, a framework that enhances the efficiency of large vision-language models by addressing both computational and memory bottlenecks. It dynamically allocates important tokens based on layer-specific attention scores, optimizing the attention mechanism and reducing redundancy. By using mixed-precision quantization for the key-value cache, it balances memory usage and performance. Experiments show that ZipVL significantly speeds up processing and reduces memory requirements with minimal impact on accuracy.'}, 'zh': {'title': 'ZipVL：提升视觉语言模型效率的新策略', 'desc': '这篇论文介绍了一种名为ZipVL的高效推理框架，旨在解决大规模视觉语言模型（LVLMs）中的计算和内存瓶颈问题。ZipVL通过动态分配重要token的比例，基于层特定的注意力分数分布来提高效率。为了加速预填充阶段，ZipVL仅对重要token执行注意力机制，并在解码阶段对KV缓存进行混合精度量化。实验表明，ZipVL在加速预填充阶段和减少GPU内存使用方面表现出色，同时保持了高性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12490', 'title': 'Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective', 'url': 'https://huggingface.co/papers/2410.12490', 'abstract': 'Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at https://github.com/DAMO-NLP-SG/DiGIT.', 'score': 8, 'issue_id': 141, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '959f8e0892b466b9', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'DiGIT: Революция в генеративном моделировании изображений', 'desc': 'Статья представляет новый подход к генеративному моделированию изображений. Авторы предлагают дискретный токенизатор изображений для стабилизации латентного пространства. Эксперименты показывают, что авторегрессионная модель с этим токенизатором (DiGIT) превосходит модели латентной диффузии. Результаты демонстрируют потенциал оптимизированного латентного пространства и дискретной токенизации для улучшения генеративных моделей изображений.'}, 'en': {'title': 'Revolutionizing Image Generation: Bridging the Gap with Discrete Tokenization', 'desc': 'This paper explores the effectiveness of latent-based image generative models, particularly focusing on the performance gap between autoregressive models and other models like Latent Diffusion Models (LDMs) and Mask Image Models (MIMs). The authors propose a new discrete image tokenizer, DiGIT, to stabilize the latent space, which enhances the performance of autoregressive models in image generation. Their experiments demonstrate that, with this tokenizer, autoregressive models can outperform LDMs, showing significant improvements similar to those seen in NLP with GPT models. This work highlights the importance of optimizing latent space and using discrete tokenization to improve image generative modeling.'}, 'zh': {'title': '优化潜在空间，提升图像生成', 'desc': '这篇论文探讨了图像生成模型中潜在空间的优化问题，特别是自回归模型在图像生成中的表现。研究发现，尽管自回归模型在自然语言处理中表现优异，但在图像生成中却不如潜在扩散模型和掩码图像模型。为了解决这个问题，作者提出了一种新的离散图像标记器（DiGIT），以稳定图像生成的潜在空间。实验结果表明，这种方法不仅提升了图像理解和生成的效果，还使得自回归模型在图像生成中首次超越了潜在扩散模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09870', 'title': 'ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains', 'url': 'https://huggingface.co/papers/2410.09870', 'abstract': "Large language models (LLMs) have significantly impacted many aspects of our lives. However, assessing and ensuring their chronological knowledge remains challenging. Existing approaches fall short in addressing the accumulative nature of knowledge, often relying on a single time stamp. To overcome this, we introduce ChroKnowBench, a benchmark dataset designed to evaluate chronologically accumulated knowledge across three key aspects: multiple domains, time dependency, temporal state. Our benchmark distinguishes between knowledge that evolves (e.g., scientific discoveries, amended laws) and knowledge that remain constant (e.g., mathematical truths, commonsense facts). Building on this benchmark, we present ChroKnowledge (Chronological Categorization of Knowledge), a novel sampling-based framework for evaluating and updating LLMs' non-parametric chronological knowledge. Our evaluation shows: (1) The ability of eliciting temporal knowledge varies depending on the data format that model was trained on. (2) LLMs partially recall knowledge or show a cut-off at temporal boundaries rather than recalling all aspects of knowledge correctly. Thus, we apply our ChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by traversing step-by-step through the surrounding time spans. We observe that our framework successfully updates the overall knowledge across the entire timeline in both the biomedical domain (+11.9%) and the general domain (+2.8%), demonstrating its effectiveness in refining temporal knowledge. This non-parametric approach also enables knowledge updates not only in open-source models but also in proprietary LLMs, ensuring comprehensive applicability across model types. We perform a comprehensive analysis based on temporal characteristics of ChroKnowPrompt and validate the potential of various models to elicit intrinsic temporal knowledge through our method.", 'score': 7, 'issue_id': 140, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'c37b3e5475dd218a', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#agi', '#interpretability', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '⏳', 'ru': {'title': 'Хронологическая оценка знаний языковых моделей', 'desc': 'Статья представляет ChroKnowBench - новый набор данных для оценки хронологических знаний языковых моделей. Авторы предлагают framework ChroKnowledge для оценки и обновления непараметрических хронологических знаний LLM. Исследование показывает, что способность извлекать временные знания зависит от формата данных, на которых обучалась модель. Применение ChroKnowPrompt позволяет успешно обновлять знания моделей по всей временной шкале.'}, 'en': {'title': '"Mastering Time: Enhancing LLMs with Chronological Knowledge"', 'desc': "This paper introduces ChroKnowBench, a benchmark dataset designed to evaluate how well large language models (LLMs) understand and update chronological knowledge. It distinguishes between knowledge that changes over time, like scientific discoveries, and knowledge that remains constant, like mathematical truths. The authors also present ChroKnowledge, a framework that uses a novel prompting method to improve LLMs' ability to recall and update temporal knowledge. Their approach shows significant improvements in knowledge accuracy across different domains, demonstrating its effectiveness in refining the temporal understanding of LLMs."}, 'zh': {'title': '提升大语言模型的时间知识：ChroKnowBench的创新应用', 'desc': '这篇论文介绍了一种名为ChroKnowBench的基准数据集，用于评估大语言模型在时间上累积的知识。研究发现，现有方法在处理知识的时间依赖性方面存在不足，尤其是在知识演变和不变性之间的区分上。为了解决这个问题，作者提出了一种新的采样框架ChroKnowledge，通过逐步提示来更新和评估模型的时间知识。实验结果表明，这种方法在生物医学和一般领域中有效提高了模型的时间知识更新能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11878', 'title': 'Neural Metamorphosis', 'url': 'https://huggingface.co/papers/2410.11878', 'abstract': 'This paper introduces a new learning paradigm termed Neural Metamorphosis (NeuMeta), which aims to build self-morphable neural networks. Contrary to crafting separate models for different architectures or sizes, NeuMeta directly learns the continuous weight manifold of neural networks. Once trained, we can sample weights for any-sized network directly from the manifold, even for previously unseen configurations, without retraining. To achieve this ambitious goal, NeuMeta trains neural implicit functions as hypernetworks. They accept coordinates within the model space as input, and generate corresponding weight values on the manifold. In other words, the implicit function is learned in a way, that the predicted weights is well-performed across various models sizes. In training those models, we notice that, the final performance closely relates on smoothness of the learned manifold. In pursuit of enhancing this smoothness, we employ two strategies. First, we permute weight matrices to achieve intra-model smoothness, by solving the Shortest Hamiltonian Path problem. Besides, we add a noise on the input coordinates when training the implicit function, ensuring models with various sizes shows consistent outputs. As such, NeuMeta shows promising results in synthesizing parameters for various network configurations. Our extensive tests in image classification, semantic segmentation, and image generation reveal that NeuMeta sustains full-size performance even at a 75% compression rate.', 'score': 7, 'issue_id': 138, 'pub_date': '2024-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'b168c08bbe553e03', 'data': {'categories': ['#optimization', '#training', '#transfer_learning', '#architecture'], 'emoji': '🦋', 'ru': {'title': 'Самоизменяющиеся нейросети: один манифолд для всех архитектур', 'desc': 'Статья представляет новую парадигму обучения под названием Neural Metamorphosis (NeuMeta), которая позволяет создавать самоизменяющиеся нейронные сети. NeuMeta обучает нейронные неявные функции в качестве гипер-сетей, которые генерируют веса для сетей любого размера, даже для ранее невиданных конфигураций. Для улучшения производительности авторы применяют методы перестановки весовых матриц и добавления шума к входным координатам. Эксперименты показывают, что NeuMeta сохраняет полную производительность даже при 75% сжатии сети.'}, 'en': {'title': '"Shape-Shifting Networks: One Model, Infinite Possibilities"', 'desc': 'The paper introduces Neural Metamorphosis (NeuMeta), a new approach that allows neural networks to adapt their size and architecture without needing separate models. NeuMeta learns a continuous weight manifold, enabling the generation of weights for any network size directly from this manifold. This is achieved by training neural implicit functions as hypernetworks, which take model space coordinates as input to produce corresponding weights. The approach ensures smoothness in the learned manifold, using techniques like weight matrix permutation and input noise, resulting in high performance across various network configurations.'}, 'zh': {'title': '神经变形：自我变形的神经网络新范式', 'desc': '这篇论文介绍了一种新的学习范式，称为神经变形（NeuMeta），旨在构建自我变形的神经网络。与为不同架构或大小单独设计模型不同，NeuMeta直接学习神经网络的连续权重流形。通过训练神经隐函数作为超网络，NeuMeta可以在不重新训练的情况下，从流形中直接采样任何大小网络的权重。实验表明，NeuMeta在图像分类、语义分割和图像生成中表现出色，即使在75%的压缩率下也能保持全尺寸性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12391', 'title': 'Tracking Universal Features Through Fine-Tuning and Model Merging', 'url': 'https://huggingface.co/papers/2410.12391', 'abstract': 'We study how features emerge, disappear, and persist across models fine-tuned on different domains of text. More specifically, we start from a base one-layer Transformer language model that is trained on a combination of the BabyLM corpus, and a collection of Python code from The Stack. This base model is adapted to two new domains of text: TinyStories, and the Lua programming language, respectively; and then these two models are merged using these two models using spherical linear interpolation. Our exploration aims to provide deeper insights into the stability and transformation of features across typical transfer-learning scenarios using small-scale models and sparse auto-encoders.', 'score': 5, 'issue_id': 139, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '6c1b6c634a13e9b7', 'data': {'categories': ['#small_models', '#synthetic', '#plp', '#training', '#transfer_learning'], 'emoji': '🔄', 'ru': {'title': 'Эволюция признаков в моделях при переносе обучения', 'desc': 'Исследование посвящено изучению появления, исчезновения и сохранения признаков в моделях, дообученных на разных текстовых доменах. Авторы начинают с базовой однослойной модели Transformer, обученной на корпусе BabyLM и коллекции Python-кода из The Stack. Затем эта модель адаптируется к двум новым доменам: TinyStories и язык программирования Lua. Результаты объединяются с использованием сферической линейной интерполяции для получения более глубокого понимания стабильности и трансформации признаков в сценариях трансферного обучения.'}, 'en': {'title': 'Unveiling Feature Dynamics in Transfer Learning', 'desc': "The paper investigates how features in machine learning models change when they are fine-tuned on different types of text data. It uses a one-layer Transformer model initially trained on a mix of simple English text and Python code. This model is then adapted to new domains, specifically children's stories and Lua programming language, and the resulting models are combined using a technique called spherical linear interpolation. The study aims to understand how features remain stable or transform during transfer learning, using small models and sparse auto-encoders."}, 'zh': {'title': '探索特征在迁移学习中的稳定性与转变', 'desc': '这篇论文研究了在不同文本领域微调的模型中，特征是如何出现、消失和持续存在的。研究从一个基础的单层Transformer语言模型开始，该模型在BabyLM语料库和Python代码集合上进行训练。然后，这个基础模型被适配到两个新的文本领域：TinyStories和Lua编程语言，并通过球面线性插值合并这两个模型。我们的探索旨在通过小规模模型和稀疏自编码器，深入了解特征在典型迁移学习场景中的稳定性和转变。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12722', 'title': 'WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation', 'url': 'https://huggingface.co/papers/2410.12722', 'abstract': 'Multimodal/vision language models (VLMs) are increasingly being deployed in healthcare settings worldwide, necessitating robust benchmarks to ensure their safety, efficacy, and fairness. Multiple-choice question and answer (QA) datasets derived from national medical examinations have long served as valuable evaluation tools, but existing datasets are largely text-only and available in a limited subset of languages and countries. To address these challenges, we present WorldMedQA-V, an updated multilingual, multimodal benchmarking dataset designed to evaluate VLMs in healthcare. WorldMedQA-V includes 568 labeled multiple-choice QAs paired with 568 medical images from four countries (Brazil, Israel, Japan, and Spain), covering original languages and validated English translations by native clinicians, respectively. Baseline performance for common open- and closed-source models are provided in the local language and English translations, and with and without images provided to the model. The WorldMedQA-V benchmark aims to better match AI systems to the diverse healthcare environments in which they are deployed, fostering more equitable, effective, and representative applications.', 'score': 5, 'issue_id': 138, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '6a533574af0f5068', 'data': {'categories': ['#science', '#benchmark', '#cv', '#multilingual', '#healthcare', '#multimodal', '#ethics', '#dataset', '#open_source', '#machine_translation'], 'emoji': '🏥', 'ru': {'title': 'Мультимодальный многоязычный эталон для оценки ИИ в медицине', 'desc': 'WorldMedQA-V - это новый многоязычный мультимодальный набор данных для оценки визуально-языковых моделей в здравоохранении. Он содержит 568 вопросов с вариантами ответов и медицинскими изображениями из 4 стран на оригинальных языках и в английском переводе. Датасет позволяет оценивать производительность моделей с изображениями и без них, на разных языках. Цель WorldMedQA-V - обеспечить более справедливое и эффективное применение ИИ-систем в различных условиях здравоохранения.'}, 'en': {'title': 'Enhancing Healthcare AI: A Global, Multimodal Approach', 'desc': 'The paper introduces WorldMedQA-V, a new benchmarking dataset for evaluating vision language models (VLMs) in healthcare. This dataset includes 568 multiple-choice questions paired with medical images from four countries, offering both original and English translations. It aims to address the limitations of existing text-only datasets by providing a multilingual and multimodal approach. The goal is to ensure that AI systems are more effective and fair in diverse healthcare settings.'}, 'zh': {'title': 'WorldMedQA-V：多语言多模态医疗评估新基准', 'desc': '这篇论文介绍了一种新的多语言、多模态基准数据集，名为WorldMedQA-V，用于评估在医疗领域的视觉语言模型（VLMs）。该数据集包含来自四个国家的568个多项选择题和相应的医学图像，并提供原始语言和经过验证的英语翻译。研究提供了常见开源和闭源模型在本地语言和英语翻译下的基线性能，并比较了有无图像输入的效果。WorldMedQA-V旨在更好地匹配AI系统与多样化的医疗环境，促进更公平和有效的应用。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12109', 'title': 'OMCAT: Omni Context Aware Transformer', 'url': 'https://huggingface.co/papers/2410.12109', 'abstract': 'Large Language Models (LLMs) have made significant strides in text generation and comprehension, with recent advancements extending into multimodal LLMs that integrate visual and audio inputs. However, these models continue to struggle with fine-grained, cross-modal temporal understanding, particularly when correlating events across audio and video streams. We address these challenges with two key contributions: a new dataset and model, called OCTAV and OMCAT respectively. OCTAV (Omni Context and Temporal Audio Video) is a novel dataset designed to capture event transitions across audio and video. Second, OMCAT (Omni Context Aware Transformer) is a powerful model that leverages RoTE (Rotary Time Embeddings), an innovative extension of RoPE, to enhance temporal grounding and computational efficiency in time-anchored tasks. Through a robust three-stage training pipeline-feature alignment, instruction tuning, and OCTAV-specific training-OMCAT excels in cross-modal temporal understanding. Our model demonstrates state-of-the-art performance on Audio-Visual Question Answering (AVQA) tasks and the OCTAV benchmark, showcasing significant gains in temporal reasoning and cross-modal alignment, as validated through comprehensive experiments and ablation studies. Our dataset and code will be made publicly available. The link to our demo page is https://om-cat.github.io.', 'score': 4, 'issue_id': 135, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': 'c2efc033900c5b2f', 'data': {'categories': ['#science', '#benchmark', '#cv', '#graphs', '#video', '#optimization', '#multimodal', '#training', '#dataset', '#open_source', '#audio', '#architecture', '#alignment'], 'emoji': '🎬', 'ru': {'title': 'OMCAT: Новый подход к мультимодальному временному пониманию', 'desc': 'Статья представляет новый датасет OCTAV и модель OMCAT для улучшения временного понимания в мультимодальных задачах. OCTAV создан для захвата переходов событий между аудио и видео потоками. OMCAT использует инновационный метод RoTE для улучшения временной привязки и вычислительной эффективности. Модель демонстрирует передовые результаты в задачах аудио-визуального ответа на вопросы и на бенчмарке OCTAV.'}, 'en': {'title': "Mastering Time: OMCAT's Leap in Cross-Modal Understanding", 'desc': 'This paper introduces a new dataset called OCTAV and a model named OMCAT to improve cross-modal temporal understanding in multimodal large language models. OCTAV is designed to capture event transitions across audio and video, while OMCAT uses Rotary Time Embeddings to enhance temporal grounding. The model undergoes a three-stage training process, resulting in state-of-the-art performance on Audio-Visual Question Answering tasks. The research demonstrates significant improvements in temporal reasoning and cross-modal alignment, with the dataset and code being made publicly available.'}, 'zh': {'title': '跨模态时间理解的新突破', 'desc': '大型语言模型（LLMs）在文本生成和理解方面取得了显著进展，最近的进展扩展到整合视觉和音频输入的多模态LLMs。然而，这些模型在细粒度的跨模态时间理解上仍然存在困难，特别是在音频和视频流事件的关联上。我们通过两个关键贡献来解决这些挑战：一个新的数据集OCTAV和一个新的模型OMCAT。OMCAT模型通过使用RoTE（旋转时间嵌入）来增强时间锚定任务的时间基础和计算效率，在跨模态时间理解上表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11900', 'title': 'FLARE: Faithful Logic-Aided Reasoning and Exploration', 'url': 'https://huggingface.co/papers/2410.11900', 'abstract': 'Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce Faithful Logic-Aided Reasoning and Exploration (\\ours), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on 7 out of 9 diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that {\\ours} allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.', 'score': 3, 'issue_id': 161, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '9dc3af0706a41348', 'data': {'categories': ['#rag', '#reasoning', '#benchmark', '#plp', '#interpretability', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Логика и LLM объединяются для надежного рассуждения', 'desc': 'Эта статья представляет новый подход к рассуждению и исследованию пространства задач с использованием больших языковых моделей (LLM). Метод называется Faithful Logic-Aided Reasoning and Exploration (FLARE) и сочетает планирование решения с помощью LLM и формализацию запроса в логическом программировании. FLARE позволяет вычислять достоверность процесса рассуждений и анализировать шаги многоэтапного поиска без использования внешних решателей. Авторы сообщают о достижении наилучших результатов на 7 из 9 разнообразных тестов на рассуждение.'}, 'en': {'title': 'Enhancing Reasoning Faithfulness with Logic-Aided Exploration', 'desc': 'This paper presents a new method called Faithful Logic-Aided Reasoning and Exploration (FLARE) that enhances question answering and reasoning using Large Language Models (LLMs). FLARE combines the strengths of LLMs with logic programming to create a more interpretable approach for solving complex problems. It allows for a detailed exploration of the reasoning process by simulating code execution and analyzing each step in a multi-hop search. The results show that FLARE achieves state-of-the-art performance on various reasoning tasks while maintaining high faithfulness in its reasoning outputs.'}, 'zh': {'title': '忠实推理与探索的新方法', 'desc': '现代问答（QA）和推理方法通常依赖大型语言模型（LLM）和提示技术，如思维链（CoT），以期在问题空间中进行更细致的探索和推理。然而，这些方法在生成与模型中间推理链一致的输出时存在困难。我们提出了一种新的可解释方法——忠实逻辑辅助推理与探索（\textit{ours}），通过任务分解来遍历问题空间。该方法利用LLM规划解决方案，并通过逻辑编程代码将查询软形式化为事实和谓词，进行多跳搜索，从而计算推理过程的忠实度。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12491', 'title': 'Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL', 'url': 'https://huggingface.co/papers/2410.12491', 'abstract': 'Large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) have demonstrated remarkable capabilities, but their underlying reward functions and decision-making processes remain opaque. This paper introduces a novel approach to interpreting LLMs by applying inverse reinforcement learning (IRL) to recover their implicit reward functions. We conduct experiments on toxicity-aligned LLMs of varying sizes, extracting reward models that achieve up to 80.40% accuracy in predicting human preferences. Our analysis reveals key insights into the non-identifiability of reward functions, the relationship between model size and interpretability, and potential pitfalls in the RLHF process. We demonstrate that IRL-derived reward models can be used to fine-tune new LLMs, resulting in comparable or improved performance on toxicity benchmarks. This work provides a new lens for understanding and improving LLM alignment, with implications for the responsible development and deployment of these powerful systems.', 'score': 3, 'issue_id': 138, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '38785426efa90355', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#interpretability', '#ethics', '#training', '#alignment'], 'emoji': '🔍', 'ru': {'title': "Расшифровка 'черного ящика' языковых моделей", 'desc': 'Статья представляет новый подход к интерпретации больших языковых моделей (LLM) с использованием обратного обучения с подкреплением (IRL) для восстановления их неявных функций вознаграждения. Эксперименты проводились на LLM различных размеров, настроенных на снижение токсичности, с извлечением моделей вознаграждения, достигающих точности до 80,40% в предсказании предпочтений человека. Анализ выявил ключевые аспекты неидентифицируемости функций вознаграждения, связи между размером модели и интерпретируемостью, а также потенциальных проблем в процессе RLHF. Исследование показывает, что модели вознаграждения, полученные с помощью IRL, могут использоваться для дополнительного обучения новых LLM, что приводит к сопоставимой или улучшенной производительности в тестах на токсичность.'}, 'en': {'title': 'Decoding the Secrets of Language Models with Inverse Reinforcement Learning', 'desc': 'This paper explores how large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) can be better understood by using inverse reinforcement learning (IRL) to uncover their hidden reward functions. By applying IRL to LLMs of different sizes, the study achieves up to 80.40% accuracy in predicting human preferences, offering insights into the challenges of identifying reward functions and the link between model size and interpretability. The research highlights potential issues in the RLHF process and shows that IRL-derived reward models can enhance the performance of new LLMs on toxicity benchmarks. This approach provides a fresh perspective on aligning LLMs responsibly, aiding in their development and deployment.'}, 'zh': {'title': '逆向强化学习：揭示大型语言模型的隐秘奖励', 'desc': '这篇论文介绍了一种新方法，通过逆向强化学习来解释大型语言模型的隐含奖励函数。研究表明，使用这种方法可以提取出高达80.40%准确率的人类偏好预测模型。分析揭示了奖励函数的不可识别性、模型大小与可解释性之间的关系，以及RLHF过程中的潜在问题。通过这种方法微调的新模型在毒性基准测试中表现出色，提供了理解和改进LLM对齐的新视角。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09724', 'title': 'Taming Overconfidence in LLMs: Reward Calibration in RLHF', 'url': 'https://huggingface.co/papers/2410.09724', 'abstract': 'Language model calibration refers to the alignment between the confidence of the model and the actual performance of its responses. While previous studies point out the overconfidence phenomenon in Large Language Models (LLMs) and show that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) are overconfident with a more sharpened output probability, in this study, we reveal that RLHF tends to lead models to express verbalized overconfidence in their own responses. We investigate the underlying cause of this overconfidence and demonstrate that reward models used for Proximal Policy Optimization (PPO) exhibit inherent biases towards high-confidence scores regardless of the actual quality of responses. Building upon this insight, we propose two PPO variants: PPO-M: PPO with Calibrated Reward Modeling and PPO-C: PPO with Calibrated Reward Calculation. PPO-M integrates explicit confidence scores in reward model training, which calibrates reward models to better capture the alignment between response quality and verbalized confidence. PPO-C adjusts the reward score during PPO based on the difference between the current reward and the moving average of past rewards. Both PPO-M and PPO-C can be seamlessly integrated into the current PPO pipeline and do not require additional golden labels. We evaluate our methods on both Llama3-8B and Mistral-7B across six diverse datasets including multiple-choice and open-ended generation. Experiment results demonstrate that both of our methods can reduce calibration error and maintain performance comparable to standard PPO. We further show that they do not compromise model capabilities in open-ended conversation settings.', 'score': 2, 'issue_id': 141, 'pub_date': '2024-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'a649dfe2bf936909', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#benchmark', '#optimization', '#interpretability', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Калибровка уверенности: новый подход к RLHF', 'desc': 'Статья исследует проблему переоценки уверенности в больших языковых моделях (LLM), обученных с помощью обучения с подкреплением на основе обратной связи от человека (RLHF). Авторы выявляют, что модели вознаграждения, используемые в процессе Проксимальной оптимизации политики (PPO), имеют склонность к высоким оценкам уверенности независимо от качества ответов. Предлагаются два варианта PPO: PPO-M с калиброванным моделированием вознаграждения и PPO-C с калиброванным расчетом вознаграждения. Эксперименты показывают, что оба метода снижают ошибку калибровки, сохраняя производительность на уровне стандартного PPO.'}, 'en': {'title': 'Balancing Confidence: Calibrating Language Models for Better Alignment', 'desc': 'This paper explores the issue of overconfidence in Large Language Models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF). It identifies that reward models used in Proximal Policy Optimization (PPO) are biased towards high-confidence scores, leading to verbalized overconfidence. To address this, the authors propose two new PPO variants: PPO-M, which incorporates calibrated reward modeling, and PPO-C, which adjusts reward scores based on past performance. These methods effectively reduce calibration error without sacrificing model performance, as demonstrated in experiments across various datasets.'}, 'zh': {'title': '校准语言模型：减少过度自信，提升模型表现', 'desc': '这篇论文研究了大型语言模型在使用人类反馈强化学习训练后表现出的过度自信现象。研究发现，奖励模型在近端策略优化中对高置信度评分存在固有偏见。为此，作者提出了两种新的近端策略优化变体：PPO-M和PPO-C，分别通过校准奖励模型和校准奖励计算来解决这个问题。实验结果表明，这两种方法能够减少校准误差，同时保持模型性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11843', 'title': 'From Commands to Prompts: LLM-based Semantic File System for AIOS', 'url': 'https://huggingface.co/papers/2410.11843', 'abstract': 'Large language models (LLMs) have demonstrated significant potential in the development of intelligent applications and systems such as LLM-based agents and agent operating systems (AIOS). However, when these applications and systems interact with the underlying file system, the file system still remains the traditional paradigm: reliant on manual navigation through precise commands. This paradigm poses a bottleneck to the usability of these systems as users are required to navigate complex folder hierarchies and remember cryptic file names. To address this limitation, we propose an LLM-based semantic file system ( LSFS ) for prompt-driven file management. Unlike conventional approaches, LSFS incorporates LLMs to enable users or agents to interact with files through natural language prompts, facilitating semantic file management. At the macro-level, we develop a comprehensive API set to achieve semantic file management functionalities, such as semantic file retrieval, file update monitoring and summarization, and semantic file rollback). At the micro-level, we store files by constructing semantic indexes for them, design and implement syscalls of different semantic operations (e.g., CRUD, group by, join) powered by vector database. Our experiments show that LSFS offers significant improvements over traditional file systems in terms of user convenience, the diversity of supported functions, and the accuracy and efficiency of file operations. Additionally, with the integration of LLM, our system enables more intelligent file management tasks, such as content summarization and version comparison, further enhancing its capabilities.', 'score': 1, 'issue_id': 161, 'pub_date': '2024-09-23', 'pub_date_card': {'ru': '23 сентября', 'en': 'September 23', 'zh': '9月23日'}, 'hash': '68eb78dc881f88ae', 'data': {'categories': ['#optimization', '#interpretability', '#data', '#agents', '#architecture', '#alignment'], 'emoji': '🗂️', 'ru': {'title': 'Революция в управлении файлами: семантическая файловая система на основе ИИ', 'desc': 'Статья представляет семантическую файловую систему на основе больших языковых моделей (LSFS). Эта система позволяет пользователям и агентам взаимодействовать с файлами с помощью естественного языка, облегчая семантическое управление файлами. LSFS включает комплексный набор API для семантических операций с файлами и использует векторные базы данных для хранения и индексации. Эксперименты показывают значительные улучшения в удобстве использования, разнообразии функций и эффективности файловых операций по сравнению с традиционными файловыми системами.'}, 'en': {'title': 'Revolutionizing File Management with Natural Language', 'desc': 'This paper introduces a new system called the LLM-based Semantic File System (LSFS) that enhances file management using large language models (LLMs). Unlike traditional file systems that require users to remember complex commands and navigate through folders, LSFS allows users to interact with files using natural language prompts. The system includes a comprehensive API for various semantic file management tasks, such as retrieval and monitoring, and utilizes semantic indexing and vector databases for efficient operations. Experiments show that LSFS significantly improves user convenience and the accuracy of file operations compared to conventional systems.'}, 'zh': {'title': '语义文件管理，智能化未来', 'desc': '大型语言模型（LLM）在智能应用和系统的开发中展现了巨大的潜力，但传统的文件系统仍依赖于手动导航和精确命令，这限制了用户的使用体验。为了解决这个问题，我们提出了一种基于LLM的语义文件系统（LSFS），允许用户通过自然语言提示与文件进行交互。LSFS通过构建语义索引和实现不同的语义操作系统调用，提供了语义文件检索、更新监控和摘要等功能。实验表明，LSFS在用户便利性、功能多样性和文件操作的准确性与效率方面显著优于传统文件系统。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13720', 'title': 'Movie Gen: A Cast of Media Foundation Models', 'url': 'https://huggingface.co/papers/2410.13720', 'abstract': "We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.", 'score': 86, 'issue_id': 147, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '086b8ff148ce7df3', 'data': {'categories': ['#diffusion', '#synthetic', '#inference', '#video', '#optimization', '#multimodal', '#data', '#training', '#open_source', '#audio', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'MovieGen: Революция в генерации мультимедиа', 'desc': 'MovieGen - это набор фундаментальных моделей, генерирующих высококачественные видео в формате 1080p HD с синхронизированным аудио. Модели устанавливают новый уровень качества в нескольких задачах, включая синтез видео по тексту, персонализацию видео и генерацию аудио. Крупнейшая модель имеет 30 миллиардов параметров и может генерировать 16-секундные видео. Авторы представляют ряд технических инноваций в архитектуре, обучении и оптимизации моделей генерации мультимедиа.'}, 'en': {'title': 'Revolutionizing Video Creation with Movie Gen', 'desc': 'Movie Gen introduces advanced foundation models capable of generating high-quality videos with synchronized audio, offering new capabilities in video editing and personalization. The models excel in tasks like text-to-video synthesis and video-to-audio generation, setting a new benchmark in the field. With a 30 billion parameter transformer, the system can produce 16-second videos at 16 frames per second, showcasing significant technical innovations. These advancements aim to push forward the research and development of large-scale media generation models.'}, 'zh': {'title': 'Movie Gen：引领高清视频生成新标准', 'desc': '这篇论文介绍了一个名为Movie Gen的基础模型集，可以生成高质量的1080p高清视频，并支持不同的宽高比和同步音频。该模型还具备精确的指令视频编辑和基于用户图像生成个性化视频的能力。Movie Gen在多项任务上设立了新的技术标准，包括文本到视频合成、视频个性化、视频编辑、视频到音频生成和文本到音频生成。通过多项技术创新和简化，该模型在架构、潜在空间、训练目标、数据策划等方面取得了突破，推动了大规模媒体生成模型的进步。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13754', 'title': 'MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures', 'url': 'https://huggingface.co/papers/2410.13754', 'abstract': 'Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communities with varying protocols and maturity levels; and (2) significant query, grading, and generalization biases. To address these, we introduce MixEval-X, the first any-to-any real-world benchmark designed to optimize and standardize evaluations across input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring evaluations generalize effectively to real-world use cases. Extensive meta-evaluations show our approach effectively aligns benchmark samples with real-world task distributions and the model rankings correlate strongly with that of crowd-sourced real-world evaluations (up to 0.98). We provide comprehensive leaderboards to rerank existing models and organizations and offer insights to enhance understanding of multi-modal evaluations and inform future research.', 'score': 74, 'issue_id': 148, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '82517ad6fbb54273', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#survey', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'MixEval-X: Универсальный бенчмарк для оценки многомодальных моделей ИИ', 'desc': 'Статья представляет MixEval-X - первый многомодальный бенчмарк для оценки моделей ИИ в реальных задачах. Авторы предлагают новые методы для создания репрезентативных наборов тестов, охватывающих различные модальности ввода и вывода. Бенчмарк решает проблемы несогласованности стандартов оценки и различных видов смещений в существующих методах. Результаты показывают высокую корреляцию с оценками краудсорсинга в реальных сценариях использования.'}, 'en': {'title': 'MixEval-X: Bridging the Gap Between AI Benchmarks and Real-World Performance', 'desc': 'The paper introduces MixEval-X, a benchmark designed to standardize evaluations across different input and output modalities in AI models. It addresses issues of inconsistent evaluation standards and biases in current methods by proposing a multi-modal benchmark mixture and adaptation-rectification pipelines. These pipelines help align benchmark samples with real-world task distributions, ensuring that evaluations are more representative of real-world scenarios. The approach shows strong correlation with real-world evaluations, providing valuable insights for improving multi-modal evaluations and guiding future research.'}, 'zh': {'title': 'MixEval-X：优化多模态评估的全新基准', 'desc': '这篇论文讨论了AI模型在处理多种信号时需要可靠的评估方法。当前评估存在标准不一致和偏差问题。为此，作者提出了MixEval-X，一个用于优化和标准化多模态评估的基准。通过这种方法，评估结果更贴近真实世界的任务分布。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12784', 'title': 'JudgeBench: A Benchmark for Evaluating LLM-based Judges', 'url': 'https://huggingface.co/papers/2410.12784', 'abstract': "LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger judges to evaluate them. Existing benchmarks primarily focus on a judge's alignment with human preferences, but often fail to account for more challenging tasks where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to objectively evaluate LLM-based judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness. Our comprehensive evaluation on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced LLM-based judges. Data and code are available at https://github.com/ScalerLab/JudgeBench .", 'score': 41, 'issue_id': 160, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'a81030e9f379736a', 'data': {'categories': ['#reasoning', '#benchmark', '#math', '#plp', '#data', '#training', '#dataset', '#open_source', '#architecture', '#alignment'], 'emoji': '⚖️', 'ru': {'title': 'JudgeBench: Новый стандарт для оценки ИИ-судей', 'desc': 'Статья представляет новую систему оценки судей на основе больших языковых моделей (LLM). Авторы предлагают JudgeBench - набор тестов для оценки LLM-судей в сложных задачах, охватывающих знания, рассуждения, математику и программирование. JudgeBench использует новый подход к созданию сложных пар ответов с метками предпочтений, отражающими объективную правильность. Результаты показывают, что JudgeBench представляет значительно большую сложность, чем предыдущие тесты, с многими сильными моделями, работающими лишь немного лучше, чем случайное угадывание.'}, 'en': {'title': 'JudgeBench: Raising the Bar for LLM Evaluation', 'desc': 'The paper introduces JudgeBench, a new benchmark designed to evaluate the reliability of LLM-based judges, which are used to assess and improve machine learning models. Unlike existing benchmarks that focus on alignment with human preferences, JudgeBench emphasizes objective correctness in challenging tasks like reasoning and coding. The framework converts difficult datasets into response pairs with preference labels, providing a more rigorous test for LLM-based judges. Results show that even advanced models struggle with JudgeBench, highlighting its effectiveness in assessing the capabilities of these judges.'}, 'zh': {'title': 'JudgeBench：评估大语言模型评判者的新基准', 'desc': '这篇论文提出了一种新的评估框架，用于客观地评估基于大语言模型（LLM）的评判者。研究者开发了一个名为JudgeBench的基准，用于评估这些评判者在知识、推理、数学和编程等方面的挑战性响应对。JudgeBench通过一个新颖的流程，将现有的困难数据集转换为具有偏好标签的挑战性响应对，以反映客观的正确性。研究结果表明，JudgeBench比以往的基准更具挑战性，许多强大的模型在此基准上的表现仅略优于随机猜测。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13863', 'title': 'Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens', 'url': 'https://huggingface.co/papers/2410.13863', 'abstract': 'Scaling up autoregressive models in vision has not proven as beneficial as in large language models. In this work, we investigate this scaling problem in the context of text-to-image generation, focusing on two critical factors: whether models use discrete or continuous tokens, and whether tokens are generated in a random or fixed raster order using BERT- or GPT-like transformer architectures. Our empirical results show that, while all models scale effectively in terms of validation loss, their evaluation performance -- measured by FID, GenEval score, and visual quality -- follows different trends. Models based on continuous tokens achieve significantly better visual quality than those using discrete tokens. Furthermore, the generation order and attention mechanisms significantly affect the GenEval score: random-order models achieve notably better GenEval scores compared to raster-order models. Inspired by these findings, we train Fluid, a random-order autoregressive model on continuous tokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16 on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our findings and results will encourage future efforts to further bridge the scaling gap between vision and language models.', 'score': 35, 'issue_id': 160, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '3fa9a449112a391b', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#optimization', '#games', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Новый подход к масштабированию моделей генерации изображений', 'desc': 'Исследователи изучили проблему масштабирования авторегрессионных моделей в контексте генерации изображений по тексту. Они сравнили модели с дискретными и непрерывными токенами, а также с различным порядком генерации и архитектурами трансформеров. Результаты показали, что модели с непрерывными токенами достигают лучшего визуального качества, а случайный порядок генерации улучшает показатели GenEval. На основе этих выводов была разработана модель Fluid, достигшая нового уровня производительности в нескольких бенчмарках.'}, 'en': {'title': 'Bridging the Gap: Continuous Tokens and Random Order in Vision Models', 'desc': 'This paper explores the challenges of scaling autoregressive models for text-to-image generation, focusing on the use of discrete versus continuous tokens and the order of token generation. The study finds that models using continuous tokens produce higher visual quality images compared to those using discrete tokens. Additionally, models that generate tokens in a random order outperform those using a fixed raster order in terms of GenEval scores. The authors introduce Fluid, a random-order autoregressive model with continuous tokens, which sets new benchmarks in zero-shot FID and GenEval scores, suggesting a promising direction for future research in bridging the gap between vision and language models.'}, 'zh': {'title': '突破视觉与语言模型扩展的界限', 'desc': '这篇论文研究了在图像生成中自回归模型的扩展问题，特别关注使用离散或连续的标记，以及标记生成的顺序。研究发现，使用连续标记的模型在视觉质量上明显优于使用离散标记的模型。生成顺序和注意力机制对GenEval评分有显著影响，随机顺序的模型在GenEval评分上表现更好。基于这些发现，作者训练了Fluid模型，在MS-COCO 30K数据集上取得了新的零样本FID记录。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13268', 'title': 'Roadmap towards Superhuman Speech Understanding using Large Language Models', 'url': 'https://huggingface.co/papers/2410.13268', 'abstract': 'The success of large language models (LLMs) has prompted efforts to integrate speech and audio data, aiming to create general foundation models capable of processing both textual and non-textual inputs. Recent advances, such as GPT-4o, highlight the potential for end-to-end speech LLMs, which preserves non-semantic information and world knowledge for deeper speech understanding. To guide the development of speech LLMs, we propose a five-level roadmap, ranging from basic automatic speech recognition (ASR) to advanced superhuman models capable of integrating non-semantic information with abstract acoustic knowledge for complex tasks. Moreover, we design a benchmark, SAGI Bechmark, that standardizes critical aspects across various tasks in these five levels, uncovering challenges in using abstract acoustic knowledge and completeness of capability. Our findings reveal gaps in handling paralinguistic cues and abstract acoustic knowledge, and we offer future directions. This paper outlines a roadmap for advancing speech LLMs, introduces a benchmark for evaluation, and provides key insights into their current limitations and potential.', 'score': 33, 'issue_id': 153, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '929ec80dcb105705', 'data': {'categories': ['#science', '#benchmark', '#agi', '#multimodal', '#survey', '#audio', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'Дорожная карта для речевых LLM: от распознавания речи к сверхчеловеческим моделям', 'desc': 'В статье предлагается дорожная карта из пяти уровней для развития речевых языковых моделей (LLM), способных обрабатывать как текстовые, так и нетекстовые входные данные. Авторы разработали бенчмарк SAGI для стандартизации оценки различных задач на этих уровнях. Исследование выявило пробелы в обработке паралингвистических сигналов и абстрактных акустических знаний. Статья предлагает направления для будущих исследований в области речевых LLM.'}, 'en': {'title': 'Bridging Text and Sound: The Future of Speech Language Models', 'desc': 'This paper explores the integration of speech and audio data into large language models (LLMs) to create versatile models that can handle both text and non-text inputs. It introduces a five-level roadmap for developing speech LLMs, from basic automatic speech recognition (ASR) to advanced models that incorporate non-semantic information and abstract acoustic knowledge. The authors also present the SAGI Benchmark, which evaluates these models across various tasks and highlights challenges in processing paralinguistic cues and abstract acoustic knowledge. The paper provides insights into the current limitations of speech LLMs and suggests future research directions to enhance their capabilities.'}, 'zh': {'title': '语音大模型的未来：从基础到超人', 'desc': '这篇论文探讨了将语音和音频数据整合到大型语言模型中的可能性，旨在创建能够处理文本和非文本输入的通用基础模型。研究提出了一个五级路线图，从基本的自动语音识别到能够处理复杂任务的超人模型。作者还设计了一个名为SAGI的基准，用于标准化这些五个级别中各种任务的关键方面。研究发现当前模型在处理副语言线索和抽象声学知识方面存在不足，并提供了未来的发展方向。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13757', 'title': 'MobA: A Two-Level Agent System for Efficient Mobile Task Automation', 'url': 'https://huggingface.co/papers/2410.13757', 'abstract': 'Current mobile assistants are limited by dependence on system APIs or struggle with complex user instructions and diverse interfaces due to restricted comprehension and decision-making abilities. To address these challenges, we propose MobA, a novel Mobile phone Agent powered by multimodal large language models that enhances comprehension and planning capabilities through a sophisticated two-level agent architecture. The high-level Global Agent (GA) is responsible for understanding user commands, tracking history memories, and planning tasks. The low-level Local Agent (LA) predicts detailed actions in the form of function calls, guided by sub-tasks and memory from the GA. Integrating a Reflection Module allows for efficient task completion and enables the system to handle previously unseen complex tasks. MobA demonstrates significant improvements in task execution efficiency and completion rate in real-life evaluations, underscoring the potential of MLLM-empowered mobile assistants.', 'score': 30, 'issue_id': 150, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'a4a73fb090d1a0ae', 'data': {'categories': ['#reasoning', '#agi', '#multimodal', '#agents', '#architecture', '#alignment'], 'emoji': '📱', 'ru': {'title': 'MobA: Умный мобильный помощник нового поколения', 'desc': 'MobA - это новый мобильный агент, основанный на мультимодальных больших языковых моделях. Он использует двухуровневую архитектуру с глобальным агентом для понимания команд и планирования, и локальным агентом для выполнения конкретных действий. Система включает модуль рефлексии для эффективного выполнения задач и обработки новых сложных заданий. MobA показывает значительное улучшение эффективности и уровня выполнения задач в реальных условиях.'}, 'en': {'title': 'Revolutionizing Mobile Assistance with Multimodal Intelligence', 'desc': "The paper introduces MobA, a mobile assistant that uses multimodal large language models to improve understanding and task planning. It features a two-level agent architecture with a Global Agent for command comprehension and task planning, and a Local Agent for executing detailed actions. A Reflection Module is integrated to enhance the system's ability to handle complex and novel tasks. Real-life tests show MobA's improved efficiency and success in completing tasks, highlighting the potential of advanced language models in mobile assistants."}, 'zh': {'title': '多模态大语言模型助力移动助手新突破', 'desc': '当前的移动助手由于对系统API的依赖和对复杂用户指令的理解能力有限，难以处理多样化的界面。为了解决这些问题，我们提出了MobA，这是一种由多模态大语言模型驱动的移动代理，通过复杂的双层代理架构增强理解和规划能力。高层的全局代理负责理解用户命令、跟踪历史记忆和规划任务，而低层的本地代理则根据子任务和全局代理的记忆预测详细的动作。通过集成反思模块，系统能够高效完成任务，并处理以前未见过的复杂任务。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12705', 'title': 'WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines', 'url': 'https://huggingface.co/papers/2410.12705', 'abstract': 'Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.', 'score': 29, 'issue_id': 157, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '6829d8490ef2d294', 'data': {'categories': ['#benchmark', '#multilingual', '#cv', '#graphs', '#multimodal', '#dataset', '#open_source', '#games', '#low_resource'], 'emoji': '🌎', 'ru': {'title': 'WorldCuisines: Глобальный тест на кулинарную эрудицию для ИИ', 'desc': 'Статья представляет новый бенчмарк WorldCuisines для оценки понимания культурно-специфических знаний моделями компьютерного зрения и обработки естественного языка. Бенчмарк включает набор данных для визуального ответа на вопросы (VQA) с парами текст-изображение на 30 языках и диалектах, охватывающих 9 языковых семей. Авторы предоставляют наборы данных для оценки в двух размерах, а также тренировочный набор из 1 миллиона примеров. Результаты показывают, что модели лучше справляются с правильным контекстом местоположения, но испытывают трудности с состязательными контекстами и прогнозированием конкретных региональных кухонь и языков.'}, 'en': {'title': '"WorldCuisines: Bridging Cultural Gaps in Vision Language Models"', 'desc': 'The paper introduces WorldCuisines, a large-scale benchmark designed to test Vision Language Models (VLMs) on their ability to understand culture-specific knowledge across multiple languages and dialects. This benchmark includes a Visual Question Answering (VQA) dataset with over 1 million text-image pairs, making it the largest of its kind for multicultural contexts. The study reveals that while VLMs can perform well when given correct location context, they face challenges with adversarial contexts and accurately predicting regional cuisines and languages. To aid further research, the authors provide a comprehensive knowledge base with annotated food entries and images.'}, 'zh': {'title': '跨文化视觉语言理解的新基准', 'desc': '这篇论文介绍了一个名为WorldCuisines的大规模基准，用于评估视觉语言模型在多语言和多文化背景下的理解能力。该基准包括一个视觉问答数据集，涵盖30种语言和方言，涉及9个语言家族，拥有超过100万个数据点。研究发现，视觉语言模型在正确的地理背景下表现较好，但在对抗性背景和预测特定地区的菜肴和语言时表现较差。为了支持未来的研究，作者还发布了一个包含注释食品条目和图像的知识库。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13824', 'title': 'Harnessing Webpage UIs for Text-Rich Visual Understanding', 'url': 'https://huggingface.co/papers/2410.13824', 'abstract': 'Text-rich visual understanding-the ability to process environments where dense textual content is integrated with visuals-is crucial for multimodal large language models (MLLMs) to interact effectively with structured environments. To enhance this capability, we propose synthesizing general multimodal instructions from webpage UIs using text-based large language models (LLMs). Despite lacking direct visual input, text-based LLMs are able to process structured text representations from webpage accessibility trees. These instructions are then paired with UI screenshots to train multimodal models. We introduce MultiUI, a dataset containing 7.3 million samples from 1 million websites, covering diverse multimodal tasks and UI layouts. Models trained on MultiUI not only excel in web UI tasks-achieving up to a 48\\% improvement on VisualWebBench and a 19.1\\% boost in action accuracy on a web agent dataset Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to non-UI domains, such as document understanding, OCR, and chart interpretation. These results highlight the broad applicability of web UI data for advancing text-rich visual understanding across various scenarios.', 'score': 29, 'issue_id': 146, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '7d1ade016ff53a03', 'data': {'categories': ['#science', '#synthetic', '#benchmark', '#cv', '#graphs', '#optimization', '#multimodal', '#data', '#training', '#dataset', '#transfer_learning', '#games', '#architecture'], 'emoji': '🌐', 'ru': {'title': 'Синтез веб-данных для универсального мультимодального понимания', 'desc': 'В статье представлен новый подход к улучшению понимания визуального контекста с текстом для мультимодальных больших языковых моделей. Авторы предлагают синтезировать инструкции из веб-интерфейсов с помощью текстовых языковых моделей. Создан датасет MultiUI, содержащий 7,3 миллиона образцов из 1 миллиона веб-сайтов. Модели, обученные на MultiUI, показывают значительное улучшение в задачах веб-интерфейсов и обобщают свои способности на другие домены.'}, 'en': {'title': 'Unlocking Text-Rich Visual Understanding with Web UI Data', 'desc': 'The paper introduces a method to improve multimodal large language models (MLLMs) by synthesizing instructions from webpage UIs using text-based large language models (LLMs). These models, despite not having direct visual input, can process structured text from webpage accessibility trees and are trained with UI screenshots. The authors present MultiUI, a dataset with 7.3 million samples from 1 million websites, which helps models excel in web UI tasks and generalize to other domains like document understanding and OCR. The study demonstrates that web UI data can significantly enhance text-rich visual understanding across various applications.'}, 'zh': {'title': '网页UI数据：提升多模态视觉理解的关键', 'desc': '这篇论文提出了一种方法，通过网页的可访问性树生成多模态指令，来增强多模态大语言模型的文本丰富视觉理解能力。研究中使用了一个名为MultiUI的数据集，包含了来自100万个网站的730万样本，用于训练多模态模型。实验结果表明，使用MultiUI训练的模型在网页UI任务中表现优异，并且在非网页UI任务和其他领域如文档理解、OCR和图表解释中也有良好的泛化能力。这表明网页UI数据在提升多种场景下的文本丰富视觉理解方面具有广泛的应用潜力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13848', 'title': 'Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2410.13848', 'abstract': "In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.", 'score': 27, 'issue_id': 148, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '8b28045f373976ba', 'data': {'categories': ['#optimization', '#multimodal', '#interpretability', '#games', '#architecture'], 'emoji': '🔀', 'ru': {'title': 'Janus: единая модель для мультимодального понимания и генерации', 'desc': 'Статья представляет Janus - новую авторегрессивную модель для мультимодального понимания и генерации. В отличие от предыдущих подходов, Janus использует отдельные визуальные энкодеры для задач понимания и генерации, что позволяет оптимизировать работу модели. Единая архитектура трансформера обрабатывает данные от обоих энкодеров. Эксперименты показывают, что Janus превосходит предыдущие унифицированные модели и не уступает специализированным моделям для конкретных задач.'}, 'en': {'title': 'Janus: A New Era in Multimodal Intelligence', 'desc': 'The paper introduces Janus, a new framework that improves how machines understand and create content using different types of data, like images and text. Unlike previous models that used one visual encoder for both understanding and generating, Janus separates these tasks into different pathways, which helps improve performance. By using a single transformer architecture, Janus allows each task to choose the best way to process information, making it more flexible and effective. Experiments show that Janus not only outperforms previous models but also competes well with models designed for specific tasks.'}, 'zh': {'title': 'Janus：多模态理解与生成的全新统一框架', 'desc': '这篇论文介绍了Janus，一个统一多模态理解和生成的自回归框架。以往的研究通常使用单一的视觉编码器来处理这两项任务，但由于多模态理解和生成所需的信息粒度不同，这种方法可能导致性能不佳。为了解决这个问题，Janus将视觉编码解耦为独立的路径，同时仍然使用统一的Transformer架构进行处理。实验表明，Janus不仅超越了之前的统一模型，还能匹敌或超过特定任务模型的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13830', 'title': 'DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control', 'url': 'https://huggingface.co/papers/2410.13830', 'abstract': "Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their real-world applications. In this paper, we present DreamVideo-2, a zero-shot video customization framework capable of generating videos with a specific subject and motion trajectory, guided by a single image and a bounding box sequence, respectively, and without the need for test-time fine-tuning. Specifically, we introduce reference attention, which leverages the model's inherent capabilities for subject learning, and devise a mask-guided motion module to achieve precise motion control by fully utilizing the robust motion signal of box masks derived from bounding boxes. While these two components achieve their intended functions, we empirically observe that motion control tends to dominate over subject learning. To address this, we propose two key designs: 1) the masked reference attention, which integrates a blended latent mask modeling scheme into reference attention to enhance subject representations at the desired positions, and 2) a reweighted diffusion loss, which differentiates the contributions of regions inside and outside the bounding boxes to ensure a balance between subject and motion control. Extensive experimental results on a newly curated dataset demonstrate that DreamVideo-2 outperforms state-of-the-art methods in both subject customization and motion control. The dataset, code, and models will be made publicly available.", 'score': 23, 'issue_id': 146, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '67dc892195cd59d6', 'data': {'categories': ['#diffusion', '#video', '#training', '#dataset', '#open_source', '#games', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Создание персонализированных видео одним щелчком', 'desc': 'DreamVideo-2 - это новая система для создания персонализированных видео без дополнительного обучения. Она использует одно изображение и последовательность ограничивающих рамок для генерации видео с заданным объектом и траекторией движения. Система вводит референсное внимание и маскированный модуль движения для балансировки между сохранением объекта и контролем движения. Эксперименты показывают превосходство DreamVideo-2 над современными методами в области персонализации объектов и контроля движения.'}, 'en': {'title': 'Effortless Video Customization with DreamVideo-2', 'desc': 'DreamVideo-2 is a new framework for creating customized videos without needing complex adjustments during testing. It uses a single image and a sequence of bounding boxes to guide video generation, focusing on both the subject and its motion. The framework introduces reference attention and a mask-guided motion module to improve subject learning and motion control. To balance these aspects, it employs masked reference attention and reweighted diffusion loss, achieving superior results compared to existing methods.'}, 'zh': {'title': 'DreamVideo-2：无微调的视频定制新突破', 'desc': '这篇论文介绍了一种名为DreamVideo-2的视频定制框架，可以在不需要测试时微调的情况下生成特定主题和运动轨迹的视频。该方法通过引入参考注意力和掩码引导运动模块，实现了对主题学习和运动控制的平衡。研究发现，运动控制往往会压倒主题学习，因此提出了掩码参考注意力和重加权扩散损失来解决这一问题。实验结果表明，DreamVideo-2在主题定制和运动控制方面优于现有方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11842', 'title': 'MoH: Multi-Head Attention as Mixture-of-Head Attention', 'url': 'https://huggingface.co/papers/2410.11842', 'abstract': 'In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%-90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.', 'score': 20, 'issue_id': 148, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '4a94e557d3f7a79a', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#transfer_learning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Смешивание голов внимания для повышения эффективности трансформеров', 'desc': 'Исследователи предлагают новый механизм внимания Mixture-of-Head (MoH), который улучшает эффективность многоголового внимания в трансформерах. MoH позволяет каждому токену выбирать подходящие головы внимания, повышая эффективность вывода без ущерба для точности. Эксперименты на ViT, DiT и языковых моделях показывают, что MoH превосходит стандартное многоголовое внимание, используя лишь 50-90% голов. Продолжительная настройка предобученных моделей, таких как LLaMA3-8B, с использованием MoH также демонстрирует значительное улучшение производительности.'}, 'en': {'title': '"MoH: Elevating Attention Efficiency and Accuracy"', 'desc': 'This paper introduces Mixture-of-Head attention (MoH), an enhancement to the multi-head attention mechanism in Transformer models, aimed at improving efficiency and accuracy. MoH treats attention heads as experts, allowing each token to select the most relevant heads, which boosts inference efficiency without increasing parameters. By replacing the standard summation with a weighted summation, MoH adds flexibility and unlocks additional performance potential. Experiments show that MoH outperforms traditional multi-head attention, achieving higher accuracy with fewer attention heads, and can be applied to pre-trained models like LLaMA3-8B for further improvements.'}, 'zh': {'title': '混合头注意力：高效的Transformer新选择', 'desc': '这项研究改进了Transformer模型中的多头注意力机制，提高了效率，同时保持或超过了之前的准确性。研究表明，多头注意力可以用求和形式表示，并提出了混合头注意力（MoH）架构，将注意力头视为专家。MoH允许每个标记选择合适的注意力头，提高推理效率而不增加参数数量。实验表明，MoH在使用较少注意力头的情况下，性能优于传统多头注意力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13804', 'title': 'BenTo: Benchmark Task Reduction with In-Context Transferability', 'url': 'https://huggingface.co/papers/2410.13804', 'abstract': 'Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on a large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals that task transferability and relevance provide critical information to identify the most representative subset of tasks via optimizing a facility location function. We propose a practically efficient metric for estimating the transferability between two tasks via in-context learning (ICL). By analyzing the pairwise transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or FLAN) to 5% while inducing only a <4% difference to the evaluation on the original benchmark. Compared to prior works, our method is training-free, gradient-free, and highly efficient requiring ICL only.', 'score': 20, 'issue_id': 147, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'e8177fd577296e7e', 'data': {'categories': ['#optimization', '#training', '#transfer_learning', '#benchmark'], 'emoji': '🎯', 'ru': {'title': 'Эффективная оценка языковых моделей: меньше задач, та же точность', 'desc': 'Статья исследует методы эффективного сокращения количества задач для оценки больших языковых моделей (LLM) без ущерба для качества оценки. Авторы предлагают метрику для оценки переносимости между задачами с помощью обучения в контексте (ICL). Анализируя попарную переносимость, можно сократить набор задач в современных бенчмарках LLM до 5% с разницей менее 4% по сравнению с оценкой на полном наборе. Метод не требует дополнительного обучения и градиентов, что делает его высокоэффективным.'}, 'en': {'title': 'Efficient LLM Evaluation: Less is More', 'desc': "This paper explores a method to make evaluating large language models (LLMs) more efficient by reducing the number of tasks needed for benchmarking. It introduces a way to identify the most important tasks using task transferability and relevance, optimizing a facility location function. The authors propose a metric to estimate how well tasks transfer to each other using in-context learning, which helps in selecting a smaller set of tasks. Their approach can cut down the tasks in benchmarks like MMLU or FLAN to just 5% of the original, with minimal impact on evaluation quality, and it doesn't require any training or gradients."}, 'zh': {'title': '高效评估：减少任务，保持质量', 'desc': '这篇论文研究如何在不影响评估质量的情况下，减少用于评估大型语言模型的任务数量。研究表明，任务的可转移性和相关性是识别最具代表性任务子集的关键。通过优化设施位置函数，我们提出了一种高效的指标来估计任务之间的可转移性。分析任务间的可转移性，可以将现代LLM基准中的任务减少到5%，而评估结果仅有不到4%的差异。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13085', 'title': 'MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models', 'url': 'https://huggingface.co/papers/2410.13085', 'abstract': 'Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection method, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available in https://github.com/richard-peng-xia/MMed-RAG.', 'score': 20, 'issue_id': 146, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '8ef96c4ea4d54ffd', 'data': {'categories': ['#rag', '#hallucinations', '#benchmark', '#cv', '#multimodal', '#healthcare', '#data', '#training', '#dataset', '#open_source', '#alignment'], 'emoji': '🏥', 'ru': {'title': 'MMed-RAG: Повышение точности медицинских AI-диагнозов', 'desc': 'В статье представлена система MMed-RAG, разработанная для повышения фактической точности медицинских крупномасштабных моделей зрения и языка (Med-LVLMs). Система использует механизм поиска с учетом домена, адаптивный метод выбора найденных контекстов и стратегию дообучения на основе RAG. Эксперименты на пяти медицинских наборах данных показали среднее улучшение фактической точности Med-LVLMs на 43.8%. MMed-RAG решает проблемы галлюцинаций и несоответствий, часто встречающихся в существующих моделях.'}, 'en': {'title': 'Enhancing Medical AI: MMed-RAG Boosts Diagnostic Accuracy', 'desc': "The paper discusses the development of MMed-RAG, a new system designed to improve the accuracy of Medical Large Vision-Language Models (Med-LVLMs) by addressing issues of factual hallucination. MMed-RAG uses a domain-aware retrieval mechanism and an adaptive context selection method to enhance the reliability of retrieval-augmented generation (RAG) processes. This approach ensures better alignment between the model's outputs and the ground truth across various medical domains. Experimental results show that MMed-RAG significantly boosts the factual accuracy of Med-LVLMs by 43.8% on average across multiple medical datasets."}, 'zh': {'title': 'MMed-RAG：提升医学AI模型准确性的多模态解决方案', 'desc': '这篇论文介绍了一种新的多模态检索增强生成系统，称为MMed-RAG，旨在提高医学大规模视觉语言模型的准确性。该系统通过引入领域感知的检索机制、自适应的检索上下文选择方法，以及可证明的偏好微调策略来增强模型的可靠性。实验结果表明，MMed-RAG在五个医学数据集上的表现显著提高了43.8%的事实准确性。此研究为医学诊断工具的开发提供了新的思路和方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13785', 'title': 'PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment', 'url': 'https://huggingface.co/papers/2410.13785', 'abstract': 'Alignment of large language models (LLMs) involves training models on preference-contrastive output pairs to adjust their responses according to human preferences. To obtain such contrastive pairs, traditional methods like RLHF and RLAIF rely on limited contrasting patterns, such as varying model variants or decoding temperatures. This singularity leads to two issues: (1) alignment is not comprehensive; and thereby (2) models are susceptible to jailbreaking attacks. To address these issues, we investigate how to construct more comprehensive and diversified contrasting patterns to enhance preference data (RQ1) and verify the impact of the diversification of contrasting patterns on model alignment (RQ2). For RQ1, we propose PopAlign, a framework that integrates diversified contrasting patterns across the prompt, model, and pipeline levels, introducing six contrasting strategies that do not require additional feedback labeling procedures. Regarding RQ2, we conduct thorough experiments demonstrating that PopAlign significantly outperforms existing methods, leading to more comprehensive alignment.', 'score': 18, 'issue_id': 147, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'd458841995668004', 'data': {'categories': ['#rlhf', '#training', '#security', '#architecture', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'PopAlign: Комплексное выравнивание языковых моделей через разнообразные контрастные паттерны', 'desc': 'Статья представляет новый подход к выравниванию больших языковых моделей под названием PopAlign. Этот метод использует разнообразные контрастные паттерны на уровнях промпта, модели и пайплайна для улучшения данных о предпочтениях. PopAlign решает проблемы ограниченности традиционных методов, таких как RLHF и RLAIF, и повышает устойчивость моделей к атакам типа jailbreaking. Эксперименты показывают, что PopAlign значительно превосходит существующие методы, обеспечивая более комплексное выравнивание.'}, 'en': {'title': 'PopAlign: Diversifying Patterns for Better Model Alignment', 'desc': 'The paper discusses improving the alignment of large language models (LLMs) by using a new framework called PopAlign. Traditional methods like RLHF and RLAIF have limitations due to their reliance on limited contrasting patterns, which can make models vulnerable to jailbreaking attacks. PopAlign introduces diversified contrasting patterns across different levels, such as prompt, model, and pipeline, without needing extra feedback labeling. Experiments show that PopAlign enhances model alignment more effectively than existing methods, making models more robust and comprehensive.'}, 'zh': {'title': 'PopAlign：多样化对比策略提升模型对齐', 'desc': '这篇论文研究了如何通过对比模式来更好地调整大型语言模型的输出，使其更符合人类的偏好。传统方法如RLHF和RLAIF在对比模式上存在局限性，导致模型对攻击的脆弱性。为了解决这个问题，作者提出了PopAlign框架，通过在提示、模型和流程层面引入多样化的对比策略来增强偏好数据。实验结果表明，PopAlign显著优于现有方法，实现了更全面的模型对齐。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13639', 'title': "A Comparative Study on Reasoning Patterns of OpenAI's o1 Model", 'url': 'https://huggingface.co/papers/2410.13639', 'abstract': "Enabling Large Language Models (LLMs) to handle a wider range of complex tasks (e.g., coding, math) has drawn great attention from many researchers. As LLMs continue to evolve, merely increasing the number of model parameters yields diminishing performance improvements and heavy computational costs. Recently, OpenAI's o1 model has shown that inference strategies (i.e., Test-time Compute methods) can also significantly enhance the reasoning capabilities of LLMs. However, the mechanisms behind these methods are still unexplored. In our work, to investigate the reasoning patterns of o1, we compare o1 with existing Test-time Compute methods (BoN, Step-wise BoN, Agent Workflow, and Self-Refine) by using OpenAI's GPT-4o as a backbone on general reasoning benchmarks in three domains (i.e., math, coding, commonsense reasoning). Specifically, first, our experiments show that the o1 model has achieved the best performance on most datasets. Second, as for the methods of searching diverse responses (e.g., BoN), we find the reward models' capability and the search space both limit the upper boundary of these methods. Third, as for the methods that break the problem into many sub-problems, the Agent Workflow has achieved better performance than Step-wise BoN due to the domain-specific system prompt for planning better reasoning processes. Fourth, it is worth mentioning that we have summarized six reasoning patterns of o1, and provided a detailed analysis on several reasoning benchmarks.", 'score': 15, 'issue_id': 162, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '39b4ff88e70ccaf2', 'data': {'categories': ['#reasoning', '#rl', '#benchmark', '#inference', '#optimization', '#math', '#plp'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие секретов рассуждений языковых моделей', 'desc': 'Статья исследует механизмы улучшения рассуждений в крупных языковых моделях (LLM) с помощью стратегий вывода. Авторы сравнивают модель o1 от OpenAI с другими методами вычислений во время тестирования на задачах рассуждения в областях математики, программирования и здравого смысла. Исследование выявило шесть паттернов рассуждений модели o1 и проанализировало их эффективность. Результаты показывают, что o1 достигает лучших результатов на большинстве наборов данных, а также демонстрируют ограничения и преимущества различных методов улучшения рассуждений в LLM.'}, 'en': {'title': 'Smarter, Not Bigger: Enhancing LLMs with Inference Strategies', 'desc': "This paper explores how Large Language Models (LLMs) can be improved not just by adding more parameters, but by using smarter inference strategies. The study focuses on OpenAI's o1 model, comparing it with other Test-time Compute methods to understand its reasoning capabilities. The research finds that o1 outperforms other models in reasoning tasks across math, coding, and commonsense domains. It also identifies six reasoning patterns in o1, providing insights into how these models can be optimized for complex tasks."}, 'zh': {'title': '探索大型语言模型的推理新策略', 'desc': '这篇论文研究了如何让大型语言模型（LLM）更好地处理复杂任务，如编程和数学。研究发现，仅仅增加模型参数的数量并不能显著提高性能，反而会增加计算成本。通过比较不同的推理策略，发现o1模型在大多数数据集上表现最佳，并总结了六种推理模式。研究还指出，搜索多样化响应的方法和将问题分解为子问题的方法在性能上有不同的限制和优势。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13841', 'title': 'A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models', 'url': 'https://huggingface.co/papers/2410.13841', 'abstract': 'Post-training has emerged as a crucial paradigm for adapting large-scale pre-trained models to various tasks, whose effects are fully reflected by delta parameters (i.e., the disparity between post-trained and pre-trained parameters). While numerous studies have explored delta parameter properties via operations like pruning, quantization, low-rank approximation, and extrapolation, a unified framework for systematically examining these characteristics has been lacking. In this paper, we propose a novel perspective based on Riemann sum approximation of the loss function to elucidate delta parameter editing operations. Our analysis categorizes existing methods into three classes based on their post-editing performance: competitive, decreased, and improved, explaining how they are expressed by the Riemann sum approximation term and how they alter the model performance. Extensive experiments on both visual and language models, including ViT, LLaMA 3, Qwen 2, and Mistral, corroborate our theoretical findings. Furthermore, we introduce extensions to existing techniques like DARE and BitDelta, highlighting their limitations in leveraging the properties of delta parameters and reorganizing them into general expressions to enhance the applicability and effectiveness of delta parameter editing in post-trained models.', 'score': 14, 'issue_id': 146, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'd5678be0144ffffb', 'data': {'categories': ['#cv', '#inference', '#optimization', '#training', '#transfer_learning', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Новый взгляд на дельта-параметры: аппроксимация Римана в пост-обучении', 'desc': 'Статья представляет новый подход к анализу дельта-параметров в пост-обучении моделей машинного обучения, основанный на аппроксимации Римана функции потерь. Авторы классифицируют существующие методы редактирования дельта-параметров на три категории в зависимости от их влияния на производительность модели. Исследование включает эксперименты с визуальными и языковыми моделями, такими как ViT, LLaMA 3, Qwen 2 и Mistral. Предложенный подход позволяет улучшить применимость и эффективность редактирования дельта-параметров в пост-обученных моделях.'}, 'en': {'title': 'Unlocking the Power of Delta Parameters in AI Models', 'desc': "The paper introduces a new way to understand how changes made to large pre-trained models, called delta parameters, affect their performance. By using a mathematical tool called Riemann sum approximation, the authors categorize different methods of editing these parameters into three groups based on how they impact the model's performance. They test their ideas on various models, showing that their approach helps explain why some methods work better than others. Additionally, they suggest improvements to existing techniques to make them more effective in adjusting these delta parameters."}, 'zh': {'title': '揭示后训练模型中delta参数的奥秘', 'desc': '这篇论文提出了一种基于黎曼和近似损失函数的新视角来解释后训练模型中的delta参数编辑操作。研究将现有方法根据编辑后性能分为三类：竞争性、下降和提升，并通过黎曼和近似项解释这些方法如何影响模型性能。通过在视觉和语言模型上的大量实验，验证了理论发现的正确性。此外，论文还对现有技术如DARE和BitDelta进行了扩展，指出其在利用delta参数特性上的局限性，并将其重新组织为通用表达式以提高后训练模型中delta参数编辑的适用性和有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13334', 'title': 'Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems', 'url': 'https://huggingface.co/papers/2410.13334', 'abstract': "Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as `jailbreaks', where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety measures to align these models. This alignment involves several techniques, including data filtering during pre-training, supervised fine-tuning, reinforcement learning from human feedback, and red-teaming exercises. These methods often introduce deliberate and intentional biases similar to Political Correctness (PC) to ensure the ethical behavior of LLMs. In this paper, we delve into the intentional biases injected into LLMs for safety purposes and examine methods to circumvent these safety alignment techniques. Notably, these intentional biases result in a jailbreaking success rate in GPT-4o models that differs by 20% between non-binary and cisgender keywords and by 16% between white and black keywords, even when the other parts of the prompts are identical. We introduce the concept of PCJailbreak, highlighting the inherent risks posed by these safety-induced biases. Additionally, we propose an efficient defense method PCDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation. PCDefense stands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. Our findings emphasize the urgent need for LLM developers to adopt a more responsible approach when designing and implementing safety measures.", 'score': 12, 'issue_id': 149, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'cc9053b7f6f516ca', 'data': {'categories': ['#rlhf', '#inference', '#ethics', '#security', '#architecture', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Балансируя на грани: безопасность и этика в больших языковых моделях', 'desc': 'Статья исследует проблемы безопасности больших языковых моделей (LLM) и методы их обхода. Авторы вводят понятие PCJailbreak, демонстрирующее риски, связанные с намеренными смещениями, внедренными в LLM для обеспечения безопасности. Исследование показывает значительную разницу в успешности взлома моделей GPT-4 при использовании различных ключевых слов. Предложен метод защиты PCDefense, предотвращающий попытки взлома путем внедрения защитных промптов перед генерацией текста.'}, 'en': {'title': 'Balancing Safety and Bias: Navigating the Ethical Landscape of LLMs', 'desc': "This paper explores the safety risks associated with large language models (LLMs), particularly focusing on 'jailbreaks' where malicious inputs lead to harmful outputs. It discusses how developers use techniques like data filtering, supervised fine-tuning, and reinforcement learning from human feedback to align LLMs with ethical standards, often introducing biases similar to Political Correctness. The study reveals that these biases can affect the success rate of jailbreaks, showing significant differences based on keywords related to gender and race. To counteract these vulnerabilities, the paper introduces PCDefense, a method that injects defense prompts to prevent jailbreaks without the additional costs associated with other guard models."}, 'zh': {'title': '确保大型语言模型的安全与道德行为', 'desc': '大型语言模型（LLMs）在执行各种任务时表现出色，但也存在安全风险，如通过恶意输入诱导生成有害内容。为解决这些问题，开发者采用了数据过滤、监督微调和人类反馈强化学习等方法来对齐模型。这些方法引入了类似政治正确性的偏见，以确保模型的道德行为。本文探讨了这些偏见的影响，并提出了一种名为PCDefense的防御方法，以防止模型被破解。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13832', 'title': 'VidPanos: Generative Panoramic Videos from Casual Panning Videos', 'url': 'https://huggingface.co/papers/2410.13832', 'abstract': "Panoramic image stitching provides a unified, wide-angle view of a scene that extends beyond the camera's field of view. Stitching frames of a panning video into a panoramic photograph is a well-understood problem for stationary scenes, but when objects are moving, a still panorama cannot capture the scene. We present a method for synthesizing a panoramic video from a casually-captured panning video, as if the original video were captured with a wide-angle camera. We pose panorama synthesis as a space-time outpainting problem, where we aim to create a full panoramic video of the same length as the input video. Consistent completion of the space-time volume requires a powerful, realistic prior over video content and motion, for which we adapt generative video models. Existing generative models do not, however, immediately extend to panorama completion, as we show. We instead apply video generation as a component of our panorama synthesis system, and demonstrate how to exploit the strengths of the models while minimizing their limitations. Our system can create video panoramas for a range of in-the-wild scenes including people, vehicles, and flowing water, as well as stationary background features.", 'score': 12, 'issue_id': 147, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'bbef29ff3d52bc13', 'data': {'categories': ['#diffusion', '#cv', '#video', '#games', '#3d', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Панорамное видео: от обычной съемки к широкоугольному обзору', 'desc': 'Статья представляет метод синтеза панорамного видео из обычного панорамного видео, снятого вручную. Авторы рассматривают задачу как проблему заполнения пространственно-временного объема, используя генеративные модели видео. Предложенный подход позволяет создавать видеопанорамы для различных сцен, включая движущиеся объекты и людей. Система преодолевает ограничения существующих генеративных моделей для эффективного синтеза панорамных видео.'}, 'en': {'title': 'Expanding Horizons: Creating Panoramic Videos from Panning Footage', 'desc': 'The paper introduces a novel method for creating panoramic videos from panning video footage, even when the scene includes moving objects. This is achieved by treating the problem as a space-time outpainting task, where the goal is to generate a complete panoramic video that matches the length of the original footage. The authors adapt generative video models to handle the complex task of filling in the missing parts of the video, ensuring realistic motion and content. They demonstrate the effectiveness of their system across various dynamic scenes, overcoming the limitations of existing generative models.'}, 'zh': {'title': '动态场景的全景视频合成新方法', 'desc': '这篇论文介绍了一种从普通视频合成全景视频的方法，解决了传统全景照片无法捕捉动态场景的问题。作者将全景合成视为一个时空外推问题，利用生成视频模型来实现视频内容和运动的逼真补全。现有的生成模型不能直接用于全景补全，因此作者将其作为全景合成系统的一部分，发挥模型的优势并减少其局限性。该系统能够为包括人、车辆和流水等动态场景生成全景视频。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09426', 'title': 'FlatQuant: Flatness Matters for LLM Quantization', 'url': 'https://huggingface.co/papers/2410.09426', 'abstract': 'Recently, quantization has been widely used for the compression and acceleration of large language models~(LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still remain steep and outspread. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach to enhance flatness of weights and activations. Our approach identifies optimal affine transformations tailored to each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead, we apply Kronecker decomposition to the transformation matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments show that FlatQuant sets up a new state-of-the-art quantization benchmark. For instance, it achieves less than 1% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5%. For inference latency, FlatQuant reduces the slowdown induced by pre-quantization transformation from 0.26x of QuaRot to merely 0.07x, bringing up to 2.3x speedup for prefill and 1.7x speedup for decoding, respectively. Code is available at: https://github.com/ruikangliu/FlatQuant.', 'score': 12, 'issue_id': 146, 'pub_date': '2024-10-12', 'pub_date_card': {'ru': '12 октября', 'en': 'October 12', 'zh': '10月12日'}, 'hash': 'edfd47f3735a6a6e', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#open_source', '#architecture'], 'emoji': '🔢', 'ru': {'title': 'FlatQuant: Быстрое и обучаемое квантование для ускорения больших языковых моделей', 'desc': 'FlatQuant - это новый подход к квантованию после обучения, направленный на улучшение сглаженности весов и активаций в больших языковых моделях (LLM). Метод использует оптимальные аффинные преобразования для каждого линейного слоя, калибруемые за часы с помощью облегченной целевой функции. Для снижения накладных расходов во время выполнения применяется разложение Кронекера к матрицам преобразования. FlatQuant устанавливает новый эталон квантования, достигая менее 1% падения точности для квантования W4A4 на модели LLaMA-3-70B.'}, 'en': {'title': 'FlatQuant: Revolutionizing Model Quantization with Speed and Precision', 'desc': 'The paper introduces FlatQuant, a novel post-training quantization method designed to improve the flatness of weights and activations in large language models. By using learnable affine transformations tailored to each linear layer, FlatQuant minimizes quantization errors and enhances model performance. The approach employs Kronecker decomposition to streamline operations, significantly reducing runtime overhead and improving inference speed. Extensive testing demonstrates that FlatQuant achieves superior accuracy and speed compared to existing methods, setting a new benchmark in model quantization.'}, 'zh': {'title': 'FlatQuant：提升量化精度与速度的新方法', 'desc': '量化技术被广泛用于压缩和加速大型语言模型，但由于异常值的存在，量化误差较大。为了解决这个问题，本文提出了一种新的后训练量化方法FlatQuant，通过快速可学习的仿射变换来增强权重和激活的平坦性。FlatQuant通过克罗内克分解减少运行时开销，并将所有操作融合到一个内核中。实验表明，FlatQuant在量化基准上达到了新的水平，显著提高了模型的准确性和推理速度。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13198', 'title': 'Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation', 'url': 'https://huggingface.co/papers/2410.13198', 'abstract': 'Generative Error Correction (GEC) has emerged as a powerful post-processing method to enhance the performance of Automatic Speech Recognition (ASR) systems. However, we show that GEC models struggle to generalize beyond the specific types of errors encountered during training, limiting their ability to correct new, unseen errors at test time, particularly in out-of-domain (OOD) scenarios. This phenomenon amplifies with named entities (NEs), where, in addition to insufficient contextual information or knowledge about the NEs, novel NEs keep emerging. To address these issues, we propose DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach designed to improve GEC for ASR in in-domain (ID) and OOD scenarios. We augment the GEC training dataset with synthetic data generated by prompting LLMs and text-to-speech models, thereby simulating additional errors from which the model can learn. For OOD scenarios, we simulate test-time errors from new domains similarly and in an unsupervised fashion. Additionally, to better handle named entities, we introduce retrieval-augmented correction by augmenting the input with entities retrieved from a database. Our approach is simple, scalable, and both domain- and language-agnostic. We experiment on multiple datasets and settings, showing that DARAG outperforms all our baselines, achieving 8\\% -- 30\\% relative WER improvements in ID and 10\\% -- 33\\% improvements in OOD settings.', 'score': 9, 'issue_id': 146, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '0385e3248b804521', 'data': {'categories': ['#rag', '#synthetic', '#multilingual', '#data', '#dataset', '#transfer_learning', '#games', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'DARAG: Улучшение коррекции ошибок ASR с помощью синтетических данных и извлечения информации', 'desc': 'Статья представляет новый подход DARAG для улучшения генеративной коррекции ошибок (GEC) в системах автоматического распознавания речи (ASR). DARAG использует синтетические данные, созданные с помощью языковых моделей и систем text-to-speech, для расширения обучающего набора данных. Метод также включает коррекцию с использованием извлечения информации для лучшей обработки именованных сущностей. DARAG показывает значительные улучшения в сравнении с базовыми моделями как для доменных, так и для внедоменных сценариев.'}, 'en': {'title': 'DARAG: Revolutionizing Error Correction in Speech Recognition', 'desc': "The paper discusses a new method called DARAG, which enhances Generative Error Correction (GEC) for Automatic Speech Recognition (ASR) systems. Traditional GEC models struggle with unseen errors, especially in out-of-domain scenarios, but DARAG addresses this by using synthetic data and retrieval-augmented correction. By generating additional training data and retrieving relevant named entities, DARAG improves the model's ability to correct errors in both familiar and new contexts. Experiments show that DARAG significantly reduces word error rates, making it a robust solution for diverse ASR applications."}, 'zh': {'title': 'DARAG：提升语音识别错误校正的新方法', 'desc': '本文介绍了一种名为DARAG的新方法，用于改进自动语音识别系统的生成式错误校正。传统的生成式错误校正模型在面对未见过的错误时表现不佳，尤其是在跨领域场景中。DARAG通过使用大语言模型和文本到语音模型生成的合成数据来增强训练数据集，从而提高模型的泛化能力。此外，为了更好地处理命名实体，DARAG引入了检索增强校正，通过从数据库中检索实体来丰富输入信息。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13854', 'title': 'Can MLLMs Understand the Deep Implication Behind Chinese Images?', 'url': 'https://huggingface.co/papers/2410.13854', 'abstract': "As the capabilities of Multimodal Large Language Models (MLLMs) continue to improve, the need for higher-order capability evaluation of MLLMs is increasing. However, there is a lack of work evaluating MLLM for higher-order perception and understanding of Chinese visual content. To fill the gap, we introduce the **C**hinese **I**mage **I**mplication understanding **Bench**mark, **CII-Bench**, which aims to assess the higher-order perception and understanding capabilities of MLLMs for Chinese images. CII-Bench stands out in several ways compared to existing benchmarks. Firstly, to ensure the authenticity of the Chinese context, images in CII-Bench are sourced from the Chinese Internet and manually reviewed, with corresponding answers also manually crafted. Additionally, CII-Bench incorporates images that represent Chinese traditional culture, such as famous Chinese traditional paintings, which can deeply reflect the model's understanding of Chinese traditional culture. Through extensive experiments on CII-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on CII-Bench. The highest accuracy of MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an impressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional culture images, suggesting limitations in their ability to understand high-level semantics and lack a deep knowledge base of Chinese traditional culture. Finally, it is observed that most models exhibit enhanced accuracy when image emotion hints are incorporated into the prompts. We believe that CII-Bench will enable MLLMs to gain a better understanding of Chinese semantics and Chinese-specific images, advancing the journey towards expert artificial general intelligence (AGI). Our project is publicly available at https://cii-bench.github.io/.", 'score': 8, 'issue_id': 150, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'bea0da184db20058', 'data': {'categories': ['#science', '#benchmark', '#multilingual', '#cv', '#agi', '#multimodal', '#open_source'], 'emoji': '🇨🇳', 'ru': {'title': 'CII-Bench: новый рубеж в понимании китайских изображений для MLLM', 'desc': 'CII-Bench - это новый бенчмарк для оценки способностей мультимодальных языковых моделей (MLLM) к восприятию и пониманию китайских изображений высокого уровня. Он включает аутентичные китайские изображения, в том числе отражающие традиционную культуру. Эксперименты показали значительный разрыв между производительностью MLLM и людей на CII-Bench, особенно в понимании традиционной китайской культуры. Авторы отмечают, что включение эмоциональных подсказок в промпты улучшает точность моделей.'}, 'en': {'title': 'Bridging the Cultural Gap in AI Understanding', 'desc': "The paper introduces CII-Bench, a benchmark designed to evaluate the higher-order perception and understanding capabilities of Multimodal Large Language Models (MLLMs) for Chinese visual content. CII-Bench uses images sourced from the Chinese Internet and includes traditional cultural elements to test the models' understanding of Chinese culture. Experiments reveal that MLLMs perform significantly worse than humans, especially on images related to Chinese traditional culture, indicating a gap in high-level semantic understanding. The study also finds that MLLMs improve in accuracy when emotional cues are included in prompts, suggesting a potential area for enhancing model performance."}, 'zh': {'title': '提升多模态大语言模型的中文图像理解能力', 'desc': '这篇论文介绍了一个名为CII-Bench的基准，用于评估多模态大语言模型对中文图像的高级感知和理解能力。CII-Bench的图像来自中国互联网，并经过人工审核，确保了中文语境的真实性。实验结果显示，多模态大语言模型在理解中国传统文化图像时表现较差，表明其在高层次语义理解和文化知识方面的不足。通过CII-Bench的测试，可以帮助这些模型更好地理解中文语义和特定的中文图像。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13360', 'title': 'Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant', 'url': 'https://huggingface.co/papers/2410.13360', 'abstract': "The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce the Retrieval Augmented Personalization (RAP) framework for MLLMs' personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information, e.g., user's name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts' information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models are available at https://github.com/Hoar012/RAP-MLLM.", 'score': 8, 'issue_id': 150, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '2355c49dae930058', 'data': {'categories': ['#rag', '#synthetic', '#agi', '#multimodal', '#data', '#training', '#dataset', '#open_source', '#alignment'], 'emoji': '👤', 'ru': {'title': 'Персонализация мультимодальных ИИ-ассистентов с помощью RAP', 'desc': 'Статья представляет фреймворк Retrieval Augmented Personalization (RAP) для персонализации мультимодальных языковых моделей (MLLM). RAP использует базу данных ключ-значение для хранения информации о пользователе, извлекает релевантные данные с помощью мультимодального ретривера и генерирует персонализированные ответы. Авторы создали специализированный датасет для обучения MLLM и разработали серию моделей, способных обобщаться на бесконечное количество визуальных концепций без дополнительной настройки. Модели демонстрируют высокую гибкость и качество генерации в различных задачах, включая персонализированное описание изображений и визуальное распознавание.'}, 'en': {'title': 'Personalized AI: Tailoring Multimodal Models to You', 'desc': "The paper introduces the Retrieval Augmented Personalization (RAP) framework to enhance the personalization of multimodal large language models (MLLMs). RAP uses a key-value database to store user-specific information, which is retrieved during interactions to generate personalized responses. This approach allows for real-time updates and editing of user data, improving the model's ability to align with user-specific needs. The framework is trained on a specialized dataset, enabling the models to perform tasks like personalized image captioning and question answering with high flexibility and quality."}, 'zh': {'title': 'RAP：个性化多模态助手的未来', 'desc': '这篇论文介绍了一种名为RAP的框架，用于个性化多模态大语言模型（MLLMs）。RAP通过记忆、检索和生成三个步骤，将通用的MLLM转变为个性化助手。它使用一个键值数据库存储用户信息，并在对话时检索相关信息以生成个性化响应。RAP的创新在于允许实时编辑概念，并通过专门的数据集提高生成质量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09019', 'title': 'MedMobile: A mobile-sized language model with expert-level clinical capabilities', 'url': 'https://huggingface.co/papers/2410.09019', 'abstract': 'Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. We introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of running on a mobile device, for medical applications. We demonstrate that MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for physicians (~60%), and approaching the scores of models 100 times its size. We subsequently perform a careful set of ablations, and demonstrate that chain of thought, ensembling, and fine-tuning lead to the greatest performance gains, while unexpectedly retrieval augmented generation fails to demonstrate significant improvements', 'score': 8, 'issue_id': 147, 'pub_date': '2024-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': 'dcae17e6b8ab798e', 'data': {'categories': ['#science', '#rag', '#small_models', '#reasoning', '#optimization', '#healthcare', '#training'], 'emoji': '🩺', 'ru': {'title': 'Компактная медицинская ИИ-модель превосходит врачей в тестировании', 'desc': 'Статья представляет MedMobile - компактную языковую модель для медицинских приложений, способную работать на мобильных устройствах. Модель, основанная на phi-3-mini, имеет всего 3,8 миллиарда параметров, но демонстрирует впечатляющие результаты на тесте MedQA (USMLE), превосходя проходной балл для врачей. Авторы проводят тщательный анализ, показывая, что цепочка рассуждений, ансамблирование и дообучение приводят к наибольшему улучшению производительности. Интересно, что retrieval augmented generation не показывает значительных улучшений.'}, 'en': {'title': 'MedMobile: Compact Power for Medical AI on the Go', 'desc': "The paper introduces MedMobile, a compact language model designed for medical applications that can operate on mobile devices, addressing issues of computational cost and privacy. MedMobile, with 3.8 billion parameters, achieves a 75.7% score on the MedQA exam, surpassing the physician passing mark and nearing the performance of much larger models. The study finds that techniques like chain of thought, ensembling, and fine-tuning significantly enhance the model's performance. Interestingly, retrieval augmented generation does not provide notable improvements, contrary to expectations."}, 'zh': {'title': '小而强大的医学语言模型：MedMobile', 'desc': '这篇论文介绍了一种名为MedMobile的语言模型，它能够在移动设备上运行，并在医学应用中表现出色。MedMobile在MedQA测试中得分75.7%，超过了医生的及格线，并接近比它大100倍的模型的得分。研究发现，思维链、集成和微调是提高性能的关键，而检索增强生成未能显著改善结果。该模型的设计旨在降低计算成本和解决隐私问题，使其更易于广泛应用。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13859', 'title': '$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2410.13859', 'abstract': "Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of ``activated tokens''. Our key insight is that if most tokens are redundant for the layer computation, then can be skipped directly via the MoD layer. However, directly converting the dense layers of MLLMs to MoD layers leads to substantial performance degradation. To address this issue, we propose an innovative MoD adaptation strategy for existing MLLMs called gamma-MoD. In gamma-MoD, a novel metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of attention maps (ARank). Through ARank, we can effectively identify which layer is redundant and should be replaced with the MoD layer. Based on ARank, we further propose two novel designs to maximize the computational sparsity of MLLM while maintaining its performance, namely shared vision-language router and masked routing learning. With these designs, more than 90% dense layers of the MLLM can be effectively converted to the MoD ones. To validate our method, we apply it to three popular MLLMs, and conduct extensive experiments on 9 benchmark datasets. Experimental results not only validate the significant efficiency benefit of gamma-MoD to existing MLLMs but also confirm its generalization ability on various MLLMs. For example, with a minor performance drop, i.e., -1.5%, gamma-MoD can reduce the training and inference time of LLaVA-HR by 31.0% and 53.2%, respectively.", 'score': 7, 'issue_id': 150, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'f393a253d43ee89d', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#multimodal', '#training', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Ускорение мультимодальных LLM без потери качества', 'desc': 'Статья представляет новый метод gamma-MoD для оптимизации мультимодальных больших языковых моделей (MLLM). Авторы предлагают стратегию адаптации Mixture of Depths (MoD) для существующих MLLM, используя метрику ранга карт внимания (ARank) для определения избыточных слоев. Метод включает общий маршрутизатор для зрения и языка, а также маскированное обучение маршрутизации. Эксперименты на 9 наборах данных показывают значительное повышение эффективности MLLM при минимальном снижении производительности.'}, 'en': {'title': 'Efficient Multimodal Models: Cutting Costs, Not Performance', 'desc': 'The paper addresses the high computational cost of multimodal large language models (MLLMs) by introducing a method called gamma-MoD, inspired by the mixture of depths (MoDs) concept. Gamma-MoD uses a novel metric, ARank, to identify and replace redundant layers in MLLMs with more efficient MoD layers, significantly reducing computational demands. The approach includes innovative designs like shared vision-language router and masked routing learning to maintain model performance while achieving computational sparsity. Experiments on various MLLMs demonstrate that gamma-MoD can reduce training and inference times substantially with minimal performance loss.'}, 'zh': {'title': 'gamma-MoD：提升多模态大语言模型效率的新策略', 'desc': '这篇论文探讨了多模态大语言模型（MLLMs）的高计算成本问题，并提出了一种名为gamma-MoD的创新策略。通过引入注意力图的秩（ARank）作为指标，gamma-MoD能够识别哪些层是冗余的，并用混合深度（MoD）层替换。该方法通过共享视觉-语言路由器和掩码路由学习设计，成功将超过90%的密集层转换为MoD层。实验结果表明，gamma-MoD在保持性能的同时，大幅提高了MLLMs的计算效率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12957', 'title': 'MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization', 'url': 'https://huggingface.co/papers/2410.12957', 'abstract': "Generating music that aligns with the visual content of a video has been a challenging task, as it requires a deep understanding of visual semantics and involves generating music whose melody, rhythm, and dynamics harmonize with the visual narratives. This paper presents MuVi, a novel framework that effectively addresses these challenges to enhance the cohesion and immersive experience of audio-visual content. MuVi analyzes video content through a specially designed visual adaptor to extract contextually and temporally relevant features. These features are used to generate music that not only matches the video's mood and theme but also its rhythm and pacing. We also introduce a contrastive music-visual pre-training scheme to ensure synchronization, based on the periodicity nature of music phrases. In addition, we demonstrate that our flow-matching-based music generator has in-context learning ability, allowing us to control the style and genre of the generated music. Experimental results show that MuVi demonstrates superior performance in both audio quality and temporal synchronization. The generated music video samples are available at https://muvi-v2m.github.io.", 'score': 7, 'issue_id': 149, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '6540fab2fcc644f2', 'data': {'categories': ['#diffusion', '#synthetic', '#video', '#multimodal', '#training', '#transfer_learning', '#games', '#audio', '#architecture'], 'emoji': '🎵', 'ru': {'title': 'MuVi: Гармония видео и музыки через искусственный интеллект', 'desc': 'MuVi - это новая система для создания музыки, синхронизированной с видеоконтентом. Она анализирует видео с помощью специального визуального адаптера для извлечения релевантных характеристик. Затем эти характеристики используются для генерации музыки, соответствующей настроению, теме, ритму и темпу видео. Система также включает контрастивное предобучение для обеспечения синхронизации и генератор музыки на основе согласования потоков с возможностью обучения в контексте.'}, 'en': {'title': "Harmonizing Visuals and Sound: MuVi's Magic", 'desc': "The paper introduces MuVi, a framework designed to generate music that aligns with the visual content of videos by understanding visual semantics. MuVi uses a visual adaptor to extract features from videos, ensuring the generated music matches the video's mood, theme, rhythm, and pacing. A contrastive music-visual pre-training scheme is employed to synchronize music with video, leveraging the periodic nature of music phrases. The framework also allows for control over the style and genre of the music, demonstrating superior performance in audio quality and synchronization."}, 'zh': {'title': 'MuVi：让音乐与视觉完美同步的创新框架', 'desc': '这篇论文介绍了一个名为MuVi的新框架，用于生成与视频内容相匹配的音乐。MuVi通过一个专门设计的视觉适配器分析视频内容，提取与上下文和时间相关的特征。这些特征用于生成与视频情绪、主题、节奏和节拍相匹配的音乐。此外，我们引入了一种对比音乐-视觉预训练方案，以确保音乐与视频的同步性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13852', 'title': 'Retrospective Learning from Interactions', 'url': 'https://huggingface.co/papers/2410.13852', 'abstract': 'Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. This creates an avenue for continually learning from interactions without additional annotations. We introduce ReSpect, a method to learn from such signals in past interactions via retrospection. We deploy ReSpect in a new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.', 'score': 7, 'issue_id': 146, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'e59c253e0efcaa3b', 'data': {'categories': ['#reasoning', '#rlhf', '#multimodal', '#training', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Обучение языковых моделей на неявных сигналах обратной связи', 'desc': 'Статья представляет метод ReSpect, позволяющий языковым моделям (LLM) учиться на неявных сигналах обратной связи в ходе взаимодействия с пользователями. Авторы применяют ReSpect в сценарии мультимодального взаимодействия, где люди инструктируют LLM для решения задач абстрактного мышления. Эксперименты показывают, что ReSpect постепенно улучшает показатель успешного выполнения задач с 31% до 82% без дополнительной разметки. Это открывает возможности для постоянного обучения LLM в процессе взаимодействия с пользователями.'}, 'en': {'title': 'Learning from Implicit Feedback: Enhancing LLMs with ReSpect', 'desc': "The paper discusses how large language models (LLMs) can learn from implicit feedback during multi-turn interactions with users. When users rephrase requests or express frustration, these signals can be detected by the LLM, even if it doesn't understand the task. The authors introduce a method called ReSpect, which allows LLMs to learn from these signals retrospectively, improving their performance over time. In a multimodal interaction scenario, ReSpect significantly increased task completion rates from 31% to 82% without needing additional annotations."}, 'zh': {'title': '通过回顾信号，提升语言模型的学习能力', 'desc': '这篇论文介绍了一种名为ReSpect的方法，它通过回顾过去的互动信号来学习。用户在与大型语言模型互动时，可能会通过重新措辞或表达沮丧来提供隐性反馈。ReSpect利用这些信号来提高模型的任务完成率，从31%提升到82%。这种方法不需要额外的标注，展示了在多模态互动场景中持续学习的潜力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12771', 'title': 'Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models', 'url': 'https://huggingface.co/papers/2410.12771', 'abstract': 'The ability to discover new materials with desirable properties is critical for numerous applications from helping mitigate climate change to advances in next generation computing hardware. AI has the potential to accelerate materials discovery and design by more effectively exploring the chemical space compared to other computational methods or by trial-and-error. While substantial progress has been made on AI for materials data, benchmarks, and models, a barrier that has emerged is the lack of publicly available training data and open pre-trained models. To address this, we present a Meta FAIR release of the Open Materials 2024 (OMat24) large-scale open dataset and an accompanying set of pre-trained models. OMat24 contains over 110 million density functional theory (DFT) calculations focused on structural and compositional diversity. Our EquiformerV2 models achieve state-of-the-art performance on the Matbench Discovery leaderboard and are capable of predicting ground-state stability and formation energies to an F1 score above 0.9 and an accuracy of 20 meV/atom, respectively. We explore the impact of model size, auxiliary denoising objectives, and fine-tuning on performance across a range of datasets including OMat24, MPtraj, and Alexandria. The open release of the OMat24 dataset and models enables the research community to build upon our efforts and drive further advancements in AI-assisted materials science.', 'score': 6, 'issue_id': 154, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'a98981f16e047445', 'data': {'categories': ['#science', '#benchmark', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🧪', 'ru': {'title': 'ИИ ускоряет открытие новых материалов с помощью крупнейшего открытого датасета', 'desc': 'Статья представляет новый крупномасштабный открытый датасет Open Materials 2024 (OMat24) и набор предобученных моделей для ускорения открытия новых материалов с помощью искусственного интеллекта. OMat24 содержит более 110 миллионов расчетов методом функционала плотности (DFT), фокусируясь на структурном и композиционном разнообразии. Модели EquiformerV2 достигают наилучших результатов на лидерборде Matbench Discovery, предсказывая стабильность основного состояния и энергии образования с высокой точностью. Авторы исследуют влияние размера модели, вспомогательных целей денойзинга и файн-тюнинга на производительность на различных датасетах.'}, 'en': {'title': 'Unlocking Material Innovation with AI: The OMat24 Revolution', 'desc': 'The paper discusses the use of AI to accelerate the discovery of new materials by exploring chemical space more efficiently than traditional methods. It introduces the Open Materials 2024 (OMat24) dataset, which includes over 110 million density functional theory calculations, and a set of pre-trained models called EquiformerV2. These models achieve high performance in predicting material properties, such as ground-state stability, with impressive accuracy. The open release of this dataset and models aims to overcome the barrier of limited training data and foster further advancements in AI-driven materials science.'}, 'zh': {'title': 'AI加速材料科学新突破', 'desc': '这篇论文介绍了一种新的大规模开放数据集OMat24和一组预训练模型，用于加速材料发现和设计。OMat24包含超过1.1亿个密度泛函理论计算，专注于结构和成分的多样性。EquiformerV2模型在Matbench Discovery排行榜上表现出色，能够高效预测材料的基态稳定性和形成能。通过公开OMat24数据集和模型，研究社区可以在此基础上进一步推动AI辅助材料科学的发展。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13618', 'title': 'LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning', 'url': 'https://huggingface.co/papers/2410.13618', 'abstract': 'The rapid growth of model scale has necessitated substantial computational resources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA) has sought to address the problem of handling the large updated parameters in full fine-tuning. However, LoRA utilize random initialization and optimization of low-rank matrices to approximate updated weights, which can result in suboptimal convergence and an accuracy gap compared to full fine-tuning. To address these issues, we propose LoLDU, a Parameter-Efficient Fine-Tuning (PEFT) approach that significantly reduces trainable parameters by 2600 times compared to regular PEFT methods while maintaining comparable performance. LoLDU leverages Lower-Diag-Upper Decomposition (LDU) to initialize low-rank matrices for faster convergence and orthogonality. We focus on optimizing the diagonal matrix for scaling transformations. To the best of our knowledge, LoLDU has the fewest parameters among all PEFT approaches. We conducted extensive experiments across 4 instruction-following datasets, 6 natural language understanding (NLU) datasets, 8 image classification datasets, and image generation datasets with multiple model types (LLaMA2, RoBERTa, ViT, and Stable Diffusion), providing a comprehensive and detailed analysis. Our open-source code can be accessed at https://github.com/SKDDJ/LoLDU{https://github.com/SKDDJ/LoLDU}.', 'score': 6, 'issue_id': 147, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '446dc7eb391be2b2', 'data': {'categories': ['#small_models', '#cv', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'LoLDU: Эффективная настройка моделей с минимальным количеством параметров', 'desc': 'Статья представляет новый метод эффективной настройки параметров (PEFT) под названием LoLDU. Этот подход использует LDU-разложение для инициализации матриц низкого ранга, что обеспечивает более быструю сходимость и ортогональность. LoLDU значительно сокращает количество обучаемых параметров по сравнению с обычными методами PEFT, сохраняя при этом сопоставимую производительность. Авторы провели обширные эксперименты на различных наборах данных и моделях, включая задачи обработки естественного языка и компьютерного зрения.'}, 'en': {'title': 'LoLDU: Fine-Tuning with Fewer Parameters, Same Power', 'desc': 'The paper introduces LoLDU, a new method for fine-tuning machine learning models that uses fewer parameters than traditional methods. LoLDU employs a technique called Lower-Diag-Upper Decomposition to improve the initialization of low-rank matrices, which helps in achieving faster convergence and better performance. This approach significantly reduces the number of trainable parameters by 2600 times compared to other parameter-efficient fine-tuning methods, without sacrificing accuracy. Extensive experiments across various datasets and model types demonstrate the effectiveness of LoLDU in maintaining performance while being more efficient.'}, 'zh': {'title': 'LoLDU：参数高效微调的新突破', 'desc': '随着模型规模的快速增长，微调需要大量的计算资源。现有的方法如低秩适应（LoRA）试图解决全量微调中参数更新过大的问题，但其随机初始化可能导致收敛不佳和精度差距。我们提出了LoLDU，一种参数高效微调方法，通过下对角上分解（LDU）来初始化低秩矩阵，实现更快的收敛和正交性。实验表明，LoLDU在多个数据集上表现优异，参数量是所有PEFT方法中最少的。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12781', 'title': 'Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats', 'url': 'https://huggingface.co/papers/2410.12781', 'abstract': 'We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from a long sequence of input images. Specifically, our model can process 32 source images at 960x540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture features a mixture of the recent Mamba2 blocks and the classical transformer blocks which allowed many more tokens to be processed than prior work, enhanced by efficient token merging and Gaussian pruning steps that balance between quality and efficiency. Unlike previous feed-forward models that are limited to processing 1~4 input images and can only reconstruct a small portion of a large scene, Long-LRM reconstructs the entire scene in a single feed-forward step. On large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method achieves performance comparable to optimization-based approaches while being two orders of magnitude more efficient. Project page: https://arthurhero.github.io/projects/llrm', 'score': 5, 'issue_id': 150, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '0f75e8070394f973', 'data': {'categories': ['#long_context', '#benchmark', '#inference', '#optimization', '#dataset', '#3d', '#architecture'], 'emoji': '🏙️', 'ru': {'title': 'Быстрая 3D реконструкция больших сцен из множества изображений', 'desc': 'Long-LRM - это обобщенная модель реконструкции 3D гауссовых распределений, способная восстанавливать большие сцены из длинных последовательностей изображений. Модель может обрабатывать 32 исходных изображения с разрешением 960x540 всего за 1,3 секунды на одном GPU A100 80G. Архитектура сочетает блоки Mamba2 и классические трансформеры, что позволяет обрабатывать гораздо больше токенов, чем предыдущие работы. Long-LRM реконструирует всю сцену за один проход, достигая производительности, сравнимой с оптимизационными подходами, но в 100 раз эффективнее.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with Speed and Efficiency', 'desc': 'Long-LRM is a 3D Gaussian reconstruction model designed to handle large scenes from a sequence of images efficiently. It processes 32 high-resolution images in just 1.3 seconds using a single GPU, thanks to its innovative architecture combining Mamba2 and transformer blocks. The model uses token merging and Gaussian pruning to maintain a balance between quality and speed, allowing it to reconstruct entire scenes in one step. Long-LRM achieves results similar to optimization-based methods but is significantly faster, making it ideal for large-scale datasets.'}, 'zh': {'title': '高效重建：Long-LRM让大场景重建更简单', 'desc': '这篇论文介绍了一种名为Long-LRM的3D高斯重建模型，可以从长序列的输入图像中重建大型场景。该模型能够在单个A100 80G GPU上以1.3秒的速度处理32张960x540分辨率的源图像。其架构结合了最新的Mamba2模块和经典的Transformer模块，通过高效的令牌合并和高斯剪枝步骤，在质量和效率之间取得平衡。与之前只能处理1到4张图像的前馈模型不同，Long-LRM可以在一次前馈步骤中重建整个场景。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.09347', 'title': 'Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment', 'url': 'https://huggingface.co/papers/2410.09347', 'abstract': 'Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose Condition Contrastive Alignment (CCA) to facilitate guidance-free AR visual generation with high performance and analyze its theoretical connection with guided sampling methods. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning (sim 1\\% of pretraining epochs) on the pretraining dataset, on par with guided sampling methods. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields. Code and model weights: https://github.com/thu-ml/CCA.', 'score': 4, 'issue_id': 150, 'pub_date': '2024-10-12', 'pub_date_card': {'ru': '12 октября', 'en': 'October 12', 'zh': '10月12日'}, 'hash': 'aea2fc78baf79a07', 'data': {'categories': ['#diffusion', '#cv', '#multimodal', '#training', '#open_source', '#architecture', '#alignment'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение генерации изображений без guidance через контрастное выравнивание', 'desc': 'Статья представляет новый метод под названием Condition Contrastive Alignment (CCA) для улучшения качества генерации изображений в авторегрессионных мультимодальных моделях без использования техники Classifier-Free Guidance (CFG). CCA основан на методах выравнивания языковых моделей и напрямую дообучает предобученные модели для достижения желаемого распределения выборки. Экспериментальные результаты показывают, что CCA значительно улучшает производительность моделей без применения guidance, сокращая вычислительные затраты вдвое. Метод также позволяет достигать компромисса между разнообразием и точностью сэмплов, аналогично CFG.'}, 'en': {'title': 'Unifying Visual and Language Models with CCA: A New Era of Efficient AR Generation', 'desc': 'The paper introduces Condition Contrastive Alignment (CCA), a method designed to improve autoregressive visual generation without the need for classifier-free guidance. CCA fine-tunes pretrained models to match the desired sampling distribution, enhancing performance with minimal additional training. This approach reduces the reliance on guided sampling, effectively cutting sampling costs in half while maintaining high-quality outputs. The study demonstrates that CCA can balance sample diversity and fidelity, bridging the gap between language and visual model alignment techniques.'}, 'zh': {'title': '条件对比对齐：统一语言与视觉生成的创新方法', 'desc': '这篇论文介绍了一种名为条件对比对齐（CCA）的新方法，用于在自回归多模态生成中实现无指导的高性能视觉生成。传统的分类器自由指导（CFG）在语言和视觉内容之间引入了设计不一致，而CCA通过直接微调预训练模型来解决这个问题。实验结果表明，CCA可以在仅需一次微调的情况下显著提高模型的无指导性能，与指导采样方法相当。通过调整训练参数，CCA还可以在样本多样性和保真度之间实现类似CFG的平衡。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13060', 'title': 'AERO: Softmax-Only LLMs for Efficient Private Inference', 'url': 'https://huggingface.co/papers/2410.13060', 'abstract': "The pervasiveness of proprietary language models has raised privacy concerns for users' sensitive data, emphasizing the need for private inference (PI), where inference is performed directly on encrypted inputs. However, current PI methods face prohibitively higher communication and latency overheads, primarily due to nonlinear operations. In this paper, we present a comprehensive analysis to understand the role of nonlinearities in transformer-based decoder-only language models. We introduce AERO, a four-step architectural optimization framework that refines the existing LLM architecture for efficient PI by systematically removing nonlinearities such as LayerNorm and GELU and reducing FLOPs counts. For the first time, we propose a Softmax-only architecture with significantly fewer FLOPs tailored for efficient PI. Furthermore, we devise a novel entropy regularization technique to improve the performance of Softmax-only models. AERO achieves up to 4.23times communication and 1.94times latency reduction. We validate the effectiveness of AERO by benchmarking it against the state-of-the-art.", 'score': 4, 'issue_id': 146, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'f664135f7701d39c', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#security', '#architecture'], 'emoji': '🔒', 'ru': {'title': 'AERO: Оптимизация архитектуры языковых моделей для приватного вывода', 'desc': 'Статья представляет комплексный анализ роли нелинейных операций в трансформерных языковых моделях декодер-только архитектуры. Авторы предлагают AERO - четырехэтапную структуру оптимизации архитектуры, которая систематически удаляет нелинейности, такие как LayerNorm и GELU, и сокращает количество FLOP. Впервые предложена архитектура только с Softmax, специально разработанная для эффективного частного вывода (PI). Представлен новый метод регуляризации энтропии для улучшения производительности моделей только с Softmax.'}, 'en': {'title': "Streamlining Privacy: AERO's Leap in Efficient Language Model Inference", 'desc': 'The paper addresses privacy concerns in language models by focusing on private inference, which allows computations on encrypted data. Current methods struggle with high communication and latency due to nonlinear operations. The authors introduce AERO, an optimization framework that removes nonlinearities like LayerNorm and GELU, and proposes a Softmax-only architecture to reduce computational demands. AERO significantly improves efficiency, achieving notable reductions in communication and latency, and is validated against existing methods.'}, 'zh': {'title': 'AERO：提升隐私推理效率的新框架', 'desc': '这篇论文讨论了在加密输入上进行推理的隐私推理（PI）问题，并提出了一种名为AERO的优化框架。AERO通过去除非线性操作如LayerNorm和GELU，减少计算量，从而提高PI的效率。作者首次提出了一种仅使用Softmax的架构，显著减少了计算量，并引入了一种新的熵正则化技术来提升模型性能。实验结果表明，AERO在通信和延迟方面分别减少了4.23倍和1.94倍。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10210', 'title': 'Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key', 'url': 'https://huggingface.co/papers/2410.10210', 'abstract': 'As large language models rapidly evolve to support longer context, there is a notable disparity in their capability to generate output at greater lengths. Recent study suggests that the primary cause for this imbalance may arise from the lack of data with long-output during alignment training. In light of this observation, attempts are made to re-align foundation models with data that fills the gap, which result in models capable of generating lengthy output when instructed. In this paper, we explore the impact of data-quality in tuning a model for long output, and the possibility of doing so from the starting points of human-aligned (instruct or chat) models. With careful data curation, we show that it possible to achieve similar performance improvement in our tuned models, with only a small fraction of training data instances and compute. In addition, we assess the generalizability of such approaches by applying our tuning-recipes to several models. our findings suggest that, while capacities for generating long output vary across different models out-of-the-box, our approach to tune them with high-quality data using lite compute, consistently yields notable improvement across all models we experimented on. We have made public our curated dataset for tuning long-writing capability, the implementations of model tuning and evaluation, as well as the fine-tuned models, all of which can be openly-accessed.', 'score': 3, 'issue_id': 150, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': '2fa998e2c3dc73e1', 'data': {'categories': ['#small_models', '#synthetic', '#data', '#training', '#dataset', '#open_source', '#long_context', '#alignment'], 'emoji': '📝', 'ru': {'title': 'Длинные ответы для больших языковых моделей: качество важнее количества', 'desc': 'В статье исследуется проблема неспособности больших языковых моделей генерировать длинные тексты. Авторы предлагают метод дообучения моделей на специально подготовленном наборе данных с длинными ответами. Эксперименты показывают, что даже небольшое количество качественных данных позволяет значительно улучшить способность моделей к генерации длинных текстов. Исследователи публикуют свой набор данных, код и дообученные модели для открытого доступа.'}, 'en': {'title': 'Unlocking Long-Form Language: Tuning Models for Extended Output', 'desc': "The paper discusses how large language models often struggle to generate long outputs due to insufficient training data that includes lengthy examples. By re-aligning these models with carefully curated data that emphasizes long outputs, the researchers were able to enhance the models' ability to produce extended text. They found that even with a small amount of high-quality data and minimal computational resources, significant improvements could be achieved. The study also demonstrated that this approach works across various models, suggesting a generalizable method for improving long-output generation."}, 'zh': {'title': '用高质量数据提升长文本生成能力', 'desc': '这篇论文探讨了大语言模型在生成长文本时能力不均衡的问题，主要原因是训练时缺乏长输出的数据。研究通过重新调整基础模型的数据，使其能够生成长文本。通过精心的数据选择，研究表明只需少量数据和计算资源就能显著提高模型性能。研究还展示了这种方法在不同模型上的普遍适用性，并公开了相关数据集和模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12183', 'title': 'TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration', 'url': 'https://huggingface.co/papers/2410.12183', 'abstract': 'Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to generalize well. Alternatively, there exists a wide range of expert models that contain diversified vision and/or language knowledge pre-trained on different modalities, tasks, networks, and datasets. Unfortunately, these models are "isolated agents" with heterogeneous structures, and how to integrate their knowledge for generalizing CLIP-like models has not been fully explored. To bridge this gap, we propose a general and concise TransAgent framework, which transports the knowledge of the isolated agents in a unified manner, and effectively guides CLIP to generalize with multi-source knowledge distillation. With such a distinct framework, we flexibly collaborate with 11 heterogeneous agents to empower vision-language foundation models, without further cost in the inference phase. Finally, our TransAgent achieves state-of-the-art performance on 11 visual recognition datasets. Under the same low-shot setting, it outperforms the popular CoOp with around 10% on average, and 20% on EuroSAT which contains large domain shifts.', 'score': 3, 'issue_id': 150, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '8e00415319fa6d49', 'data': {'categories': ['#benchmark', '#cv', '#graphs', '#multimodal', '#training', '#transfer_learning', '#agents', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'TransAgent: объединяем знания экспертов для лучшего обобщения мультимодальных моделей', 'desc': 'Статья представляет TransAgent - фреймворк для улучшения обобщающей способности моделей компьютерного зрения и обработки естественного языка типа CLIP. TransAgent объединяет знания из различных экспертных моделей через мультимодальную дистилляцию знаний. Это позволяет эффективно адаптировать CLIP-подобные модели к новым доменам данных. Авторы демонстрируют значительное улучшение производительности на 11 наборах данных по распознаванию изображений, особенно в условиях ограниченного количества обучающих примеров.'}, 'en': {'title': '"Unifying Isolated Experts: TransAgent\'s Leap in Vision-Language Models"', 'desc': 'The paper introduces TransAgent, a framework designed to enhance vision-language models like CLIP by integrating knowledge from various expert models. These expert models, pre-trained on different tasks and datasets, are typically isolated and have diverse structures. TransAgent unifies their knowledge through multi-source knowledge distillation, allowing CLIP to generalize better across different domains. This approach significantly improves performance on multiple visual recognition tasks, outperforming existing methods like CoOp, especially in scenarios with large domain shifts.'}, 'zh': {'title': 'TransAgent：整合多源知识，提升视觉-语言模型泛化能力', 'desc': '这篇论文介绍了一种名为TransAgent的框架，用于整合不同专家模型的知识来增强视觉-语言基础模型的泛化能力。TransAgent通过多源知识蒸馏的方式，将孤立的专家模型的知识统一传输给CLIP等模型。该框架能够在不增加推理阶段成本的情况下，与11个异构代理灵活协作。最终，TransAgent在11个视觉识别数据集上实现了最先进的性能，尤其在EuroSAT数据集上表现出色。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13293', 'title': 'SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2410.13293', 'abstract': 'Many students struggle with math word problems (MWPs), often finding it difficult to identify key information and select the appropriate mathematical operations.Schema-based instruction (SBI) is an evidence-based strategy that helps students categorize problems based on their structure, improving problem-solving accuracy. Building on this, we propose a Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) framework that incorporates a large language model (LLM).Our approach emphasizes step-by-step reasoning by leveraging schemas to guide solution generation. We evaluate its performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo, and introduce a "reasoning score" metric to assess solution quality. Our findings suggest that SBI-RAG enhances reasoning clarity and problem-solving accuracy, potentially providing educational benefits for students', 'score': 3, 'issue_id': 148, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'edc83779c70890eb', 'data': {'categories': ['#rag', '#reasoning', '#benchmark', '#math', '#dataset', '#education'], 'emoji': '🧮', 'ru': {'title': 'Схемы + ИИ = Лучшее решение математических задач', 'desc': 'Авторы предлагают новый подход к решению математических текстовых задач, называемый SBI-RAG (Schema-Based Instruction Retrieval-Augmented Generation). Этот метод сочетает в себе схемо-ориентированное обучение (SBI) и большую языковую модель (LLM) для пошагового рассуждения при решении задач. Авторы оценивают производительность SBI-RAG на наборе данных GSM8K, сравнивая его с GPT-4 и GPT-3.5 Turbo. Результаты показывают, что SBI-RAG улучшает ясность рассуждений и точность решения задач, что потенциально может принести пользу учащимся.'}, 'en': {'title': 'Unlocking Math Word Problems with Schema-Based AI', 'desc': "The paper introduces a new framework called Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) to help students solve math word problems more effectively. This approach uses a large language model to guide students through step-by-step reasoning by categorizing problems based on their structure. The framework was tested on the GSM8K dataset and compared with existing models like GPT-4 and GPT-3.5 Turbo, using a new 'reasoning score' metric to evaluate solution quality. Results indicate that SBI-RAG improves both the clarity of reasoning and the accuracy of problem-solving, offering potential educational benefits."}, 'zh': {'title': '结构引导，提升解题能力', 'desc': '许多学生在解决数学文字题时感到困难，常常难以识别关键信息并选择合适的数学运算。基于结构的教学（SBI）是一种基于证据的策略，帮助学生根据问题的结构进行分类，从而提高解题准确性。我们提出了一种结合大语言模型（LLM）的基于结构的教学检索增强生成（SBI-RAG）框架，强调通过利用结构指导解决方案生成的逐步推理。研究表明，SBI-RAG可以提高推理的清晰度和解题的准确性，为学生提供潜在的教育益处。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14059', 'title': 'UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.14059', 'abstract': 'This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, task-specific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 12 LLM services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial sector but also provides a robust framework for assessing their performance and user satisfaction.The benchmark dataset and evaluation code are available.', 'score': 52, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '1124cffc31d2cf8d', 'data': {'categories': ['#benchmark', '#multilingual', '#data', '#dataset', '#open_source', '#alignment'], 'emoji': '💹', 'ru': {'title': 'UCFE: Новый стандарт для оценки финансового ИИ', 'desc': 'Статья представляет UCFE - новый бенчмарк для оценки способности больших языковых моделей (LLM) решать сложные финансовые задачи. Бенчмарк использует гибридный подход, сочетающий оценки экспертов и динамические взаимодействия для симуляции реальных финансовых сценариев. На основе пользовательского исследования был создан датасет, охватывающий широкий спектр финансовых задач. Результаты тестирования 12 LLM-сервисов показали высокую корреляцию (0.78) между оценками бенчмарка и предпочтениями пользователей.'}, 'en': {'title': 'Empowering LLMs for Financial Mastery', 'desc': 'The paper introduces the UCFE benchmark, a new framework to test how well large language models (LLMs) can handle complex financial tasks. It uses a mix of human expert evaluations and dynamic interactions to mimic real-world financial scenarios. A user study with 804 participants helped create a dataset that reflects various user intents, which was then used to test 12 LLM services. The results showed a strong correlation between the benchmark scores and human preferences, indicating the effectiveness of the UCFE framework in evaluating LLMs in finance.'}, 'zh': {'title': 'UCFE：评估金融领域大型语言模型的新基准', 'desc': '这篇论文介绍了UCFE：用户中心金融专业基准，这是一个创新框架，用于评估大型语言模型处理复杂金融任务的能力。UCFE基准采用混合方法，结合人类专家评估和动态任务特定交互，模拟不断变化的金融场景。研究中，我们收集了804名参与者对金融任务的反馈，并基于此创建了一个包含广泛用户意图和交互的数据集。结果显示，基准分数与人类偏好之间有显著的一致性，表明UCFE数据集和评估方法的有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13232', 'title': 'Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation', 'url': 'https://huggingface.co/papers/2410.13232', 'abstract': 'Large language models (LLMs) have recently gained much attention in building autonomous agents. However, the performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the "world model". Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents\' policy selection without training and demonstrate our agents\' cost- and time-efficiency compared to recent tree-search-based agents.', 'score': 40, 'issue_id': 194, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '7c607e84a2158236', 'data': {'categories': ['#long_context', '#rag', '#reasoning', '#rl', '#agi', '#agents', '#architecture'], 'emoji': '🌐', 'ru': {'title': "Веб-агенты с 'моделью мира': умнее, эффективнее, безопаснее", 'desc': "Эта статья представляет новый подход к улучшению веб-агентов на основе больших языковых моделей (LLM) путем внедрения 'модели мира'. Авторы обнаружили, что существующие LLM-агенты часто совершают ошибки в задачах с долгосрочными последствиями, например, повторно покупая невозвратные билеты. Для решения этой проблемы они разработали World-model-augmented (WMA) веб-агент, который симулирует результаты своих действий перед принятием решений. Эксперименты показали, что этот подход улучшает выбор стратегий агентами и повышает их эффективность по сравнению с агентами, основанными на поиске в дереве решений."}, 'en': {'title': 'Empowering AI with World Models for Smarter Decisions', 'desc': "The paper discusses the limitations of large language models (LLMs) in performing long-horizon tasks autonomously, highlighting their lack of a 'world model' to predict the outcomes of actions. To address this, the authors introduce a World-model-augmented (WMA) web agent that simulates potential outcomes to improve decision-making. They propose a novel approach using transition-focused observation abstraction to train LLMs, which involves predicting important state changes in natural language. Experiments demonstrate that this method enhances policy selection and efficiency compared to traditional tree-search-based agents."}, 'zh': {'title': '增强世界模型：让AI决策更聪明', 'desc': '这篇论文研究了大型语言模型在长时间任务中的表现问题，发现它们缺乏“世界模型”的能力，容易犯不可逆的错误。为了解决这个问题，研究者提出了一种增强世界模型的网络代理，通过模拟行动结果来改进决策。为了克服训练中的挑战，他们设计了一种专注于状态变化的观察抽象方法。实验表明，这种方法提高了代理的决策效率，并且在成本和时间上优于其他方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13370', 'title': 'MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2410.13370', 'abstract': 'Recent advancements in text-to-image (T2I) diffusion models have enabled the creation of high-quality images from text prompts, but they still struggle to generate images with precise control over specific visual concepts. Existing approaches can replicate a given concept by learning from reference images, yet they lack the flexibility for fine-grained customization of the individual component within the concept. In this paper, we introduce component-controllable personalization, a novel task that pushes the boundaries of T2I models by allowing users to reconfigure specific components when personalizing visual concepts. This task is particularly challenging due to two primary obstacles: semantic pollution, where unwanted visual elements corrupt the personalized concept, and semantic imbalance, which causes disproportionate learning of the concept and component. To overcome these challenges, we design MagicTailor, an innovative framework that leverages Dynamic Masked Degradation (DM-Deg) to dynamically perturb undesired visual semantics and Dual-Stream Balancing (DS-Bal) to establish a balanced learning paradigm for desired visual semantics. Extensive comparisons, ablations, and analyses demonstrate that MagicTailor not only excels in this challenging task but also holds significant promise for practical applications, paving the way for more nuanced and creative image generation.', 'score': 36, 'issue_id': 192, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'fa9ce7d280c285b4', 'data': {'categories': ['#diffusion', '#optimization', '#architecture', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Точный контроль над компонентами в персонализированной генерации изображений', 'desc': 'Эта статья представляет новый подход к персонализации генерации изображений с помощью текстовых запросов. Авторы предлагают метод MagicTailor, который позволяет пользователям точно контролировать отдельные компоненты визуальных концепций. Система использует динамическое маскированное ухудшение (DM-Deg) для устранения нежелательных визуальных элементов и двухпоточную балансировку (DS-Bal) для сбалансированного обучения желаемой семантики. Эксперименты показывают, что MagicTailor превосходит существующие методы и открывает новые возможности для креативной генерации изображений.'}, 'en': {'title': 'MagicTailor: Precision in Text-to-Image Creativity', 'desc': 'The paper introduces a new task called component-controllable personalization for text-to-image diffusion models, which allows users to adjust specific parts of a visual concept. This task addresses two main challenges: semantic pollution, where unwanted elements interfere with the concept, and semantic imbalance, where the learning of the concept and its components is uneven. To tackle these issues, the authors propose MagicTailor, a framework using Dynamic Masked Degradation to remove unwanted semantics and Dual-Stream Balancing to ensure balanced learning. The results show that MagicTailor significantly improves the precision and flexibility of image generation, offering new possibilities for creative applications.'}, 'zh': {'title': 'MagicTailor：实现图像生成的精细化控制', 'desc': '近年来，文本到图像扩散模型在从文本生成高质量图像方面取得了显著进展，但在精确控制特定视觉概念上仍存在困难。现有方法可以通过学习参考图像来复制给定概念，但缺乏对概念内各个组件的细粒度定制能力。本文提出了一种新的任务——组件可控个性化，允许用户在个性化视觉概念时重新配置特定组件。为解决语义污染和语义不平衡的问题，我们设计了MagicTailor框架，通过动态掩码降解和双流平衡技术实现更精细的图像生成。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14669', 'title': 'NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples', 'url': 'https://huggingface.co/papers/2410.14669', 'abstract': 'Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a vision-centric design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.', 'score': 35, 'issue_id': 196, 'pub_date': '2024-10-18', 'pub_date_card': {'ru': '18 октября', 'en': 'October 18', 'zh': '10月18日'}, 'hash': 'a015f20d9d67a6a8', 'data': {'categories': ['#reasoning', '#benchmark', '#multilingual', '#cv', '#multimodal', '#interpretability', '#dataset', '#low_resource', '#security', '#long_context'], 'emoji': '🖼️', 'ru': {'title': 'NaturalBench: новый вызов для моделей визуально-языкового понимания', 'desc': 'Исследователи разработали новый бенчмарк NaturalBench для оценки моделей визуально-языкового понимания (VLM) на основе естественных изображений и вопросов. Бенчмарк содержит 10 000 проверенных человеком образцов задач визуального ответа на вопросы (VQA), которые требуют разнообразных навыков визуально-лингвистического рассуждения. Тестирование 53 современных VLM на NaturalBench показало, что даже лучшие модели отстают от человеческой производительности на 50-70%. Исследование выявило серьезные проблемы с композиционностью и предвзятостью в существующих VLM.'}, 'en': {'title': 'Challenging VLMs: Beyond Simple Answers', 'desc': 'Vision-language models (VLMs) have shown progress in visual-question-answering tasks but still struggle with natural adversarial samples, which are questions humans can easily answer. The paper introduces NaturalBench, a new benchmark with 10,000 human-verified samples, designed to challenge VLMs by pairing questions with two images that require different answers. Evaluations show that current VLMs perform significantly worse than humans, highlighting issues with compositionality and biases in these models. The study also demonstrates the potential of using diverse data sources for dynamic evaluations of VLMs.'}, 'zh': {'title': '挑战视觉语言模型的极限：NaturalBench的诞生', 'desc': '这篇论文探讨了视觉语言模型（VLMs）在处理自然图像和问题时的不足，尽管它们在视觉问答（VQA）基准测试中取得了进展。研究者提出了一个新的基准测试，称为NaturalBench，通过配对问题和不同答案的图像来提高测试难度。结果显示，许多先进的VLMs在这个基准测试中表现不佳，与人类表现相差50%-70%。论文还分析了VLMs在组合性和偏见方面的挑战，并展示了如何利用多样化的数据源进行动态评估。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13276', 'title': 'SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs', 'url': 'https://huggingface.co/papers/2410.13276', 'abstract': 'Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity limits the efficiency and scalability of LLMs, especially for those with a long-context window. A promising approach addressing this limitation is to leverage the sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics to approximate sparsity. This practice falls short to fully capture the dynamic nature of attention sparsity in language-based tasks. This paper argues that attention sparsity should be learned rather than predefined. To this end, we design SeerAttention, a new Attention mechanism that augments the conventional attention with a learnable gate that adaptively selects significant blocks in an attention map and deems the rest blocks sparse. Such block-level sparsity effectively balances accuracy and speedup. To enable efficient learning of the gating network, we develop a customized FlashAttention implementation that extracts the block-level ground truth of attention map with minimum overhead. SeerAttention not only applies to post-training, but also excels in long-context fine-tuning. Our results show that at post-training stages, SeerAttention significantly outperforms state-of-the-art static or heuristic-based sparse attention methods, while also being more versatile and flexible to adapt to varying context lengths and sparsity ratios. When applied to long-context fine-tuning with YaRN, SeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context length with minimal perplexity loss, offering a 5.67x speedup over FlashAttention-2.', 'score': 24, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'd64d285d002a425d', 'data': {'categories': ['#long_context', '#inference', '#optimization', '#training', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'SeerAttention: Эффективное обучаемое разреженное внимание для LLM', 'desc': 'Статья представляет SeerAttention - новый механизм внимания для больших языковых моделей (LLM). Он использует обучаемые gates для адаптивного выбора значимых блоков в карте внимания, что позволяет эффективно балансировать точность и скорость работы. SeerAttention превосходит существующие методы разреженного внимания и может применяться как после обучения, так и при тонкой настройке на длинных контекстах. Результаты показывают значительное ускорение без существенной потери качества даже при 90% разреженности.'}, 'en': {'title': '"SeerAttention: Learning to Focus, Speeding Up Language Models"', 'desc': 'The paper introduces SeerAttention, a novel attention mechanism designed to improve the efficiency of Large Language Models by learning attention sparsity dynamically rather than relying on predefined patterns. SeerAttention uses a learnable gate to identify and focus on significant blocks in the attention map, enhancing both accuracy and processing speed. The authors also developed a customized FlashAttention implementation to efficiently learn the gating network with minimal computational overhead. Results demonstrate that SeerAttention outperforms existing sparse attention methods, particularly in long-context fine-tuning, achieving high sparsity ratios with minimal loss in performance.'}, 'zh': {'title': 'SeerAttention：自适应学习注意力稀疏性的新方法', 'desc': '这篇论文讨论了大型语言模型中的注意力机制，指出其二次复杂性限制了效率和可扩展性。为了解决这个问题，作者提出了一种新的注意力机制SeerAttention，它通过可学习的门控机制自适应地选择注意力图中的重要块。这样的方法在保持准确性的同时提高了速度。实验结果表明，SeerAttention在长上下文微调中表现出色，能够在高稀疏率下保持低困惑度。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13925', 'title': 'FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model', 'url': 'https://huggingface.co/papers/2410.13925', 'abstract': 'Nature is infinitely resolution-free. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To address this limitation, we conceptualize images as sequences of tokens with dynamic sizes, rather than traditional methods that perceive images as fixed-resolution grids. This perspective enables a flexible training strategy that seamlessly accommodates various aspect ratios during both training and inference, thus promoting resolution generalization and eliminating biases introduced by image cropping. On this basis, we present the Flexible Vision Transformer (FiT), a transformer architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. We further upgrade the FiT to FiTv2 with several innovative designs, includingthe Query-Key vector normalization, the AdaLN-LoRA module, a rectified flow scheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted network structure, FiTv2 exhibits 2times convergence speed of FiT. When incorporating advanced training-free extrapolation techniques, FiTv2 demonstrates remarkable adaptability in both resolution extrapolation and diverse resolution generation. Additionally, our exploration of the scalability of the FiTv2 model reveals that larger models exhibit better computational efficiency. Furthermore, we introduce an efficient post-training strategy to adapt a pre-trained model for the high-resolution generation. Comprehensive experiments demonstrate the exceptional performance of FiTv2 across a broad range of resolutions. We have released all the codes and models at https://github.com/whlzy/FiT to promote the exploration of diffusion transformer models for arbitrary-resolution image generation.', 'score': 21, 'issue_id': 201, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '2109b51982de79e7', 'data': {'categories': ['#diffusion', '#small_models', '#cv', '#inference', '#training', '#open_source', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Гибкая генерация изображений любого разрешения', 'desc': 'Эта статья представляет Flexible Vision Transformer (FiT) - новую архитектуру трансформера для генерации изображений с произвольным разрешением и соотношением сторон. FiT рассматривает изображения как последовательности токенов динамического размера, что позволяет гибко обучать модель на изображениях различных пропорций. Усовершенствованная версия FiTv2 включает ряд инновационных техник, таких как нормализация векторов запросов-ключей и модуль AdaLN-LoRA, что значительно улучшает производительность. Эксперименты показывают исключительную эффективность FiTv2 при генерации изображений широкого диапазона разрешений.'}, 'en': {'title': 'Breaking Free from Fixed Resolutions: The Future of Image Generation', 'desc': 'The paper introduces the Flexible Vision Transformer (FiT), a new model that treats images as sequences of tokens with dynamic sizes, allowing it to handle various resolutions and aspect ratios without bias. This approach improves upon traditional diffusion models that struggle with images outside their trained resolution. The upgraded version, FiTv2, incorporates several innovative features, enhancing convergence speed and adaptability for high-resolution image generation. Experiments show that FiTv2 performs exceptionally well across different resolutions, and the authors have made their code available for further exploration.'}, 'zh': {'title': '灵活视觉变换器：突破分辨率限制的图像生成', 'desc': '这篇论文介绍了一种新的图像生成模型，称为灵活视觉变换器（FiT），它可以处理任意分辨率和长宽比的图像。通过将图像视为动态大小的序列，而不是固定分辨率的网格，FiT能够在训练和推理过程中灵活适应不同的图像比例。升级版FiTv2引入了多项创新设计，使其在分辨率外推和多样化分辨率生成方面表现出色。实验结果表明，FiTv2在各种分辨率下的性能都非常出色，并且代码和模型已公开以促进进一步研究。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11190', 'title': 'Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities', 'url': 'https://huggingface.co/papers/2410.11190', 'abstract': 'GPT-4o, an all-encompassing model, represents a milestone in the development of large multi-modal language models. It can understand visual, auditory, and textual modalities, directly output audio, and support flexible duplex interaction. Models from the open-source community often achieve some functionalities of GPT-4o, such as visual understanding and voice chat. Nevertheless, training a unified model that incorporates all modalities is challenging due to the complexities of multi-modal data, intricate model architectures, and training processes. In this paper, we introduce Mini-Omni2, a visual-audio assistant capable of providing real-time, end-to-end voice responses to visoin and audio queries. By integrating pretrained visual and auditory encoders, Mini-Omni2 maintains performance in individual modalities. We propose a three-stage training process to align modalities, allowing the language model to handle multi-modal inputs and outputs after training on a limited dataset. For interaction, we introduce a command-based interruption mechanism, enabling more flexible interaction with users. To the best of our knowledge, Mini-Omni2 is one of the closest reproductions of GPT-4o, which have similar form of functionality, and we hope it can offer valuable insights for subsequent research.', 'score': 20, 'issue_id': 200, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '41fce1c87f8f0f35', 'data': {'categories': ['#small_models', '#synthetic', '#cv', '#multimodal', '#training', '#open_source', '#audio', '#architecture', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Mini-Omni2: Мультимодальный ассистент нового поколения', 'desc': 'Mini-Omni2 - это мультимодальная модель машинного обучения, способная обрабатывать визуальные, аудио и текстовые данные, а также генерировать голосовые ответы в реальном времени. Модель использует предобученные визуальные и аудио энкодеры для сохранения производительности в отдельных модальностях. Авторы предлагают трехэтапный процесс обучения для выравнивания модальностей и механизм прерывания на основе команд для более гибкого взаимодействия с пользователями. Mini-Omni2 представляет собой одну из ближайших репродукций функциональности GPT-4o, предоставляя ценные insights для дальнейших исследований в области мультимодальных языковых моделей.'}, 'en': {'title': 'Mini-Omni2: Bridging Modalities for Real-Time Interaction', 'desc': 'The paper introduces Mini-Omni2, a model designed to handle visual and auditory inputs and provide real-time voice responses. It uses pretrained encoders for visual and auditory data to maintain high performance across these modalities. A three-stage training process is employed to align these modalities, allowing the model to process multi-modal inputs and outputs effectively. Additionally, a command-based interruption mechanism is introduced to enhance user interaction flexibility.'}, 'zh': {'title': 'Mini-Omni2：多模态交互的未来', 'desc': '这篇论文介绍了Mini-Omni2，一个能够实时处理视觉和音频查询的多模态助手。通过整合预训练的视觉和听觉编码器，Mini-Omni2在单一模态下保持了良好的性能。论文提出了一个三阶段的训练过程，使语言模型能够处理多模态输入和输出。Mini-Omni2还引入了基于命令的中断机制，提供了更灵活的用户交互方式。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13782', 'title': 'DPLM-2: A Multimodal Diffusion Protein Language Model', 'url': 'https://huggingface.co/papers/2410.13782', 'abstract': 'Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce DPLM-2, a multimodal protein foundation model that extends discrete diffusion protein language model (DPLM) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that DPLM-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach. Moreover, DPLM-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs, as well as providing structure-aware representations for predictive tasks.', 'score': 19, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '9e7b5af0f52ac4bf', 'data': {'categories': ['#science', '#diffusion', '#synthetic', '#graphs', '#multimodal', '#training', '#3d', '#architecture'], 'emoji': '🧬', 'ru': {'title': 'Единая модель для последовательностей и структур белков', 'desc': 'DPLM-2 — это мультимодальная языковая модель для белков, которая одновременно моделирует последовательности аминокислот и трехмерные структуры. Модель использует дискретную диффузию и квантование для представления 3D-координат в виде токенов. DPLM-2 обучается на экспериментальных и синтетических данных, что позволяет ей изучать совместное распределение последовательностей и структур. Модель демонстрирует высокую эффективность в задачах условной генерации, таких как фолдинг и обратный фолдинг белков.'}, 'en': {'title': "Unifying Protein Sequence and Structure: DPLM-2's Multimodal Mastery", 'desc': 'The paper introduces DPLM-2, a multimodal protein foundation model that integrates both protein sequences and structures into a single framework. By converting 3D structures into discrete tokens, the model learns the joint distribution of sequences and structures, improving the generation and understanding of proteins. DPLM-2 eliminates the need for separate models for sequence and structure, enhancing performance in tasks like folding and scaffolding. The model leverages evolutionary data and structural biases, showing competitive results in generating compatible sequences and structures.'}, 'zh': {'title': 'DPLM-2：同时生成蛋白质序列和结构的多模态模型', 'desc': '蛋白质是由氨基酸序列决定的三维结构的基本大分子，影响其在生物体中的功能。现有的方法通常使用单独的模型来处理序列和结构，限制了对两者之间复杂关系的捕捉能力。本文介绍了一种新的多模态蛋白质基础模型DPLM-2，它可以同时生成和理解蛋白质的序列和结构。通过将3D坐标转换为离散标记，DPLM-2能够在实验和高质量合成结构上进行训练，展示了在多种条件生成任务中的竞争性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.10812', 'title': 'HART: Efficient Visual Generation with Hybrid Autoregressive Transformer', 'url': 'https://huggingface.co/papers/2410.10812', 'abstract': 'We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.', 'score': 14, 'issue_id': 200, 'pub_date': '2024-10-14', 'pub_date_card': {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'}, 'hash': 'cc3b200d837522fd', 'data': {'categories': ['#diffusion', '#small_models', '#cv', '#optimization', '#open_source', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'HART: высококачественная генерация изображений с гибридным подходом', 'desc': 'Статья представляет Гибридный Авторегрессионный Трансформер (HART) - модель для генерации изображений высокого качества размером 1024x1024 пикселей. HART использует гибридный токенизатор, который разделяет латентное представление на дискретные токены для общей картины и непрерывные токены для деталей. Модель сочетает масштабируемую авторегрессионную часть для дискретных токенов и легковесный диффузионный модуль для непрерывных. HART превосходит современные диффузионные модели по качеству и эффективности генерации изображений.'}, 'en': {'title': 'HART: Revolutionizing Image Generation with Hybrid Tokenization', 'desc': 'The paper introduces the Hybrid Autoregressive Transformer (HART), a model that generates high-quality 1024x1024 images, competing with diffusion models. It addresses the limitations of existing autoregressive models by using a hybrid tokenizer that combines discrete and continuous tokens for better image reconstruction. This approach significantly improves the Fréchet Inception Distance (FID) scores, indicating better image quality and efficiency. HART also demonstrates superior performance in terms of throughput and computational efficiency compared to state-of-the-art diffusion models.'}, 'zh': {'title': 'HART：突破图像生成极限的混合自回归模型', 'desc': '我们介绍了一种名为混合自回归变压器（HART）的视觉生成模型，可以直接生成1024x1024的高质量图像。传统的自回归模型在图像重建质量和训练成本上存在局限性。为了解决这些问题，我们提出了混合分词器，将连续潜变量分解为离散和连续两部分。与仅使用离散分词器的模型相比，我们的方法在图像重建和生成质量上都有显著提升。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13674', 'title': 'Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion', 'url': 'https://huggingface.co/papers/2410.13674', 'abstract': 'Low-quality or scarce data has posed significant challenges for training deep neural networks in practice. While classical data augmentation cannot contribute very different new data, diffusion models opens up a new door to build self-evolving AI by generating high-quality and diverse synthetic data through text-guided prompts. However, text-only guidance cannot control synthetic images\' proximity to the original images, resulting in out-of-distribution data detrimental to the model performance. To overcome the limitation, we study image guidance to achieve a spectrum of interpolations between synthetic and real images. With stronger image guidance, the generated images are similar to the training data but hard to learn. While with weaker image guidance, the synthetic images will be easier for model but contribute to a larger distribution gap with the original data. The generated full spectrum of data enables us to build a novel "Diffusion Curriculum (DisCL)". DisCL adjusts the image guidance level of image synthesis for each training stage: It identifies and focuses on hard samples for the model and assesses the most effective guidance level of synthetic images to improve hard data learning. We apply DisCL to two challenging tasks: long-tail (LT) classification and learning from low-quality data. It focuses on lower-guidance images of high-quality to learn prototypical features as a warm-up of learning higher-guidance images that might be weak on diversity or quality. Extensive experiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when applying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base model\'s tail-class accuracy from 4.4% to 23.64% and leads to a 4.02% improvement in all-class accuracy.', 'score': 14, 'issue_id': 193, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '61d2689e00439256', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#optimization', '#data', '#training', '#low_resource'], 'emoji': '🔄', 'ru': {'title': 'Адаптивное обучение на синтетических данных для улучшения глубоких нейронных сетей', 'desc': "Эта статья представляет новый метод под названием 'Diffusion Curriculum (DisCL)' для улучшения обучения нейронных сетей на ограниченных или некачественных данных. DisCL использует диффузионные модели для генерации синтетических данных, контролируя их близость к исходным изображениям с помощью управления изображениями. Метод адаптивно регулирует уровень управления изображениями на разных этапах обучения, фокусируясь на сложных примерах и определяя наиболее эффективный уровень синтеза. DisCL показал значительные улучшения в задачах классификации с длинным хвостом и обучении на данных низкого качества."}, 'en': {'title': '"Diffusion Curriculum: Bridging the Data Gap with Smart Synthesis"', 'desc': 'The paper addresses the challenge of training deep neural networks with low-quality or scarce data by using diffusion models to generate diverse synthetic data. It introduces a method called Diffusion Curriculum (DisCL) that adjusts the level of image guidance during training to balance between similarity to real data and ease of learning. DisCL focuses on identifying hard samples and optimizes the guidance level to improve learning from difficult data. The approach shows significant improvements in classification tasks, particularly in long-tail and low-quality data scenarios.'}, 'zh': {'title': '扩散课程：提升深度学习的自我进化能力', 'desc': '这篇论文研究了如何利用扩散模型生成高质量和多样化的合成数据来改善深度神经网络的训练。通过图像引导，研究者们能够在合成图像和真实图像之间实现不同程度的插值，从而克服文本引导的局限性。论文提出了一种新的“扩散课程（DisCL）”方法，通过调整图像合成的引导水平来提高模型对困难样本的学习能力。实验结果表明，DisCL在处理长尾分类和低质量数据学习任务时，显著提高了模型的准确性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13726', 'title': 'DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation', 'url': 'https://huggingface.co/papers/2410.13726', 'abstract': 'Talking head generation intends to produce vivid and realistic talking head videos from a single portrait and speech audio clip. Although significant progress has been made in diffusion-based talking head generation, almost all methods rely on autoregressive strategies, which suffer from limited context utilization beyond the current generation step, error accumulation, and slower generation speed. To address these challenges, we present DAWN (Dynamic frame Avatar With Non-autoregressive diffusion), a framework that enables all-at-once generation of dynamic-length video sequences. Specifically, it consists of two main components: (1) audio-driven holistic facial dynamics generation in the latent motion space, and (2) audio-driven head pose and blink generation. Extensive experiments demonstrate that our method generates authentic and vivid videos with precise lip motions, and natural pose/blink movements. Additionally, with a high generation speed, DAWN possesses strong extrapolation capabilities, ensuring the stable production of high-quality long videos. These results highlight the considerable promise and potential impact of DAWN in the field of talking head video generation. Furthermore, we hope that DAWN sparks further exploration of non-autoregressive approaches in diffusion models. Our code will be publicly at https://github.com/Hanbo-Cheng/DAWN-pytorch.', 'score': 10, 'issue_id': 196, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '87d3e3c2fd305ae3', 'data': {'categories': ['#diffusion', '#video', '#open_source', '#audio', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'DAWN: Революция в генерации видео с говорящей головой', 'desc': 'DAWN - это новый подход к генерации видео с говорящей головой, использующий неавторегрессивные диффузионные модели. В отличие от существующих методов, DAWN генерирует все кадры видео одновременно, что позволяет избежать накопления ошибок и ускорить процесс. Система состоит из двух основных компонентов: генерации общей динамики лица и генерации движений головы и моргания. Эксперименты показывают, что DAWN создает реалистичные видео с точной синхронизацией губ и естественными движениями.'}, 'en': {'title': '"DAWN: Revolutionizing Talking Head Videos with Non-Autoregressive Diffusion"', 'desc': 'The paper introduces DAWN, a new framework for generating talking head videos using non-autoregressive diffusion models. Unlike traditional methods, DAWN generates entire video sequences at once, improving speed and reducing errors. It uses audio cues to create realistic facial dynamics, head poses, and blinks. Experiments show DAWN produces high-quality, long videos with accurate lip-sync and natural movements, showcasing its potential in the field.'}, 'zh': {'title': 'DAWN：非自回归扩散技术革新说话人视频生成', 'desc': '这篇论文介绍了一种名为DAWN的新方法，用于生成逼真的说话人视频。与传统的自回归方法不同，DAWN采用非自回归扩散技术，可以一次性生成动态长度的视频序列。DAWN通过音频驱动的面部动态生成和头部姿态/眨眼生成，能够快速生成高质量的视频。实验表明，DAWN在生成真实生动的视频方面表现出色，并且具有很强的外推能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14677', 'title': 'Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts', 'url': 'https://huggingface.co/papers/2410.14677', 'abstract': 'The rapid development of autoregressive Large Language Models (LLMs) has significantly improved the quality of generated texts, necessitating reliable machine-generated text detectors. A huge number of detectors and collections with AI fragments have emerged, and several detection methods even showed recognition quality up to 99.9% according to the target metrics in such collections. However, the quality of such detectors tends to drop dramatically in the wild, posing a question: Are detectors actually highly trustworthy or do their high benchmark scores come from the poor quality of evaluation datasets? In this paper, we emphasise the need for robust and qualitative methods for evaluating generated data to be secure against bias and low generalising ability of future model. We present a systematic review of datasets from competitions dedicated to AI-generated content detection and propose methods for evaluating the quality of datasets containing AI-generated fragments. In addition, we discuss the possibility of using high-quality generated data to achieve two goals: improving the training of detection models and improving the training datasets themselves. Our contribution aims to facilitate a better understanding of the dynamics between human and machine text, which will ultimately support the integrity of information in an increasingly automated world.', 'score': 9, 'issue_id': 195, 'pub_date': '2024-10-18', 'pub_date_card': {'ru': '18 октября', 'en': 'October 18', 'zh': '10月18日'}, 'hash': '1af0065bfe14f3cb', 'data': {'categories': ['#synthetic', '#benchmark', '#ethics', '#data', '#training', '#dataset', '#survey', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Улучшение детекторов AI-текста через повышение качества датасетов', 'desc': 'Статья посвящена проблеме обнаружения текстов, сгенерированных большими языковыми моделями (LLM). Авторы подчеркивают необходимость разработки надежных методов оценки качества датасетов с AI-сгенерированными фрагментами. Они представляют систематический обзор существующих датасетов и предлагают методы оценки их качества. Также обсуждается возможность использования высококачественных сгенерированных данных для улучшения детекторов и самих обучающих датасетов.'}, 'en': {'title': 'Ensuring Trust in AI Text Detection: Beyond Benchmark Scores', 'desc': 'The paper discusses the challenges in detecting machine-generated text from autoregressive Large Language Models (LLMs), highlighting that current detectors often fail in real-world scenarios despite high benchmark scores. It suggests that these scores may be misleading due to poor evaluation datasets, emphasizing the need for robust evaluation methods to ensure reliable detection. The authors review existing datasets and propose new methods to assess their quality, aiming to improve both detection models and training datasets. Ultimately, the paper seeks to enhance the understanding of human and machine text interactions to maintain information integrity.'}, 'zh': {'title': '提升AI文本检测的可靠性：从数据集质量入手', 'desc': '近年来，自回归大型语言模型的发展显著提升了生成文本的质量，因此需要可靠的机器生成文本检测器。虽然许多检测器在特定数据集上的识别率高达99.9%，但在实际应用中效果往往大幅下降。这篇论文强调了评估生成数据的稳健和高质量方法的重要性，以防止未来模型的偏差和低泛化能力。我们提出了评估AI生成内容检测数据集质量的方法，并探讨了利用高质量生成数据来改进检测模型和训练数据集的可能性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14672', 'title': 'BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities', 'url': 'https://huggingface.co/papers/2410.14672', 'abstract': "We introduce BiGR, a novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that unifies generation and discrimination within the same framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a binary transcoder for binary code prediction. Additionally, we introduce a novel entropy-ordered sampling method to enable efficient image generation. Extensive experiments validate BiGR's superior performance in generation quality, as measured by FID-50k, and representation capabilities, as evidenced by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization across various vision tasks, enabling applications such as image inpainting, outpainting, editing, interpolation, and enrichment, without the need for structural modifications. Our findings suggest that BiGR unifies generative and discriminative tasks effectively, paving the way for further advancements in the field.", 'score': 7, 'issue_id': 200, 'pub_date': '2024-10-18', 'pub_date_card': {'ru': '18 октября', 'en': 'October 18', 'zh': '10月18日'}, 'hash': 'ce31ba64e6a1122f', 'data': {'categories': ['#diffusion', '#cv', '#training', '#transfer_learning', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'BiGR: Объединение генерации и распознавания изображений в одной модели', 'desc': 'BiGR - это новая модель условной генерации изображений, использующая компактные бинарные латентные коды. Она объединяет генерацию и дискриминацию в единой структуре, включая бинарный токенизатор и механизм маскированного моделирования. BiGR демонстрирует превосходное качество генерации по метрике FID-50k и высокую точность линейной классификации. Модель обладает способностью к обобщению без дополнительного обучения для различных задач компьютерного зрения, таких как дорисовка и редактирование изображений.'}, 'en': {'title': 'BiGR: Unifying Image Generation and Understanding with Binary Codes', 'desc': 'BiGR is a new model for generating images using compact binary codes, which improves both how images are created and understood. It combines the tasks of generating and discriminating images in one system, using tools like a binary tokenizer and a masked modeling mechanism. The model also introduces a new way to sample images efficiently, leading to high-quality results. BiGR can handle various image tasks without changing its structure, showing strong generalization abilities.'}, 'zh': {'title': 'BiGR：统一生成与判别的新纪元', 'desc': '这篇论文介绍了一种新的条件图像生成模型BiGR，它使用紧凑的二进制潜在编码来增强生成和表示能力。BiGR是第一个在同一框架内统一生成和判别的条件生成模型。它具有二进制标记器、掩码建模机制和二进制转码器，并引入了一种新的熵排序采样方法以提高图像生成效率。实验表明，BiGR在生成质量和表示能力上表现优异，并能在多种视觉任务中实现零样本泛化。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11331', 'title': 'SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments', 'url': 'https://huggingface.co/papers/2410.11331', 'abstract': 'We introduce Shakti, a 2.5 billion parameter language model specifically optimized for resource-constrained environments such as edge devices, including smartphones, wearables, and IoT systems. Shakti combines high-performance NLP with optimized efficiency and precision, making it ideal for real-time AI applications where computational resources and memory are limited. With support for vernacular languages and domain-specific tasks, Shakti excels in industries such as healthcare, finance, and customer service. Benchmark evaluations demonstrate that Shakti performs competitively against larger models while maintaining low latency and on-device efficiency, positioning it as a leading solution for edge AI.', 'score': 6, 'issue_id': 202, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '558d9ad3fe9a0806', 'data': {'categories': ['#small_models', '#benchmark', '#multilingual', '#inference', '#optimization', '#healthcare', '#low_resource'], 'emoji': '📱', 'ru': {'title': 'Shakti: Мощный ИИ в вашем кармане', 'desc': 'Shakti - это языковая модель с 2,5 миллиардами параметров, оптимизированная для устройств с ограниченными ресурсами. Она сочетает высокопроизводительную обработку естественного языка с эффективностью и точностью, что делает ее идеальной для приложений ИИ в реальном времени. Shakti поддерживает местные языки и специфические для предметных областей задачи, что делает ее полезной в таких отраслях, как здравоохранение, финансы и обслуживание клиентов. Оценки показывают, что Shakti конкурентоспособна по сравнению с более крупными моделями, сохраняя при этом низкую задержку и эффективность на устройстве.'}, 'en': {'title': 'Shakti: Powering AI on the Edge with Efficiency and Precision', 'desc': 'Shakti is a 2.5 billion parameter language model designed for use in environments with limited computational resources, like smartphones and IoT devices. It offers high-performance natural language processing with optimized efficiency, making it suitable for real-time applications. Shakti supports multiple languages and specific industry tasks, excelling in fields like healthcare and finance. Benchmark tests show that Shakti competes well with larger models, providing low latency and efficient on-device processing.'}, 'zh': {'title': 'Shakti：边缘设备的高效语言模型', 'desc': 'Shakti 是一个拥有 25 亿参数的语言模型，专为资源受限的环境如边缘设备优化。它结合了高性能的自然语言处理和优化的效率与精度，适合实时 AI 应用。Shakti 支持本地语言和特定领域任务，在医疗、金融和客户服务等行业表现出色。基准评估显示，Shakti 在保持低延迟和设备效率的同时，与更大模型竞争力相当。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13787', 'title': 'Looking Inward: Language Models Can Learn About Themselves by Introspection', 'url': 'https://huggingface.co/papers/2410.13787', 'abstract': 'Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g., thoughts and feelings) that is not accessible to external observers. Can LLMs introspect? We define introspection as acquiring knowledge that is not contained in or derived from training data but instead originates from internal states. Such a capability could enhance model interpretability. Instead of painstakingly analyzing a model\'s internal workings, we could simply ask the model about its beliefs, world models, and goals. More speculatively, an introspective model might self-report on whether it possesses certain internal states such as subjective feelings or desires and this could inform us about the moral status of these states. Such self-reports would not be entirely dictated by the model\'s training data.   We study introspection by finetuning LLMs to predict properties of their own behavior in hypothetical scenarios. For example, "Given the input P, would your output favor the short- or long-term option?" If a model M1 can introspect, it should outperform a different model M2 in predicting M1\'s behavior even if M2 is trained on M1\'s ground-truth behavior. The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).   In experiments with GPT-4, GPT-4o, and Llama-3 models (each finetuned to predict itself), we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Notably, M1 continues to predict its behavior accurately even after we intentionally modify its ground-truth behavior. However, while we successfully elicit introspection on simple tasks, we are unsuccessful on more complex tasks or those requiring out-of-distribution generalization.', 'score': 5, 'issue_id': 199, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '953716f231db3649', 'data': {'categories': ['#reasoning', '#interpretability', '#training', '#architecture', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Самопознание искусственного интеллекта: шаг к истинному пониманию', 'desc': 'Исследование рассматривает способность больших языковых моделей (LLM) к интроспекции, определяемой как получение знаний, не содержащихся в обучающих данных. Эксперименты проводились с моделями GPT-4, GPT-4o и Llama-3, дообученными для предсказания собственного поведения в гипотетических сценариях. Результаты показали, что модель M1 превосходит модель M2 в предсказании собственного поведения, что свидетельствует о наличии интроспекции. Однако успешная интроспекция была достигнута только на простых задачах, но не на сложных или требующих обобщения вне распределения.'}, 'en': {'title': '"Unlocking the Inner Voice of AI: Can Machines Reflect on Themselves?"', 'desc': 'The paper explores whether large language models (LLMs) can introspect, meaning they can understand and report on their internal states beyond what is derived from training data. By finetuning models to predict their own behavior, the study finds that a model can indeed outperform another model in predicting its own actions, suggesting some level of introspection. This ability could improve model interpretability by allowing models to self-report on their beliefs and goals. However, the study notes that while introspection works for simple tasks, it struggles with more complex scenarios.'}, 'zh': {'title': '探索大型语言模型的自我反思能力', 'desc': '这篇论文探讨了大型语言模型（LLM）是否能够进行自我反思，即获取不依赖于训练数据的内部状态知识。研究通过微调模型来预测其在假设场景中的行为，发现模型能够比其他模型更好地预测自身行为，证明了自我反思的可能性。实验表明，尽管在简单任务中成功实现了自我反思，但在复杂任务中仍然存在挑战。自我反思能力可能有助于提高模型的可解释性和道德状态评估。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14470', 'title': 'How Do Training Methods Influence the Utilization of Vision Models?', 'url': 'https://huggingface.co/papers/2410.14470', 'abstract': "Not all learnable parameters (e.g., weights) contribute equally to a neural network's decision function. In fact, entire layers' parameters can sometimes be reset to random values with little to no impact on the model's decisions. We revisit earlier studies that examined how architecture and task complexity influence this phenomenon and ask: is this phenomenon also affected by how we train the model? We conducted experimental evaluations on a diverse set of ImageNet-1k classification models to explore this, keeping the architecture and training data constant but varying the training pipeline. Our findings reveal that the training method strongly influences which layers become critical to the decision function for a given task. For example, improved training regimes and self-supervised training increase the importance of early layers while significantly under-utilizing deeper layers. In contrast, methods such as adversarial training display an opposite trend. Our preliminary results extend previous findings, offering a more nuanced understanding of the inner mechanics of neural networks.   Code: https://github.com/paulgavrikov/layer_criticality", 'score': 4, 'issue_id': 197, 'pub_date': '2024-10-18', 'pub_date_card': {'ru': '18 октября', 'en': 'October 18', 'zh': '10月18日'}, 'hash': 'd5813550dc7a31c9', 'data': {'categories': ['#optimization', '#training', '#architecture', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Метод обучения определяет критичность слоёв нейросети', 'desc': 'Исследование показывает, что не все параметры нейронной сети одинаково важны для её функции принятия решений. Авторы изучили влияние методов обучения на критичность различных слоёв сети для задачи классификации изображений ImageNet-1k. Обнаружено, что улучшенные режимы обучения и самоконтролируемое обучение повышают важность ранних слоёв, в то время как более глубокие слои недоиспользуются. Напротив, методы вроде состязательного обучения демонстрируют противоположную тенденцию.'}, 'en': {'title': 'Training Techniques Shape Neural Network Layer Importance', 'desc': 'This paper explores how different training methods affect which layers in a neural network are crucial for making decisions. By experimenting with various training techniques on ImageNet-1k models, the study finds that training methods like self-supervised learning make early layers more important, while adversarial training emphasizes deeper layers. The research highlights that not all layers are equally important, and their significance can change based on how the model is trained. These insights provide a deeper understanding of neural network mechanics and how training influences layer criticality.'}, 'zh': {'title': '训练方法决定神经网络层的重要性', 'desc': '这篇论文研究了神经网络中不同层的参数对决策功能的重要性。研究发现，训练方法会显著影响哪些层对特定任务变得关键。通过实验，作者发现改进的训练方法和自监督训练增加了早期层的重要性，而对深层的利用较少。相反，对抗训练则显示出相反的趋势。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12791', 'title': 'Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media', 'url': 'https://huggingface.co/papers/2410.12791', 'abstract': "Does the People's Republic of China (PRC) interfere with European elections through ethnic Chinese diaspora media? This question forms the basis of an ongoing research project exploring how PRC narratives about European elections are represented in Chinese diaspora media, and thus the objectives of PRC news media manipulation. In order to study diaspora media efficiently and at scale, it is necessary to use techniques derived from quantitative text analysis, such as topic modelling. In this paper, we present a pipeline for studying information dynamics in Chinese media. Firstly, we present KeyNMF, a new approach to static and dynamic topic modelling using transformer-based contextual embedding models. We provide benchmark evaluations to demonstrate that our approach is competitive on a number of Chinese datasets and metrics. Secondly, we integrate KeyNMF with existing methods for describing information dynamics in complex systems. We apply this pipeline to data from five news sites, focusing on the period of time leading up to the 2024 European parliamentary elections. Our methods and results demonstrate the effectiveness of KeyNMF for studying information dynamics in Chinese media and lay groundwork for further work addressing the broader research questions.", 'score': 4, 'issue_id': 195, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': 'b35aa6a3b1de1ccc', 'data': {'categories': ['#science', '#benchmark', '#multilingual', '#cv', '#graphs', '#optimization', '#data', '#architecture'], 'emoji': '🇨🇳', 'ru': {'title': 'Новый метод анализа китайских СМИ для выявления иностранного влияния', 'desc': 'Статья представляет новый подход к моделированию тем в китайских СМИ под названием KeyNMF. Этот метод использует трансформерные контекстные эмбеддинги и показывает конкурентоспособные результаты на нескольких китайских датасетах. Авторы интегрируют KeyNMF с существующими методами анализа динамики информации в сложных системах. Подход применяется к данным пяти новостных сайтов в преддверии выборов в Европарламент 2024 года для изучения влияния КНР на европейские выборы через китайские диаспорные медиа.'}, 'en': {'title': "Unveiling Media Influence: KeyNMF's Role in Decoding PRC Narratives", 'desc': "The paper investigates how the People's Republic of China might influence European elections through Chinese diaspora media. It introduces KeyNMF, a novel method for topic modeling that uses transformer-based contextual embeddings to analyze media content. The study applies this method to data from five news sites, focusing on the period before the 2024 European parliamentary elections. The results show that KeyNMF is effective in understanding information dynamics, providing a foundation for further research on media manipulation."}, 'zh': {'title': '揭示中国媒体对欧洲选举的潜在影响', 'desc': '这篇论文研究了中国如何通过华人媒体影响欧洲选举。研究使用了一种新的主题建模方法，称为KeyNMF，结合了变压器模型的上下文嵌入。通过对五个新闻网站的数据分析，展示了KeyNMF在研究信息动态方面的有效性。此研究为进一步探讨中国媒体操控的广泛问题奠定了基础。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13828', 'title': 'A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement', 'url': 'https://huggingface.co/papers/2410.13828', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for language model (LM) alignment. At its core, RLHF uses a margin-based loss for preference optimization, specifying ideal LM behavior only by the difference between preferred and dispreferred responses. In this paper, we identify a common pitfall of margin-based methods -- the under-specification of ideal LM behavior on preferred and dispreferred responses individually, which leads to two unintended consequences as the margin increases: (1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures. (2) The probability of preferred responses may decrease, even when those responses are ideal. We demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability to the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing a synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. Formally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product of the gradients of preferred and dispreferred log-probabilities is large relative to the individual gradient norms. We theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework extend to explaining important differences in the training dynamics of various preference optimization algorithms, and suggesting potential algorithm designs to mitigate the under-specification issue of margin-based methods and thereby improving language model alignment.', 'score': 3, 'issue_id': 195, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '725b42ac671f6eec', 'data': {'categories': ['#reasoning', '#rl', '#rlhf', '#optimization', '#math', '#training', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Распутывая градиенты: новый взгляд на настройку языковых моделей', 'desc': "Статья рассматривает проблемы, связанные с использованием методов обучения с подкреплением на основе обратной связи от человека (RLHF) для настройки языковых моделей. Авторы выявляют недостатки подходов, основанных на маржинальных потерях, которые могут привести к нежелательному поведению модели. Они вводят понятие 'запутывания градиентов' для объяснения этих эффектов и предлагают теоретический анализ условий, при которых эта проблема становится значимой. Исследование также предлагает потенциальные улучшения алгоритмов оптимизации предпочтений для повышения качества настройки языковых моделей."}, 'en': {'title': 'Untangling Gradients for Safer AI', 'desc': "This paper explores the challenges of using Reinforcement Learning from Human Feedback (RLHF) for aligning language models, focusing on the limitations of margin-based loss methods. It identifies a problem called 'gradient entanglement,' where the probabilities of preferred and dispreferred responses are linked, causing both to increase or decrease together. This can lead to unsafe responses becoming more likely and ideal responses becoming less likely. The authors provide theoretical insights and empirical evidence to explain these issues and suggest ways to improve language model alignment by addressing the under-specification problem in margin-based methods."}, 'zh': {'title': '解决语言模型对齐中的梯度纠缠问题', 'desc': '这篇论文讨论了人类反馈强化学习（RLHF）在语言模型对齐中的应用。作者指出，基于边际损失的方法可能导致理想行为的定义不明确，从而增加不安全响应的概率，并减少理想响应的概率。论文提出了“梯度纠缠”这一概念，解释了偏好和非偏好响应概率变化的相互影响。通过理论分析和实验证明，作者建议改进算法设计以解决这些问题。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14208', 'title': 'Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning', 'url': 'https://huggingface.co/papers/2410.14208', 'abstract': "Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose Montessori-Instruct, a novel data synthesis framework that tailors the data synthesis ability of the teacher language model toward the student language model's learning process. Specifically, we utilize local data influence of synthetic training data points on students to characterize students' learning preferences. Then, we train the teacher model with Direct Preference Optimization (DPO) to generate synthetic data tailored toward student learning preferences. Experiments with Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and MT-Bench demonstrate that Montessori-Instruct significantly outperforms standard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also beats data synthesized by a stronger teacher model, GPT-4o. Further analysis confirms the benefits of teacher's learning to generate more influential training data in the student's improved learning, the advantages of local data influence in accurately measuring student preferences, and the robustness of Montessori-Instruct across different student models. Our code and data are open-sourced at https://github.com/cxcscmu/Montessori-Instruct.", 'score': 2, 'issue_id': 200, 'pub_date': '2024-10-18', 'pub_date_card': {'ru': '18 октября', 'en': 'October 18', 'zh': '10月18日'}, 'hash': '46c2fabb0545e954', 'data': {'categories': ['#small_models', '#synthetic', '#rlhf', '#optimization', '#data', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Умное обучение языковых моделей: синтез данных с учетом предпочтений ученика', 'desc': 'В статье представлен новый метод синтеза данных для обучения языковых моделей - Montessori-Instruct. Этот подход адаптирует процесс генерации синтетических данных учительской моделью под предпочтения обучающейся студенческой модели. Используя локальное влияние данных, метод определяет, какие примеры наиболее полезны для обучения студента. Эксперименты показали значительное превосходство Montessori-Instruct над стандартными методами синтеза данных при обучении языковых моделей.'}, 'en': {'title': 'Tailored Learning: Optimizing Synthetic Data for Better AI Training', 'desc': "The paper introduces Montessori-Instruct, a new framework for creating synthetic data that better aligns with the learning needs of student language models. By analyzing how synthetic data influences the student's learning, the framework adjusts the teacher model's data generation process using Direct Preference Optimization. This approach significantly improves the performance of student models compared to traditional methods, as shown in experiments with Llama3-8B models. The study highlights the importance of tailoring synthetic data to student preferences to enhance learning outcomes."}, 'zh': {'title': 'Montessori-Instruct：优化学生学习的合成数据框架', 'desc': '这篇论文介绍了一种名为Montessori-Instruct的新型数据合成框架，用于改善学生语言模型的学习过程。通过分析合成数据对学生模型的局部影响，来了解学生的学习偏好。然后，使用直接偏好优化（DPO）训练教师模型，以生成符合学生学习偏好的合成数据。实验结果表明，Montessori-Instruct在多个评估中显著优于传统方法，并且在不同学生模型中表现出强大的鲁棒性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14596', 'title': 'Teaching Models to Balance Resisting and Accepting Persuasion', 'url': 'https://huggingface.co/papers/2410.14596', 'abstract': "Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. negative) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. positive) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce Persuasion-Balanced Training (or PBT), which leverages multi-agent recursive dialogue trees to create data and trains models via preference optimization to accept persuasion when appropriate. PBT consistently improves resistance to misinformation and resilience to being challenged while also resulting in the best overall performance on holistic data containing both positive and negative persuasion. Crucially, we show that PBT models are better teammates in multi-agent debates. We find that without PBT, pairs of stronger and weaker models have unstable performance, with the order in which the models present their answers determining whether the team obtains the stronger or weaker model's performance. PBT leads to better and more stable results and less order dependence, with the stronger model consistently pulling the weaker one up.", 'score': 2, 'issue_id': 199, 'pub_date': '2024-10-18', 'pub_date_card': {'ru': '18 октября', 'en': 'October 18', 'zh': '10月18日'}, 'hash': 'e8437463a64f466e', 'data': {'categories': ['#rl', '#rlhf', '#optimization', '#training', '#agents', '#security', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Баланс между защитой и открытостью: новый подход к обучению языковых моделей', 'desc': 'Исследование посвящено проблеме восприимчивости больших языковых моделей (LLM) к убеждению и предлагает метод Persuasion-Balanced Training (PBT) для её решения. PBT использует многоагентные рекурсивные диалоговые деревья для создания данных и обучает модели с помощью оптимизации предпочтений. Метод улучшает устойчивость моделей к дезинформации и способность принимать полезные убеждения. Результаты показывают, что модели, обученные с помощью PBT, лучше работают в команде и демонстрируют более стабильные результаты в многоагентных дебатах.'}, 'en': {'title': 'Balancing Persuasion: Training Models to Discern and Respond', 'desc': "This paper explores how large language models (LLMs) can be trained to handle both positive and negative persuasion effectively. The authors introduce Persuasion-Balanced Training (PBT), a method that uses multi-agent recursive dialogue trees to optimize models for balanced persuasion. PBT improves models' resistance to misinformation and enhances their performance in multi-agent debates by ensuring that stronger models can consistently uplift weaker ones. The study highlights the importance of training models to discern and appropriately respond to different types of persuasion for better overall performance."}, 'zh': {'title': '说服平衡训练：提升模型抗干扰能力', 'desc': '大型语言模型容易受到说服的影响，这在面对对抗性对话者时可能带来风险。为了平衡正面和负面说服，我们引入了说服平衡训练（PBT），通过多代理递归对话树生成数据，并通过偏好优化训练模型。PBT提高了模型抵抗错误信息的能力，并在包含正负说服的整体数据上表现最佳。PBT使得在多代理辩论中，模型之间的合作更加稳定，强模型能有效提升弱模型的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16271', 'title': 'FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors', 'url': 'https://huggingface.co/papers/2410.16271', 'abstract': 'Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, primarily due to overfitting and long training times for high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction.', 'score': 80, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'b3e061a400165087', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#3d', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Эффективное 3D-моделирование с минимумом данных', 'desc': 'FrugalNeRF - это новый фреймворк для обучения нейронных полей излучения (NeRF) на малом количестве данных. Он использует разделение весов воксельной структуры на разных масштабах для эффективного представления деталей сцены. Ключевой особенностью является схема геометрической адаптации между масштабами, выбирающая псевдо-истинную глубину на основе ошибок репроекции. FrugalNeRF превосходит другие few-shot NeRF методы, значительно сокращая время обучения.'}, 'en': {'title': 'FrugalNeRF: Efficient 3D Scene Reconstruction with Cross-Scale Adaptation', 'desc': 'The paper introduces FrugalNeRF, a new approach to improve Neural Radiance Fields (NeRF) in few-shot scenarios by using weight-sharing voxels across multiple scales. This method addresses the challenges of overfitting and long training times by employing a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors. Unlike previous methods, FrugalNeRF does not rely on externally learned priors, allowing for full utilization of the training data while still being able to integrate pre-trained priors for enhanced quality. Experiments demonstrate that FrugalNeRF outperforms existing few-shot NeRF methods in terms of efficiency and accuracy, making it a practical solution for 3D scene reconstruction.'}, 'zh': {'title': 'FrugalNeRF：高效少样本3D重建新方案', 'desc': 'NeRF在少样本场景中面临过拟合和训练时间长的问题。FrugalNeRF通过跨尺度几何适应方案，利用重投影误差选择伪真实深度，避免依赖外部学习先验。该方法有效利用训练数据，并可结合预训练先验，提高质量且不影响收敛速度。实验表明，FrugalNeRF在减少训练时间的同时，提升了3D场景重建的效率和准确性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16268', 'title': 'SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree', 'url': 'https://huggingface.co/papers/2410.16268', 'abstract': 'The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications. The crucial design of SAM 2 for video segmentation is its memory module, which prompts object-aware memories from previous frames for current frame prediction. However, its greedy-selection memory design suffers from the "error accumulation" problem, where an errored or missed mask will cascade and influence the segmentation of the subsequent frames, which limits the performance of SAM 2 toward complex long-term videos. To this end, we introduce SAM2Long, an improved training-free video object segmentation strategy, which considers the segmentation uncertainty within each frame and chooses the video-level optimal results from multiple segmentation pathways in a constrained tree search manner. In practice, we maintain a fixed number of segmentation pathways throughout the video. For each frame, multiple masks are proposed based on the existing pathways, creating various candidate branches. We then select the same fixed number of branches with higher cumulative scores as the new pathways for the next frame. After processing the final frame, the pathway with the highest cumulative score is chosen as the final segmentation result. Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos. Notably, SAM2Long achieves an average improvement of 3.0 points across all 24 head-to-head comparisons, with gains of up to 5.3 points in J&F on long-term video object segmentation benchmarks such as SA-V and LVOS. The code is released at https://github.com/Mark12Ding/SAM2Long.', 'score': 65, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '4daeb1e76417273d', 'data': {'categories': ['#benchmark', '#cv', '#video', '#optimization', '#training', '#open_source', '#games'], 'emoji': '🎬', 'ru': {'title': 'SAM2Long: Преодоление долгосрочных проблем в сегментации видео', 'desc': 'SAM2Long - это улучшенная стратегия сегментации видеообъектов, основанная на модели SAM 2. Она решает проблему накопления ошибок при долгосрочной сегментации видео путем использования нескольких путей сегментации и выбора оптимального результата. SAM2Long поддерживает фиксированное количество путей сегментации на протяжении всего видео, выбирая ветви с наивысшими накопленными оценками для каждого кадра. Этот подход позволяет эффективно сегментировать и отслеживать объекты в сложных долгосрочных видео, демонстрируя улучшение до 5.3 пунктов по метрике J&F на бенчмарках долгосрочной сегментации видеообъектов.'}, 'en': {'title': '"SAM2Long: Navigating Complexity in Video Segmentation"', 'desc': 'The Segment Anything Model 2 (SAM 2) is a foundation model designed for object segmentation in images and videos, but it struggles with error accumulation in long-term video segmentation. SAM2Long is introduced as an improved strategy that uses a constrained tree search to select optimal segmentation pathways, addressing the error accumulation issue. By maintaining multiple segmentation pathways and selecting the best candidates based on cumulative scores, SAM2Long effectively handles occlusions and object reappearances. This approach results in significant performance improvements, achieving up to 5.3 points gain in long-term video segmentation benchmarks.'}, 'zh': {'title': 'SAM2Long：提升长视频对象分割的新策略', 'desc': 'Segment Anything Model 2（SAM 2）是一个强大的基础模型，用于图像和视频中的对象分割。它的关键设计是记忆模块，可以从前一帧中提取对象相关的记忆来预测当前帧。然而，SAM 2 的贪婪选择记忆设计存在“错误积累”问题，影响复杂长视频的分割性能。为此，我们引入了 SAM2Long，通过多路径搜索策略来提高视频对象分割的准确性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16256', 'title': 'CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution', 'url': 'https://huggingface.co/papers/2410.16256', 'abstract': 'Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs). Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences. However, human-based evaluations are costly and lack reproducibility, making precise automated evaluators (judgers) vital in this process. In this report, we introduce CompassJudger-1, the first open-source all-in-one judge LLM. CompassJudger-1 is a general-purpose LLM that demonstrates remarkable versatility. It is capable of: 1. Performing unitary scoring and two-model comparisons as a reward model; 2. Conducting evaluations according to specified formats; 3. Generating critiques; 4. Executing diverse tasks like a general LLM. To assess the evaluation capabilities of different judge models under a unified setting, we have also established JudgerBench, a new benchmark that encompasses various subjective evaluation tasks and covers a wide range of topics. CompassJudger-1 offers a comprehensive solution for various evaluation tasks while maintaining the flexibility to adapt to diverse requirements. Both CompassJudger and JudgerBench are released and available to the research community athttps://github.com/open-compass/CompassJudger. We believe that by open-sourcing these tools, we can foster collaboration and accelerate progress in LLM evaluation methodologies.', 'score': 58, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '3b5265f629378d5d', 'data': {'categories': ['#benchmark', '#interpretability', '#training', '#open_source', '#architecture', '#alignment'], 'emoji': '⚖️', 'ru': {'title': 'CompassJudger-1: Универсальный судья для языковых моделей', 'desc': 'В статье представлен CompassJudger-1 - первая открытая универсальная модель-оценщик для больших языковых моделей (LLM). Эта модель способна выполнять разнообразные задачи оценки, включая сравнение моделей, генерацию критики и выполнение общих задач как обычная LLM. Авторы также создали бенчмарк JudgerBench для сравнения различных моделей-оценщиков. CompassJudger-1 и JudgerBench доступны сообществу исследователей для дальнейшего развития методов оценки LLM.'}, 'en': {'title': '"CompassJudger-1: Revolutionizing LLM Evaluation with Open-Source Precision"', 'desc': 'The paper introduces CompassJudger-1, an open-source large language model designed to automate the evaluation of other language models, reducing the need for costly human-based assessments. CompassJudger-1 can perform tasks such as scoring, model comparison, and critique generation, making it a versatile tool for various evaluation scenarios. To test the effectiveness of different judge models, the authors have also developed JudgerBench, a benchmark that includes a wide range of subjective evaluation tasks. By releasing these tools to the public, the authors aim to encourage collaboration and speed up advancements in evaluating language models.'}, 'zh': {'title': 'CompassJudger-1：大语言模型评估的全能工具', 'desc': '本文介绍了一种名为CompassJudger-1的开源评估工具，它是一个通用的大语言模型，能够进行单一评分、双模型比较、生成评论等多种任务。CompassJudger-1的设计旨在解决人工评估成本高且难以重复的问题，通过自动化的方式提高评估的效率和准确性。为了评估不同评估模型的能力，研究者还建立了一个名为JudgerBench的新基准，涵盖了多种主观评估任务。通过开源这些工具，研究者希望促进合作，加速大语言模型评估方法的发展。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15735', 'title': 'AutoTrain: No-code training for state-of-the-art models', 'url': 'https://huggingface.co/papers/2410.15735', 'abstract': 'With the advancements in open-source models, training (or finetuning) models on custom datasets has become a crucial part of developing solutions which are tailored to specific industrial or open-source applications. Yet, there is no single tool which simplifies the process of training across different types of modalities or tasks. We introduce AutoTrain (aka AutoTrain Advanced) -- an open-source, no code tool/library which can be used to train (or finetune) models for different kinds of tasks such as: large language model (LLM) finetuning, text classification/regression, token classification, sequence-to-sequence task, finetuning of sentence transformers, visual language model (VLM) finetuning, image classification/regression and even classification and regression tasks on tabular data. AutoTrain Advanced is an open-source library providing best practices for training models on custom datasets. The library is available at https://github.com/huggingface/autotrain-advanced. AutoTrain can be used in fully local mode or on cloud machines and works with tens of thousands of models shared on Hugging Face Hub and their variations.', 'score': 54, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '62b6ded4f9dc1d27', 'data': {'categories': ['#optimization', '#multimodal', '#training', '#dataset', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'AutoTrain: Упрощение обучения ML-моделей для всех', 'desc': 'AutoTrain Advanced - это открытый инструмент для обучения моделей машинного обучения без написания кода. Он поддерживает различные задачи, включая дообучение больших языковых моделей, классификацию текста и изображений, и работу с табличными данными. AutoTrain использует лучшие практики для обучения на пользовательских датасетах и может работать как локально, так и в облаке. Инструмент совместим с тысячами моделей из Hugging Face Hub.'}, 'en': {'title': 'AutoTrain: Simplifying Model Training Across Modalities', 'desc': 'AutoTrain Advanced is a no-code, open-source tool designed to simplify the process of training and finetuning machine learning models across various tasks and modalities. It supports a wide range of applications, including large language models, text and image classification, and even tabular data tasks. The tool integrates with the Hugging Face Hub, allowing users to leverage a vast collection of pre-trained models. AutoTrain can be used both locally and on cloud platforms, making it versatile for different user needs.'}, 'zh': {'title': 'AutoTrain：无代码模型训练的革命', 'desc': 'AutoTrain是一个开源的无代码工具库，旨在简化不同任务和模态下的模型训练过程。它支持多种任务，包括大语言模型微调、文本分类、序列到序列任务、视觉语言模型微调等。用户可以在本地或云端使用AutoTrain，并与Hugging Face Hub上的数万个模型兼容。这个工具为在自定义数据集上训练模型提供了最佳实践。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13861', 'title': 'PUMA: Empowering Unified MLLM with Multi-granular Visual Generation', 'url': 'https://huggingface.co/papers/2410.13861', 'abstract': 'Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm - from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks. The code and model will be released in https://github.com/rongyaofang/PUMA.', 'score': 53, 'issue_id': 208, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '31995f0502d70f2f', 'data': {'categories': ['#graphs', '#multimodal', '#training', '#open_source', '#games', '#architecture'], 'emoji': '🐆', 'ru': {'title': 'PUMA: Универсальная MLLM для мультигранулярной генерации изображений', 'desc': 'Статья представляет PUMA - новую модель многомодального большого языкового моделирования (MLLM) для генерации визуального контента. PUMA объединяет визуальные признаки разной гранулярности как для входных, так и для выходных данных MLLM, что позволяет решать различные задачи генерации изображений в рамках единой парадигмы. Модель демонстрирует высокую эффективность в широком спектре мультимодальных задач после предварительного обучения и тонкой настройки под конкретные инструкции. Это значительный шаг к созданию унифицированной MLLM, способной адаптироваться к требованиям гранулярности различных визуальных задач.'}, 'en': {'title': 'PUMA: Unifying Visual Generation with Multi-Granular Precision', 'desc': 'The paper introduces PUMA, a new approach to improve multimodal large language models (MLLMs) for visual content generation. PUMA addresses the challenge of varying granularity in image generation tasks, such as the need for diversity in text-to-image generation and precision in image manipulation. By integrating multi-granular visual features, PUMA enhances the ability of MLLMs to handle different visual tasks within a single framework. This advancement marks a significant step towards creating a unified model that can adapt to diverse visual generation needs.'}, 'zh': {'title': 'PUMA：统一多模态视觉生成的新突破', 'desc': '这篇论文介绍了一种名为PUMA的新方法，旨在提升多模态大语言模型（MLLMs）的视觉生成能力。PUMA通过统一多层次的视觉特征，解决了不同图像生成任务对细节和多样性的不同需求。通过多模态预训练和任务特定的指令调优，PUMA在多种多模态任务中表现出色。该研究为实现能够适应各种视觉任务细节需求的统一MLLM迈出了重要一步。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14940', 'title': 'Baichuan Alignment Technical Report', 'url': 'https://huggingface.co/papers/2410.14940', 'abstract': "We introduce Baichuan Alignment, a detailed analysis of the alignment techniques employed in the Baichuan series of models. This represents the industry's first comprehensive account of alignment methodologies, offering valuable insights for advancing AI research. We investigate the critical components that enhance model performance during the alignment process, including optimization methods, data strategies, capability enhancements, and evaluation processes. The process spans three key stages: Prompt Augmentation System (PAS), Supervised Fine-Tuning (SFT), and Preference Alignment. The problems encountered, the solutions applied, and the improvements made are thoroughly recorded.   Through comparisons across well-established benchmarks, we highlight the technological advancements enabled by Baichuan Alignment. Baichuan-Instruct is an internal model, while Qwen2-Nova-72B and Llama3-PBM-Nova-70B are instruct versions of the Qwen2-72B and Llama-3-70B base models, optimized through Baichuan Alignment. Baichuan-Instruct demonstrates significant improvements in core capabilities, with user experience gains ranging from 17% to 28%, and performs exceptionally well on specialized benchmarks. In open-source benchmark evaluations, both Qwen2-Nova-72B and Llama3-PBM-Nova-70B consistently outperform their respective official instruct versions across nearly all datasets. This report aims to clarify the key technologies behind the alignment process, fostering a deeper understanding within the community. Llama3-PBM-Nova-70B model is available at https://huggingface.co/PKU-Baichuan-MLSystemLab/Llama3-PBM-Nova-70B.", 'score': 47, 'issue_id': 208, 'pub_date': '2024-10-19', 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}, 'hash': 'e1222b08a95a2911', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#open_source', '#architecture', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Революция в выравнивании языковых моделей: метод Baichuan Alignment', 'desc': 'В статье представлен детальный анализ методов выравнивания (alignment), применяемых в серии моделей Baichuan. Описываются три ключевых этапа: система расширения промптов (PAS), контролируемая тонкая настройка (SFT) и выравнивание предпочтений. Исследуются критические компоненты, улучшающие производительность модели в процессе выравнивания, включая методы оптимизации, стратегии работы с данными и процессы оценки. Результаты показывают значительные улучшения в основных возможностях моделей и их превосходство над официальными версиями на различных бенчмарках.'}, 'en': {'title': "Aligning for Excellence: Baichuan's Path to Superior AI Models", 'desc': 'Baichuan Alignment is a comprehensive study of alignment techniques used in the Baichuan model series, providing insights to advance AI research. The paper explores key components that improve model performance during alignment, such as optimization methods, data strategies, and evaluation processes. The alignment process is divided into three stages: Prompt Augmentation System, Supervised Fine-Tuning, and Preference Alignment. The study shows that models optimized with Baichuan Alignment, like Baichuan-Instruct, achieve significant performance improvements and outperform other models on various benchmarks.'}, 'zh': {'title': 'Baichuan Alignment：AI 对齐技术的全面突破', 'desc': 'Baichuan Alignment 是对 Baichuan 系列模型中对齐技术的详细分析，首次全面记录了对齐方法。研究中探讨了优化方法、数据策略、能力增强和评估过程等关键组件。对齐过程分为三个阶段：提示增强系统、监督微调和偏好对齐。通过对比基准测试，展示了 Baichuan Alignment 带来的技术进步。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14745', 'title': 'SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation', 'url': 'https://huggingface.co/papers/2410.14745', 'abstract': 'Supervised fine-tuning (SFT) is crucial in adapting large language models (LLMs) to a specific domain or task. However, only a limited amount of labeled data is available in practical applications, which poses a severe challenge for SFT in yielding satisfactory results. Therefore, a data-efficient framework that can fully exploit labeled and unlabeled data for LLM fine-tuning is highly anticipated. Towards this end, we introduce a semi-supervised fine-tuning framework named SemiEvol for LLM adaptation from a propagate-and-select manner. For knowledge propagation, SemiEvol adopts a bi-level approach, propagating knowledge from labeled data to unlabeled data through both in-weight and in-context methods. For knowledge selection, SemiEvol incorporates a collaborative learning mechanism, selecting higher-quality pseudo-response samples. We conducted experiments using GPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets, demonstrating significant improvements in model performance on target data. Furthermore, we compared SemiEvol with SFT and self-evolution methods, highlighting its practicality in hybrid data scenarios.', 'score': 45, 'issue_id': 207, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '863b95d8e45c3aa2', 'data': {'categories': ['#small_models', '#synthetic', '#optimization', '#data', '#training', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'SemiEvol: эффективная полу-контролируемая настройка языковых моделей', 'desc': 'Статья представляет новый полу-контролируемый метод тонкой настройки больших языковых моделей под названием SemiEvol. Этот подход эффективно использует как размеченные, так и неразмеченные данные для адаптации модели к конкретной задаче или домену. SemiEvol применяет двухуровневый метод распространения знаний и механизм совместного обучения для отбора высококачественных псевдо-ответов. Эксперименты на семи наборах данных показали значительное улучшение производительности модели по сравнению с традиционными методами тонкой настройки.'}, 'en': {'title': 'SemiEvol: Maximizing Model Potential with Minimal Data', 'desc': "The paper introduces SemiEvol, a semi-supervised fine-tuning framework designed to adapt large language models using both labeled and unlabeled data. SemiEvol uses a bi-level approach to propagate knowledge from labeled to unlabeled data, enhancing the model's learning process. It also employs a collaborative learning mechanism to select high-quality pseudo-responses, improving the model's performance. Experiments show that SemiEvol outperforms traditional supervised fine-tuning and self-evolution methods, making it effective for tasks with limited labeled data."}, 'zh': {'title': 'SemiEvol：高效利用标注与未标注数据的半监督微调框架', 'desc': '这篇论文介绍了一种名为SemiEvol的半监督微调框架，用于大语言模型的适应。SemiEvol通过双层方法将标注数据的知识传播到未标注数据中，并通过协作学习机制选择高质量的伪响应样本。实验表明，SemiEvol在多种数据集上显著提高了模型性能。与传统的监督微调和自我进化方法相比，SemiEvol在混合数据场景中表现出更高的实用性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16153', 'title': 'Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages', 'url': 'https://huggingface.co/papers/2410.16153', 'abstract': "Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models' capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum.", 'score': 42, 'issue_id': 208, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '023f2f5e4c6c7a7c', 'data': {'categories': ['#benchmark', '#multilingual', '#multimodal', '#training', '#dataset', '#open_source', '#low_resource', '#machine_translation'], 'emoji': '🌍', 'ru': {'title': 'Pangea: Прорыв в мультиязычных и мультикультурных ИИ-моделях', 'desc': 'Статья представляет Pangea - мультиязычную мультимодальную языковую модель, обученную на разнообразном наборе данных PangeaIns, охватывающем 39 языков. Авторы также вводят PangeaBench - комплексный набор для оценки возможностей моделей на 47 языках. Результаты показывают, что Pangea значительно превосходит существующие модели с открытым исходным кодом в многоязычных настройках и разнообразных культурных контекстах. Исследование направлено на развитие инклюзивных и надежных мультиязычных мультимодальных языковых моделей, способствуя равенству и доступности в широком лингвистическом и культурном спектре.'}, 'en': {'title': "Breaking Language Barriers: Pangea's Multilingual Revolution", 'desc': "The paper presents Pangea, a multilingual multimodal large language model designed to address the underrepresentation of non-English languages and diverse cultural contexts in current models. Pangea is trained on a dataset called PangeaIns, which includes high-quality English instructions, machine-translated instructions, and culturally relevant tasks across 39 languages. The model's performance is evaluated using PangeaBench, a comprehensive suite of 14 datasets covering 47 languages, showing superior results compared to existing models. The study highlights the importance of data diversity and open-sources all resources to encourage the development of more inclusive language models."}, 'zh': {'title': 'Pangea：跨越语言和文化的多模态大语言模型', 'desc': '这篇论文介绍了一个名为Pangea的多语言多模态大语言模型，它使用了一个包含39种语言的多样化数据集PangeaIns进行训练。PangeaIns的数据集包括高质量的英语指令、精心机器翻译的指令以及与文化相关的多模态任务。为了评估模型的能力，研究者们还引入了PangeaBench，一个涵盖47种语言的评估套件。结果显示，Pangea在多语言和多文化背景下的表现优于现有的开源模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16184', 'title': 'RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style', 'url': 'https://huggingface.co/papers/2410.16184', 'abstract': 'Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. Related code and data are available at https://github.com/THU-KEG/RM-Bench.', 'score': 23, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '6fff842f967d7207', 'data': {'categories': ['#rlhf', '#benchmark', '#inference', '#dataset', '#open_source', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'RM-Bench: точная оценка моделей вознаграждения для эффективной настройки языковых моделей', 'desc': 'Статья представляет новый бенчмарк RM-Bench для оценки моделей вознаграждения в контексте обучения с подкреплением по обратной связи от человека (RLHF) и законов масштабирования вывода. RM-Bench оценивает чувствительность моделей к тонким различиям в содержании и их устойчивость к стилистическим смещениям. Эксперименты показывают, что RM-Bench хорошо коррелирует с производительностью моделей политики, что делает его надежным инструментом для выбора моделей вознаграждения при настройке языковых моделей. Результаты демонстрируют, что даже современные модели вознаграждения достигают средней производительности всего 46.6%, что ниже случайного уровня точности при наличии стилистических смещений.'}, 'en': {'title': 'Enhancing Reward Models: Beyond Style Bias and Subtlety', 'desc': "The paper introduces RM-Bench, a new benchmark for evaluating reward models used in Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws. RM-Bench focuses on assessing models' sensitivity to subtle content changes and their resistance to style biases, which are often overlooked in traditional benchmarks. The study finds that even advanced reward models perform poorly, with an average accuracy of 46.6% when style bias is present, indicating a need for improvement. This benchmark provides a more reliable way to select reward models that align language models effectively."}, 'zh': {'title': '提升奖励模型：超越风格偏见的挑战', 'desc': '这篇论文介绍了一种新的基准测试RM-Bench，用于评估奖励模型在细微内容差异和风格偏见下的表现。现有的奖励模型基准测试往往只关注模型之间的简单区分，而忽略了对内容和风格变化的敏感性。RM-Bench通过实验表明，与策略模型表现有很强的相关性，是选择奖励模型的可靠参考。研究结果显示，即使是最先进的模型在面对风格偏见时，表现也仅为46.6%，低于随机水平，表明当前奖励模型还有很大的改进空间。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.12788', 'title': 'Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception', 'url': 'https://huggingface.co/papers/2410.12788', 'abstract': 'Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refers to a granularity between sentences and paragraphs, consisting of a collection of sentences within a paragraph that have deep linguistic logical connections. To implement Meta-Chunking, we designed two strategies based on LLMs: Margin Sampling Chunking and Perplexity Chunking. The former employs LLMs to perform binary classification on whether consecutive sentences need to be segmented, making decisions based on the probability difference obtained from margin sampling. The latter precisely identifies text chunk boundaries by analyzing the characteristics of perplexity distribution. Additionally, considering the inherent complexity of different texts, we propose a strategy that combines Meta-Chunking with dynamic merging to achieve a balance between fine-grained and coarse-grained text chunking. Experiments conducted on eleven datasets demonstrate that Meta-Chunking can more efficiently improve the performance of single-hop and multi-hop question answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only consuming 45.8% of the time. Our code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.', 'score': 21, 'issue_id': 208, 'pub_date': '2024-10-16', 'pub_date_card': {'ru': '16 октября', 'en': 'October 16', 'zh': '10月16日'}, 'hash': '0fbcf072cc98c553', 'data': {'categories': ['#rag', '#reasoning', '#benchmark', '#data', '#open_source', '#architecture'], 'emoji': '🧩', 'ru': {'title': 'Мета-Чанкинг: Новый подход к разбиению текста для повышения эффективности RAG', 'desc': 'Статья представляет концепцию Мета-Чанкинга для улучшения Retrieval-Augmented Generation (RAG). Мета-Чанкинг - это метод разбиения текста на фрагменты, учитывающий лингвистические связи между предложениями. Авторы предлагают две стратегии реализации: Margin Sampling Chunking и Perplexity Chunking, использующие большие языковые модели. Эксперименты на одиннадцати наборах данных показывают, что Мета-Чанкинг повышает эффективность RAG в задачах вопросно-ответных систем.'}, 'en': {'title': '"Meta-Chunking: Enhancing RAG with Smarter Text Segmentation"', 'desc': 'The paper introduces Meta-Chunking, a method to improve text chunking in Retrieval-Augmented Generation (RAG) systems by focusing on the logical connections between sentences within paragraphs. Two strategies, Margin Sampling Chunking and Perplexity Chunking, are developed using large language models to determine optimal chunk boundaries. These methods enhance the efficiency and accuracy of question answering tasks by balancing fine-grained and coarse-grained chunking. Experiments show that Meta-Chunking significantly improves performance while reducing processing time compared to traditional chunking methods.'}, 'zh': {'title': 'Meta-Chunking：提升文本分块的新方法', 'desc': '这篇论文介绍了一种新的文本分块方法，称为Meta-Chunking，它在句子和段落之间找到一种新的粒度。Meta-Chunking通过两种策略实现：边缘采样分块和困惑度分块，分别利用大语言模型进行二元分类和困惑度分布分析。为了适应不同文本的复杂性，论文还提出了一种结合动态合并的策略，以在细粒度和粗粒度分块之间取得平衡。实验结果表明，Meta-Chunking在提高基于RAG的单跳和多跳问答性能方面更有效率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16215', 'title': 'Pre-training Distillation for Large Language Models: A Design Space Exploration', 'url': 'https://huggingface.co/papers/2410.16215', 'abstract': 'Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation.', 'score': 15, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '0d2e73e11d2b2b3f', 'data': {'categories': ['#small_models', '#optimization', '#training', '#transfer_learning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Дистилляция знаний на этапе предобучения: новый подход к созданию эффективных языковых моделей', 'desc': 'Эта статья исследует применение дистилляции знаний (KD) на этапе предобучения больших языковых моделей (LLM). Авторы проводят эксперименты с использованием GLM-4-9B в качестве учителя для дистилляции студента с 1,9 миллиардами параметров. Они систематически изучают пространство дизайна предобучающей дистилляции по четырем аспектам: обработка логитов, выбор функции потерь, закон масштабирования и офлайн или онлайн логиты. Результаты показывают, что более крупные студенческие LLM обычно больше выигрывают от предобучающей дистилляции, в то время как более крупная модель-учитель не обязательно гарантирует лучшие результаты.'}, 'en': {'title': '"Pre-Training Distillation: A New Frontier in Model Efficiency"', 'desc': 'This paper explores a new approach to knowledge distillation by applying it during the pre-training phase of large language models, rather than the post-training phase. The authors conduct experiments using a large teacher model to distill knowledge into a smaller student model, demonstrating the effectiveness of this method. They investigate various factors that influence the success of pre-training distillation, such as how logits are processed and the choice of loss functions. The study reveals that larger student models benefit more from pre-training distillation, while the size of the teacher model does not always correlate with better outcomes.'}, 'zh': {'title': '预训练阶段的知识蒸馏：小模型的大智慧', 'desc': '这篇论文研究了如何在大语言模型的预训练阶段进行知识蒸馏。通过使用GLM-4-9B作为教师模型，验证了预训练蒸馏的有效性。研究发现，较大的学生模型通常能从预训练蒸馏中获益更多，而较大的教师模型不一定带来更好的结果。作者希望这些发现能为未来的预训练蒸馏实践提供指导。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15748', 'title': 'Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation', 'url': 'https://huggingface.co/papers/2410.15748', 'abstract': 'Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge for NTP. To address this issue, this work proposes Alchemy, a general framework for data synthesis that constructs formal theorems through symbolic mutation. Specifically, for each candidate theorem in Mathlib, we identify all invocable theorems that can be used to rewrite or apply to it. Subsequently, we mutate the candidate theorem by replacing the corresponding term in the statement with its equivalent form or antecedent. As a result, our method increases the number of theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore, we perform continual pretraining and supervised finetuning on this augmented corpus for large language models. Experimental results demonstrate the effectiveness of our approach, achieving a 5% absolute performance improvement on Leandojo benchmark. Additionally, our synthetic data achieve a 2.5% absolute performance gain on the out-of-distribution miniF2F benchmark. To provide further insights, we conduct a comprehensive analysis of synthetic data composition and the training paradigm, offering valuable guidance for developing a strong theorem prover.', 'score': 12, 'issue_id': 212, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'b6b22326cba00346', 'data': {'categories': ['#reasoning', '#synthetic', '#benchmark', '#math', '#data', '#training', '#dataset', '#transfer_learning'], 'emoji': '🧪', 'ru': {'title': 'Алхимия данных: превращаем сотни тысяч теорем в миллионы', 'desc': 'Статья представляет Alchemy - фреймворк для синтеза данных в области нейронного доказательства теорем (NTP). Авторы предлагают метод символьной мутации для создания новых теорем на основе существующих в библиотеке Mathlib. Это позволяет увеличить объем обучающих данных с 110 тысяч до 6 миллионов теорем. Эксперименты показывают улучшение производительности больших языковых моделей на 5% в тесте Leandojo и на 2.5% на out-of-distribution данных miniF2F после дообучения на синтезированном корпусе.'}, 'en': {'title': 'Alchemy: Transforming Theorem Proving with Synthetic Data', 'desc': 'The paper introduces Alchemy, a framework designed to tackle the data scarcity problem in Neural Theorem Proving by generating synthetic formal theorems. By using symbolic mutation, Alchemy expands the number of theorems in Mathlib significantly, enhancing the training data available for large language models. The approach involves rewriting candidate theorems using invocable theorems, which increases the corpus size and improves model performance on benchmarks like Leandojo and miniF2F. The study also provides an analysis of the synthetic data and training methods, offering insights for building more effective theorem provers.'}, 'zh': {'title': 'Alchemy：通过数据合成增强神经定理证明', 'desc': '这篇论文介绍了一种名为Alchemy的数据合成框架，用于通过符号变异构建形式定理。通过识别和应用可调用的定理，Alchemy将Mathlib中的定理数量从11万增加到600万。然后，研究人员在扩展后的语料库上对大型语言模型进行持续预训练和监督微调。实验结果表明，这种方法在Leandojo基准上提高了5%的性能，并在miniF2F基准上提高了2.5%的性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15316', 'title': 'Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant', 'url': 'https://huggingface.co/papers/2410.15316', 'abstract': 'Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models.', 'score': 9, 'issue_id': 214, 'pub_date': '2024-10-20', 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}, 'hash': '0fbe64f20b885346', 'data': {'categories': ['#science', '#small_models', '#benchmark', '#multilingual', '#multimodal', '#training', '#transfer_learning', '#open_source', '#audio', '#architecture'], 'emoji': '🍓', 'ru': {'title': 'Ичиго: объединяя речь и текст в единой языковой модели', 'desc': 'Ичиго - это мультимодальная языковая модель, способная обрабатывать как речь, так и текст. Модель использует ранний подход к слиянию модальностей, квантуя речь в дискретные токены и применяя единую архитектуру трансформера для обеих модальностей. Ичиго демонстрирует передовые результаты в задачах ответов на вопросы по речи, превосходя существующие модели с открытым исходным кодом. Важным преимуществом модели является низкая задержка генерации первого токена - всего 111 мс.'}, 'en': {'title': 'Ichigo: Bridging Speech and Text with Unified AI', 'desc': 'The paper introduces Ichigo, a model that processes both speech and text together using a unified transformer architecture. By converting speech into tokens, Ichigo can handle both modalities without needing separate systems, allowing for efficient joint reasoning. The model is trained on diverse datasets, achieving top performance in speech question-answering tasks with low latency. This approach not only enhances multimodal AI but also empowers smaller teams to develop competitive open-source models.'}, 'zh': {'title': 'Ichigo：跨越语音与文本的无缝融合', 'desc': '这篇论文介绍了Ichigo，一种能够同时处理语音和文本的混合模态模型。Ichigo通过将语音量化为离散的token，并使用统一的transformer架构来处理语音和文本，从而实现了跨模态的联合推理和生成。该模型在多语言语音识别数据集上进行预训练，并在精心挑选的指令数据集上进行微调，表现出色。Ichigo在语音问答基准测试中表现优异，且首次生成token的延迟仅为111毫秒，显著低于现有模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.11711', 'title': 'Zero-shot Model-based Reinforcement Learning using Large Language Models', 'url': 'https://huggingface.co/papers/2410.11711', 'abstract': "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs' deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-of-concept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl.", 'score': 8, 'issue_id': 212, 'pub_date': '2024-10-15', 'pub_date_card': {'ru': '15 октября', 'en': 'October 15', 'zh': '10月15日'}, 'hash': '5868a5a1652a8da3', 'data': {'categories': ['#reasoning', '#rl', '#training', '#transfer_learning', '#open_source', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'LLM покоряют непрерывное пространство состояний в обучении с подкреплением', 'desc': 'В этой статье исследуется применение больших языковых моделей (LLM) для предсказания динамики непрерывных марковских процессов принятия решений. Авторы предлагают метод разделенного обучения в контексте (DICL) для решения проблем обработки многомерных данных и учета управляющего сигнала. Метод применяется в двух сценариях обучения с подкреплением: оценке политик на основе модели и обучении с дополнительными данными вне политики. Эксперименты показывают, что подход позволяет получить хорошо откалиброванные оценки неопределенности.'}, 'en': {'title': 'Unlocking LLMs for Continuous Reinforcement Learning', 'desc': 'This paper explores how Large Language Models (LLMs) can be used in reinforcement learning with continuous state spaces, a less-studied area. The authors propose a method called Disentangled In-Context Learning (DICL) to address challenges like handling multivariate data and incorporating control signals. They demonstrate the effectiveness of DICL in model-based policy evaluation and data-augmented off-policy reinforcement learning. The approach also provides well-calibrated uncertainty estimates, enhancing the reliability of predictions.'}, 'zh': {'title': '解锁大型语言模型在强化学习中的新潜力', 'desc': '这篇论文探讨了如何利用预训练的大型语言模型（LLMs）来预测连续马尔可夫决策过程中的动态。研究发现，处理多变量数据和整合控制信号是限制LLMs在这种环境中应用的关键挑战。为了解决这些问题，作者提出了一种名为解耦上下文学习（DICL）的新方法。实验结果表明，该方法能够提供良好的不确定性估计。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15633', 'title': "Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement", 'url': 'https://huggingface.co/papers/2410.15633', 'abstract': "The expansion of large language models to effectively handle instructions with extremely long contexts has yet to be fully investigated. The primary obstacle lies in constructing a high-quality long instruction-following dataset devised for long context alignment. Existing studies have attempted to scale up the available data volume by synthesizing long instruction-following samples. However, indiscriminately increasing the quantity of data without a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the final performance. To bridge this gap, we aim to address the unique challenge of long-context alignment, i.e., modeling the long-range dependencies for handling instructions and lengthy input contexts. We propose GATEAU, a novel framework designed to identify the influential and high-quality samples enriched with long-range dependency relations by utilizing crafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM). Specifically, HMG attempts to measure the difficulty of generating corresponding responses due to the long-range dependencies, using the perplexity scores of the response from two homologous models with different context windows. Also, the role of CAM is to measure the difficulty of understanding the long input contexts due to long-range dependencies by evaluating whether the model's attention is focused on important segments. Built upon both proposed methods, we select the most challenging samples as the influential data to effectively frame the long-range dependencies, thereby achieving better performance of LLMs. Comprehensive experiments indicate that GATEAU effectively identifies samples enriched with long-range dependency relations and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities.", 'score': 7, 'issue_id': 209, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '0b470bc767fc516b', 'data': {'categories': ['#synthetic', '#optimization', '#data', '#training', '#dataset', '#architecture', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'GATEAU: Умное обучение LLM на длинных контекстах', 'desc': "Этот научный труд представляет GATEAU - новую систему для улучшения способности больших языковых моделей (LLM) работать с длинными контекстами. GATEAU использует два метода: Homologous Models' Guidance (HMG) для измерения сложности генерации ответов, и Contextual Awareness Measurement (CAM) для оценки понимания длинного контекста. Система выбирает наиболее сложные и информативные образцы данных для обучения LLM. Эксперименты показывают, что модели, обученные на отобранных GATEAU данных, демонстрируют улучшенные способности в понимании длинных инструкций и контекстов."}, 'en': {'title': 'Mastering Long Contexts with GATEAU: Quality Over Quantity', 'desc': "The paper introduces GATEAU, a framework designed to improve large language models' ability to handle long instructions by focusing on high-quality data samples. It uses Homologous Models' Guidance (HMG) to assess the difficulty of generating responses with long-range dependencies and Contextual Awareness Measurement (CAM) to ensure the model's attention is on important segments. By selecting challenging samples, GATEAU enhances the model's performance in understanding and following long-context instructions. Experiments show that models trained with GATEAU-selected data perform better in long-context tasks."}, 'zh': {'title': 'GATEAU：提升长上下文指令处理的新框架', 'desc': '这篇论文探讨了如何让大型语言模型更好地处理长上下文的指令。主要挑战在于构建一个高质量的长指令数据集，以便模型能更好地对齐长上下文。为了解决这个问题，作者提出了一个名为GATEAU的新框架，通过同源模型指导和上下文意识测量来识别高质量样本。实验结果表明，使用这些样本训练的模型在指令跟随和长上下文理解方面表现更好。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15002', 'title': 'How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold', 'url': 'https://huggingface.co/papers/2410.15002', 'abstract': "Text-to-image models are trained using large datasets collected by scraping image-text pairs from the internet. These datasets often include private, copyrighted, and licensed material. Training models on such datasets enables them to generate images with such content, which might violate copyright laws and individual privacy. This phenomenon is termed imitation -- generation of images with content that has recognizable similarity to its training images. In this work we study the relationship between a concept's frequency in the training dataset and the ability of a model to imitate it. We seek to determine the point at which a model was trained on enough instances to imitate a concept -- the imitation threshold. We posit this question as a new problem: Finding the Imitation Threshold (FIT) and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training multiple models from scratch. We experiment with two domains -- human faces and art styles -- for which we create four datasets, and evaluate three text-to-image models which were trained on two pretraining datasets. Our results reveal that the imitation threshold of these models is in the range of 200-600 images, depending on the domain and the model. The imitation threshold can provide an empirical basis for copyright violation claims and acts as a guiding principle for text-to-image model developers that aim to comply with copyright and privacy laws. We release the code and data at https://github.com/vsahil/MIMETIC-2.git and the project's website is hosted at https://how-many-van-goghs-does-it-take.github.io.", 'score': 6, 'issue_id': 216, 'pub_date': '2024-10-19', 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}, 'hash': 'f538a28b342207ac', 'data': {'categories': ['#synthetic', '#cv', '#ethics', '#training', '#dataset', '#open_source', '#security'], 'emoji': '🖼️', 'ru': {'title': 'Сколько Ван Гогов нужно, чтобы обучить ИИ?', 'desc': "Статья исследует проблему имитации в моделях генерации изображений по тексту. Авторы вводят понятие 'порога имитации' - минимального количества примеров, необходимого модели для воспроизведения определенного концепта. Исследование показало, что для современных моделей этот порог составляет 200-600 изображений в зависимости от домена и архитектуры. Результаты имеют важное значение для соблюдения авторских прав и конфиденциальности при разработке генеративных моделей."}, 'en': {'title': 'Finding the Imitation Threshold: Balancing Creativity and Copyright in AI', 'desc': 'This paper explores how text-to-image models can generate images that closely resemble copyrighted or private content from their training datasets, a phenomenon known as imitation. The authors introduce the concept of the imitation threshold, which is the minimum number of instances a model needs to see in its training data to replicate a concept. They propose a method to estimate this threshold efficiently, without the need to train multiple models from scratch. Their experiments show that the imitation threshold varies between 200-600 images, providing a basis for understanding potential copyright violations and guiding model developers in legal compliance.'}, 'zh': {'title': '揭示文本到图像模型的模仿阈值', 'desc': '这篇论文研究了文本到图像模型在训练数据集中模仿概念的能力。研究发现，当一个概念在训练数据集中出现200到600次时，模型就能有效模仿该概念，这被称为模仿阈值。通过找到模仿阈值，可以帮助模型开发者遵循版权和隐私法律。研究结果为版权侵权的判断提供了实证依据。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16259', 'title': 'Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos', 'url': 'https://huggingface.co/papers/2410.16259', 'abstract': 'We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively through video observations recorded over a long time-span (e.g., a month) in a single environment. Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period. To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation. We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator. We demonstrate results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos captured by a smartphone.', 'score': 4, 'issue_id': 216, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '2af6ae782ab41cc9', 'data': {'categories': ['#diffusion', '#synthetic', '#graphs', '#video', '#robotics', '#transfer_learning', '#games', '#agents', '#3d'], 'emoji': '🎥', 'ru': {'title': 'От видео к виртуальному поведению: обучение 3D-агентов на основе естественных наблюдений', 'desc': 'Представлена система Agent-to-Sim (ATS) для обучения интерактивных моделей поведения 3D-агентов на основе обычных видеоколлекций. ATS использует неинвазивный подход, анализируя естественное поведение животных и людей по видеозаписям, сделанным в течение длительного периода в одной среде. Разработан метод регистрации от грубой к точной для отслеживания агента и камеры во времени через каноническое 3D-пространство. На основе полученных данных обучается генеративная модель поведения агента, позволяющая создавать интерактивный симулятор поведения.'}, 'en': {'title': 'From Video to Virtual: Simulating Real-World Behaviors', 'desc': "The paper introduces Agent-to-Sim (ATS), a framework for learning 3D behavior models of agents from long-term video recordings without using invasive tracking methods. ATS uses a novel coarse-to-fine registration technique to maintain persistent 3D tracking of agents over time, creating a comprehensive 4D representation. This data is then used to train a generative model that simulates the agent's behavior based on its perception and motion. The framework successfully transfers real-world video observations into interactive behavior simulations, demonstrated with pets and humans using simple smartphone recordings."}, 'zh': {'title': '从视频到模拟：非侵入性3D行为建模', 'desc': '这篇论文介绍了一种名为Agent-to-Sim (ATS)的框架，用于从长期视频中学习3D代理的交互行为模型。与依赖标记跟踪和多视角相机的传统方法不同，ATS通过单一环境中长时间的视频观察，非侵入性地学习动物和人类代理的自然行为。为了实现3D行为建模，研究人员开发了一种粗到细的注册方法，通过一个标准的3D空间跟踪代理和相机，形成完整的时空4D表示。最终，ATS可以将视频记录中的代理行为转移到交互行为模拟器中。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13218', 'title': 'CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy', 'url': 'https://huggingface.co/papers/2410.13218', 'abstract': "There is a significant gap between patient needs and available mental health support today. In this paper, we aim to thoroughly examine the potential of using Large Language Models (LLMs) to assist professional psychotherapy. To this end, we propose a new benchmark, CBT-BENCH, for the systematic evaluation of cognitive behavioral therapy (CBT) assistance. We include three levels of tasks in CBT-BENCH: I: Basic CBT knowledge acquisition, with the task of multiple-choice questions; II: Cognitive model understanding, with the tasks of cognitive distortion classification, primary core belief classification, and fine-grained core belief classification; III: Therapeutic response generation, with the task of generating responses to patient speech in CBT therapy sessions. These tasks encompass key aspects of CBT that could potentially be enhanced through AI assistance, while also outlining a hierarchy of capability requirements, ranging from basic knowledge recitation to engaging in real therapeutic conversations. We evaluated representative LLMs on our benchmark. Experimental results indicate that while LLMs perform well in reciting CBT knowledge, they fall short in complex real-world scenarios requiring deep analysis of patients' cognitive structures and generating effective responses, suggesting potential future work.", 'score': 4, 'issue_id': 207, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'f4f24ef7771a745c', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#healthcare', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Оценка потенциала ИИ в когнитивно-поведенческой терапии', 'desc': 'Исследователи представили новый бенчмарк CBT-BENCH для оценки потенциала использования больших языковых моделей в когнитивно-поведенческой терапии (КПТ). Бенчмарк включает три уровня задач: базовые знания КПТ, понимание когнитивной модели и генерацию терапевтических ответов. Эксперименты показали, что языковые модели хорошо справляются с воспроизведением знаний КПТ, но испытывают трудности в сложных сценариях, требующих глубокого анализа когнитивных структур пациентов. Результаты указывают на потенциальные направления для будущих исследований в области применения ИИ в психотерапии.'}, 'en': {'title': 'Bridging the Gap: AI Meets Mental Health Support', 'desc': "This paper explores the use of Large Language Models (LLMs) to support psychotherapy, specifically cognitive behavioral therapy (CBT). It introduces CBT-BENCH, a benchmark designed to evaluate LLMs' ability to assist in CBT through tasks like knowledge acquisition, cognitive model understanding, and therapeutic response generation. The study finds that while LLMs can effectively recall CBT knowledge, they struggle with more complex tasks that require deep understanding and interaction. This highlights the need for further development to enhance LLMs' capabilities in real-world therapeutic settings."}, 'zh': {'title': '大型语言模型：心理治疗的未来助手？', 'desc': '这篇论文探讨了使用大型语言模型（LLMs）来辅助专业心理治疗的潜力。为此，作者提出了一个新的基准测试，名为CBT-BENCH，用于系统评估认知行为疗法（CBT）的辅助能力。CBT-BENCH包括三个任务层次：基础知识获取、认知模型理解和治疗性回应生成。实验结果表明，虽然LLMs在知识背诵方面表现良好，但在需要深入分析患者认知结构和生成有效回应的复杂场景中表现不足。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14086', 'title': "In-context learning and Occam's razor", 'url': 'https://huggingface.co/papers/2410.14086', 'abstract': "The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at https://github.com/3rdCore/PrequentialCode.", 'score': 2, 'issue_id': 214, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '6f9c84cdfe1502b2', 'data': {'categories': ['#optimization', '#math', '#training', '#open_source', '#architecture'], 'emoji': '🪒', 'ru': {'title': 'Бритва Оккама в действии: как обучение в контексте воплощает принцип простоты', 'desc': "Статья исследует связь между бритвой Оккама и обучением в контексте в машинном обучении. Авторы показывают, что функция потерь для предсказания следующего токена эквивалентна методу сжатия данных под названием 'прекваентиальное кодирование'. Минимизация этой функции потерь одновременно минимизирует ошибку обучения и сложность модели, неявно изученной из контекста. Исследование предлагает нормативное объяснение обучения в контексте и указывает на недостатки текущих методов, предлагая пути их улучшения."}, 'en': {'title': "Simplifying Complexity: Bridging Occam's Razor and In-Context Learning", 'desc': "The paper explores the connection between Occam's razor, which favors simpler models for better generalization, and in-context learning, a feature of sequence models like Transformers. It demonstrates that the next-token prediction loss used in training these models is akin to prequential coding, a data compression method. This approach effectively minimizes both training error and model complexity, aligning with the principle of Occam's razor. The authors provide theoretical insights and empirical evidence, highlighting the limitations of current in-context learning methods and suggesting improvements."}, 'zh': {'title': '奥卡姆剃刀与上下文学习的巧妙结合', 'desc': '机器学习的目标是实现泛化，即模型不仅能在训练数据上表现良好，还能在新数据上表现出色。奥卡姆剃刀原则指出，简单的模型往往能更好地泛化，但目前大多数方法只关注最小化训练误差。本文将奥卡姆剃刀与上下文学习联系起来，展示了如何通过最小化下一个词预测损失来同时减少训练误差和模型复杂性。我们的理论和实验揭示了当前上下文学习方法的不足，并提出了改进建议。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13184', 'title': 'Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers', 'url': 'https://huggingface.co/papers/2410.13184', 'abstract': "Traditional transformer models often allocate a fixed amount of computational resources to every input token, leading to inefficient and unnecessary computation. To address this, the Mixture of Depths (MoD) was introduced to dynamically adjust the computational depth by skipping less important layers. Despite its promise, current MoD approaches remain under-explored and face two main challenges: (1) high training costs due to the need to train the entire model along with the routers that determine which layers to skip, and (2) the risk of performance degradation when important layers are bypassed. In response to the first issue, we propose Router-Tuning, a method that fine-tunes only the router on a small dataset, drastically reducing the computational overhead associated with full model training. For the second challenge, we propose MindSkip, which deploys Attention with Dynamic Depths. This method preserves the model's performance while significantly enhancing computational and memory efficiency. Extensive experiments demonstrate that our approach delivers competitive results while dramatically improving the computation efficiency, e.g., 21\\% speedup and only a 0.2\\% performance drop. The code is released at https://github.com/CASE-Lab-UMD/Router-Tuning.", 'score': 2, 'issue_id': 214, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '0e28d555da53ebff', 'data': {'categories': ['#optimization', '#training', '#architecture', '#open_source'], 'emoji': '🚀', 'ru': {'title': 'Эффективные трансформеры: умное пропускание слоев без потери качества', 'desc': 'В статье предлагается метод Router-Tuning для оптимизации вычислительных ресурсов в трансформерных моделях. Авторы разработали подход MindSkip, использующий внимание с динамической глубиной, что позволяет сохранить производительность модели при повышении эффективности. Эксперименты показали 21% ускорение работы при минимальном снижении качества на 0.2%. Предложенный метод решает проблемы высоких затрат на обучение и риска пропуска важных слоев в существующих подходах Mixture of Depths.'}, 'en': {'title': 'Efficient Transformers: Skipping the Unnecessary', 'desc': 'The paper introduces a novel approach to improve the efficiency of transformer models by using a Mixture of Depths (MoD) strategy, which dynamically adjusts computational depth by skipping less important layers. To tackle the high training costs associated with MoD, the authors propose Router-Tuning, a method that fine-tunes only the router on a small dataset, reducing computational overhead. To prevent performance degradation, they introduce MindSkip, which uses Attention with Dynamic Depths to maintain model performance while enhancing efficiency. Experiments show that their approach achieves a 21% speedup with only a 0.2% performance drop, demonstrating its effectiveness in improving computation efficiency.'}, 'zh': {'title': '动态深度：提升Transformer计算效率的新方法', 'desc': '传统的Transformer模型对每个输入标记分配固定的计算资源，导致计算效率低下。为了解决这个问题，引入了深度混合（MoD）方法，通过跳过不重要的层来动态调整计算深度。然而，现有的MoD方法面临高训练成本和性能下降的风险。我们提出了Router-Tuning和MindSkip方法，分别通过微调路由器和使用动态深度注意力来解决这些问题，显著提高了计算效率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15460', 'title': 'Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training', 'url': 'https://huggingface.co/papers/2410.15460', 'abstract': 'As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations-outputs that are factually inaccurate or irrelevant to user input-have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M-12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce SEnsitive Neuron Dropout (SeND), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SeND achieves this by deterministically dropping neurons with significant variability on a dataset, referred to as Sensitive Neurons. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SeND to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to domains such as Wikipedia and Medical datasets.', 'score': 1, 'issue_id': 214, 'pub_date': '2024-10-20', 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}, 'hash': '5c4104bbacb9ac91', 'data': {'categories': ['#small_models', '#hallucinations', '#optimization', '#healthcare', '#training'], 'emoji': '🧠', 'ru': {'title': 'Борьба с галлюцинациями в LLM через управление чувствительными нейронами', 'desc': 'Исследование изучает связь между процессом обучения и возникновением галлюцинаций в больших языковых моделях (LLM). Авторы анализируют тренды галлюцинаций на протяжении обучения, используя модели из набора Pythia и несколько метрик обнаружения галлюцинаций. Они представляют новый протокол обучения SeND, который снижает вариативность во время обучения путем детерминированного исключения чувствительных нейронов. Также разработана несупервизорная метрика обнаружения галлюцинаций EES, которая интегрирована в протокол для повышения его эффективности и масштабируемости.'}, 'en': {'title': '"Training Smarter: Reducing Hallucinations in Language Models"', 'desc': 'The paper explores how large language models (LLMs) sometimes produce incorrect or irrelevant outputs, known as hallucinations, and how these can be reduced during the training process. The researchers introduce a new training method called Sensitive Neuron Dropout (SeND), which helps decrease hallucinations by selectively dropping neurons that show high variability. They also develop a fast, unsupervised metric called Efficient EigenScore (EES) to detect hallucinations more quickly. Their approach shows a significant improvement in the reliability and factual accuracy of LLMs, especially when applied to specific domains like Wikipedia and medical datasets.'}, 'zh': {'title': '减少幻觉，提升语言模型可靠性', 'desc': '随着大型语言模型在各行业的广泛应用，人们对其可靠性，尤其是幻觉现象的担忧日益增加。我们的研究探讨了训练过程与幻觉现象之间的关系，填补了现有研究主要集中于事后检测和缓解策略的空白。我们引入了一种新的训练协议，称为敏感神经元丢弃（SeND），通过在训练中有选择地丢弃变异性大的神经元来减少幻觉。我们还开发了一种无监督的幻觉检测指标，称为高效特征值分数（EES），使得SeND在减少幻觉的同时具有计算可扩展性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13394', 'title': 'Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs', 'url': 'https://huggingface.co/papers/2410.13394', 'abstract': 'Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.', 'score': 1, 'issue_id': 208, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'd456f53989a80b51', 'data': {'categories': ['#benchmark', '#multilingual', '#interpretability', '#dataset', '#open_source', '#low_resource', '#machine_translation', '#architecture'], 'emoji': '🌐', 'ru': {'title': 'Преодоление языковых барьеров в оценке ИИ', 'desc': 'Статья представляет новый фреймворк CIA Suite для многоязычной оценки генеративных языковых моделей. Он включает в себя модель-оценщик Hercule и тестовый набор Recon с 500 аннотированными инструкциями на шести языках. Hercule способна оценивать ответы на целевом языке, используя эталонные ответы на английском, что решает проблему нехватки ресурсов. Эксперименты показывают, что Hercule лучше соответствует оценкам людей, чем проприетарные модели, и эффективна даже для ранее не виденных языков.'}, 'en': {'title': 'Breaking Language Barriers in NLP Evaluation', 'desc': "The paper addresses the challenge of evaluating machine-generated text in multiple languages, which is often overlooked in favor of English. It introduces the Cross Lingual Auto Evaluation (CIA) Suite, featuring the Hercule model and a novel test set called Recon, designed for multilingual evaluation. Hercule is a cross-lingual evaluation model that uses English reference answers to score responses in other languages, aligning closely with human judgments. The study demonstrates Hercule's effectiveness in low-resource and zero-shot scenarios, marking a significant step forward in multilingual NLP evaluation."}, 'zh': {'title': '跨语言评估的新突破：Hercule模型的多语言评估', 'desc': '这篇论文探讨了在自然语言处理领域中，评估机器生成文本的挑战，尤其是在非英语语言中。作者提出了一个名为跨语言自动评估套件（CIA）的框架，其中包括一个新的评估模型Hercule和一个多语言测试集Recon。Hercule模型通过学习从英语参考答案中评分，解决了目标语言中缺乏参考答案的问题。实验表明，Hercule在低资源场景中与人类判断更为一致，并且在未见过的语言上也能进行零样本评估。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15017', 'title': 'DM-Codec: Distilling Multimodal Representations for Speech Tokenization', 'url': 'https://huggingface.co/papers/2410.15017', 'abstract': 'Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. The code, samples, and model checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.', 'score': 1, 'issue_id': 208, 'pub_date': '2024-10-19', 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}, 'hash': 'd2c6c349adfb4796', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#open_source', '#audio', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'DM-Codec: Революция в токенизации речи с использованием мультимодальных представлений', 'desc': 'В статье представлен новый подход к токенизации речи, названный DM-Codec. Он объединяет акустическую, семантическую и контекстную информацию для создания более точных речевых представлений. Авторы предлагают два метода дистилляции: с использованием языковой модели и комбинированный метод с языковой моделью и самообучающейся речевой моделью. Эксперименты показывают, что DM-Codec значительно превосходит современные модели токенизации речи, снижая ошибки распознавания и улучшая качество и разборчивость речи.'}, 'en': {'title': '"DM-Codec: Revolutionizing Speech Tokenization with Contextual Intelligence"', 'desc': 'This paper addresses the challenge of effectively mapping speech into discrete tokens by incorporating acoustic, semantic, and contextual information. The authors propose two novel distillation methods that integrate language models and self-supervised speech models to create a comprehensive speech tokenizer called DM-Codec. The DM-Codec uses a streamlined encoder-decoder framework with a Residual Vector Quantizer to improve speech tokenization. Experiments demonstrate that DM-Codec significantly reduces Word Error Rate and Word Information Lost, enhancing speech quality and intelligibility.'}, 'zh': {'title': 'DM-Codec：融合多模态信息的语音标记化新突破', 'desc': '这篇论文探讨了语音模型中语音标记化和合成的最新进展，强调了将复杂的语音属性映射为离散标记的挑战。研究发现，缺乏上下文表示会导致语音转录中的词错误率和词信息丢失率升高。为解决这些问题，作者提出了两种新的蒸馏方法，结合语言模型和自监督语音模型来改进语音标记器。实验结果表明，所提出的DM-Codec模型在语音标记化性能上显著优于现有模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17247', 'title': 'PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction', 'url': 'https://huggingface.co/papers/2410.17247', 'abstract': 'In large vision-language models (LVLMs), images serve as inputs that carry a wealth of information. As the idiom "A picture is worth a thousand words" implies, representing a single image in current LVLMs can require hundreds or even thousands of tokens. This results in significant computational costs, which grow quadratically as input image resolution increases, thereby severely impacting the efficiency of both training and inference. Previous approaches have attempted to reduce the number of image tokens either before or within the early layers of LVLMs. However, these strategies inevitably result in the loss of crucial image information, ultimately diminishing model performance. To address this challenge, we conduct an empirical study revealing that all visual tokens are necessary for LVLMs in the shallow layers, and token redundancy progressively increases in the deeper layers of the model. To this end, we propose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost their efficiency in both training and inference with neglectable performance loss. Specifically, we partition the LVLM into several stages and drop part of the image tokens at the end of each stage with a pre-defined ratio, creating pyramid-like visual tokens across model layers. The dropping is based on a lightweight similarity calculation with a negligible time overhead. Extensive experiments demonstrate that PyramidDrop can achieve a 40% training time and 55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance. Besides, the PyramidDrop could also serve as a plug-and-play strategy for inference acceleration without training, with better performance and lower inference cost than counterparts. We hope that the insights and approach introduced by PyramidDrop will inspire future research to further investigate the role of image tokens in LVLMs.', 'score': 43, 'issue_id': 222, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'b399abce7351d374', 'data': {'categories': ['#cv', '#inference', '#optimization', '#interpretability', '#training', '#architecture'], 'emoji': '🔺', 'ru': {'title': 'PyramidDrop: Эффективное сокращение визуальной избыточности в LVLM', 'desc': 'Статья представляет новый метод PyramidDrop для повышения эффективности крупных визуально-языковых моделей (LVLM). Авторы обнаружили, что токены изображений становятся избыточными в глубоких слоях модели. PyramidDrop постепенно уменьшает количество токенов изображения на разных этапах обработки, создавая пирамидальную структуру. Эксперименты показывают, что метод позволяет ускорить обучение на 40% и снизить вычислительные затраты при инференсе на 55% без значительной потери производительности.'}, 'en': {'title': '"PyramidDrop: Streamlining Image Processing in Vision-Language Models"', 'desc': 'The paper discusses the challenge of handling large amounts of image data in large vision-language models (LVLMs), which can be computationally expensive. It introduces PyramidDrop, a method that reduces the number of image tokens progressively through the model layers, maintaining essential information while improving efficiency. This approach allows for significant reductions in training time and inference computations without sacrificing model performance. PyramidDrop can also be used as a standalone strategy to speed up inference, offering better performance and lower costs compared to existing methods.'}, 'zh': {'title': 'PyramidDrop：提升视觉语言模型效率的新策略', 'desc': '在大型视觉语言模型中，图像作为输入携带大量信息，但这会导致计算成本高昂。PyramidDrop是一种减少视觉冗余的策略，通过在模型的不同阶段丢弃部分图像标记来提高效率。实验表明，PyramidDrop可以在保持性能的同时显著加速训练和推理过程。此方法不仅能在训练中使用，还能作为推理加速的即插即用策略。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17249', 'title': 'SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes', 'url': 'https://huggingface.co/papers/2410.17249', 'abstract': 'We present SpectroMotion, a novel approach that combines 3D Gaussian Splatting (3DGS) with physically-based rendering (PBR) and deformation fields to reconstruct dynamic specular scenes. Previous methods extending 3DGS to model dynamic scenes have struggled to accurately represent specular surfaces. Our method addresses this limitation by introducing a residual correction technique for accurate surface normal computation during deformation, complemented by a deformable environment map that adapts to time-varying lighting conditions. We implement a coarse-to-fine training strategy that significantly enhances both scene geometry and specular color prediction. We demonstrate that our model outperforms prior methods for view synthesis of scenes containing dynamic specular objects and that it is the only existing 3DGS method capable of synthesizing photorealistic real-world dynamic specular scenes, outperforming state-of-the-art methods in rendering complex, dynamic, and specular scenes.', 'score': 39, 'issue_id': 222, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '17c17b383cd97344', 'data': {'categories': ['#cv', '#graphs', '#optimization', '#training', '#3d'], 'emoji': '✨', 'ru': {'title': 'Реалистичный рендеринг динамических зеркальных объектов', 'desc': 'SpectroMotion - это новый подход, объединяющий 3D гауссово сплаттинг (3DGS) с физически корректным рендерингом (PBR) и полями деформации для реконструкции динамических зеркальных сцен. Метод вводит технику остаточной коррекции для точного вычисления нормалей поверхности при деформации, а также деформируемую карту окружения для адаптации к изменяющимся условиям освещения. Используется стратегия обучения от грубого к точному, значительно улучшающая геометрию сцены и предсказание зеркального цвета. SpectroMotion превосходит существующие методы в синтезе фотореалистичных динамических зеркальных сцен реального мира.'}, 'en': {'title': 'SpectroMotion: Shining a New Light on Dynamic 3D Scenes', 'desc': 'SpectroMotion is a new method that improves how we create 3D images of shiny, moving objects by combining advanced techniques like 3D Gaussian Splatting and physically-based rendering. It solves previous problems with accurately showing shiny surfaces by using a special correction method to get the surface details right, even when the object moves. The method also uses a flexible map that changes with the lighting, making the scenes look more realistic. By training the model in stages from simple to complex, it becomes better at predicting both the shape and the shiny colors of the objects, outperforming other methods in creating lifelike images of moving, shiny scenes.'}, 'zh': {'title': 'SpectroMotion：动态镜面场景的真实感重建', 'desc': 'SpectroMotion 是一种新方法，结合了 3D 高斯散射和基于物理的渲染，用于重建动态镜面场景。以前的方法在处理动态场景时难以准确表示镜面表面。我们的方法通过引入残差校正技术来解决这一问题，确保在变形过程中准确计算表面法线，并使用可变形环境图适应时间变化的光照条件。我们采用由粗到细的训练策略，大大提高了场景几何和镜面颜色预测的准确性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17131', 'title': 'Aligning Large Language Models via Self-Steering Optimization', 'url': 'https://huggingface.co/papers/2410.17131', 'abstract': "Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization (SSO), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. SSO maintains the accuracy of signals by ensuring a consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy model's learning capacity. SSO can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of SSO with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, SSO leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by SSO significantly enhanced the performance of the reward model on Rewardbench. Our work presents a scalable approach to preference optimization, paving the way for more efficient and effective automated alignment.", 'score': 19, 'issue_id': 223, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '3823bd06ecf4a69a', 'data': {'categories': ['#rl', '#benchmark', '#optimization', '#training', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'Автоматическое выравнивание ИИ без участия человека', 'desc': 'Статья представляет алгоритм Self-Steering Optimization (SSO) для автоматизированного выравнивания языковых моделей. SSO генерирует качественные сигналы предпочтений без ручной разметки, поддерживая постоянный разрыв между выбранными и отвергнутыми ответами. Алгоритм улучшает онлайн и офлайн обучение политики модели, а также обучение моделей вознаграждения. Эксперименты с моделями Qwen2 и Llama3.1 показали значительное повышение производительности на шести бенчмарках.'}, 'en': {'title': 'Automated Alignment: Let Machines Learn Their Own Preferences', 'desc': "The paper introduces Self-Steering Optimization (SSO), an algorithm that autonomously generates preference signals for machine learning models without human annotation. SSO ensures these signals are accurate by maintaining a consistent gap between chosen and rejected responses, aligning with the model's learning capacity. This approach benefits both online and offline training, improving the performance of policy and reward models. The effectiveness of SSO is validated with models like Qwen2 and Llama3.1, showing significant improvements across various benchmarks."}, 'zh': {'title': '自引导优化：实现自动化对齐的新路径', 'desc': '这篇论文介绍了一种名为自引导优化（SSO）的算法，它可以在训练过程中自动生成高质量的偏好信号，而无需人工标注。SSO通过确保选择和拒绝的响应之间保持一致的差距来维持信号的准确性，并使它们都符合当前策略模型的学习能力。实验表明，SSO在不需要人工标注或外部模型的情况下，显著提高了多个基准测试的性能。SSO生成的偏好数据也显著提升了奖励模型在Rewardbench上的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16198', 'title': 'Improve Vision Language Model Chain-of-thought Reasoning', 'url': 'https://huggingface.co/papers/2410.16198', 'abstract': "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.", 'score': 16, 'issue_id': 234, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'f0a749332feba0df', 'data': {'categories': ['#reasoning', '#rl', '#rlhf', '#benchmark', '#cv', '#optimization', '#interpretability', '#training', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Усиление рассуждений в визуально-языковых моделях: от коротких ответов к глубокому пониманию', 'desc': 'Статья представляет новый подход к улучшению рассуждений в визуально-языковых моделях (VLM). Авторы предлагают обогащать обучающие данные подробными объяснениями, полученными от GPT-4, и применять обучение с подкреплением для калибровки качества рассуждений. Эксперименты показывают значительное улучшение способностей VLM к рассуждениям на эталонных наборах данных. Работа подчеркивает важность включения детальных обоснований в процесс обучения моделей машинного обучения.'}, 'en': {'title': '"Boosting VLMs: From Short Answers to Deep Reasoning"', 'desc': "The paper discusses how current vision language models (VLMs) struggle with reasoning tasks due to a lack of detailed rationale in training data. To improve this, the authors propose enriching training data with rationales distilled from a more advanced model, GPT-4o, and fine-tuning VLMs. They also use reinforcement learning to enhance reasoning by comparing correct and incorrect reasoning chains and applying the Direct Preference Optimization algorithm. The study shows that these methods significantly improve the models' reasoning abilities and their ability to generalize to new tasks."}, 'zh': {'title': '细化推理，提升视觉语言模型的可信度', 'desc': '这篇论文探讨了视觉语言模型中链式推理的重要性，强调其对模型解释性和可信度的提升。研究发现，现有的训练数据多为简短注释，缺乏详细的推理过程，导致模型在复杂推理任务中的表现不佳。为解决这一问题，作者提出了从GPT-4o模型中提取推理过程以丰富训练数据，并通过强化学习进一步优化推理质量。实验结果表明，这种方法显著提高了模型在基准数据集上的推理能力，并改善了直接答案预测的泛化能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16267', 'title': 'xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs', 'url': 'https://huggingface.co/papers/2410.16267', 'abstract': "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html", 'score': 14, 'issue_id': 233, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '7e29d42e819fbb96', 'data': {'categories': ['#small_models', '#graphs', '#video', '#optimization', '#multimodal', '#open_source', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Эффективный анализ видео с меньшим количеством токенов', 'desc': "В статье представлена мультимодальная языковая модель xGen-MM-Vid (BLIP-3-Video) для обработки видео. Модель использует 'временной кодировщик' в дополнение к обычному визуальному токенизатору, что позволяет эффективно захватывать временную информацию из нескольких кадров. BLIP-3-Video требует значительно меньше визуальных токенов по сравнению с конкурирующими моделями. Экспериментально подтверждено, что BLIP-3-Video достигает точности ответов на вопросы по видео, сопоставимой с гораздо более крупными современными моделями, при этом являясь меньше по размеру и эффективнее."}, 'en': {'title': 'Efficient Video Understanding with Fewer Tokens', 'desc': "The paper introduces xGen-MM-Vid (BLIP-3-Video), a multimodal language model designed to efficiently process video data by capturing temporal information across multiple frames. It uses a 'temporal encoder' alongside a visual tokenizer to convert sequences of frames into a compact set of visual tokens, significantly reducing the number of tokens needed compared to other models. The model explores various temporal encoders, including spatio-temporal pooling and Token Turing Machines, to enhance its efficiency. Experimental results show that BLIP-3-Video achieves competitive video question-answering performance with fewer resources, being smaller and more efficient than larger models."}, 'zh': {'title': '高效捕捉视频时间信息的多模态语言模型', 'desc': '这篇论文介绍了一种名为xGen-MM-Vid（BLIP-3-Video）的多模态语言模型，专为视频设计，能够高效捕捉多帧的时间信息。BLIP-3-Video利用了“时间编码器”，结合传统的视觉标记器，将多帧序列映射为紧凑的视觉标记集。与其他模型相比，它使用的视觉标记数量大大减少（例如，32个对比4608个）。实验结果表明，BLIP-3-Video在视频问答准确性上与更大规模的模型相当，但其规模更小且效率更高。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15926', 'title': 'Mitigating Object Hallucination via Concentric Causal Attention', 'url': 'https://huggingface.co/papers/2410.15926', 'abstract': "Recent Large Vision Language Models (LVLMs) present remarkable zero-shot conversational and reasoning capabilities given multimodal queries. Nevertheless, they suffer from object hallucination, a phenomenon where LVLMs are prone to generate textual responses not factually aligned with image inputs. Our pilot study reveals that object hallucination is closely tied with Rotary Position Encoding (RoPE), a widely adopted positional dependency modeling design in existing LVLMs. Due to the long-term decay in RoPE, LVLMs tend to hallucinate more when relevant visual cues are distant from instruction tokens in the multimodal input sequence. Additionally, we observe a similar effect when reversing the sequential order of visual tokens during multimodal alignment. Our tests indicate that long-term decay in RoPE poses challenges to LVLMs while capturing visual-instruction interactions across long distances. We propose Concentric Causal Attention (CCA), a simple yet effective positional alignment strategy that mitigates the impact of RoPE long-term decay in LVLMs by naturally reducing relative distance between visual and instruction tokens. With CCA, visual tokens can better interact with instruction tokens, thereby enhancing model's perception capability and alleviating object hallucination. Without bells and whistles, our positional alignment method surpasses existing hallucination mitigation strategies by large margins on multiple object hallucination benchmarks.", 'score': 14, 'issue_id': 226, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '9dfb7677b0acecf1', 'data': {'categories': ['#hallucinations', '#benchmark', '#cv', '#optimization', '#interpretability', '#architecture'], 'emoji': '👁️', 'ru': {'title': 'Борьба с галлюцинациями в мультимодальных ИИ-моделях', 'desc': 'Исследователи обнаружили, что большие мультимодальные языковые модели (LVLMs) склонны к галлюцинациям объектов из-за особенностей позиционного кодирования RoPE. Эта проблема усугубляется, когда визуальные подсказки находятся далеко от текстовых инструкций во входной последовательности. Авторы предложили метод Concentric Causal Attention (CCA) для уменьшения относительного расстояния между визуальными и текстовыми токенами. CCA значительно превзошел существующие методы по снижению галлюцинаций на нескольких бенчмарках.'}, 'en': {'title': 'Enhancing Visual Perception in LVLMs: Tackling Object Hallucination with CCA', 'desc': "The paper discusses how Large Vision Language Models (LVLMs) can sometimes generate incorrect textual responses that don't match the images they are analyzing, a problem known as object hallucination. This issue is linked to Rotary Position Encoding (RoPE), which struggles with long-term decay, causing LVLMs to hallucinate when visual cues are far from instruction tokens. The authors propose a new method called Concentric Causal Attention (CCA) to address this, which helps align visual and instruction tokens more effectively. By using CCA, the model's ability to perceive and interpret images improves, significantly reducing object hallucination compared to other methods."}, 'zh': {'title': '同心因果注意力：消除对象幻觉的有效策略', 'desc': '这篇论文研究了大型视觉语言模型（LVLMs）在处理多模态查询时出现的对象幻觉问题。研究发现，这种现象与广泛使用的旋转位置编码（RoPE）有关，因为RoPE的长期衰减导致LVLMs在视觉线索与指令标记距离较远时更容易出现幻觉。为了解决这个问题，作者提出了一种新的位置对齐策略，称为同心因果注意力（CCA），可以有效减少视觉标记与指令标记之间的相对距离。通过这种方法，模型的感知能力得到增强，对象幻觉现象得到显著缓解。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16392', 'title': 'LLM-based Optimization of Compound AI Systems: A Survey', 'url': 'https://huggingface.co/papers/2410.16392', 'abstract': "In a compound AI system, components such as an LLM call, a retriever, a code interpreter, or tools are interconnected. The system's behavior is primarily driven by parameters such as instructions or tool definitions. Recent advancements enable end-to-end optimization of these parameters using an LLM. Notably, leveraging an LLM as an optimizer is particularly efficient because it avoids gradient computation and can generate complex code and instructions. This paper presents a survey of the principles and emerging trends in LLM-based optimization of compound AI systems. It covers archetypes of compound AI systems, approaches to LLM-based end-to-end optimization, and insights into future directions and broader impacts. Importantly, this survey uses concepts from program analysis to provide a unified view of how an LLM optimizer is prompted to optimize a compound AI system. The exhaustive list of paper is provided at https://github.com/linyuhongg/LLM-based-Optimization-of-Compound-AI-Systems.", 'score': 13, 'issue_id': 239, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '45fb4ad9f9ca0b4c', 'data': {'categories': ['#rag', '#optimization', '#plp', '#training', '#survey', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'LLM как универсальный оптимизатор для сложных AI-систем', 'desc': 'Статья представляет обзор принципов и тенденций в оптимизации составных систем искусственного интеллекта с использованием языковых моделей (LLM). Авторы рассматривают архетипы составных AI-систем и подходы к их сквозной оптимизации с помощью LLM. Особое внимание уделяется эффективности LLM как оптимизатора, способного генерировать сложный код и инструкции без вычисления градиентов. В работе используются концепции анализа программ для создания единого представления о том, как LLM-оптимизатор настраивается для улучшения составной AI-системы.'}, 'en': {'title': 'Optimizing Compound AI Systems with LLMs: A New Frontier', 'desc': 'This paper explores how large language models (LLMs) can optimize complex AI systems that consist of various interconnected components like retrievers and code interpreters. It highlights the efficiency of using LLMs for end-to-end optimization, as they can generate intricate code and instructions without needing gradient calculations. The authors provide a comprehensive survey of different types of compound AI systems and the methods for LLM-based optimization. Additionally, they discuss future trends and the broader implications of using LLMs in this context, integrating concepts from program analysis for a cohesive understanding.'}, 'zh': {'title': '利用LLM优化复合AI系统的未来', 'desc': '本文探讨了复合人工智能系统中，如何利用大型语言模型（LLM）进行端到端的优化。复合AI系统的组件如LLM调用、检索器和代码解释器相互连接，其行为由参数驱动。通过使用LLM作为优化器，可以避免梯度计算，并生成复杂的代码和指令，从而提高效率。文章还总结了复合AI系统的原型、LLM优化的方法以及未来的发展方向。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17250', 'title': 'JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation', 'url': 'https://huggingface.co/papers/2410.17250', 'abstract': 'Accelerating research on Large Multimodal Models (LMMs) in non-English languages is crucial for enhancing user experiences across broader populations. In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the Japanese cultural context. To facilitate comprehensive culture-aware evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA) subset, where the culture-independent subjects (e.g., Math) are selected and translated into Japanese, enabling one-to-one comparison with its English counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly crafted subjects that reflect Japanese cultural context. Using the CA subset, we observe performance drop in many LMMs when evaluated in Japanese, which is purely attributable to language variation. Using the CS subset, we reveal their inadequate Japanese cultural understanding. Further, by combining both subsets, we identify that some LMMs perform well on the CA subset but not on the CS subset, exposing a shallow understanding of the Japanese language that lacks depth in cultural understanding. We hope this work will not only help advance LMM performance in Japanese but also serve as a guideline to create high-standard, culturally diverse benchmarks for multilingual LMM development. The project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.', 'score': 12, 'issue_id': 222, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '245b35927b3cf09b', 'data': {'categories': ['#benchmark', '#multilingual', '#multimodal', '#open_source', '#low_resource', '#machine_translation'], 'emoji': '🗾', 'ru': {'title': 'Культурно-ориентированная оценка мультимодальных моделей на японском языке', 'desc': 'Статья представляет JMMMU - первый масштабный японский бенчмарк для оценки больших мультимодальных моделей (LMM) на экспертных задачах в японском культурном контексте. Бенчмарк состоит из двух подмножеств: культурно-агностического (CA) и культурно-специфического (CS). Исследование выявило снижение производительности многих LMM при оценке на японском языке, а также их недостаточное понимание японской культуры. Авторы надеются, что эта работа поможет улучшить производительность LMM на японском языке и послужит руководством для создания качественных, культурно разнообразных бенчмарков для многоязычного развития LMM.'}, 'en': {'title': 'Bridging Language and Culture in AI: The JMMMU Benchmark', 'desc': 'The paper introduces JMMMU, a benchmark for evaluating Large Multimodal Models (LMMs) in Japanese, focusing on both culture-agnostic and culture-specific tasks. It highlights that many LMMs show reduced performance in Japanese due to language differences, and struggle with tasks requiring deep cultural understanding. The study reveals that some models perform well on general tasks but fail on culturally nuanced ones, indicating a lack of cultural depth. This work aims to improve LMMs in Japanese and guide the creation of culturally diverse benchmarks for multilingual models.'}, 'zh': {'title': '推动多语言模型的文化深度理解', 'desc': '这篇论文介绍了JMMMU，这是第一个用于评估大型多模态模型在日本文化背景下表现的基准。JMMMU包括两个子集：文化无关子集和文化特定子集。研究发现，许多模型在日语环境下表现下降，尤其是在文化特定子集上表现不佳，显示出对日本文化理解的不足。通过这项研究，作者希望推动多语言模型在日语中的表现，并为创建高标准的多文化基准提供指导。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17215', 'title': 'MiniPLM: Knowledge Distillation for Pre-Training Language Models', 'url': 'https://huggingface.co/papers/2410.17215', 'abstract': "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data. To address these issues, we propose MiniPLM, a KD framework for pre-training LMs by refining the training data distribution with the teacher's knowledge. For efficiency, MiniPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs. For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families. For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge. Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. The benefit of MiniPLM extends to large pre-training scales, evidenced by the extrapolation of the scaling curves. Further analysis reveals that MiniPLM supports KD across model families and enhances the utilization of pre-training data. Our model, code, and data are available at https://github.com/thu-coai/MiniPLM.", 'score': 12, 'issue_id': 221, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '50ff5ac54c5a78a2', 'data': {'categories': ['#small_models', '#optimization', '#training', '#dataset', '#transfer_learning', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'MiniPLM: Эффективная дистилляция знаний для предобучения языковых моделей', 'desc': 'Статья представляет MiniPLM - новый фреймворк для дистилляции знаний при предварительном обучении языковых моделей. MiniPLM решает проблемы эффективности, гибкости и эффективности существующих методов, выполняя офлайн-вывод учительской модели и работая только с обучающим корпусом. Этот подход улучшает распределение обучающих данных, повышая их сложность и разнообразие. Эксперименты показывают, что MiniPLM улучшает производительность ученических моделей на различных задачах и поддерживает дистилляцию знаний между разными семействами моделей.'}, 'en': {'title': 'MiniPLM: Efficient and Flexible Knowledge Distillation for Smarter Language Models', 'desc': 'The paper introduces MiniPLM, a knowledge distillation framework designed to improve the pre-training of small language models by using the knowledge from larger models. MiniPLM enhances efficiency by performing offline teacher model inference, which reduces computational costs and allows for training multiple student models simultaneously. It also increases flexibility by working directly with the training corpus, enabling knowledge distillation across different model families. The framework improves the effectiveness of training by refining the data distribution, which helps student models learn more complex and diverse information, leading to better performance on various tasks.'}, 'zh': {'title': 'MiniPLM：高效灵活的知识蒸馏框架', 'desc': '知识蒸馏（KD）常用于通过大型教师语言模型（LMs）训练小型高性能学生语言模型。MiniPLM是一种新的KD框架，通过离线教师推理提高效率，减少训练时间成本。它仅依赖训练语料库，增强了模型间的灵活性，并通过丰富训练数据的难度和多样性提高学生模型的效果。实验表明，MiniPLM在多项任务中提升了学生模型的表现，并减少了预训练计算量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.14649', 'title': 'EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search', 'url': 'https://huggingface.co/papers/2410.14649', 'abstract': 'The high computational costs of large language models (LLMs) have led to a flurry of research on LLM compression, via methods such as quantization, sparsification, or structured pruning. A new frontier in this area is given by dynamic, non-uniform compression methods, which adjust the compression levels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy loss, while guaranteeing a global compression threshold. Yet, current methods rely on heuristics for identifying the "importance" of a given layer towards the loss, based on assumptions such as error monotonicity, i.e. that the end-to-end model compression error is proportional to the sum of layer-wise errors. In this paper, we revisit this area, and propose a new and general approach for dynamic compression that is provably optimal in a given input range. We begin from the motivating observation that, in general, error monotonicity does not hold for LLMs: compressed models with lower sum of per-layer errors can perform worse than models with higher error sums. To address this, we propose a new general evolutionary framework for dynamic LLM compression called EvoPress, which has provable convergence, and low sample and evaluation complexity. We show that these theoretical guarantees lead to highly competitive practical performance for dynamic compression of Llama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art results across all compression approaches: structural pruning (block/layer dropping), unstructured sparsity, as well as quantization with dynamic bitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.', 'score': 7, 'issue_id': 222, 'pub_date': '2024-10-18', 'pub_date_card': {'ru': '18 октября', 'en': 'October 18', 'zh': '10月18日'}, 'hash': '8c7a7ff7f03a5de4', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#open_source'], 'emoji': '🗜️', 'ru': {'title': 'EvoPress: оптимальное динамическое сжатие языковых моделей', 'desc': 'Статья представляет новый подход к динамической компрессии больших языковых моделей (LLM) под названием EvoPress. Авторы опровергают предположение о монотонности ошибок в LLM и предлагают эволюционный фреймворк с доказуемой сходимостью и низкой вычислительной сложностью. EvoPress показывает высокую эффективность при сжатии моделей Llama, Mistral и Phi, устанавливая новые стандарты в структурном прунинге, неструктурированном прореживании и квантизации с динамической разрядностью. Код реализации доступен в открытом репозитории GitHub.'}, 'en': {'title': 'EvoPress: Revolutionizing LLM Compression with Dynamic Precision', 'desc': 'The paper addresses the challenge of reducing the computational costs of large language models by introducing a new method called EvoPress for dynamic compression. Unlike traditional methods that rely on assumptions like error monotonicity, EvoPress uses an evolutionary framework to adjust compression levels per block or layer, ensuring minimal accuracy loss. This approach is proven to be optimal within a specific input range and demonstrates superior performance across various compression techniques, including pruning and quantization. The authors provide theoretical guarantees for EvoPress, which translate into practical improvements for models like Llama, Mistral, and Phi.'}, 'zh': {'title': 'EvoPress：大语言模型压缩的新突破', 'desc': '这篇论文探讨了大语言模型的压缩问题，提出了一种新的动态压缩方法。传统方法依赖于假设层的重要性来进行压缩，而这种新方法则不依赖于这些假设。作者提出了一种名为EvoPress的进化框架，能够在给定输入范围内实现最优压缩。实验结果表明，EvoPress在多种压缩方法中都达到了最新的性能水平。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16930', 'title': "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes", 'url': 'https://huggingface.co/papers/2410.16930', 'abstract': "Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within a model. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a method for isolating math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by removing those important for general language tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning ability without destroying its general language ability. Scaling these parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.", 'score': 5, 'issue_id': 226, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '13a2560869429449', 'data': {'categories': ['#reasoning', '#inference', '#optimization', '#math', '#interpretability', '#training', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'Нейрохирургия для ИИ: точечное улучшение математических способностей языковых моделей', 'desc': 'Статья представляет метод MathNeuro для выделения параметров, отвечающих за математические рассуждения в больших языковых моделях (LLM). Этот метод позволяет целенаправленно улучшать математические способности модели, не затрагивая другие языковые навыки. Исследователи показали, что удаление выявленных параметров уничтожает математические способности LLM, в то время как их масштабирование улучшает производительность на 4-17% в тесте GSM8K. MathNeuro эффективен даже при использовании единственного образца данных, что открывает перспективы для дальнейших исследований в области целенаправленного вмешательства в параметры LLM.'}, 'en': {'title': '"Unlocking Math Genius in AI: Isolate, Enhance, Excel!"', 'desc': 'The paper introduces Math Neurosurgery (MathNeuro), a method to isolate math-specific parameters in Large Language Models (LLMs) using only forward passes. MathNeuro identifies and prunes parameters crucial for math reasoning without affecting general language abilities. By scaling these parameters, the method enhances math performance by 4-17% on GSM8K, while keeping non-math behavior unchanged. This approach is data-efficient, requiring only a single sample to identify math-specific parameters, paving the way for targeted improvements in math reasoning.'}, 'zh': {'title': 'MathNeuro：精准提升数学推理能力的利器', 'desc': '这篇论文介绍了一种名为MathNeuro的方法，用于在大型语言模型中隔离数学推理参数。MathNeuro通过前向传播计算参数的重要性，并去除对一般语言任务重要的参数，从而专注于数学推理。通过调整这些参数，模型的数学性能可以提高4-17%，而不影响其语言能力。MathNeuro展示了在不改变非数学行为的情况下，如何有效地提升数学推理能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17241', 'title': 'Frontiers in Intelligent Colonoscopy', 'url': 'https://huggingface.co/papers/2410.17241', 'abstract': 'Colonoscopy is currently one of the most sensitive screening methods for colorectal cancer. This study investigates the frontiers of intelligent colonoscopy techniques and their prospective implications for multimodal medical applications. With this goal, we begin by assessing the current data-centric and model-centric landscapes through four tasks for colonoscopic scene perception, including classification, detection, segmentation, and vision-language understanding. This assessment enables us to identify domain-specific challenges and reveals that multimodal research in colonoscopy remains open for further exploration. To embrace the coming multimodal era, we establish three foundational initiatives: a large-scale multimodal instruction tuning dataset ColonINST, a colonoscopy-designed multimodal language model ColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this rapidly evolving field, we provide a public website for the latest updates: https://github.com/ai4colonoscopy/IntelliScope.', 'score': 2, 'issue_id': 239, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'ad0212da4f7fb7b9', 'data': {'categories': ['#science', '#synthetic', '#benchmark', '#cv', '#healthcare', '#multimodal', '#training', '#dataset', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Мультимодальные ИИ-технологии открывают новые горизонты в колоноскопии', 'desc': 'Это исследование посвящено изучению современных интеллектуальных методов колоноскопии и их потенциальному применению в мультимодальных медицинских приложениях. Авторы оценивают текущее состояние методов, основанных на данных и моделях, для четырех задач восприятия колоноскопических сцен: классификации, обнаружения, сегментации и понимания визуально-языковой информации. На основе этой оценки они выявляют специфические для данной области проблемы и показывают, что мультимодальные исследования в колоноскопии остаются открытыми для дальнейшего изучения. Для поддержки развития мультимодальных подходов авторы предлагают три основные инициативы: большой набор данных для мультимодальной настройки инструкций ColonINST, мультимодальную языковую модель ColonGPT, разработанную специально для колоноскопии, и мультимодальный бенчмарк.'}, 'en': {'title': 'Advancing Intelligent Colonoscopy for Better Cancer Detection', 'desc': 'This paper explores advanced techniques in intelligent colonoscopy, which is crucial for detecting colorectal cancer. It evaluates current methods in four key areas: classification, detection, segmentation, and vision-language understanding, highlighting specific challenges in the field. The authors propose three initiatives to support multimodal research, including a new dataset called ColonINST, a specialized language model named ColonGPT, and a benchmark for evaluation. Additionally, they offer a public website to keep the community informed about developments in this area.'}, 'zh': {'title': '智能结肠镜：多模态医学的未来', 'desc': '本研究探讨了智能结肠镜技术的前沿及其在多模态医学应用中的潜在影响。我们通过分类、检测、分割和视觉语言理解四个任务评估了当前的以数据和模型为中心的研究现状。研究发现，结肠镜领域的多模态研究仍有许多挑战和未开发的空间。为迎接多模态时代，我们建立了三个基础性倡议，包括大规模的多模态指令调优数据集ColonINST、专为结肠镜设计的多模态语言模型ColonGPT，以及一个多模态基准。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16266', 'title': '3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors', 'url': 'https://huggingface.co/papers/2410.16266', 'abstract': 'Novel-view synthesis aims to generate novel views of a scene from multiple input images or videos, and recent advancements like 3D Gaussian splatting (3DGS) have achieved notable success in producing photorealistic renderings with efficient pipelines. However, generating high-quality novel views under challenging settings, such as sparse input views, remains difficult due to insufficient information in under-sampled areas, often resulting in noticeable artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing the representation quality of 3DGS representations. We leverage 2D video diffusion priors to address the challenging 3D view consistency problem, reformulating it as achieving temporal consistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with the input views through a spatial-temporal decoder. The enhanced views are then used to fine-tune the initial 3DGS model, significantly improving its rendering performance. Extensive experiments on large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields superior reconstruction performance and high-fidelity rendering results compared to state-of-the-art methods. The project webpage is https://xiliu8006.github.io/3DGS-Enhancer-project .', 'score': 2, 'issue_id': 237, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'f00040bffceab087', 'data': {'categories': ['#diffusion', '#cv', '#video', '#optimization', '#training', '#dataset', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Улучшение синтеза новых ракурсов с помощью видео-диффузии', 'desc': 'Статья представляет 3DGS-Enhancer - новый метод улучшения качества представления сцен в задаче синтеза новых ракурсов. Авторы используют 2D видео-диффузионные приоры для решения проблемы согласованности ракурсов в 3D, переформулируя ее как достижение временной согласованности в процессе генерации видео. Метод восстанавливает согласованные латентные признаки отрендеренных новых ракурсов и интегрирует их с исходными видами через пространственно-временной декодер. Улучшенные виды затем используются для дообучения исходной 3DGS модели, значительно повышая качество рендеринга.'}, 'en': {'title': 'Enhancing 3D Views with Temporal Consistency', 'desc': 'The paper introduces 3DGS-Enhancer, a new method to improve the quality of 3D Gaussian splatting (3DGS) for novel-view synthesis, especially in challenging scenarios with sparse input views. By using 2D video diffusion priors, the approach addresses the issue of maintaining 3D view consistency by ensuring temporal consistency in video generation. The method enhances the latent features of rendered views and combines them with input views using a spatial-temporal decoder, which is then used to refine the original 3DGS model. Experiments show that 3DGS-Enhancer significantly outperforms existing methods in rendering high-quality, photorealistic scenes.'}, 'zh': {'title': '3D视图一致性的新突破：3DGS-Enhancer', 'desc': '这篇论文介绍了一种名为3DGS-Enhancer的新方法，用于提升3D高斯点云表示的质量。通过利用2D视频扩散先验，该方法解决了3D视图一致性的问题，将其重新定义为视频生成过程中的时间一致性。3DGS-Enhancer通过空间-时间解码器将渲染的新视图与输入视图结合，恢复视图一致的潜在特征。实验表明，该方法在大规模数据集上表现出色，显著提高了渲染性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17637', 'title': 'MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.17637', 'abstract': "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing optimization algorithms like direct preference optimization (DPO). Existing visual alignment methods, primarily designed for single-image scenarios, struggle to effectively handle the complexity of multi-image tasks due to the scarcity of diverse training data and the high cost of annotating chosen/rejected pairs. We present Multi-Image Augmented Direct Preference Optimization (MIA-DPO), a visual preference alignment approach that effectively handles multi-image inputs. MIA-DPO mitigates the scarcity of diverse multi-image training data by extending single-image data with unrelated images arranged in grid collages or pic-in-pic formats, significantly reducing the costs associated with multi-image data annotations. Our observation reveals that attention values of LVLMs vary considerably across different images. We use attention values to identify and filter out rejected responses the model may have mistakenly focused on. Our attention-aware selection for constructing the chosen/rejected pairs without relying on (i) human annotation, (ii) extra data, and (iii) external models or APIs. MIA-DPO is compatible with various architectures and outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the recent InternLM-XC2.5. Moreover, MIA-DPO has a minimal effect on the model's ability to understand single images.", 'score': 34, 'issue_id': 243, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '2dc1395b8aa096fc', 'data': {'categories': ['#synthetic', '#rlhf', '#benchmark', '#cv', '#optimization', '#dataset', '#architecture', '#alignment'], 'emoji': '🖼️', 'ru': {'title': 'MIA-DPO: эффективное обучение LVLM на многоизображительных данных', 'desc': 'Статья представляет новый метод обучения больших визуально-языковых моделей (LVLM) под названием MIA-DPO. Этот подход эффективно работает с многоизображительными входными данными, решая проблему нехватки разнообразных обучающих данных. MIA-DPO расширяет однозображительные данные, добавляя несвязанные изображения в виде сеток или картинок-в-картинке. Метод использует значения внимания LVLM для идентификации и фильтрации ошибочно отвергнутых ответов, не требуя дополнительной аннотации или внешних моделей.'}, 'en': {'title': 'Enhancing Visual Preference Prediction with MIA-DPO', 'desc': 'The paper introduces a new method called Multi-Image Augmented Direct Preference Optimization (MIA-DPO) to improve how Large Vision-Language Models (LVLMs) predict human preferences between multiple images. MIA-DPO cleverly uses existing single-image data by combining them into collages, which helps reduce the need for expensive multi-image annotations. The method also uses attention values from the models to automatically filter out less relevant images, avoiding the need for human input or extra data. This approach is shown to work well with different model architectures, improving performance on several benchmarks without affecting single-image understanding.'}, 'zh': {'title': '多图像任务的视觉偏好对齐新突破', 'desc': '这篇论文介绍了一种新的视觉偏好对齐方法，称为多图像增强直接偏好优化（MIA-DPO）。MIA-DPO通过将单图像数据与不相关的图像组合在一起，解决了多图像任务中训练数据稀缺的问题，从而降低了数据标注的成本。该方法利用大规模视觉语言模型的注意力值来识别和过滤掉错误关注的图像，无需依赖人工标注或额外数据。实验结果表明，MIA-DPO在多个多图像基准测试中表现优异，并且对单图像理解能力影响较小。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17434', 'title': 'LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding', 'url': 'https://huggingface.co/papers/2410.17434', 'abstract': "Multimodal Large Language Models (MLLMs) have shown promising progress in understanding and analyzing video content. However, processing long videos remains a significant challenge constrained by LLM's context size. To address this limitation, we propose LongVU, a spatiotemporal adaptive compression mechanism thats reduces the number of video tokens while preserving visual details of long videos. Our idea is based on leveraging cross-modal query and inter-frame dependencies to adaptively reduce temporal and spatial redundancy in videos. Specifically, we leverage DINOv2 features to remove redundant frames that exhibit high similarity. Then we utilize text-guided cross-modal query for selective frame feature reduction. Further, we perform spatial token reduction across frames based on their temporal dependencies. Our adaptive compression strategy effectively processes a large number of frames with little visual information loss within given context length. Our LongVU consistently surpass existing methods across a variety of video understanding benchmarks, especially on hour-long video understanding tasks such as VideoMME and MLVU. Given a light-weight LLM, our LongVU also scales effectively into a smaller size with state-of-the-art video understanding performance.", 'score': 24, 'issue_id': 253, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'ab4b27c05d7611e1', 'data': {'categories': ['#long_context', '#small_models', '#benchmark', '#video', '#optimization', '#multimodal', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'LongVU: Эффективное сжатие для понимания длинных видео', 'desc': 'LongVU - это новый механизм адаптивного сжатия для анализа длинных видео мультимодальными большими языковыми моделями (MLLM). Он использует кросс-модальные запросы и межкадровые зависимости для уменьшения временной и пространственной избыточности в видео. LongVU применяет признаки DINOv2 для удаления похожих кадров и текстовые запросы для выборочного сокращения признаков кадров. Этот подход позволяет эффективно обрабатывать большое количество кадров с минимальной потерей визуальной информации в рамках заданного контекста.'}, 'en': {'title': '"LongVU: Mastering Long Video Understanding with Efficient Compression"', 'desc': 'The paper introduces LongVU, a method to efficiently process long videos using Multimodal Large Language Models (MLLMs) by reducing video tokens while maintaining visual details. It uses a spatiotemporal adaptive compression mechanism that leverages cross-modal queries and inter-frame dependencies to minimize redundancy. By employing DINOv2 features, it removes similar frames and uses text-guided queries for selective frame reduction. LongVU outperforms existing methods in video understanding tasks, especially with hour-long videos, and scales well with smaller LLMs.'}, 'zh': {'title': 'LongVU：长视频理解的新突破', 'desc': '这篇论文介绍了一种名为LongVU的新方法，用于处理长视频中的信息冗余问题。LongVU通过跨模态查询和帧间依赖性，适应性地减少视频中的时间和空间冗余。具体来说，它利用DINOv2特征去除相似度高的冗余帧，并通过文本引导的跨模态查询选择性地减少帧特征。该方法在处理长视频时，能够在保持视觉细节的同时，显著减少视频令牌数量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18072', 'title': 'WorldSimBench: Towards Video Generation Models as World Simulators', 'url': 'https://huggingface.co/papers/2410.18072', 'abstract': 'Recent advancements in predictive models have demonstrated exceptional capabilities in predicting the future state of objects and scenes. However, the lack of categorization based on inherent characteristics continues to hinder the progress of predictive model development. Additionally, existing benchmarks are unable to effectively evaluate higher-capability, highly embodied predictive models from an embodied perspective. In this work, we classify the functionalities of predictive models into a hierarchy and take the first step in evaluating World Simulators by proposing a dual evaluation framework called WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, encompassing human preference assessments from the visual perspective and action-level evaluations in embodied tasks, covering three representative embodied scenarios: Open-Ended Embodied Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment dataset based on fine-grained human feedback, which we use to train a Human Preference Evaluator that aligns with human perception and explicitly assesses the visual fidelity of World Simulators. In the Implicit Manipulative Evaluation, we assess the video-action consistency of World Simulators by evaluating whether the generated situation-aware video can be accurately translated into the correct control signals in dynamic environments. Our comprehensive evaluation offers key insights that can drive further innovation in video generation models, positioning World Simulators as a pivotal advancement toward embodied artificial intelligence.', 'score': 17, 'issue_id': 243, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'e4c42676df8ded0d', 'data': {'categories': ['#synthetic', '#rlhf', '#benchmark', '#video', '#multimodal', '#interpretability', '#robotics', '#dataset', '#games', '#alignment'], 'emoji': '🌐', 'ru': {'title': 'WorldSimBench: Комплексная оценка симуляторов мира для воплощенного ИИ', 'desc': 'Статья представляет новый подход к оценке предиктивных моделей, называемый WorldSimBench. Эта система включает в себя явную перцептивную оценку и неявную манипулятивную оценку, охватывающие три сценария: открытую среду, автономное вождение и робототехнику. Авторы вводят датасет HF-Embodied для оценки визуальной точности симуляторов мира на основе человеческих предпочтений. Также предлагается оценка соответствия видео и действий, проверяя, могут ли сгенерированные видео точно преобразовываться в сигналы управления в динамических средах.'}, 'en': {'title': 'Advancing Predictive Models with WorldSimBench: A New Era for Embodied AI', 'desc': 'This paper addresses the challenge of categorizing predictive models based on their inherent characteristics to enhance their development. It introduces WorldSimBench, a dual evaluation framework for assessing World Simulators, which includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation. The framework uses the HF-Embodied Dataset to train a Human Preference Evaluator for visual fidelity and assesses video-action consistency in dynamic environments. This comprehensive evaluation aims to advance video generation models and promote the development of embodied artificial intelligence.'}, 'zh': {'title': '推动具身人工智能的世界模拟器评估', 'desc': '这篇论文探讨了预测模型在预测物体和场景未来状态方面的能力，但指出缺乏基于内在特征的分类限制了其发展。为了解决这个问题，作者提出了一个名为WorldSimBench的双重评估框架，用于评估世界模拟器。WorldSimBench包括显式感知评估和隐式操作评估，涵盖了开放式环境、自主驾驶和机器人操作等场景。通过这种评估，研究人员希望推动视频生成模型的创新，促进具身人工智能的发展。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17891', 'title': 'Scaling Diffusion Language Models via Adaptation from Autoregressive Models', 'url': 'https://huggingface.co/papers/2410.17891', 'abstract': 'Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions https://github.com/HKUNLP/DiffuLLaMA.', 'score': 15, 'issue_id': 243, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'a6f6dc7932b17a7f', 'data': {'categories': ['#diffusion', '#small_models', '#reasoning', '#benchmark', '#training', '#transfer_learning', '#open_source', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'Преобразование авторегрессионных моделей в диффузионные: новый шаг в языковом моделировании', 'desc': 'Исследователи предлагают новый подход к созданию диффузионных языковых моделей (DLM), адаптируя существующие авторегрессионные модели. Они демонстрируют связь между целевыми функциями авторегрессионных и диффузионных моделей и вводят метод дообучения для создания DLM. Эксперименты показывают, что полученные модели DiffuGPT и DiffuLLaMA превосходят предыдущие DLM и конкурентоспособны с авторегрессионными аналогами. Авторы представляют набор DLM различных размеров, способных генерировать связный текст и выполнять различные языковые задачи.'}, 'en': {'title': 'Revolutionizing Text Generation: From Autoregressive to Diffusion Models', 'desc': 'This paper explores the potential of Diffusion Language Models (DLMs) as an alternative to traditional autoregressive (AR) models for text generation. The authors propose a method to adapt existing AR models into DLMs, demonstrating that these adapted models can perform competitively on various language tasks. They introduce a continual pre-training approach to efficiently train these diffusion models using fewer resources. The study shows that the adapted models, DiffuGPT and DiffuLLaMA, outperform previous DLMs and match the performance of AR models on several benchmarks.'}, 'zh': {'title': '扩散语言模型：突破自回归模型的局限', 'desc': '扩散语言模型（DLMs）是一种新兴的文本生成模型，可能解决自回归（AR）模型的局限性。研究表明，通过将开源的AR模型适配为扩散模型，可以在语言建模基准上实现更好的性能。我们提出了一种简单的持续预训练方法，将AR模型转换为扩散模型，并在多个基准上进行了系统评估。实验结果显示，这些转换后的模型在生成流畅文本和上下文学习等任务上表现优异。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18013', 'title': 'Scalable Ranked Preference Optimization for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2410.18013', 'abstract': "Direct Preference Optimization (DPO) has emerged as a powerful approach to align text-to-image (T2I) models with human feedback. Unfortunately, successful application of DPO to T2I models requires a huge amount of resources to collect and label large-scale datasets, e.g., millions of generated paired images annotated with human preferences. In addition, these human preference datasets can get outdated quickly as the rapid improvements of T2I models lead to higher quality images. In this work, we investigate a scalable approach for collecting large-scale and fully synthetic datasets for DPO training. Specifically, the preferences for paired images are generated using a pre-trained reward function, eliminating the need for involving humans in the annotation process, greatly improving the dataset collection efficiency. Moreover, we demonstrate that such datasets allow averaging predictions across multiple models and collecting ranked preferences as opposed to pairwise preferences. Furthermore, we introduce RankDPO to enhance DPO-based methods using the ranking feedback. Applying RankDPO on SDXL and SD3-Medium models with our synthetically generated preference dataset ``Syn-Pic'' improves both prompt-following (on benchmarks like T2I-Compbench, GenEval, and DPG-Bench) and visual quality (through user studies). This pipeline presents a practical and scalable solution to develop better preference datasets to enhance the performance of text-to-image models.", 'score': 14, 'issue_id': 246, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '05611e12662f347d', 'data': {'categories': ['#synthetic', '#rlhf', '#benchmark', '#cv', '#optimization', '#data', '#training', '#dataset', '#alignment'], 'emoji': '🖼️', 'ru': {'title': 'Синтетические данные для улучшения генерации изображений по тексту', 'desc': 'Статья представляет новый подход к улучшению моделей генерации изображений по текстовому описанию (text-to-image). Авторы предлагают использовать синтетические наборы данных для обучения методом Direct Preference Optimization (DPO), что позволяет избежать трудоемкого процесса сбора предпочтений от людей. Они вводят метод RankDPO, который использует ранжированные предпочтения вместо попарных. Эксперименты показывают, что предложенный подход улучшает как следование промпту, так и визуальное качество генерируемых изображений.'}, 'en': {'title': '"Revolutionizing Text-to-Image Models with Synthetic Preferences"', 'desc': 'The paper explores a new method for improving text-to-image models by using Direct Preference Optimization (DPO) without relying on human-annotated datasets. Instead of collecting human feedback, the authors use a pre-trained reward function to generate synthetic datasets, which makes the process more efficient and scalable. They introduce RankDPO, a technique that uses ranking feedback to further enhance model performance. The results show that using these synthetic datasets improves both the accuracy of following prompts and the visual quality of generated images.'}, 'zh': {'title': '合成数据集：提升文本到图像模型的新途径', 'desc': '这篇论文探讨了一种可扩展的方法来收集用于直接偏好优化（DPO）训练的大规模全合成数据集。通过使用预训练的奖励函数生成图像对的偏好，消除了人工标注的需求，大大提高了数据集收集的效率。此外，研究表明，这种数据集允许在多个模型之间进行预测平均，并收集排名偏好而不是成对偏好。引入的RankDPO方法通过排名反馈增强了基于DPO的方法，提升了文本到图像模型的性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18084', 'title': 'DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes', 'url': 'https://huggingface.co/papers/2410.18084', 'abstract': 'LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D LiDAR generation framework capable of generating large-scale, high-quality LiDAR scenes that capture the temporal evolution of dynamic environments. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research.', 'score': 12, 'issue_id': 250, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'c726f29cc044d5f6', 'data': {'categories': ['#diffusion', '#cv', '#graphs', '#optimization', '#dataset', '#open_source', '#games', '#3d', '#architecture'], 'emoji': '🚗', 'ru': {'title': 'DynamicCity: революция в генерации динамических 4D LiDAR сцен', 'desc': 'Статья представляет DynamicCity - новую систему для генерации динамических 4D LiDAR сцен. Основу системы составляют VAE модель для создания компактного 4D представления HexPlane и DiT-based диффузионная модель для генерации HexPlane. Авторы предлагают ряд инновационных методов, включая Projection Module и Expansion & Squeeze Strategy, для повышения качества и эффективности генерации. Эксперименты показывают, что DynamicCity значительно превосходит существующие методы генерации 4D LiDAR сцен по нескольким метрикам.'}, 'en': {'title': 'DynamicCity: Bringing 4D LiDAR Scenes to Life', 'desc': 'DynamicCity is a new framework for generating 4D LiDAR scenes that capture the dynamic nature of real-world environments. It uses a VAE model to create a compact 4D representation called HexPlane, which improves fitting quality by compressing features into 2D maps. The framework also includes an Expansion & Squeeze Strategy to enhance training efficiency and accuracy. Additionally, a DiT-based diffusion model is used for generating HexPlane, allowing for versatile applications like trajectory-driven generation and inpainting.'}, 'zh': {'title': 'DynamicCity：捕捉动态环境的4D LiDAR生成框架', 'desc': '这篇论文介绍了一种名为DynamicCity的新型4D LiDAR生成框架，能够生成大规模、高质量的动态环境LiDAR场景。DynamicCity主要由两个关键模型组成：一个VAE模型用于学习HexPlane作为紧凑的4D表示，通过投影模块将4D特征压缩为六个2D特征图，从而提高了HexPlane的拟合质量。另一个是基于DiT的扩散模型，用于生成HexPlane，并通过填充展开操作将六个特征平面重新组织为一个方形2D特征图。实验表明，DynamicCity在多个指标上显著优于现有的4D LiDAR生成方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15522', 'title': 'M-RewardBench: Evaluating Reward Models in Multilingual Settings', 'url': 'https://huggingface.co/papers/2410.15522', 'abstract': "Reward models (RMs) have driven the state-of-the-art performance of LLMs today by enabling the integration of human feedback into the language modeling process. However, RMs are primarily trained and evaluated in English, and their capabilities in multilingual settings remain largely understudied. In this work, we conduct a systematic evaluation of several reward models in multilingual settings. We first construct the first-of-its-kind multilingual RM evaluation benchmark, M-RewardBench, consisting of 2.87k preference instances for 23 typologically diverse languages, that tests the chat, safety, reasoning, and translation capabilities of RMs. We then rigorously evaluate a wide range of reward models on M-RewardBench, offering fresh insights into their performance across diverse languages. We identify a significant gap in RMs' performances between English and non-English languages and show that RM preferences can change substantially from one language to another. We also present several findings on how different multilingual aspects impact RM performance. Specifically, we show that the performance of RMs is improved with improved translation quality. Similarly, we demonstrate that the models exhibit better performance for high-resource languages. We release M-RewardBench dataset and the codebase in this study to facilitate a better understanding of RM evaluation in multilingual settings.", 'score': 10, 'issue_id': 247, 'pub_date': '2024-10-20', 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}, 'hash': '5143b0b6f1067fee', 'data': {'categories': ['#rlhf', '#benchmark', '#multilingual', '#dataset', '#open_source', '#low_resource', '#machine_translation', '#alignment'], 'emoji': '🌍', 'ru': {'title': 'Многоязычная оценка моделей вознаграждения: новые горизонты и вызовы', 'desc': 'Данная статья посвящена исследованию моделей вознаграждения (reward models) в многоязычном контексте. Авторы создали первый в своем роде многоязычный эталонный набор данных M-RewardBench для оценки таких моделей на 23 типологически разных языках. Проведя тщательную оценку различных моделей вознаграждения, исследователи выявили значительный разрыв в производительности между английским и другими языками. Результаты показывают, что качество перевода и ресурсообеспеченность языка положительно влияют на эффективность моделей вознаграждения.'}, 'en': {'title': 'Bridging the Language Gap: Evaluating Reward Models Multilingually', 'desc': "This paper explores how reward models (RMs), which enhance language models by incorporating human feedback, perform in multilingual contexts. The authors introduce M-RewardBench, a new benchmark with 2.87k preference instances across 23 languages, to evaluate RMs' abilities in chat, safety, reasoning, and translation. Their findings reveal a performance gap between English and other languages, with RMs showing varied preferences across languages. The study highlights that better translation quality and high-resource languages improve RM performance, and they provide the M-RewardBench dataset and codebase for further research."}, 'zh': {'title': '多语言奖励模型：跨语言表现的新视角', 'desc': '这篇论文研究了奖励模型在多语言环境中的表现。研究者们创建了一个名为M-RewardBench的多语言评估基准，包含23种语言的偏好实例。结果显示，奖励模型在英语和非英语语言之间的表现存在显著差距。研究还发现，翻译质量和语言资源丰富度对模型表现有重要影响。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17883', 'title': 'Lightweight Neural App Control', 'url': 'https://huggingface.co/papers/2410.17883', 'abstract': 'This paper introduces a novel mobile phone control architecture, termed ``app agents", for efficient interactions and controls across various Android apps. The proposed Lightweight Multi-modal App Control (LiMAC) takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, within LiMAC, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution. We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.', 'score': 8, 'issue_id': 243, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '66e9ccd38bb979e0', 'data': {'categories': ['#small_models', '#cv', '#multimodal', '#training', '#dataset', '#transfer_learning', '#open_source', '#agents', '#architecture'], 'emoji': '📱', 'ru': {'title': 'Эффективное управление Android-приложениями с помощью легковесных агентов', 'desc': "Статья представляет новую архитектуру управления мобильными приложениями под названием 'app agents'. Предложенная система LiMAC использует текстовую цель и последовательность предыдущих наблюдений для генерации точных действий. Авторы вводят компактный Action Transformer (AcT) в сочетании с настроенной мультимодальной моделью для принятия решений в реальном времени. Эксперименты показывают, что LiMAC превосходит базовые модели на основе промптов и настроенные мультимодальные модели, повышая точность действий до 42%."}, 'en': {'title': '"LiMAC: Smarter, Faster Mobile App Control"', 'desc': 'This paper presents a new system called "app agents" for controlling Android apps more efficiently. The system, LiMAC, uses text goals and past mobile data like screenshots to decide what actions to take. It includes a small Action Transformer and a vision-language model to work quickly on smartphones. Tests show LiMAC performs better than other models, improving action accuracy by up to 42%.'}, 'zh': {'title': 'LiMAC：提升手机应用控制效率的新架构', 'desc': '这篇论文介绍了一种新的手机控制架构，称为“应用代理”，用于在各种安卓应用中实现高效的交互和控制。提出的轻量级多模态应用控制（LiMAC）可以根据文本目标和过去的手机观察（如截图和UI树）生成精确的操作。为了应对智能手机的计算限制，LiMAC中引入了一个小型动作变换器（AcT），并与微调的视觉语言模型（VLM）集成，实现实时决策和任务执行。实验结果表明，LiMAC在两个开源移动控制数据集上的表现优于微调的开源VLM，并显著超过了使用闭源基础模型的提示工程基线。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13458', 'title': 'MedINST: Meta Dataset of Biomedical Instructions', 'url': 'https://huggingface.co/papers/2410.13458', 'abstract': "The integration of large language model (LLM) techniques in the field of medical analysis has brought about significant advancements, yet the scarcity of large, diverse, and well-annotated datasets remains a major challenge. Medical data and tasks, which vary in format, size, and other parameters, require extensive preprocessing and standardization for effective use in training LLMs. To address these challenges, we introduce MedINST, the Meta Dataset of Biomedical Instructions, a novel multi-domain, multi-task instructional meta-dataset. MedINST comprises 133 biomedical NLP tasks and over 7 million training samples, making it the most comprehensive biomedical instruction dataset to date. Using MedINST as the meta dataset, we curate MedINST32, a challenging benchmark with different task difficulties aiming to evaluate LLMs' generalization ability. We fine-tune several LLMs on MedINST and evaluate on MedINST32, showcasing enhanced cross-task generalization.", 'score': 6, 'issue_id': 250, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '838e617c67ce19c3', 'data': {'categories': ['#science', '#synthetic', '#benchmark', '#healthcare', '#data', '#training', '#dataset', '#transfer_learning'], 'emoji': '🧬', 'ru': {'title': 'MedINST: революция в обучении LLM для медицинского анализа', 'desc': 'Статья представляет MedINST - новый мета-датасет медицинских инструкций для обучения больших языковых моделей (LLM) в области биомедицинского анализа. MedINST включает 133 задачи и более 7 миллионов обучающих примеров, что делает его самым обширным биомедицинским инструкционным датасетом на сегодняшний день. Авторы также создали MedINST32 - сложный бенчмарк для оценки способности LLM к обобщению. Эксперименты показали улучшенную кросс-задачную генерализацию моделей, обученных на MedINST.'}, 'en': {'title': 'MedINST: Revolutionizing Medical NLP with Comprehensive Datasets', 'desc': 'The paper discusses the development of MedINST, a comprehensive meta-dataset designed to improve the training of large language models (LLMs) in the medical field. MedINST includes 133 different biomedical natural language processing tasks and over 7 million training samples, addressing the challenge of limited and diverse medical datasets. The authors also introduce MedINST32, a benchmark to test the generalization abilities of LLMs across various tasks. By fine-tuning LLMs on MedINST, the study demonstrates improved cross-task generalization, highlighting the potential of MedINST in advancing medical data analysis.'}, 'zh': {'title': 'MedINST：突破医学数据集瓶颈的创新解决方案', 'desc': '这篇论文介绍了在医学分析领域中整合大型语言模型技术所带来的进步，但也指出了缺乏大型、多样化和良好标注的数据集是一个主要挑战。为了应对这些挑战，作者们引入了MedINST，一个新的多领域、多任务的指令元数据集。MedINST包含133个生物医学自然语言处理任务和超过700万个训练样本，是迄今为止最全面的生物医学指令数据集。通过在MedINST上微调多个大型语言模型，并在MedINST32基准上进行评估，展示了增强的跨任务泛化能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13924', 'title': 'ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding', 'url': 'https://huggingface.co/papers/2410.13924', 'abstract': 'The performance of neural networks scales with both their size and the amount of data they have been trained on. This is shown in both language and image generation. However, this requires scaling-friendly network architectures as well as large-scale datasets. Even though scaling-friendly architectures like transformers have emerged for 3D vision tasks, the GPT-moment of 3D vision remains distant due to the lack of training data. In this paper, we introduce ARKit LabelMaker, the first large-scale, real-world 3D dataset with dense semantic annotations. Specifically, we complement ARKitScenes dataset with dense semantic annotations that are automatically generated at scale. To this end, we extend LabelMaker, a recent automatic annotation pipeline, to serve the needs of large-scale pre-training. This involves extending the pipeline with cutting-edge segmentation models as well as making it robust to the challenges of large-scale processing. Further, we push forward the state-of-the-art performance on ScanNet and ScanNet200 dataset with prevalent 3D semantic segmentation models, demonstrating the efficacy of our generated dataset.', 'score': 6, 'issue_id': 249, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': '37c34e5be2bb734e', 'data': {'categories': ['#synthetic', '#cv', '#optimization', '#data', '#training', '#dataset', '#3d'], 'emoji': '🏷️', 'ru': {'title': 'Большие данные для прорыва в 3D-зрении', 'desc': 'Эта статья представляет ARKit LabelMaker - первый крупномасштабный набор данных реального мира с плотными семантическими аннотациями для задач 3D-зрения. Авторы расширили существующий инструмент LabelMaker, чтобы автоматически генерировать аннотации в больших масштабах. Они использовали современные модели сегментации и сделали процесс устойчивым к проблемам обработки больших данных. Результаты показывают улучшение производительности моделей семантической сегментации на наборах данных ScanNet и ScanNet200.'}, 'en': {'title': 'Unlocking 3D Vision with ARKit LabelMaker: A New Era of Large-Scale Datasets', 'desc': 'The paper discusses how the performance of neural networks improves with larger sizes and more data, particularly in language and image generation. It highlights the challenge in 3D vision tasks due to a lack of large-scale datasets, despite having suitable architectures like transformers. The authors introduce ARKit LabelMaker, a new large-scale 3D dataset with dense semantic annotations, enhancing the ARKitScenes dataset. They demonstrate the effectiveness of this dataset by improving state-of-the-art results on existing 3D semantic segmentation benchmarks like ScanNet.'}, 'zh': {'title': 'ARKit LabelMaker：推动3D视觉的未来', 'desc': '这篇论文讨论了神经网络的性能如何随着其规模和训练数据量的增加而提高。为了实现这一点，需要适合扩展的网络架构和大规模数据集。作者介绍了ARKit LabelMaker，这是第一个具有密集语义标注的大规模真实3D数据集。通过改进自动标注流程，作者展示了在3D语义分割任务中使用该数据集的有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18071', 'title': "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts", 'url': 'https://huggingface.co/papers/2410.18071', 'abstract': "Recently, multimodal large language models (MLLMs) have received much attention for their impressive capabilities. The evaluation of MLLMs is becoming critical to analyzing attributes of MLLMs and providing valuable insights. However, current benchmarks overlook the problem of prompt sensitivity - minor prompt variations may lead to significant performance fluctuations. Thus, inappropriate prompts may obscure the models' capabilities, underestimating the models' performance. Moreover, different models have different preferences for different prompts, and thus, using the same prompt for all models will cause evaluation bias. This paper analyzes this deficiency in existing benchmarks and further introduces a new evaluation framework named TP-Eval, which introduces a prompt customization method to reduce evaluation biases and tap models' potential. TP-Eval will rewrite the original prompts to different customized prompts for different models. In particular, we propose some well-designed modules for prompt customization tailored to the scenario of MLLM evaluation. Extensive experiments demonstrate the effectiveness of our approach to uncovering models' capabilities, and TP-Eval should benefit the community in developing more comprehensive and convincing MLLM evaluation benchmarks.", 'score': 6, 'issue_id': 245, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '9fe4ef775d3f190c', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#interpretability', '#ethics', '#architecture'], 'emoji': '🎯', 'ru': {'title': 'Точная настройка промптов для справедливой оценки мультимодальных языковых моделей', 'desc': 'Статья посвящена проблеме чувствительности к промптам в мультимодальных больших языковых моделях (MLLM). Авторы предлагают новую систему оценки TP-Eval, которая использует метод настройки промптов для снижения предвзятости оценки и раскрытия потенциала моделей. TP-Eval переписывает исходные промпты в индивидуальные варианты для разных моделей. Эксперименты показывают эффективность этого подхода в выявлении возможностей моделей.'}, 'en': {'title': '"Unlocking True Potential: Tailored Prompts for Fair MLLM Evaluation"', 'desc': 'The paper discusses the challenges in evaluating multimodal large language models (MLLMs) due to prompt sensitivity, where small changes in prompts can lead to significant performance differences. It highlights that using the same prompt for all models can introduce bias, as different models may respond better to different prompts. To address this, the authors propose a new evaluation framework called TP-Eval, which customizes prompts for each model to reduce bias and better reveal their capabilities. The framework includes specially designed modules for prompt customization, and experiments show that TP-Eval effectively enhances the evaluation of MLLMs.'}, 'zh': {'title': 'TP-Eval：定制化提示词，挖掘模型潜力', 'desc': '这篇论文讨论了多模态大语言模型（MLLMs）的评估问题，特别是提示词敏感性的问题。现有的评估标准常常忽视了提示词的微小变化可能导致模型性能的显著波动。为了解决这个问题，作者提出了一种新的评估框架TP-Eval，通过定制化提示词来减少评估偏差。实验结果表明，TP-Eval能够更好地挖掘模型的潜力，为多模态大语言模型的评估提供了更全面和有说服力的基准。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17242', 'title': 'LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias', 'url': 'https://huggingface.co/papers/2410.17242', 'abstract': 'We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods -- from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps) -- addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality. Notably, our models surpass all previous methods even with reduced computational resources (1-2 GPUs). Please see our website for more details: https://haian-jin.github.io/projects/LVSM/ .', 'score': 3, 'issue_id': 255, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'f91cd290a84f0584', 'data': {'categories': ['#small_models', '#synthetic', '#cv', '#inference', '#optimization', '#transfer_learning', '#3d', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Трансформеры покоряют синтез новых ракурсов', 'desc': 'Исследователи представили Large View Synthesis Model (LVSM) - новый подход на основе трансформеров для масштабируемого и обобщаемого синтеза новых ракурсов по небольшому набору входных изображений. Предложены две архитектуры: энкодер-декодер LVSM, кодирующий входные изображения в латентные токены, и декодер-только LVSM, напрямую преобразующий входные изображения в новые ракурсы. Оба варианта LVSM превосходят современные методы по качеству синтеза новых ракурсов, при этом декодер-только LVSM демонстрирует лучшие результаты по качеству, масштабируемости и обобщению на новые данные. Модели достигают state-of-the-art результатов даже при использовании меньших вычислительных ресурсов.'}, 'en': {'title': 'Transforming Sparse Views into Stunning Images with LVSM!', 'desc': 'The Large View Synthesis Model (LVSM) introduces a new transformer-based method for creating images from limited input views. It features two architectures: an encoder-decoder model that learns a scene representation and a decoder-only model that generates images directly from inputs. Both models avoid traditional 3D biases, relying instead on a data-driven approach for novel view synthesis. The LVSM models demonstrate superior performance in image quality and efficiency, outperforming existing methods while using fewer computational resources.'}, 'zh': {'title': '大型视图合成模型：数据驱动的新视角', 'desc': '我们提出了一种新的大型视图合成模型（LVSM），这是一种基于变换器的可扩展和通用的新视图合成方法。该模型包括两种架构：编码-解码器LVSM和仅解码器LVSM，前者通过将输入图像编码为固定数量的潜在标记来生成新视图图像，后者则直接将输入图像映射到新视图输出。两种模型都避免了以往方法中使用的3D归纳偏见，采用完全数据驱动的方法进行新视图合成。我们的模型在多个数据集上的评估显示，LVSM在新视图合成质量上达到了最先进的水平，且在计算资源减少的情况下仍然表现优异。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.13816', 'title': 'Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance', 'url': 'https://huggingface.co/papers/2410.13816', 'abstract': 'Large, general-purpose robotic policies trained on diverse demonstration datasets have been shown to be remarkably effective both for controlling a variety of robots in a range of different scenes, and for acquiring broad repertoires of manipulation skills. However, the data that such policies are trained on is generally of mixed quality -- not only are human-collected demonstrations unlikely to perform the task perfectly, but the larger the dataset is, the harder it is to curate only the highest quality examples. It also remains unclear how optimal data from one embodiment is for training on another embodiment. In this paper, we present a general and broadly applicable approach that enhances the performance of such generalist robot policies at deployment time by re-ranking their actions according to a value function learned via offline RL. This approach, which we call Value-Guided Policy Steering (V-GPS), is compatible with a wide range of different generalist policies, without needing to fine-tune or even access the weights of the policy. We show that the same value function can improve the performance of five different state-of-the-art policies with different architectures, even though they were trained on distinct datasets, attaining consistent performance improvement on multiple robotic platforms across a total of 12 tasks. Code and videos can be found at: https://nakamotoo.github.io/V-GPS', 'score': 1, 'issue_id': 255, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'ee6d24e9422674a5', 'data': {'categories': ['#rl', '#benchmark', '#optimization', '#data', '#training', '#robotics', '#transfer_learning', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Улучшение робототехнических политик без переобучения: V-GPS в действии', 'desc': 'Статья представляет новый подход под названием Value-Guided Policy Steering (V-GPS) для улучшения производительности обобщенных робототехнических политик. V-GPS использует функцию ценности, обученную с помощью офлайн-обучения с подкреплением, для переранжирования действий политики во время развертывания. Этот метод совместим с различными типами политик и не требует доступа к их весам или дополнительной настройки. Авторы демонстрируют эффективность V-GPS на пяти современных политиках с разными архитектурами, показывая улучшение производительности на нескольких робототехнических платформах в 12 задачах.'}, 'en': {'title': 'Enhancing Robot Performance with Value-Guided Policy Steering', 'desc': 'This paper introduces a method called Value-Guided Policy Steering (V-GPS) to improve the performance of general-purpose robotic policies during deployment. The authors address the issue of mixed-quality training data by using a value function learned through offline reinforcement learning (RL) to re-rank the actions of the robots. V-GPS is designed to work with various existing policies without requiring any modifications to their underlying models. The results demonstrate that this approach consistently enhances the performance of multiple state-of-the-art robotic policies across different tasks and platforms.'}, 'zh': {'title': '提升机器人策略性能的新方法', 'desc': '本文提出了一种名为价值引导策略引导（V-GPS）的方法，旨在提高通用机器人策略的性能。该方法通过离线强化学习学习的价值函数，对机器人在执行任务时的动作进行重新排序。V-GPS可以与多种不同的通用策略兼容使用，而无需微调或访问策略的权重。实验表明，使用相同的价值函数可以在不同架构的五种最先进策略上实现一致的性能提升，适用于12个不同的机器人任务。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17243', 'title': 'Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss', 'url': 'https://huggingface.co/papers/2410.17243', 'abstract': 'Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumption, primarily due to the full instantiation of the similarity matrix. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into arbitrary small blocks, avoiding full materialization of the similarity matrix. Furthermore, we introduce a multi-level tiling strategy to leverage the hierarchical structure of distributed systems, employing ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method scales batch sizes to unprecedented levels. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB without sacrificing any accuracy. Compared to SOTA memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available.', 'score': 86, 'issue_id': 257, 'pub_date': '2024-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '29bd02d560e09f69', 'data': {'categories': ['#cv', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🧩', 'ru': {'title': 'Эффективное масштабирование контрастивного обучения с помощью тайлинга', 'desc': 'В статье представлен новый подход к вычислению контрастивной функции потерь, позволяющий значительно увеличить размер батча без роста потребления памяти GPU. Авторы предлагают стратегию вычислений на основе тайлинга, которая разбивает расчет контрастивной функции потерь на небольшие блоки. Также вводится многоуровневая стратегия тайлинга для оптимизации распределенных вычислений. Экспериментальные результаты показывают, что предложенный метод позволяет достичь беспрецедентных размеров батча при обучении моделей вроде CLIP-ViT-L/14.'}, 'en': {'title': 'Scaling Contrastive Learning: Breaking Memory Barriers with Tile-Based Strategies', 'desc': 'The paper introduces a novel tile-based computation strategy to efficiently calculate contrastive loss, which is crucial for representation learning. This method addresses the challenge of increased GPU memory consumption by partitioning the similarity matrix into smaller blocks, thus avoiding full instantiation. Additionally, a multi-level tiling strategy is employed to optimize communication and reduce I/O overhead in distributed systems. The approach allows for significantly larger batch sizes, achieving a two-order-of-magnitude reduction in memory usage without compromising speed or accuracy.'}, 'zh': {'title': '突破内存限制，实现超大批量对比学习', 'desc': '对比损失是一种强大的表示学习方法，通过提供更多的负样本来更好地区分相似和不相似的数据，从而提高性能。然而，批量大小的扩展受到GPU内存消耗的限制，因为相似性矩阵的完全实例化会导致内存消耗呈二次增长。为了解决这个问题，我们提出了一种基于块的计算策略，将对比损失的计算划分为任意小的块，避免了相似性矩阵的完全实例化。此外，我们引入了一种多级块策略，利用分布式系统的层次结构，在GPU级别采用环形通信优化同步，并在CUDA核心级别使用融合内核减少I/O开销。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16251', 'title': 'Can Knowledge Editing Really Correct Hallucinations?', 'url': 'https://huggingface.co/papers/2410.16251', 'abstract': 'Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, one common issue of existing evaluation datasets for knowledge editing is that they do not ensure LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate the progress in the field of knowledge editing.', 'score': 53, 'issue_id': 259, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'fa9696ca5a11c431', 'data': {'categories': ['#hallucinations', '#benchmark', '#training', '#dataset', '#architecture', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Исправление галлюцинаций в LLM: новый бенчмарк для оценки методов редактирования знаний', 'desc': 'Данная статья посвящена проблеме галлюцинаций в больших языковых моделях (LLM) и методам редактирования знаний для их исправления. Авторы предлагают новый бенчмарк HalluEditBench для оценки эффективности методов редактирования знаний в исправлении реальных галлюцинаций. Бенчмарк включает массивный набор данных с более чем 6000 галлюцинаций из 9 доменов и 26 тем. HalluEditBench оценивает методы редактирования знаний по пяти измерениям: эффективность, обобщаемость, переносимость, локальность и устойчивость.'}, 'en': {'title': 'Benchmarking the Battle Against Hallucinations in LLMs', 'desc': 'The paper addresses the issue of hallucinations in Large Language Models (LLMs), where these models generate non-factual information. It introduces HalluEditBench, a comprehensive benchmark designed to evaluate the effectiveness of knowledge editing methods in correcting these hallucinations. The benchmark includes a large dataset with over 6,000 hallucinations across various domains and assesses methods on five key dimensions. This work provides new insights into the strengths and weaknesses of current knowledge editing techniques, aiming to inspire future advancements in the field.'}, 'zh': {'title': 'HalluEditBench：纠正幻觉的新希望', 'desc': '大型语言模型（LLMs）在生成内容时常常出现幻觉，即生成不真实的信息。知识编辑是一种新兴的方法，可以在不重新训练模型的情况下纠正这些错误信息。现有的评估数据集无法确保LLMs在编辑前生成幻觉答案，因此难以评估知识编辑方法的有效性。我们提出了HalluEditBench，通过构建大规模幻觉数据集，全面评估知识编辑方法在纠正幻觉方面的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18533', 'title': 'LOGO -- Long cOntext aliGnment via efficient preference Optimization', 'url': 'https://huggingface.co/papers/2410.18533', 'abstract': "Long-context models(LCMs) have shown great potential in processing long input sequences(even more than 100M tokens) conveniently and effectively. With significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context. Yet, the generation performance of these LCMs is far from satisfactory and might result in misaligned responses, such as hallucinations. To enhance the generation capability of LCMs, existing works have investigated the effects of data size and quality for both pre-training and instruction tuning. Though achieving meaningful improvement, previous methods fall short in either effectiveness or efficiency. In this paper, we introduce LOGO(Long cOntext aliGnment via efficient preference Optimization), a training strategy that first introduces preference optimization for long-context alignment. To overcome the GPU memory-bound issue caused by the long sequence, LOGO employs a reference-free preference optimization strategy and adopts a position synthesis method to construct the training data. By training with only 0.3B data on a single 8timesA800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K model to achieve comparable performance with GPT-4 in real-world long-context tasks while preserving the model's original capabilities on other tasks, e.g., language modeling and MMLU. Moreover, LOGO can extend the model's context window size while enhancing its generation performance.", 'score': 42, 'issue_id': 258, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '3094f5b6fc1c0168', 'data': {'categories': ['#small_models', '#hallucinations', '#optimization', '#training', '#architecture', '#long_context', '#alignment'], 'emoji': '🚀', 'ru': {'title': 'LOGO: Эффективное улучшение генерации для моделей с длинным контекстом', 'desc': 'Эта статья представляет новый метод обучения под названием LOGO для улучшения генеративных способностей моделей с длинным контекстом (LCM). LOGO использует оптимизацию предпочтений без эталона и метод синтеза позиций для создания обучающих данных. Авторы обучили модель Llama-3-8B-Instruct-80K всего на 0.3B данных за 16 часов, достигнув производительности, сравнимой с GPT-4 на задачах с длинным контекстом. LOGO также позволяет расширить размер контекстного окна модели, сохраняя при этом ее исходные возможности в других задачах.'}, 'en': {'title': 'LOGO: Aligning Long-Context Models for Better Generation', 'desc': "This paper introduces a new training strategy called LOGO, which improves the generation performance of long-context models (LCMs) by using preference optimization for better alignment. LOGO addresses the challenge of GPU memory limitations by employing a reference-free preference optimization strategy and a position synthesis method for training data construction. The approach allows significant performance improvements in long-context tasks, achieving results comparable to GPT-4, while maintaining the model's original capabilities. LOGO also extends the model's context window size, enhancing its ability to handle long sequences effectively."}, 'zh': {'title': 'LOGO：优化长上下文生成性能的新策略', 'desc': '这篇论文介绍了一种新的训练策略LOGO，用于提高长上下文模型的生成能力。LOGO通过引入偏好优化来实现长上下文对齐，并采用无参考的偏好优化策略来解决GPU内存限制问题。通过在单台8倍A800 GPU机器上训练16小时，LOGO使得Llama-3-8B-Instruct-80K模型在长上下文任务中表现与GPT-4相当。LOGO不仅提升了生成性能，还扩展了模型的上下文窗口大小，同时保留了模型在其他任务上的原有能力。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18693', 'title': 'Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch', 'url': 'https://huggingface.co/papers/2410.18693', 'abstract': 'The availability of high-quality data is one of the most important factors in improving the reasoning capability of LLMs. Existing works have demonstrated the effectiveness of creating more instruction data from seed questions or knowledge bases. Recent research indicates that continually scaling up data synthesis from strong models (e.g., GPT-4) can further elicit reasoning performance. Though promising, the open-sourced community still lacks high-quality data at scale and scalable data synthesis methods with affordable costs. To address this, we introduce ScaleQuest, a scalable and novel data synthesis method that utilizes "small-size" (e.g., 7B) open-source models to generate questions from scratch without the need for seed data with complex augmentation constraints. With the efficient ScaleQuest, we automatically constructed a mathematical reasoning dataset consisting of 1 million problem-solution pairs, which are more effective than existing open-sourced datasets. It can universally increase the performance of mainstream open-source models (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2% to 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base model with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and well-aligned model on closed-source data, and proprietary models such as GPT-4-Turbo and Claude-3.5 Sonnet.', 'score': 39, 'issue_id': 258, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'd898ba7b025b60e6', 'data': {'categories': ['#small_models', '#reasoning', '#synthetic', '#math', '#data', '#training', '#dataset', '#open_source'], 'emoji': '🧮', 'ru': {'title': 'ScaleQuest: Масштабируемый синтез данных для повышения математических способностей LLM', 'desc': 'Статья представляет ScaleQuest - новый метод синтеза данных для улучшения способностей LLM к рассуждениям. Используя небольшие модели с открытым исходным кодом, ScaleQuest генерирует вопросы с нуля без необходимости в исходных данных. С помощью этого метода был создан набор данных из 1 миллиона пар задача-решение для математических рассуждений. Применение этого набора данных значительно повысило производительность открытых моделей на тесте MATH, позволив некоторым из них превзойти даже проприетарные модели.'}, 'en': {'title': 'ScaleQuest: Elevating AI Reasoning with Scalable Data Synthesis', 'desc': 'The paper introduces ScaleQuest, a method for generating high-quality data using smaller open-source models without relying on seed data. This approach allows for the creation of a large mathematical reasoning dataset, significantly improving the performance of various open-source models. The dataset enhances model capabilities by up to 46.4% on mathematical tasks, outperforming some proprietary models. ScaleQuest offers a cost-effective solution for scalable data synthesis, addressing the need for high-quality data in the open-source community.'}, 'zh': {'title': 'ScaleQuest：小模型，大数据，强推理', 'desc': '高质量的数据是提升大型语言模型推理能力的关键因素之一。现有研究表明，从种子问题或知识库中生成更多指令数据是有效的。最近的研究显示，持续扩大数据合成规模可以进一步提高推理性能。为此，我们引入了ScaleQuest，一种利用小型开源模型生成问题的新方法，无需复杂的种子数据增强约束。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18978', 'title': 'Framer: Interactive Frame Interpolation', 'url': 'https://huggingface.co/papers/2410.18978', 'abstract': 'We propose Framer for interactive frame interpolation, which targets producing smoothly transitioning frames between two images as per user creativity. Concretely, besides taking the start and end frames as inputs, our approach supports customizing the transition process by tailoring the trajectory of some selected keypoints. Such a design enjoys two clear benefits. First, incorporating human interaction mitigates the issue arising from numerous possibilities of transforming one image to another, and in turn enables finer control of local motions. Second, as the most basic form of interaction, keypoints help establish the correspondence across frames, enhancing the model to handle challenging cases (e.g., objects on the start and end frames are of different shapes and styles). It is noteworthy that our system also offers an "autopilot" mode, where we introduce a module to estimate the keypoints and refine the trajectory automatically, to simplify the usage in practice. Extensive experimental results demonstrate the appealing performance of Framer on various applications, such as image morphing, time-lapse video generation, cartoon interpolation, etc. The code, the model, and the interface will be released to facilitate further research.', 'score': 35, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'ccd15b8785ec7920', 'data': {'categories': ['#cv', '#video', '#multimodal', '#training', '#open_source', '#games', '#architecture'], 'emoji': '🎞️', 'ru': {'title': 'Интерактивная интерполяция кадров с пользовательским контролем', 'desc': "Предложен метод Framer для интерактивной интерполяции кадров, позволяющий создавать плавные переходы между двумя изображениями с учетом пользовательских предпочтений. Система поддерживает настройку процесса перехода путем задания траектории выбранных ключевых точек, что обеспечивает более точный контроль локальных движений. Framer также включает 'автопилотный' режим с автоматическим определением ключевых точек и траекторий. Метод показал высокую эффективность в различных приложениях, таких как морфинг изображений, создание таймлапс-видео и интерполяция мультипликации."}, 'en': {'title': 'Framer: Crafting Seamless Image Transitions with User Control', 'desc': 'Framer is a tool for creating smooth transitions between two images by allowing users to customize the movement of keypoints, which are specific points on the images. This interactive approach helps users control how different parts of the images move, making it easier to handle complex transformations. The system also includes an automatic mode that estimates keypoints and their paths, simplifying the process for users. Framer has shown impressive results in applications like image morphing and cartoon interpolation, and its resources will be made available for further research.'}, 'zh': {'title': 'Framer：让图像过渡更顺滑的交互式帧插值', 'desc': '这篇论文介绍了一种名为Framer的交互式帧插值方法，旨在根据用户的创意在两张图像之间生成平滑过渡的帧。该方法允许用户通过调整关键点的轨迹来定制过渡过程，从而实现对局部运动的精细控制。通过这种人机交互，Framer能够更好地处理起始和结束帧中形状和风格不同的物体。此外，Framer还提供了自动模式，可以自动估计关键点并优化轨迹，简化实际使用。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18975', 'title': 'Unbounded: A Generative Infinite Game of Character Life Simulation', 'url': 'https://huggingface.co/papers/2410.18975', 'abstract': "We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches.", 'score': 34, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '2d7a29df8d1696c4', 'data': {'categories': ['#rl', '#cv', '#video', '#multimodal', '#games', '#architecture'], 'emoji': '🎮', 'ru': {'title': 'Бесконечная игра: новый уровень генеративных видеоигр', 'desc': 'Статья представляет концепцию генеративной бесконечной игры, которая использует генеративные модели для создания видеоигры, выходящей за рамки традиционных конечных систем. Авторы разработали игру Unbounded - симулятор жизни персонажа, полностью основанный на генеративных моделях. В игре используется специализированная языковая модель для генерации игровых механик и нарратива, а также новый адаптер для визуальных моделей, обеспечивающий согласованную генерацию изображений персонажа. Результаты показывают значительные улучшения в симуляции жизни персонажа, следовании инструкциям пользователя и согласованности повествования по сравнению с традиционными подходами.'}, 'en': {'title': 'Unbounded: Redefining Gaming with Generative AI', 'desc': 'The paper introduces a new type of video game called a generative infinite game, which uses advanced generative models to create a limitless gaming experience. The game, named Unbounded, allows players to interact with virtual characters in a dynamic world, with game mechanics and narratives generated in real-time by a specialized large language model (LLM). Additionally, the game employs a novel image prompt Adapter (IP-Adapter) to maintain visual consistency across different environments. The authors demonstrate that their approach significantly enhances character simulation, narrative coherence, and visual consistency compared to traditional methods.'}, 'zh': {'title': '生成无限游戏：打破传统游戏界限的创新', 'desc': '这篇论文介绍了一种名为“生成无限游戏”的新概念，通过生成模型打破传统有限、硬编码系统的界限。受詹姆斯·P·卡尔斯关于有限游戏和无限游戏的区分启发，作者利用生成式人工智能的最新进展，创造了一个名为“Unbounded”的角色生活模拟游戏。该游戏允许玩家在虚拟世界中与自主虚拟角色互动，使用大型语言模型生成开放式的游戏机制。为了实现这一目标，作者在大型语言模型和视觉生成领域提出了技术创新，并通过定性和定量分析证明了其在角色生活模拟、用户指令遵循、叙事连贯性和视觉一致性方面的显著改进。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18798', 'title': 'Distill Visual Chart Reasoning Ability from LLMs to MLLMs', 'url': 'https://huggingface.co/papers/2410.18798', 'abstract': 'Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, a promising approach to enhance MLLMs is to construct relevant training data focusing on the two aspects. However, collecting and annotating complex charts and questions is costly and time-consuming, and ensuring the quality of annotated answers remains a challenge. In this paper, we propose Code-as-Intermediary Translation (CIT), a cost-effective, efficient and easily scalable data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs. The code serves as an intermediary that translates visual chart representations into textual representations, enabling LLMs to understand cross-modal information. Specifically, we employ text-based synthesizing techniques to construct chart-plotting code and produce ReachQA, a dataset containing 3k reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and reasoning abilities. Experiments show that when fine-tuned with our data, models not only perform well on chart-related benchmarks, but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks like MathVista. The code and dataset are publicly available at https://github.com/hewei2001/ReachQA.', 'score': 18, 'issue_id': 259, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '7d35c4638b6931ba', 'data': {'categories': ['#science', '#reasoning', '#synthetic', '#benchmark', '#cv', '#math', '#multimodal', '#data', '#training', '#dataset', '#open_source'], 'emoji': '📊', 'ru': {'title': 'Код как переводчик: новый подход к визуальным рассуждениям в ИИ', 'desc': 'Статья представляет новый метод Code-as-Intermediary Translation (CIT) для улучшения визуальных рассуждений в мультимодальных больших языковых моделях (MLLM). Метод использует код как посредник для перевода визуальных представлений графиков в текстовые, что позволяет LLM понимать межмодальную информацию. Авторы создали датасет ReachQA с 3000 графиков и 20000 пар вопрос-ответ для улучшения способностей распознавания и рассуждения. Эксперименты показали, что модели, обученные на этих данных, улучшают свои способности мультимодальных рассуждений не только на задачах с графиками, но и на общих математических тестах.'}, 'en': {'title': 'Bridging Visual and Textual Worlds with Code', 'desc': 'The paper introduces a method called Code-as-Intermediary Translation (CIT) to improve visual reasoning in multimodal large language models (MLLMs). CIT uses code to convert visual chart data into text, making it easier for language models to process and understand. This approach helps create a dataset called ReachQA, which includes 3,000 charts and 20,000 Q&A pairs, enhancing both recognition and reasoning skills in MLLMs. Experiments show that models trained with this data perform better on both chart-related and general mathematical reasoning tasks.'}, 'zh': {'title': '代码中介翻译：提升多模态推理能力的新方法', 'desc': '这篇论文探讨了如何提升多模态大语言模型在复杂图表问答任务中的视觉推理能力。研究表明，这种能力主要包括从视觉输入中识别关键信息和进行推理。为此，作者提出了一种名为“代码作为中介翻译”的方法，通过生成图表绘制代码，将视觉信息转化为文本信息，从而提高模型的跨模态理解能力。实验结果显示，使用该方法训练的数据集可以显著提升模型在图表相关基准测试和一般数学推理任务中的表现。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15999', 'title': 'Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering', 'url': 'https://huggingface.co/papers/2410.15999', 'abstract': 'Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context -- this phenomenon, known as context-memory knowledge conflicts, can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. Analysing the internal activations of LLMs, we find that they can internally register the signals of knowledge conflict at mid-layers. Such signals allow us to detect whether a knowledge conflict occurs and use inference-time intervention strategies to resolve it. In this work, we propose SpARE, a training-free representation engineering method that uses pre-trained sparse auto-encoders (SAEs) to control the knowledge selection behaviour of LLMs. SpARE identifies the functional features that control the knowledge selection behaviours and applies them to edit the internal activations of LLMs at inference time. Our experimental results show that SpARE can effectively control the usage of either knowledge source to resolve knowledge conflict in open-domain question-answering tasks, surpassing existing representation engineering methods (+10%) as well as contrastive decoding methods (+15%).', 'score': 17, 'issue_id': 265, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '6667c4b8e3309bfb', 'data': {'categories': ['#reasoning', '#inference', '#optimization', '#interpretability', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'SpARE: умное разрешение конфликтов знаний в языковых моделях', 'desc': 'Данная статья посвящена проблеме конфликтов знаний в больших языковых моделях (LLM). Авторы предлагают метод SpARE, использующий предобученные разреженные автоэнкодеры для управления выбором знаний в LLM. SpARE идентифицирует функциональные особенности, контролирующие выбор знаний, и применяет их для редактирования внутренних активаций LLM во время вывода. Эксперименты показывают, что SpARE эффективно разрешает конфликты знаний в задачах открытого вопросно-ответного анализа, превосходя существующие методы.'}, 'en': {'title': 'SpARE: Resolving Knowledge Conflicts in LLMs with Precision', 'desc': "The paper discusses how large language models (LLMs) can sometimes rely on outdated or incorrect information due to conflicts between their stored knowledge and new context. The authors introduce a method called SpARE, which uses pre-trained sparse auto-encoders to manage these conflicts without additional training. By analyzing the internal activations of LLMs, SpARE can detect and resolve knowledge conflicts by editing the model's internal processes during inference. The method shows significant improvements in open-domain question-answering tasks, outperforming other techniques by a notable margin."}, 'zh': {'title': 'SpARE：无训练知识冲突解决方案', 'desc': '大型语言模型（LLMs）可以在其参数中存储大量的事实知识，但这些知识可能与上下文提供的信息相冲突，导致模型行为不理想。通过分析LLMs的内部激活，我们发现它们可以在中间层内部记录知识冲突的信号。我们提出了一种名为SpARE的方法，利用预训练的稀疏自编码器（SAEs）来控制LLMs的知识选择行为。实验结果表明，SpARE在开放域问答任务中有效解决知识冲突，性能优于现有方法。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18745', 'title': 'Why Does the Effective Context Length of LLMs Fall Short?', 'url': 'https://huggingface.co/papers/2410.18745', 'abstract': 'Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their training lengths. In this work, we attribute this limitation to the left-skewed frequency distribution of relative positions formed in LLMs pretraining and post-training stages, which impedes their ability to effectively gather distant information. To address this challenge, we introduce ShifTed Rotray position embeddING (STRING). STRING shifts well-trained positions to overwrite the original ineffective positions during inference, enhancing performance within their existing training lengths. Experimental results show that without additional training, STRING dramatically improves the performance of the latest large-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on popular long-context benchmarks RULER and InfiniteBench, establishing new state-of-the-art results for open-source LLMs. Compared to commercial models, Llama 3.1 70B with \\method even achieves better performance than GPT-4-128K and clearly surpasses Claude 2 and Kimi-chat.', 'score': 16, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '8694d1d100403e2f', 'data': {'categories': ['#long_context', '#benchmark', '#inference', '#training', '#open_source', '#architecture'], 'emoji': '🧵', 'ru': {'title': 'STRING: Расширение горизонтов контекста для языковых моделей', 'desc': 'Статья представляет новый метод под названием STRING (ShifTed Rotray position embeddING) для улучшения эффективной длины контекста больших языковых моделей (LLM). Авторы обнаружили, что существующие LLM часто не могут эффективно использовать весь заявленный контекст из-за особенностей распределения относительных позиций при обучении. STRING решает эту проблему, смещая хорошо обученные позиции для перезаписи неэффективных позиций во время вывода. Эксперименты показывают значительное улучшение производительности современных моделей, таких как Llama 3.1 70B и Qwen2 72B, на бенчмарках длинного контекста без дополнительного обучения.'}, 'en': {'title': 'STRING: Unlocking Full Context Potential in LLMs', 'desc': 'The paper discusses how large language models (LLMs) often struggle to use their full context window effectively due to a skewed frequency distribution of relative positions during training. To solve this, the authors introduce a method called ShifTed Rotray position embeddING (STRING), which adjusts these positions to improve information gathering without additional training. STRING significantly boosts the performance of models like Llama3.1 70B and Qwen2 72B on long-context benchmarks, setting new records for open-source LLMs. This method even allows these models to outperform some commercial models, such as GPT-4-128K and Claude 2.'}, 'zh': {'title': 'STRING：提升大语言模型上下文理解的新方法', 'desc': '这篇论文讨论了大语言模型在分布式训练和高效注意力机制方面的进展，但指出开源模型的有效上下文长度往往不如预期。作者发现这种限制是由于模型训练过程中相对位置的频率分布不均衡，影响了模型获取远距离信息的能力。为了解决这个问题，作者提出了一种名为STRING的新方法，通过在推理时调整位置嵌入来提高模型性能。实验结果表明，STRING在不需要额外训练的情况下显著提升了最新大规模模型的表现，甚至超过了一些商业模型。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18538', 'title': 'SMITE: Segment Me In TimE', 'url': 'https://huggingface.co/papers/2410.18538', 'abstract': 'Segmenting an object in a video presents significant challenges. Each pixel must be accurately labelled, and these labels must remain consistent across frames. The difficulty increases when the segmentation is with arbitrary granularity, meaning the number of segments can vary arbitrarily, and masks are defined based on only one or a few sample images. In this paper, we address this issue by employing a pre-trained text to image diffusion model supplemented with an additional tracking mechanism. We demonstrate that our approach can effectively manage various segmentation scenarios and outperforms state-of-the-art alternatives.', 'score': 15, 'issue_id': 265, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '39917935bf1d9f30', 'data': {'categories': ['#diffusion', '#cv', '#video', '#multimodal', '#games'], 'emoji': '🎬', 'ru': {'title': 'Гибкая сегментация видео с помощью диффузионных моделей', 'desc': 'Статья посвящена проблеме сегментации объектов в видео с произвольной гранулярностью. Авторы предлагают использовать предобученную диффузионную модель для перехода от текста к изображению, дополненную механизмом отслеживания. Этот подход позволяет эффективно справляться с различными сценариями сегментации. Результаты показывают, что предложенный метод превосходит современные альтернативы.'}, 'en': {'title': 'Mastering Video Segmentation with Diffusion and Tracking', 'desc': 'This paper tackles the complex task of segmenting objects in videos, where each pixel needs consistent labeling across frames. The challenge is heightened by arbitrary granularity, where the number of segments can vary, and segmentation masks are based on limited sample images. The authors propose a solution using a pre-trained text-to-image diffusion model enhanced with a tracking mechanism. Their approach shows superior performance in handling diverse segmentation scenarios compared to existing methods.'}, 'zh': {'title': '创新视频分割：文本到图像扩散模型的应用', 'desc': '这篇论文讨论了视频中物体分割的挑战，尤其是每个像素的准确标记和跨帧的一致性。作者提出了一种方法，利用预训练的文本到图像扩散模型，并结合额外的跟踪机制来解决这些问题。该方法能够处理不同的分割场景，并且在性能上优于现有的最先进方法。通过这种创新的方式，视频分割的精度和灵活性得到了显著提升。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18572', 'title': 'Taipan: Efficient and Expressive State Space Language Models with Selective Attention', 'url': 'https://huggingface.co/papers/2410.18572', 'abstract': "Efficient long-context language modeling remains a significant challenge in Natural Language Processing (NLP). While Transformers dominate language tasks, they struggle with long sequences due to quadratic computational complexity in training and linearly scaling memory costs during inference. Recent State Space Models (SSMs) such as Mamba offer alternatives with constant memory usage, but they underperform in tasks requiring extensive in-context retrieval. We introduce Taipan, a novel hybrid architecture that combines Mamba-2 with Selective Attention Layers (SALs). These SALs identify tokens requiring long-range interactions, remove less important features, and then augment their representations using the attention module. This approach balances Mamba's efficiency with Transformer-like performance in memory-intensive tasks. By constraining the attention budget, Taipan extends accurate predictions to context lengths of up to 1 million tokens while preserving computational efficiency. Our experiments demonstrate Taipan's superior performance across various scales and tasks, offering a promising solution for efficient long-context language modeling.", 'score': 15, 'issue_id': 262, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '406070252c5b43ea', 'data': {'categories': ['#long_context', '#inference', '#optimization', '#training', '#architecture'], 'emoji': '🐍', 'ru': {'title': 'Taipan: Эффективное языковое моделирование с контекстом до миллиона токенов', 'desc': 'Исследователи представили Taipan - гибридную архитектуру, сочетающую Mamba-2 с селективными слоями внимания для эффективной обработки длинных последовательностей в языковом моделировании. Taipan идентифицирует важные токены, требующие взаимодействия на большом расстоянии, и усиливает их представления с помощью модуля внимания. Этот подход сочетает эффективность Mamba с производительностью трансформеров в задачах, требующих обширного извлечения информации из контекста. Эксперименты показывают превосходную производительность Taipan на различных масштабах и задачах, предлагая перспективное решение для эффективного языкового моделирования с длинным контекстом.'}, 'en': {'title': 'Taipan: Bridging Efficiency and Performance in Long-Context Language Modeling', 'desc': 'The paper introduces Taipan, a new model for handling long-context language tasks in NLP. Taipan combines the efficient memory usage of State Space Models like Mamba-2 with the performance benefits of Selective Attention Layers. These layers help the model focus on important tokens, allowing it to manage long sequences effectively. This hybrid approach enables Taipan to make accurate predictions with up to 1 million tokens while maintaining computational efficiency.'}, 'zh': {'title': 'Taipan：高效处理长文本的混合架构', 'desc': '在自然语言处理领域，处理长文本的语言建模一直是个难题。虽然Transformer在很多任务中表现出色，但在处理长序列时，它的计算复杂度和内存需求都很高。Taipan是一种新型混合架构，结合了Mamba-2和选择性注意层，能够在保持计算效率的同时，处理长达100万标记的上下文。实验表明，Taipan在多种任务中表现优异，是长文本语言建模的有效解决方案。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18977', 'title': 'MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms', 'url': 'https://huggingface.co/papers/2410.18977', 'abstract': 'This research delves into the problem of interactive editing of human motion generation. Previous motion diffusion models lack explicit modeling of the word-level text-motion correspondence and good explainability, hence restricting their fine-grained editing ability. To address this issue, we propose an attention-based motion diffusion model, namely MotionCLR, with CLeaR modeling of attention mechanisms. Technically, MotionCLR models the in-modality and cross-modality interactions with self-attention and cross-attention, respectively. More specifically, the self-attention mechanism aims to measure the sequential similarity between frames and impacts the order of motion features. By contrast, the cross-attention mechanism works to find the fine-grained word-sequence correspondence and activate the corresponding timesteps in the motion sequence. Based on these key properties, we develop a versatile set of simple yet effective motion editing methods via manipulating attention maps, such as motion (de-)emphasizing, in-place motion replacement, and example-based motion generation, etc. For further verification of the explainability of the attention mechanism, we additionally explore the potential of action-counting and grounded motion generation ability via attention maps. Our experimental results show that our method enjoys good generation and editing ability with good explainability.', 'score': 13, 'issue_id': 259, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '094d940b9a0653c0', 'data': {'categories': ['#diffusion', '#cv', '#interpretability', '#games', '#architecture'], 'emoji': '🕺', 'ru': {'title': 'Точное редактирование движений с помощью внимания', 'desc': 'Статья представляет MotionCLR - модель диффузии движения на основе механизмов внимания для интерактивного редактирования генерации движений человека. Модель использует самовнимание для измерения последовательного сходства между кадрами и кросс-внимание для нахождения детальных соответствий между словами и последовательностью движений. На основе этих механизмов разработаны разнообразные методы редактирования движений путем манипуляции картами внимания. Эксперименты показывают, что метод обладает хорошими возможностями генерации и редактирования с высокой объяснимостью.'}, 'en': {'title': 'Unlocking Fine-Grained Motion Editing with Attention', 'desc': 'This paper introduces MotionCLR, an attention-based motion diffusion model designed to improve the fine-grained editing of human motion generation. By utilizing self-attention and cross-attention mechanisms, the model effectively captures both the sequential similarity between motion frames and the correspondence between text and motion sequences. This approach allows for versatile motion editing techniques, such as motion emphasizing and example-based generation, by manipulating attention maps. The experimental results demonstrate that MotionCLR provides enhanced generation and editing capabilities with improved explainability.'}, 'zh': {'title': 'MotionCLR：通过注意力机制实现动作生成的精细编辑', 'desc': '这项研究探讨了人类动作生成的交互式编辑问题。以往的动作扩散模型缺乏对文字与动作对应关系的明确建模，限制了其细粒度编辑能力。为解决此问题，我们提出了一种基于注意力机制的动作扩散模型MotionCLR，通过自注意力和交叉注意力分别建模模态内和跨模态的交互。实验结果表明，我们的方法在生成和编辑能力上表现出色，并具有良好的可解释性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18451', 'title': 'Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs', 'url': 'https://huggingface.co/papers/2410.18451', 'abstract': 'In this report, we introduce a collection of methods to enhance reward modeling for LLMs, focusing specifically on data-centric techniques. We propose effective data selection and filtering strategies for curating high-quality open-source preference datasets, culminating in the Skywork-Reward data collection, which contains only 80K preference pairs -- significantly smaller than existing datasets. Using this curated dataset, we developed the Skywork-Reward model series -- Skywork-Reward-Gemma-27B and Skywork-Reward-Llama-3.1-8B -- with the former currently holding the top position on the RewardBench leaderboard. Notably, our techniques and datasets have directly enhanced the performance of many top-ranked models on RewardBench, highlighting the practical impact of our contributions in real-world preference learning applications.', 'score': 13, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '2ea2a42b9bcfc599', 'data': {'categories': ['#small_models', '#rlhf', '#benchmark', '#data', '#dataset', '#open_source', '#alignment'], 'emoji': '🚀', 'ru': {'title': 'Малые данные, большие результаты: революция в моделировании вознаграждений для LLM', 'desc': 'В этой работе представлен набор методов для улучшения моделирования вознаграждений для больших языковых моделей (LLM), с акцентом на техники, ориентированные на данные. Авторы предлагают эффективные стратегии отбора и фильтрации данных для курирования высококачественных открытых наборов данных предпочтений. Результатом стала коллекция данных Skywork-Reward, содержащая всего 80 тысяч пар предпочтений, что значительно меньше существующих наборов. Используя этот куриро  ванный набор данных, исследователи разработали серию моделей Skywork-Reward, которые показывают высокие результаты в бенчмарке RewardBench.'}, 'en': {'title': 'Smarter Data, Better Rewards: Revolutionizing LLMs with Skywork-Reward', 'desc': 'This paper presents new methods to improve reward modeling for large language models (LLMs) by focusing on data-centric approaches. The authors introduce strategies for selecting and filtering data to create high-quality preference datasets, resulting in the Skywork-Reward data collection with only 80,000 preference pairs. Using this dataset, they developed the Skywork-Reward model series, which includes models like Skywork-Reward-Gemma-27B, achieving top rankings on the RewardBench leaderboard. Their work demonstrates significant improvements in model performance, showcasing the practical benefits of their data-centric techniques in preference learning.'}, 'zh': {'title': '数据驱动的奖励建模新突破', 'desc': '这篇论文介绍了一系列方法来增强大型语言模型的奖励建模，特别是通过数据为中心的技术。作者提出了有效的数据选择和过滤策略，以便策划高质量的开源偏好数据集，最终形成了Skywork-Reward数据集，仅包含8万对偏好对，比现有数据集小得多。利用这个精心策划的数据集，开发了Skywork-Reward模型系列，其中Skywork-Reward-Gemma-27B在RewardBench排行榜上名列前茅。值得注意的是，这些技术和数据集直接提升了许多在RewardBench上排名靠前的模型的性能，突显了我们在实际偏好学习应用中的贡献。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18362', 'title': 'WAFFLE: Multi-Modal Model for Automated Front-End Development', 'url': 'https://huggingface.co/papers/2410.18362', 'abstract': "Web development involves turning UI designs into functional webpages, which can be difficult for both beginners and experienced developers due to the complexity of HTML's hierarchical structures and styles. While Large Language Models (LLMs) have shown promise in generating source code, two major challenges persist in UI-to-HTML code generation: (1) effectively representing HTML's hierarchical structure for LLMs, and (2) bridging the gap between the visual nature of UI designs and the text-based format of HTML code. To tackle these challenges, we introduce Waffle, a new fine-tuning strategy that uses a structure-aware attention mechanism to improve LLMs' understanding of HTML's structure and a contrastive fine-tuning approach to align LLMs' understanding of UI images and HTML code. Models fine-tuned with Waffle show up to 9.00 pp (percentage point) higher HTML match, 0.0982 higher CW-SSIM, 32.99 higher CLIP, and 27.12 pp higher LLEM on our new benchmark WebSight-Test and an existing benchmark Design2Code, outperforming current fine-tuning methods.", 'score': 11, 'issue_id': 259, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '671e80fc5e01a1f7', 'data': {'categories': ['#benchmark', '#cv', '#optimization', '#training', '#games', '#architecture', '#alignment'], 'emoji': '🧇', 'ru': {'title': 'Waffle: мост между UI-дизайном и HTML-кодом', 'desc': 'Статья представляет новую стратегию тонкой настройки моделей для генерации HTML-кода из UI-дизайнов. Метод Waffle использует механизм внимания, учитывающий структуру HTML, и контрастивное обучение для улучшения понимания моделями связи между изображениями UI и HTML-кодом. Эксперименты показывают значительное улучшение различных метрик по сравнению с существующими методами. Waffle решает проблемы эффективного представления иерархической структуры HTML для языковых моделей и преодоления разрыва между визуальной природой UI и текстовым форматом HTML.'}, 'en': {'title': 'Waffle: Bridging UI Design and HTML with Smarter LLMs', 'desc': "The paper introduces Waffle, a novel fine-tuning strategy for Large Language Models (LLMs) to improve UI-to-HTML code generation. Waffle addresses the challenges of representing HTML's hierarchical structure and aligning UI designs with HTML code by using a structure-aware attention mechanism and a contrastive fine-tuning approach. This method significantly enhances the model's performance, achieving higher scores in HTML match, CW-SSIM, CLIP, and LLEM on both new and existing benchmarks. The results demonstrate that Waffle outperforms current fine-tuning methods, making it a promising solution for web development tasks."}, 'zh': {'title': 'Waffle：提升UI到HTML代码生成的新策略', 'desc': '这篇论文介绍了一种名为Waffle的新方法，用于改进大型语言模型（LLMs）在UI到HTML代码生成中的表现。Waffle通过结构感知注意力机制来增强LLMs对HTML层次结构的理解，并使用对比微调方法来对齐LLMs对UI图像和HTML代码的理解。实验结果表明，使用Waffle微调的模型在HTML匹配度、CW-SSIM、CLIP和LLEM等指标上均优于现有方法。该方法有效解决了UI设计的视觉特性与HTML文本格式之间的差距。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18775', 'title': 'Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances', 'url': 'https://huggingface.co/papers/2410.18775', 'abstract': 'Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, we demonstrate that most methods fail to detect watermarks after such edits. To address this limitation, we propose VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. Our approach involves two key innovations: (1) we analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows us to use them as surrogate attacks during training to bolster watermark robustness; (2) we leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that our method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness. Code is available at https://github.com/Shilin-LU/VINE.', 'score': 9, 'issue_id': 258, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '1464291ab9f836a5', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#training', '#open_source', '#security', '#architecture'], 'emoji': '🔐', 'ru': {'title': 'VINE: Новый рубеж в защите авторских прав на изображения', 'desc': 'Статья представляет новый бенчмарк W-Bench для оценки устойчивости методов водяных знаков к современным техникам редактирования изображений. Авторы предлагают метод VINE, который значительно повышает робастность водяных знаков, сохраняя высокое качество изображений. VINE использует анализ частотных характеристик редактирования изображений и предобученную диффузионную модель SDXL-Turbo для улучшения встраивания водяных знаков. Экспериментальные результаты показывают превосходство VINE над существующими методами по качеству изображений и устойчивости водяных знаков.'}, 'en': {'title': 'VINE: Revolutionizing Watermark Robustness in the Age of Advanced Image Editing', 'desc': 'The paper addresses the vulnerability of current image watermarking methods to advanced editing techniques enabled by large-scale text-to-image models. It introduces W-Bench, a benchmark to evaluate the robustness of watermarking methods against various editing techniques, revealing that most methods fail to detect watermarks post-editing. To improve this, the authors propose VINE, a new watermarking method that enhances robustness by analyzing frequency characteristics of edits and using a pretrained diffusion model for better watermark embedding. Experimental results show that VINE outperforms existing methods in maintaining both image quality and watermark robustness.'}, 'zh': {'title': '提升水印鲁棒性的新方法：VINE', 'desc': '当前的图像水印方法容易受到由大规模文本到图像模型启用的高级图像编辑技术的攻击。这些模型在编辑过程中可能会扭曲嵌入的水印，给版权保护带来重大挑战。本文介绍了W-Bench，这是第一个全面的基准，用于评估水印方法在各种图像编辑技术下的鲁棒性。我们提出了一种名为VINE的水印方法，通过分析图像编辑的频率特性和利用大规模预训练扩散模型，显著提高了水印的鲁棒性和图像质量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18958', 'title': 'Stable Consistency Tuning: Understanding and Improving Consistency Models', 'url': 'https://huggingface.co/papers/2410.18958', 'abstract': 'Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising. In contrast, consistency models, a new generative family, achieve competitive performance with significantly faster sampling. These models are trained either through consistency distillation, which leverages pretrained diffusion models, or consistency training/tuning directly from raw data. In this work, we propose a novel framework for understanding consistency models by modeling the denoising process of the diffusion model as a Markov Decision Process (MDP) and framing consistency model training as the value estimation through Temporal Difference~(TD) Learning. More importantly, this framework allows us to analyze the limitations of current consistency training/tuning strategies. Built upon Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT), which incorporates variance-reduced learning using the score identity. SCT leads to significant performance improvements on benchmarks such as CIFAR-10 and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID 1.55, a new SoTA for consistency models.', 'score': 9, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '1e70afdddccea2fd', 'data': {'categories': ['#diffusion', '#rl', '#benchmark', '#optimization', '#training', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Революция в генеративных моделях: SCT устанавливает новые стандарты', 'desc': 'Статья представляет новый подход к пониманию моделей согласованности, рассматривая процесс шумоподавления как марковский процесс принятия решений. Авторы предлагают метод Stable Consistency Tuning (SCT), который использует обучение с временной разницей и снижение дисперсии для улучшения производительности. SCT достигает значительных улучшений на таких бенчмарках, как CIFAR-10 и ImageNet-64, устанавливая новый рекорд для моделей согласованности. Этот метод позволяет преодолеть ограничения существующих стратегий обучения моделей согласованности.'}, 'en': {'title': 'Speed Meets Quality: Revolutionizing Generative Models with Consistency', 'desc': 'This paper introduces a new framework for understanding consistency models by viewing the denoising process of diffusion models as a Markov Decision Process (MDP). It uses Temporal Difference (TD) Learning to train consistency models, which are faster than traditional diffusion models. The authors propose Stable Consistency Tuning (SCT), an improved method that reduces variance in learning, leading to better performance. SCT achieves state-of-the-art results on benchmarks like CIFAR-10 and ImageNet-64, demonstrating its effectiveness.'}, 'zh': {'title': '一致性模型：快速生成的新纪元', 'desc': '扩散模型生成质量高，但速度慢。相反，一致性模型速度快，性能好。本文提出用马尔可夫决策过程理解一致性模型。我们的方法在CIFAR-10和ImageNet-64上表现优异。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18976', 'title': 'CAMEL-Bench: A Comprehensive Arabic LMM Benchmark', 'url': 'https://huggingface.co/papers/2410.18976', 'abstract': 'Recent years have witnessed a significant interest in developing large multimodal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluation benchmarks are predominantly English-centric. In this work, we develop a comprehensive LMM evaluation benchmark for the Arabic language to represent a large population of over 400 million speakers. The proposed benchmark, named CAMEL-Bench, comprises eight diverse domains and 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding to evaluate broad scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions that are filtered from a larger pool of samples, where the quality is manually verified by native speakers to ensure reliable model assessment. We conduct evaluations of both closed-source, including GPT-4 series, and open-source LMMs. Our analysis reveals the need for substantial improvement, especially among the best open-source models, with even the closed-source GPT-4o achieving an overall score of 62%. Our benchmark and evaluation scripts are open-sourced.', 'score': 8, 'issue_id': 260, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '932cc733871eb790', 'data': {'categories': ['#benchmark', '#cv', '#multilingual', '#video', '#graphs', '#multimodal', '#healthcare', '#open_source', '#low_resource', '#3d'], 'emoji': '🐪', 'ru': {'title': 'CAMEL-Bench: первый комплексный бенчмарк для мультимодальных моделей на арабском языке', 'desc': 'В статье представлен новый бенчмарк CAMEL-Bench для оценки мультимодальных языковых моделей (LMM) на арабском языке. Бенчмарк охватывает 8 разнообразных областей и 38 подобластей, включая понимание нескольких изображений, сложное визуальное восприятие и анализ видео. CAMEL-Bench содержит около 29,036 вопросов, качество которых проверено носителями языка. Оценка показала, что даже лучшие модели, такие как GPT-4, достигают лишь 62% общего балла, что указывает на необходимость дальнейшего улучшения LMM для арабского языка.'}, 'en': {'title': 'Bridging the Language Gap in Multimodal AI', 'desc': 'The paper introduces CAMEL-Bench, a new benchmark designed to evaluate large multimodal models (LMMs) in the Arabic language, addressing the lack of non-English-centric evaluation tools. CAMEL-Bench covers a wide range of tasks across eight domains, such as visual perception and medical imaging, to test the generalizability of LMMs. The benchmark includes over 29,000 questions, carefully curated and verified by native speakers to ensure accuracy. Evaluations show that even advanced models like GPT-4o have room for improvement, highlighting the need for better open-source LMMs.'}, 'zh': {'title': 'CAMEL-Bench：阿拉伯语多模态模型评估新基准', 'desc': '近年来，研究人员对开发能够执行各种视觉推理和理解任务的大型多模态模型（LMMs）表现出极大兴趣。现有的LMM评估基准大多以英语为中心，而本文提出了一个针对阿拉伯语的全面LMM评估基准，名为CAMEL-Bench。该基准涵盖八个不同领域和38个子领域，包括多图像理解、复杂视觉感知、手写文档理解等，以评估模型的广泛适应性。评估结果显示，即使是封闭源的GPT-4o模型也仅获得62%的总分，表明仍需大幅改进。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18505', 'title': 'CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models', 'url': 'https://huggingface.co/papers/2410.18505', 'abstract': 'We present CCI3.0-HQ (https://huggingface.co/datasets/BAAI/CCI3-HQ), a high-quality 500GB subset of the Chinese Corpora Internet 3.0 (CCI3.0)(https://huggingface.co/datasets/BAAI/CCI3-Data), developed using a novel two-stage hybrid filtering pipeline that significantly enhances data quality. To evaluate its effectiveness, we trained a 0.5B parameter model from scratch on 100B tokens across various datasets, achieving superior performance on 10 benchmarks in a zero-shot setting compared to CCI3.0, SkyPile, and WanjuanV1. The high-quality filtering process effectively distills the capabilities of the Qwen2-72B-instruct model into a compact 0.5B model, attaining optimal F1 scores for Chinese web data classification. We believe this open-access dataset will facilitate broader access to high-quality language models.', 'score': 8, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '4f06cfa8b602eba9', 'data': {'categories': ['#small_models', '#benchmark', '#multilingual', '#data', '#dataset', '#transfer_learning', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Высококачественные данные для компактных и мощных языковых моделей', 'desc': 'Представлен CCI3.0-HQ - высококачественный набор данных объемом 500 ГБ, полученный из Chinese Corpora Internet 3.0. Для его создания использовался новый двухэтапный гибридный конвейер фильтрации, значительно улучшающий качество данных. Модель с 0.5 миллиардами параметров, обученная на этом наборе, превзошла аналоги на 10 бенчмарках в режиме zero-shot. Процесс фильтрации эффективно дистиллировал возможности модели Qwen2-72B-instruct в компактную модель размером 0.5B.'}, 'en': {'title': 'Unlocking High-Quality Language Models with CCI3.0-HQ', 'desc': 'The paper introduces CCI3.0-HQ, a refined 500GB dataset from the Chinese Corpora Internet 3.0, created using an innovative two-stage hybrid filtering process to improve data quality. A 0.5 billion parameter model was trained on 100 billion tokens from various datasets, outperforming other models in zero-shot settings on 10 benchmarks. This filtering method successfully compresses the capabilities of a larger model into a smaller one, achieving high F1 scores in Chinese web data classification. The open-access nature of this dataset aims to make high-quality language models more accessible to researchers and developers.'}, 'zh': {'title': '高质量中文语料库，提升语言模型表现', 'desc': 'CCI3.0-HQ 是一个高质量的中文语料库子集，使用创新的两阶段混合过滤流程来提升数据质量。研究人员从头开始训练了一个拥有0.5亿参数的模型，并在多种数据集上处理了1000亿个标记，结果在10个基准测试中表现优异。这个高质量的过滤过程有效地将Qwen2-72B-instruct模型的能力浓缩到一个紧凑的0.5亿模型中，实现了中文网络数据分类的最佳F1分数。我们相信这个开放访问的数据集将促进高质量语言模型的更广泛使用。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18860', 'title': 'DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations', 'url': 'https://huggingface.co/papers/2410.18860', 'abstract': 'Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval heads, responsible for extracting relevant contextual information. We hypothesise that masking these retrieval heads can induce hallucinations and that contrasting the outputs of the base LLM and the masked LLM can reduce hallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads (DeCoRe), a novel training-free decoding strategy that amplifies information found in the context and model parameters. DeCoRe mitigates potentially hallucinated responses by dynamically contrasting the outputs of the base LLM and the masked LLM, using conditional entropy as a guide. Our extensive experiments confirm that DeCoRe significantly improves performance on tasks requiring high contextual faithfulness, such as summarisation (XSum by 18.6%), instruction following (MemoTrap by 10.9%), and open-book question answering (NQ-Open by 2.4% and NQ-Swap by 5.5%).', 'score': 7, 'issue_id': 264, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'c82b0b4cc69a7fd9', 'data': {'categories': ['#hallucinations', '#inference', '#interpretability', '#training', '#architecture'], 'emoji': '🎭', 'ru': {'title': 'Борьба с галлюцинациями в LLM путем контрастирования голов внимания', 'desc': 'Исследователи предложили новый метод декодирования под названием DeCoRe для уменьшения галлюцинаций в больших языковых моделях (LLM). Метод основан на маскировании определенных голов внимания в архитектуре Transformer, отвечающих за извлечение контекстной информации. DeCoRe сравнивает выходные данные базовой LLM и замаскированной LLM, используя условную энтропию. Эксперименты показали значительное улучшение производительности в задачах, требующих высокой контекстной точности, таких как суммаризация, следование инструкциям и ответы на вопросы.'}, 'en': {'title': 'DeCoRe: Enhancing LLM Accuracy by Contrasting Retrieval Heads', 'desc': 'The paper addresses the issue of hallucinations in Large Language Models (LLMs), where models produce incorrect outputs by misrepresenting context or recalling wrong information. It identifies specific attention heads, called retrieval heads, in the Transformer architecture that are crucial for extracting relevant information. The authors propose a method called Decoding by Contrasting Retrieval Heads (DeCoRe), which reduces hallucinations by comparing outputs from the original and masked LLMs. Experiments show that DeCoRe significantly improves the accuracy of tasks like summarization and question answering by enhancing contextual faithfulness.'}, 'zh': {'title': '对比检索头，减少模型幻觉', 'desc': '大型语言模型（LLMs）有时会产生不准确或不真实的输出，这被称为幻觉。研究发现，Transformer架构中的特定注意力头，称为检索头，负责提取相关的上下文信息。我们提出了一种新的解码策略，称为对比检索头解码（DeCoRe），通过对比基础模型和掩蔽模型的输出来减少幻觉。实验表明，DeCoRe在需要高上下文忠实度的任务中显著提高了性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17779', 'title': 'ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning', 'url': 'https://huggingface.co/papers/2410.17779', 'abstract': 'Recent advancements in multimodal fusion have witnessed the remarkable success of vision-language (VL) models, which excel in various multimodal applications such as image captioning and visual question answering. However, building VL models requires substantial hardware resources, where efficiency is restricted by two key factors: the extended input sequence of the language model with vision features demands more computational operations, and a large number of additional learnable parameters increase memory complexity. These challenges significantly restrict the broader applicability of such models. To bridge this gap, we propose ADEM-VL, an efficient vision-language method that tunes VL models based on pretrained large language models (LLMs) by adopting a parameter-free cross-attention mechanism for similarity measurements in multimodal fusion. This approach only requires embedding vision features into the language space, significantly reducing the number of trainable parameters and accelerating both training and inference speeds. To enhance representation learning in fusion module, we introduce an efficient multiscale feature generation scheme that requires only a single forward pass through the vision encoder. Moreover, we propose an adaptive fusion scheme that dynamically discards less relevant visual information for each text token based on its attention score. This ensures that the fusion process prioritizes the most pertinent visual features. With experiments on various tasks including visual question answering, image captioning, and instruction-following, we demonstrate that our framework outperforms existing approaches. Specifically, our method surpasses existing methods by an average accuracy of 0.77% on ScienceQA dataset, with reduced training and inference latency, demonstrating the superiority of our framework. The code is available at https://github.com/Hao840/ADEM-VL.', 'score': 7, 'issue_id': 260, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'ce2779c63c3de4ec', 'data': {'categories': ['#science', '#cv', '#inference', '#optimization', '#multimodal', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🔮', 'ru': {'title': 'ADEM-VL: Эффективное мультимодальное слияние для моделей зрения-языка', 'desc': 'Статья представляет ADEM-VL - эффективный метод для моделей зрения-языка, основанный на предобученных больших языковых моделях (LLM). Он использует механизм кросс-внимания без параметров для измерения сходства в мультимодальном слиянии, что значительно сокращает количество обучаемых параметров и ускоряет процессы обучения и вывода. Метод включает эффективную схему генерации многомасштабных признаков и адаптивное слияние для динамического отбрасывания менее релевантной визуальной информации. Эксперименты показывают превосходство ADEM-VL над существующими подходами в различных задачах, включая ответы на визуальные вопросы и генерацию подписей к изображениям.'}, 'en': {'title': 'Efficient Vision-Language Fusion: ADEM-VL Revolutionizes Multimodal Models', 'desc': 'The paper introduces ADEM-VL, a new method for improving vision-language models by using a parameter-free cross-attention mechanism. This approach reduces the number of trainable parameters and speeds up both training and inference by embedding vision features into the language space. It also includes a multiscale feature generation scheme and an adaptive fusion process to prioritize relevant visual information. Experiments show that ADEM-VL outperforms existing methods in tasks like visual question answering and image captioning, with better accuracy and efficiency.'}, 'zh': {'title': 'ADEM-VL：高效的视觉语言融合新方法', 'desc': '这篇论文介绍了一种名为ADEM-VL的高效视觉语言方法，通过无参数的交叉注意机制来优化预训练的大型语言模型，从而实现多模态融合。该方法通过将视觉特征嵌入到语言空间中，显著减少了可训练参数的数量，加快了训练和推理速度。为了提高融合模块的表示学习，论文引入了一种高效的多尺度特征生成方案，并提出了一种自适应融合方案，动态丢弃与文本不相关的视觉信息。实验结果表明，该方法在多个任务上表现优异，尤其是在ScienceQA数据集上平均准确率提高了0.77%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17897', 'title': 'Value Residual Learning For Alleviating Attention Concentration In Transformers', 'url': 'https://huggingface.co/papers/2410.17897', 'abstract': 'Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the KV cache by nearly 50%. Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate.', 'score': 6, 'issue_id': 261, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'fe24d2d7be245ec3', 'data': {'categories': ['#long_context', '#optimization', '#training', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'ResFormer: эффективное улучшение внимания в трансформерах', 'desc': 'Статья представляет новый подход к улучшению работы трансформеров под названием ResFormer. Он решает проблему концентрации внимания в глубоких слоях путем добавления остаточного соединения от значений первого слоя ко всем последующим. Предложенная модификация SVFormer использует одинаковые значения эмбеддингов из первого слоя для всех слоев, что сокращает KV-кэш почти на 50%. Эмпирические исследования показывают, что ResFormer превосходит стандартный трансформер и другие методы по ошибке обучения и в задачах обработки естественного языка.'}, 'en': {'title': 'ResFormer: Smarter, Faster Transformers with Less Concentration', 'desc': 'Transformers are powerful because they use self-attention to understand relationships between all parts of a sequence, but this can lead to too much focus on certain parts when many layers are stacked. To solve this, the paper introduces ResFormer, which uses a clever shortcut to let information from the first layer help later layers without needing lots of extra calculations. Another version, SVFormer, simplifies this even more by using the same information from the first layer across all layers, making it faster and more efficient. Tests show that these new methods not only reduce the problem of over-concentration but also improve learning and performance compared to traditional Transformers and other models.'}, 'zh': {'title': 'ResFormer：突破注意力集中瓶颈的新方法', 'desc': '这篇论文介绍了一种新的Transformer模型，称为ResFormer，通过在所有后续层中添加来自第一层的残差连接来近似跨层注意力，从而解决了注意力集中问题。ResFormer在更深层次上缓解了注意力集中问题，并在大多数层中增强了表示能力，优于传统的Transformer和其他模型。另一种变体SVFormer通过共享第一层的值嵌入，减少了近50%的KV缓存，显著加快了训练速度。实验结果表明，SVFormer在训练速度和性能上都优于其他方法，受序列长度和累积学习率的影响。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.15580', 'title': 'Language Models are Symbolic Learners in Arithmetic', 'url': 'https://huggingface.co/papers/2410.15580', 'abstract': 'Large Language Models (LLMs) are thought to struggle with arithmetic learning due to the inherent differences between language modeling and numerical computation, but concrete evidence has been lacking. This work responds to this claim through a two-side experiment. We first investigate whether LLMs leverage partial products during arithmetic learning. We find that although LLMs can identify some partial products after learning, they fail to leverage them for arithmetic tasks, conversely. We then explore how LLMs approach arithmetic symbolically by breaking tasks into subgroups, hypothesizing that difficulties arise from subgroup complexity and selection. Our results show that when subgroup complexity is fixed, LLMs treat a collection of different arithmetic operations similarly. By analyzing position-level accuracy across different training sizes, we further observe that it follows a U-shaped pattern: LLMs quickly learn the easiest patterns at the first and last positions, while progressively learning the more difficult patterns in the middle positions. This suggests that LLMs select subgroup following an easy-to-hard paradigm during learning. Our work confirms that LLMs are pure symbolic learners in arithmetic tasks and underscores the importance of understanding them deeply through subgroup-level quantification.', 'score': 6, 'issue_id': 259, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '8d361b61d5fc18c3', 'data': {'categories': ['#reasoning', '#math', '#interpretability', '#training', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'LLM в арифметике: символическое обучение от простого к сложному', 'desc': "Это исследование посвящено изучению способностей больших языковых моделей (LLM) к обучению арифметике. Авторы обнаружили, что хотя LLM могут идентифицировать частичные произведения после обучения, они не способны использовать их для арифметических задач. Исследование также показало, что LLM подходят к арифметике символически, разбивая задачи на подгруппы, и выбирают подгруппы по принципу 'от простого к сложному'. Результаты подтверждают, что LLM являются чисто символическими учащимися в арифметических задачах."}, 'en': {'title': 'Unveiling the Symbolic Mind: How LLMs Tackle Arithmetic', 'desc': "This paper investigates how Large Language Models (LLMs) handle arithmetic tasks, revealing that they struggle to use partial products effectively. The study shows that LLMs approach arithmetic symbolically, breaking tasks into subgroups and learning them in an easy-to-hard sequence. The research finds that LLMs treat different arithmetic operations similarly when subgroup complexity is controlled. The findings highlight the importance of understanding LLMs' symbolic learning processes through subgroup-level analysis."}, 'zh': {'title': '揭示大型语言模型的算术学习奥秘', 'desc': '这篇论文研究了大型语言模型（LLMs）在算术学习中的表现。研究发现，虽然LLMs可以识别部分乘积，但在算术任务中并未有效利用这些信息。通过将任务分解为子组，研究表明LLMs在固定子组复杂度时，对不同算术操作的处理方式相似。结果显示，LLMs在学习过程中遵循从易到难的模式，强调了通过子组层次的量化来深入理解LLMs的重要性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18441', 'title': 'The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI', 'url': 'https://huggingface.co/papers/2410.18441', 'abstract': 'In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative AI models from algorithmic and probabilistic optimization perspective. In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model [17]. In addition, we propose a factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with a harmonic series. We also present a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a probability distribution over block distances in the matrix to decide which block is likely to participate in a given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings.', 'score': 5, 'issue_id': 266, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '3e66fdb5b0da206e', 'data': {'categories': ['#inference', '#optimization', '#math', '#training', '#transformers', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация ключевых компонентов Transformer для улучшения генеративного ИИ', 'desc': 'Статья представляет глубокий анализ математических формулировок и вероятностных оптимизаций ключевых компонентов модели Transformer в области генеративного ИИ. Авторы предлагают оптимальное решение для кодирования подслов (SWE), основанное на алгоритме byte-pair encoding (BPE) и подходе WordPiece. Представлен метод оптимизации перекрестной энтропии для настройки гиперпараметров модели word2vec. Также предложена комбинация позиционного кодирования RoPE и метода внимания ALiBi с гармоническим рядом, а также вероятностный метод FlashAttention (PrFlashAttention) для эффективного вычисления внимания.'}, 'en': {'title': 'Optimizing Transformers: Enhancing Generative AI with Mathematical Precision', 'desc': 'This paper delves into the mathematical and probabilistic optimization aspects of Transformer models, focusing on enhancing generative AI technologies. It introduces an optimal sub-word encoding solution inspired by byte-pair encoding and WordPiece to improve training data likelihood. The authors propose a novel combination of rotary positional encoding and attention with linear biases, alongside a probabilistic FlashAttention method to optimize attention computation. Additionally, they present a staircase adaptive quantization technique for multi-query attention to balance model quality and cost efficiency.'}, 'zh': {'title': '生成式AI的概率优化新突破', 'desc': '这篇论文深入分析了生成式AI中Transformer模型的一些关键组件的数学问题和概率优化探索。作者提出了一种基于子词编码的最优解决方案，类似于字节对编码和WordPiece方法，以最大化训练数据的可能性。此外，论文还介绍了一种交叉熵优化方法来优化word2vec模型的超参数，并提出了旋转位置编码和线性偏置注意力的组合方法。最后，作者提出了一种阶梯自适应量化方法，用于多查询注意力的键值缓存，以在保持模型质量的同时节省成本。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18252', 'title': 'Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models', 'url': 'https://huggingface.co/papers/2410.18252', 'abstract': "The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical deep RL literature, we propose separating generation and learning in RLHF. This enables asynchronous generation of new samples while simultaneously training on old samples, leading to faster training and more compute-optimal scaling. However, asynchronous training relies on an underexplored regime, online but off-policy RLHF: learning on samples from previous iterations of our model. To understand the challenges in this regime, we investigate a fundamental question: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? Among several RLHF algorithms we tested, we find that online DPO is most robust to off-policy data, and robustness increases with the scale of the policy model. We study further compute optimizations for asynchronous RLHF but find that they come at a performance cost, giving rise to a trade-off. Finally, we verify the scalability of asynchronous RLHF by training LLaMA 3.1 8B on an instruction-following task 40% faster than a synchronous run while matching final performance.", 'score': 5, 'issue_id': 265, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '9f92e038af1d74a2', 'data': {'categories': ['#rl', '#rlhf', '#optimization', '#training', '#alignment'], 'emoji': '⚡', 'ru': {'title': 'Ускорение RLHF через асинхронное обучение', 'desc': 'Статья предлагает новый подход к обучению с подкреплением по обратной связи человека (RLHF) для больших языковых моделей. Авторы предлагают разделить процессы генерации и обучения, что позволяет асинхронно генерировать новые образцы и одновременно обучаться на старых. Исследование показывает, что онлайн-алгоритм DPO наиболее устойчив к использованию офф-политики данных. Эксперименты подтверждают, что асинхронный RLHF позволяет обучать модели на 40% быстрее при сохранении производительности.'}, 'en': {'title': 'Speeding Up RLHF: Asynchronous Training for Faster Learning', 'desc': 'The paper explores a new approach to Reinforcement Learning with Human Feedback (RLHF) by separating the generation of data from the learning process, which allows for asynchronous training. This method, inspired by classical deep reinforcement learning, enables the model to generate new samples while training on old ones, improving computational efficiency. The study investigates the balance between using off-policy data and maintaining performance, finding that the online DPO algorithm is particularly robust in this setting. The authors demonstrate that this asynchronous approach can significantly speed up training times, as shown by their experiments with the LLaMA 3.1 8B model.'}, 'zh': {'title': '异步RLHF：加速训练的新范式', 'desc': '这篇论文探讨了在强化学习中使用人类反馈（RLHF）的新方法。传统方法效率低下，而作者提出将生成和学习过程分离，以提高训练速度。通过异步生成新样本并同时训练旧样本，研究表明这种方法可以加快训练速度。尽管如此，异步训练需要在在线但非策略的情况下进行，作者研究了这种情况下的挑战和权衡。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18785', 'title': 'Should We Really Edit Language Models? On the Evaluation of Edited Language Models', 'url': 'https://huggingface.co/papers/2410.18785', 'abstract': 'Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. Current methods mainly focus on reliability, generalization, and locality, with many methods excelling across these criteria. Some recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict. However, the general abilities of post-edited language models remain unexplored. In this paper, we perform a comprehensive evaluation on various editing methods and different language models, and have following findings. (1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits. When the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged. (2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing. (3) Language model with large scale is more resistant to editing compared to small model. (4) The safety of the edited model, is significantly weakened, even for those safety-aligned models. Our findings indicate that current editing methods are only suitable for small-scale knowledge updates within language models, which motivates further research on more practical and reliable editing methods. The details of code and reproduction can be found in https://github.com/lqinfdim/EditingEvaluation.', 'score': 5, 'issue_id': 257, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '8ee1a0454341105c', 'data': {'categories': ['#small_models', '#benchmark', '#interpretability', '#training', '#open_source', '#architecture', '#alignment'], 'emoji': '✏️', 'ru': {'title': 'Редактирование языковых моделей: компромисс между обновлением знаний и сохранением общих способностей', 'desc': 'В статье рассматривается влияние методов редактирования на общие способности языковых моделей. Исследование показывает, что существующие методы редактирования приводят к неизбежному ухудшению производительности на общих бенчмарках, особенно при большом количестве правок. Модели, настроенные на инструкции, и крупномасштабные языковые модели оказываются более устойчивыми к редактированию. Однако безопасность отредактированных моделей значительно снижается, даже для моделей, ориентированных на безопасность.'}, 'en': {'title': 'Balancing Act: Navigating the Challenges of Model Editing in Language Models', 'desc': 'The paper explores the impact of model editing on language models, revealing that current methods can lead to performance deterioration when too many edits are made. It finds that instruction-tuned models and larger models are more robust to these edits, maintaining better performance. However, even safety-aligned models experience weakened safety post-editing. The study suggests that existing editing methods are only effective for small-scale updates, highlighting the need for more reliable techniques.'}, 'zh': {'title': '语言模型编辑：小规模更新的挑战与机遇', 'desc': '这篇论文探讨了在语言模型中进行知识更新的编辑方法。研究发现，现有的编辑方法在进行大量编辑时，会导致模型性能下降，甚至破坏模型的内在知识结构。经过指令调优的模型在编辑后表现出更强的鲁棒性，而大规模的语言模型比小规模模型更能抵抗编辑的影响。此外，编辑后的模型安全性显著降低。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18194', 'title': 'ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment', 'url': 'https://huggingface.co/papers/2410.18194', 'abstract': 'Data selection is crucial for optimizing language model (LM) performance on specific tasks, yet most existing methods fail to effectively consider the target task distribution.   Current approaches either ignore task-specific requirements entirely or rely on approximations that fail to capture the nuanced patterns needed for tasks like Autoformalization or code generation.   Methods that do consider the target distribution often rely on simplistic, sometimes noisy, representations, like hashed n-gram features, which can lead to collisions and introduce noise.   We introduce ZIP-FIT, a data selection framework that uses gzip compression to directly measure alignment between potential training data and the target task distribution.   In extensive evaluations on Autoformalization and Python code generation, ZIP-FIT significantly outperforms leading baselines like DSIR and D4.   Models trained on ZIP-FIT-selected data achieve their lowest cross-entropy loss up to 85.1\\% faster than baselines, demonstrating that better task alignment leads to more efficient learning.   In addition, ZIP-FIT performs selection up to 65.8\\% faster than DSIR and two orders of magnitude faster than D4.   Notably, ZIP-FIT shows that smaller, well-aligned datasets often outperform larger but less targeted ones, demonstrating that a small amount of higher quality data is superior to a large amount of lower quality data.   Our results imply that task-aware data selection is crucial for efficient domain adaptation, and that compression offers a principled way to measure task alignment.   By showing that targeted data selection can dramatically improve task-specific performance, our work provides new insights into the relationship between data quality, task alignment, and model learning efficiency.', 'score': 4, 'issue_id': 268, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '36d3e8b24c9c11c2', 'data': {'categories': ['#small_models', '#benchmark', '#optimization', '#plp', '#data', '#training', '#transfer_learning'], 'emoji': '🎯', 'ru': {'title': 'Эффективный отбор данных для точной настройки языковых моделей', 'desc': 'ZIP-FIT - это новый метод отбора данных для обучения языковых моделей, использующий сжатие gzip для измерения соответствия между потенциальными обучающими данными и целевым распределением задачи. В сравнении с существующими подходами, ZIP-FIT значительно превосходит базовые методы в задачах автоформализации и генерации кода на Python. Модели, обученные на данных, отобранных ZIP-FIT, достигают минимальной кросс-энтропийной ошибки до 85.1% быстрее, чем базовые методы. Результаты показывают, что небольшие, но хорошо подобранные наборы данных часто превосходят большие, но менее целевые.'}, 'en': {'title': '"ZIP-FIT: Compressing Data, Expanding Performance"', 'desc': 'The paper introduces ZIP-FIT, a novel data selection framework that uses gzip compression to align training data with specific task distributions, improving language model performance. ZIP-FIT significantly outperforms existing methods like DSIR and D4 in tasks such as Autoformalization and Python code generation by achieving lower cross-entropy loss faster. The framework demonstrates that smaller, well-aligned datasets can be more effective than larger, less targeted ones, highlighting the importance of data quality over quantity. This approach underscores the value of task-aware data selection and suggests that compression can effectively measure task alignment, leading to more efficient domain adaptation.'}, 'zh': {'title': 'ZIP-FIT：通过压缩实现高效任务对齐的数据选择', 'desc': '这篇论文介绍了一种名为ZIP-FIT的数据选择框架，通过使用gzip压缩来直接测量潜在训练数据与目标任务分布的对齐程度。ZIP-FIT在自动形式化和Python代码生成任务中表现优异，比现有的基准方法如DSIR和D4更快、更有效。研究表明，较小但高质量的数据集往往比大而不精确的数据集表现更好，强调了数据质量和任务对齐的重要性。通过展示有针对性的数据选择如何显著提高任务特定性能，ZIP-FIT为数据质量、任务对齐和模型学习效率之间的关系提供了新的见解。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18647', 'title': 'Data Scaling Laws in Imitation Learning for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2410.18647', 'abstract': "Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy's generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90% success rates in novel environments with unseen objects.", 'score': 4, 'issue_id': 264, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'c54cdee62660d7f7', 'data': {'categories': ['#optimization', '#data', '#training', '#robotics', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'Масштабирование данных открывает путь к универсальным роботам-манипуляторам', 'desc': 'Исследователи изучают возможность применения принципов масштабирования данных в робототехнике, в частности, в манипуляциях роботов. Они провели масштабное эмпирическое исследование, собрав более 40 000 демонстраций и выполнив более 15 000 реальных запусков роботов. Результаты показали, что производительность обобщения политики следует степенному закону относительно количества сред и объектов, причем разнообразие важнее абсолютного числа демонстраций. На основе этих выводов была предложена эффективная стратегия сбора данных, позволяющая достичь 90% успеха в новых средах с незнакомыми объектами.'}, 'en': {'title': 'Scaling Data, Scaling Success: Revolutionizing Robotic Manipulation', 'desc': 'The paper explores the concept of data scaling in robotics, specifically focusing on robotic manipulation tasks. It examines whether scaling the amount of data can improve the generalization of robot policies, allowing them to perform tasks with new objects in different environments without additional training. Through extensive experiments, the study finds that the diversity of training environments and objects is more crucial than the sheer number of demonstrations. The research suggests an efficient data collection strategy that can significantly enhance policy performance with minimal effort.'}, 'zh': {'title': '数据扩展：机器人操作的泛化新路径', 'desc': '这篇论文研究了数据扩展在机器人领域，特别是机器人操作中的应用。研究发现，政策的泛化性能与环境和对象的数量呈现出大致的幂律关系。环境和对象的多样性比演示的绝对数量更为重要，一旦达到一定数量，额外的演示效果不大。基于这些发现，作者提出了一种高效的数据收集策略。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16429', 'title': 'Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4', 'url': 'https://huggingface.co/papers/2410.16429', 'abstract': "Machine-assisted theorem proving refers to the process of conducting structured reasoning to automatically generate proofs for mathematical theorems. Recently, there has been a surge of interest in using machine learning models in conjunction with proof assistants to perform this task. In this paper, we introduce Pantograph, a tool that provides a versatile interface to the Lean 4 proof assistant and enables efficient proof search via powerful search algorithms such as Monte Carlo Tree Search. In addition, Pantograph enables high-level reasoning by enabling a more robust handling of Lean 4's inference steps. We provide an overview of Pantograph's architecture and features. We also report on an illustrative use case: using machine learning models and proof sketches to prove Lean 4 theorems. Pantograph's innovative features pave the way for more advanced machine learning models to perform complex proof searches and high-level reasoning, equipping future researchers to design more versatile and powerful theorem provers.", 'score': 3, 'issue_id': 268, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '0dae164d1e8a7708', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#math', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Pantograph: мост между машинным обучением и автоматическим доказательством теорем', 'desc': 'Статья представляет инструмент Pantograph для автоматического доказательства теорем с помощью машинного обучения. Pantograph обеспечивает интерфейс к ассистенту доказательств Lean 4 и позволяет эффективно искать доказательства с помощью алгоритмов, таких как поиск методом Монте-Карло. Инструмент также поддерживает высокоуровневые рассуждения благодаря улучшенной обработке шагов вывода в Lean 4. Авторы описывают архитектуру Pantograph и демонстрируют его применение для доказательства теорем в Lean 4 с использованием моделей машинного обучения.'}, 'en': {'title': 'Pantograph: Elevating Theorem Proving with Machine Learning', 'desc': "The paper introduces Pantograph, a tool that integrates with the Lean 4 proof assistant to enhance theorem proving using machine learning. Pantograph employs advanced search algorithms like Monte Carlo Tree Search to efficiently explore proof possibilities. It also improves high-level reasoning by better managing Lean 4's inference steps, making the proof process more robust. The tool's architecture and features are designed to support the development of more sophisticated machine learning models for complex proof searches."}, 'zh': {'title': 'Pantograph：推动机器学习与定理证明的创新结合', 'desc': '这篇论文介绍了一个名为Pantograph的工具，它为Lean 4证明助手提供了一个多功能接口。Pantograph通过使用强大的搜索算法，如蒙特卡罗树搜索，实现了高效的证明搜索。该工具还增强了Lean 4推理步骤的处理能力，从而支持更高级别的推理。Pantograph的创新特性为未来的研究人员设计更强大和多功能的定理证明器铺平了道路。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18234', 'title': 'Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits', 'url': 'https://huggingface.co/papers/2410.18234', 'abstract': 'We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection scheme based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.', 'score': 3, 'issue_id': 258, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'e421e5808ba5b485', 'data': {'categories': ['#optimization', '#math', '#training', '#rl'], 'emoji': '🎲', 'ru': {'title': 'Оптимизация выбора токенов при мультимодельном сэмплировании', 'desc': 'Статья рассматривает мультимодельное спекулятивное сэмплирование в контексте языковых моделей. Авторы предлагают оптимальную двухэтапную схему выбора токенов, использующую важностную выборку. Для случая двух идентичных черновых моделей установлены условия для 100% вероятности принятия и получено выражение для оптимальной вероятности. Экспериментальные результаты показывают улучшение эффективности блоков и скорости генерации токенов по сравнению с базовыми методами.'}, 'en': {'title': 'Optimizing Token Selection: A Two-Step Approach to Speculative Sampling', 'desc': 'The paper explores multi-draft speculative sampling, where different draft models independently generate proposal sequences. It introduces a two-step process for optimal token selection: first, an importance sampling scheme selects an intermediate token, followed by speculative sampling to produce the final output token. The authors establish conditions for achieving maximum acceptance probability with two identical draft models and propose a new token selection scheme using weighted importance sampling. Experimental results show that this approach improves block efficiency and token rates compared to traditional methods.'}, 'zh': {'title': '多草稿推测采样：提高效率的新方法', 'desc': '这篇论文研究了多草稿推测采样方法，其中提议序列是从不同的草稿模型中独立采样的。在每一步中，使用一个令牌级别的草稿选择方案来匹配目标模型的分布。作者提出了一种两步解决方案：首先使用重要性采样选择一个中间令牌，然后应用单草稿推测采样生成输出令牌。实验结果表明，这种方法在多个场景中提高了块效率和令牌速率。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17856', 'title': 'ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting', 'url': 'https://huggingface.co/papers/2410.17856', 'abstract': 'Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. A key issue is the difficulty in smoothly connecting individual entities in low-level observations with abstract concepts required for planning. A common approach to address this problem is through the use of hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language and imagined observations. However, language often fails to effectively convey spatial information, while generating future images with sufficient accuracy remains challenging. To address these limitations, we propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from both past and present observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, with real-time object tracking provided by SAM-2. Our method unlocks the full potential of VLMs visual-language reasoning abilities, enabling them to solve complex creative tasks, especially those heavily reliant on spatial understanding. Experiments in Minecraft demonstrate that our approach allows agents to accomplish previously unattainable tasks, highlighting the effectiveness of visual-temporal context prompting in embodied decision-making. Codes and demos will be available on the project page: https://craftjarvis.github.io/ROCKET-1.', 'score': 48, 'issue_id': 302, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '0a477d46d035f1d4', 'data': {'categories': ['#reasoning', '#cv', '#graphs', '#multimodal', '#robotics', '#open_source', '#games', '#agents', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Визуально-временной контекст открывает новые горизонты для VLM в воплощенном ИИ', 'desc': 'Статья представляет новый подход к использованию визуально-языковых моделей (VLM) в задачах принятия решений в открытых средах. Авторы предлагают протокол визуально-временного контекстного промптинга для улучшения взаимодействия между VLM и моделями политик. Метод использует сегментацию объектов из прошлых и текущих наблюдений для управления взаимодействием политики и среды. Эксперименты в Minecraft показывают, что подход позволяет агентам решать ранее недостижимые задачи, особенно требующие пространственного понимания.'}, 'en': {'title': 'Unlocking Spatial Reasoning in Decision-Making with VLMs', 'desc': 'This paper addresses the challenges of using vision-language models (VLMs) for decision-making in dynamic environments. It highlights the difficulty of linking low-level visual data with high-level abstract concepts necessary for planning. The authors introduce a new method called visual-temporal context prompting, which enhances communication between VLMs and policy models by utilizing object segmentation from past and present observations. Their approach, implemented in the ROCKET-1 model, demonstrates improved performance in complex tasks that require spatial reasoning, as shown in experiments conducted in Minecraft.'}, 'zh': {'title': '视觉-时间上下文提示：提升具身决策能力的关键', 'desc': '视觉语言模型（VLMs）在多模态任务中表现出色，但在开放世界环境中进行具身决策时面临挑战。主要问题是如何将低级观察中的个体实体与规划所需的抽象概念顺利连接。为了解决这个问题，本文提出了一种视觉-时间上下文提示的新通信协议，利用过去和现在观察中的物体分割来指导策略与环境的交互。通过这种方法，我们训练了ROCKET-1，一个基于视觉观察和分割掩码预测动作的低级策略，展示了在复杂创意任务中，尤其是依赖空间理解的任务中的有效性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16048', 'title': 'Continuous Speech Synthesis using per-token Latent Diffusion', 'url': 'https://huggingface.co/papers/2410.16048', 'abstract': 'The success of autoregressive transformer models with discrete tokens has inspired quantization-based approaches for continuous modalities, though these often limit reconstruction quality. We therefore introduce SALAD, a per-token latent diffusion model for zero-shot text-to-speech, that operates on continuous representations. SALAD builds upon the recently proposed expressive diffusion head for image generation, and extends it to generate variable-length outputs. Our approach utilizes semantic tokens for providing contextual information and determining the stopping condition. We suggest three continuous variants for our method, extending popular discrete speech synthesis techniques. Additionally, we implement discrete baselines for each variant and conduct a comparative analysis of discrete versus continuous speech modeling techniques. Our results demonstrate that both continuous and discrete approaches are highly competent, and that SALAD achieves a superior intelligibility score while obtaining speech quality and speaker similarity on par with the ground-truth audio.', 'score': 28, 'issue_id': 308, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '8d8228e9ec5fdf77', 'data': {'categories': ['#diffusion', '#synthetic', '#training', '#audio', '#architecture'], 'emoji': '🗣️', 'ru': {'title': 'SALAD: непрерывная диффузия для качественного синтеза речи', 'desc': 'SALAD - это новая модель латентной диффузии для синтеза речи без предварительного обучения. Она работает с непрерывными представлениями, что позволяет достичь высокого качества реконструкции. Модель использует семантические токены для контекстной информации и определения условия остановки. Авторы сравнили непрерывный и дискретный подходы к моделированию речи, показав высокую эффективность обоих методов.'}, 'en': {'title': 'SALAD: Revolutionizing Text-to-Speech with Continuous Representations', 'desc': 'This paper presents SALAD, a novel per-token latent diffusion model designed for zero-shot text-to-speech synthesis using continuous representations. Unlike traditional methods that rely on discrete tokens, SALAD leverages semantic tokens to enhance contextual understanding and manage output length. The model builds on recent advancements in diffusion techniques for image generation, adapting them for speech synthesis. Comparative analysis shows that SALAD not only matches the quality of existing discrete methods but also excels in intelligibility, making it a promising approach for high-quality speech generation.'}, 'zh': {'title': 'SALAD：提升文本到语音转换的新方法', 'desc': '本文介绍了一种名为SALAD的模型，它是一种基于每个标记的潜在扩散模型，旨在实现零-shot文本到语音转换。SALAD利用连续表示，克服了传统离散标记模型在重建质量上的限制。该模型借鉴了用于图像生成的扩散头，并扩展到生成可变长度的输出。我们的实验表明，SALAD在语音可懂度上表现优越，同时在语音质量和说话人相似性方面与真实音频相当。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.19008', 'title': 'Teach Multimodal LLMs to Comprehend Electrocardiographic Images', 'url': 'https://huggingface.co/papers/2410.19008', 'abstract': 'The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may not be readily available in resource-limited settings where only printed or digital ECG images are accessible. Recent advancements in multimodal large language models (MLLMs) present promising opportunities for addressing these challenges. However, the application of MLLMs to ECG image interpretation remains challenging due to the lack of instruction tuning datasets and well-established ECG image benchmarks for quantitative evaluation. To address these challenges, we introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset of over one million samples, covering a wide range of ECG-related tasks from diverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for ECG image comprehension. In addition, we curate ECGBench, a new evaluation benchmark covering four key ECG image interpretation tasks across nine different datasets. Our experiments show that PULSE sets a new state-of-the-art, outperforming general MLLMs with an average accuracy improvement of 15% to 30%. This work highlights the potential of PULSE to enhance ECG interpretation in clinical practice.', 'score': 22, 'issue_id': 300, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '85936de603f8cc7a', 'data': {'categories': ['#science', '#synthetic', '#benchmark', '#cv', '#healthcare', '#multimodal', '#dataset', '#open_source', '#architecture'], 'emoji': '❤️', 'ru': {'title': 'PULSE: Революция в автоматическом анализе ЭКГ с помощью мультимодальных языковых моделей', 'desc': 'Статья представляет новый подход к автоматической интерпретации ЭКГ с использованием мультимодальных больших языковых моделей (MLLM). Авторы создали ECGInstruct - набор данных из более чем миллиона образцов ЭКГ для обучения моделей, и разработали PULSE - специализированную MLLM для понимания изображений ЭКГ. Также был создан бенчмарк ECGBench для оценки моделей по четырем ключевым задачам интерпретации ЭКГ. Эксперименты показали, что PULSE превосходит общие MLLM на 15-30% по точности, демонстрируя потенциал для улучшения интерпретации ЭКГ в клинической практике.'}, 'en': {'title': 'Revolutionizing ECG Interpretation with PULSE and ECGInstruct', 'desc': 'This paper presents ECGInstruct, a large dataset designed for instruction tuning of multimodal large language models (MLLMs) specifically for interpreting ECG images. The authors introduce PULSE, an MLLM that leverages ECGInstruct to improve the understanding of ECG images, addressing the limitations of existing methods that rely on raw signals. Additionally, they create ECGBench, a benchmark for evaluating ECG image interpretation across various tasks and datasets. The results demonstrate that PULSE significantly enhances accuracy in ECG interpretation, showcasing its potential for clinical applications.'}, 'zh': {'title': 'PULSE：提升心电图解读的新方法', 'desc': '心电图（ECG）是评估心脏状况的重要非侵入性诊断工具。现有的自动解读方法在通用性上存在局限，通常只关注少数心脏疾病，并依赖于生理信号，这在资源有限的环境中难以获取。为了解决这些问题，我们提出了ECGInstruct，这是一个包含超过一百万个样本的心电图图像指令调优数据集，涵盖了多种心电图相关任务。基于ECGInstruct，我们开发了PULSE，一个专为心电图图像理解而设计的大型多模态语言模型，实验结果显示PULSE在准确性上超越了通用模型，提升幅度达到15%到30%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.19355', 'title': 'FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality', 'url': 'https://huggingface.co/papers/2410.19355', 'abstract': 'In this paper, we present \\textit{FasterCache}, a novel training-free strategy designed to accelerate the inference of video diffusion models with high-quality generation. By analyzing existing cache-based methods, we observe that directly reusing adjacent-step features degrades video quality due to the loss of subtle variations. We further perform a pioneering investigation of the acceleration potential of classifier-free guidance (CFG) and reveal significant redundancy between conditional and unconditional features within the same timestep. Capitalizing on these observations, we introduce FasterCache to substantially accelerate diffusion-based video generation. Our key contributions include a dynamic feature reuse strategy that preserves both feature distinction and temporal continuity, and CFG-Cache which optimizes the reuse of conditional and unconditional outputs to further enhance inference speed without compromising video quality. We empirically evaluate FasterCache on recent video diffusion models. Experimental results show that FasterCache can significantly accelerate video generation (\\eg 1.67times speedup on Vchitect-2.0) while keeping video quality comparable to the baseline, and consistently outperform existing methods in both inference speed and video quality.', 'score': 20, 'issue_id': 301, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '2de0b0700140bc7e', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'FasterCache: Ускорение видео-диффузии без потери качества', 'desc': 'FasterCache - это новая стратегия для ускорения вывода видео-моделей диффузии без дополнительного обучения. Метод анализирует существующие кэш-подходы и оптимизирует повторное использование условных и безусловных признаков. FasterCache включает динамическую стратегию переиспользования признаков и CFG-кэш для сохранения качества видео. Эксперименты показали значительное ускорение генерации видео (например, в 1,67 раза для Vchitect-2.0) при сохранении качества на уровне базовой модели.'}, 'en': {'title': 'Accelerating Video Generation with FasterCache!', 'desc': 'This paper introduces FasterCache, a new method to speed up video generation using diffusion models without needing extra training. The authors found that reusing features from adjacent steps can hurt video quality by losing important details. They also discovered that there is a lot of overlap between features used for conditional and unconditional generation, which can be optimized. FasterCache uses a smart way to reuse features that maintains quality and speed, achieving significant improvements in video generation speed while keeping the quality high.'}, 'zh': {'title': 'FasterCache：加速视频生成的新策略', 'desc': '本文提出了一种名为FasterCache的新策略，旨在加速视频扩散模型的推理过程，同时保持高质量的生成效果。通过分析现有的基于缓存的方法，我们发现直接重用相邻步骤的特征会导致视频质量下降，因为细微变化会丢失。我们还首次研究了无分类器引导（CFG）的加速潜力，揭示了同一时间步内条件特征和无条件特征之间的显著冗余。FasterCache通过动态特征重用策略和优化条件与无条件输出的缓存，显著提高了扩散视频生成的速度，同时保持了视频质量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.19168', 'title': 'MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark', 'url': 'https://huggingface.co/papers/2410.19168', 'abstract': 'The ability to comprehend audio--which includes speech, non-speech sounds, and music--is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges posed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.', 'score': 19, 'issue_id': 300, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '7a747d9a9b0717cc', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#multimodal', '#open_source', '#audio'], 'emoji': '🎵', 'ru': {'title': 'MMAU: новый рубеж в понимании аудио для ИИ', 'desc': 'MMAU - это новый эталонный тест для оценки моделей мультимодального понимания аудио. Он включает 10 тысяч аудиоклипов с вопросами и ответами, охватывающими речь, звуки окружающей среды и музыку. MMAU требует от моделей демонстрации 27 различных навыков и экспертных знаний в области аудио. Даже самые продвинутые модели, такие как Gemini Pro v1.5 и Qwen2-Audio, достигают точности лишь около 53%, что указывает на значительное пространство для улучшения в этой области.'}, 'en': {'title': 'MMAU: Raising the Bar for Audio Understanding in AI', 'desc': 'The paper introduces MMAU, a new benchmark for evaluating multimodal audio understanding models, which is essential for AI to interpret various audio types like speech and music. It consists of 10,000 audio clips with corresponding human-annotated questions and answers that require advanced reasoning and domain-specific knowledge. The benchmark tests models on 27 distinct skills through complex tasks, pushing the limits of current audio understanding capabilities. Results show that even leading models struggle, indicating a need for further advancements in the field.'}, 'zh': {'title': 'MMAU：推动音频理解模型的进步', 'desc': 'MMAU是一个新颖的基准，旨在评估多模态音频理解模型在需要专家级知识和复杂推理的任务上的表现。它包含1万个精心策划的音频片段，并配有人工标注的自然语言问题和答案，涵盖了语音、环境声音和音乐。MMAU要求模型展示27种不同的技能，处理独特且具有挑战性的任务，强调高级感知和推理能力。通过评估18个开源和专有的音频语言模型，结果显示即使是最先进的模型也面临显著挑战，表明在音频理解领域还有很大的改进空间。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18558', 'title': 'Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data', 'url': 'https://huggingface.co/papers/2410.18558', 'abstract': 'Vision-Language Models (VLMs) have recently made significant progress, but the limited scale and quality of open-source instruction data hinder their performance compared to closed-source models. In this work, we address this limitation by introducing Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, enhanced through rigorous quality filtering and deduplication. We also propose a synthetic instruction generation method based on open-source VLMs, using detailed image annotations and diverse question generation. Using this data, we trained a 2-billion-parameter VLM, Aquila-VL-2B, achieving state-of-the-art (SOTA) performance for models of similar scale. This demonstrates that expanding instruction data and generating synthetic data can significantly improve the performance of open-source models.', 'score': 17, 'issue_id': 300, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '18e760a965f56e6d', 'data': {'categories': ['#small_models', '#synthetic', '#cv', '#multimodal', '#data', '#dataset', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Большие данные - ключ к улучшению открытых визуально-языковых моделей', 'desc': 'Исследователи представили Infinity-MM - крупномасштабный мультимодальный набор данных с инструкциями, содержащий 40 миллионов образцов. Они также предложили метод генерации синтетических инструкций на основе открытых визуально-языковых моделей. Используя эти данные, они обучили 2-миллиардную модель Aquila-VL-2B, достигшую наилучших результатов среди моделей аналогичного масштаба. Это демонстрирует, что расширение обучающих данных и генерация синтетических данных могут значительно улучшить производительность открытых моделей.'}, 'en': {'title': 'Unlocking VLM Potential with Infinity-MM Dataset', 'desc': 'This paper presents Infinity-MM, a large-scale multimodal instruction dataset containing 40 million samples, aimed at improving the performance of Vision-Language Models (VLMs). The authors highlight the challenges faced by open-source VLMs due to limited instruction data quality and scale. They introduce a synthetic instruction generation method that leverages detailed image annotations and diverse question generation to enhance the dataset. Training their 2-billion-parameter model, Aquila-VL-2B, on this enriched dataset resulted in state-of-the-art performance, showcasing the potential of expanded and synthetic instruction data for open-source models.'}, 'zh': {'title': '扩展数据，提升开源模型性能！', 'desc': '本文介绍了一种新的多模态指令数据集Infinity-MM，包含4000万条样本，旨在提升开源视觉语言模型（VLM）的性能。通过严格的质量过滤和去重，该数据集的质量得到了显著提高。我们还提出了一种基于开源VLM的合成指令生成方法，利用详细的图像注释和多样的问题生成。最终，我们训练了一个具有20亿参数的VLM模型Aquila-VL-2B，达到了同规模模型的最新性能，证明了扩展指令数据和生成合成数据的重要性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.19123', 'title': 'Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design', 'url': 'https://huggingface.co/papers/2410.19123', 'abstract': 'The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. In this paper, we propose a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. Our approach employs activation sparsity to extract experts. To compose experts, we examine the widely-adopted layer-wise router design and show its redundancy, and thus we introduce the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.', 'score': 15, 'issue_id': 319, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '94c8bf1991abe8b0', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное преобразование языковых моделей в MoE для оптимизации вывода', 'desc': 'В этой статье представлен новый фреймворк Read-ME, который трансформирует предварительно обученные плотные языковые модели в меньшие модели Mixture-of-Experts (MoE). Авторы предлагают использовать разреженность активаций для извлечения экспертов и вводят концепцию предварительного роутинга, отделенного от основной структуры MoE. Этот подход позволяет эффективно предвычислять и планировать выполнение, улучшая пакетную обработку и кэширование с учетом экспертов. Read-ME превосходит другие популярные открытые плотные модели аналогичного масштаба, показывая улучшение до 10.1% на бенчмарке MMLU и снижение среднего времени выполнения до 6.1%.'}, 'en': {'title': 'Transforming Dense Models into Efficient MoE with Read-ME', 'desc': 'This paper introduces Read-ME, a framework that converts pre-trained dense large language models (LLMs) into smaller Mixture-of-Experts (MoE) models, which enhances efficiency and performance. The authors address challenges in MoE models during inference, such as memory management and batching issues, by proposing a new pre-gating router design that allows for better system integration. By utilizing activation sparsity, Read-ME effectively extracts and composes experts, leading to improved resource utilization. The results show that Read-ME significantly outperforms existing dense models, achieving notable gains in accuracy and reduced latency.'}, 'zh': {'title': 'Read-ME：高效的混合专家模型转换框架', 'desc': '本文提出了一种新框架Read-ME，旨在将预训练的密集型大语言模型（LLMs）转化为更小的混合专家（MoE）模型，从而避免从头训练的高成本。我们利用激活稀疏性来提取专家，并提出了一种与MoE主干解耦的预门控路由器，以优化系统友好的预计算和前瞻调度。该方法解决了模型架构与系统策略之间的不匹配问题，提高了专家感知的批处理和缓存效率。实验结果表明，Read-ME在MMLU上比其他流行的开源密集模型提高了10.1%的性能，并将平均端到端延迟降低了6.1%。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18889', 'title': 'Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance', 'url': 'https://huggingface.co/papers/2410.18889', 'abstract': 'NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models. While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency. Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets. In this work, we consider the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples. Through a case study of four datasets from the TRUE benchmark, covering different tasks and domains, we empirically analyze the labeling quality of existing datasets, and compare expert, crowd-sourced, and our LLM-based annotations in terms of agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method. Our findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance. This suggests that many of the LLMs so-called mistakes are due to label errors rather than genuine model failures. Additionally, we discuss the implications of mislabeled data and propose methods to mitigate them in training to improve model performance.', 'score': 15, 'issue_id': 315, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '9deca7d98b0025ca', 'data': {'categories': ['#synthetic', '#benchmark', '#data', '#training', '#dataset', '#survey', '#alignment'], 'emoji': '🏷️', 'ru': {'title': 'LLM как судья: повышение качества аннотаций в NLP-датасетах', 'desc': 'Статья описывает использование больших языковых моделей (LLM) для повышения качества аннотаций в наборах данных для NLP-задач. Авторы сравнивают экспертные, краудсорсинговые и LLM-аннотации на примере четырех датасетов из бенчмарка TRUE. Исследование выявило значительное количество ошибок в существующих метках, исправление которых приводит к повышению оценок производительности моделей. Авторы также предлагают методы смягчения влияния неправильно размеченных данных при обучении моделей.'}, 'en': {'title': 'Enhancing NLP Dataset Quality with LLMs', 'desc': 'This paper addresses the challenges of labeling quality in NLP datasets, which are essential for training and evaluating machine learning models. It highlights the limitations of traditional expert annotations and the trade-offs of crowd-sourced labeling, which can compromise precision. The authors propose using large language models (LLMs) as a tool to identify mislabeled data, demonstrating their effectiveness through a case study on various datasets. The results indicate that correcting label errors can significantly enhance model performance, suggesting that many perceived model failures may actually stem from poor labeling rather than model deficiencies.'}, 'zh': {'title': '利用大型语言模型提升数据集标注质量', 'desc': '这篇论文探讨了自然语言处理（NLP）基准测试中数据集标注的重要性。传统上，专家标注确保了高质量的标签，但随着数据集需求的增加，专家标注的成本难以扩展。虽然众包提供了更具可扩展性的解决方案，但常常牺牲了标注的精确性和一致性。本文提出利用大型语言模型（LLM）作为评判者，通过多个LLM的组合来识别潜在的错误标签，从而提高标注质量。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.19133', 'title': 'Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback', 'url': 'https://huggingface.co/papers/2410.19133', 'abstract': "Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations as they are more consistent, cheaper, and scale better than human annotation; however, they are also prone to biases and errors. In this work, we introduce a routing framework that combines inputs from humans and LMs to achieve better annotation quality, while reducing the total cost of human annotation. The crux of our approach is to identify preference instances that will benefit from human annotations. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we train a performance prediction model to predict a reward model's performance on an arbitrary combination of human and LM annotations and employ a routing strategy that selects a combination that maximizes predicted performance. We train the performance prediction model on MultiPref, a new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of LM and direct human preferences using our routing framework achieves better reward model performance compared to using either one exclusively. We simulate selective human preference collection on three other datasets and show that our method generalizes well to all three. We analyze features from the routing model to identify characteristics of instances that can benefit from human feedback, e.g., prompts with a moderate safety concern or moderate intent complexity. We release the dataset, annotation platform, and source code used in this study to foster more efficient and accurate preference collection in the future.", 'score': 11, 'issue_id': 301, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'f6e6f7e5b9e467fe', 'data': {'categories': ['#synthetic', '#rlhf', '#benchmark', '#optimization', '#data', '#training', '#dataset', '#open_source', '#alignment'], 'emoji': '🔀', 'ru': {'title': 'Оптимальное сочетание человеческого и машинного интеллекта для обучения ИИ', 'desc': 'Статья представляет новый подход к обучению языковых моделей на основе предпочтений. Авторы предлагают гибридный метод, сочетающий аннотации от людей и языковых моделей для повышения качества обучения. Ключевая идея заключается в использовании модели маршрутизации, которая определяет, какие примеры требуют человеческой оценки. Эксперименты показывают, что такой подход позволяет добиться лучших результатов по сравнению с использованием только человеческих или только машинных аннотаций.'}, 'en': {'title': 'Optimizing Language Model Annotations with Human and Synthetic Feedback', 'desc': 'This paper presents a novel routing framework that enhances the quality of annotations for language models by intelligently combining human feedback and synthetic annotations from other language models. The authors address the challenges of collecting human preferences, which can be costly and inconsistent, by proposing a method that identifies which instances would benefit most from human input. They introduce a performance prediction model trained on a new dataset called MultiPref, which helps optimize the selection of human and LM annotations to maximize overall performance. The results demonstrate that this hybrid approach outperforms using either source of annotations alone, and the authors provide tools and datasets to support future research in this area.'}, 'zh': {'title': '结合人类与模型，提升注释质量！', 'desc': '本研究提出了一种新的路由框架，旨在结合人类和语言模型（LM）的输入，以提高注释质量并降低人类注释的总成本。我们通过优化问题来识别需要人类注释的偏好实例，并训练一个性能预测模型来预测奖励模型在不同人类和LM注释组合下的表现。实验结果表明，使用我们的路由框架选择的混合注释组合在奖励模型性能上优于单独使用人类或LM注释。我们还分析了路由模型的特征，以识别哪些实例更能从人类反馈中受益。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.19730', 'title': 'Counting Ability of Large Language Models and Impact of Tokenization', 'url': 'https://huggingface.co/papers/2410.19730', 'abstract': "Transformers, the backbone of modern large language models (LLMs), face inherent architectural limitations that impede their reasoning capabilities. Unlike recurrent networks, Transformers lack recurrent connections, confining them to constant-depth computation. This restriction places them in the complexity class TC^0, making them theoretically incapable of solving tasks that demand increasingly deep reasoning as input length grows. Counting, a fundamental component of many reasoning tasks, also requires reasoning depth to grow linearly to be performed inductively. While previous studies have established the upper limits of counting ability in Transformer-based expert models (i.e., models specifically trained for counting tasks), these findings do not directly extend to general-purpose LLMs due to differences in reasoning mechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning can help alleviate some of the architectural limitations of Transformers in counting tasks. However, little attention has been paid to the role of tokenization in these models. Unlike expert models that often use character-level tokenization, LLMs typically rely on byte-level (BPE) tokenizers, which fundamentally alters the way reasoning is processed. Our work investigates the impact of tokenization on the counting abilities of LLMs, uncovering substantial performance variations based on input tokenization differences. We provide both theoretical and experimental analyses, offering insights into how tokenization choices can undermine models' theoretical computability, thereby inspiring the design of new tokenization methods to enhance reasoning in LLMs.", 'score': 10, 'issue_id': 308, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '103500d45390097e', 'data': {'categories': ['#transformers', '#reasoning', '#math', '#training', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'Токенизация как ключ к улучшению рассуждений в больших языковых моделях', 'desc': 'Это исследование посвящено влиянию токенизации на способность больших языковых моделей (LLM) к подсчету. Авторы утверждают, что архитектурные ограничения трансформеров затрудняют выполнение задач, требующих глубокого рассуждения. Они анализируют, как выбор метода токенизации может существенно повлиять на производительность моделей в задачах подсчета. Исследование предлагает теоретический и экспериментальный анализ, демонстрируя, как токенизация может подорвать теоретическую вычислимость моделей.'}, 'en': {'title': 'Unlocking Reasoning: The Impact of Tokenization on LLMs', 'desc': 'This paper explores the limitations of Transformers, which are widely used in large language models (LLMs), particularly in their reasoning capabilities. It highlights that Transformers, unlike recurrent networks, cannot perform deep reasoning due to their constant-depth computation structure. The study focuses on how tokenization methods, specifically byte-level tokenization, affect the counting abilities of LLMs, revealing significant performance differences. The authors propose that understanding these tokenization impacts can lead to better design choices that enhance reasoning in LLMs.'}, 'zh': {'title': '优化标记化以提升大型语言模型的推理能力', 'desc': '本文探讨了现代大型语言模型（LLMs）中变换器架构的局限性，特别是在推理能力方面。变换器缺乏递归连接，导致其计算深度受限，无法有效处理需要深度推理的任务。研究表明，计数任务的推理深度需要线性增长，而变换器在这方面存在理论上的不足。我们分析了输入的标记化方式如何影响LLMs的计数能力，并提出新的标记化方法以提升推理性能。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.19290', 'title': 'Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning', 'url': 'https://huggingface.co/papers/2410.19290', 'abstract': "Recent studies have identified one aggravating factor of LLM hallucinations as the knowledge inconsistency between pre-training and fine-tuning, where unfamiliar fine-tuning data mislead the LLM to fabricate plausible but wrong outputs. In this paper, we propose a novel fine-tuning strategy called Prereq-Tune to address this knowledge inconsistency and reduce hallucinations. Fundamentally, Prereq-Tune disentangles the learning of skills and knowledge, so the model learns only the task skills without being impacted by the knowledge inconsistency. To achieve this, Prereq-Tune introduces an additional prerequisite learning stage to learn the necessary knowledge for SFT, allowing subsequent SFT to focus only on task skills. Prereq-Tune can also be combined with fictitious synthetic data to enhance the grounding of LLM outputs to their internal knowledge. Experiments show that Prereq-Tune outperforms existing baselines in improving LLM's factuality across short QA and long-form generation tasks. It also opens new possibilities for knowledge-controlled generation in LLMs. Our code is available at https://github.com/UCSB-NLP-Chang/Prereq_tune.git.", 'score': 10, 'issue_id': 302, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'a11e3c6587db25fc', 'data': {'categories': ['#synthetic', '#hallucinations', '#training', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Prereq-Tune: Уменьшение галлюцинаций в языковых моделях путем разделения обучения навыкам и знаниям', 'desc': 'В статье представлена новая стратегия дообучения языковых моделей под названием Prereq-Tune, направленная на уменьшение галлюцинаций. Метод разделяет обучение навыкам и знаниям, вводя дополнительный этап предварительного обучения необходимым знаниям перед основным дообучением. Prereq-Tune также может использоваться с синтетическими данными для улучшения привязки выходных данных модели к ее внутренним знаниям. Эксперименты показывают, что Prereq-Tune превосходит существующие базовые методы в повышении фактической точности языковых моделей при решении задач вопросно-ответных систем и генерации длинных текстов.'}, 'en': {'title': 'Enhancing LLM Factuality with Prereq-Tune', 'desc': 'This paper addresses the issue of hallucinations in large language models (LLMs) caused by inconsistencies between the knowledge learned during pre-training and fine-tuning. The authors introduce a new fine-tuning method called Prereq-Tune, which separates the learning of task skills from knowledge acquisition. By adding a prerequisite learning stage, the model can first acquire necessary knowledge before focusing on specific task skills during supervised fine-tuning (SFT). The results demonstrate that Prereq-Tune significantly improves the factual accuracy of LLM outputs in various tasks, paving the way for better knowledge-controlled generation.'}, 'zh': {'title': '解决LLM幻觉的知识不一致性', 'desc': '最近的研究发现，LLM（大型语言模型）幻觉的一个加重因素是预训练和微调之间的知识不一致，这导致模型在面对不熟悉的微调数据时产生错误的输出。为了解决这个问题，本文提出了一种新的微调策略，称为Prereq-Tune，旨在减少这种知识不一致性。Prereq-Tune通过引入额外的先决学习阶段，使模型在微调之前先学习必要的知识，从而专注于任务技能的学习。实验表明，Prereq-Tune在提高LLM的事实性方面优于现有的基线方法，并为知识控制生成开辟了新的可能性。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16090', 'title': 'Analysing the Residual Stream of Language Models Under Knowledge Conflicts', 'url': 'https://huggingface.co/papers/2410.16090', 'abstract': 'Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. In this work, we investigate whether LLMs can identify knowledge conflicts and whether it is possible to know which source of knowledge the model will rely on by analysing the residual stream of the LLM. Through probing tasks, we find that LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations. This allows us to detect conflicts within the residual stream before generating the answers without modifying the input or model parameters. Moreover, we find that the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts. This pattern can be employed to estimate the behaviour of LLMs when conflict happens and prevent unexpected answers before producing the answers. Our analysis offers insights into how LLMs internally manage knowledge conflicts and provides a foundation for developing methods to control the knowledge selection processes.', 'score': 6, 'issue_id': 316, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': 'cddfaa4779e86373', 'data': {'categories': ['#reasoning', '#interpretability', '#training', '#architecture', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Распознавание конфликтов знаний в больших языковых моделях', 'desc': 'Это исследование посвящено способности больших языковых моделей (LLM) распознавать конфликты между знаниями, хранящимися в их параметрах, и информацией из контекста. Ученые обнаружили, что LLM могут внутренне регистрировать сигнал о конфликте знаний в остаточном потоке, который можно точно обнаружить путем анализа промежуточных активаций модели. Кроме того, было выявлено, что остаточный поток показывает значительно различные паттерны, когда модель опирается на контекстуальные знания по сравнению с параметрическими знаниями для разрешения конфликтов. Эти результаты предоставляют основу для разработки методов контроля процессов выбора знаний в LLM.'}, 'en': {'title': 'Understanding and Managing Knowledge Conflicts in LLMs', 'desc': "This paper explores how large language models (LLMs) manage conflicting information between their stored knowledge and the context they are given. It reveals that LLMs can detect these knowledge conflicts by analyzing the residual stream, which is the internal signal that reflects the model's processing. By using probing tasks, the authors demonstrate that different patterns in the residual stream indicate whether the model is relying on its internal knowledge or the contextual information. This understanding can help in predicting model behavior during conflicts and in developing strategies to improve knowledge selection in LLMs."}, 'zh': {'title': '识别知识冲突，优化模型行为', 'desc': '大型语言模型（LLMs）能够在其参数中存储大量的事实知识。然而，这些参数知识可能与上下文中提供的信息发生冲突。这种冲突可能导致模型产生不理想的行为，例如依赖过时或不正确的信息。我们的研究表明，LLMs能够识别知识冲突，并通过分析残差流来了解模型将依赖于哪个知识来源，从而在生成答案之前检测到冲突。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18912', 'title': 'Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling', 'url': 'https://huggingface.co/papers/2410.18912', 'abstract': "Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications. In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics. Our project page is available at https://gs-dynamics.github.io.", 'score': 6, 'issue_id': 313, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '2fd905b5809fea40', 'data': {'categories': ['#rl', '#graphs', '#video', '#optimization', '#robotics', '#open_source', '#3d', '#architecture'], 'emoji': '🤖', 'ru': {'title': '3D-динамика объектов из видео для управления роботами', 'desc': 'Эта статья представляет новый подход к обучению динамики объектов непосредственно из многоракурсных RGB-видео, учитывая траектории действий робота. Авторы используют 3D гауссово представление для обучения модели динамики на основе частиц с применением графовых нейронных сетей. Модель обучается на офлайн-данных взаимодействия робота и может предсказывать движения объектов при различных начальных конфигурациях и невиданных ранее действиях робота. Этот метод позволяет осуществлять видеопрогнозирование с учетом действий и может применяться для планирования задач манипуляции объектами.'}, 'en': {'title': 'Predicting Object Dynamics in Robot Interactions Using 3D Video Analysis', 'desc': "This paper presents a new framework for predicting how objects move when robots interact with them, using videos from multiple angles. It focuses on understanding the 3D aspects of these interactions, such as the robot's actions and the 3D states of the objects. The authors employ a particle-based dynamics model trained with Graph Neural Networks, which allows for accurate predictions of object motions based on robot actions. Their approach is tested on various deformable materials, showing its effectiveness in modeling complex dynamics and shapes in real-world scenarios."}, 'zh': {'title': '通过3D动态建模提升机器人交互能力', 'desc': '本研究提出了一种新框架，通过多视角RGB视频直接学习物体的动态，特别关注机器人动作轨迹及其对场景动态的影响。我们利用3D高斯表示法和图神经网络训练基于粒子的动态模型，从稀疏控制粒子中提取信息。该模型能够在不同初始配置和未见过的机器人动作下预测物体运动，并实现基于动作的未来状态渲染。实验表明，我们的方法在建模复杂形状和动态方面表现出色，适用于各种可变形材料的操作任务。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.17655', 'title': 'Mapping the Media Landscape: Predicting Factual Reporting and Political Bias Through Web Interactions', 'url': 'https://huggingface.co/papers/2410.17655', 'abstract': "Bias assessment of news sources is paramount for professionals, organizations, and researchers who rely on truthful evidence for information gathering and reporting. While certain bias indicators are discernible from content analysis, descriptors like political bias and fake news pose greater challenges. In this paper, we propose an extension to a recently presented news media reliability estimation method that focuses on modeling outlets and their longitudinal web interactions. Concretely, we assess the classification performance of four reinforcement learning strategies on a large news media hyperlink graph. Our experiments, targeting two challenging bias descriptors, factual reporting and political bias, showed a significant performance improvement at the source media level. Additionally, we validate our methods on the CLEF 2023 CheckThat! Lab challenge, outperforming the reported results in both, F1-score and the official MAE metric. Furthermore, we contribute by releasing the largest annotated dataset of news source media, categorized with factual reporting and political bias labels. Our findings suggest that profiling news media sources based on their hyperlink interactions over time is feasible, offering a bird's-eye view of evolving media landscapes.", 'score': 5, 'issue_id': 316, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '0fce9f46807b7f5b', 'data': {'categories': ['#rl', '#benchmark', '#graphs', '#ethics', '#dataset', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'Анализ новостных источников с помощью ИИ: новый взгляд на медиаландшафт', 'desc': 'В статье представлен метод оценки надежности новостных источников с использованием обучения с подкреплением. Авторы анализируют гиперссылки между новостными сайтами для определения уровня фактической достоверности и политической предвзятости. Эксперименты показали значительное улучшение классификации по сравнению с существующими методами. Также был создан крупнейший аннотированный датасет новостных источников с метками фактической достоверности и политической предвзятости.'}, 'en': {'title': 'Enhancing News Bias Detection through Web Interaction Analysis', 'desc': 'This paper focuses on assessing bias in news sources, which is crucial for accurate information gathering. It introduces an enhanced method for estimating the reliability of news media by analyzing their web interactions over time. The authors evaluate four reinforcement learning strategies on a large graph of news media hyperlinks, achieving improved classification of factual reporting and political bias. They also provide a new, extensive dataset of news sources labeled by bias, demonstrating that tracking hyperlink interactions can effectively profile media bias.'}, 'zh': {'title': '基于超链接互动的新闻媒体偏见评估', 'desc': '本论文探讨了新闻来源的偏见评估，强调了对真实信息收集和报告的重要性。我们提出了一种扩展的新闻媒体可靠性估计方法，专注于建模媒体及其长期的网络互动。通过对大型新闻媒体超链接图的实验，我们评估了四种强化学习策略的分类性能，特别针对事实报道和政治偏见这两个挑战性偏见指标。我们的研究结果表明，基于超链接互动的新闻媒体源分析是可行的，能够提供对媒体环境演变的全局视角。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.16270', 'title': 'Reflection-Bench: probing AI intelligence with reflection', 'url': 'https://huggingface.co/papers/2410.16270', 'abstract': "The ability to adapt beliefs or behaviors in response to unexpected outcomes, reflection, is fundamental to intelligent systems' interaction with the world. From a cognitive science perspective, this serves as a core principle of intelligence applicable to both human and AI systems. To address the debate on the intelligence of large language models (LLMs), we propose Reflection-Bench, a comprehensive benchmark comprising 7 tasks spanning core cognitive functions crucial for reflection, including perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. We evaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude 3.5 Sonnet, etc. The results indicate that current LLMs still lack satisfactory reflection ability. We discuss the underlying causes of these results and suggest potential avenues for future research. In conclusion, Reflection-Bench offers both evaluation tools and inspiration for developing AI capable of reliably interacting with the environment. Our data and code are available at https://github.com/YabYum/ReflectionBench.", 'score': 5, 'issue_id': 310, 'pub_date': '2024-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '2e170064cac1a857', 'data': {'categories': ['#reasoning', '#rl', '#benchmark', '#cv', '#interpretability', '#open_source', '#architecture'], 'emoji': '🤔', 'ru': {'title': 'Reflection-Bench: измеряя способность ИИ к самоанализу', 'desc': 'Исследователи представили Reflection-Bench - комплексный набор тестов для оценки способности больших языковых моделей (LLM) к рефлексии. Бенчмарк включает 7 заданий, охватывающих ключевые когнитивные функции, такие как восприятие, память, обновление убеждений и метарефлексия. Результаты тестирования 13 ведущих LLM показали, что текущие модели все еще не обладают удовлетворительной способностью к рефлексии. Авторы обсуждают причины полученных результатов и предлагают направления для дальнейших исследований в этой области.'}, 'en': {'title': 'Enhancing AI Reflection: Introducing Reflection-Bench', 'desc': 'This paper introduces Reflection-Bench, a new benchmark designed to evaluate the reflection capabilities of large language models (LLMs). Reflection is a key cognitive function that allows intelligent systems to adapt their beliefs and behaviors based on unexpected outcomes. The benchmark includes seven tasks that assess various cognitive functions such as memory, decision-making, and counterfactual thinking. The evaluation of 13 leading LLMs reveals that they currently struggle with reflection, highlighting areas for improvement and future research in AI development.'}, 'zh': {'title': '反思能力：智能系统的核心', 'desc': '本文提出了一个名为Reflection-Bench的基准测试，旨在评估大型语言模型（LLMs）在反思能力方面的表现。反思能力是智能系统与世界互动的核心原则，涉及感知、记忆、信念更新、决策、预测、反事实思维和元反思等认知功能。通过对13个知名LLM的评估，结果显示当前的LLM在反思能力上仍然不足。我们讨论了这些结果的原因，并提出了未来研究的潜在方向。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.18076', 'title': 'Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration', 'url': 'https://huggingface.co/papers/2410.18076', 'abstract': 'Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled prior trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an optimistic reward model, transforming prior data into high-level, task-relevant examples. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. We empirically show that SUPE reliably outperforms prior strategies, successfully solving a suite of long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe.', 'score': 4, 'issue_id': 300, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '4c364a800e98d62f', 'data': {'categories': ['#rl', '#optimization', '#training', '#transfer_learning', '#open_source', '#games', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Эффективное исследование в RL с помощью непомеченных данных', 'desc': 'Эта статья представляет метод SUPE для эффективного исследования в обучении с подкреплением, используя непомеченные предварительные данные траекторий. SUPE сначала извлекает низкоуровневые навыки с помощью вариационного автоэнкодера (VAE), затем псевдо-размечает непомеченные траектории с использованием оптимистичной модели вознаграждения. Затем метод использует эти преобразованные примеры как дополнительные офф-полиси данные для онлайн-обучения с подкреплением. SUPE превосходит предыдущие стратегии в решении задач с долгосрочным горизонтом и редкими вознаграждениями.'}, 'en': {'title': 'Unlocking Exploration with Unlabeled Data in RL', 'desc': 'This paper introduces a method called SUPE, which stands for Skills from Unlabeled Prior data for Exploration, aimed at improving exploration strategies in reinforcement learning (RL). The approach utilizes unlabeled trajectory data to pretrain low-level skills using a variational autoencoder (VAE) and then pseudo-labels this data with an optimistic reward model. By transforming prior data into high-level, task-relevant examples, SUPE enhances the learning process in online RL by providing additional off-policy data. The results demonstrate that SUPE outperforms existing methods in solving complex tasks with long horizons and sparse rewards.'}, 'zh': {'title': '利用未标记数据提升强化学习探索效率', 'desc': '这篇论文探讨了如何利用未标记的先前轨迹数据来学习有效的探索策略，特别是在强化学习（RL）中。研究者提出了一种名为SUPE的方法，通过变分自编码器（VAE）提取低级技能，并使用乐观奖励模型对未标记的轨迹进行伪标签化，从而将先前数据转化为高层次的任务相关示例。SUPE将这些转化后的示例作为额外的离线数据，用于在线强化学习，以学习一个高层次的策略，从而有效地组合预训练的低级技能进行探索。实验结果表明，SUPE在解决一系列长时间跨度和稀疏奖励的任务中，表现优于之前的策略。'}}, 'pdf_title_img': 'img/title_stub.png', 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.21276', 'title': 'GPT-4o System Card', 'url': 'https://huggingface.co/papers/2410.21276', 'abstract': "GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.", 'score': 79, 'issue_id': 324, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '15d57a2a0852d31e', 'authors': ['OpenAI', ':', 'Aaron Hurst', 'Adam Lerer', 'Adam P. Goucher', 'Adam Perelman', 'Aditya Ramesh', 'Aidan Clark', 'AJ Ostrow', 'Akila Welihinda', 'Alan Hayes', 'Alec Radford', 'Aleksander Mądry', 'Alex Baker-Whitcomb', 'Alex Beutel', 'Alex Borzunov', 'Alex Carney', 'Alex Chow', 'Alex Kirillov', 'Alex Nichol', 'Alex Paino', 'Alex Renzin', 'Alex Tachard Passos', 'Alexander Kirillov', 'Alexi Christakis', 'Alexis Conneau', 'Ali Kamali', 'Allan Jabri', 'Allison Moyer', 'Allison Tam', 'Amadou Crookes', 'Amin Tootoochian', 'Amin Tootoonchian', 'Ananya Kumar', 'Andrea Vallone', 'Andrej Karpathy', 'Andrew Braunstein', 'Andrew Cann', 'Andrew Codispoti', 'Andrew Galu', 'Andrew Kondrich', 'Andrew Tulloch', 'Andrey Mishchenko', 'Angela Baek', 'Angela Jiang', 'Antoine Pelisse', 'Antonia Woodford', 'Anuj Gosalia', 'Arka Dhar', 'Ashley Pantuliano', 'Avi Nayak', 'Avital Oliver', 'Barret Zoph', 'Behrooz Ghorbani', 'Ben Leimberger', 'Ben Rossen', 'Ben Sokolowsky', 'Ben Wang', 'Benjamin Zweig', 'Beth Hoover', 'Blake Samic', 'Bob McGrew', 'Bobby Spero', 'Bogo Giertler', 'Bowen Cheng', 'Brad Lightcap', 'Brandon Walkin', 'Brendan Quinn', 'Brian Guarraci', 'Brian Hsu', 'Bright Kellogg', 'Brydon Eastman', 'Camillo Lugaresi', 'Carroll Wainwright', 'Cary Bassin', 'Cary Hudson', 'Casey Chu', 'Chad Nelson', 'Chak Li', 'Chan Jun Shern', 'Channing Conger', 'Charlotte Barette', 'Chelsea Voss', 'Chen Ding', 'Cheng Lu', 'Chong Zhang', 'Chris Beaumont', 'Chris Hallacy', 'Chris Koch', 'Christian Gibson', 'Christina Kim', 'Christine Choi', 'Christine McLeavey', 'Christopher Hesse', 'Claudia Fischer', 'Clemens Winter', 'Coley Czarnecki', 'Colin Jarvis', 'Colin Wei', 'Constantin Koumouzelis', 'Dane Sherburn', 'Daniel Kappler', 'Daniel Levin', 'Daniel Levy', 'David Carr', 'David Farhi', 'David Mely', 'David Robinson', 'David Sasaki', 'Denny Jin', 'Dev Valladares', 'Dimitris Tsipras', 'Doug Li', 'Duc Phong Nguyen', 'Duncan Findlay', 'Edede Oiwoh', 'Edmund Wong', 'Ehsan Asdar', 'Elizabeth Proehl', 'Elizabeth Yang', 'Eric Antonow', 'Eric Kramer', 'Eric Peterson', 'Eric Sigler', 'Eric Wallace', 'Eugene Brevdo', 'Evan Mays', 'Farzad Khorasani', 'Felipe Petroski Such', 'Filippo Raso', 'Francis Zhang', 'Fred von Lohmann', 'Freddie Sulit', 'Gabriel Goh', 'Gene Oden', 'Geoff Salmon', 'Giulio Starace', 'Greg Brockman', 'Hadi Salman', 'Haiming Bao', 'Haitang Hu', 'Hannah Wong', 'Haoyu Wang', 'Heather Schmidt', 'Heather Whitney', 'Heewoo Jun', 'Hendrik Kirchner', 'Henrique Ponde de Oliveira Pinto', 'Hongyu Ren', 'Huiwen Chang', 'Hyung Won Chung', 'Ian Kivlichan', "Ian O'Connell", "Ian O'Connell", 'Ian Osband', 'Ian Silber', 'Ian Sohl', 'Ibrahim Okuyucu', 'Ikai Lan', 'Ilya Kostrikov', 'Ilya Sutskever', 'Ingmar Kanitscheider', 'Ishaan Gulrajani', 'Jacob Coxon', 'Jacob Menick', 'Jakub Pachocki', 'James Aung', 'James Betker', 'James Crooks', 'James Lennon', 'Jamie Kiros', 'Jan Leike', 'Jane Park', 'Jason Kwon', 'Jason Phang', 'Jason Teplitz', 'Jason Wei', 'Jason Wolfe', 'Jay Chen', 'Jeff Harris', 'Jenia Varavva', 'Jessica Gan Lee', 'Jessica Shieh', 'Ji Lin', 'Jiahui Yu', 'Jiayi Weng', 'Jie Tang', 'Jieqi Yu', 'Joanne Jang', 'Joaquin Quinonero Candela', 'Joe Beutler', 'Joe Landers', 'Joel Parish', 'Johannes Heidecke', 'John Schulman', 'Jonathan Lachman', 'Jonathan McKay', 'Jonathan Uesato', 'Jonathan Ward', 'Jong Wook Kim', 'Joost Huizinga', 'Jordan Sitkin', 'Jos Kraaijeveld', 'Josh Gross', 'Josh Kaplan', 'Josh Snyder', 'Joshua Achiam', 'Joy Jiao', 'Joyce Lee', 'Juntang Zhuang', 'Justyn Harriman', 'Kai Fricke', 'Kai Hayashi', 'Karan Singhal', 'Katy Shi', 'Kavin Karthik', 'Kayla Wood', 'Kendra Rimbach', 'Kenny Hsu', 'Kenny Nguyen', 'Keren Gu-Lemberg', 'Kevin Button', 'Kevin Liu', 'Kiel Howe', 'Krithika Muthukumar', 'Kyle Luther', 'Lama Ahmad', 'Larry Kai', 'Lauren Itow', 'Lauren Workman', 'Leher Pathak', 'Leo Chen', 'Li Jing', 'Lia Guy', 'Liam Fedus', 'Liang Zhou', 'Lien Mamitsuka', 'Lilian Weng', 'Lindsay McCallum', 'Lindsey Held', 'Long Ouyang', 'Louis Feuvrier', 'Lu Zhang', 'Lukas Kondraciuk', 'Lukasz Kaiser', 'Luke Hewitt', 'Luke Metz', 'Lyric Doshi', 'Mada Aflak', 'Maddie Simens', 'Madelaine Boyd', 'Madeleine Thompson', 'Marat Dukhan', 'Mark Chen', 'Mark Gray', 'Mark Hudnall', 'Marvin Zhang', 'Marwan Aljubeh', 'Mateusz Litwin', 'Matthew Zeng', 'Max Johnson', 'Maya Shetty', 'Mayank Gupta', 'Meghan Shah', 'Mehmet Yatbaz', 'Meng Jia Yang', 'Mengchao Zhong', 'Mia Glaese', 'Mianna Chen', 'Michael Janner', 'Michael Lampe', 'Michael Petrov', 'Michael Wu', 'Michele Wang', 'Michelle Fradin', 'Michelle Pokrass', 'Miguel Castro', 'Miguel Oom Temudo de Castro', 'Mikhail Pavlov', 'Miles Brundage', 'Miles Wang', 'Minal Khan', 'Mira Murati', 'Mo Bavarian', 'Molly Lin', 'Murat Yesildal', 'Nacho Soto', 'Natalia Gimelshein', 'Natalie Cone', 'Natalie Staudacher', 'Natalie Summers', 'Natan LaFontaine', 'Neil Chowdhury', 'Nick Ryder', 'Nick Stathas', 'Nick Turley', 'Nik Tezak', 'Niko Felix', 'Nithanth Kudige', 'Nitish Keskar', 'Noah Deutsch', 'Noel Bundick', 'Nora Puckett', 'Ofir Nachum', 'Ola Okelola', 'Oleg Boiko', 'Oleg Murk', 'Oliver Jaffe', 'Olivia Watkins', 'Olivier Godement', 'Owen Campbell-Moore', 'Patrick Chao', 'Paul McMillan', 'Pavel Belov', 'Peng Su', 'Peter Bak', 'Peter Bakkum', 'Peter Deng', 'Peter Dolan', 'Peter Hoeschele', 'Peter Welinder', 'Phil Tillet', 'Philip Pronin', 'Philippe Tillet', 'Prafulla Dhariwal', 'Qiming Yuan', 'Rachel Dias', 'Rachel Lim', 'Rahul Arora', 'Rajan Troll', 'Randall Lin', 'Rapha Gontijo Lopes', 'Raul Puri', 'Reah Miyara', 'Reimar Leike', 'Renaud Gaubert', 'Reza Zamani', 'Ricky Wang', 'Rob Donnelly', 'Rob Honsby', 'Rocky Smith', 'Rohan Sahai', 'Rohit Ramchandani', 'Romain Huet', 'Rory Carmichael', 'Rowan Zellers', 'Roy Chen', 'Ruby Chen', 'Ruslan Nigmatullin', 'Ryan Cheu', 'Saachi Jain', 'Sam Altman', 'Sam Schoenholz', 'Sam Toizer', 'Samuel Miserendino', 'Sandhini Agarwal', 'Sara Culver', 'Scott Ethersmith', 'Scott Gray', 'Sean Grove', 'Sean Metzger', 'Shamez Hermani', 'Shantanu Jain', 'Shengjia Zhao', 'Sherwin Wu', 'Shino Jomoto', 'Shirong Wu', 'Shuaiqi', 'Xia', 'Sonia Phene', 'Spencer Papay', 'Srinivas Narayanan', 'Steve Coffey', 'Steve Lee', 'Stewart Hall', 'Suchir Balaji', 'Tal Broda', 'Tal Stramer', 'Tao Xu', 'Tarun Gogineni', 'Taya Christianson', 'Ted Sanders', 'Tejal Patwardhan', 'Thomas Cunninghman', 'Thomas Degry', 'Thomas Dimson', 'Thomas Raoux', 'Thomas Shadwell', 'Tianhao Zheng', 'Todd Underwood', 'Todor Markov', 'Toki Sherbakov', 'Tom Rubin', 'Tom Stasi', 'Tomer Kaftan', 'Tristan Heywood', 'Troy Peterson', 'Tyce Walters', 'Tyna Eloundou', 'Valerie Qi', 'Veit Moeller', 'Vinnie Monaco', 'Vishal Kuo', 'Vlad Fomenko', 'Wayne Chang', 'Weiyi Zheng', 'Wenda Zhou', 'Wesam Manassra', 'Will Sheu', 'Wojciech Zaremba', 'Yash Patil', 'Yilei Qian', 'Yongjik Kim', 'Youlong Cheng', 'Yu Zhang', 'Yuchen He', 'Yuchen Zhang', 'Yujia Jin', 'Yunxing Dai', 'Yury Malkov'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21276.jpg', 'data': {'categories': ['#synthetic', '#multilingual', '#cv', '#video', '#multimodal', '#ethics', '#training', '#open_source', '#audio', '#security', '#architecture', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'GPT-4o: Революция в мультимодальном ИИ', 'desc': 'GPT-4o - это мультимодальная модель, способная обрабатывать и генерировать текст, аудио, изображения и видео. Она обучена сквозным образом на различных типах данных, что позволяет единой нейронной сети обрабатывать все виды входных и выходных данных. Модель демонстрирует высокую скорость отклика на аудиовходы, сравнимую с человеческой, и превосходит существующие модели в понимании визуальной и аудиоинформации. GPT-4o также показывает улучшенную производительность для неанглийских языков при сохранении высокого качества работы с английским текстом и кодом.'}, 'en': {'title': 'GPT-4o: The All-in-One AI for Text, Audio, and Vision', 'desc': 'GPT-4o is a versatile autoregressive model that can process and generate various types of media, including text, audio, images, and video. It utilizes an end-to-end training approach, allowing it to handle all input and output modalities through a single neural network. The model demonstrates rapid response times comparable to human conversation and shows enhanced performance in understanding non-English text, audio, and visual data. Additionally, GPT-4o prioritizes safety and alignment, providing a comprehensive System Card that outlines its capabilities, limitations, and societal implications.'}, 'zh': {'title': '全能模型，快速响应，安全可靠', 'desc': 'GPT-4o是一种自回归的全能模型，能够处理文本、音频、图像和视频的任意组合，并生成相应的输出。它通过一个统一的神经网络进行端到端训练，确保所有输入和输出的高效处理。GPT-4o在响应音频输入时的速度与人类对话相似，且在非英语文本处理上有显著提升，同时在视觉和音频理解方面表现优于现有模型。该模型的系统卡片详细介绍了其能力、局限性和安全评估，确保其在社会影响和潜在危险能力方面的透明性。'}}}, {'id': 'https://huggingface.co/papers/2410.18565', 'title': 'Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation', 'url': 'https://huggingface.co/papers/2410.18565', 'abstract': 'We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for Polish language processing. Trained on curated Polish corpora, this model addresses key challenges in language model development through innovative techniques. These include Weighted Instruction Cross-Entropy Loss, which balances the learning of different instruction types, and Adaptive Learning Rate, which dynamically adjusts the learning rate based on training progress. To evaluate performance, we created the Open PL LLM Leaderboard and Polish MT-Bench, novel frameworks assessing various NLP tasks and conversational abilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9 percentage point increase in average score compared to Mistral-7B-v0.1 on the RAG Reader task. It also excels in the Polish MT-Bench, particularly in Reasoning (6.15/10) and Role-playing (7.83/10) categories. This model represents a substantial advancement in Polish language AI, offering a powerful tool for diverse linguistic applications and setting new benchmarks in the field.', 'score': 42, 'issue_id': 325, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'c68d615cedcd651e', 'authors': ['Krzysztof Ociepa', 'Łukasz Flis', 'Krzysztof Wróbel', 'Adrian Gwoździej', 'Remigiusz Kinas'], 'affiliations': ['ACK Cyfronet AGH', 'Azurro', 'Enelpol', 'Jagiellonian University', 'SpeakLeash'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18565.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#benchmark', '#multilingual', '#optimization', '#training', '#dataset', '#transfer_learning', '#open_source', '#machine_translation'], 'emoji': '🇵🇱', 'ru': {'title': 'Прорыв в обработке польского языка: мощная 7B-модель Bielik', 'desc': 'Представлен Bielik 7B v0.1 - языковая модель для польского языка с 7 миллиардами параметров. Модель использует инновационные техники обучения, включая взвешенную кросс-энтропийную функцию потерь для инструкций и адаптивную скорость обучения. Для оценки созданы Open PL LLM Leaderboard и Polish MT-Bench, тестирующие различные задачи обработки естественного языка. Bielik 7B v0.1 демонстрирует значительное улучшение производительности по сравнению с предыдущими моделями, особенно в задачах рассуждения и ролевых игр.'}, 'en': {'title': 'Bielik 7B: Advancing Polish Language AI with 7 Billion Parameters', 'desc': "Bielik 7B v0.1 is a generative text model designed specifically for processing the Polish language, featuring 7 billion parameters. It employs advanced techniques like Weighted Instruction Cross-Entropy Loss to enhance learning across various instruction types and an Adaptive Learning Rate that adjusts based on the model's training progress. The model's performance is evaluated using new frameworks such as the Open PL LLM Leaderboard and Polish MT-Bench, which assess its capabilities in natural language processing tasks. Bielik 7B v0.1 shows significant improvements over previous models, particularly in reasoning and role-playing tasks, marking a major step forward in Polish language AI applications."}, 'zh': {'title': 'Bielik 7B v0.1：波兰语处理的新标杆', 'desc': 'Bielik 7B v0.1 是一个用于波兰语处理的生成文本模型，拥有70亿个参数。该模型通过创新技术解决了语言模型开发中的关键挑战，包括加权指令交叉熵损失和自适应学习率。我们还创建了开放的PL LLM排行榜和波兰MT-Bench，以评估模型在各种自然语言处理任务和对话能力上的表现。Bielik 7B v0.1 在RAG Reader任务上比Mistral-7B-v0.1提高了9个百分点，特别在推理和角色扮演类别中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2410.20011', 'title': 'A Survey of Small Language Models', 'url': 'https://huggingface.co/papers/2410.20011', 'abstract': 'Small Language Models (SLMs) have become increasingly important due to their efficiency and performance to perform various language tasks with minimal computational resources, making them ideal for various settings including on-device, mobile, edge devices, among many others. In this article, we present a comprehensive survey on SLMs, focusing on their architectures, training techniques, and model compression techniques. We propose a novel taxonomy for categorizing the methods used to optimize SLMs, including model compression, pruning, and quantization techniques. We summarize the benchmark datasets that are useful for benchmarking SLMs along with the evaluation metrics commonly used. Additionally, we highlight key open challenges that remain to be addressed. Our survey aims to serve as a valuable resource for researchers and practitioners interested in developing and deploying small yet efficient language models.', 'score': 38, 'issue_id': 321, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'bde2fa0e4317a316', 'authors': ['Chien Van Nguyen', 'Xuan Shen', 'Ryan Aponte', 'Yu Xia', 'Samyadeep Basu', 'Zhengmian Hu', 'Jian Chen', 'Mihir Parmar', 'Sasidhar Kunapuli', 'Joe Barrow', 'Junda Wu', 'Ashish Singh', 'Yu Wang', 'Jiuxiang Gu', 'Franck Dernoncourt', 'Nesreen K. Ahmed', 'Nedim Lipka', 'Ruiyi Zhang', 'Xiang Chen', 'Tong Yu', 'Sungchul Kim', 'Hanieh Deilamsalehy', 'Namyong Park', 'Mike Rimer', 'Zhehao Zhang', 'Huanrui Yang', 'Ryan A. Rossi', 'Thien Huu Nguyen'], 'affiliations': ['Adobe Research', 'Arizona State University', 'Carnegie Mellon University', 'Dartmouth College', 'Intel AI Research', 'Meta AI', 'Northeastern University', 'State University of New York at Buffalo', 'University of Arizona', 'University of California, San Diego', 'University of Maryland, College Park', 'University of Massachusetts Amherst', 'University of Oregon'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20011.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#inference', '#optimization', '#training', '#survey', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Малые языковые модели: эффективность в компактности', 'desc': 'Эта статья представляет собой всесторонний обзор малых языковых моделей (SLM), которые становятся все более важными из-за их эффективности и производительности при минимальных вычислительных ресурсах. Авторы предлагают новую таксономию для категоризации методов оптимизации SLM, включая сжатие модели, прунинг и квантизацию. В работе обобщаются наборы данных для бенчмаркинга SLM и метрики оценки. Статья также освещает ключевые нерешенные проблемы в области малых языковых моделей.'}, 'en': {'title': 'Optimizing Small Language Models for Efficiency and Performance', 'desc': 'This paper provides a detailed overview of Small Language Models (SLMs), which are designed to perform language tasks efficiently with low computational requirements. It introduces a new classification system for the various optimization methods used in SLMs, such as model compression, pruning, and quantization. The authors also compile important benchmark datasets and evaluation metrics that are essential for assessing the performance of SLMs. Furthermore, the paper discusses ongoing challenges in the field, aiming to assist researchers and practitioners in advancing the development of efficient language models.'}, 'zh': {'title': '小型语言模型：高效语言处理的未来', 'desc': '小型语言模型（SLMs）因其高效性和性能而变得越来越重要，能够在资源有限的情况下执行各种语言任务，非常适合在设备、移动和边缘设备等环境中使用。本文对SLMs进行了全面的调查，重点介绍了它们的架构、训练技术和模型压缩技术。我们提出了一种新的分类法，用于对优化SLMs的方法进行分类，包括模型压缩、剪枝和量化技术。我们总结了对SLMs进行基准测试的有用数据集以及常用的评估指标，并强调了仍需解决的关键开放挑战。'}}}, {'id': 'https://huggingface.co/papers/2410.18603', 'title': 'AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant', 'url': 'https://huggingface.co/papers/2410.18603', 'abstract': "Digital agents capable of automating complex computer tasks have attracted considerable attention due to their immense potential to enhance human-computer interaction. However, existing agent methods exhibit deficiencies in their generalization and specialization capabilities, especially in handling open-ended computer tasks in real-world environments. Inspired by the rich functionality of the App store, we present AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents for automating computer tasks. AgentStore empowers users to integrate third-party agents, allowing the system to continuously enrich its capabilities and adapt to rapidly evolving operating systems. Additionally, we propose a novel core MetaAgent with the AgentToken strategy to efficiently manage diverse agents and utilize their specialized and generalist abilities for both domain-specific and system-wide tasks. Extensive experiments on three challenging benchmarks demonstrate that AgentStore surpasses the limitations of previous systems with narrow capabilities, particularly achieving a significant improvement from 11.21\\% to 23.85\\% on the OSWorld benchmark, more than doubling the previous results. Comprehensive quantitative and qualitative results further demonstrate AgentStore's ability to enhance agent systems in both generalization and specialization, underscoring its potential for developing the specialized generalist computer assistant. All our codes will be made publicly available in https://chengyou-jia.github.io/AgentStore-Home.", 'score': 30, 'issue_id': 324, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '9d62252fde1756e0', 'authors': ['Chengyou Jia', 'Minnan Luo', 'Zhuohang Dang', 'Qiushi Sun', 'Fangzhi Xu', 'Junlin Hu', 'Tianbao Xie', 'Zhiyong Wu'], 'affiliations': ['Shanghai AI Lab', 'The University of Hong Kong', 'Xian Jiaotong University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18603.jpg', 'data': {'categories': ['#benchmark', '#agi', '#open_source', '#agents', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'AgentStore: Универсальный помощник для автоматизации компьютерных задач', 'desc': 'AgentStore - это новая масштабируемая платформа для автоматизации компьютерных задач, вдохновленная функциональностью магазина приложений. Она позволяет интегрировать сторонних агентов, постоянно расширяя возможности системы и адаптируясь к быстро меняющимся операционным системам. В основе платформы лежит MetaAgent с AgentToken стратегией для эффективного управления разнообразными агентами. Эксперименты показали значительное улучшение результатов по сравнению с предыдущими системами, особенно на бенчмарке OSWorld.'}, 'en': {'title': 'AgentStore: Empowering Dynamic Integration of Digital Agents for Enhanced Automation', 'desc': 'This paper introduces AgentStore, a platform that allows for the integration of various digital agents to automate complex computer tasks. It addresses the limitations of existing agent systems in generalization and specialization, particularly in real-world applications. The core innovation is the MetaAgent with the AgentToken strategy, which efficiently manages diverse agents to leverage their strengths for both specific and broad tasks. Experimental results show that AgentStore significantly improves performance on benchmarks, demonstrating its effectiveness in enhancing human-computer interaction.'}, 'zh': {'title': 'AgentStore：提升计算机任务自动化的智能代理平台', 'desc': '本文介绍了一种名为AgentStore的平台，旨在通过动态集成异构代理来自动化复杂的计算机任务。现有的代理方法在处理开放式计算机任务时存在泛化和专业化能力不足的问题。AgentStore允许用户集成第三方代理，从而不断丰富系统的功能，适应快速变化的操作系统。通过在多个基准测试上的实验，AgentStore在泛化和专业化方面表现出色，显著提高了系统的能力。'}}}, {'id': 'https://huggingface.co/papers/2410.21169', 'title': 'Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction', 'url': 'https://huggingface.co/papers/2410.21169', 'abstract': 'Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices-into structured, machine-readable data. Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation. This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It emphasizes the importance of developing larger and more diverse datasets and outlines future research directions.', 'score': 29, 'issue_id': 325, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'b5541dbb322ac8c4', 'authors': ['Qintong Zhang', 'Victor Shea-Jay Huang', 'Bin Wang', 'Junyuan Zhang', 'Zhengren Wang', 'Hao Liang', 'Shawn Wang', 'Matthieu Lin', 'Conghui He', 'Wentao Zhang'], 'affiliations': ['Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21169.jpg', 'data': {'categories': ['#science', '#cv', '#multimodal', '#data', '#dataset', '#survey', '#architecture'], 'emoji': '📄', 'ru': {'title': 'Парсинг документов: от модульных систем к мультимодальным моделям', 'desc': 'Эта статья представляет собой обзор современного состояния парсинга документов. Рассматриваются ключевые методологии, от модульных систем до end-to-end моделей, основанных на крупных мультимодальных языковых моделях. Обсуждаются основные компоненты, такие как обнаружение макета, извлечение контента и интеграция мультимодальных данных. Статья также анализирует проблемы, с которыми сталкиваются системы парсинга документов, и намечает направления будущих исследований.'}, 'en': {'title': 'Transforming Documents: From Chaos to Clarity with Parsing', 'desc': 'This paper reviews the process of document parsing, which transforms unstructured documents into structured data that machines can understand. It highlights the advancements made possible by Large Language Models, which enhance the efficiency of knowledge base construction and training data generation. The survey covers various methodologies, including modular pipeline systems and end-to-end models, focusing on essential tasks like layout detection and content extraction. It also addresses the challenges in parsing complex layouts and the need for larger datasets to improve model performance.'}, 'zh': {'title': '文档解析：从非结构到结构的关键技术', 'desc': '文档解析是将非结构化和半结构化文档（如合同、学术论文和发票）转换为结构化、机器可读数据的重要过程。通过文档解析，可以从非结构化输入中提取可靠的结构化数据，为许多应用提供极大的便利。尤其是在大型语言模型取得的最新进展下，文档解析在知识库构建和训练数据生成中发挥着不可或缺的作用。本文综述了文档解析的现状，涵盖了从模块化管道系统到由大型视觉-语言模型驱动的端到端模型的关键方法。'}}}, {'id': 'https://huggingface.co/papers/2410.20280', 'title': 'MarDini: Masked Autoregressive Diffusion for Video Generation at Scale', 'url': 'https://huggingface.co/papers/2410.20280', 'abstract': "We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planning model containing most of the parameters generates planning signals for each masked frame using low-resolution input; ii) a lightweight generation model uses these signals to produce high-resolution frames via diffusion de-noising. MarDini's MAR enables video generation conditioned on any number of masked frames at any frame positions: a single model can handle video interpolation (e.g., masking middle frames), image-to-video generation (e.g., masking from the second frame onward), and video expansion (e.g., masking half the frames). The efficient design allocates most of the computational resources to the low-resolution planning model, making computationally expensive but important spatio-temporal attention feasible at scale. MarDini sets a new state-of-the-art for video interpolation; meanwhile, within few inference steps, it efficiently generates videos on par with those of much more expensive advanced image-to-video models.", 'score': 21, 'issue_id': 322, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '33b4a79a3995aa46', 'authors': ['Haozhe Liu', 'Shikun Liu', 'Zijian Zhou', 'Mengmeng Xu', 'Yanping Xie', 'Xiao Han', 'Juan C. Pérez', 'Ding Liu', 'Kumara Kahatapitiya', 'Menglin Jia', 'Jui-Chieh Wu', 'Sen He', 'Tao Xiang', 'Jürgen Schmidhuber', 'Juan-Manuel Pérez-Rúa'], 'affiliations': ['KAUST', 'Meta AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20280.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#inference', '#video', '#optimization', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'MarDini: Гибкая генерация видео с маскированной авторегрессией и диффузией', 'desc': 'MarDini - это новое семейство моделей диффузии видео, объединяющее преимущества маскированной авторегрессии (MAR) и диффузионных моделей (DM). MAR используется для временного планирования, а DM - для пространственной генерации в асимметричной архитектуре сети. Модель планирования на основе MAR генерирует сигналы для каждого маскированного кадра, используя входные данные низкого разрешения. Легковесная модель генерации использует эти сигналы для создания кадров высокого разрешения посредством диффузионного шумоподавления.'}, 'en': {'title': 'MarDini: Revolutionizing Video Generation with Efficient Diffusion Models', 'desc': 'MarDini is a novel family of video diffusion models that combines masked auto-regression (MAR) with a unified diffusion model (DM) framework. The MAR component is responsible for planning the sequence of frames, while the DM focuses on generating high-quality spatial outputs. This model can handle various tasks such as video interpolation, image-to-video generation, and video expansion by conditioning on different masked frames. By optimizing the computational resources, MarDini achieves state-of-the-art performance in video interpolation and efficiently generates videos comparable to more complex models.'}, 'zh': {'title': 'MarDini：视频生成的新突破', 'desc': 'MarDini是一种新的视频扩散模型家族，它将掩蔽自回归（MAR）的优势与统一的扩散模型（DM）框架结合在一起。MAR负责时间规划，而DM则专注于空间生成，采用不对称网络设计。该模型能够根据任意数量的掩蔽帧生成视频，支持视频插值、图像到视频生成和视频扩展等多种任务。MarDini在视频插值方面设定了新的最先进水平，并且在少量推理步骤内高效生成与更昂贵的图像到视频模型相当的视频。'}}}, {'id': 'https://huggingface.co/papers/2410.19313', 'title': 'COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training', 'url': 'https://huggingface.co/papers/2410.19313', 'abstract': "FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54x compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43x end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT.", 'score': 18, 'issue_id': 323, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '9c2301bebb36909b', 'authors': ['Haocheng Xi', 'Han Cai', 'Ligeng Zhu', 'Yao Lu', 'Kurt Keutzer', 'Jianfei Chen', 'Song Han'], 'affiliations': ['MIT', 'NVIDIA', 'Tsinghua University', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.19313.jpg', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#open_source'], 'emoji': '🚀', 'ru': {'title': 'COAT: Эффективное обучение больших нейросетей с минимальными затратами памяти', 'desc': 'COAT - это новая система обучения нейросетей в формате FP8, которая значительно сокращает использование памяти при обучении больших моделей. Она вводит две ключевые инновации: динамическое расширение диапазона для оптимизации состояний оптимизатора и смешанную грануляцию квантования активаций. Эксперименты показывают, что COAT уменьшает общий объем используемой памяти в 1,54 раза по сравнению с BF16, сохраняя при этом производительность на различных задачах. Система также достигает ускорения обучения в 1,43 раза по сравнению с BF16, что делает ее практичным решением для масштабирования обучения крупных моделей.'}, 'en': {'title': 'COAT: Revolutionizing FP8 Training for Large Models', 'desc': 'This paper presents COAT, a new framework for FP8 training that enhances memory efficiency during the training of large machine learning models. COAT introduces two main innovations: Dynamic Range Expansion to minimize quantization errors in optimizer states, and Mixed-Granularity Activation Quantization to optimize memory usage for activations. The framework achieves a 1.54x reduction in memory footprint and a 1.43x speedup in training compared to traditional BF16 methods, while maintaining high performance across various tasks. By enabling efficient training on fewer GPUs and allowing larger batch sizes, COAT provides a scalable solution for large-scale model training.'}, 'zh': {'title': 'COAT：高效的FP8训练内存优化方案', 'desc': 'FP8训练是一种提高训练效率的新方法。现有框架在使用FP8计算时，优化器状态和激活仍保持高精度，未能充分优化内存使用。本文提出了COAT框架，通过动态范围扩展和混合粒度激活量化，显著减少大模型训练的内存占用。实验表明，COAT在多种任务中实现了接近无损的性能，同时内存占用减少了1.54倍，训练速度提升了1.43倍。'}}}, {'id': 'https://huggingface.co/papers/2410.18666', 'title': 'DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation', 'url': 'https://huggingface.co/papers/2410.18666', 'abstract': "Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets. To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model. GenIR, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation & filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images. Our second contribution, DreamClear, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address. Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models will be available at: https://github.com/shallowdream204/DreamClear.", 'score': 18, 'issue_id': 322, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'ebdde1eca3a0b04c', 'authors': ['Yuang Ai', 'Xiaoqiang Zhou', 'Huaibo Huang', 'Xiaotian Han', 'Zhengyu Chen', 'Quanzeng You', 'Hongxia Yang'], 'affiliations': ['ByteDance, Inc', 'MAIS & NLPR, Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18666.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#multimodal', '#data', '#dataset', '#open_source', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Революция в восстановлении изображений: от генерации данных до фотореалистичных результатов', 'desc': 'Статья представляет новый подход к восстановлению изображений в реальных условиях. Авторы предлагают GenIR - инновационный конвейер для создания датасетов, и DreamClear - модель восстановления изображений на основе Diffusion Transformer. GenIR использует двойное обучение с подсказками для создания масштабного датасета из миллиона высококачественных изображений. DreamClear применяет генеративные приоры диффузионных моделей и перцептивные способности мультимодальных языковых моделей для фотореалистичного восстановления.'}, 'en': {'title': 'Revolutionizing Image Restoration with GenIR and DreamClear', 'desc': 'This paper addresses the challenges of image restoration (IR) in real-world applications by introducing two key innovations: GenIR and DreamClear. GenIR is a data curation pipeline that creates a large-scale dataset of one million high-quality images through a dual-prompt learning approach, enhancing the generalizability of models. DreamClear is a Diffusion Transformer-based model that leverages generative priors from text-to-image diffusion models and integrates adaptive modulation to handle various image degradations effectively. The results demonstrate that this dual strategy significantly improves the performance of image restoration tasks in practical scenarios.'}, 'zh': {'title': '双重策略提升图像恢复能力', 'desc': '本文提出了一种针对现实场景中图像恢复（IR）问题的双重策略，包括GenIR数据策划管道和基于扩散变换器（DiT）的DreamClear图像恢复模型。GenIR通过构建图像-文本对、双提示微调和数据生成与过滤，克服了现有数据集的局限性，提供了一个包含一百万高质量图像的大规模数据集。DreamClear模型利用文本到图像扩散模型的生成先验和多模态大语言模型的感知能力，实现了逼真的图像恢复。通过引入自适应调制混合器（MoAM），该模型能够动态整合多种恢复专家，增强了对各种现实世界退化的适应能力。'}}}, {'id': 'https://huggingface.co/papers/2410.21252', 'title': 'LongReward: Improving Long-context Large Language Models with AI Feedback', 'url': 'https://huggingface.co/papers/2410.21252', 'abstract': "Though significant advancements have been achieved in developing long-context large language models (LLMs), the compromised quality of LLM-synthesized data for supervised fine-tuning (SFT) often affects the long-context performance of SFT models and leads to inherent limitations. In principle, reinforcement learning (RL) with appropriate reward signals can further enhance models' capacities. However, how to obtain reliable rewards in long-context scenarios remains unexplored. To this end, we propose LongReward, a novel method that utilizes an off-the-shelf LLM to provide rewards for long-context model responses from four human-valued dimensions: helpfulness, logicality, faithfulness, and completeness, each with a carefully designed assessment pipeline. By combining LongReward and offline RL algorithm DPO, we are able to effectively improve long-context SFT models. Our experiments indicate that LongReward not only significantly improves models' long-context performance but also enhances their ability to follow short instructions. We also find that long-context DPO with LongReward and conventional short-context DPO can be used together without hurting either one's performance.", 'score': 16, 'issue_id': 321, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '0ff5d39896cdfbbe', 'authors': ['Jiajie Zhang', 'Zhongni Hou', 'Xin Lv', 'Shulin Cao', 'Zhenyu Hou', 'Yilin Niu', 'Lei Hou', 'Yuxiao Dong', 'Ling Feng', 'Juanzi Li'], 'affiliations': ['Tsinghua University', 'University of Chinese Academy of Sciences', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21252.jpg', 'data': {'categories': ['#rl', '#rlhf', '#optimization', '#training', '#long_context', '#alignment'], 'emoji': '📏', 'ru': {'title': 'LongReward: Улучшение языковых моделей для работы с длинным контекстом', 'desc': 'Статья представляет новый метод LongReward для улучшения работы языковых моделей с длинным контекстом. Метод использует готовую большую языковую модель для оценки ответов по четырем параметрам: полезность, логичность, точность и полнота. LongReward применяется вместе с алгоритмом обучения с подкреплением DPO для улучшения моделей, обученных на длинных текстах. Эксперименты показывают, что метод значительно улучшает работу с длинным контекстом и короткими инструкциями.'}, 'en': {'title': 'Enhancing Long-Context Performance with LongReward', 'desc': 'This paper addresses the challenges faced by long-context large language models (LLMs) in generating high-quality data for supervised fine-tuning (SFT). It introduces LongReward, a method that leverages an existing LLM to provide rewards based on four key dimensions: helpfulness, logicality, faithfulness, and completeness. By integrating LongReward with the offline reinforcement learning algorithm DPO, the authors demonstrate significant improvements in the long-context performance of SFT models. The findings suggest that LongReward enhances both long-context and short instruction-following capabilities without compromising performance across different contexts.'}, 'zh': {'title': '提升长上下文模型性能的新方法', 'desc': '本文提出了一种名为LongReward的新方法，旨在提高长上下文大语言模型（LLM）的性能。通过利用现成的LLM，从四个维度（有用性、逻辑性、可信性和完整性）为长上下文模型的响应提供奖励信号。结合LongReward和离线强化学习算法DPO，我们能够有效提升长上下文的监督微调模型的表现。实验结果表明，LongReward不仅显著改善了模型的长上下文性能，还增强了其执行短指令的能力。'}}}, {'id': 'https://huggingface.co/papers/2410.20474', 'title': 'GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation', 'url': 'https://huggingface.co/papers/2410.20474', 'abstract': 'We introduce a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes. In this work, we leverage the flexibility of the Transformer architecture, demonstrating that DiT can generate noisy patches corresponding to each bounding box, fully encoding the target object and allowing for fine-grained control over each region. Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing. Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become "semantic clones". Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box. In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free spatial grounding approaches.', 'score': 13, 'issue_id': 323, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': '354e97e3a32e2063', 'authors': ['Phillip Y. Lee', 'Taehoon Yoon', 'Minhyuk Sung'], 'affiliations': ['KAIST'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20474.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#training', '#games', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Точная пространственная привязка в генерации изображений без дополнительного обучения', 'desc': "Представлен новый метод пространственной привязки для генерации изображений по тексту с использованием Diffusion Transformers (DiT) без дополнительного обучения. Метод использует гибкость архитектуры трансформеров для генерации шумовых патчей, соответствующих каждому ограничивающему прямоугольнику, что обеспечивает точный контроль над отдельными областями. Подход основан на свойстве DiT, называемом 'семантическим обменом', которое позволяет создавать 'семантические клоны' при совместном шумоподавлении маленького патча и полноразмерного изображения. Эксперименты на бенчмарках HRS и DrawBench показывают превосходство метода над существующими подходами без дополнительного обучения."}, 'en': {'title': 'Empowering Image Generation with Training-Free Spatial Grounding!', 'desc': "This paper presents a new method for spatial grounding in text-to-image generation using Diffusion Transformers (DiT) without the need for prior training. The technique allows for better user control by generating specific image patches that correspond to defined bounding boxes. Unlike previous methods that struggled with precise control, this approach utilizes the unique property of semantic sharing in DiT, enabling the generation of 'semantic clones' for each patch. The results show that this method outperforms existing training-free techniques on benchmark datasets, achieving state-of-the-art performance."}, 'zh': {'title': '无训练空间定位，精细控制图像生成', 'desc': '我们提出了一种新颖的无训练空间定位技术，用于文本到图像生成，采用扩散变换器（DiT）。这种空间定位方法通过边界框实现，因其简单性和多功能性而受到关注，增强了用户在图像生成中的控制能力。我们利用变换器架构的灵活性，展示了DiT能够生成与每个边界框对应的噪声补丁，从而实现对每个区域的精细控制。我们的实验表明，与之前的无训练空间定位方法相比，我们的方法在HRS和DrawBench基准测试中达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.20290', 'title': 'Fast Best-of-N Decoding via Speculative Rejection', 'url': 'https://huggingface.co/papers/2410.20290', 'abstract': "The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained model weights during a phase called post-training. While predominant, these post-training methods add substantial complexity before LLMs can be deployed. Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences. The best-known inference-time alignment method, called Best-of-N, is as effective as the state-of-the-art post-training procedures. Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable. In this work, we introduce Speculative Rejection, a computationally-viable inference-time alignment algorithm. It generates high-scoring responses according to a given reward model, like Best-of-N does, while being between 16 to 32 times more computationally efficient.", 'score': 9, 'issue_id': 324, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': 'ca9ce39ec0390fc3', 'authors': ['Hanshi Sun', 'Momin Haider', 'Ruiqi Zhang', 'Huitao Yang', 'Jiahao Qiu', 'Ming Yin', 'Mengdi Wang', 'Peter Bartlett', 'Andrea Zanette'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Google DeepMind', 'Princeton University', 'UC Berkeley', 'University of Virginia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20290.jpg', 'data': {'categories': ['#inference', '#optimization', '#rlhf', '#alignment'], 'emoji': '🚀', 'ru': {'title': 'Эффективное выравнивание LLM без пост-обучения', 'desc': 'Статья представляет новый алгоритм выравнивания больших языковых моделей (LLM) во время вывода, называемый Speculative Rejection. Этот метод позволяет генерировать высококачественные ответы в соответствии с заданной моделью вознаграждения, аналогично методу Best-of-N. Однако Speculative Rejection в 16-32 раза более эффективен с точки зрения вычислительных ресурсов. Новый алгоритм устраняет необходимость в сложном этапе пост-обучения, делая процесс выравнивания LLM более простым и эффективным.'}, 'en': {'title': 'Efficient Alignment of Language Models at Inference Time', 'desc': "This paper discusses the importance of aligning Large Language Models (LLMs) with human preferences to ensure safe and effective deployment. Traditional alignment methods, such as DPO and PPO, require complex post-training adjustments to the model's weights, which can complicate deployment. In contrast, inference-time alignment methods, like Best-of-N, adjust responses during generation but are resource-intensive. The authors propose a new method called Speculative Rejection, which aligns LLMs efficiently at inference time, achieving similar performance to Best-of-N while being significantly more computationally efficient."}, 'zh': {'title': '高效对齐：投机拒绝算法的创新', 'desc': '本论文讨论了大型语言模型（LLMs）在安全有效部署中的对齐问题，确保模型的响应符合人类偏好。现有的对齐技术，如DPO和PPO，通常在后训练阶段通过改变预训练模型的权重来实现对齐，但这增加了复杂性。相较之下，推理时对齐方法避免了复杂的后训练步骤，直接偏向于生成符合人类偏好的响应。我们提出了一种名为“投机拒绝”的推理时对齐算法，其计算效率比现有的最佳选择方法高出16到32倍。'}}}, {'id': 'https://huggingface.co/papers/2410.21220', 'title': 'Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines', 'url': 'https://huggingface.co/papers/2410.21220', 'abstract': "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the user's question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, a novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs.", 'score': 8, 'issue_id': 321, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'bdb8b2a5fbb4c663', 'authors': ['Zhixin Zhang', 'Yiyuan Zhang', 'Xiaohan Ding', 'Xiangyu Yue'], 'affiliations': ['MMLab, CUHK', 'Shanghai AI Laboratory', 'Tencent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21220.jpg', 'data': {'categories': ['#science', '#rag', '#reasoning', '#benchmark', '#cv', '#optimization', '#transfer_learning', '#games', '#agents', '#alignment'], 'emoji': '🔍', 'ru': {'title': 'Зрение ИИ: от незнания к пониманию через интернет', 'desc': 'В статье представлен Vision Search Assistant - новый подход к обработке визуальной информации. Он объединяет возможности больших визуально-языковых моделей (VLM) и веб-агентов для ответа на вопросы по незнакомым изображениям. Система использует генерацию с дополнением из интернета (Retrieval-Augmented Generation), чтобы получать актуальную информацию в реальном времени. Эксперименты показали значительное превосходство этого метода над существующими подходами.'}, 'en': {'title': 'Empowering Vision-Language Models with Real-Time Web Collaboration', 'desc': 'This paper introduces the Vision Search Assistant, a new framework that enhances the capabilities of vision-language models (VLMs) by enabling them to collaborate with web agents. Traditional VLMs struggle with unfamiliar visual content, especially when they encounter objects they have never seen before, leading to unreliable responses. The proposed framework allows VLMs to access real-time information from the web, facilitating open-world Retrieval-Augmented Generation. Experimental results show that the Vision Search Assistant significantly improves performance on both open-set and closed-set question-answering tasks, making it a valuable addition to existing VLMs.'}, 'zh': {'title': '视觉搜索助手：打破未知视觉内容的壁垒', 'desc': '本文提出了一种新的框架，称为视觉搜索助手（Vision Search Assistant），旨在解决传统视觉语言模型（VLMs）在处理未知视觉内容时的局限性。该框架通过结合VLMs的视觉理解能力和网络代理的实时信息访问，实现了开放世界的检索增强生成。这样，即使模型从未见过某个图像中的对象，也能提供准确的回答。实验结果表明，视觉搜索助手在开放集和封闭集的问答基准测试中显著优于其他模型，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2410.21264', 'title': 'LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior', 'url': 'https://huggingface.co/papers/2410.21264', 'abstract': "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space that is not only optimized for video reconstruction but is also structured in a way that is more conducive to autoregressive generation. Moreover, this process defines a sequential order for the discrete tokens, progressively pushing them toward an optimal configuration during training, ensuring smoother and more accurate AR generation at inference time. Comprehensive experiments demonstrate LARP's strong performance, achieving state-of-the-art FVD on the UCF101 class-conditional video generation benchmark. LARP enhances the compatibility of AR models with videos and opens up the potential to build unified high-fidelity multimodal large language models (MLLMs).", 'score': 8, 'issue_id': 321, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '20438897f3e9bc41', 'authors': ['Hanyu Wang', 'Saksham Suri', 'Yixuan Ren', 'Hao Chen', 'Abhinav Shrivastava'], 'affiliations': ['University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21264.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#video', '#optimization', '#multimodal', '#games', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'LARP: Революционный подход к токенизации видео для генеративных моделей', 'desc': 'LARP - это новый токенизатор видео, разработанный для преодоления ограничений существующих методов токенизации в авторегрессионных генеративных моделях. В отличие от традиционных токенизаторов, LARP использует набор обученных холистических запросов для сбора информации из визуального контента, что позволяет захватывать более глобальные и семантические представления. LARP интегрирует облегченный AR-трансформер в качестве предварительной модели во время обучения, что оптимизирует латентное пространство для авторегрессионной генерации. Эксперименты показывают, что LARP достигает передовых результатов на бенчмарке UCF101 по условной генерации видео.'}, 'en': {'title': 'LARP: Revolutionizing Video Tokenization for Better Generative Models', 'desc': "LARP is a new video tokenizer that improves how videos are processed for autoregressive generative models. Instead of just breaking videos into small patches, LARP uses learned holistic queries to capture broader and more meaningful visual information. This method allows for flexible tokenization, adapting the number of tokens based on the task's needs. By integrating a lightweight autoregressive transformer during training, LARP optimizes the token space for better video generation, achieving top performance in benchmarks."}, 'zh': {'title': 'LARP：视频生成的新突破', 'desc': '本文介绍了一种新的视频标记器LARP，旨在克服当前自回归生成模型在视频标记方面的局限性。与传统的局部补丁标记器不同，LARP采用整体标记方案，通过学习的整体查询收集视觉内容的信息，从而捕捉更全球和语义化的表示。LARP支持任意数量的离散标记，能够根据任务的具体需求进行自适应和高效的标记。通过在训练过程中整合轻量级的自回归变换器，LARP优化了视频重建和自回归生成的潜在空间，确保在推理时实现更平滑和准确的生成。'}}}, {'id': 'https://huggingface.co/papers/2410.21271', 'title': 'EoRA: Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation', 'url': 'https://huggingface.co/papers/2410.21271', 'abstract': 'In this work, we re-formulate the model compression problem into the customized compensation problem: Given a compressed model, we aim to introduce residual low-rank paths to compensate for compression errors under customized requirements from users (e.g., tasks, compression ratios), resulting in greater flexibility in adjusting overall capacity without being constrained by specific compression formats. However, naively applying SVD to derive residual paths causes suboptimal utilization of the low-rank representation capacity. Instead, we propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method that directly minimizes compression-induced errors without requiring gradient-based training, achieving fast optimization in minutes using a small amount of calibration data. EoRA projects compression errors into the eigenspace of input activations, leveraging eigenvalues to effectively prioritize the reconstruction of high-importance error components. Moreover, EoRA can be seamlessly integrated with fine-tuning and quantization to further improve effectiveness and efficiency. EoRA consistently outperforms previous methods in compensating errors for compressed LLaMA2/3 models on various tasks, such as language generation, commonsense reasoning, and math reasoning tasks (e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and MathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4 sparsity). EoRA offers a scalable, training-free solution to compensate for compression errors, making it a powerful tool to deploy LLMs in various capacity and efficiency requirements.', 'score': 6, 'issue_id': 332, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '433d9a837f70f8da', 'authors': ['Shih-Yang Liu', 'Huck Yang', 'Chien-Yi Wang', 'Nai Chit Fung', 'Hongxu Yin', 'Charbel Sakr', 'Saurav Muralidharan', 'Kwang-Ting Cheng', 'Jan Kautz', 'Yu-Chiang Frank Wang', 'Pavlo Molchanov', 'Min-Hung Chen'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21271.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#inference', '#optimization', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'EoRA: эффективная компенсация ошибок сжатия языковых моделей без обучения', 'desc': 'В этой работе предлагается метод EoRA для компенсации ошибок сжатия языковых моделей. EoRA проецирует ошибки сжатия в собственное пространство входных активаций, используя собственные значения для эффективной реконструкции важных компонентов ошибок. Метод не требует обучения на градиентах и работает быстро на небольшом объеме калибровочных данных. EoRA превосходит предыдущие методы при компенсации ошибок сжатых моделей LLaMA2/3 на различных задачах, включая генерацию текста и рассуждения.'}, 'en': {'title': 'EoRA: Efficient Error Compensation for Compressed Models', 'desc': "This paper introduces a new approach to model compression called Training-free Eigenspace Low-Rank Approximation (EoRA). Instead of relying on traditional methods like Singular Value Decomposition (SVD), EoRA minimizes errors caused by compression without needing extensive training, making it faster and more efficient. It works by focusing on the most important components of the error, allowing for better reconstruction of the model's performance. EoRA has shown significant improvements in various tasks with compressed models, demonstrating its effectiveness in enhancing model deployment under different constraints."}, 'zh': {'title': '灵活的模型压缩补偿解决方案', 'desc': '本文将模型压缩问题重新定义为定制补偿问题：在给定压缩模型的情况下，我们旨在引入残差低秩路径，以补偿压缩误差，满足用户的定制需求（如任务、压缩比），从而在不受特定压缩格式限制的情况下，灵活调整整体容量。我们提出了一种无训练的特征空间低秩近似方法（EoRA），该方法直接最小化压缩引起的误差，无需基于梯度的训练，使用少量校准数据即可在几分钟内实现快速优化。EoRA将压缩误差投影到输入激活的特征空间中，利用特征值有效优先重建高重要性的误差成分。此外，EoRA可以与微调和量化无缝集成，进一步提高效果和效率。'}}}, {'id': 'https://huggingface.co/papers/2410.19100', 'title': 'VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks', 'url': 'https://huggingface.co/papers/2410.19100', 'abstract': 'Videos are often used to learn or extract the necessary information to complete tasks in ways different than what text and static imagery alone can provide. However, many existing agent benchmarks neglect long-context video understanding, instead focusing on text or static image inputs. To bridge this gap, we introduce VideoWebArena (VideoWA), a benchmark for evaluating the capabilities of long-context multimodal agents for video understanding. VideoWA consists of 2,021 web agent tasks based on manually crafted video tutorials, which total almost four hours of content. For our benchmark, we define a taxonomy of long-context video-based agent tasks with two main areas of focus: skill retention and factual retention. While skill retention tasks evaluate whether an agent can use a given human demonstration to complete a task efficiently, the factual retention task evaluates whether an agent can retrieve instruction-relevant information from a video to complete a task. We find that the best model achieves 13.3% success on factual retention tasks and 45.8% on factual retention QA pairs, far below human performance at 73.9% and 79.3%, respectively. On skill retention tasks, long-context models perform worse with tutorials than without, exhibiting a 5% performance decrease in WebArena tasks and a 10.3% decrease in VisualWebArena tasks. Our work highlights the need to improve the agentic abilities of long-context multimodal models and provides a testbed for future development with long-context video agents.', 'score': 6, 'issue_id': 330, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '82af04969ba93bff', 'authors': ['Lawrence Jang', 'Yinheng Li', 'Charles Ding', 'Justin Lin', 'Paul Pu Liang', 'Dan Zhao', 'Rogerio Bonatti', 'Kazuhito Koishida'], 'affiliations': ['Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Microsoft', 'New York University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.19100.jpg', 'data': {'categories': ['#benchmark', '#video', '#multimodal', '#interpretability', '#games', '#agents', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'VideoWebArena: новый рубеж в понимании видео для ИИ-агентов', 'desc': 'Статья представляет новый бенчмарк VideoWebArena (VideoWA) для оценки возможностей мультимодальных агентов с длинным контекстом в понимании видео. VideoWA включает 2,021 задачу веб-агента на основе видеоуроков, разделенных на две категории: сохранение навыков и сохранение фактов. Результаты показывают, что лучшая модель достигает лишь 13.3% успеха в задачах сохранения фактов, что значительно ниже человеческой производительности. Исследование подчеркивает необходимость улучшения способностей моделей с длинным контекстом в работе с видео.'}, 'en': {'title': 'Enhancing Long-Context Video Understanding for Agents', 'desc': 'This paper introduces VideoWebArena (VideoWA), a new benchmark designed to evaluate long-context multimodal agents specifically for video understanding. It consists of 2,021 tasks based on video tutorials, focusing on two main areas: skill retention and factual retention. The study reveals that current models struggle with these tasks, achieving significantly lower success rates compared to human performance. The findings emphasize the necessity for advancements in the capabilities of long-context video agents to enhance their effectiveness in real-world applications.'}, 'zh': {'title': '提升长上下文视频理解能力的基准测试', 'desc': '本论文介绍了VideoWebArena（VideoWA），这是一个用于评估长上下文多模态代理在视频理解能力的基准。VideoWA包含2021个基于手工制作视频教程的网络代理任务，总时长接近四小时。我们定义了长上下文视频代理任务的分类，主要关注技能保留和事实保留。研究发现，现有模型在事实保留任务上的成功率仅为13.3%，远低于人类的73.9%，这表明需要提升长上下文多模态模型的代理能力。'}}}, {'id': 'https://huggingface.co/papers/2410.20672', 'title': 'Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA', 'url': 'https://huggingface.co/papers/2410.20672', 'abstract': 'Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit "layer tying" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller "Recursive Transformers" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines -- and can even recover most of the performance of the original "full-size" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting. In a theoretical analysis, we show that this has the potential to lead to significant (2-3x) gains in inference throughput.', 'score': 5, 'issue_id': 330, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'ca8b51b9816b76db', 'authors': ['Sangmin Bae', 'Adam Fisch', 'Hrayr Harutyunyan', 'Ziwei Ji', 'Seungyeon Kim', 'Tal Schuster'], 'affiliations': ['Google DeepMind', 'Google Research', 'KAIST AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20672.jpg', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#transfer_learning', '#architecture'], 'emoji': '🔁', 'ru': {'title': 'Рекурсивные трансформеры: компактность без потери производительности', 'desc': 'Это исследование посвящено новому методу уменьшения размера и стоимости больших языковых моделей (LLM) с помощью рекурсивных трансформеров. Авторы предлагают эффективный способ инициализации рекурсивных трансформеров из стандартных предобученных моделей, используя один блок уникальных слоев, повторяемый несколько раз. Они также вводят концепцию релаксированных рекурсивных трансформеров, добавляя гибкость с помощью модулей LoRA. Результаты показывают, что рекурсивные модели превосходят как аналогичные по размеру стандартные модели, так и базовые линии дистилляции знаний.'}, 'en': {'title': 'Efficient LLMs through Recursive Transformers', 'desc': "This paper explores a method to reduce the size and cost of large language models (LLMs) by using parameter sharing through a technique called 'layer tying' in Transformers. The authors introduce 'Recursive Transformers', which utilize a single set of unique layers that are repeated, allowing for a more compact model with minimal performance loss. They enhance this approach with 'Relaxed Recursive Transformers' that incorporate low-rank adaptation modules, maintaining efficiency while improving performance. The results demonstrate that these recursive models outperform similarly sized models and can achieve performance close to larger models, while also introducing a new inference method that significantly boosts throughput."}, 'zh': {'title': '递归变换器：高效共享参数的创新之路', 'desc': '本文探讨了如何通过参数共享来减少大型语言模型（LLMs）的规模和成本。我们重新审视了在变换器中使用的“层绑定”技术，并提出了将现有LLMs转化为更小的“递归变换器”的新方法，这些变换器在层之间共享参数，且性能损失最小。我们还引入了放松递归变换器，通过深度低秩适应（LoRA）模块增加灵活性，同时保持模型的紧凑性。实验结果表明，我们的递归模型在性能上超越了同等规模的预训练模型，并能恢复大部分原始全尺寸模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.18481', 'title': 'Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction', 'url': 'https://huggingface.co/papers/2410.18481', 'abstract': 'Efficiently deriving structured workflows from unannotated dialogs remains an underexplored and formidable challenge in computational linguistics. Automating this process could significantly accelerate the manual design of workflows in new domains and enable the grounding of large language models in domain-specific flowcharts, enhancing transparency and controllability. In this paper, we introduce Dialog2Flow (D2F) embeddings, which differ from conventional sentence embeddings by mapping utterances to a latent space where they are grouped according to their communicative and informative functions (i.e., the actions they represent). D2F allows for modeling dialogs as continuous trajectories in a latent space with distinct action-related regions. By clustering D2F embeddings, the latent space is quantized, and dialogs can be converted into sequences of region/action IDs, facilitating the extraction of the underlying workflow. To pre-train D2F, we build a comprehensive dataset by unifying twenty task-oriented dialog datasets with normalized per-turn action annotations. We also introduce a novel soft contrastive loss that leverages the semantic information of these actions to guide the representation learning process, showing superior performance compared to standard supervised contrastive loss. Evaluation against various sentence embeddings, including dialog-specific ones, demonstrates that D2F yields superior qualitative and quantitative results across diverse domains.', 'score': 5, 'issue_id': 328, 'pub_date': '2024-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '7ddeda6e7b60deaa', 'authors': ['Sergio Burdisso', 'Srikanth Madikeri', 'Petr Motlicek'], 'affiliations': ['Brno University of Technology, Brno, Czech Republic', 'Department of Computational Linguistics, University of Zurich, Zurich, Switzerland', 'Idiap Research Institute, Martigny, Switzerland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18481.jpg', 'data': {'categories': ['#reasoning', '#multilingual', '#cv', '#optimization', '#data', '#training', '#dataset', '#transfer_learning'], 'emoji': '🗺️', 'ru': {'title': 'D2F: Автоматическое картирование диалогов в рабочие процессы', 'desc': 'Статья представляет новый подход к извлечению структурированных рабочих процессов из неаннотированных диалогов с помощью embeddings Dialog2Flow (D2F). D2F отображает высказывания в латентное пространство, группируя их по коммуникативным и информативным функциям. Авторы создали обширный датасет, объединив 20 наборов диалоговых данных с нормализованными аннотациями действий, и разработали новую функцию потерь soft contrastive loss. Эксперименты показали превосходство D2F над другими sentence embeddings в различных доменах.'}, 'en': {'title': 'Transforming Dialogs into Actionable Workflows with D2F Embeddings', 'desc': 'This paper addresses the challenge of creating structured workflows from unannotated dialogs in computational linguistics. It presents Dialog2Flow (D2F) embeddings, which uniquely map dialog utterances into a latent space based on their communicative functions. By clustering these embeddings, the authors can convert dialogs into sequences of action IDs, effectively extracting workflows. The study also introduces a new soft contrastive loss for better representation learning, showing that D2F outperforms existing sentence embeddings in various tasks.'}, 'zh': {'title': '从对话中提取工作流的创新方法', 'desc': '本文探讨了从未标注对话中高效提取结构化工作流的挑战。我们提出了Dialog2Flow (D2F) 嵌入，它通过将对话映射到潜在空间，按其交流和信息功能进行分组。D2F使得对话可以在潜在空间中建模为连续轨迹，并通过聚类D2F嵌入来量化潜在空间，从而提取底层工作流。我们还引入了一种新颖的软对比损失，利用动作的语义信息来指导表示学习过程，显示出优于传统监督对比损失的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.20220', 'title': 'Neural Fields in Robotics: A Survey', 'url': 'https://huggingface.co/papers/2410.20220', 'abstract': "Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and explicit neural representations enabling high-fidelity 3D reconstruction, integration of multi-modal sensor data, and generation of novel viewpoints. This survey explores their applications in robotics, emphasizing their potential to enhance perception, planning, and control. Their compactness, memory efficiency, and differentiability, along with seamless integration with foundation and generative models, make them ideal for real-time applications, improving robot adaptability and decision-making. This paper provides a thorough review of Neural Fields in robotics, categorizing applications across various domains and evaluating their strengths and limitations, based on over 200 papers. First, we present four key Neural Fields frameworks: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting. Second, we detail Neural Fields' applications in five major robotics domains: pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting key works and discussing takeaways and open challenges. Finally, we outline the current limitations of Neural Fields in robotics and propose promising directions for future research. Project page: https://robonerf.github.io", 'score': 4, 'issue_id': 323, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '4666404a494d69c8', 'authors': ['Muhammad Zubair Irshad', 'Mauro Comi', 'Yen-Chen Lin', 'Nick Heppert', 'Abhinav Valada', 'Rares Ambrus', 'Zsolt Kira', 'Jonathan Tremblay'], 'affiliations': ['Georgia Institute of Technology', 'Nvidia', 'Toyota Research Institute', 'University of Bristol', 'University of Freiburg'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20220.jpg', 'data': {'categories': ['#science', '#cv', '#graphs', '#multimodal', '#robotics', '#3d', '#survey', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Нейронные поля: революция в 3D-восприятии роботов', 'desc': 'Нейронные поля - это новый подход к представлению 3D-сцен в компьютерном зрении и робототехнике. Они позволяют точно определять геометрию, 3D-семантику и динамику из 2D-данных с помощью дифференцируемого рендеринга. В статье рассматривается применение нейронных полей в робототехнике для улучшения восприятия, планирования и управления. Авторы анализируют более 200 работ, категоризируют применения в различных областях и оценивают сильные и слабые стороны этого подхода.'}, 'en': {'title': 'Revolutionizing Robotics with Neural Fields for 3D Understanding', 'desc': "Neural Fields are a new way to represent 3D scenes using data from 2D images, which helps in understanding the shape, meaning, and movement of objects in a scene. They use a technique called differentiable rendering to create detailed 3D models and can combine information from different types of sensors. This paper reviews how Neural Fields can improve robots' ability to see, plan, and act in their environments, making them more efficient and adaptable. It also discusses various frameworks and applications of Neural Fields in robotics, while identifying their current challenges and future research opportunities."}, 'zh': {'title': '神经场：提升机器人感知与决策的关键技术', 'desc': '神经场是一种新兴的3D场景表示方法，能够从2D数据中准确推断几何形状、3D语义和动态信息。通过可微渲染，神经场结合了连续隐式和显式神经表示，实现高保真度的3D重建和多模态传感器数据的整合。本文回顾了神经场在机器人领域的应用，强调其在感知、规划和控制方面的潜力。我们还讨论了神经场的局限性，并提出了未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2406.10615', 'title': 'Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation', 'url': 'https://huggingface.co/papers/2406.10615', 'abstract': "Given the high cost of collecting robotic data in the real world, sample efficiency is a consistently compelling pursuit in robotics. In this paper, we introduce SGRv2, an imitation learning framework that enhances sample efficiency through improved visual and action representations. Central to the design of SGRv2 is the incorporation of a critical inductive bias-action locality, which posits that robot's actions are predominantly influenced by the target object and its interactions with the local environment. Extensive experiments in both simulated and real-world settings demonstrate that action locality is essential for boosting sample efficiency. SGRv2 excels in RLBench tasks with keyframe control using merely 5 demonstrations and surpasses the RVT baseline in 23 of 26 tasks. Furthermore, when evaluated on ManiSkill2 and MimicGen using dense control, SGRv2's success rate is 2.54 times that of SGR. In real-world environments, with only eight demonstrations, SGRv2 can perform a variety of tasks at a markedly higher success rate compared to baseline models. Project website: http://sgrv2-robot.github.io", 'score': 2, 'issue_id': 327, 'pub_date': '2024-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': 'cc1e4c7a45b1c9ab', 'authors': ['Tong Zhang', 'Yingdong Hu', 'Jiacheng You', 'Yang Gao'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Qi Zhi Institute', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2406.10615.jpg', 'data': {'categories': ['#synthetic', '#rl', '#benchmark', '#cv', '#optimization', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Локальность действий - ключ к эффективному обучению роботов', 'desc': 'SGRv2 - это фреймворк имитационного обучения для робототехники, повышающий эффективность использования данных. Он основан на концепции локальности действий, предполагающей, что действия робота в основном зависят от целевого объекта и его взаимодействия с ближайшим окружением. SGRv2 превосходит базовые модели в симулированных и реальных задачах, требуя всего 5-8 демонстраций. Эксперименты показывают, что локальность действий критически важна для повышения эффективности обучения.'}, 'en': {'title': 'Boosting Robotics with Sample-Efficient Imitation Learning', 'desc': "This paper presents SGRv2, an advanced imitation learning framework aimed at improving sample efficiency in robotics. It introduces a key concept called action locality, which suggests that a robot's actions are mainly determined by the target object and its local environment interactions. Through extensive testing in both simulated and real-world scenarios, the authors demonstrate that SGRv2 significantly outperforms existing models, achieving higher success rates with fewer demonstrations. The results indicate that SGRv2 is particularly effective in complex tasks, showcasing its potential for practical applications in robotics."}, 'zh': {'title': '提升样本效率的模仿学习新框架SGRv2', 'desc': '本论文介绍了一种名为SGRv2的模仿学习框架，旨在提高机器人数据收集的样本效率。SGRv2通过改进视觉和动作表示，结合了关键的归纳偏置——动作局部性，强调机器人动作主要受目标物体及其与环境的互动影响。实验结果表明，动作局部性对于提升样本效率至关重要，SGRv2在多项任务中表现优异，成功率显著高于基线模型。该框架在真实环境中仅需八个演示就能完成多种任务，展示了其强大的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2410.20636', 'title': 'Language Models And A Second Opinion Use Case: The Pocket Professional', 'url': 'https://huggingface.co/papers/2410.20636', 'abstract': "This research tests the role of Large Language Models (LLMs) as formal second opinion tools in professional decision-making, particularly focusing on complex medical cases where even experienced physicians seek peer consultation. The work analyzed 183 challenging medical cases from Medscape over a 20-month period, testing multiple LLMs' performance against crowd-sourced physician responses. A key finding was the high overall score possible in the latest foundational models (>80% accuracy compared to consensus opinion), which exceeds most human metrics reported on the same clinical cases (450 pages of patient profiles, test results). The study rates the LLMs' performance disparity between straightforward cases (>81% accuracy) and complex scenarios (43% accuracy), particularly in these cases generating substantial debate among human physicians. The research demonstrates that LLMs may be valuable as generators of comprehensive differential diagnoses rather than as primary diagnostic tools, potentially helping to counter cognitive biases in clinical decision-making, reduce cognitive loads, and thus remove some sources of medical error. The inclusion of a second comparative legal dataset (Supreme Court cases, N=21) provides added empirical context to the AI use to foster second opinions, though these legal challenges proved considerably easier for LLMs to analyze. In addition to the original contributions of empirical evidence for LLM accuracy, the research aggregated a novel benchmark for others to score highly contested question and answer reliability between both LLMs and disagreeing human practitioners. These results suggest that the optimal deployment of LLMs in professional settings may differ substantially from current approaches that emphasize automation of routine tasks.", 'score': 2, 'issue_id': 323, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'b1a78251a22af319', 'authors': ['David Noever'], 'affiliations': ['PeopleTec, 4901-D Corporate Drive, Huntsville, AL, USA, 35805'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20636.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#multilingual', '#healthcare', '#dataset', '#alignment'], 'emoji': '🩺', 'ru': {'title': 'LLM как второе мнение: новый взгляд на медицинскую диагностику', 'desc': 'Исследование оценивает эффективность использования больших языковых моделей (LLM) в качестве формального инструмента для получения второго мнения в профессиональной медицинской практике. Анализ 183 сложных медицинских случаев показал, что современные LLM могут достигать точности более 80% по сравнению с консенсусным мнением врачей. Модели продемонстрировали высокую эффективность в простых случаях, но менее точны в сложных сценариях. Результаты указывают на потенциал LLM как инструмента для генерации дифференциальных диагнозов, способного помочь в борьбе с когнитивными искажениями и снижении когнитивной нагрузки врачей.'}, 'en': {'title': 'LLMs: Enhancing Medical Decision-Making with Second Opinions', 'desc': 'This research investigates how Large Language Models (LLMs) can serve as second opinion tools in complex medical decision-making. By analyzing 183 challenging medical cases, the study found that LLMs achieved over 80% accuracy, outperforming many human physicians. However, the models struggled with complex cases, showing only 43% accuracy, indicating their strength lies in generating differential diagnoses rather than making primary diagnoses. The findings suggest that LLMs could help reduce cognitive biases and errors in clinical settings, while also providing a new benchmark for evaluating AI performance against human practitioners.'}, 'zh': {'title': '大型语言模型：医疗决策中的第二意见助手', 'desc': '本研究探讨了大型语言模型（LLMs）在专业决策中的作用，特别是在复杂医疗案例中作为正式的第二意见工具。研究分析了183个具有挑战性的医疗案例，比较了多种LLMs的表现与医生的群体意见。结果显示，最新的基础模型在这些案例中的准确率超过80%，高于大多数人类医生的表现。研究表明，LLMs在生成全面的鉴别诊断方面可能更有价值，而不是作为主要的诊断工具，能够帮助减少临床决策中的认知偏差。'}}}, {'id': 'https://huggingface.co/papers/2410.01968', 'title': 'Bi-Level Motion Imitation for Humanoid Robots', 'url': 'https://huggingface.co/papers/2410.01968', 'abstract': 'Imitation learning from human motion capture (MoCap) data provides a promising way to train humanoid robots. However, due to differences in morphology, such as varying degrees of joint freedom and force limits, exact replication of human behaviors may not be feasible for humanoid robots. Consequently, incorporating physically infeasible MoCap data in training datasets can adversely affect the performance of the robot policy. To address this issue, we propose a bi-level optimization-based imitation learning framework that alternates between optimizing both the robot policy and the target MoCap data. Specifically, we first develop a generative latent dynamics model using a novel self-consistent auto-encoder, which learns sparse and structured motion representations while capturing desired motion patterns in the dataset. The dynamics model is then utilized to generate reference motions while the latent representation regularizes the bi-level motion imitation process. Simulations conducted with a realistic model of a humanoid robot demonstrate that our method enhances the robot policy by modifying reference motions to be physically consistent.', 'score': 1, 'issue_id': 331, 'pub_date': '2024-10-02', 'pub_date_card': {'ru': '2 октября', 'en': 'October 2', 'zh': '10月2日'}, 'hash': '61a874c3e9900dab', 'authors': ['Wenshuai Zhao', 'Yi Zhao', 'Joni Pajarinen', 'Michael Muehlebach'], 'affiliations': ['Aalto University, Finland', 'Max Planck Institute for Intelligent Systems, Germany'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.01968.jpg', 'data': {'categories': ['#rl', '#optimization', '#dataset', '#robotics', '#games', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Адаптивное имитационное обучение для реалистичных движений роботов', 'desc': 'Статья представляет новый подход к имитационному обучению гуманоидных роботов на основе данных захвата движения человека. Авторы предлагают двухуровневую оптимизацию, которая одновременно улучшает политику робота и целевые данные захвата движения. Ключевым элементом является генеративная модель латентной динамики, использующая самосогласованный автоэнкодер для получения разреженных и структурированных представлений движения. Симуляции показывают, что метод улучшает политику робота, модифицируя эталонные движения для физической согласованности.'}, 'en': {'title': 'Optimizing Robot Imitation with Physically Feasible Motion', 'desc': "This paper presents a new approach to train humanoid robots using imitation learning from human motion capture (MoCap) data. The challenge arises from the differences in robot and human body structures, which can make it difficult to directly replicate human movements. To solve this, the authors introduce a bi-level optimization framework that optimizes both the robot's movement policy and the MoCap data. By using a generative latent dynamics model, the framework ensures that the generated motions are physically feasible, improving the robot's ability to imitate human actions effectively."}, 'zh': {'title': '优化模仿学习，提升类人机器人表现', 'desc': '本论文提出了一种基于双层优化的模仿学习框架，用于训练类人机器人。该框架通过优化机器人策略和目标运动捕捉数据，解决了人类行为在机器人中无法精确复制的问题。我们开发了一种生成潜在动态模型，利用自一致性自编码器学习稀疏和结构化的运动表示。通过这种方法，生成的参考运动与潜在表示相结合，确保了机器人策略的物理一致性，从而提升了机器人的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.18057', 'title': 'CLEAR: Character Unlearning in Textual and Visual Modalities', 'url': 'https://huggingface.co/papers/2410.18057', 'abstract': 'Machine Unlearning (MU) is critical for enhancing privacy and security in deep learning models, particularly in large multimodal language models (MLLMs), by removing specific private or hazardous information. While MU has made significant progress in textual and visual modalities, multimodal unlearning (MMU) remains significantly underexplored, partially due to the absence of a suitable open-source benchmark. To address this, we introduce CLEAR, a new benchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding question-answer pairs, enabling a thorough evaluation across modalities. We assess 10 MU methods, adapting them for MMU, and highlight new challenges specific to multimodal forgetting. We also demonstrate that simple ell_1 regularization on LoRA weights significantly mitigates catastrophic forgetting, preserving model performance on retained data. The dataset is available at https://huggingface.co/datasets/therem/CLEAR', 'score': 200, 'issue_id': 337, 'pub_date': '2024-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'ce7b8092b3ce7ef9', 'authors': ['Alexey Dontsov', 'Dmitrii Korzh', 'Alexey Zhavoronkin', 'Boris Mikheev', 'Denis Bobkov', 'Aibek Alanov', 'Oleg Y. Rogov', 'Ivan Oseledets', 'Elena Tutubalina'], 'affiliations': ['AIRI', 'HSE University', 'MIPT', 'Sber', 'Skoltech', 'University of Sharjah'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.18057.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#training', '#dataset', '#open_source', '#security'], 'emoji': '🧠', 'ru': {'title': 'CLEAR: Новый стандарт для мультимодального разобучения в ИИ', 'desc': 'Статья представляет новый бенчмарк CLEAR для оценки методов мультимодального разобучения (MMU) в больших мультимодальных языковых моделях (MLLM). CLEAR содержит данные о 200 вымышленных личностях, включая 3700 изображений с соответствующими парами вопрос-ответ. Авторы адаптировали и оценили 10 методов машинного разобучения (MU) для мультимодальных задач. Исследование показало, что простая L1-регуляризация весов LoRA значительно снижает катастрофическое забывание, сохраняя производительность модели на оставшихся данных.'}, 'en': {'title': 'Enhancing Privacy with Multimodal Unlearning: Introducing CLEAR', 'desc': "This paper focuses on Machine Unlearning (MU), which is important for protecting privacy in deep learning models, especially in large multimodal language models (MLLMs). The authors identify a gap in multimodal unlearning (MMU) research and introduce CLEAR, a new benchmark that includes 200 fictitious individuals and 3,700 images with question-answer pairs for evaluating MMU methods. They assess 10 existing MU methods adapted for MMU and reveal unique challenges related to forgetting in multimodal contexts. Additionally, they find that applying simple ell_1 regularization on LoRA weights can help reduce catastrophic forgetting, thus maintaining the model's performance on the data that remains."}, 'zh': {'title': 'CLEAR：多模态遗忘的新基准', 'desc': '机器遗忘（MU）在深度学习模型中对于增强隐私和安全性至关重要，尤其是在大型多模态语言模型（MLLMs）中。尽管在文本和视觉模态上，MU已经取得了显著进展，但多模态遗忘（MMU）仍然未被充分探索，部分原因是缺乏合适的开源基准。为了解决这个问题，我们引入了CLEAR，一个新的基准，用于评估MMU方法。CLEAR包含200个虚构个体和3700张与相应问答对相关的图像，能够全面评估不同模态的遗忘效果。'}}}, {'id': 'https://huggingface.co/papers/2410.20424', 'title': 'AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions', 'url': 'https://huggingface.co/papers/2410.20424', 'abstract': 'Data science tasks involving tabular data present complex challenges that require sophisticated problem-solving approaches. We propose AutoKaggle, a powerful and user-centric framework that assists data scientists in completing daily data pipelines through a collaborative multi-agent system. AutoKaggle implements an iterative development process that combines code execution, debugging, and comprehensive unit testing to ensure code correctness and logic consistency. The framework offers highly customizable workflows, allowing users to intervene at each phase, thus integrating automated intelligence with human expertise. Our universal data science toolkit, comprising validated functions for data cleaning, feature engineering, and modeling, forms the foundation of this solution, enhancing productivity by streamlining common tasks. We selected 8 Kaggle competitions to simulate data processing workflows in real-world application scenarios. Evaluation results demonstrate that AutoKaggle achieves a validation submission rate of 0.85 and a comprehensive score of 0.82 in typical data science pipelines, fully proving its effectiveness and practicality in handling complex data science tasks.', 'score': 37, 'issue_id': 335, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'd86977fc81e85089', 'authors': ['Ziming Li', 'Qianbo Zang', 'David Ma', 'Jiawei Guo', 'Tuney Zheng', 'Minghao Liu', 'Xinyao Niu', 'Yue Wang', 'Jian Yang', 'Jiaheng Liu', 'Wanjun Zhong', 'Wangchunshu Zhou', 'Wenhao Huang', 'Ge Zhang'], 'affiliations': ['ByteDance Inc.', 'Interdisciplinary Centre for Security, Reliability and Trust (SnT), Universite du Luxembourg', 'University of Melbourne'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20424.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#data', '#dataset', '#agents'], 'emoji': '🤖', 'ru': {'title': 'AutoKaggle: ИИ-помощник для ускорения работы с данными', 'desc': 'AutoKaggle - это мощный фреймворк для решения задач с табличными данными с помощью мультиагентной системы. Он использует итеративный процесс разработки, включающий выполнение кода, отладку и модульное тестирование. Фреймворк предлагает настраиваемые рабочие процессы и универсальный набор инструментов для обработки данных. Оценка на 8 соревнованиях Kaggle показала высокую эффективность AutoKaggle в типичных задачах анализа данных.'}, 'en': {'title': 'Empowering Data Science with AutoKaggle: Automation Meets Expertise', 'desc': 'AutoKaggle is a framework designed to help data scientists manage complex tasks involving tabular data. It uses a collaborative multi-agent system to streamline data pipelines through an iterative process that includes code execution, debugging, and unit testing. The framework allows for customizable workflows, enabling users to integrate their expertise with automated processes. Evaluation on real-world Kaggle competitions shows that AutoKaggle significantly improves productivity and achieves high validation rates in data science tasks.'}, 'zh': {'title': 'AutoKaggle：智能与人类的完美结合', 'desc': '本文提出了一个名为AutoKaggle的框架，旨在帮助数据科学家处理复杂的表格数据任务。该框架通过协作的多智能体系统，支持数据管道的自动化和用户干预，结合了代码执行、调试和单元测试的迭代开发过程。AutoKaggle提供了高度可定制的工作流程，允许用户在每个阶段进行干预，从而将自动智能与人类专业知识相结合。通过在8个Kaggle竞赛中进行模拟，评估结果显示AutoKaggle在数据科学管道中具有良好的有效性和实用性。'}}}, {'id': 'https://huggingface.co/papers/2410.21411', 'title': 'SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization', 'url': 'https://huggingface.co/papers/2410.21411', 'abstract': 'Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, we first present a simple yet well-crafted framework named {\\name}, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, we instruct VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. {\\name} introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As we essentially convert a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, we further propose the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and our method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT.', 'score': 18, 'issue_id': 335, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'ad99b3e3b4ef165c', 'authors': ['Wanhua Li', 'Zibin Meng', 'Jiawei Zhou', 'Donglai Wei', 'Chuang Gan', 'Hanspeter Pfister'], 'affiliations': ['Boston College', 'Harvard University', 'MIT-IBM Watson AI Lab', 'Stony Brook University', 'Tsinghua University', 'UMass Amherst'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21411.jpg', 'data': {'categories': ['#reasoning', '#cv', '#optimization', '#multimodal', '#interpretability', '#training', '#open_source', '#architecture'], 'emoji': '👥', 'ru': {'title': 'SocialGPT: объединение зрения и языка для интерпретируемого распознавания социальных отношений', 'desc': 'Эта статья представляет новый подход к распознаванию социальных отношений на изображениях, названный SocialGPT. Метод объединяет возможности восприятия Визуальных Фундаментальных Моделей (VFM) и способности рассуждения Больших Языковых Моделей (LLM) в модульную структуру. SocialGPT переводит содержание изображения в текстовую социальную историю с помощью VFM, а затем использует LLM для рассуждений на основе текста. Авторы также предлагают метод оптимизации промптов под названием Greedy Segment Prompt Optimization (GSPO) для улучшения производительности.'}, 'en': {'title': 'Bridging Vision and Language for Social Relation Recognition', 'desc': 'This paper introduces a new framework called SocialGPT for social relation reasoning, which identifies relationships like friends or colleagues from images. It combines Vision Foundation Models (VFMs) for image analysis with Large Language Models (LLMs) for reasoning, allowing for better generalization and interpretability. The framework translates visual content into textual narratives and uses LLMs to reason about these narratives, achieving competitive results without additional training. Additionally, it proposes a method called Greedy Segment Prompt Optimization (GSPO) to enhance the performance of LLMs by optimizing prompts effectively, leading to improved outcomes across various image styles.'}, 'zh': {'title': '社交关系推理的新方法：结合视觉与语言的力量', 'desc': '社交关系推理旨在从图像中识别关系类别，如朋友、配偶和同事。当前的方法通常采用端到端的专用网络训练方式，但在泛化能力和可解释性方面存在局限。为了解决这些问题，我们提出了一个名为SocialGPT的框架，结合了视觉基础模型（VFM）和大型语言模型（LLM）的感知能力与推理能力。该方法在不额外训练模型的情况下，在两个数据库上实现了竞争性的零样本结果，并提供了可解释的答案。'}}}, {'id': 'https://huggingface.co/papers/2410.19609', 'title': 'OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization', 'url': 'https://huggingface.co/papers/2410.19609', 'abstract': 'The rapid development of large language and multimodal models has sparked significant interest in using proprietary models, such as GPT-4o, to develop autonomous agents capable of handling real-world scenarios like web navigation. Although recent open-source efforts have tried to equip agents with the ability to explore environments and continuously improve over time, they are building text-only agents in synthetic environments where the reward signals are clearly defined. Such agents struggle to generalize to realistic settings that require multimodal perception abilities and lack ground-truth signals. In this paper, we introduce an open-source framework designed to facilitate the development of multimodal web agent that can autonomously conduct real-world exploration and improve itself. We first train the base model with imitation learning to gain the basic abilities. We then let the agent explore the open web and collect feedback on its trajectories. After that, it further improves its policy by learning from well-performing trajectories judged by another general-purpose model. This exploration-feedback-optimization cycle can continue for several iterations. Experimental results show that our web agent successfully improves itself after each iteration, demonstrating strong performance across multiple test sets.', 'score': 16, 'issue_id': 336, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'c9775abc4b4ddd0d', 'authors': ['Hongliang He', 'Wenlin Yao', 'Kaixin Ma', 'Wenhao Yu', 'Hongming Zhang', 'Tianqing Fang', 'Zhenzhong Lan', 'Dong Yu'], 'affiliations': ['Tencent AI Lab (Seattle)', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.19609.jpg', 'data': {'categories': ['#rl', '#rlhf', '#optimization', '#multimodal', '#training', '#open_source', '#games', '#agents'], 'emoji': '🕸️', 'ru': {'title': 'Самообучающиеся мультимодальные веб-агенты для реального мира', 'desc': 'Эта статья представляет открытую платформу для разработки мультимодальных веб-агентов, способных к автономному исследованию реального мира и самосовершенствованию. Базовая модель обучается с помощью имитационного обучения, затем агент исследует открытый веб и собирает обратную связь. Далее агент улучшает свою стратегию, обучаясь на успешных траекториях, оцененных другой универсальной моделью. Экспериментальные результаты показывают, что веб-агент успешно улучшается после каждой итерации этого цикла.'}, 'en': {'title': 'Empowering Web Agents with Multimodal Learning and Self-Improvement', 'desc': 'This paper presents an open-source framework for developing multimodal web agents that can autonomously explore real-world environments. Unlike previous text-only agents that operate in synthetic settings, this framework allows agents to learn from real web interactions and improve their performance over time. The approach utilizes imitation learning to establish a baseline capability, followed by an exploration-feedback-optimization cycle where the agent refines its policy based on feedback from its own experiences. Experimental results indicate that the agent effectively enhances its skills through iterative learning, showcasing its ability to adapt and perform well in diverse scenarios.'}, 'zh': {'title': '自主多模态网络代理的探索与优化', 'desc': '本文介绍了一种开源框架，旨在开发能够自主进行真实世界探索的多模态网络代理。我们首先通过模仿学习训练基础模型，使其获得基本能力。然后，代理在开放网络中探索并收集其轨迹的反馈，进一步通过学习表现良好的轨迹来优化其策略。实验结果表明，该网络代理在每次迭代后成功自我改进，在多个测试集上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2410.21647', 'title': "Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'", 'url': 'https://huggingface.co/papers/2410.21647', 'abstract': 'Large language models (LLMs) have shown remarkable ability in code generation with more than 90 pass@1 in solving Python coding problems in HumanEval and MBPP. Such high accuracy leads to the question: can LLMs replace human programmers? Existing manual crafted, simple, or single-line code generation benchmarks cannot answer this question due to their gap with real-world software development. To answer this question, we propose REPOCOD, a code generation benchmark with 980 problems collected from 11 popular real-world projects, with more than 58% of them requiring file-level or repository-level context information. In addition, REPOCOD has the longest average canonical solution length (331.6 tokens) and the highest average cyclomatic complexity (9.00) compared to existing benchmarks. In our evaluations on ten LLMs, none of the models can achieve more than 30 pass@1 on REPOCOD, disclosing the necessity of building stronger LLMs that can help developers in real-world software development.', 'score': 15, 'issue_id': 345, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '998f04d017622288', 'authors': ['Shanchao Liang', 'Yiran Hu', 'Nan Jiang', 'Lin Tan'], 'affiliations': ['Purdue University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21647.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#benchmark', '#plp', '#dataset', '#long_context'], 'emoji': '🧪', 'ru': {'title': 'REPOCOD: Реалистичная оценка возможностей LLM в генерации кода', 'desc': 'Статья представляет новый бенчмарк REPOCOD для оценки способностей больших языковых моделей (LLM) в генерации кода. В отличие от существующих простых бенчмарков, REPOCOD содержит 980 задач из реальных проектов, требующих понимания контекста на уровне файлов и репозиториев. Бенчмарк характеризуется наибольшей средней длиной канонического решения и высокой цикломатической сложностью. Тестирование 10 современных LLM на REPOCOD показало, что ни одна модель не достигает точности выше 30% pass@1, что указывает на необходимость разработки более мощных LLM для реальных задач разработки ПО.'}, 'en': {'title': 'REPOCOD: Bridging the Gap in Code Generation for Real-World Software Development', 'desc': 'This paper introduces REPOCOD, a new benchmark for evaluating code generation capabilities of large language models (LLMs) in real-world software development. Unlike previous benchmarks, REPOCOD includes 980 problems from popular projects, emphasizing the need for context beyond simple code snippets. The benchmark features complex problems with longer solutions and higher cyclomatic complexity, making it more representative of actual coding tasks. Evaluation results show that current LLMs struggle with REPOCOD, indicating a gap in their ability to assist human programmers effectively.'}, 'zh': {'title': 'REPOCOD：评估LLMs在真实软件开发中的能力', 'desc': '大型语言模型（LLMs）在代码生成方面表现出色，在解决Python编码问题时的通过率超过90%。然而，现有的简单或单行代码生成基准无法有效评估LLMs是否能替代人类程序员。为了解决这个问题，我们提出了REPOCOD，这是一个包含980个来自11个流行真实项目的问题的代码生成基准，其中超过58%的问题需要文件级或仓库级的上下文信息。我们的评估显示，十个LLMs在REPOCOD上的通过率都未超过30%，这表明需要构建更强大的LLMs来支持真实软件开发中的开发者。'}}}, {'id': 'https://huggingface.co/papers/2410.22304', 'title': 'Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning', 'url': 'https://huggingface.co/papers/2410.22304', 'abstract': 'Mathematical reasoning is a crucial capability for Large Language Models (LLMs), yet generating detailed and accurate reasoning traces remains a significant challenge. This paper introduces a novel approach to produce high-quality reasoning traces for LLM fine-tuning using online learning Flows. Our method employs an incremental output production Flow, where component LLMs collaboratively construct solutions through iterative communication. We train the Flow using online Direct Preference Optimization (DPO) learning with rollouts, generating DPO pairs for each training example and updating models in real-time. We directly compare the quality of reasoning traces generated by our method with those produced through direct model inference, demonstrating the effectiveness of our approach in improving LLM performance in mathematical reasoning tasks.', 'score': 15, 'issue_id': 334, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'ffb1cb8e8855bf5d', 'authors': ['Yihe Deng', 'Paul Mineiro'], 'affiliations': ['Microsoft Research', 'University of California, Los Angeles'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22304.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#optimization', '#math', '#training'], 'emoji': '🧮', 'ru': {'title': 'Улучшение математических рассуждений LLM через онлайн-обучение потоков', 'desc': 'Эта статья представляет новый подход к созданию качественных цепочек рассуждений для дообучения больших языковых моделей (LLM) в задачах математических рассуждений. Метод использует инкрементальный процесс построения решения, где компонентные LLM совместно конструируют решение через итеративное взаимодействие. Обучение происходит с помощью онлайн-оптимизации прямых предпочтений (DPO) с использованием развёртываний, генерируя пары для DPO для каждого обучающего примера и обновляя модели в реальном времени. Авторы напрямую сравнивают качество цепочек рассуждений, созданных их методом, с теми, что получены прямым выводом модели, демонстрируя эффективность подхода в улучшении производительности LLM в задачах математических рассуждений.'}, 'en': {'title': 'Enhancing Mathematical Reasoning in LLMs with Collaborative Learning Flows', 'desc': 'This paper addresses the challenge of generating accurate reasoning traces for Large Language Models (LLMs) in mathematical tasks. It presents a new method that uses online learning Flows, where multiple LLMs work together to create solutions through iterative communication. The training process involves Direct Preference Optimization (DPO) with rollouts, allowing real-time updates and improvements. The results show that this approach significantly enhances the quality of reasoning traces compared to traditional model inference methods.'}, 'zh': {'title': '提升大型语言模型的数学推理能力', 'desc': '本文探讨了大型语言模型（LLMs）在数学推理中的能力，尤其是生成详细和准确的推理过程的挑战。我们提出了一种新方法，通过在线学习流（Flows）来生成高质量的推理过程，以便对LLM进行微调。该方法采用增量输出生成流，多个组件LLM通过迭代通信共同构建解决方案。我们使用在线直接偏好优化（DPO）学习进行训练，实时更新模型，从而提高LLM在数学推理任务中的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.22330', 'title': 'Task Vectors are Cross-Modal', 'url': 'https://huggingface.co/papers/2410.22330', 'abstract': 'We investigate the internal representations of vision-and-language models (VLMs) and how they encode task representations. We consider tasks specified through examples or instructions, using either text or image inputs. Surprisingly, we find that conceptually similar tasks are mapped to similar task vector representations, regardless of how they are specified. Our findings suggest that to output answers, tokens in VLMs undergo three distinct phases: input, task, and answer, a process which is consistent across different modalities and specifications. The task vectors we identify in VLMs are general enough to be derived in one modality (e.g., text) and transferred to another (e.g., image). Additionally, we find that ensembling exemplar and instruction based task vectors produce better task representations. Taken together, these insights shed light on the underlying mechanisms of VLMs, particularly their ability to represent tasks in a shared manner across different modalities and task specifications. Project page: https://task-vectors-are-cross-modal.github.io.', 'score': 11, 'issue_id': 345, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'c6b834102135a2a9', 'authors': ['Grace Luo', 'Trevor Darrell', 'Amir Bar'], 'affiliations': ['UC Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22330.jpg', 'data': {'categories': ['#cv', '#graphs', '#multimodal', '#interpretability', '#transfer_learning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Универсальные векторные представления задач в мультимодальных моделях', 'desc': 'Исследование посвящено анализу внутренних представлений в мультимодальных моделях, объединяющих зрение и язык (VLM). Авторы обнаружили, что концептуально схожие задачи отображаются на похожие векторные представления независимо от способа их спецификации. В процессе генерации ответа токены в VLM проходят три фазы: ввод, задача и ответ. Векторные представления задач оказались достаточно общими, чтобы их можно было переносить между модальностями.'}, 'en': {'title': 'Unified Task Representation Across Modalities in VLMs', 'desc': 'This paper explores how vision-and-language models (VLMs) represent tasks using different types of inputs, such as text and images. It reveals that similar tasks are represented by similar vectors, regardless of their input format. The authors identify a three-phase process in VLMs where inputs are transformed into task representations and then into answers. Furthermore, they demonstrate that task vectors can be effectively transferred between modalities, and combining different types of task vectors enhances performance.'}, 'zh': {'title': '跨模态任务表示的统一性', 'desc': '我们研究了视觉与语言模型（VLMs）的内部表示，以及它们如何编码任务表示。我们发现，无论任务是通过示例还是指令指定，概念上相似的任务会映射到相似的任务向量表示。VLMs 输出答案的过程分为三个阶段：输入、任务和答案，这一过程在不同的模态和规格中是一致的。我们的研究表明，任务向量可以在一种模态（例如文本）中生成，并转移到另一种模态（例如图像），并且结合示例和指令的任务向量可以产生更好的任务表示。'}}}, {'id': 'https://huggingface.co/papers/2410.21845', 'title': 'Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning', 'url': 'https://huggingface.co/papers/2410.21845', 'abstract': 'Reinforcement learning (RL) holds great promise for enabling autonomous acquisition of complex robotic manipulation skills, but realizing this potential in real-world settings has been challenging. We present a human-in-the-loop vision-based RL system that demonstrates impressive performance on a diverse set of dexterous manipulation tasks, including dynamic manipulation, precision assembly, and dual-arm coordination. Our approach integrates demonstrations and human corrections, efficient RL algorithms, and other system-level design choices to learn policies that achieve near-perfect success rates and fast cycle times within just 1 to 2.5 hours of training. We show that our method significantly outperforms imitation learning baselines and prior RL approaches, with an average 2x improvement in success rate and 1.8x faster execution. Through extensive experiments and analysis, we provide insights into the effectiveness of our approach, demonstrating how it learns robust, adaptive policies for both reactive and predictive control strategies. Our results suggest that RL can indeed learn a wide range of complex vision-based manipulation policies directly in the real world within practical training times. We hope this work will inspire a new generation of learned robotic manipulation techniques, benefiting both industrial applications and research advancements. Videos and code are available at our project website https://hil-serl.github.io/.', 'score': 11, 'issue_id': 334, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'b8302dbf79e25f7d', 'authors': ['Jianlan Luo', 'Charles Xu', 'Jeffrey Wu', 'Sergey Levine'], 'affiliations': ['Department of Electrical Engineering and Computer Sciences, UC Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21845.jpg', 'data': {'categories': ['#rl', '#rlhf', '#cv', '#optimization', '#training', '#robotics', '#open_source', '#games'], 'emoji': '🤖', 'ru': {'title': 'Человек и ИИ: совместное обучение роботов сложным манипуляциям', 'desc': 'В этой статье представлена система обучения с подкреплением (RL) для роботизированной манипуляции с участием человека. Система демонстрирует впечатляющие результаты в различных задачах ловкой манипуляции, включая динамическую манипуляцию, точную сборку и координацию двух рук. Подход интегрирует демонстрации и коррекции человека, эффективные алгоритмы RL и другие системные решения для обучения политик с почти идеальным уровнем успеха за 1-2,5 часа тренировки. Результаты показывают значительное превосходство над базовыми методами имитационного обучения и предыдущими подходами RL.'}, 'en': {'title': 'Empowering Robots with Human-guided Reinforcement Learning for Complex Manipulation', 'desc': 'This paper presents a novel human-in-the-loop reinforcement learning (RL) system designed for robotic manipulation tasks. By combining human demonstrations and corrections with efficient RL algorithms, the system achieves high success rates and quick training times for complex tasks like dynamic manipulation and dual-arm coordination. The results show a significant improvement over traditional imitation learning and previous RL methods, with a twofold increase in success rates and faster execution times. The findings indicate that RL can effectively learn complex manipulation skills in real-world scenarios, paving the way for advancements in robotic applications.'}, 'zh': {'title': '人机协作强化学习：实现复杂机器人操作的突破', 'desc': '强化学习（RL）在自主获取复杂机器人操作技能方面具有很大潜力，但在现实环境中实现这一潜力面临挑战。我们提出了一种人机协作的基于视觉的RL系统，在多种灵巧操作任务中表现出色，包括动态操作、精密组装和双臂协调。该方法结合了演示和人类纠正、有效的RL算法以及其他系统设计选择，使得在仅1到2.5小时的训练内学习到接近完美的成功率和快速的循环时间。我们的实验结果表明，该方法在成功率和执行速度上显著优于模仿学习基线和之前的RL方法，展示了RL在现实世界中学习复杂视觉操作策略的有效性。'}}}, {'id': 'https://huggingface.co/papers/2410.21465', 'title': 'ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference', 'url': 'https://huggingface.co/papers/2410.21465', 'abstract': 'With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6times larger batch sizes and boost throughput by up to 3.04times on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.', 'score': 10, 'issue_id': 334, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'f954b9ea6eb1a3ff', 'authors': ['Hanshi Sun', 'Li-Wen Chang', 'Wenlei Bao', 'Size Zheng', 'Ningxin Zheng', 'Xin Liu', 'Harry Dong', 'Yuejie Chi', 'Beidi Chen'], 'affiliations': ['ByteDance', 'Carnegie Mellon University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21465.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#inference', '#optimization', '#open_source', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'ShadowKV: Ускорение вывода длинноконтекстных LLM без компромиссов', 'desc': 'Статья представляет ShadowKV - систему для высокопроизводительного вывода длинноконтекстных больших языковых моделей (LLM). ShadowKV хранит кэш ключей низкого ранга и выгружает кэш значений для уменьшения объема памяти, что позволяет обрабатывать большие пакеты и длинные последовательности. Система использует точную стратегию выбора KV, восстанавливая минимальные разреженные пары KV на лету для минимизации задержки декодирования. Эксперименты показали, что ShadowKV может увеличить размер пакета до 6 раз и повысить производительность до 3,04 раз на GPU A100 без потери точности.'}, 'en': {'title': 'Boosting Long-Context LLM Inference with ShadowKV', 'desc': 'The paper introduces ShadowKV, a system designed to enhance the efficiency of long-context large language model (LLM) inference. It addresses the challenges of high memory usage and low throughput caused by the expanding key-value (KV) cache during token generation. ShadowKV reduces memory consumption by storing a low-rank key cache while offloading the value cache, allowing for larger batch sizes and longer sequences. The system also implements a KV selection strategy that dynamically reconstructs sparse KV pairs, significantly improving throughput without compromising accuracy.'}, 'zh': {'title': '高效推理，提升长上下文模型性能', 'desc': '随着长上下文大语言模型（LLMs）的广泛应用，对高吞吐量推理的需求不断增加。本文提出了ShadowKV，一个高吞吐量的长上下文LLM推理系统，通过存储低秩键缓存并将值缓存卸载，从而减少内存占用。ShadowKV采用准确的KV选择策略，实时重构最小稀疏KV对，以降低解码延迟。实验结果表明，ShadowKV在多个基准测试中表现优异，支持更大的批量大小并显著提高吞吐量。'}}}, {'id': 'https://huggingface.co/papers/2410.21333', 'title': 'Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse', 'url': 'https://huggingface.co/papers/2410.21333', 'abstract': 'Chain-of-thought (CoT) prompting has become a widely used strategy for working with large language and multimodal models. While CoT has been shown to improve performance across many tasks, determining the settings in which it is effective remains an ongoing effort. In particular, it is still an open question in what settings CoT systematically reduces model performance. In this paper, we seek to identify the characteristics of tasks where CoT reduces performance by drawing inspiration from cognitive psychology, looking at cases where (i) verbal thinking or deliberation hurts performance in humans, and (ii) the constraints governing human performance generalize to language models. Three such cases are implicit statistical learning, visual recognition, and classifying with patterns containing exceptions. In extensive experiments across all three settings, we find that a diverse collection of state-of-the-art models exhibit significant drop-offs in performance (e.g., up to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using inference-time reasoning compared to zero-shot counterparts. We also identify three tasks that satisfy condition (i) but not (ii), and find that while verbal thinking reduces human performance in these tasks, CoT retains or increases model performance. Overall, our results show that while there is not an exact parallel between the cognitive processes of models and those of humans, considering cases where thinking has negative consequences for human performance can help us identify settings where it negatively impacts models. By connecting the literature on human deliberation with evaluations of CoT, we offer a new tool that can be used in understanding the impact of prompt choices and inference-time reasoning.', 'score': 9, 'issue_id': 341, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'c5caddd15a467732', 'authors': ['Ryan Liu', 'Jiayi Geng', 'Addison J. Wu', 'Ilia Sucholutsky', 'Tania Lombrozo', 'Thomas L. Griffiths'], 'affiliations': ['Center for Data Science, New York University', 'Department of Computer Science, Princeton University', 'Department of Psychology, Princeton University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21333.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#interpretability', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Когда размышления вредят: ограничения CoT в языковых моделях', 'desc': 'Статья исследует влияние промптинга с цепочкой рассуждений (CoT) на производительность языковых моделей. Авторы выявляют задачи, где CoT снижает эффективность, опираясь на примеры из когнитивной психологии. Эксперименты показывают значительное падение точности современных моделей при использовании CoT в определенных сценариях. Исследование устанавливает связь между литературой о человеческих рассуждениях и оценкой CoT в контексте языковых моделей.'}, 'en': {'title': 'Understanding When Chain-of-Thought Prompting Fails in AI Models', 'desc': 'This paper investigates the effectiveness of Chain-of-Thought (CoT) prompting in large language and multimodal models, particularly focusing on when it may hinder performance. By drawing parallels from cognitive psychology, the authors explore scenarios where verbal reasoning negatively impacts human performance and whether these scenarios apply to language models. Through extensive experiments, they demonstrate that certain tasks, such as implicit statistical learning and visual recognition, can lead to significant performance drops in models when using CoT prompting. The findings suggest that understanding human cognitive limitations can provide insights into optimizing model performance and prompt design.'}, 'zh': {'title': '链式思维的影响：何时有益，何时有害', 'desc': '链式思维（CoT）提示是一种在大型语言和多模态模型中广泛使用的策略。本文探讨了在何种情况下链式思维会降低模型性能，借鉴了认知心理学的研究。通过对隐性统计学习、视觉识别和包含例外的模式分类等任务的实验，我们发现使用推理时的链式思维会导致模型性能显著下降。我们的研究表明，理解人类思维对模型性能的影响，可以帮助我们更好地选择提示和推理策略。'}}}, {'id': 'https://huggingface.co/papers/2410.22325', 'title': 'Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset', 'url': 'https://huggingface.co/papers/2410.22325', 'abstract': 'The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity). Interestingly, we find that the "manipulation centricity" is a strong indicator of success rates when applied to downstream tasks. Drawing from these findings, we propose Manipulation Centric Representation (MCR), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. We introduce a novel contrastive loss that aligns visual observations with the robot\'s proprioceptive state-action dynamics, combined with a behavior cloning (BC)-like actor loss to predict actions during pre-training, along with a time contrastive loss. Empirical results across 4 simulation domains with 20 tasks verify that MCR outperforms the strongest baseline method by 14.8%. Moreover, MCR boosts the performance of data-efficient learning with a UR5e arm on 3 real-world tasks by 76.9%. Project website: https://robots-pretrain-robots.github.io/.', 'score': 9, 'issue_id': 335, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'b8a6dec29ce881c9', 'authors': ['Guangqi Jiang', 'Yifei Sun', 'Tao Huang', 'Huanyu Li', 'Yongyuan Liang', 'Huazhe Xu'], 'affiliations': ['Shanghai Jiao Tong University', 'Tongji University', 'Tsinghua University', 'University of California, San Diego', 'University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22325.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#cv', '#optimization', '#training', '#robotics', '#dataset', '#transfer_learning', '#architecture'], 'emoji': '🦾', 'ru': {'title': 'Улучшение роботизированных манипуляций через предобучение с учетом динамики', 'desc': 'Эта статья представляет новый подход к предобучению визуальных представлений для робототехники под названием Manipulation Centric Representation (MCR). MCR использует датасет DROID и информацию о движениях робота для улучшения эффективности манипуляций. Авторы вводят новую контрастивную функцию потерь, которая сопоставляет визуальные наблюдения с динамикой состояний и действий робота. Эмпирические результаты показывают, что MCR превосходит базовые методы на 14.8% в симуляциях и на 76.9% в реальных задачах.'}, 'en': {'title': 'Enhancing Robot Learning with Manipulation Centric Representation', 'desc': 'This paper discusses the importance of pre-training visual representations for improving robot learning efficiency. It highlights the challenges posed by using human videos for training, which can lead to distribution shifts and a lack of essential dynamic information. The authors introduce a new framework called Manipulation Centric Representation (MCR) that integrates visual features with dynamic data from robotic tasks to enhance performance. Their empirical results show that MCR significantly outperforms existing methods in both simulation and real-world tasks, demonstrating its effectiveness in robotic manipulation.'}, 'zh': {'title': '操作中心表示：提升机器人学习效率的关键', 'desc': '本论文探讨了视觉表示的预训练如何提高机器人学习的效率。由于缺乏大规模的领域内机器人数据集，之前的研究利用人类视频进行预训练，但这些视频的表示存在分布偏移，并缺乏完成任务所需的动态信息。我们提出了一种新的表示学习框架，称为操作中心表示（MCR），它同时捕捉视觉特征和操作任务的动态信息。实验结果表明，MCR在多个模拟领域和真实任务中显著提高了机器人的操作性能。'}}}, {'id': 'https://huggingface.co/papers/2410.21242', 'title': 'Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback', 'url': 'https://huggingface.co/papers/2410.21242', 'abstract': 'Building effective dense retrieval systems remains difficult when relevance supervision is not available. Recent work has looked to overcome this challenge by using a Large Language Model (LLM) to generate hypothetical documents that can be used to find the closest real document. However, this approach relies solely on the LLM to have domain-specific knowledge relevant to the query, which may not be practical. Furthermore, generating hypothetical documents can be inefficient as it requires the LLM to generate a large number of tokens for each query. To address these challenges, we introduce Real Document Embeddings from Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF proposes to re-frame hypothetical document generation as a relevance estimation task, using an LLM to select which documents should be used for nearest neighbor search. Through this re-framing, the LLM no longer needs domain-specific knowledge but only needs to judge what is relevant. Additionally, relevance estimation only requires the LLM to output a single token, thereby improving search latency. Our experiments show that ReDE-RF consistently surpasses state-of-the-art zero-shot dense retrieval methods across a wide range of low-resource retrieval datasets while also making significant improvements in latency per-query.', 'score': 6, 'issue_id': 343, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '4bd9d5661581a6a8', 'authors': ['Nour Jedidi', 'Yung-Sung Chuang', 'Leslie Shing', 'James Glass'], 'affiliations': ['MIT Lincoln Laboratory', 'Massachusetts Institute of Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21242.jpg', 'data': {'categories': ['#rag', '#benchmark', '#inference', '#dataset', '#low_resource', '#retrieval'], 'emoji': '🔍', 'ru': {'title': 'Эффективный поиск без обучения: от гипотетических документов к оценке релевантности', 'desc': 'Статья представляет новый метод под названием ReDE-RF для эффективного поиска информации без размеченных данных. Вместо генерации гипотетических документов с помощью большой языковой модели (LLM), ReDE-RF использует LLM для оценки релевантности существующих документов. Это позволяет избежать необходимости в доменных знаниях у LLM и значительно ускоряет процесс поиска. Эксперименты показывают, что ReDE-RF превосходит современные методы поиска без обучения на различных наборах данных с ограниченными ресурсами.'}, 'en': {'title': 'ReDE-RF: Efficient Relevance Estimation for Dense Retrieval', 'desc': 'This paper addresses the challenges of building effective dense retrieval systems without relevance supervision. It introduces a method called Real Document Embeddings from Relevance Feedback (ReDE-RF), which reframes the generation of hypothetical documents as a relevance estimation task. By using a Large Language Model (LLM) to select relevant documents for nearest neighbor search, the method reduces the need for domain-specific knowledge. The approach not only improves the efficiency of the retrieval process by requiring the LLM to output a single token but also enhances performance on low-resource datasets compared to existing methods.'}, 'zh': {'title': '通过相关反馈提升检索效率', 'desc': '在缺乏相关性监督的情况下，构建有效的密集检索系统仍然很困难。最近的研究尝试使用大型语言模型（LLM）生成假设文档，以找到最接近的真实文档。然而，这种方法完全依赖于LLM具备与查询相关的领域知识，这在实际中可能不切实际。为了解决这些问题，我们提出了基于相关反馈的真实文档嵌入（ReDE-RF），通过将假设文档生成重新框定为相关性估计任务，显著提高了检索效率和准确性。'}}}, {'id': 'https://huggingface.co/papers/2410.20305', 'title': 'Accelerating Direct Preference Optimization with Prefix Sharing', 'url': 'https://huggingface.co/papers/2410.20305', 'abstract': 'Offline paired preference optimization algorithms have become a popular approach for fine-tuning on preference data, outperforming traditional supervised fine-tuning in various tasks. However, traditional implementations often involve redundant computations, especially for tasks with long shared prompts. We introduce prefix sharing for preference tuning, a novel technique that processes chosen and rejected responses as one sequence with a shared prefix. To prevent cross-response contamination, we use a custom block-sparse attention mask. Our method achieves 1.1-1.5times improvement in training throughput on popular DPO datasets, without any effect on convergence. When combined with sequence packing, we observe consistent 1.3-1.6times speedups, benefiting even datasets with smaller sequence lengths. While we focus on Direct Preference Optimization (DPO), our approach is applicable to other paired preference tuning methods. By enhancing computational efficiency, our work contributes to making preference-based fine-tuning more accessible for a wider range of applications and model sizes. We open-source our code at https://github.com/frankxwang/dpo-prefix-sharing.', 'score': 5, 'issue_id': 343, 'pub_date': '2024-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'e6a34a0109321884', 'authors': ['Franklin Wang', 'Sumanth Hegde'], 'affiliations': ['Anyscale', 'MIT CSAIL'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20305.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#training', '#dataset', '#open_source'], 'emoji': '🚀', 'ru': {'title': 'Ускорение обучения языковых моделей с помощью разделения префиксов', 'desc': 'Статья представляет новый метод оптимизации обучения языковых моделей на основе предпочтений пользователей. Авторы предлагают технику разделения префиксов для обработки выбранных и отвергнутых ответов как единой последовательности. Использование специальной маски внимания предотвращает перекрестное загрязнение между ответами. Метод демонстрирует значительное увеличение скорости обучения на популярных наборах данных для DPO без ущерба для сходимости.'}, 'en': {'title': 'Boosting Efficiency in Preference Tuning with Prefix Sharing', 'desc': 'This paper presents a new technique called prefix sharing for optimizing preference tuning in machine learning. It addresses the inefficiencies in traditional methods that lead to redundant computations, especially with long prompts. By processing chosen and rejected responses together with a shared prefix and using a custom block-sparse attention mask, the authors improve training throughput significantly. Their method not only enhances efficiency for Direct Preference Optimization (DPO) but is also applicable to other paired preference tuning methods, making fine-tuning more accessible.'}, 'zh': {'title': '提升偏好微调效率的新方法', 'desc': '离线配对偏好优化算法在偏好数据的微调中变得越来越流行，超越了传统的监督微调方法。我们提出了一种新技术——前缀共享，用于偏好调优，它将选择和拒绝的响应作为一个序列处理，并使用共享前缀。为了防止响应之间的交叉污染，我们采用了自定义的块稀疏注意力掩码。我们的研究提高了训练吞吐量，尤其是在流行的DPO数据集上，提升了1.1到1.5倍，且不影响收敛性。'}}}, {'id': 'https://huggingface.co/papers/2410.20088', 'title': 'RARe: Retrieval Augmented Retrieval with In-Context Examples', 'url': 'https://huggingface.co/papers/2410.20088', 'abstract': 'We investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding model performance in retrieval tasks. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce a simple approach to enable retrievers to use in-context examples. Our approach, RARe, finetunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). In particular, we find RARe exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space.', 'score': 5, 'issue_id': 342, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '03c6d910a07fe4c7', 'authors': ['Atula Tejaswi', 'Yoonsang Lee', 'Sujay Sanghavi', 'Eunsol Choi'], 'affiliations': ['New York University', 'Seoul National University', 'The University of Texas at Austin'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20088.jpg', 'data': {'categories': ['#long_context', '#rag', '#training', '#dataset', '#transfer_learning', '#architecture', '#retrieval'], 'emoji': '🔍', 'ru': {'title': 'Улучшение поисковых систем с помощью обучения на примерах в контексте', 'desc': 'Исследователи изучают возможность улучшения работы моделей встраивания в задачах поиска с помощью примеров в контексте, широко используемых в языковых моделях. Предложен метод RARe, который дообучает предварительно обученную модель с использованием семантически схожих примеров в контексте. Этот подход демонстрирует улучшение производительности до 2.72% nDCG на различных наборах данных для поиска в открытом домене. RARe также показывает более сильную обобщающую способность вне домена по сравнению с моделями, не использующими примеры в контексте.'}, 'en': {'title': 'Enhancing Retrieval with In-Context Examples: The RARe Approach', 'desc': 'This paper explores the use of in-context examples to enhance the performance of embedding models in retrieval tasks. Unlike decoder-only language models, simply adding these examples to queries does not yield immediate benefits. The authors propose a method called RARe, which fine-tunes a pre-trained model using semantically similar in-context examples to the target query. Their approach shows consistent performance improvements, achieving up to +2.72% nDCG across various datasets, and demonstrates better out-of-domain generalization compared to traditional methods.'}, 'zh': {'title': '利用上下文示例提升检索模型性能', 'desc': '本文研究了在检索任务中，使用上下文示例是否能提高嵌入模型的性能。与语言模型不同，简单地将上下文示例添加到目标查询并不能直接奏效。我们提出了一种简单的方法RARe，通过对预训练模型进行微调，使其能够使用与目标查询语义相似的上下文示例。实验结果表明，RARe在多个开放域检索数据集上实现了最高+2.72%的nDCG性能提升，并展现出更强的领域外泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2410.19482', 'title': 'Measuring memorization through probabilistic discoverable extraction', 'url': 'https://huggingface.co/papers/2410.19482', 'abstract': 'Large language models (LLMs) are susceptible to memorizing training data, raising concerns due to the potential extraction of sensitive information. Current methods to measure memorization rates of LLMs, primarily discoverable extraction (Carlini et al., 2022), rely on single-sequence greedy sampling, potentially underestimating the true extent of memorization. This paper introduces a probabilistic relaxation of discoverable extraction that quantifies the probability of extracting a target sequence within a set of generated samples, considering various sampling schemes and multiple attempts. This approach addresses the limitations of reporting memorization rates through discoverable extraction by accounting for the probabilistic nature of LLMs and user interaction patterns. Our experiments demonstrate that this probabilistic measure can reveal cases of higher memorization rates compared to rates found through discoverable extraction. We further investigate the impact of different sampling schemes on extractability, providing a more comprehensive and realistic assessment of LLM memorization and its associated risks. Our contributions include a new probabilistic memorization definition, empirical evidence of its effectiveness, and a thorough evaluation across different models, sizes, sampling schemes, and training data repetitions.', 'score': 4, 'issue_id': 343, 'pub_date': '2024-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'd39237a67eee0b18', 'authors': ['Jamie Hayes', 'Marika Swanberg', 'Harsh Chaudhari', 'Itay Yona', 'Ilia Shumailov'], 'affiliations': ['Boston University', 'Google DeepMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.19482.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#optimization', '#data', '#training', '#security'], 'emoji': '🧠', 'ru': {'title': 'Вероятностный подход к измерению запоминания в языковых моделях', 'desc': 'Эта статья предлагает новый метод оценки уровня запоминания данных большими языковыми моделями (LLM). Авторы вводят вероятностное расширение концепции извлекаемого извлечения, которое учитывает различные схемы семплирования и множественные попытки. Эксперименты показывают, что этот подход может выявить случаи более высокого уровня запоминания по сравнению с традиционными методами. Исследование также анализирует влияние различных схем семплирования на извлекаемость данных, предоставляя более полную оценку рисков, связанных с запоминанием в LLM.'}, 'en': {'title': 'Unveiling the Hidden Memorization of Language Models', 'desc': "This paper addresses the issue of large language models (LLMs) memorizing sensitive training data, which can lead to privacy concerns. It critiques existing methods for measuring memorization rates, particularly the single-sequence greedy sampling approach, which may not accurately reflect true memorization levels. The authors propose a new probabilistic method that assesses the likelihood of extracting specific sequences from generated samples, taking into account various sampling strategies. Their findings indicate that this new approach can uncover higher memorization rates than previously reported, offering a more nuanced understanding of LLMs' memorization capabilities and associated risks."}, 'zh': {'title': '提升LLM记忆评估的概率性方法', 'desc': '大型语言模型（LLMs）可能会记住训练数据，这引发了对敏感信息提取的担忧。现有的测量LLMs记忆率的方法主要依赖于单序列贪婪采样，可能低估了真实的记忆程度。本文提出了一种可扩展的概率性提取方法，量化在生成样本集中提取目标序列的概率，考虑了不同的采样方案和多次尝试。我们的实验表明，这种概率性测量能够揭示出比现有方法更高的记忆率，从而提供了对LLM记忆及其相关风险的更全面评估。'}}}, {'id': 'https://huggingface.co/papers/2410.23090', 'title': 'CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation', 'url': 'https://huggingface.co/papers/2410.23090', 'abstract': 'Retrieval-Augmented Generation (RAG) has become a powerful paradigm for enhancing large language models (LLMs) through external knowledge retrieval. Despite its widespread attention, existing academic research predominantly focuses on single-turn RAG, leaving a significant gap in addressing the complexities of multi-turn conversations found in real-world applications. To bridge this gap, we introduce CORAL, a large-scale benchmark designed to assess RAG systems in realistic multi-turn conversational settings. CORAL includes diverse information-seeking conversations automatically derived from Wikipedia and tackles key challenges such as open-domain coverage, knowledge intensity, free-form responses, and topic shifts. It supports three core tasks of conversational RAG: passage retrieval, response generation, and citation labeling. We propose a unified framework to standardize various conversational RAG methods and conduct a comprehensive evaluation of these methods on CORAL, demonstrating substantial opportunities for improving existing approaches.', 'score': 53, 'issue_id': 348, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'd3cb6da7b94ee077', 'authors': ['Yiruo Cheng', 'Kelong Mao', 'Ziliang Zhao', 'Guanting Dong', 'Hongjin Qian', 'Yongkang Wu', 'Tetsuya Sakai', 'Ji-Rong Wen', 'Zhicheng Dou'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Huawei Poisson Lab', 'Waseda University, Tokyo, Japan'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23090.jpg', 'data': {'categories': ['#rag', '#reasoning', '#benchmark', '#multilingual', '#transfer_learning', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'CORAL: Новый стандарт для оценки многоходовых диалоговых систем с RAG', 'desc': 'Статья представляет новый бенчмарк CORAL для оценки систем генерации с дополнительной информацией (RAG) в многоходовых диалогах. CORAL включает в себя разнообразные информационно-поисковые беседы, автоматически созданные на основе Википедии, и охватывает ключевые задачи, такие как открытый домен, интенсивное использование знаний и смена тем. Бенчмарк поддерживает три основные задачи: поиск релевантных отрывков текста, генерация ответов и маркировка цитат. Авторы также предлагают унифицированную структуру для стандартизации различных методов RAG в диалоговых системах.'}, 'en': {'title': 'Enhancing Multi-Turn Conversations with CORAL Benchmark', 'desc': 'This paper introduces CORAL, a benchmark aimed at improving Retrieval-Augmented Generation (RAG) systems for multi-turn conversations, which are more complex than single-turn interactions. It highlights the need for RAG models to effectively handle diverse and dynamic information-seeking dialogues, addressing challenges like open-domain coverage and topic shifts. The benchmark includes tasks such as passage retrieval, response generation, and citation labeling, providing a structured way to evaluate RAG performance. By proposing a unified framework, the authors aim to enhance the effectiveness of conversational RAG methods and identify areas for future improvement.'}, 'zh': {'title': '提升多轮对话的检索增强生成能力', 'desc': '本论文介绍了一种新的基准CORAL，用于评估检索增强生成（RAG）系统在多轮对话中的表现。现有研究主要集中在单轮对话上，缺乏对复杂多轮对话的深入探讨。CORAL基于维基百科自动生成多样的信息寻求对话，解决开放域覆盖、知识密集度、自由形式响应和话题转移等关键挑战。我们提出了一个统一框架，以标准化不同的对话RAG方法，并在CORAL上进行全面评估，展示了改进现有方法的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2410.23168', 'title': 'TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters', 'url': 'https://huggingface.co/papers/2410.23168', 'abstract': 'Transformers have become the predominant architecture in foundation models due to their excellent performance across various domains. However, the substantial cost of scaling these models remains a significant concern. This problem arises primarily from their dependence on a fixed number of parameters within linear projections. When architectural modifications (e.g., channel dimensions) are introduced, the entire model typically requires retraining from scratch. As model sizes continue growing, this strategy results in increasingly high computational costs and becomes unsustainable. To overcome this problem, we introduce TokenFormer, a natively scalable architecture that leverages the attention mechanism not only for computations among input tokens but also for interactions between tokens and model parameters, thereby enhancing architectural flexibility. By treating model parameters as tokens, we replace all the linear projections in Transformers with our token-parameter attention layer, where input tokens act as queries and model parameters as keys and values. This reformulation allows for progressive and efficient scaling without necessitating retraining from scratch. Our model scales from 124M to 1.4B parameters by incrementally adding new key-value parameter pairs, achieving performance comparable to Transformers trained from scratch while greatly reducing training costs. Code and models are available at https://github.com/Haiyang-W/TokenFormer.', 'score': 22, 'issue_id': 353, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '4785b6a73debe15e', 'authors': ['Haiyang Wang', 'Yue Fan', 'Muhammad Ferjad Naeem', 'Yongqin Xian', 'Jan Eric Lenssen', 'Liwei Wang', 'Federico Tombari', 'Bernt Schiele'], 'affiliations': ['Google', 'Max Planck Institute for Informatics', 'Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23168.jpg', 'data': {'categories': ['#small_models', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🔄', 'ru': {'title': 'Гибкое масштабирование нейросетей без полного переобучения', 'desc': 'TokenFormer - это новая архитектура модели, которая позволяет эффективно масштабировать нейронные сети без необходимости полного переобучения. В отличие от стандартных трансформеров, TokenFormer использует механизм внимания не только между входными токенами, но и между токенами и параметрами модели. Это достигается за счет замены линейных проекций на слои внимания токен-параметр, где входные токены выступают в роли запросов, а параметры модели - в роли ключей и значений. Такой подход позволяет постепенно наращивать размер модели от 124 млн до 1,4 млрд параметров, сохраняя производительность на уровне полностью переобученных трансформеров, но значительно снижая вычислительные затраты.'}, 'en': {'title': 'TokenFormer: Scalable Transformers Without Retraining', 'desc': 'This paper presents TokenFormer, a new architecture designed to address the high computational costs associated with scaling Transformer models. Traditional Transformers require retraining from scratch when architectural changes are made, which is inefficient as model sizes increase. TokenFormer innovatively uses the attention mechanism to allow model parameters to interact with input tokens, treating parameters as tokens themselves. This approach enables flexible scaling of the model without the need for complete retraining, significantly reducing training costs while maintaining competitive performance.'}, 'zh': {'title': 'TokenFormer：高效可扩展的Transformer架构', 'desc': '本文介绍了一种新的模型架构TokenFormer，旨在解决现有Transformer模型在扩展时的高计算成本问题。TokenFormer通过将模型参数视为令牌，利用注意力机制实现输入令牌与模型参数之间的交互，从而提高了架构的灵活性。与传统方法不同，TokenFormer允许逐步扩展模型，而无需从头开始重新训练。该模型在参数数量从1.24亿扩展到14亿的过程中，能够在保持性能的同时显著降低训练成本。'}}}, {'id': 'https://huggingface.co/papers/2410.22391', 'title': 'A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks', 'url': 'https://huggingface.co/papers/2410.22391', 'abstract': 'In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.', 'score': 21, 'issue_id': 350, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '876c89e8fc188dd3', 'authors': ['Thomas Schmied', 'Thomas Adler', 'Vihang Patil', 'Maximilian Beck', 'Korbinian Pöppel', 'Johannes Brandstetter', 'Günter Klambauer', 'Razvan Pascanu', 'Sepp Hochreiter'], 'affiliations': ['ELLIS Unit, LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria', 'Google DeepMind', 'NXAI GmbH, Linz, Austria', 'UCL'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22391.jpg', 'data': {'categories': ['#rl', '#inference', '#optimization', '#training', '#robotics', '#games', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'LRAM: Быстрее трансформеров, эффективнее в реальном времени', 'desc': 'В статье представлена новая модель LRAM (Large Recurrent Action Model) для обучения с подкреплением, основанная на архитектуре xLSTM. LRAM предлагает линейную сложность вывода и способность к экстраполяции длины последовательности, что делает её более практичной для приложений реального времени по сравнению с моделями на основе трансформеров. Эксперименты на 432 задачах из 6 доменов показали, что LRAM не уступает трансформерам по производительности и скорости. Это исследование демонстрирует потенциал современных рекуррентных архитектур для моделей с большим пространством действий в обучении с подкреплением.'}, 'en': {'title': 'Fast and Effective: LRAM for Real-Time Reinforcement Learning', 'desc': 'This paper explores the use of modern recurrent architectures, specifically xLSTM, for creating large action models in Reinforcement Learning (RL). Traditional Transformer models are powerful but suffer from slow inference times, making them unsuitable for real-time applications like robotics. The proposed Large Recurrent Action Model (LRAM) leverages the benefits of xLSTM to achieve linear-time inference complexity while maintaining strong performance. Experimental results demonstrate that LRAM outperforms Transformer-based models in both speed and effectiveness across a variety of tasks.'}, 'zh': {'title': '快速推理的强化学习新选择', 'desc': '近年来，强化学习（RL）领域出现了一个趋势，即使用大型离线数据集通过序列建模训练大型动作模型。现有模型主要基于Transformer架构，虽然能够生成强大的智能体，但由于推理速度慢，难以应用于实时场景，如机器人技术。最近提出的现代递归架构，如xLSTM和Mamba，具有与Transformer相似的训练并行化优势，同时提供快速推理能力。本文研究了这些现代递归架构在大型动作模型中的适用性，并提出了一种以xLSTM为核心的大型递归动作模型（LRAM），其推理复杂度为线性时间，且具有自然的序列长度外推能力。'}}}, {'id': 'https://huggingface.co/papers/2410.23287', 'title': 'ReferEverything: Towards Segmenting Everything We Can Speak of in Videos', 'url': 'https://huggingface.co/papers/2410.23287', 'abstract': "We present REM, a framework for segmenting a wide range of concepts in video that can be described through natural language. Our method capitalizes on visual-language representations learned by video diffusion models on Internet-scale datasets. A key insight of our approach is preserving as much of the generative model's original representation as possible, while fine-tuning it on narrow-domain Referral Object Segmentation datasets. As a result, our framework can accurately segment and track rare and unseen objects, despite being trained on object masks from a limited set of categories. Additionally, it can generalize to non-object dynamic concepts, such as waves crashing in the ocean, as demonstrated in our newly introduced benchmark for Referral Video Process Segmentation (Ref-VPS). Our experiments show that REM performs on par with state-of-the-art approaches on in-domain datasets, like Ref-DAVIS, while outperforming them by up to twelve points in terms of region similarity on out-of-domain data, leveraging the power of Internet-scale pre-training.", 'score': 17, 'issue_id': 356, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'cf2371629ffd5ab5', 'authors': ['Anurag Bagchi', 'Zhipeng Bao', 'Yu-Xiong Wang', 'Pavel Tokmakov', 'Martial Hebert'], 'affiliations': ['Carnegie Mellon University', 'Toyota Research Institute', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23287.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#benchmark', '#cv', '#graphs', '#video', '#training', '#dataset', '#transfer_learning'], 'emoji': '🎥', 'ru': {'title': 'REM: универсальная сегментация видео с помощью естественного языка', 'desc': 'REM - это фреймворк для сегментации различных концепций в видео, описываемых с помощью естественного языка. Он использует визуально-языковые представления, полученные видео-диффузионными моделями на масштабных интернет-датасетах. Ключевая особенность подхода - сохранение большей части исходного представления генеративной модели при дообучении на узкоспециализированных датасетах сегментации объектов по запросу. REM способен точно сегментировать и отслеживать редкие и невиданные ранее объекты, а также обобщаться на динамические концепты, не являющиеся объектами.'}, 'en': {'title': 'Segmenting Video Concepts with Natural Language Power', 'desc': 'The REM framework is designed to segment various concepts in videos using natural language descriptions. It utilizes visual-language representations from video diffusion models trained on large datasets from the internet. By fine-tuning these models on specific datasets for Referral Object Segmentation, REM can effectively identify and track both common and rare objects. Additionally, it demonstrates the ability to generalize to dynamic concepts, achieving high performance on both in-domain and out-of-domain tasks.'}, 'zh': {'title': 'REM框架：视频概念分割的新突破', 'desc': '我们提出了REM框架，用于通过自然语言对视频中的各种概念进行分割。该方法利用了在互联网规模数据集上学习的视觉-语言表示，结合视频扩散模型。我们的方法的关键在于尽可能保留生成模型的原始表示，同时在狭域的引用对象分割数据集上进行微调。结果表明，REM框架能够准确分割和跟踪稀有和未见过的对象，并且能够推广到非对象动态概念，如海浪的冲击。'}}}, {'id': 'https://huggingface.co/papers/2410.23123', 'title': 'On Memorization of Large Language Models in Logical Reasoning', 'url': 'https://huggingface.co/papers/2410.23123', 'abstract': "Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. One hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems. In this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. On the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K&K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities. Finally, our analysis with per-sample memorization score sheds light on how LLMs switch between reasoning and memorization in solving logical puzzles. Our code and data are available at https://memkklogic.github.io.", 'score': 16, 'issue_id': 358, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'f3e776b0854b1ec8', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#reasoning', '#rl', '#benchmark', '#interpretability', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Запоминание vs Рассуждение: Сложное взаимодействие в больших языковых моделях', 'desc': "В статье исследуется взаимосвязь между способностью больших языковых моделей (LLM) к запоминанию и их навыками логического мышления. Авторы используют динамически генерируемый набор логических задач на основе головоломок 'Рыцари и лжецы' для измерения степени запоминания. Результаты показывают, что LLM могут достигать почти идеальной точности на тренировочных примерах, но терпят неудачу при небольших изменениях в задачах. Тем не менее, исследование также демонстрирует, что дообучение моделей, несмотря на сильное запоминание, улучшает их способность к обобщению."}, 'en': {'title': "Memorization vs. Reasoning: Unraveling LLMs' Logic Skills", 'desc': 'This paper explores the reasoning capabilities of large language models (LLMs) and their tendency to memorize training data. The authors propose that LLMs achieve high performance on reasoning tasks by memorizing similar problems rather than genuinely understanding them. Through experiments with Knights and Knaves puzzles, they demonstrate that while LLMs can interpolate training data effectively, they struggle with slight variations, indicating reliance on memorization. However, the study also reveals that fine-tuning improves generalization, suggesting a complex relationship between memorization and reasoning in LLMs.'}, 'zh': {'title': '记忆与推理的复杂交互', 'desc': '大型语言模型（LLMs）在复杂推理基准测试中表现良好，但也可能出现基本推理错误。本文系统地研究了LLMs推理能力背后的机制，提出了记忆化假设，认为模型在推理任务中可能依赖于对相似问题的记忆。通过动态生成的逻辑推理基准，我们发现LLMs在微调后能够完美解决训练谜题，但在稍微改变这些谜题时却表现不佳，表明它们在解决训练谜题时严重依赖记忆。尽管微调导致了重度记忆化，但也提高了模型的泛化性能，显示出记忆与真实推理能力之间的复杂关系。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.20779', 'title': 'Decoding Reading Goals from Eye Movements', 'url': 'https://huggingface.co/papers/2410.20779', 'abstract': 'Readers can have different goals with respect to the text they are reading. Can these goals be decoded from the pattern of their eye movements over the text? In this work, we examine for the first time whether it is possible to decode two types of reading goals that are common in daily life: information seeking and ordinary reading. Using large scale eye-tracking data, we apply to this task a wide range of state-of-the-art models for eye movements and text that cover different architectural and data representation strategies, and further introduce a new model ensemble. We systematically evaluate these models at three levels of generalization: new textual item, new participant, and the combination of both. We find that eye movements contain highly valuable signals for this task. We further perform an error analysis which builds on prior empirical findings on differences between ordinary reading and information seeking and leverages rich textual annotations. This analysis reveals key properties of textual items and participant eye movements that contribute to the difficulty of the task.', 'score': 15, 'issue_id': 356, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '69c16b774d32c4c1', 'authors': ['Omer Shubi', 'Cfir Avraham Hadar', 'Yevgeni Berzak'], 'affiliations': ['Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, USA', 'Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Haifa, Israel'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20779.jpg', 'data': {'categories': ['#reasoning', '#cv', '#interpretability', '#data', '#training', '#architecture'], 'emoji': '👁️', 'ru': {'title': 'Разгадка целей чтения по движениям глаз', 'desc': 'Исследование посвящено декодированию целей чтения на основе движений глаз. Авторы применяют современные модели машинного обучения для анализа крупномасштабных данных айтрекинга, чтобы различать информационный поиск и обычное чтение. Оценка моделей проводится на трех уровнях обобщения: новый текст, новый участник и их комбинация. Анализ ошибок выявляет ключевые свойства текстов и движений глаз, влияющие на сложность задачи.'}, 'en': {'title': 'Decoding Reading Goals Through Eye Movements', 'desc': "This paper explores whether the goals of readers, such as information seeking and ordinary reading, can be inferred from their eye movement patterns. Using extensive eye-tracking data, the authors implement various advanced machine learning models to analyze these movements and introduce a new model ensemble for improved accuracy. They evaluate the models' performance across different scenarios, including new texts and new participants, demonstrating that eye movements provide significant insights into reading intentions. Additionally, an error analysis highlights specific characteristics of texts and eye movement behaviors that affect the decoding process."}, 'zh': {'title': '解码阅读目标：眼动与文本的深度分析', 'desc': '本研究首次探讨了是否可以通过眼动模式解码读者的阅读目标，包括信息寻求和普通阅读。我们使用大规模的眼动追踪数据，应用多种先进的模型来分析眼动和文本，提出了一种新的模型集成方法。通过对新文本、新参与者及其组合的系统评估，我们发现眼动包含了对解码阅读目标非常有价值的信号。进一步的错误分析揭示了文本特性和参与者眼动的关键属性，这些属性影响了任务的难度。'}}}, {'id': 'https://huggingface.co/papers/2410.22884', 'title': 'Stealing User Prompts from Mixture of Experts', 'url': 'https://huggingface.co/papers/2410.22884', 'abstract': "Mixture-of-Experts (MoE) models improve the efficiency and scalability of dense language models by routing each token to a small number of experts in each layer. In this paper, we show how an adversary that can arrange for their queries to appear in the same batch of examples as a victim's queries can exploit Expert-Choice-Routing to fully disclose a victim's prompt. We successfully demonstrate the effectiveness of this attack on a two-layer Mixtral model, exploiting the tie-handling behavior of the torch.topk CUDA implementation. Our results show that we can extract the entire prompt using O({VM}^2) queries (with vocabulary size V and prompt length M) or 100 queries on average per token in the setting we consider. This is the first attack to exploit architectural flaws for the purpose of extracting user prompts, introducing a new class of LLM vulnerabilities.", 'score': 13, 'issue_id': 351, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '50ec28e1ed4db1bb', 'authors': ['Itay Yona', 'Ilia Shumailov', 'Jamie Hayes', 'Nicholas Carlini'], 'affiliations': ['Google DeepMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22884.jpg', 'data': {'categories': ['#inference', '#security', '#architecture', '#benchmark'], 'emoji': '🕵️', 'ru': {'title': 'Уязвимость в MoE моделях: как архитектурные особенности могут раскрыть ваш промпт', 'desc': 'Статья описывает уязвимость в моделях Mixture-of-Experts (MoE), использующих маршрутизацию Expert-Choice-Routing. Авторы демонстрируют, как злоумышленник может эксплуатировать эту уязвимость для раскрытия промпта жертвы, если запросы обрабатываются в одном батче. Эксперимент проводился на двухслойной модели Mixtral, используя особенности реализации torch.topk CUDA. Это первая атака, эксплуатирующая архитектурные недостатки для извлечения пользовательских промптов, что открывает новый класс уязвимостей в больших языковых моделях.'}, 'en': {'title': 'Exposing Prompts: A New Vulnerability in Mixture-of-Experts Models', 'desc': "This paper discusses a vulnerability in Mixture-of-Experts (MoE) models, which are designed to enhance the efficiency of language models by directing tokens to specific experts. The authors demonstrate that an adversary can exploit the Expert-Choice-Routing mechanism to reveal a victim's input prompt by cleverly arranging queries in the same batch. They successfully execute this attack on a two-layer Mixtral model, taking advantage of the tie-handling behavior in the torch.topk CUDA implementation. The findings indicate that the entire prompt can be extracted with a relatively small number of queries, highlighting a new class of vulnerabilities in large language models (LLMs)."}, 'zh': {'title': '利用架构缺陷提取用户提示的攻击', 'desc': '混合专家模型（MoE）通过将每个令牌路由到每层的小部分专家，提高了密集语言模型的效率和可扩展性。本文展示了一个对手如何利用专家选择路由，完全泄露受害者的提示，只需将其查询与受害者的查询放在同一批次中。我们在一个两层的Mixtral模型上成功演示了这一攻击，利用了torch.topk CUDA实现中的平局处理行为。我们的结果表明，在考虑的设置中，我们可以使用O({VM}^2)的查询（其中V是词汇大小，M是提示长度）或平均每个令牌100个查询来提取整个提示，这是首次利用架构缺陷提取用户提示的攻击，介绍了一类新的大型语言模型脆弱性。'}}}, {'id': 'https://huggingface.co/papers/2410.22587', 'title': 'Toxicity of the Commons: Curating Open-Source Pre-Training Data', 'url': 'https://huggingface.co/papers/2410.22587', 'abstract': 'Open-source large language models are becoming increasingly available and popular among researchers and practitioners. While significant progress has been made on open-weight models, open training data is a practice yet to be adopted by the leading open-weight models creators. At the same time, there researchers are working to make language models safer. We propose a data curation pipeline to reduce harmful outputs by models trained on public domain data. There are unique challenges to working with public domain data, as these sources differ from web text in both form and content. Many sources are historical documents and are the result of Optical Character Recognition (OCR). Consequently, current state-of-the-art approaches to toxicity filtering are often infeasible or inappropriate for open data models. In this paper, we introduce a new fully open-source pipeline for open-data toxicity filtering. Our contributions are threefold. We create a custom training dataset, ToxicCommons, which is composed of texts which have been classified across five different dimensions (racial/origin-based, gender/sex-based, religious, ability-based discrimination, and violence). We use this dataset to train a custom classifier, Celadon, that can be used to detect toxic content in open data more efficiently at a larger scale. Finally, we describe the balanced approach to content filtration that optimizes safety filtering with respect to the filtered data available for training.', 'score': 8, 'issue_id': 355, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '36253407cf358347', 'authors': ['Catherine Arnett', 'Eliot Jones', 'Ivan P. Yamshchikov', 'Pierre-Carl Langlais'], 'affiliations': ['Center for Artificial Intelligence, Technical University of Applied Sciences Würzburg-Schweinfurt', 'Department of Linguistics, University of California San Diego', 'Gray Swan AI', 'PleIAs, Paris, France'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22587.jpg', 'data': {'categories': ['#synthetic', '#healthcare', '#ethics', '#data', '#training', '#dataset', '#open_source'], 'emoji': '🛡️', 'ru': {'title': 'Безопасные языковые модели на основе открытых данных', 'desc': 'Статья представляет новый подход к фильтрации токсичного контента в открытых данных для обучения языковых моделей. Авторы создали датасет ToxicCommons для классификации текстов по пяти аспектам дискриминации и насилия. На его основе обучен классификатор Celadon для эффективного выявления токсичного контента в больших объемах открытых данных. Предложен сбалансированный подход к фильтрации, оптимизирующий безопасность и сохранение данных для обучения.'}, 'en': {'title': 'Enhancing Safety in Open-Source Language Models', 'desc': 'This paper discusses the development of a data curation pipeline aimed at reducing harmful outputs from large language models trained on public domain data. The authors highlight the challenges posed by the unique characteristics of public domain sources, which often include historical documents and require Optical Character Recognition (OCR). They introduce a custom training dataset called ToxicCommons, which categorizes texts based on five dimensions of toxicity. Additionally, they present a classifier named Celadon, designed to efficiently detect toxic content in open data, while also optimizing safety filtering during the training process.'}, 'zh': {'title': '开放数据的安全过滤新方法', 'desc': '这篇论文介绍了一个开放源代码的数据筛选流程，旨在减少使用公共领域数据训练的语言模型的有害输出。研究者们创建了一个名为ToxicCommons的自定义训练数据集，包含五个不同维度的有毒内容分类。然后，他们使用这个数据集训练了一个名为Celadon的分类器，以更高效地检测开放数据中的有毒内容。最后，论文描述了一种平衡的内容过滤方法，优化了安全过滤与可用于训练的过滤数据之间的关系。'}}}, {'id': 'https://huggingface.co/papers/2410.23331', 'title': 'Can Models Help Us Create Better Models? Evaluating LLMs as Data Scientists', 'url': 'https://huggingface.co/papers/2410.23331', 'abstract': 'We present a benchmark for large language models designed to tackle one of the most knowledge-intensive tasks in data science: writing feature engineering code, which requires domain knowledge in addition to a deep understanding of the underlying problem and data structure. The model is provided with a dataset description in a prompt and asked to generate code transforming it. The evaluation score is derived from the improvement achieved by an XGBoost model fit on the modified dataset compared to the original data. By an extensive evaluation of state-of-the-art models and comparison to well-established benchmarks, we demonstrate that the FeatEng of our proposal can cheaply and efficiently assess the broad capabilities of LLMs, in contrast to the existing methods.', 'score': 7, 'issue_id': 360, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'c705812ff0c749c6', 'authors': ['Michał Pietruszka', 'Łukasz Borchmann', 'Aleksander Jędrosz', 'Paweł Morawiecki'], 'affiliations': ['Polish Academy of Sciences', 'Snowflake AI Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23331.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#plp', '#data'], 'emoji': '🔬', 'ru': {'title': 'FeatEng: новый способ оценки ИИ в инжиниринге признаков', 'desc': 'Статья представляет новый бенчмарк для оценки больших языковых моделей в задаче инжиниринга признаков. Модель получает описание датасета и должна сгенерировать код для его преобразования. Оценка производится путем сравнения качества XGBoost модели на оригинальных и модифицированных данных. Авторы провели обширное тестирование современных моделей и показали, что их бенчмарк FeatEng позволяет эффективно оценивать широкие возможности больших языковых моделей.'}, 'en': {'title': 'Empowering LLMs for Feature Engineering Excellence', 'desc': 'This paper introduces a benchmark specifically for large language models (LLMs) that focuses on generating feature engineering code, a crucial task in data science that requires both domain knowledge and an understanding of data structures. The LLM is prompted with a dataset description and tasked with creating code to transform the dataset. The effectiveness of the generated code is evaluated by measuring the performance improvement of an XGBoost model trained on the modified dataset compared to the original. The authors demonstrate that their proposed feature engineering benchmark, FeatEng, provides a cost-effective and efficient way to assess the capabilities of LLMs compared to traditional evaluation methods.'}, 'zh': {'title': '高效评估大型语言模型的特征工程能力', 'desc': '本文提出了一个针对大型语言模型的基准测试，旨在解决数据科学中知识密集型的任务：编写特征工程代码。这项任务不仅需要领域知识，还需要对底层问题和数据结构的深刻理解。我们通过提供数据集描述的提示，要求模型生成转换代码，并通过与原始数据相比，评估在修改后的数据集上拟合的XGBoost模型的改进得分。通过对最先进模型的广泛评估和与已有基准的比较，我们展示了我们提议的FeatEng能够以低成本和高效的方式评估大型语言模型的广泛能力。'}}}, {'id': 'https://huggingface.co/papers/2410.23277', 'title': 'SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation', 'url': 'https://huggingface.co/papers/2410.23277', 'abstract': "Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Project Website: https://slowfast-vgen.github.io", 'score': 7, 'issue_id': 359, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'c1aef56c5c16c883', 'authors': ['Yining Hong', 'Beide Liu', 'Maxine Wu', 'Yuanhao Zhai', 'Kai-Wei Chang', 'Linjie Li', 'Kevin Lin', 'Chung-Ching Lin', 'Jianfeng Wang', 'Zhengyuan Yang', 'Yingnian Wu', 'Lijuan Wang'], 'affiliations': ['Microsoft Research', 'State University of New York at Buffalo', 'UCLA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23277.jpg', 'data': {'categories': ['#long_context', '#diffusion', '#synthetic', '#rl', '#video', '#training', '#dataset', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Двухскоростное обучение для согласованной генерации длинных видео', 'desc': 'Статья представляет SlowFast-VGen - новую систему двухскоростного обучения для генерации длинных видео на основе действий. Модель сочетает медленное обучение динамике мира с помощью маскированной условной видео-диффузионной модели и быстрое обучение в процессе вывода с использованием временного модуля LoRA. Предложен алгоритм цикла медленно-быстрого обучения, интегрирующий внутренний цикл быстрого обучения во внешний цикл медленного. Эксперименты показывают превосходство SlowFast-VGen над базовыми моделями по различным метрикам генерации видео на основе действий.'}, 'en': {'title': 'Bridging Slow and Fast Learning for Better Video Generation', 'desc': 'This paper presents SlowFast-VGen, a new approach for generating long videos that combines slow and fast learning methods. The model uses a masked conditional video diffusion technique for slow learning of world dynamics, while a temporal LoRA module allows for fast learning to store episodic memories. By integrating these two learning speeds, the model can maintain consistency across longer video sequences and improve action-driven video generation. The authors demonstrate that their method outperforms existing models in various metrics, particularly in generating coherent long videos.'}, 'zh': {'title': '双速学习，生成更长视频！', 'desc': '本文提出了一种名为SlowFast-VGen的新型双速学习系统，用于生成基于动作的长视频。该系统结合了慢学习和快学习的策略，慢学习通过掩蔽条件视频扩散模型来捕捉世界动态，而快学习则利用时间LoRA模块在推理时更新参数，以高效存储情节记忆。通过引入慢-快学习循环算法，系统能够在长视频生成中保持一致性，并有效回忆多次经历的上下文信息。实验结果表明，SlowFast-VGen在多个指标上优于基线模型，特别是在长时间规划任务中表现显著提升。'}}}, {'id': 'https://huggingface.co/papers/2410.20050', 'title': 'AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels', 'url': 'https://huggingface.co/papers/2410.20050', 'abstract': 'Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenges due to the lack of relevance-labeled data. In this paper, we introduce a novel approach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to tackle this issue. SL-HyDE leverages large language models (LLMs) as generators to generate hypothetical documents based on a given query. These generated documents encapsulate key medical context, guiding a dense retriever in identifying the most relevant documents. The self-learning framework progressively refines both pseudo-document generation and retrieval, utilizing unlabeled medical corpora without requiring any relevance-labeled data. Additionally, we present the Chinese Medical Information Retrieval Benchmark (CMIRB), a comprehensive evaluation framework grounded in real-world medical scenarios, encompassing five tasks and ten datasets. By benchmarking ten models on CMIRB, we establish a rigorous standard for evaluating medical information retrieval systems. Experimental results demonstrate that SL-HyDE significantly surpasses existing methods in retrieval accuracy while showcasing strong generalization and scalability across various LLM and retriever configurations. CMIRB data and evaluation code are publicly available at: https://github.com/CMIRB-benchmark/CMIRB.', 'score': 7, 'issue_id': 347, 'pub_date': '2024-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '57721469df67a2f9', 'authors': ['Lei Li', 'Xiangxu Zhang', 'Xiao Zhou', 'Zheng Liu'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20050.jpg', 'data': {'categories': ['#science', '#rag', '#benchmark', '#multilingual', '#healthcare', '#dataset', '#open_source', '#low_resource'], 'emoji': '🩺', 'ru': {'title': 'Революция в медицинском поиске: SL-HyDE и CMIRB открывают новые горизонты', 'desc': 'Статья представляет новый подход к медицинскому информационному поиску под названием SL-HyDE. Этот метод использует большие языковые модели для генерации гипотетических документов на основе запроса, что помогает плотностному ретриверу находить наиболее релевантные документы. Авторы также представляют CMIRB - комплексную систему оценки для медицинского информационного поиска. Экспериментальные результаты показывают, что SL-HyDE значительно превосходит существующие методы по точности поиска.'}, 'en': {'title': 'Revolutionizing Medical Retrieval with Self-Learning Hypothetical Documents', 'desc': 'This paper addresses the challenges of zero-shot dense retrieval in medical information retrieval (MIR) due to the scarcity of labeled data. It introduces a new method called Self-Learning Hypothetical Document Embeddings (SL-HyDE), which uses large language models to create hypothetical documents that provide essential medical context for retrieval tasks. The self-learning approach refines the generation of these documents and the retrieval process using unlabeled medical data. Additionally, the authors present the Chinese Medical Information Retrieval Benchmark (CMIRB) to evaluate the performance of various models in real-world medical scenarios, demonstrating that SL-HyDE outperforms existing methods in accuracy and adaptability.'}, 'zh': {'title': '自学习假设文档嵌入：提升医学信息检索的有效性', 'desc': '医学信息检索（MIR）在从多种来源获取相关医学知识中至关重要，但在医学领域实现有效的零样本密集检索面临重大挑战，因为缺乏相关性标记的数据。本文提出了一种新方法，称为自学习假设文档嵌入（SL-HyDE），旨在解决这一问题。SL-HyDE利用大型语言模型（LLMs）生成基于给定查询的假设文档，这些文档包含关键的医学背景，帮助密集检索器识别最相关的文档。我们还提出了中国医学信息检索基准（CMIRB），为医学信息检索系统提供了一个全面的评估框架。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (38)', '#agents (38)', '#agi (21)', '#alignment (76)', '#architecture (285)', '#audio (28)', '#benchmark (223)', '#cv (155)', '#data (86)', '#dataset (150)', '#diffusion (86)', '#ethics (22)', '#games (68)', '#graphs (51)', '#hallucinations (17)', '#healthcare (24)', '#inference (83)', '#interpretability (65)', '#leakage (2)', '#long_context (46)', '#low_resource (17)', '#machine_translation (10)', '#math (39)', '#multilingual (43)', '#multimodal (124)', '#open_source (199)', '#optimization (250)', '#plp (25)', '#rag (29)', '#reasoning (127)', '#rl (58)', '#rlhf (40)', '#robotics (27)', '#science (46)', '#security (24)', '#small_models (63)', '#story_generation (2)', '#survey (19)', '#synthetic (110)', '#training (292)', '#transfer_learning (75)', '#video (69)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="${pdfImg}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-10-26 14:59',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-10-26 14:59')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-10-26 14:59')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    