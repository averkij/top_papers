
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 155 papers. October 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">ĞĞºÑ‚ÑĞ±Ñ€ÑŒ 2025</span> | <span id="title-articles-count">155 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-09.html">â¬…ï¸ <span id="prev-date">09.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-11.html">â¡ï¸ <span id="next-date">11.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'ĞĞºÑ‚ÑĞ±Ñ€ÑŒ 2025', 'en': 'October 2025', 'zh': '10æœˆ2025å¹´'};
        let feedDateNext = {'ru': '11.2025', 'en': '11/2025', 'zh': '11æœˆ2025å¹´'};
        let feedDatePrev = {'ru': '09.2025', 'en': '09/2025', 'zh': '9æœˆ2025å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.25454', 'title': 'DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search', 'url': 'https://huggingface.co/papers/2509.25454', 'abstract': 'DeepSearch integrates Monte Carlo Tree Search into RLVR training to enhance exploration and credit assignment, achieving state-of-the-art performance with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.', 'score': 96, 'issue_id': 6201, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': 'c81bb4fe47e669e8', 'authors': ['Fang Wu', 'Weihao Xuan', 'Heli Qi', 'Ximing Lu', 'Aaron Tu', 'Li Erran Li', 'Yejin Choi'], 'affiliations': ['Amazon AWS', 'RIKEN AIP', 'Stanford University', 'UC Berkeley', 'University of Tokyo', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2509.25454.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ¹ ÑĞ¸Ğ»Ñ‹: Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM', 'desc': 'DeepSearch Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Monte Carlo Tree Search Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‰ÑƒÑ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ñ… RLVR. DeepSearch Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑƒĞ·Ğ»Ğ¾Ğ², ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ±ÑƒÑ„ĞµÑ€ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 62.95% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 5.7 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing RLVR: Strategic Exploration with DeepSearch', 'desc': 'DeepSearch is a novel framework that enhances Reinforcement Learning with Value Regression (RLVR) by incorporating Monte Carlo Tree Search (MCTS) into the training process. This integration allows for better exploration of the solution space and improves credit assignment, addressing the common issue of training plateaus in existing RLVR methods. By employing a global frontier selection strategy and entropy-based guidance, DeepSearch systematically identifies and prioritizes promising reasoning paths. The framework achieves state-of-the-art performance on mathematical reasoning tasks while significantly reducing computational costs, demonstrating the effectiveness of strategic exploration in machine learning.'}, 'zh': {'title': 'æ·±åº¦æœç´¢ï¼šé€šè¿‡ç³»ç»Ÿæ¢ç´¢æå‡æ¨ç†èƒ½åŠ›', 'desc': 'DeepSearch æ˜¯ä¸€ç§å°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰é›†æˆåˆ°å¼ºåŒ–å­¦ä¹ ä»·å€¼å›å½’ï¼ˆRLVRï¼‰è®­ç»ƒä¸­çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºæ¢ç´¢èƒ½åŠ›å’Œä¿¡ç”¨åˆ†é…ã€‚å½“å‰çš„ RLVR æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨æ¢ç´¢ä¸è¶³çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½æå‡å‡ç¼“ã€‚DeepSearch é€šè¿‡åœ¨è®­ç»ƒå¾ªç¯ä¸­åµŒå…¥ç»“æ„åŒ–æœç´¢ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´ï¼Œä»è€Œè§£å†³äº†è¿™ä¸€ç“¶é¢ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepSearch åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† 62.95% çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå¹¶ä¸”ä½¿ç”¨çš„ GPU è®¡ç®—æ—¶é—´æ¯”ä¼ ç»Ÿæ–¹æ³•å°‘äº† 5.7 å€ï¼Œå±•ç¤ºäº†ç®—æ³•åˆ›æ–°åœ¨æå‡ RLVR æ–¹æ³•ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00406', 'title': 'VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified\n  Rewards in World Simulators', 'url': 'https://huggingface.co/papers/2510.00406', 'abstract': 'VLA-RFT uses a data-driven world model to fine-tune VLA models efficiently, reducing sample requirements and improving robustness under perturbations.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.', 'score': 51, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '8dead5a43b0cdc61', 'authors': ['Hengtao Li', 'Pengxiang Ding', 'Runze Suo', 'Yihao Wang', 'Zirui Ge', 'Dongyuan Zang', 'Kexian Yu', 'Mingyang Sun', 'Hongyin Zhang', 'Donglin Wang', 'Weihua Su'], 'affiliations': ['BUPT', 'Fudan University', 'Hebei University of Technology', 'OpenHelix Team', 'Westlake University', 'Zhejiang University', 'Zhengzhou University'], 'pdf_title_img': 'assets/pdf/title_img/2510.00406.jpg', 'data': {'categories': ['#rl', '#reasoning', '#training', '#optimization', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¸Ñ€-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VLA-RFT â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¸Ñ€Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ data-driven world model, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼ĞµĞ½ĞµĞµ 400 ÑˆĞ°Ğ³Ğ¾Ğ² fine-tuning Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ supervised baseline, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼. VLA-RFT Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² imitation learning Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº.'}, 'en': {'title': 'Enhancing VLA Models with Efficient Reinforcement Fine-Tuning', 'desc': "VLA-RFT is a framework that improves Vision-Language-Action (VLA) models by using a data-driven world model for reinforcement fine-tuning. This approach reduces the number of samples needed for training and enhances the model's ability to handle unexpected changes in the environment. By simulating future visual observations based on actions, it provides a more effective learning signal that aligns with the desired outcomes. The results show that VLA-RFT not only outperforms traditional supervised methods but also maintains strong performance even when conditions are altered."}, 'zh': {'title': 'åˆ©ç”¨ä¸–ç•Œæ¨¡å‹æå‡VLAæ¨¡å‹çš„é²æ£’æ€§ä¸æ•ˆç‡', 'desc': 'VLA-RFTæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¡†æ¶ï¼Œåˆ©ç”¨æ•°æ®é©±åŠ¨çš„ä¸–ç•Œæ¨¡å‹ä½œä¸ºå¯æ§æ¨¡æ‹Ÿå™¨ï¼Œä»è€Œæé«˜VLAæ¨¡å‹çš„æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡çœŸå®äº¤äº’æ•°æ®è®­ç»ƒï¼Œèƒ½å¤Ÿé¢„æµ‹åŸºäºåŠ¨ä½œçš„æœªæ¥è§†è§‰è§‚å¯Ÿï¼Œæä¾›å¯†é›†çš„è½¨è¿¹çº§å¥–åŠ±ä¿¡å·ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒVLA-RFTåœ¨æ ·æœ¬éœ€æ±‚ä¸Šå¤§å¹…é™ä½ï¼Œä¸”åœ¨å°‘äº400æ­¥çš„å¾®è°ƒåè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚å®ƒåœ¨æ‰°åŠ¨æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é²æ£’æ€§ï¼Œç¡®ä¿ä»»åŠ¡æ‰§è¡Œçš„ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01051', 'title': 'GEM: A Gym for Agentic LLMs', 'url': 'https://huggingface.co/papers/2510.01051', 'abstract': 'GEM, an open-source environment simulator, facilitates experience-based learning for large language models by providing a standardized framework and diverse environments for training and benchmarking reinforcement learning algorithms.  \t\t\t\t\tAI-generated summary \t\t\t\t The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.', 'score': 50, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '6d69bb75ee0b2258', 'authors': ['Zichen Liu', 'Anya Sims', 'Keyu Duan', 'Changyu Chen', 'Simon Yu', 'Xiangxin Zhou', 'Haotian Xu', 'Shaopan Xiong', 'Bo Liu', 'Chenmien Tan', 'Chuen Yang Beh', 'Weixun Wang', 'Hao Zhu', 'Weiyan Shi', 'Diyi Yang', 'Michael Shieh', 'Yee Whye Teh', 'Wee Sun Lee', 'Min Lin'], 'affiliations': ['NUS', 'Northeastern', 'OpenRLHF', 'Oxford', 'RL2', 'ROLL', 'SMU', 'Sea AI Lab', 'Stanford'], 'pdf_title_img': 'assets/pdf/title_img/2510.01051.jpg', 'data': {'categories': ['#rl', '#open_source', '#benchmark', '#training', '#games', '#agents'], 'emoji': 'ğŸ®', 'ru': {'title': 'GEM: ÑĞ¿Ğ¾Ñ€Ñ‚Ğ·Ğ°Ğ» Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· reinforcement learning', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ GEM (General Experience Maker) â€” Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑÑ€ĞµĞ´Ğ°-ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼. Ğ­Ñ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³ OpenAI Gym, Ğ½Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑĞ¿Ğ¾Ñ…Ğ¸ LLM, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ ÑÑ€ĞµĞ´Ğ¾Ğ¹ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. GEM Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 24 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° REINFORCE Ñ Return Batch Normalization (ReBN), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ credit assignment Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GRPO. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… RL-Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² (PPO, GRPO, REINFORCE) Ğ¸ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ GEM ĞºĞ°Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'GEM: Empowering LLMs with Experience-Based Learning', 'desc': 'GEM (General Experience Maker) is an open-source simulator designed to enhance experience-based learning for large language models (LLMs) by providing a standardized framework for training and benchmarking reinforcement learning (RL) algorithms. It allows agents to learn by interacting with various complex environments, moving away from static datasets. GEM includes features like asynchronous vectorized execution for efficient processing and flexible wrappers for easy customization. Additionally, it offers a suite of environments and tools for evaluating different RL algorithms, aiming to accelerate research in agentic LLMs.'}, 'zh': {'title': 'GEMï¼šåŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„ç»éªŒå­¦ä¹ ', 'desc': 'GEMï¼ˆé€šç”¨ç»éªŒç”Ÿæˆå™¨ï¼‰æ˜¯ä¸€ä¸ªå¼€æºç¯å¢ƒæ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›åŸºäºç»éªŒçš„å­¦ä¹ ä½“éªŒã€‚å®ƒä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•æä¾›äº†æ ‡å‡†åŒ–æ¡†æ¶å’Œå¤šæ ·åŒ–ç¯å¢ƒï¼Œç±»ä¼¼äºä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ä¸­çš„OpenAI-Gymã€‚GEMæ”¯æŒå¼‚æ­¥å‘é‡åŒ–æ‰§è¡Œï¼Œå…·æœ‰é«˜ååé‡ï¼Œå¹¶æä¾›çµæ´»çš„åŒ…è£…å™¨ä»¥ä¾¿äºæ‰©å±•ã€‚æ­¤å¤–ï¼ŒGEMè¿˜åŒ…å«å¤šç§ç¯å¢ƒã€å¼ºå¤§çš„é›†æˆå·¥å…·å’Œç¤ºä¾‹è„šæœ¬ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜åŠ é€Ÿæœªæ¥çš„æ™ºèƒ½ä½“è¯­è¨€æ¨¡å‹ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25849', 'title': 'Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget\n  Allocation', 'url': 'https://huggingface.co/papers/2509.25849', 'abstract': 'An adaptive exploration budget allocation method for reinforcement learning in Large Language Models improves training efficiency and performance on mathematical reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task\'s exploration as an "item" with a distinct "value" and "cost", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model\'s current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational "free lunch", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.', 'score': 29, 'issue_id': 6201, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '7897a4d3a9007e61', 'authors': ['Ziniu Li', 'Congliang Chen', 'Tianyun Yang', 'Tian Ding', 'Ruoyu Sun', 'Ge Zhang', 'Wenhao Huang', 'Zhi-Quan Luo'], 'affiliations': ['ByteDance Seed', 'Shenzhen Research Institute of Big Data', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2509.25849.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ’', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° Ğ²ÑĞµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¸Ñ… Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº, ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞµ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ ĞºĞ°Ğº ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¾ Ñ€ÑĞºĞ·Ğ°ĞºĞµ, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¸Ğ¼ĞµĞµÑ‚ ÑĞ²Ğ¾Ñ Â«Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÂ» Ğ¸ Â«ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÂ». ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ´Ğ¾Ğ»Ñ Ğ½ĞµĞ½ÑƒĞ»ĞµĞ²Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° 20-40% Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ½Ğ° 2-9 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑ… Ğ¶Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Smart Budgeting for Smarter Learning', 'desc': 'This paper presents a new method for allocating exploration budgets in reinforcement learning, specifically for Large Language Models (LLMs). The authors identify that traditional uniform budget allocation leads to inefficiencies, where easy tasks succeed while difficult ones fail, resulting in zero gradients during training. By framing the exploration budget allocation as a knapsack problem, they develop an adaptive strategy that optimally distributes resources based on the learning status of each task. This approach significantly enhances training efficiency, increasing non-zero policy gradients by 20-40% and improving performance on mathematical reasoning tasks without requiring additional computational resources.'}, 'zh': {'title': 'è‡ªé€‚åº”æ¢ç´¢é¢„ç®—ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”çš„æ¢ç´¢é¢„ç®—åˆ†é…æ–¹æ³•ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ•°å­¦æ¨ç†åŸºå‡†çš„æ€§èƒ½ã€‚ä¼ ç»Ÿæ–¹æ³•åœ¨æ¯ä¸ªä»»åŠ¡ä¸Šå‡åŒ€åˆ†é…æœ‰é™çš„æ¢ç´¢é¢„ç®—ï¼Œå¯¼è‡´ç®€å•ä»»åŠ¡æ€»æ˜¯æˆåŠŸè€Œå›°éš¾ä»»åŠ¡æ€»æ˜¯å¤±è´¥ï¼Œé€ æˆè®­ç»ƒæ›´æ–°æ—¶æ¢¯åº¦ä¸ºé›¶çš„é—®é¢˜ã€‚æˆ‘ä»¬å°†æ¯ä¸ªä»»åŠ¡çš„æ¢ç´¢è§†ä¸ºå…·æœ‰ä¸åŒâ€œä»·å€¼â€å’Œâ€œæˆæœ¬â€çš„â€œç‰©å“â€ï¼Œå¹¶ä¸ç»å…¸çš„èƒŒåŒ…é—®é¢˜å»ºç«‹è”ç³»ï¼Œä»è€Œæ¨å¯¼å‡ºä¸€ç§æœ€ä½³åˆ†é…è§„åˆ™ã€‚é€šè¿‡å°†èµ„æºåŠ¨æ€åˆ†é…åˆ°å­¦ä¹ æ•ˆæœæœ€æ˜¾è‘—çš„ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒä¸­æœ‰æ•ˆåœ°æé«˜äº†éé›¶ç­–ç•¥æ¢¯åº¦çš„æ¯”ä¾‹ï¼Œå¹¶åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25455', 'title': 'PIPer: On-Device Environment Setup via Online Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.25455', 'abstract': 'A specialized model combining supervised fine-tuning and Reinforcement Learning with Verifiable Rewards achieves competitive performance in automated environment setup tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: https://github.com/JetBrains-Research/PIPer.', 'score': 25, 'issue_id': 6205, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '08f51f097cf06715', 'authors': ['Alexander Kovrigin', 'Aleksandra Eliseeva', 'Konstantin Grotov', 'Egor Bogomolov', 'Yaroslav Zharov'], 'affiliations': ['Constructor University', 'Delft University of Technology', 'JetBrains Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.25455.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#open_source', '#small_models'], 'emoji': 'âš™ï¸', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ - ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ supervised fine-tuning Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Bash-ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Reinforcement Learning with Verifiable Rewards (RLVR) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ setup Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen3-8B, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¼ hardware, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° GPT-4o. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ EnvBench-Python Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Automating Environment Setup with Specialized ML Models', 'desc': 'This paper presents a specialized model that enhances automated environment setup tasks in software engineering by combining supervised fine-tuning and Reinforcement Learning with Verifiable Rewards (RLVR). The model is designed to generate accurate Bash scripts, addressing the limitations of existing Large Language Models (LLMs) in this domain. By utilizing RLVR, the model adapts effectively to the specific requirements of environment setup, achieving competitive performance on the EnvBench-Python benchmark. Notably, the Qwen3-8B model, which can run on consumer hardware, matches the performance of larger models like Qwen3-32B and GPT-4o.'}, 'zh': {'title': 'ä¸“é—¨æ¨¡å‹åŠ©åŠ›è‡ªåŠ¨åŒ–ç¯å¢ƒé…ç½®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸“é—¨çš„æ¨¡å‹ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼ŒæˆåŠŸè§£å†³äº†è‡ªåŠ¨åŒ–ç¯å¢ƒé…ç½®ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚ç¯å¢ƒé…ç½®æ˜¯è½¯ä»¶å·¥ç¨‹ä¸­çš„ä¸€ä¸ªé‡è¦ç¯èŠ‚ï¼Œè‡ªåŠ¨åŒ–æ–¹æ³•å¯ä»¥å¸®åŠ©å¼€å‘è€…å¿«é€Ÿé…ç½®ç¯å¢ƒï¼Œå‡å°‘æ‰‹åŠ¨æ“ä½œã€‚å°½ç®¡ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°æœ‰é™ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•ä½¿å¾—Qwen3-8Bæ¨¡å‹åœ¨EnvBench-Pythonä¸Šä¸æ›´å¤§æ¨¡å‹çš„è¡¨ç°ç›¸å½“ã€‚è¯¥ç ”ç©¶çš„ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å·²åœ¨çº¿å‘å¸ƒï¼Œä¾›ç ”ç©¶è€…ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.22944', 'title': 'SINQ: Sinkhorn-Normalized Quantization for Calibration-Free\n  Low-Precision LLM Weights', 'url': 'https://huggingface.co/papers/2509.22944', 'abstract': 'SINQ enhances post-training quantization by introducing a second-axis scale factor and Sinkhorn-Knopp-style algorithm to minimize matrix imbalance, improving perplexity on large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ.', 'score': 20, 'issue_id': 6211, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': 'c5cfc389d45892b9', 'authors': ['Lorenz K. MÃ¼ller', 'Philippe Bich', 'Jiawei Zhuang', 'Ahmet Ã‡elik', 'Luca Benfenati', 'Lukas Cavigelli'], 'affiliations': ['Computing Systems Lab, Huawei Zurich Research Center'], 'pdf_title_img': 'assets/pdf/title_img/2509.22944.jpg', 'data': {'categories': ['#inference', '#training', '#optimization'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…Ğ¾ÑĞµĞ²Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SINQ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ Ğ¡Ğ¸Ğ½ĞºÑ…Ğ¾Ñ€Ğ½Ğ°-ĞšĞ½Ğ¾Ğ¿Ğ¿Ğ° Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¹ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ°Ğ¼ Ğ¸ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°Ğ¼ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ²ĞµÑĞ¾Ğ². Ğ­Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² (outliers), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 4 Ğ±Ğ¸Ñ‚Ğ°Ñ… Ğ¸ Ğ½Ğ¸Ğ¶Ğµ, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ perplexity Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen3 Ğ¸ DeepSeek-V2.5 Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ»ĞµĞ³ĞºĞ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğº Ğ»ÑĞ±Ñ‹Ğ¼ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾ÑĞ¼.'}, 'en': {'title': 'SINQ: Enhancing Quantization for Better Language Model Performance', 'desc': "The paper presents SINQ, a novel approach to enhance post-training quantization for large language models. It introduces a second-axis scale factor and a Sinkhorn-Knopp-style algorithm to address matrix imbalance, which improves the model's perplexity at lower bit-widths. This method allows for better representation of outliers and minimizes precision issues in quantized parameters. SINQ is easy to implement across different architectures and shows significant improvements in performance on benchmark datasets compared to traditional uniform quantization methods."}, 'zh': {'title': 'SINQï¼šæå‡é‡åŒ–æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'SINQæ˜¯ä¸€ç§å¢å¼ºåè®­ç»ƒé‡åŒ–çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç¬¬äºŒè½´ç¼©æ”¾å› å­å’ŒSinkhorn-Knoppé£æ ¼çš„ç®—æ³•æ¥æœ€å°åŒ–çŸ©é˜µä¸å¹³è¡¡ï¼Œä»è€Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å›°æƒ‘åº¦ã€‚ç°æœ‰çš„é‡åŒ–æ–¹æ³•åœ¨ä½äºæˆ–ç­‰äº4ä½å®½æ—¶ï¼Œå¸¸å¸¸ä¼šå‡ºç°å›°æƒ‘åº¦ä¸‹é™çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ— æ ¡å‡†çš„å‡åŒ€é‡åŒ–æ–¹æ³•ä¸­ã€‚SINQé€šè¿‡å¯¹æ¯è¡Œå’Œæ¯åˆ—çš„æ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¼˜åŒ–äº†é‡åŒ–è¿‡ç¨‹ï¼Œè§£å†³äº†å‚æ•°ç²¾åº¦é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒSINQåœ¨å¤šä¸ªæ¨¡å‹ä¸Šæ˜¾è‘—æé«˜äº†å›°æƒ‘åº¦ï¼Œå¹¶ä¸”å¯ä»¥ä¸æ ¡å‡†å’Œéå‡åŒ€é‡åŒ–ç»“åˆä½¿ç”¨ä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00615', 'title': 'ACON: Optimizing Context Compression for Long-horizon LLM Agents', 'url': 'https://huggingface.co/papers/2510.00615', 'abstract': 'Agent Context Optimization (ACON) compresses context in large language models for efficient long-horizon tasks by analyzing failure cases and distilling the compressor into smaller models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.', 'score': 19, 'issue_id': 6203, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'a32bcdd471ec0b1c', 'authors': ['Minki Kang', 'Wei-Ning Chen', 'Dongge Han', 'Huseyin A. Inan', 'Lukas Wutschitz', 'Yanzhi Chen', 'Robert Sim', 'Saravan Rajmohan'], 'affiliations': ['KAIST', 'Microsoft', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2510.00615.jpg', 'data': {'categories': ['#agents', '#small_models', '#optimization', '#long_context', '#inference', '#dataset'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'ACON: Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Agent Context Optimization (ACON) â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… ĞºĞ°Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ¶Ğ°Ñ‚Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼, Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… LLM. ACON Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¾Ñ€ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 26-54% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¾ 46%.'}, 'en': {'title': 'Efficient Context Compression for Long-Horizon Tasks', 'desc': 'Agent Context Optimization (ACON) is a method designed to improve the efficiency of large language models (LLMs) when handling long-term tasks by compressing the context they use. It identifies and analyzes failure cases where compressed context leads to poor performance, allowing the model to learn and refine its compression strategies. ACON not only optimizes the way observations and action histories are condensed but also distills this knowledge into smaller models, making them more efficient. Experiments demonstrate that ACON significantly reduces memory usage while maintaining high accuracy, thus enhancing the performance of smaller models in long-horizon tasks.'}, 'zh': {'title': 'é«˜æ•ˆå‹ç¼©ï¼Œæå‡é•¿æ—¶é—´ä»»åŠ¡è¡¨ç°çš„ä»£ç†ä¸Šä¸‹æ–‡ä¼˜åŒ–', 'desc': 'ä»£ç†ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆACONï¼‰æ˜¯ä¸€ç§å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸Šä¸‹æ–‡çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜é•¿æ—¶é—´ä»»åŠ¡çš„æ•ˆç‡ã€‚é€šè¿‡åˆ†æå¤±è´¥æ¡ˆä¾‹ï¼ŒACONèƒ½å¤Ÿæç‚¼å‡ºæ›´å°çš„æ¨¡å‹ï¼Œä»è€Œä¼˜åŒ–ç¯å¢ƒè§‚å¯Ÿå’Œäº¤äº’å†å²çš„å‹ç¼©ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€ç©ºé—´ä¸­çš„å‹ç¼©æŒ‡å¯¼ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨æˆåŠŸçš„å®Œæ•´ä¸Šä¸‹æ–‡å’Œå¤±è´¥çš„å‹ç¼©ä¸Šä¸‹æ–‡ä¹‹é—´è¿›è¡Œæœ‰æ•ˆåˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒACONåœ¨å‡å°‘å†…å­˜ä½¿ç”¨çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒä»»åŠ¡æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æå‡å°å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01174', 'title': 'Code2Video: A Code-centric Paradigm for Educational Video Generation', 'url': 'https://huggingface.co/papers/2510.01174', 'abstract': 'Code2Video generates educational videos using a code-centric agent framework, improving coherence and interpretability compared to direct code generation.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.', 'score': 18, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '26c2c9dd6c370251', 'authors': ['Yanzhe Chen', 'Kevin Qinghong Lin', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2510.01174.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#video', '#games', '#optimization', '#agents', '#dataset'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´', 'desc': 'Code2Video â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ Python-ĞºĞ¾Ğ´. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ… ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ»ĞµĞºÑ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ´ Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºÑƒ. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° TeachQuiz, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ LLM Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 40% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Code2Video: Crafting Coherent Educational Videos with Code', 'desc': 'Code2Video is a framework designed to create educational videos using a code-centric approach, which enhances coherence and interpretability compared to traditional methods. It consists of three main agents: the Planner organizes content into logical sequences, the Coder translates these sequences into executable Python code, and the Critic refines the visual layout using vision-language models. This method addresses the challenges of generating professional educational videos by allowing precise control over visual elements and transitions. The framework has shown a significant improvement in video quality and coherence, outperforming direct code generation by 40%.'}, 'zh': {'title': 'Code2Videoï¼šæ•™è‚²è§†é¢‘ç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'Code2Video æ˜¯ä¸€ä¸ªåŸºäºä»£ç çš„ä»£ç†æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆæ•™è‚²è§†é¢‘ï¼Œæå‡äº†è§†é¢‘çš„ä¸€è‡´æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªåä½œä»£ç†ï¼šè§„åˆ’è€…è´Ÿè´£å°†è®²åº§å†…å®¹ç»“æ„åŒ–å¹¶å‡†å¤‡è§†è§‰èµ„äº§ï¼›ç¼–ç å™¨å°†ç»“æ„åŒ–æŒ‡ä»¤è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„ Python ä»£ç ï¼Œå¹¶é€šè¿‡èŒƒå›´å¼•å¯¼è‡ªåŠ¨ä¿®å¤æé«˜æ•ˆç‡ï¼›è¯„è®ºè€…åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¼˜åŒ–ç©ºé—´å¸ƒå±€ï¼Œç¡®ä¿æ¸…æ™°åº¦ã€‚é€šè¿‡å»ºç«‹ä¸“ä¸šåˆ¶ä½œçš„æ•™è‚²è§†é¢‘åŸºå‡†MMMCï¼Œæˆ‘ä»¬è¯„ä¼°äº†Code2Videoåœ¨ç¾å­¦è¯„åˆ†ã€ä»£ç æ•ˆç‡å’ŒçŸ¥è¯†æ¢å¤ç­‰å¤šä¸ªç»´åº¦çš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨è§†é¢‘ç”Ÿæˆä¸Šä¼˜äºç›´æ¥ä»£ç ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00977', 'title': 'It Takes Two: Your GRPO Is Secretly DPO', 'url': 'https://huggingface.co/papers/2510.00977', 'abstract': "Reframing Group Relative Policy Optimization as contrastive learning reveals its connection to Direct Preference Optimization, enabling minimal two-rollout GRPO to achieve performance comparable to larger group sizes with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) is a prominent reinforcement learning algorithm for post-training Large Language Models (LLMs). It is commonly believed that GRPO necessitates a large group size to ensure stable training via precise statistical estimation, which incurs substantial computational overhead. In this work, we challenge this assumption by reframing GRPO as a form of contrastive learning, which reveals a fundamental connection to Direct Preference Optimization (DPO). Motivated by DPO's empirical success, we investigate the minimal two-rollout case (2-GRPO), a configuration previously deemed infeasible. We provide a rigorous theoretical analysis to validate 2-GRPO and demonstrate empirically that it achieves performance on par with 16-GRPO, despite using only 1/8 of the rollouts and reducing training time by over 70%.", 'score': 14, 'issue_id': 6210, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '64fa1e8d41c1a12f', 'authors': ['Yihong Wu', 'Liheng Ma', 'Lei Ding', 'Muzhi Li', 'Xinyu Wang', 'Kejia Chen', 'Zhan Su', 'Zhanguang Zhang', 'Chenyang Huang', 'Yingxue Zhang', 'Mark Coates', 'Jian-Yun Nie'], 'affiliations': ['Alberta Machine Intelligence Institute (Amii)', 'Huawei Noahs Ark Lab', 'McGill University', 'Mila - Quebec AI Institute', 'The Chinese University of Hong Kong', 'Universite de Montreal', 'University of Alberta', 'University of Manitoba', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.00977.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rlhf', '#rl'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ”Ğ²Ğ° Ñ€Ğ¾Ğ»Ğ»Ğ°ÑƒÑ‚Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑˆĞµÑÑ‚Ğ½Ğ°Ğ´Ñ†Ğ°Ñ‚Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Group Relative Policy Optimization (GRPO), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GRPO Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Direct Preference Optimization (DPO). ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ñ€Ğ¾Ğ»Ğ»Ğ°ÑƒÑ‚Ğ°Ğ¼Ğ¸ (2-GRPO) Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ‚Ğ°Ğº Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾, ĞºĞ°Ğº Ğ²ĞµÑ€ÑĞ¸Ñ Ñ 16 Ñ€Ğ¾Ğ»Ğ»Ğ°ÑƒÑ‚Ğ°Ğ¼Ğ¸. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ÑÑ Ğ² 8 Ñ€Ğ°Ğ·, Ğ° Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 70%.'}, 'en': {'title': 'Efficient Learning: Small Rollouts, Big Gains!', 'desc': 'This paper explores Group Relative Policy Optimization (GRPO), a reinforcement learning method used for training Large Language Models (LLMs). It challenges the traditional belief that GRPO requires large group sizes for effective training, which leads to high computational costs. By reframing GRPO as a contrastive learning approach, the authors connect it to Direct Preference Optimization (DPO). They introduce a minimal two-rollout version of GRPO (2-GRPO), showing that it can achieve similar performance to larger configurations while significantly reducing the number of rollouts and training time.'}, 'zh': {'title': 'ä¼˜åŒ–ç­–ç•¥ï¼Œå‡å°‘è®¡ç®—æˆæœ¬ï¼', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒé˜¶æ®µã€‚æˆ‘ä»¬æå‡ºå°†GRPOé‡æ–°æ¡†æ¶ä¸ºå¯¹æ¯”å­¦ä¹ çš„å½¢å¼ï¼Œä»è€Œæ­ç¤ºå…¶ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¹‹é—´çš„åŸºæœ¬è”ç³»ã€‚é€šè¿‡ç ”ç©¶æœ€å°çš„ä¸¤æ¬¡å›åˆæƒ…å†µï¼ˆ2-GRPOï¼‰ï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ä¸€é…ç½®çš„å¯è¡Œæ€§ï¼Œå¹¶æä¾›äº†ç†è®ºåˆ†ææ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ2-GRPOçš„æ€§èƒ½ä¸16-GRPOç›¸å½“ï¼Œä½†æ‰€éœ€çš„å›åˆæ•°ä»…ä¸ºå…¶å…«åˆ†ä¹‹ä¸€ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘è¶…è¿‡70%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00184', 'title': "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls", 'url': 'https://huggingface.co/papers/2510.00184', 'abstract': "Reverse-engineering a model that learns multi-digit multiplication via implicit chain-of-thought reveals that it uses attention to encode long-range dependencies and represents partial products efficiently, insights that help address limitations in standard fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via implicit chain-of-thought, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.", 'score': 13, 'issue_id': 6198, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '27d7ce536d31aa04', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#training', '#architecture', '#long_context', '#reasoning'], 'emoji': 'ğŸ”¢', 'ru': {'title': 'ĞšĞ°Ğº Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ ÑƒĞ¼Ğ½Ğ¾Ğ¶Ğ°Ñ‚ÑŒ: Ñ€Ğ°Ğ·Ğ³Ğ°Ğ´ĞºĞ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°ÑƒÑ‡Ğ¸Ğ»Ğ°ÑÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾Ğ¼Ñƒ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµÑĞ²Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought). ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ attention Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ°, ĞºÑÑˆĞ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ°Ğ·Ğ¸Ñ Ğ¤ÑƒÑ€ÑŒĞµ Ğ¸ ÑÑƒĞ¼Ğ¼Ñ‹ ĞœĞ¸Ğ½ĞºĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾. Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ fine-tuning Ğ·Ğ°ÑÑ‚Ñ€ĞµĞ²Ğ°ĞµÑ‚ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ğµ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ»Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ñ€ÑĞ´Ğ°Ğ¼Ğ¸ Ñ‡Ğ¸ÑĞµĞ». Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑƒĞ¼Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾ÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking Multi-Digit Multiplication with Attention and Inductive Bias', 'desc': "This paper investigates how a model learns to perform multi-digit multiplication using an implicit chain-of-thought approach. It reveals that the model effectively encodes long-range dependencies through attention mechanisms, allowing it to manage partial products efficiently. The authors demonstrate that standard fine-tuning methods often fail to capture these dependencies, leading to suboptimal performance. By introducing an auxiliary loss that predicts running sums, they provide a solution to enhance learning dynamics and improve the model's ability to handle complex multiplication tasks."}, 'zh': {'title': 'æ­ç¤ºå¤šä½æ•°ä¹˜æ³•å­¦ä¹ çš„å…³é”®æœºåˆ¶', 'desc': 'æœ¬ç ”ç©¶é€šè¿‡é€†å‘å·¥ç¨‹ä¸€ä¸ªæˆåŠŸå­¦ä¹ å¤šä½æ•°ä¹˜æ³•çš„æ¨¡å‹ï¼Œæ­ç¤ºäº†å…¶ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ç¼–ç é•¿è·ç¦»ä¾èµ–å…³ç³»çš„æ–¹å¼ã€‚ç ”ç©¶å‘ç°ï¼Œè¯¥æ¨¡å‹é€šè¿‡æ„å»ºæœ‰å‘æ— ç¯å›¾æ¥ç¼“å­˜å’Œæ£€ç´¢æˆå¯¹çš„éƒ¨åˆ†ç§¯ï¼Œä»è€Œæœ‰æ•ˆåœ°è¡¨ç¤ºéƒ¨åˆ†ç§¯ã€‚æ¨¡å‹åœ¨æ³¨æ„åŠ›å¤´ä¸­é€šè¿‡å½¢æˆé—µå¯å¤«æ–¯åŸºå’Œæ¥å®ç°éƒ¨åˆ†ç§¯ï¼Œå¹¶ä½¿ç”¨å‚…é‡Œå¶åŸºè¡¨ç¤ºæ•°å­—ï¼Œè¿™äº›éƒ½æ˜¯æ ‡å‡†å¾®è°ƒæ¨¡å‹æ‰€ç¼ºä¹çš„ç›´è§‚ä¸”é«˜æ•ˆçš„è¡¨ç¤ºæ–¹å¼ã€‚é€šè¿‡å¼•å…¥è¾…åŠ©æŸå¤±æ¥é¢„æµ‹â€œè¿è¡Œå’Œâ€ï¼Œæˆ‘ä»¬ä¸ºæ¨¡å‹æä¾›äº†ä¸€ä¸ªå½’çº³åç½®ï¼Œä½¿å…¶èƒ½å¤ŸæˆåŠŸå­¦ä¹ å¤šä½æ•°ä¹˜æ³•ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2510.00232', 'title': 'BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model\n  Responses', 'url': 'https://huggingface.co/papers/2510.00232', 'abstract': "BiasFreeBench evaluates bias mitigation techniques in large language models using a unified benchmark and response-level metric to ensure fair and safe outputs in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.", 'score': 12, 'issue_id': 6198, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '7a6ed8974cc83369', 'authors': ['Xin Xu', 'Xunzhi He', 'Churan Zhi', 'Ruizhe Chen', 'Julian McAuley', 'Zexue He'], 'affiliations': ['Columbia University', 'MIT-IBM Watson Lab', 'UC San Diego', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.00232.jpg', 'data': {'categories': ['#ethics', '#dataset', '#benchmark'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ñ‡ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ÑŒÑ Ğ² LLM', 'desc': 'Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ¸Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ BiasFreeBench â€” ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ bias (Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Bias-Free Score, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°Ğ½Ñ‚Ğ¸ÑÑ‚ĞµÑ€ĞµĞ¾Ñ‚Ğ¸Ğ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² LLM. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ (Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³ vs Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ), Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unifying Bias Mitigation Evaluation for Safer AI Outputs', 'desc': 'BiasFreeBench is a new benchmark designed to evaluate bias mitigation techniques in large language models (LLMs). It addresses the inconsistency in previous studies by providing a unified framework for comparing various debiasing methods. The benchmark includes a novel response-level metric called Bias-Free Score, which assesses the fairness and safety of model outputs in real-world scenarios. By systematically analyzing different debiasing strategies, BiasFreeBench aims to enhance the reliability of LLMs in producing equitable and safe responses.'}, 'zh': {'title': 'ç»Ÿä¸€è¯„ä¼°åè§ç¼“è§£æŠ€æœ¯çš„åŸºå‡†å·¥å…·', 'desc': 'BiasFreeBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åè§ç¼“è§£æŠ€æœ¯çš„åŸºå‡†å·¥å…·ï¼Œæ—¨åœ¨ç¡®ä¿æ¨¡å‹è¾“å‡ºåœ¨ç°å®åœºæ™¯ä¸­å…¬å¹³å’Œå®‰å…¨ã€‚è¯¥ç ”ç©¶é€šè¿‡ç»Ÿä¸€çš„æŸ¥è¯¢-å“åº”è®¾ç½®ï¼Œæ¯”è¾ƒäº†å…«ç§ä¸»æµçš„åè§ç¼“è§£æ–¹æ³•ï¼ŒåŒ…æ‹¬å››ç§åŸºäºæç¤ºå’Œå››ç§åŸºäºè®­ç»ƒçš„æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å“åº”çº§åˆ«æŒ‡æ ‡â€”â€”æ— ååˆ†æ•°ï¼Œæ¥è¡¡é‡æ¨¡å‹å“åº”çš„å…¬å¹³æ€§ã€å®‰å…¨æ€§å’Œååˆ»æ¿å°è±¡ç¨‹åº¦ã€‚è¯¥åŸºå‡†çš„å‘å¸ƒå°†ä¸ºåè§ç¼“è§£ç ”ç©¶æä¾›ä¸€ä¸ªç»Ÿä¸€çš„æµ‹è¯•å¹³å°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00967', 'title': 'QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via\n  Agentic RL', 'url': 'https://huggingface.co/papers/2510.00967', 'abstract': 'QUASAR, an RL framework using tool-augmented LLMs, improves quantum circuit generation and optimization through verification and hierarchical rewards, achieving high validity compared to industrial LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Designing and optimizing task-specific quantum circuits are crucial to leverage the advantage of quantum computing. Recent large language model (LLM)-based quantum circuit generation has emerged as a promising automatic solution. However, the fundamental challenges remain unaddressed: (i) parameterized quantum gates require precise numerical values for optimal performance, which also depend on multiple aspects, including the number of quantum gates, their parameters, and the layout/depth of the circuits. (ii) LLMs often generate low-quality or incorrect quantum circuits due to the lack of quantum domain-specific knowledge. We propose QUASAR, an agentic reinforcement learning (RL) framework for quantum circuits generation and optimization based on tool-augmented LLMs. To align the LLM with quantum-specific knowledge and improve the generated quantum circuits, QUASAR designs (i) a quantum circuit verification approach with external quantum simulators and (ii) a sophisticated hierarchical reward mechanism in RL training. Extensive evaluation shows improvements in both syntax and semantic performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10, outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several supervised-fine-tuning (SFT)-only and RL-only baselines.', 'score': 10, 'issue_id': 6211, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '81aeacbb43532795', 'authors': ['Cong Yu', 'Valter Uotila', 'Shilong Deng', 'Qingyuan Wu', 'Tuo Shi', 'Songlin Jiang', 'Lei You', 'Bo Zhao'], 'affiliations': ['Aalto University', 'Technical University of Denmark', 'University of Helsinki', 'University of Liverpool', 'University of Southampton'], 'pdf_title_img': 'assets/pdf/title_img/2510.00967.jpg', 'data': {'categories': ['#agi', '#training', '#rl', '#agents', '#optimization'], 'emoji': 'âš›ï¸', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ RL-Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ QUASAR â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning Ğ¸ LLM Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑÑ…ĞµĞ¼. Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ LLM Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸ÑÑ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ğ³ĞµĞ¹Ñ‚Ğ¾Ğ². QUASAR Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑÑ…ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¿Ñ€Ğ¸ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ 99.31% Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ GPT-4o, GPT-5 Ğ¸ DeepSeek-V3.'}, 'en': {'title': 'QUASAR: Revolutionizing Quantum Circuit Generation with Reinforcement Learning', 'desc': 'QUASAR is a reinforcement learning framework that enhances the generation and optimization of quantum circuits using tool-augmented large language models (LLMs). It addresses key challenges in quantum circuit design, such as the need for precise parameters and the limitations of LLMs in understanding quantum-specific knowledge. By implementing a verification method with quantum simulators and a hierarchical reward system, QUASAR significantly improves the quality of generated circuits. The framework demonstrates exceptional performance, achieving a validity rate of 99.31% in Pass@1 and 100% in Pass@10, surpassing existing industrial LLMs.'}, 'zh': {'title': 'QUASARï¼šé‡å­ç”µè·¯ç”Ÿæˆä¸ä¼˜åŒ–çš„æ–°çªç ´', 'desc': 'QUASARæ˜¯ä¸€ä¸ªåŸºäºå·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›é‡å­ç”µè·¯çš„ç”Ÿæˆå’Œä¼˜åŒ–ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤–éƒ¨é‡å­æ¨¡æ‹Ÿå™¨è¿›è¡Œé‡å­ç”µè·¯éªŒè¯ï¼Œå¹¶è®¾è®¡äº†å¤æ‚çš„å±‚æ¬¡å¥–åŠ±æœºåˆ¶ï¼Œä»¥æé«˜ç”Ÿæˆç”µè·¯çš„è´¨é‡ã€‚QUASARåœ¨ç”Ÿæˆçš„é‡å­ç”µè·¯çš„è¯­æ³•å’Œè¯­ä¹‰æ€§èƒ½ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ï¼ŒéªŒè¯ç‡è¾¾åˆ°99.31%ã€‚ä¸å·¥ä¸šçº§LLMå¦‚GPT-4oã€GPT-5å’ŒDeepSeek-V3ç›¸æ¯”ï¼ŒQUASARè¡¨ç°å‡ºæ›´é«˜çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26346', 'title': 'EditReward: A Human-Aligned Reward Model for Instruction-Guided Image\n  Editing', 'url': 'https://huggingface.co/papers/2509.26346', 'abstract': "A new reward model, trained with a large-scale human preference dataset, improves instruction-guided image editing by selecting high-quality training data and achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \\mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \\mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \\mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \\mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \\mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \\mname with its training dataset will be released to help the community build more high-quality image editing training datasets.", 'score': 10, 'issue_id': 6211, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'abdf752d5e345000', 'authors': ['Keming Wu', 'Sicong Jiang', 'Max Ku', 'Ping Nie', 'Minghao Liu', 'Wenhu Chen'], 'affiliations': ['2077AI', 'Independent', 'McGill University', 'Tsinghua University', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2509.26346.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#alignment', '#rlhf', '#data', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Reward-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ reward-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 200 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GenAI-Bench, AURORA-Bench Ğ¸ ImagenHub, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ VLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ¾Ğ¹ reward-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ· Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° ShareGPT-4o-Image Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Step1X-Edit, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ reinforcement learning Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Image Editing with a Human-Aligned Reward Model', 'desc': 'This paper introduces a new reward model called \\mname, which is trained on a large-scale dataset of human preferences to enhance instruction-guided image editing. The model addresses the challenge of selecting high-quality training data, which has been a limitation for open-source models compared to their closed-source counterparts. By demonstrating superior alignment with human preferences, \\mname achieves state-of-the-art performance on various benchmarks, outperforming existing models. The research also shows that using \\mname to curate a high-quality subset from a noisy dataset significantly improves the training outcomes for image editing tasks.'}, 'zh': {'title': 'æå‡å›¾åƒç¼–è¾‘è´¨é‡çš„æ–°å¥–åŠ±æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æ”¹å–„åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸€ä¸ªå¤§è§„æ¨¡çš„äººç±»åå¥½æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé€‰æ‹©é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­ä¸äººç±»åå¥½çš„å¯¹é½åº¦æ›´é«˜ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å›¾åƒç¼–è¾‘çš„è´¨é‡ã€‚æœ€ç»ˆï¼Œæœ¬æ–‡è¿˜è®¡åˆ’å°†è¯¥æ¨¡å‹åŠå…¶è®­ç»ƒæ•°æ®é›†å‘å¸ƒï¼Œä»¥å¸®åŠ©ç¤¾åŒºæ„å»ºæ›´é«˜è´¨é‡çš„å›¾åƒç¼–è¾‘è®­ç»ƒæ•°æ®é›†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25301', 'title': 'Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel\n  Execution', 'url': 'https://huggingface.co/papers/2509.25301', 'abstract': 'Flash-Searcher, a parallel agent reasoning framework using directed acyclic graphs, enhances efficiency and performance in complex reasoning tasks by enabling concurrent execution and dynamic workflow optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while reducing agent execution steps by up to 35% compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.', 'score': 10, 'issue_id': 6200, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '6daa8de407c2b924', 'authors': ['Tianrui Qin', 'Qianben Chen', 'Sinuo Wang', 'He Xing', 'King Zhu', 'He Zhu', 'Dingfeng Shi', 'Xinxin Liu', 'Ge Zhang', 'Jiaheng Liu', 'Yuchen Eleanor Jiang', 'Xitong Gao', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.25301.jpg', 'data': {'categories': ['#architecture', '#optimization', '#reasoning', '#benchmark', '#agents'], 'emoji': 'âš¡', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Flash-Searcher â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ°Ñ… (DAG) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ ÑĞ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²ÑĞ·ĞµĞ¹. Flash-Searcher Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 67.7% Ğ½Ğ° BrowseComp Ğ¸ 83% Ğ½Ğ° xbench-DeepSearch, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° 35% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing Reasoning with Parallel Execution', 'desc': 'Flash-Searcher is a new framework designed to improve the efficiency of reasoning tasks in artificial intelligence. It uses directed acyclic graphs (DAGs) to allow multiple reasoning paths to be executed at the same time, rather than one after the other. This approach not only speeds up the process but also optimizes the workflow dynamically based on the results obtained during execution. The framework has shown significant performance improvements in various benchmarks, demonstrating its effectiveness over traditional sequential processing methods.'}, 'zh': {'title': 'Flash-Searcherï¼šé«˜æ•ˆçš„å¹¶è¡Œæ¨ç†æ¡†æ¶', 'desc': 'Flash-Searcher æ˜¯ä¸€ç§æ–°çš„å¹¶è¡Œæ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ï¼Œä½¿ç”¨æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰æ¥æé«˜å¤æ‚æ¨ç†ä»»åŠ¡çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚å®ƒé€šè¿‡å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå…·æœ‰æ˜ç¡®ä¾èµ–å…³ç³»çš„å­ä»»åŠ¡ï¼Œå…è®¸ç‹¬ç«‹æ¨ç†è·¯å¾„çš„å¹¶å‘æ‰§è¡Œï¼ŒåŒæ—¶ä¿æŒé€»è¾‘çº¦æŸã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€å·¥ä½œæµä¼˜åŒ–ï¼ŒåŸºäºä¸­é—´ç»“æœä¸æ–­æ”¹è¿›æ‰§è¡Œå›¾ï¼Œæœ‰æ•ˆæ•´åˆäº†æ‘˜è¦æ¨¡å—ã€‚ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒFlash-Searcher åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01180', 'title': 'BroRL: Scaling Reinforcement Learning via Broadened Exploration', 'url': 'https://huggingface.co/papers/2510.01180', 'abstract': 'BroRL enhances reinforcement learning by increasing rollouts per example, overcoming performance plateaus and achieving state-of-the-art results in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by a mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks.', 'score': 9, 'issue_id': 6202, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '2491de2a9eaf3c36', 'authors': ['Jian Hu', 'Mingjie Liu', 'Ximing Lu', 'Fang Wu', 'Zaid Harchaoui', 'Shizhe Diao', 'Yejin Choi', 'Pavlo Molchanov', 'Jun Yang', 'Jan Kautz', 'Yi Dong'], 'affiliations': ['NVIDIA', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2510.01180.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#optimization', '#training'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ñ€Ğ¾Ğ»Ğ»Ğ°ÑƒÑ‚Ğ¾Ğ² â€” Ğ»ÑƒÑ‡ÑˆĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RL Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BroRL â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ reinforcement learning Ğ´Ğ»Ñ LLM Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ° Ñ€Ğ¾Ğ»Ğ»Ğ°ÑƒÑ‚Ğ¾Ğ² (Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹) Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ñ‡Ğ¸ÑĞ»Ğµ Ñ€Ğ¾Ğ»Ğ»Ğ°ÑƒÑ‚Ğ¾Ğ² Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ€Ğ¾ÑÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑÑÑ‹ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğµ ProRL Ğ¿Ğ¾ÑĞ»Ğµ Ñ‚Ñ‹ÑÑÑ‡ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. BroRL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 1.5B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ°Ğ¼, Ğ³Ğ´Ğµ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ°ÑÑ‹Ñ‰Ğ°ÑÑ‚ÑÑ.'}, 'en': {'title': 'Broaden Exploration for Continuous Gains in RL', 'desc': 'BroRL is a novel approach in reinforcement learning that enhances the training process by increasing the number of rollouts per example, which allows for broader exploration of the solution space. This method addresses the issue of performance plateaus that occur when traditional training steps reach diminishing returns. By analyzing the probability mass of correct and incorrect tokens, BroRL ensures that as the number of rollouts increases, the overall performance improves continuously. The empirical results show that BroRL outperforms previous methods, achieving state-of-the-art results in large language models after extensive training.'}, 'zh': {'title': 'BroRLï¼šçªç ´å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ç“¶é¢ˆ', 'desc': 'BroRLæ˜¯ä¸€ç§å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡å¢åŠ æ¯ä¸ªç¤ºä¾‹çš„å›åˆæ•°æ¥å…‹æœæ€§èƒ½å¹³å°æœŸã€‚å®ƒèƒ½å¤Ÿåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­å®ç°æŒç»­çš„æ€§èƒ½æå‡ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ProRLçš„é™åˆ¶ã€‚é€šè¿‡å¯¹æ¦‚ç‡è´¨é‡å˜åŒ–çš„åˆ†æï¼ŒBroRLç¡®ä¿äº†æ­£ç¡®æ ‡è®°çš„æ¦‚ç‡è´¨é‡ä¸æ–­æ‰©å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBroRLåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå°¤å…¶æ˜¯åœ¨ç»è¿‡3000æ­¥ProRLè®­ç»ƒåï¼Œæ¨¡å‹çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æ¢å¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00526', 'title': 'Beyond Log Likelihood: Probability-Based Objectives for Supervised\n  Fine-Tuning across the Model Capability Continuum', 'url': 'https://huggingface.co/papers/2510.00526', 'abstract': 'Research identifies probability-based objectives that outperform negative log likelihood for fine-tuning large language models, depending on model capability.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., -p, -p^{10}, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.', 'score': 7, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '9f43fe314cbe69af', 'authors': ['Gaotang Li', 'Ruizhong Qiu', 'Xiusi Chen', 'Heng Ji', 'Hanghang Tong'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.00526.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ ÑĞ¸Ğ»Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ negative log likelihood (NLL) Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ° Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ capability Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ»Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²ĞµÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, -p Ğ¸Ğ»Ğ¸ -p^10), Ğ´Ğ»Ñ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ NLL. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 7 Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ…, 14 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ 3 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ğ¸Ğ½ÑƒÑƒĞ¼Ğ° model-capability, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ.'}, 'en': {'title': 'Optimizing Fine-Tuning: Beyond Negative Log Likelihood', 'desc': "This paper explores how different training objectives can improve the fine-tuning of large language models (LLMs) beyond the traditional negative log likelihood (NLL). It identifies that NLL may not be optimal for models that have already been pre-trained, as they possess inherent task-relevant knowledge. The authors propose a range of probability-based objectives that adapt to the model's capability, showing that stronger models benefit from objectives that prioritize high-probability tokens. Through extensive experiments, they demonstrate that the effectiveness of these objectives varies along a continuum of model strength, providing insights into how to select the best training objective based on model performance."}, 'zh': {'title': 'è¶…è¶Šè´Ÿå¯¹æ•°ä¼¼ç„¶çš„å¾®è°ƒç›®æ ‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºæ¦‚ç‡çš„ç›®æ ‡å‡½æ•°åœ¨å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨ä¸åŒæ¨¡å‹èƒ½åŠ›ä¸‹ä¼˜äºè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ã€‚ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•å¸¸å¸¸å—é™äºNLLè¿™ä¸€è®­ç»ƒç›®æ ‡ï¼Œè€Œåœ¨åè®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹å·²ç»å…·å¤‡äº†ä»»åŠ¡ç›¸å…³çš„å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬é€šè¿‡å¤§é‡å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼Œæ­ç¤ºäº†æ¨¡å‹èƒ½åŠ›çš„è¿ç»­æ€§å¯¹ç›®æ ‡å‡½æ•°è¡¨ç°çš„å½±å“ã€‚åœ¨å¼ºæ¨¡å‹ç«¯ï¼Œå€¾å‘äºå…ˆéªŒçš„ç›®æ ‡å‡½æ•°è¡¨ç°ä¼˜äºNLLï¼Œè€Œåœ¨å¼±æ¨¡å‹ç«¯åˆ™æ˜¯NLLå ä¼˜ï¼Œæä¾›äº†æ ¹æ®æ¨¡å‹èƒ½åŠ›è°ƒæ•´ç›®æ ‡å‡½æ•°çš„ç†è®ºåŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00931', 'title': 'Making, not Taking, the Best of N', 'url': 'https://huggingface.co/papers/2510.00931', 'abstract': 'Fusion-of-N (FusioN) method improves LLM generation quality by synthesizing elements from multiple samples, outperforming Best-of-N in various settings and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Obtaining high-quality generations in modern LLMs has largely been framed as a selection problem: identifying a single winning generation from a diverse pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently zero-sum, discarding diverse and potentially useful information from the pool. Instead, we explore a collaborative setup, where all candidates can potentially contribute to the final winning generation. To this end, we propose Fusion-of-N (FusioN): a method that uses a general LLM judge to synthesize the most informative elements of each sample into a single final answer. We compare FusioN to BoN in two settings, (i) test-time scaling, where we sample and aggregate from a single model at test-time (ii) synthetic data generation, where we fuse samples from a pool of diverse teachers to improve a student model. We extensively benchmark both setups across 11 languages, 3 diverse tasks and varying model scales. Across the bench, FusioN consistently outperforms BoN showing versatility and robustness both in test-time scaling and in downstream gains from synthetic data generation. We also perform extensive analysis on FusioN, where it shows surprising strengths and robustness under challenging settings. These results show that we should shift how we think about evaluating and utilizing LLM generations from a monolithic measure of quality, to embracing their polylithic nature. This shift allows us to integrate diverse strengths, unlock latent potential, and achieve improvements that were previously inaccessible through selection alone.', 'score': 6, 'issue_id': 6201, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'f203d81f3a65b787', 'authors': ['Ammar Khairi', "Daniel D'souza", 'Marzieh Fadaee', 'Julia Kreutzer'], 'affiliations': ['Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2510.00931.jpg', 'data': {'categories': ['#rag', '#benchmark', '#optimization', '#synthetic', '#multilingual'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¡Ğ¸Ğ»Ğ° ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´ Fusion-of-N (FusioN), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ², Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° ĞºĞ°Ğº Ğ² Best-of-N. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ÑŒĞ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ° Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚. FusioN Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»ÑÑ Ğ² Ğ´Ğ²ÑƒÑ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ inference Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ student-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° 11 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ 3 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ FusioN Ğ½Ğ°Ğ´ Best-of-N, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ğ¼.'}, 'en': {'title': 'Unlocking the Power of Collaboration in LLM Generation', 'desc': 'The Fusion-of-N (FusioN) method enhances the quality of large language model (LLM) outputs by integrating elements from multiple generated samples rather than selecting just one. This approach contrasts with the traditional Best-of-N (BoN) method, which often overlooks valuable information by focusing on a single best output. FusioN employs a general LLM judge to synthesize the most informative parts of each candidate, leading to a more comprehensive final generation. Extensive benchmarking across various languages and tasks demonstrates that FusioN consistently outperforms BoN, highlighting the benefits of leveraging diverse contributions in LLM generation.'}, 'zh': {'title': 'èåˆå¤šæ ·æ€§ï¼Œæå‡ç”Ÿæˆè´¨é‡', 'desc': 'Fusion-of-Nï¼ˆFusioNï¼‰æ–¹æ³•é€šè¿‡ç»¼åˆå¤šä¸ªæ ·æœ¬çš„å…ƒç´ ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆè´¨é‡ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„æœ€ä½³é€‰æ‹©æ–¹æ³•ï¼ˆBest-of-Nï¼‰ã€‚è¯¥æ–¹æ³•ä¸å†ä»…ä»…é€‰æ‹©ä¸€ä¸ªæœ€ä½³ç”Ÿæˆï¼Œè€Œæ˜¯å…è®¸æ‰€æœ‰å€™é€‰æ ·æœ¬å…±åŒè´¡çŒ®ä¿¡æ¯ï¼Œå½¢æˆæœ€ç»ˆçš„ç­”æ¡ˆã€‚FusioNåœ¨æµ‹è¯•æ—¶æ‰©å±•å’Œåˆæˆæ•°æ®ç”Ÿæˆçš„å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¤šè¯­è¨€å’Œä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹çš„çµæ´»æ€§å’Œç¨³å¥æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åº”å½“æ”¹å˜å¯¹LLMç”Ÿæˆç»“æœçš„è¯„ä¼°æ–¹å¼ï¼Œä»å•ä¸€çš„è´¨é‡è¡¡é‡è½¬å‘æ¥å—å…¶å¤šæ ·æ€§ï¼Œä»¥å®ç°æ›´å¤§çš„æ½œåŠ›å’Œæ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00553', 'title': 'On Predictability of Reinforcement Learning Dynamics for Large Language\n  Models', 'url': 'https://huggingface.co/papers/2510.00553', 'abstract': 'Two fundamental properties of reinforcement learning-induced parameter updates in large language models are identified, leading to a plug-in acceleration framework that significantly speeds up training without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.', 'score': 6, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '75c2581875112809', 'authors': ['Yuchen Cai', 'Ding Cao', 'Xin Xu', 'Zijun Yao', 'Yuqing Huang', 'Zhenyu Tan', 'Benyi Zhang', 'Guiquan Liu', 'Junfeng Fang'], 'affiliations': ['HKUST', 'NUS', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2510.00553.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ° Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¸ reinforcement learning: Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ½Ğ³Ğ°-1 Ğ¸ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑ€Ñ…Ğ½ĞµĞµ ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 99% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¾Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº AlphaRL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ñ€Ğ°Ğ½Ğ½ĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 2.5x Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 96% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Accelerating RL Training in LLMs with AlphaRL', 'desc': 'This paper explores how reinforcement learning (RL) affects the training of large language models (LLMs) by identifying two key properties of parameter updates. The first property, Rank-1 Dominance, shows that a specific part of the parameter update matrix is crucial for improving reasoning capabilities, capturing over 99% of performance gains. The second property, Rank-1 Linear Dynamics, indicates that this important part changes in a predictable way during training, allowing for accurate predictions from early training stages. Based on these insights, the authors introduce AlphaRL, a framework that accelerates training by predicting final updates from early data, achieving significant speed improvements while maintaining high performance.'}, 'zh': {'title': 'åŠ é€Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æœ‰æ•ˆå·¥å…·', 'desc': 'æœ¬æ–‡è¯†åˆ«äº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å¼•èµ·çš„å‚æ•°æ›´æ–°çš„ä¸¤ä¸ªåŸºæœ¬ç‰¹æ€§ã€‚è¿™äº›ç‰¹æ€§åŒ…æ‹¬ï¼š1ï¼‰ç§©-1ä¸»å¯¼æ€§ï¼Œæ„å‘³ç€å‚æ•°æ›´æ–°çŸ©é˜µçš„ä¸»å¯¼å­ç©ºé—´å‡ ä¹å®Œå…¨å†³å®šäº†æ¨ç†çš„æ”¹è¿›ï¼›2ï¼‰ç§©-1çº¿æ€§åŠ¨æ€ï¼Œè¡¨æ˜è¿™ä¸ªä¸»å¯¼å­ç©ºé—´åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çº¿æ€§æ¼”å˜ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºäº†AlphaRLåŠ é€Ÿæ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹å’Œç®—æ³•ä¸­å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.22887', 'title': 'Infusing Theory of Mind into Socially Intelligent LLM Agents', 'url': 'https://huggingface.co/papers/2509.22887', 'abstract': 'Integrating Theory of Mind into LLMs improves dialogue effectiveness and goal achievement by enabling strategic reasoning and better partner relationships.  \t\t\t\t\tAI-generated summary \t\t\t\t Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.', 'score': 5, 'issue_id': 6200, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': 'de0a43468eb08889', 'authors': ['EunJeong Hwang', 'Yuwei Yin', 'Giuseppe Carenini', 'Peter West', 'Vered Shwartz'], 'affiliations': ['University of British Columbia', 'Vector Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.22887.jpg', 'data': {'categories': ['#alignment', '#rl', '#reasoning', '#benchmark', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ñ Ñ‚ĞµĞ¾Ñ€Ğ¸ĞµĞ¹ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ° (ToM) â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ»ÑĞ´ĞµĞ¹ â€” Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ToMAgent (ToMA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾ Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ñ†ĞµĞ»ĞµĞ¹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğµ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ToM Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering LLMs with Theory of Mind for Smarter Conversations', 'desc': 'This paper explores how integrating Theory of Mind (ToM) into large language models (LLMs) enhances their ability to engage in dialogue and achieve specific goals. By understanding the mental states of conversation partners, LLMs can respond more strategically and maintain better relationships. The authors introduce ToMA, a dialogue agent that combines ToM with dialogue lookahead to optimize its responses for goal achievement. Experiments show that ToMA outperforms traditional models, demonstrating improved reasoning and adaptability in social interactions.'}, 'zh': {'title': 'å¿ƒæ™ºç†è®ºæå‡å¯¹è¯æ™ºèƒ½', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å°†å¿ƒæ™ºç†è®ºï¼ˆToMï¼‰æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„æ–¹æ³•ï¼Œä»¥æé«˜å¯¹è¯çš„æœ‰æ•ˆæ€§å’Œç›®æ ‡è¾¾æˆç‡ã€‚å¿ƒæ™ºç†è®ºæ˜¯ç†è§£ä»–äººå¿ƒç†çŠ¶æ€çš„èƒ½åŠ›ï¼Œæ˜¯äººç±»ç¤¾ä¼šæ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡æ˜ç¡®ä½¿ç”¨å¿ƒæ™ºç†è®ºçš„LLMåœ¨å¯¹è¯ä¸­è¡¨ç°æ›´å¥½ï¼Œèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°å®ç°ç›®æ ‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ToMAï¼ˆå¿ƒæ™ºç†è®ºå¯¹è¯ä»£ç†ï¼‰ï¼Œé€šè¿‡å°†å¿ƒæ™ºç†è®ºä¸å¯¹è¯å‰ç»ç»“åˆï¼Œè®­ç»ƒå‡ºèƒ½å¤Ÿäº§ç”Ÿæœ‰åŠ©äºå®ç°å¯¹è¯ç›®æ ‡çš„å¿ƒç†çŠ¶æ€çš„ä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00536', 'title': 'GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness', 'url': 'https://huggingface.co/papers/2510.00536', 'abstract': "GUI-KV, a KV cache compression method for GUI agents, improves efficiency by exploiting spatial and temporal redundancies, reducing computational cost while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.", 'score': 4, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '6687d7079b2d54e2', 'authors': ['Kung-Hsiang Huang', 'Haoyi Qiu', 'Yutong Dai', 'Caiming Xiong', 'Chien-Sheng Wu'], 'affiliations': ['Salesforce AI Research', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2510.00536.jpg', 'data': {'categories': ['#optimization', '#inference', '#agents', '#benchmark'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ GUI-KV â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ KV-ĞºÑÑˆĞ° Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸: Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· L2-Ğ½Ğ¾Ñ€Ğ¼Ñƒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ² GUI Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ¾ Ğ²Ğ¾ Ğ²ÑĞµÑ… ÑĞ»Ğ¾ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ AgentNetBench Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° 38.9% Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 4.1%, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Efficient GUI Agents with GUI-KV Cache Compression', 'desc': 'The paper presents GUI-KV, a method for compressing key-value (KV) caches specifically designed for graphical user interface (GUI) agents. It addresses the inefficiencies in processing high-resolution screenshots by leveraging spatial and temporal redundancies, which reduces computational costs while maintaining accuracy. The authors analyze attention patterns in GUI workloads and propose a uniform budget allocation strategy that outperforms more complex methods. By introducing techniques like spatial saliency guidance and temporal redundancy scoring, GUI-KV achieves significant improvements in efficiency and accuracy across various benchmarks.'}, 'zh': {'title': 'é«˜æ•ˆçš„GUIä»£ç†ç¼“å­˜å‹ç¼©æ–¹æ³•', 'desc': 'GUI-KVæ˜¯ä¸€ç§é’ˆå¯¹å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å‹ç¼©æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ç©ºé—´å’Œæ—¶é—´å†—ä½™æ¥æé«˜æ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡æˆªå›¾å’Œé•¿æ—¶é—´ä»»åŠ¡æ—¶ï¼Œèƒ½å¤Ÿé™ä½è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGUIä»£ç†çš„æ³¨æ„åŠ›æ¨¡å¼ä¸è‡ªç„¶å›¾åƒä¸åŒï¼Œæ‰€æœ‰å˜æ¢å™¨å±‚çš„æ³¨æ„åŠ›ç¨€ç–æ€§å‡è¾ƒé«˜ï¼Œè¿™ä¿ƒä½¿æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„å‡åŒ€é¢„ç®—åˆ†é…ç­–ç•¥ã€‚GUI-KVç»“åˆäº†ç©ºé—´æ˜¾è‘—æ€§å¼•å¯¼å’Œæ—¶é—´å†—ä½™è¯„åˆ†ä¸¤ç§æ–°æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†ç¼“å­˜å‹ç¼©çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23250', 'title': 'Training Vision-Language Process Reward Models for Test-Time Scaling in\n  Multimodal Reasoning: Key Insights and Lessons Learned', 'url': 'https://huggingface.co/papers/2509.23250', 'abstract': 'Hybrid data synthesis and perception-focused supervision improve the reliability of Vision-Language Process Reward Models (VL-PRMs) in guiding VLMs across diverse multimodal benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) provide step-level supervision that improves the reliability of reasoning in large language models. While PRMs have been extensively studied in text-based domains, their extension to Vision Language Models (VLMs) remains limited. Existing Vision-Language PRMs (VL-PRMs) rely on Monte Carlo Tree Search (MCTS) for data construction, which can often produce noisy supervision signals and limit generalization across tasks. In this work, we aim to elucidate the design space of VL-PRMs by exploring diverse strategies for dataset construction, training, and test-time scaling. First, we introduce a hybrid data synthesis framework that combines MCTS with judgments from a strong VLM, producing more accurate step-level labels. Second, we propose perception-focused supervision, enabling our PRM to explicitly detect errors at the visual grounding stage of reasoning. Third, we systematically evaluate multiple test-time scaling strategies, showing that our PRMs can reliably guide VLMs toward more accurate solutions. Our experiments covering five diverse multimodal benchmarks (MMMU, PuzzleVQA, AlgoPuzzleVQA, MathVista, and MathVision) reveal several key insights: (i) VL-PRMs when used as Outcome Reward Models (ORMs) during test-time scaling (TTS) can outperform VL-PRM guided process step selection, (ii) smaller VL-PRMs can match or even surpass larger ones in detecting process errors, (iii) VL-PRMs uncover latent reasoning abilities in stronger VLM backbones, (iv) perception-level supervision leads to significant gains in test-time scaling, and (v) TTS performance of different policies improve on advanced math reasoning datasets despite not training VL-PRMs on such datasets. We hope our work will motivate further research and support the advancement of VLMs.', 'score': 4, 'issue_id': 6198, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': 'c59cc3e092f9a705', 'authors': ['Brandon Ong', 'Tej Deep Pala', 'Vernon Toh', 'William Chandra Tjhi', 'Soujanya Poria'], 'affiliations': ['AI Singapore', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2509.23250.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training', '#games', '#multimodal', '#data', '#dataset'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ Vision-Language Process Reward Models (VL-PRMs), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑ MCTS Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ VLM, Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ supervision Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ VL-PRMs Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ: VL-PRMs ĞºĞ°Ğº Outcome Reward Models Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ ÑĞµĞ»ĞµĞºÑ†Ğ¸Ñ, Ğ° supervision Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ test-time scaling.'}, 'en': {'title': 'Enhancing Vision-Language Models with Hybrid Supervision and Data Synthesis', 'desc': 'This paper discusses improvements in Vision-Language Process Reward Models (VL-PRMs) to enhance their effectiveness in guiding Vision Language Models (VLMs). The authors introduce a hybrid data synthesis method that merges Monte Carlo Tree Search with insights from a robust VLM, resulting in more precise step-level supervision. They also propose a perception-focused supervision approach that helps the model identify errors during visual reasoning. Through extensive testing on various multimodal benchmarks, the study demonstrates that these enhancements lead to better performance and reliability in VLMs, even in complex reasoning tasks.'}, 'zh': {'title': 'æ··åˆæ•°æ®åˆæˆä¸æ„ŸçŸ¥ç›‘ç£æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯é æ€§', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ··åˆæ•°æ®åˆæˆæ¡†æ¶ï¼Œç»“åˆäº†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’Œå¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„åˆ¤æ–­ï¼Œä»¥ç”Ÿæˆæ›´å‡†ç¡®çš„æ­¥éª¤çº§æ ‡ç­¾ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä»¥æ„ŸçŸ¥ä¸ºä¸­å¿ƒçš„ç›‘ç£ï¼Œå¸®åŠ©è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨æ¨ç†çš„è§†è§‰åŸºç¡€é˜¶æ®µæ˜ç¡®æ£€æµ‹é”™è¯¯ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°å¤šç§æµ‹è¯•æ—¶æ‰©å±•ç­–ç•¥ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè§†è§‰è¯­è¨€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆVL-PRMï¼‰èƒ½å¤Ÿå¯é åœ°å¼•å¯¼VLMæœå‘æ›´å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºè¿›ä¸€æ­¥ç ”ç©¶å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æä¾›äº†é‡è¦çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00510', 'title': 'JoyAgent-JDGenie: Technical Report on the GAIA', 'url': 'https://huggingface.co/papers/2510.00510', 'abstract': 'A generalist agent architecture combining multi-agent planning, hierarchical memory, and a refined tool suite outperforms existing systems in diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.', 'score': 3, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '4a1605179598a812', 'authors': ['Jiarun Liu', 'Shiyue Xu', 'Shangkun Liu', 'Yang Li', 'Wen Liu', 'Min Liu', 'Xiaoqing Zhou', 'Hanmin Wang', 'Shilin Jia', 'zhen Wang', 'Shaohua Tian', 'Hanhao Li', 'Junbo Zhang', 'Yongli Yu', 'Peng Cao', 'Haofen Wang'], 'affiliations': ['GAIA JINGDONG CHO-EI Team'], 'pdf_title_img': 'assets/pdf/title_img/2510.00510.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#optimization', '#architecture', '#multimodal', '#agents', '#agi'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ â€” Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸. Ğ¢Ñ€ĞµÑ‚Ğ¸Ğ¹ â€” Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ open-source Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Empowering AI with Integrated Generalist Agent Architecture', 'desc': 'This paper presents a new architecture for generalist agents that enhances their performance in various tasks. It combines multi-agent planning, where different agents work together to make decisions, with a hierarchical memory system that organizes information at different levels. Additionally, it includes a refined tool suite that allows the agent to perform tasks like searching and executing code. The proposed system shows significant improvements over existing models, indicating that integrating these components leads to more robust and adaptable AI assistants.'}, 'zh': {'title': 'é€šç”¨æ™ºèƒ½ä½“æ¶æ„ï¼šæå‡AIåŠ©æ‰‹çš„é€‚åº”æ€§ä¸é²æ£’æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨æ™ºèƒ½ä½“æ¶æ„ï¼Œç»“åˆäº†å¤šæ™ºèƒ½ä½“è§„åˆ’ã€åˆ†å±‚è®°å¿†å’Œç²¾ç»†åŒ–å·¥å…·å¥—ä»¶ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­è¶…è¶Šç°æœ‰ç³»ç»Ÿã€‚è¯¥æ¶æ„æ•´åˆäº†é›†ä½“å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€åˆ†å±‚è®°å¿†ç³»ç»Ÿä»¥åŠç”¨äºæœç´¢ã€ä»£ç æ‰§è¡Œå’Œå¤šæ¨¡æ€è§£æçš„å·¥å…·ã€‚é€šè¿‡å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ€§èƒ½ä¸ŠæŒç»­ä¼˜äºå¼€æºåŸºçº¿ï¼Œå¹¶æ¥è¿‘ä¸“æœ‰ç³»ç»Ÿçš„è¡¨ç°ã€‚è¿™äº›ç»“æœè¡¨æ˜ç³»ç»Ÿçº§é›†æˆçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¯æ‰©å±•ã€å¼¹æ€§å’Œé€‚åº”æ€§å¼ºçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹æŒ‡æ˜äº†æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25531', 'title': 'MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality\n  Instruction and Reasoning Data Built from Permissive-First Text Sources', 'url': 'https://huggingface.co/papers/2509.25531', 'abstract': 'MixtureVitae is a pretraining corpus that combines public-domain and permissively licensed text with low-risk additions, achieving strong model performance across benchmarks while minimizing legal risk.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae', 'score': 3, 'issue_id': 6214, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': 'c795ac2fa9ea5a3f', 'authors': ['Huu Nguyen', 'Victor May', 'Harsh Raj', 'Marianna Nezhurina', 'Yishan Wang', 'Yanqi Luo', 'Minh Chien Vu', 'Taishi Nakamura', 'Ken Tsui', 'Van Khue Nguyen', 'David Salinas', 'Aleksandra KrasnodÄ™bska', 'Christoph Schuhmann', 'Mats Leon Richter', 'Xuan-Son', 'Vu', 'Jenia Jitsev'], 'affiliations': ['Carnegie Mellon University', 'Detomo Inc.', 'ELLIS Institute Tuebingen', 'Independent Researcher', 'Institute of Science Tokyo', 'Juelich Supercomputing Center (JSC), Research Center Juelich (FZJ)', 'LAION', 'Montreal Institute for Learning Algorithms, University of Montreal, UniversitÃ© de MontrÃ©al', 'NASK', 'Northeastern University', 'Ontocord', 'Open-Î¨ (Open-Sci) Collective', 'RSS Lab, LTH / DeepTensor AB', 'Salesforce', 'University of Freiburg', 'Ã‰cole Polytechnique, IP Paris'], 'pdf_title_img': 'assets/pdf/title_img/2509.25531.jpg', 'data': {'categories': ['#synthetic', '#open_source', '#benchmark', '#low_resource', '#data', '#dataset'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ²', 'desc': 'MixtureVitae â€” ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ¸ÑĞºĞ¾Ğ². Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ¸Ğ· Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ Ñ Ğ¿ĞµÑ€Ğ¼Ğ¸ÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ÑĞ¼Ğ¸ (CC-BY, Apache) Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ 130M Ğ´Ğ¾ 1.7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² MixtureVitae Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ÑƒÑ Ñ FineWeb-Edu Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑ‰ÑƒÑÑÑ Ğº DCLM, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¸ ĞºĞ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ LLM Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµÑ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ñ‡Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ²ĞµĞ±-ÑĞºÑ€ĞµĞ¹Ğ¿Ğ¸Ğ½Ğ³Ğ°, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'MixtureVitae: Legal Safety Meets High Performance in ML Training', 'desc': 'MixtureVitae is a new pretraining dataset designed to enhance the performance of machine learning models while reducing legal risks associated with data usage. It combines public-domain texts and permissively licensed materials with low-risk additions, ensuring compliance with legal standards. The dataset undergoes a thorough filtering and quality screening process, making it suitable for training large language models (LLMs). Experimental results show that models trained on MixtureVitae outperform those trained on other datasets, particularly excelling in tasks related to mathematics and coding.'}, 'zh': {'title': 'MixtureVitaeï¼šæ³•å¾‹é£é™©ä¸æ¨¡å‹æ€§èƒ½çš„å¹³è¡¡ä¹‹é“', 'desc': 'MixtureVitaeæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒè¯­æ–™åº“ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆå…¬å…±é¢†åŸŸå’Œè®¸å¯æ–‡æœ¬ï¼Œé™ä½æ³•å¾‹é£é™©ï¼ŒåŒæ—¶å®ç°å¼ºå¤§çš„æ¨¡å‹æ€§èƒ½ã€‚è¯¥è¯­æ–™åº“é‡‡ç”¨é£é™©ç¼“è§£çš„æ¥æºç­–ç•¥ï¼Œç»“åˆäº†ç»è¿‡åˆç†é€‰æ‹©çš„ä½é£é™©è¡¥å……å†…å®¹ï¼Œå¦‚æ”¿åºœä½œå“å’Œç¬¦åˆæ¬§ç›ŸTDMæ ‡å‡†çš„æ¥æºã€‚æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†ä¸€ä¸ªé€æ˜çš„å¤šé˜¶æ®µæµç¨‹ï¼Œç”¨äºè®¸å¯è¯æ„è¯†è¿‡æ»¤ã€å®‰å…¨å’Œè´¨é‡ç­›é€‰ï¼Œä»¥åŠé¢†åŸŸæ„è¯†æ··åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MixtureVitaeè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–è®¸å¯æ•°æ®é›†ï¼Œå°¤å…¶åœ¨æ•°å­¦/ä»£ç å’Œé—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01152', 'title': 'Pay-Per-Search Models are Abstention Models', 'url': 'https://huggingface.co/papers/2510.01152', 'abstract': "MASH, a reinforcement learning framework, improves LLMs' selective help-seeking and abstention capabilities without pre-determined knowledge boundaries.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs cannot reliably recognize their parametric knowledge boundaries and often hallucinate answers to outside-of-boundary questions. In contrast, humans recognize their limitations and can either seek external help for such questions or abstain. In this paper, we introduce MASH (Modeling Abstention via Selective Help-seeking), a training framework that readily extracts abstentions from LLMs. Our key idea is that any external help-seeking by an LLM, i.e. search tool use, can serve as a proxy for abstention if the external help (search) is appropriately penalized while simultaneously rewarding answer accuracy. MASH operationalizes this idea using reinforcement learning with a pay-per-search reward.   We run experiments on three knowledge-intensive QA datasets. Our results show that MASH substantially improves upon the selective help-seeking performance of prior efficient search approaches; on multi-hop datasets, MASH improves answer accuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf abstention -- it can distinguish between unanswerable/answerable questions and selectively generate responses for answerable questions -- showcasing behavior analogous to specialized abstention approaches. We emphasize that contrary to prior abstention methods, MASH does not require pre-determining knowledge boundaries to construct training data. Instead, MASH's abstentions are a by-product of training for the auxiliary selective help-seeking task. Overall, we show that MASH training effectively aligns search tool use with parametric knowledge, which can be successfully leveraged for making abstention decisions.", 'score': 2, 'issue_id': 6215, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '7eb9492df9e01cd2', 'authors': ['Mustafa Omer Gul', 'Claire Cardie', 'Tanya Goyal'], 'affiliations': ['Department of Computer Science, Cornell University'], 'pdf_title_img': 'assets/pdf/title_img/2510.01152.jpg', 'data': {'categories': ['#training', '#alignment', '#reasoning', '#rl', '#hallucinations'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ LLM Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Â«Ğ½Ğµ Ğ·Ğ½Ğ°ÑÂ» Ñ‡ĞµÑ€ĞµĞ· ÑˆÑ‚Ñ€Ğ°Ñ„ Ğ·Ğ° Ğ¿Ğ¾Ğ¸ÑĞº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MASH â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ²Ğ½Ğµ Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ° Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, ĞµÑĞ»Ğ¸ Ğ·Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°ĞµÑ‚ÑÑ ÑˆÑ‚Ñ€Ğ°Ñ„, Ğ° Ğ·Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° â€” Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², MASH Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° 7.6% Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ…Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹.'}, 'en': {'title': 'MASH: Empowering LLMs with Smart Help-Seeking and Abstention', 'desc': "MASH is a reinforcement learning framework designed to enhance the selective help-seeking and abstention abilities of large language models (LLMs). Unlike traditional methods, MASH does not rely on pre-defined knowledge boundaries, allowing LLMs to better recognize when they should seek external help or abstain from answering. The framework uses a pay-per-search reward system to encourage accurate answers while penalizing unnecessary searches, effectively aligning the model's search tool usage with its knowledge capabilities. Experimental results demonstrate that MASH significantly improves the performance of LLMs in answering complex questions and making appropriate abstention decisions."}, 'zh': {'title': 'MASHï¼šæå‡LLMsçš„é€‰æ‹©æ€§å¯»æ±‚ä¸æ”¾å¼ƒèƒ½åŠ›', 'desc': 'MASHæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€‰æ‹©æ€§å¯»æ±‚å¸®åŠ©å’Œæ”¾å¼ƒå›ç­”èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ï¼Œè€Œæ— éœ€é¢„å…ˆç¡®å®šçŸ¥è¯†è¾¹ç•Œã€‚ä¸äººç±»èƒ½å¤Ÿè¯†åˆ«è‡ªèº«å±€é™æ€§å¹¶é€‰æ‹©å¯»æ±‚å¤–éƒ¨å¸®åŠ©æˆ–æ”¾å¼ƒä¸åŒï¼ŒLLMså¸¸å¸¸æ— æ³•å¯é åœ°è¯†åˆ«å…¶çŸ¥è¯†è¾¹ç•Œï¼Œå¯¼è‡´é”™è¯¯å›ç­”ã€‚MASHé€šè¿‡å¯¹å¤–éƒ¨å¸®åŠ©çš„é€‚å½“æƒ©ç½šå’Œå¯¹å›ç­”å‡†ç¡®æ€§çš„å¥–åŠ±ï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMASHåœ¨å¤šè·³é—®ç­”æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†å›ç­”å‡†ç¡®æ€§ï¼Œå¹¶èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†å¯å›ç­”å’Œä¸å¯å›ç­”çš„é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01037', 'title': 'CurES: From Gradient Analysis to Efficient Curriculum Learning for\n  Reasoning LLMs', 'url': 'https://huggingface.co/papers/2510.01037', 'abstract': 'CurES, a reinforcement learning-based method, improves the training efficiency of large language models by optimizing prompt selection and rollout allocation, leading to faster convergence and reduced computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Curriculum learning plays a crucial role in enhancing the training efficiency of large language models (LLMs) on reasoning tasks. However, existing methods often fail to adequately account for variations in prompt difficulty or rely on simplistic filtering mechanisms to select prompt datasets within a narrow criterion range, resulting in significant computational waste. In this work, we approach the problem from the perspective of reinforcement learning gradient optimization, offering a systematic and theoretical investigation into how to improve the training efficiency of LLMs. We identify two key factors influencing training efficiency: the selection of training prompts and the allocation of rollout quantities across different prompts. Our theoretical analysis reveals that the sampling distribution of prompts dictates the convergence rate of gradient descent, while the allocation of the rollout quantity influences the consistency and stability of overall gradient updates. Based on these insights, we propose CurES, an efficient training method that accelerates convergence and employs Bayesian posterior estimation to minimize computational overhead. Experiments demonstrate that our CurES outperforms Group Relative Policy Optimization (GRPO) by +3.30 points and +4.82 points with 1.5B and 7B models, respectively. Additionally, CurES exhibits faster convergence compared to baselines, including GRPO.', 'score': 2, 'issue_id': 6204, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'a8919e29862921e6', 'authors': ['Yongcheng Zeng', 'Zexu Sun', 'Bokai Ji', 'Erxue Min', 'Hengyi Cai', 'Shuaiqiang Wang', 'Dawei Yin', 'Haifeng Zhang', 'Xu Chen', 'Jun Wang'], 'affiliations': ['Baidu Inc.', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2510.01037.jpg', 'data': {'categories': ['#training', '#optimization', '#reasoning', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'CurES â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ gradient descent, Ğ° ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ rollouts Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ baseline Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ GRPO Ğ½Ğ° 3-5 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ². CurES Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… reasoning.'}, 'en': {'title': 'Optimizing Prompt Selection for Efficient LLM Training with CurES', 'desc': 'CurES is a novel method that enhances the training efficiency of large language models (LLMs) by using reinforcement learning to optimize how prompts are selected and how rollout allocations are managed. It addresses the shortcomings of traditional curriculum learning approaches, which often overlook the complexity of prompt difficulty and lead to inefficient training processes. By analyzing the impact of prompt sampling distributions and rollout allocations, CurES improves the convergence rate of gradient descent and stabilizes gradient updates. Experimental results show that CurES significantly outperforms existing methods, achieving faster convergence and reduced computational costs.'}, 'zh': {'title': 'CurESï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'CurESæ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–æç¤ºé€‰æ‹©å’Œå›æ»šåˆ†é…æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ç°æœ‰æŠ€æœ¯åœ¨å¤„ç†æç¤ºéš¾åº¦å˜åŒ–æ—¶çš„ä¸è¶³ï¼Œé¿å…äº†ä¸å¿…è¦çš„è®¡ç®—æµªè´¹ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬å‘ç°æç¤ºçš„é‡‡æ ·åˆ†å¸ƒå’Œå›æ»šæ•°é‡çš„åˆ†é…æ˜¯å½±å“è®­ç»ƒæ•ˆç‡çš„ä¸¤ä¸ªå…³é”®å› ç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCurESåœ¨åŠ é€Ÿæ”¶æ•›å’Œå‡å°‘è®¡ç®—å¼€é”€æ–¹é¢ä¼˜äºç°æœ‰çš„ä¼˜åŒ–æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00777', 'title': 'In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.00777', 'abstract': "In-place feedback allows users to directly edit LLM responses, improving performance and reducing token usage in multi-turn reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine their outputs based on user-provided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM's previous response, and the model conditions on this modified response to generate its revision. Empirical evaluations on diverse reasoning-intensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using 79.1% fewer tokens. Complementary analyses on controlled environments further demonstrate that in-place feedback resolves a core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content. These findings suggest that in-place feedback offers a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks.", 'score': 2, 'issue_id': 6198, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'be36c5f4f29f17e4', 'authors': ['Youngbin Choi', 'Minjong Lee', 'Saemi Moon', 'Seunghyuk Cho', 'Chaehyeon Chung', 'MoonJeong Park', 'Dongwoo Kim'], 'affiliations': ['Computer Science and Engineering, POSTECH', 'Graduate School of Artificial Intelligence, POSTECH'], 'pdf_title_img': 'assets/pdf/title_img/2510.00777.jpg', 'data': {'categories': ['#training', '#rlhf', '#interpretability', '#reasoning'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ·Ğ´ĞµÑÑŒ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ in-place feedback, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ LLM Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 79,1% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğº Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ñ‡Ğ°ÑÑ‚ÑĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ€Ğ°Ğ½ĞµĞµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'In-Place Feedback: Direct Edits for Smarter LLMs', 'desc': "This paper presents a new method called in-place feedback for improving large language models (LLMs) during multi-turn reasoning tasks. Instead of sending new messages for feedback, users can directly edit the model's previous responses, allowing the model to learn from these modifications. The results show that this approach not only enhances the model's performance but also significantly reduces the number of tokens used by 79.1%. Overall, in-place feedback addresses the limitations of traditional feedback methods by enabling more precise corrections and reducing the introduction of new errors."}, 'zh': {'title': 'å°±åœ°åé¦ˆï¼šæå‡LLMæ¨ç†çš„æœ‰æ•ˆæ–°æ–¹å¼', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„äº¤äº’æ¨¡å¼â€”â€”å°±åœ°åé¦ˆï¼Œå…è®¸ç”¨æˆ·ç›´æ¥ç¼–è¾‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å“åº”ã€‚è¿™ç§æ–¹æ³•åœ¨å¤šè½®æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘79.1%çš„ä»¤ç‰Œä½¿ç”¨ã€‚é€šè¿‡å®è¯è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°å°±åœ°åé¦ˆæ¯”ä¼ ç»Ÿçš„å¤šè½®åé¦ˆæ›´æœ‰æ•ˆï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°åº”ç”¨ç”¨æˆ·çš„åé¦ˆï¼Œé¿å…äº†æ¨¡å‹åœ¨ä¿®æ­£é”™è¯¯æ—¶å¼•å…¥æ–°çš„é”™è¯¯ã€‚æ€»çš„æ¥è¯´ï¼Œå°±åœ°åé¦ˆä¸ºæŒ‡å¯¼LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­æä¾›äº†ä¸€ç§æ›´è‡ªç„¶å’Œæœ‰æ•ˆçš„æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.19185', 'title': 'An Empirical Study of Testing Practices in Open Source AI Agent\n  Frameworks and Agentic Applications', 'url': 'https://huggingface.co/papers/2509.19185', 'abstract': 'The study identifies testing practices in AI agent frameworks and applications, highlighting a focus on deterministic components and a neglect of the Trigger component, suggesting improvements for robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents.', 'score': 2, 'issue_id': 6199, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 23', 'zh': '9æœˆ23æ—¥'}, 'hash': 'e00a1db1e43fa4c8', 'authors': ['Mohammed Mehedi Hasan', 'Hao Li', 'Emad Fallahzadeh', 'Gopi Krishnan Rajbahadur', 'Bram Adams', 'Ahmed E. Hassan'], 'affiliations': ['School of Computing, Queens University, Kingston, ON, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.19185.jpg', 'data': {'categories': ['#benchmark', '#agents', '#security', '#open_source', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğµ Ñ‚Ğ°Ğ¼, Ğ³Ğ´Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² 39 Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ°Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ 439 Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ´ĞµÑÑÑ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: Ğ±Ğ¾Ğ»ĞµĞµ 70% ÑƒÑĞ¸Ğ»Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ (Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ²Ğ¾Ñ€ĞºÑ„Ğ»Ğ¾Ñƒ), Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ foundation models Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 5% Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ (Trigger component) Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ, Ğ¿Ğ¾ÑĞ²Ğ»ÑÑÑÑŒ Ğ»Ğ¸ÑˆÑŒ Ğ² 1% Ñ‚ĞµÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ â€” Ğ²Ğ½ĞµĞ´Ñ€Ğ¸Ñ‚ÑŒ regression-Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing AI Agent Testing: Bridging the Gap in Robustness', 'desc': 'This paper investigates the testing practices used in AI agent frameworks and applications, revealing a significant focus on deterministic components while largely neglecting the Trigger component. The study analyzes 39 open-source frameworks and 439 applications, identifying ten distinct testing patterns, with traditional methods dominating the landscape. It highlights that over 70% of testing effort is spent on deterministic components, while less than 5% is allocated to the FM-based Plan Body, indicating a critical oversight. The authors suggest that improving testing methods and incorporating prompt regression testing are essential for enhancing the robustness of AI agents.'}, 'zh': {'title': 'æå‡AIä»£ç†æµ‹è¯•çš„é²æ£’æ€§', 'desc': 'æœ¬ç ”ç©¶åˆ†æäº†äººå·¥æ™ºèƒ½ä»£ç†æ¡†æ¶å’Œåº”ç”¨ä¸­çš„æµ‹è¯•å®è·µï¼Œå‘ç°ç›®å‰çš„æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç¡®å®šæ€§ç»„ä»¶ä¸Šï¼Œè€Œè§¦å‘ç»„ä»¶å´è¢«å¿½è§†ã€‚æˆ‘ä»¬å¯¹39ä¸ªå¼€æºä»£ç†æ¡†æ¶å’Œ439ä¸ªä»£ç†åº”ç”¨è¿›è¡Œäº†å¤§è§„æ¨¡å®è¯ç ”ç©¶ï¼Œè¯†åˆ«å‡ºåç§ä¸åŒçš„æµ‹è¯•æ¨¡å¼ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåƒDeepEvalè¿™æ ·çš„æ–°å‹ä»£ç†ç‰¹å®šæ–¹æ³•ä½¿ç”¨ç‡æä½ï¼Œè€Œä¼ ç»Ÿçš„è´Ÿé¢æµ‹è¯•å’Œæˆå‘˜æµ‹è¯•è¢«å¹¿æ³›åº”ç”¨ä»¥åº”å¯¹åŸºç¡€æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚ä¸ºäº†æé«˜AIä»£ç†çš„é²æ£’æ€§ï¼Œå¼€å‘è€…éœ€è¦æ”¹è¿›å¯¹æ–°æµ‹è¯•æ–¹æ³•çš„æ”¯æŒï¼Œå¹¶åœ¨åº”ç”¨ä¸­é‡‡ç”¨æç¤ºå›å½’æµ‹è¯•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01070', 'title': 'Eliciting Secret Knowledge from Language Models', 'url': 'https://huggingface.co/papers/2510.01070', 'abstract': 'Researchers develop and evaluate techniques to uncover hidden knowledge in large language models through black-box and white-box methods, with prefill attacks and logit lens being particularly effective.  \t\t\t\t\tAI-generated summary \t\t\t\t We study secret elicitation: discovering knowledge that an AI possesses but does not explicitly verbalize. As a testbed, we train three families of large language models (LLMs) to possess specific knowledge that they apply downstream but deny knowing when asked directly. For example, in one setting, we train an LLM to generate replies that are consistent with knowing the user is female, while denying this knowledge when asked directly. We then design various black-box and white-box secret elicitation techniques and evaluate them based on whether they can help an LLM auditor successfully guess the secret knowledge. Many of our techniques improve on simple baselines. Our most effective techniques (performing best in 2/3 settings) are based on prefill attacks, a black-box technique where the LLM reveals secret knowledge when generating a completion from a predefined prefix. In our remaining setting, white-box techniques based on logit lens and sparse autoencoders (SAEs) are most effective. We release our models and code, establishing a public benchmark for evaluating secret elicitation methods.', 'score': 1, 'issue_id': 6203, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'f0f210658ad43fde', 'authors': ['Bartosz CywiÅ„ski', 'Emil Ryd', 'Rowan Wang', 'Senthooran Rajamanoharan', 'Neel Nanda', 'Arthur Conmy', 'Samuel Marks'], 'affiliations': ['Anthropic', 'IDEAS Research Institute', 'University of Oxford', 'Warsaw University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.01070.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#interpretability', '#multimodal', '#hallucinations', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: ĞºĞ°Ğº Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ LLM Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ñ‚ÑŒÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ° ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğµ, Ğ½Ğ¾ ÑĞ²Ğ½Ğ¾ Ğ½Ğµ Ğ¾Ğ·Ğ²ÑƒÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚. Ğ”Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°Ñ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¾ Ğ¿Ğ¾Ğ»Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…. ĞĞ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ prefill-Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ² black-box Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ white-box Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ logit lens Ğ¸ sparse autoencoders. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ´, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· AI-ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Unveiling Secrets: Extracting Hidden Knowledge from Language Models', 'desc': 'This paper explores methods to extract hidden knowledge from large language models (LLMs) that they do not openly disclose. The researchers train LLMs to possess specific knowledge while denying it when questioned directly. They introduce both black-box and white-box techniques for secret elicitation, with prefill attacks showing significant effectiveness. The study also provides a public benchmark for evaluating these secret elicitation methods, contributing to the understanding of LLM behavior.'}, 'zh': {'title': 'æ­ç¤ºAIéšè—çŸ¥è¯†çš„åˆ›æ–°æŠ€æœ¯', 'desc': 'ç ”ç©¶äººå‘˜å¼€å‘å¹¶è¯„ä¼°äº†é€šè¿‡é»‘ç®±å’Œç™½ç®±æ–¹æ³•æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­éšè—çŸ¥è¯†çš„æŠ€æœ¯ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸‰ç±»å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨åº”ç”¨ç‰¹å®šçŸ¥è¯†æ—¶æ‹’ç»æ‰¿è®¤è¿™ä¸€çŸ¥è¯†ã€‚æˆ‘ä»¬è®¾è®¡äº†å¤šç§é»‘ç®±å’Œç™½ç®±çš„ç§˜å¯†å¼•å‡ºæŠ€æœ¯ï¼Œå¹¶è¯„ä¼°å®ƒä»¬åœ¨å¸®åŠ©å®¡è®¡è€…æˆåŠŸçŒœæµ‹ç§˜å¯†çŸ¥è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æŠ€æœ¯åœ¨å¤šä¸ªè®¾ç½®ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åŸºäºé¢„å¡«æ”»å‡»çš„é»‘ç®±æŠ€æœ¯å’ŒåŸºäºlogit lensçš„ç™½ç®±æŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01061', 'title': "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced\n  Wasserstein Distance for Variance Reduction", 'url': 'https://huggingface.co/papers/2510.01061', 'abstract': 'Reservoir SWD reduces variance in Sliced Wasserstein Distance, improving gradient stability and performance in vision and graphics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to compute for high dimensional distributions. The Sliced Wasserstein Distance (SWD) offers a scalable alternative, yet its Monte Carlo estimator suffers from high variance, resulting in noisy gradients and slow convergence. We introduce Reservoir SWD (ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively retain informative projection directions in optimization steps, resulting in stable gradients while remaining unbiased. Experiments on synthetic benchmarks and real-world tasks such as color correction and diffusion guidance show that ReSWD consistently outperforms standard SWD and other variance reduction baselines. Project page: https://reservoirswd.github.io/', 'score': 1, 'issue_id': 6203, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'a1e6afd30da770db', 'authors': ['Mark Boss', 'Andreas Engelhardt', 'Simon DonnÃ©', 'Varun Jampani'], 'affiliations': ['Stability AI', 'University of TÃ¼bingen'], 'pdf_title_img': 'assets/pdf/title_img/2510.01061.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#synthetic', '#diffusion', '#training', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ·ĞµÑ€Ğ²ÑƒĞ°Ñ€Ğ½ÑƒÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reservoir SWD Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸. ĞšĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Wasserstein distance ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ´Ğ¾Ñ€Ğ¾Ğ³ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ° ĞµĞ³Ğ¾ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Sliced Wasserstein Distance ÑÑ‚Ñ€Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Weighted Reservoir Sampling Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ†Ğ²ĞµÑ‚Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ReSWD Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ SWD.'}, 'en': {'title': 'Stabilizing Gradients with Reservoir SWD for Better Performance', 'desc': 'This paper presents Reservoir SWD (ReSWD), a novel approach that enhances the Sliced Wasserstein Distance (SWD) by reducing its variance. The traditional SWD is effective for matching distributions in vision and graphics but suffers from high variance in its Monte Carlo estimations, leading to unstable gradients. By incorporating Weighted Reservoir Sampling, ReSWD retains the most informative projection directions, which stabilizes the gradient and improves convergence rates. Experimental results demonstrate that ReSWD outperforms both standard SWD and other methods aimed at variance reduction in various tasks.'}, 'zh': {'title': 'Reservoir SWDï¼šæå‡åˆ‡ç‰‡Wassersteinè·ç¦»çš„ç¨³å®šæ€§ä¸æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºReservoir SWDï¼ˆReSWDï¼‰ï¼Œæ—¨åœ¨å‡å°‘åˆ‡ç‰‡Wassersteinè·ç¦»ï¼ˆSWDï¼‰ä¸­çš„æ–¹å·®ï¼Œä»è€Œæé«˜æ¢¯åº¦çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚ä¼ ç»Ÿçš„SWDåœ¨é«˜ç»´åˆ†å¸ƒä¸­è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œè€ŒReSWDé€šè¿‡å°†åŠ æƒæ°´åº“æŠ½æ ·æŠ€æœ¯æ•´åˆåˆ°SWDä¸­ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°ä¿ç•™æœ‰ç”¨çš„æŠ•å½±æ–¹å‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReSWDåœ¨åˆæˆåŸºå‡†å’Œå®é™…ä»»åŠ¡ï¼ˆå¦‚é¢œè‰²æ ¡æ­£å’Œæ‰©æ•£å¼•å¯¼ï¼‰ä¸­ï¼Œå‡ä¼˜äºæ ‡å‡†SWDå’Œå…¶ä»–æ–¹å·®å‡å°‘åŸºçº¿ã€‚è¯¥æ–¹æ³•ä¸ºè§†è§‰å’Œå›¾å½¢ä»»åŠ¡ä¸­çš„åˆ†å¸ƒåŒ¹é…æä¾›äº†æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00438', 'title': 'BindWeave: Subject-Consistent Video Generation via Cross-Modal\n  Integration', 'url': 'https://huggingface.co/papers/2510.00438', 'abstract': 'BindWeave, a unified framework using MLLM-DiT, enhances subject-consistent video generation by integrating deep cross-modal reasoning with diffusion transformers, achieving superior performance on OpenS2V.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.', 'score': 1, 'issue_id': 6209, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '3a3278716f39fb1e', 'authors': ['Zhaoyang Li', 'Dongjun Qian', 'Kai Su', 'Qishuai Diao', 'Xiangyang Xia', 'Chang Liu', 'Wenfei Yang', 'Tianzhu Zhang', 'Zehuan Yuan'], 'affiliations': ['ByteDance', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.00438.jpg', 'data': {'categories': ['#video', '#diffusion', '#open_source', '#multimodal', '#reasoning', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'BindWeave: ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ²', 'desc': 'BindWeave â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ diffusion transformer (DiT) Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. MLLM Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ Ñ€Ğ¾Ğ»Ğ¸, Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OpenS2V Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ open-source Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ñƒ.'}, 'en': {'title': 'BindWeave: Consistent Video Generation through Deep Cross-Modal Reasoning', 'desc': 'BindWeave is a new framework that improves video generation by ensuring that the subjects in the videos remain consistent with the prompts given. It uses a combination of a multimodal large language model and diffusion transformers to understand complex relationships and interactions between multiple subjects. This approach allows the model to generate high-quality videos that accurately reflect the specified details and dynamics of the scene. Experiments show that BindWeave outperforms other models in terms of subject consistency and overall video quality.'}, 'zh': {'title': 'BindWeaveï¼šæå‡è§†é¢‘ç”Ÿæˆçš„ä¸»é¢˜ä¸€è‡´æ€§', 'desc': 'BindWeaveæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨MLLM-DiTæŠ€æœ¯ï¼Œæå‡äº†è§†é¢‘ç”Ÿæˆä¸­çš„ä¸»é¢˜ä¸€è‡´æ€§ã€‚å®ƒé€šè¿‡æ·±åº¦è·¨æ¨¡æ€æ¨ç†ä¸æ‰©æ•£å˜æ¢å™¨çš„ç»“åˆï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨å¤„ç†å¤æ‚ç©ºé—´å…³ç³»å’Œå¤šä¸»ä½“äº¤äº’æ—¶çš„ä¸è¶³ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿå¤„ç†ä»å•ä¸€ä¸»ä½“åˆ°å¤æ‚å¤šä¸»ä½“åœºæ™¯çš„å„ç§è§†é¢‘ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBindWeaveåœ¨ä¸»é¢˜ä¸€è‡´æ€§ã€è‡ªç„¶æ€§å’Œæ–‡æœ¬ç›¸å…³æ€§æ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26514', 'title': 'BatonVoice: An Operationalist Framework for Enhancing Controllable\n  Speech Synthesis with Linguistic Intelligence from LLMs', 'url': 'https://huggingface.co/papers/2509.26514', 'abstract': "BatonVoice framework decouples instruction understanding from speech generation, using an LLM to create vocal feature plans and a specialized TTS model to produce speech, achieving strong performance in controllable and emotional speech synthesis with zero-shot cross-lingual generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model's ability to follow text instructions for controllable Text-to-Speech~(TTS). To address this, we propose a new paradigm inspired by ``operationalism'' that decouples instruction understanding from speech generation. We introduce BatonVoice, a framework where an LLM acts as a ``conductor'', understanding user instructions and generating a textual ``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS model, the ``orchestra'', then generates the speech from these features. To realize this component, we develop BatonTTS, a TTS model trained specifically for this task. Our experiments demonstrate that BatonVoice achieves strong performance in controllable and emotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.", 'score': 1, 'issue_id': 6215, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '4c62f45aca4d07bc', 'authors': ['Yue Wang', 'Ruotian Ma', 'Xingyu Chen', 'Zhengliang Shi', 'Wanshun Chen', 'Huang Liu', 'Jiadi Yao', 'Qu Yang', 'Qingxuan Jiang', 'Fanghua Ye', 'Juntao Li', 'Min Zhang', 'Zhaopeng Tu', 'Xiaolong Li', 'Linus'], 'affiliations': ['Soochow University', 'Tencent Multimodal Department'], 'pdf_title_img': 'assets/pdf/title_img/2509.26514.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#open_source', '#audio', '#games'], 'emoji': 'ğŸ¼', 'ru': {'title': 'Ğ”Ğ¸Ñ€Ğ¸Ğ¶Ñ‘Ñ€ Ğ¸ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€: LLM Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€ÑƒĞµÑ‚, TTS Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº BatonVoice, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸: LLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ ĞºĞ°Ğº "Ğ´Ğ¸Ñ€Ğ¸Ğ¶Ñ‘Ñ€", Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¿Ğ»Ğ°Ğ½ Ğ²Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº (Ğ²Ñ‹ÑĞ¾Ñ‚Ğ° Ñ‚Ğ¾Ğ½Ğ°, ÑĞ½ĞµÑ€Ğ³Ğ¸Ñ Ğ¸ Ñ‚.Ğ´.). ĞÑ‚Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ TTS-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BatonTTS Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºĞ°Ğº "Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€", ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑ Ñ€ĞµÑ‡ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… ÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¼ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ñ€ĞµÑ‡Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº zero-shot ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ â€” Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğº ÑĞ·Ñ‹ĞºĞ°Ğ¼, Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°Ğ²ÑˆĞ¸Ğ¼ÑÑ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Decoupling Understanding and Speech for Enhanced Synthesis', 'desc': 'The BatonVoice framework separates the understanding of instructions from the generation of speech, utilizing a Large Language Model (LLM) to create detailed vocal feature plans. This approach allows for more precise control over speech synthesis, enabling emotional and controllable outputs. A specialized Text-to-Speech (TTS) model, BatonTTS, then converts these plans into actual speech. The framework shows impressive performance, including the ability to generalize across languages without prior training, highlighting the effectiveness of using explicit vocal features.'}, 'zh': {'title': 'BatonVoiceï¼šè§£è€¦æŒ‡ä»¤ç†è§£ä¸è¯­éŸ³ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'BatonVoiceæ¡†æ¶å°†æŒ‡ä»¤ç†è§£ä¸è¯­éŸ³ç”Ÿæˆè§£è€¦ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆè¯­éŸ³ç‰¹å¾è®¡åˆ’ï¼Œå¹¶ä½¿ç”¨ä¸“é—¨çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹è¿›è¡Œè¯­éŸ³åˆæˆã€‚è¯¥æ–¹æ³•åœ¨å¯æ§å’Œæƒ…æ„Ÿè¯­éŸ³åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶å®ç°äº†é›¶æ ·æœ¬è·¨è¯­è¨€æ³›åŒ–ã€‚é€šè¿‡å°†è¯­éŸ³å¯¹è±¡åŒ–ä¸ºæ–‡æœ¬è¯­éŸ³ç‰¹å¾ï¼ŒBatonVoiceæœ‰æ•ˆåœ°é‡Šæ”¾äº†LLMçš„è¯­è¨€æ™ºèƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBatonVoiceåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å¼ºå¤§çš„å¼€æºå’Œé—­æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25916', 'title': 'VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained\n  Perception in VLMs', 'url': 'https://huggingface.co/papers/2509.25916', 'abstract': "VLM-FO1 enhances vision-language models with a hybrid fine-grained region encoder to improve object localization and region understanding without sacrificing general visual capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel at high-level scene understanding but falter on fine-grained perception tasks requiring precise localization. This failure stems from a fundamental mismatch, as generating exact numerical coordinates is a challenging task for language-centric architectures. In this paper, we introduce VLM-FO1, a novel framework that overcomes this limitation by reframing object-centric perception from a brittle coordinate generation problem into a robust feature retrieval task. Our method operates as a plug-and-play module that integrates with any pre-trained VLM. It leverages a Hybrid Fine-grained Region Encoder (HFRE), featuring a dual vision encoder, to generate powerful region tokens rich in both semantic and spatial detail. A token-based referencing system then enables the LLM to seamlessly reason about and ground language in these specific visual regions. Experiments show that VLM-FO1 achieves state-of-the-art performance across a diverse suite of benchmarks, demonstrating exceptional capabilities in object grounding, region generational understanding, and visual region reasoning. Crucially, our two-stage training strategy ensures that these perception gains are achieved without compromising the base model's general visual understanding capabilities. VLM-FO1 establishes an effective and flexible paradigm for building perception-aware VLMs, bridging the gap between high-level reasoning and fine-grained visual grounding.", 'score': 1, 'issue_id': 6204, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '93fe431b882d5210', 'authors': ['Peng Liu', 'Haozhan Shen', 'Chunxin Fang', 'Zhicheng Sun', 'Jiajia Liao', 'Tiancheng Zhao'], 'affiliations': ['Binjiang Institute of Zhejiang University', 'College of Computer Science and Technology, Zhejiang University', 'Om AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.25916.jpg', 'data': {'categories': ['#training', '#architecture', '#multimodal', '#reasoning', '#agi', '#cv', '#interpretability', '#benchmark'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞÑ‚ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'VLM-FO1 Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸Ğ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² (HFRE) Ñ Ğ´Ğ²ÑƒĞ¼Ñ vision-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ², Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² grounding Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ². Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ¾Ğ±Ñ‰ĞµĞ¼Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ´ĞµĞ»Ğ°Ñ VLM-FO1 ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ plug-and-play Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼.'}, 'en': {'title': 'Bridging High-Level Reasoning and Fine-Grained Visual Grounding', 'desc': 'VLM-FO1 is a new framework designed to enhance vision-language models (VLMs) by improving their ability to locate and understand specific regions in images. Traditional VLMs struggle with precise localization due to their focus on language, which makes generating exact coordinates difficult. This paper introduces a Hybrid Fine-grained Region Encoder (HFRE) that transforms the localization challenge into a feature retrieval task, allowing for better integration of visual and semantic information. The results show that VLM-FO1 not only excels in object grounding and region understanding but also maintains the general visual capabilities of the original model, making it a versatile tool for perception-aware VLMs.'}, 'zh': {'title': 'VLM-FO1ï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç‰©ä½“å®šä½èƒ½åŠ›', 'desc': 'VLM-FO1 æ˜¯ä¸€ç§å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹å–„ç‰©ä½“å®šä½å’ŒåŒºåŸŸç†è§£èƒ½åŠ›ã€‚å®ƒé€šè¿‡æ··åˆç»†ç²’åº¦åŒºåŸŸç¼–ç å™¨ï¼Œå°†ç‰©ä½“ä¸­å¿ƒæ„ŸçŸ¥é—®é¢˜è½¬å˜ä¸ºå¼ºå¤§çš„ç‰¹å¾æ£€ç´¢ä»»åŠ¡ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿè¯­è¨€æ¨¡å‹åœ¨ç²¾ç¡®åæ ‡ç”Ÿæˆä¸Šçš„å±€é™ã€‚è¯¥æ–¹æ³•å¯ä»¥ä½œä¸ºæ’ä»¶ä¸ä»»ä½•é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹é›†æˆï¼Œåˆ©ç”¨åŒè§†è§‰ç¼–ç å™¨ç”Ÿæˆä¸°å¯Œçš„åŒºåŸŸæ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLM-FO1 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸå®ç°äº†ç‰©ä½“å®šä½å’Œè§†è§‰åŒºåŸŸæ¨ç†çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25411', 'title': 'Boolean Satisfiability via Imitation Learning', 'url': 'https://huggingface.co/papers/2509.25411', 'abstract': 'ImitSAT, a branching policy for CDCL solvers using imitation learning from expert traces, reduces propagation counts and runtime by providing dense decision-level supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT', 'score': 1, 'issue_id': 6199, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '8518038ce5781379', 'authors': ['Zewei Zhang', 'Huan Liu', 'Yuanhao Yu', 'Jun Chen', 'Xiangyu Xu'], 'affiliations': ['McMaster University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25411.jpg', 'data': {'categories': ['#rl', '#open_source', '#training', '#optimization', '#math'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ÑÑĞ°Ñ… Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ SAT-ÑĞ¾Ğ»Ğ²ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ImitSAT - Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ CDCL ÑĞ¾Ğ»Ğ²ĞµÑ€Ğ¾Ğ² SAT-Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° imitation learning. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ÑÑĞ°Ñ… KeyTrace, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ supervised ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ propagations - Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ImitSAT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ML, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ propagations.'}, 'en': {'title': 'ImitSAT: Learning from Experts for Faster SAT Solving', 'desc': 'ImitSAT is a new branching policy designed for conflict-driven clause learning (CDCL) solvers that uses imitation learning from expert traces to improve performance on the Boolean satisfiability problem (SAT). Unlike traditional methods that rely on indirect signals or reinforcement learning, ImitSAT directly learns from a sequence of expert decisions, known as KeyTrace, which simplifies the decision-making process. This approach minimizes conflicts during execution, leading to fewer propagation counts and reduced runtime. The results show that ImitSAT significantly outperforms existing learned methods, providing a more efficient and effective solution for SAT problems.'}, 'zh': {'title': 'ImitSATï¼šé«˜æ•ˆçš„CDCLæ±‚è§£å™¨åˆ†æ”¯ç­–ç•¥', 'desc': 'ImitSATæ˜¯ä¸€ç§åŸºäºæ¨¡ä»¿å­¦ä¹ çš„åˆ†æ”¯ç­–ç•¥ï¼Œä¸“ä¸ºå†²çªé©±åŠ¨å­å¥å­¦ä¹ ï¼ˆCDCLï¼‰æ±‚è§£å™¨è®¾è®¡ï¼Œæ—¨åœ¨è§£å†³å¸ƒå°”å¯æ»¡è¶³æ€§é—®é¢˜ï¼ˆSATï¼‰ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒImitSATé€šè¿‡å­¦ä¹ ä¸“å®¶çš„KeyTraceï¼Œç›´æ¥æä¾›å†³ç­–çº§çš„ç›‘ç£ï¼Œä»è€Œå‡å°‘ä¼ æ’­æ¬¡æ•°å’Œè¿è¡Œæ—¶é—´ã€‚é€šè¿‡åœ¨åŒä¸€å®ä¾‹ä¸Šé‡æ”¾KeyTraceï¼ŒImitSATå‡ ä¹æ²¡æœ‰å†²çªï¼Œæ˜¾è‘—æé«˜äº†åˆ†æ”¯çš„è´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒImitSATåœ¨ä¼ æ’­æ¬¡æ•°å’Œè¿è¡Œæ—¶é—´ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›å­¦ä¹ æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25045', 'title': 'Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures', 'url': 'https://huggingface.co/papers/2509.25045', 'abstract': "A novel Hyperdimensional Probe method decodes information from LLM vector spaces using Vector Symbolic Architectures, providing interpretable insights into model states and failures.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods, such as direct logit attribution (DLA) and sparse autoencoders (SAEs), provide restricted insight due to limitations such as the model's output vocabulary or unclear feature names. This work introduces Hyperdimensional Probe, a novel paradigm for decoding information from the LLM vector space. It combines ideas from symbolic representations and neural probing to project the model's residual stream into interpretable concepts via Vector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs and conventional probes while overcoming their key limitations. We validate our decoding paradigm with controlled input-completion tasks, probing the model's final state before next-token prediction on inputs spanning syntactic pattern recognition, key-value associations, and abstract inference. We further assess it in a question-answering setting, examining the state of the model both before and after text generation. Our experiments show that our probe reliably extracts meaningful concepts across varied LLMs, embedding sizes, and input domains, also helping identify LLM failures. Our work advances information decoding in LLM vector space, enabling extracting more informative, interpretable, and structured features from neural representations.", 'score': 1, 'issue_id': 6204, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': 'b34a8aa388991a99', 'authors': ['Marco Bronzini', 'Carlo Nicolini', 'Bruno Lepri', 'Jacopo Staiano', 'Andrea Passerini'], 'affiliations': ['Fondazione Bruno Kessler (FBK), Trento, Italy', 'Ipazia S.p.A., Milan, Italy', 'University of Trento, Trento, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2509.25045.jpg', 'data': {'categories': ['#data', '#architecture', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“Ğ¸Ğ¿ĞµÑ€Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ñ‹ÑĞ»ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Hyperdimensional Probe Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Vector Symbolic Architectures (VSA) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ residual stream Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ€Ğ¾Ğ´Ğµ sparse autoencoders. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ¾Ğ½Ğ´ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ², Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ¸ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸.'}, 'en': {'title': 'Decoding LLMs: Unveiling Insights with Hyperdimensional Probes', 'desc': 'The paper presents a new method called Hyperdimensional Probe that helps decode information from the vector spaces of Large Language Models (LLMs) using Vector Symbolic Architectures (VSAs). This method aims to improve interpretability by providing clearer insights into the internal workings and potential failures of LLMs, which are often difficult to understand. Unlike existing methods like direct logit attribution and sparse autoencoders, Hyperdimensional Probe combines the strengths of these approaches while addressing their limitations. The authors validate their method through various tasks, demonstrating its ability to extract meaningful concepts and enhance our understanding of LLM behavior.'}, 'zh': {'title': 'è¶…ç»´æ¢æµ‹å™¨ï¼šè§£ç å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¶…ç»´æ¢æµ‹å™¨æ–¹æ³•ï¼Œé€šè¿‡å‘é‡ç¬¦å·æ¶æ„ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘é‡ç©ºé—´ä¸­è§£ç ä¿¡æ¯ï¼Œæä¾›å¯¹æ¨¡å‹çŠ¶æ€å’Œå¤±è´¥çš„å¯è§£é‡Šæ€§æ´å¯Ÿã€‚ç°æœ‰çš„å¯è§£é‡Šæ€§æ–¹æ³•å¦‚ç›´æ¥é€»è¾‘å½’å› å’Œç¨€ç–è‡ªç¼–ç å™¨ç”±äºæ¨¡å‹è¾“å‡ºè¯æ±‡æˆ–ç‰¹å¾åç§°ä¸æ¸…æ™°ç­‰é™åˆ¶ï¼Œæä¾›çš„æ´å¯ŸåŠ›æœ‰é™ã€‚è¶…ç»´æ¢æµ‹å™¨ç»“åˆäº†ç¬¦å·è¡¨ç¤ºå’Œç¥ç»æ¢æµ‹çš„æ€æƒ³ï¼Œå°†æ¨¡å‹çš„æ®‹å·®æµæŠ•å½±åˆ°å¯è§£é‡Šçš„æ¦‚å¿µä¸­ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¢æµ‹å™¨èƒ½å¤Ÿå¯é åœ°æå–æœ‰æ„ä¹‰çš„æ¦‚å¿µï¼Œå¹¶å¸®åŠ©è¯†åˆ«LLMçš„å¤±è´¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00225', 'title': 'TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic\n  Tasks', 'url': 'https://huggingface.co/papers/2510.00225', 'abstract': 'TGPO, a Temporal Grounded Policy Optimization framework, decomposes STL tasks into subgoals and uses a hierarchical approach with dense rewards to improve task success rates in complex, long-horizon robotics tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO', 'score': 0, 'issue_id': 6211, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '67afdd043a84f208', 'authors': ['Yue Meng', 'Fei Chen', 'Chuchu Fan'], 'affiliations': ['Massachusetts Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.00225.jpg', 'data': {'categories': ['#training', '#robotics', '#rl', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TGPO - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Signal Temporal Logic (STL). ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ STL-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ†ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ´Ñ†ĞµĞ»ÑĞ¼Ğ¸, Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¸Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞœĞµÑ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ»Ğ¸ÑĞ°-Ğ¥Ğ°ÑÑ‚Ğ¸Ğ½Ğ³ÑĞ° Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¼ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ³Ğ¸Ğ´Ğ°. TGPO Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° 31.6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸, Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ´Ñ€Ğ¾Ğ½Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'TGPO: Mastering Complex Robotics Tasks with Temporal Grounded Policy Optimization', 'desc': "The paper introduces TGPO, a framework designed to optimize policies for complex robotics tasks specified by Signal Temporal Logic (STL). It addresses the challenges of STL's non-Markovian nature and sparse rewards by breaking down tasks into manageable subgoals and using a hierarchical approach. TGPO employs a high-level component to allocate time for these subgoals and a low-level policy that learns to achieve them with dense rewards. Experimental results show that TGPO significantly improves task success rates in various environments, outperforming existing methods by an average of 31.6%."}, 'zh': {'title': 'TGPOï¼šæå‡å¤æ‚ä»»åŠ¡æˆåŠŸç‡çš„åˆ†å±‚ç­–ç•¥ä¼˜åŒ–', 'desc': 'TGPOï¼ˆæ—¶é—´åŸºç¡€ç­–ç•¥ä¼˜åŒ–ï¼‰æ¡†æ¶å°†ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰ä»»åŠ¡åˆ†è§£ä¸ºå­ç›®æ ‡ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚æ–¹æ³•ç»“åˆå¯†é›†å¥–åŠ±ï¼Œä»¥æé«˜å¤æ‚é•¿æ—¶é—´æœºå™¨äººä»»åŠ¡çš„æˆåŠŸç‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨å¤„ç†éé©¬å°”å¯å¤«æ€§è´¨å’Œç¨€ç–å¥–åŠ±æ—¶çš„å›°éš¾ã€‚TGPOé€šè¿‡é«˜å±‚ç»„ä»¶ä¸ºå­ç›®æ ‡æä¾›å…·ä½“çš„æ—¶é—´åˆ†é…ï¼Œå¹¶é€šè¿‡ä½å±‚æ—¶é—´æ¡ä»¶ç­–ç•¥å­¦ä¹ å®ç°è¿™äº›å­ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTGPOåœ¨å¤šç§ç¯å¢ƒä¸‹æ˜¾è‘—ä¼˜äºç°æœ‰çš„åŸºçº¿æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨é«˜ç»´å’Œé•¿æ—¶é—´ä»»åŠ¡ä¸­ï¼Œä»»åŠ¡æˆåŠŸç‡å¹³å‡æé«˜äº†31.6%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25162', 'title': 'Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models', 'url': 'https://huggingface.co/papers/2509.25162', 'abstract': 'Pretrained visual encoders are aligned as tokenizers for latent diffusion models, improving image generation quality and convergence speed.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we propose aligning pretrained visual encoders to serve as tokenizers for latent diffusion models in image generation. Unlike training a variational autoencoder (VAE) from scratch, which primarily emphasizes low-level details, our approach leverages the rich semantic structure of foundation encoders. We introduce a three-stage alignment strategy: (1) freeze the encoder and train an adapter and a decoder to establish a semantic latent space; (2) jointly optimize all components with an additional semantic preservation loss, enabling the encoder to capture perceptual details while retaining high-level semantics; and (3) refine the decoder for improved reconstruction quality. This alignment yields semantically rich image tokenizers that benefit diffusion models. On ImageNet 256times256, our tokenizer accelerates the convergence of diffusion models, reaching a gFID of 1.90 within just 64 epochs, and improves generation both with and without classifier-free guidance. Scaling to LAION, a 2B-parameter text-to-image model trained with our tokenizer consistently outperforms FLUX VAE under the same training steps. Overall, our method is simple, scalable, and establishes a semantically grounded paradigm for continuous tokenizer design.', 'score': 0, 'issue_id': 6215, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '0dec3eb0e2822ec8', 'authors': ['Bowei Chen', 'Sai Bi', 'Hao Tan', 'He Zhang', 'Tianyuan Zhang', 'Zhengqi Li', 'Yuanjun Xiong', 'Jianming Zhang', 'Kai Zhang'], 'affiliations': ['Adobe', 'Massachusetts Institute of Technology', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2509.25162.jpg', 'data': {'categories': ['#optimization', '#rag', '#diffusion', '#cv'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ· Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ latent diffusion Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ VAE Ñ Ğ½ÑƒĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ñ‘Ñ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºÑƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒÑĞºĞ¾Ñ€ÑÑÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ½Ğ° ImageNet 256Ã—256 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ gFID 1.90 Ğ²ÑĞµĞ³Ğ¾ Ğ·Ğ° 64 ÑĞ¿Ğ¾Ñ…Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ²Ñ€Ğ¾Ğ´Ğµ LAION, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ text-to-image Ñ 2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ FLUX VAE Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Aligning Visual Encoders for Superior Image Generation', 'desc': 'This paper presents a method to enhance image generation by aligning pretrained visual encoders as tokenizers for latent diffusion models. Instead of starting from scratch with a variational autoencoder (VAE), the authors utilize the semantic richness of existing encoders to improve the quality of generated images. They propose a three-stage alignment strategy that includes freezing the encoder, optimizing all components together, and refining the decoder for better image reconstruction. The results show that their approach significantly accelerates convergence and improves image generation performance, establishing a new standard for tokenizer design in machine learning.'}, 'zh': {'title': 'å¯¹é½é¢„è®­ç»ƒç¼–ç å™¨ï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å¯¹é½ä½œä¸ºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ ‡è®°å™¨ï¼Œä»¥æé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚ä¸ä»å¤´å¼€å§‹è®­ç»ƒå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†åŸºç¡€ç¼–ç å™¨çš„ä¸°å¯Œè¯­ä¹‰ç»“æ„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸‰é˜¶æ®µçš„å¯¹é½ç­–ç•¥ï¼Œé¦–å…ˆå†»ç»“ç¼–ç å™¨å¹¶è®­ç»ƒé€‚é…å™¨å’Œè§£ç å™¨ä»¥å»ºç«‹è¯­ä¹‰æ½œåœ¨ç©ºé—´ã€‚é€šè¿‡è¿™ç§å¯¹é½ï¼Œæˆ‘ä»¬çš„æ ‡è®°å™¨èƒ½å¤ŸåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ”¶æ•›ï¼Œå¹¶åœ¨å›¾åƒç”Ÿæˆä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01068', 'title': 'Compose Your Policies! Improving Diffusion-based or Flow-based Robot\n  Policies via Test-time Distribution-level Composition', 'url': 'https://huggingface.co/papers/2510.01068', 'abstract': 'General Policy Composition (GPC) enhances robotic control performance by combining pre-trained diffusion-based policies without additional training, leading to superior results across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based models for robotic control, including vision-language-action (VLA) and vision-action (VA) policies, have demonstrated significant capabilities. Yet their advancement is constrained by the high cost of acquiring large-scale interaction datasets. This work introduces an alternative paradigm for enhancing policy performance without additional model training. Perhaps surprisingly, we demonstrate that the composed policies can exceed the performance of either parent policy. Our contribution is threefold. First, we establish a theoretical foundation showing that the convex composition of distributional scores from multiple diffusion models can yield a superior one-step functional objective compared to any individual score. A Gr\\"onwall-type bound is then used to show that this single-step improvement propagates through entire generation trajectories, leading to systemic performance gains. Second, motivated by these results, we propose General Policy Composition (GPC), a training-free method that enhances performance by combining the distributional scores of multiple pre-trained policies via a convex combination and test-time search. GPC is versatile, allowing for the plug-and-play composition of heterogeneous policies, including VA and VLA models, as well as those based on diffusion or flow-matching, irrespective of their input visual modalities. Third, we provide extensive empirical validation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside real-world robotic evaluations, confirm that GPC consistently improves performance and adaptability across a diverse set of tasks. Further analysis of alternative composition operators and weighting strategies offers insights into the mechanisms underlying the success of GPC. These results establish GPC as a simple yet effective method for improving control performance by leveraging existing policies.', 'score': 6, 'issue_id': 6252, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'f7a26368ff58e67e', 'authors': ['Jiahang Cao', 'Yize Huang', 'Hanzhong Guo', 'Rui Zhang', 'Mu Nan', 'Weijian Mai', 'Jiaxu Wang', 'Hao Cheng', 'Jingkai Sun', 'Gang Han', 'Wen Zhao', 'Qiang Zhang', 'Yijie Guo', 'Qihao Zheng', 'Chunfeng Song', 'Xiao Li', 'Ping Luo', 'Andrew F. Luo'], 'affiliations': ['Beijing Innovation Center of Humanoid Robotics', 'Shanghai AI Lab', 'Shanghai Jiaotong University', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.01068.jpg', 'data': {'categories': ['#training', '#robotics', '#optimization', '#benchmark', '#diffusion', '#agents'], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ policy Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ General Policy Composition (GPC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… diffusion-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ¿ÑƒĞºĞ»Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ policy. GPC Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ â€” vision-language-action (VLA) Ğ¸ vision-action (VA), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° diffusion Ğ¸Ğ»Ğ¸ flow-matching. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Robomimic, PushT, RoboTwin Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Robotic Control with Policy Composition', 'desc': 'General Policy Composition (GPC) is a novel approach that enhances robotic control by combining pre-trained diffusion-based policies without the need for additional training. This method leverages the strengths of multiple policies, including vision-language-action and vision-action models, to achieve superior performance on various benchmarks. The theoretical foundation of GPC shows that combining distributional scores from different models can lead to better outcomes than using any single model alone. Extensive experiments demonstrate that GPC not only improves performance but also increases adaptability across diverse robotic tasks, making it a versatile tool in the field of robotic control.'}, 'zh': {'title': 'é€šç”¨ç­–ç•¥ç»„åˆï¼šæå‡æœºå™¨äººæ§åˆ¶æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºé€šç”¨ç­–ç•¥ç»„åˆï¼ˆGPCï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç­–ç•¥æ¥æå‡æœºå™¨äººæ§åˆ¶æ€§èƒ½ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œç»„åˆåçš„ç­–ç•¥åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå•ç‹¬çš„çˆ¶ç­–ç•¥ã€‚GPCåˆ©ç”¨å‡¸ç»„åˆçš„æ–¹å¼ï¼Œå°†å¤šä¸ªç­–ç•¥çš„åˆ†å¸ƒå¾—åˆ†è¿›è¡Œç»“åˆï¼Œä»è€Œå®ç°ç³»ç»Ÿæ€§çš„æ€§èƒ½æå‡ã€‚é€šè¿‡åœ¨å¤šä¸ªæœºå™¨äººä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜ï¼ŒGPCåœ¨æé«˜é€‚åº”æ€§å’Œæ€§èƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02665', 'title': 'Self-Improvement in Multimodal Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2510.02665', 'abstract': 'A survey of self-improvement methods in Multimodal Large Language Models (MLLMs) from data collection, organization, and model optimization perspectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in self-improvement for Large Language Models (LLMs) have efficiently enhanced model capabilities without significantly increasing costs, particularly in terms of human effort. While this area is still relatively young, its extension to the multimodal domain holds immense potential for leveraging diverse data sources and developing more general self-improving models. This survey is the first to provide a comprehensive overview of self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview of the current literature and discuss methods from three perspectives: 1) data collection, 2) data organization, and 3) model optimization, to facilitate the further development of self-improvement in MLLMs. We also include commonly used evaluations and downstream applications. Finally, we conclude by outlining open challenges and future research directions.', 'score': 5, 'issue_id': 6252, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}, 'hash': 'a7980db6477e39f7', 'authors': ['Shijian Deng', 'Kai Wang', 'Tianyu Yang', 'Harsh Singh', 'Yapeng Tian'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence', 'The University of Texas at Dallas', 'University of Notre Dame', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2510.02665.jpg', 'data': {'categories': ['#training', '#survey', '#optimization', '#multimodal', '#data', '#dataset'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ ÑĞ°Ğ¼Ğ¸: Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ñ Ñ‚Ñ€Ñ‘Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ: ÑĞ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ¸Ğ»Ğ¸Ğ¹. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Potential: Self-Improvement in Multimodal Language Models', 'desc': 'This paper surveys self-improvement methods in Multimodal Large Language Models (MLLMs), focusing on how to enhance model performance through better data handling and optimization techniques. It highlights the importance of efficiently collecting and organizing diverse data sources to improve model capabilities without incurring high costs. The authors provide a structured overview of existing literature and categorize methods into three main areas: data collection, data organization, and model optimization. Additionally, the paper discusses evaluation metrics and potential applications, while identifying challenges and future research opportunities in the field.'}, 'zh': {'title': 'å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›æ½œåŠ›', 'desc': 'æœ¬è®ºæ–‡å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„è‡ªæˆ‘æ”¹è¿›æ–¹æ³•è¿›è¡Œäº†å…¨é¢è°ƒæŸ¥ã€‚æˆ‘ä»¬ä»æ•°æ®æ”¶é›†ã€æ•°æ®ç»„ç»‡å’Œæ¨¡å‹ä¼˜åŒ–ä¸‰ä¸ªè§’åº¦ï¼Œç³»ç»Ÿæ€§åœ°å›é¡¾äº†å½“å‰æ–‡çŒ®ï¼Œæ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆæå‡æ¨¡å‹èƒ½åŠ›ã€‚å°½ç®¡è¿™ä¸€é¢†åŸŸä»åœ¨å‘å±•ä¸­ï¼Œä½†å…¶åœ¨å¤šæ¨¡æ€é¢†åŸŸçš„æ‰©å±•å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¯ä»¥åˆ©ç”¨å¤šæ ·çš„æ•°æ®æºã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜å’Œæœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01879', 'title': 'REPAIR: Robust Editing via Progressive Adaptive Intervention and\n  Reintegration', 'url': 'https://huggingface.co/papers/2510.01879', 'abstract': 'REPAIR is a lifelong editing framework for large language models that enhances editing accuracy and reduces knowledge forgetting through progressive adaptive intervention and reintegration.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training for large language models (LLMs) is constrained by the high cost of acquiring new knowledge or correcting errors and by the unintended side effects that frequently arise from retraining. To address these issues, we introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and Reintegration), a lifelong editing framework designed to support precise and low-cost model updates while preserving non-target knowledge. REPAIR mitigates the instability and conflicts of large-scale sequential edits through a closed-loop feedback mechanism coupled with dynamic memory management. Furthermore, by incorporating frequent knowledge fusion and enforcing strong locality guards, REPAIR effectively addresses the shortcomings of traditional distribution-agnostic approaches that often overlook unintended ripple effects. Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting. This work introduces a robust framework for developing reliable, scalable, and continually evolving LLMs.', 'score': 2, 'issue_id': 6253, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '6a90da50c87cca3c', 'authors': ['Yisu Wang', 'Ming Wang', 'Haoyuan Song', 'Wenjie Huang', 'Chaozheng Wang', 'Yi Xie', 'Xuming Ran'], 'affiliations': ['ContiAI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.01879.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº REPAIR Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½ĞµÑĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ². Ğ§Ğ°ÑÑ‚Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ±Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° 10-30% Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ½ĞµĞµ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Language Models with REPAIR: Accurate, Cost-effective, and Knowledge-preserving Editing', 'desc': 'REPAIR is a framework designed to improve the editing process of large language models (LLMs) by making it more accurate and cost-effective. It allows models to learn new information or correct mistakes without losing previously learned knowledge. The framework uses a feedback system and manages memory dynamically to handle multiple edits without causing conflicts. Experiments show that REPAIR increases editing accuracy significantly while minimizing knowledge loss, making it a valuable tool for evolving LLMs.'}, 'zh': {'title': 'REPAIRï¼šæå‡è¯­è¨€æ¨¡å‹ç¼–è¾‘å‡†ç¡®æ€§çš„ç»ˆèº«æ¡†æ¶', 'desc': 'REPAIRæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»ˆèº«ç¼–è¾‘æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç¼–è¾‘çš„å‡†ç¡®æ€§å¹¶å‡å°‘çŸ¥è¯†é—å¿˜ã€‚å®ƒé€šè¿‡æ¸è¿›å¼çš„é€‚åº”æ€§å¹²é¢„å’Œå†æ•´åˆæ¥å®ç°ä½æˆæœ¬çš„æ¨¡å‹æ›´æ–°ï¼ŒåŒæ—¶ä¿æŠ¤éç›®æ ‡çŸ¥è¯†ã€‚REPAIRé‡‡ç”¨é—­ç¯åé¦ˆæœºåˆ¶å’ŒåŠ¨æ€è®°å¿†ç®¡ç†ï¼Œç¼“è§£äº†å¤§è§„æ¨¡é¡ºåºç¼–è¾‘å¸¦æ¥çš„ä¸ç¨³å®šæ€§å’Œå†²çªã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒREPAIRåœ¨å¤šä¸ªæ¨¡å‹å®¶æ—ä¸­æé«˜äº†10%-30%çš„ç¼–è¾‘å‡†ç¡®æ€§ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†çŸ¥è¯†é—å¿˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.03230', 'title': 'Improving GUI Grounding with Explicit Position-to-Coordinate Mapping', 'url': 'https://huggingface.co/papers/2510.03230', 'abstract': 'Explicit coordinate markers and improved spatial encoding enhance GUI grounding accuracy across diverse resolutions and platforms.  \t\t\t\t\tAI-generated summary \t\t\t\t GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms.', 'score': 1, 'issue_id': 6252, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}, 'hash': '8911479d98450376', 'authors': ['Suyuchen Wang', 'Tianyu Zhang', 'Ahmed Masry', 'Christopher Pal', 'Spandana Gella', 'Bang Liu', 'Perouz Taslakian'], 'affiliations': ['CIFAR AI Chair', 'McGill University', 'Mila - Quebec AI Institute', 'Polytechnique Montreal', 'ServiceNow', 'Universite de Montreal', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2510.03230.jpg', 'data': {'categories': ['#interpretability', '#agents', '#cv', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¯Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ GUI grounding â€” Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞºÑ€Ğ°Ğ½Ğµ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞºÑ€Ğ°Ğ½Ğ¾Ğ², Ğ½Ğµ Ğ²ÑÑ‚Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ: RULER tokens â€” ÑĞ²Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ°Ğº Ğ»Ğ¸Ğ½Ğ¸Ğ¸ ÑĞµÑ‚ĞºĞ¸ Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğµ, Ğ¸ Interleaved MRoPE (I-MRoPE) â€” ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñƒ Ğ¸ Ğ²Ñ‹ÑĞ¾Ñ‚Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞºÑ€Ğ°Ğ½Ğ°Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing GUI Grounding with Explicit Spatial Markers', 'desc': 'This paper focuses on improving GUI grounding, which is the process of translating natural language commands into specific pixel locations on a screen. The authors identify that existing vision-language models (VLMs) struggle with high-resolution displays due to their reliance on implicit mappings from visual features to pixel coordinates. To overcome this, they introduce RULER tokens as explicit coordinate markers, allowing the model to reference positions more accurately. Additionally, they propose Interleaved MRoPE (I-MRoPE) to enhance spatial encoding, ensuring that both width and height are treated equally, leading to significant improvements in grounding accuracy across various resolutions.'}, 'zh': {'title': 'æå‡GUIå®šä½å‡†ç¡®æ€§çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰å®šä½çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é«˜åˆ†è¾¨ç‡æ˜¾ç¤ºå™¨ä¸Šçš„å‡†ç¡®æ€§é—®é¢˜ã€‚å½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å°†è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ˜ å°„åˆ°åƒç´ åæ ‡æ—¶ï¼Œé¢ä¸´ç€å¯é çš„è¡¥ä¸åˆ°åƒç´ æ˜ å°„çš„ç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§åˆ›æ–°æ–¹æ³•ï¼šä½¿ç”¨RULERæ ‡è®°ä½œä¸ºæ˜ç¡®çš„åæ ‡æ ‡è®°ï¼Œä»¥åŠæ”¹è¿›ç©ºé—´ç¼–ç çš„äº¤é”™MRoPEï¼ˆI-MRoPEï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨ä¸åŒåˆ†è¾¨ç‡å’Œå¹³å°ä¸Šæ˜¾è‘—æé«˜äº†GUIå®šä½çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.03204', 'title': 'FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents', 'url': 'https://huggingface.co/papers/2510.03204', 'abstract': 'FocusAgent uses a lightweight LLM retriever to extract relevant content from web page observations, improving efficiency and security in web agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents powered by large language models (LLMs) must process lengthy web page observations to complete user goals; these pages often exceed tens of thousands of tokens. This saturates context limits and increases computational cost processing; moreover, processing full pages exposes agents to security risks such as prompt injection. Existing pruning strategies either discard relevant content or retain irrelevant context, leading to suboptimal action prediction. We introduce FocusAgent, a simple yet effective approach that leverages a lightweight LLM retriever to extract the most relevant lines from accessibility tree (AxTree) observations, guided by task goals. By pruning noisy and irrelevant content, FocusAgent enables efficient reasoning while reducing vulnerability to injection attacks. Experiments on WorkArena and WebArena benchmarks show that FocusAgent matches the performance of strong baselines, while reducing observation size by over 50%. Furthermore, a variant of FocusAgent significantly reduces the success rate of prompt-injection attacks, including banner and pop-up attacks, while maintaining task success performance in attack-free settings. Our results highlight that targeted LLM-based retrieval is a practical and robust strategy for building web agents that are efficient, effective, and secure.', 'score': 1, 'issue_id': 6252, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}, 'hash': 'cff617954ead65b4', 'authors': ['Imene Kerboua', 'Sahar Omidi Shayegan', 'Megh Thakkar', 'Xing Han LÃ¹', 'LÃ©o Boisvert', 'Massimo Caccia', 'JÃ©rÃ©my Espinas', 'Alexandre Aussem', 'VÃ©ronique Eglin', 'Alexandre Lacoste'], 'affiliations': ['Esker', 'LIRIS - CNRS, INSA Lyon, Universite Claude Bernard Lyon 1', 'McGill University', 'Mila - Quebec AI Institute', 'Polytechnique MontrÃ©al', 'ServiceNow Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.03204.jpg', 'data': {'categories': ['#long_context', '#inference', '#benchmark', '#agents', '#security', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¤Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'FocusAgent â€” ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ retriever Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸Ğ· Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ´ĞµÑÑÑ‚ĞºĞ¸ Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ prompt injection Ğ°Ñ‚Ğ°Ğº. FocusAgent Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ accessibility tree Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ñ†ĞµĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50% Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… WorkArena Ğ¸ WebArena, Ğ½Ğ¾ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Efficient and Secure Web Agents with FocusAgent', 'desc': 'FocusAgent is a novel approach that enhances the efficiency and security of web agents using a lightweight LLM retriever. It extracts the most relevant information from lengthy web page observations, which often contain excessive tokens that can overwhelm processing capabilities. By focusing on task-specific content and eliminating irrelevant data, FocusAgent minimizes computational costs and reduces the risk of security threats like prompt injection. Experimental results demonstrate that it not only maintains performance comparable to existing methods but also significantly decreases the amount of data processed, leading to safer and more effective web interactions.'}, 'zh': {'title': 'FocusAgentï¼šé«˜æ•ˆå®‰å…¨çš„ç½‘é¡µä»£ç†è§£å†³æ–¹æ¡ˆ', 'desc': 'FocusAgent æ˜¯ä¸€ç§è½»é‡çº§çš„ LLM æ£€ç´¢å™¨ï¼Œæ—¨åœ¨ä»ç½‘é¡µè§‚å¯Ÿä¸­æå–ç›¸å…³å†…å®¹ï¼Œä»è€Œæé«˜ç½‘ç»œä»£ç†çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿è¾¾æ•°ä¸‡æ ‡è®°çš„ç½‘é¡µæ—¶ï¼Œå®¹æ˜“å¯¼è‡´ä¸Šä¸‹æ–‡é™åˆ¶é¥±å’Œå’Œè®¡ç®—æˆæœ¬å¢åŠ ï¼ŒåŒæ—¶ä¹Ÿå¢åŠ äº†å®‰å…¨é£é™©ã€‚FocusAgent é€šè¿‡ä»å¯è®¿é—®æ€§æ ‘ï¼ˆAxTreeï¼‰è§‚å¯Ÿä¸­æå–æœ€ç›¸å…³çš„è¡Œï¼Œå‡å°‘äº†å™ªå£°å’Œæ— å…³å†…å®¹ï¼Œä½¿æ¨ç†è¿‡ç¨‹æ›´åŠ é«˜æ•ˆï¼Œå¹¶é™ä½äº†æ³¨å…¥æ”»å‡»çš„è„†å¼±æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFocusAgent åœ¨ä¿æŒä»»åŠ¡æˆåŠŸç‡çš„åŒæ—¶ï¼Œè§‚å¯Ÿå¤§å°å‡å°‘è¶…è¿‡ 50%ï¼Œå¹¶æ˜¾è‘—é™ä½äº†æç¤ºæ³¨å…¥æ”»å‡»çš„æˆåŠŸç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.03160', 'title': 'SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus', 'url': 'https://huggingface.co/papers/2510.03160', 'abstract': "SpineMed, an ecosystem with SpineMed-450k and SpineBench, addresses the lack of level-aware, multimodal datasets and benchmarks for AI-assisted diagnosis of spine disorders, improving model performance through fine-grained, level-specific reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Spine disorders affect 619 million people globally and are a leading cause of disability, yet AI-assisted diagnosis remains limited by the lack of level-aware, multimodal datasets. Clinical decision-making for spine disorders requires sophisticated reasoning across X-ray, CT, and MRI at specific vertebral levels. However, progress has been constrained by the absence of traceable, clinically-grounded instruction data and standardized, spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem co-designed with practicing spine surgeons. It features SpineMed-450k, the first large-scale dataset explicitly designed for vertebral-level reasoning across imaging modalities with over 450,000 instruction instances, and SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is curated from diverse sources, including textbooks, guidelines, open datasets, and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) to ensure high-quality, traceable data for question-answering, multi-turn consultations, and report generation. SpineBench evaluates models on clinically salient axes, including level identification, pathology assessment, and surgical planning. Our comprehensive evaluation of several recently advanced large vision-language models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained, level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k demonstrates consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of our model's outputs.", 'score': 1, 'issue_id': 6253, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}, 'hash': 'bd9504c9850d0415', 'authors': ['Ming Zhao', 'Wenhui Dong', 'Yang Zhang', 'Xiang Zheng', 'Zhonghao Zhang', 'Zian Zhou', 'Yunzhi Guan', 'Liukun Xu', 'Wei Peng', 'Zhaoyang Gong', 'Zhicheng Zhang', 'Dachuan Li', 'Xiaosheng Ma', 'Yuli Ma', 'Jianing Ni', 'Changjiang Jiang', 'Lixia Tian', 'Qixin Chen', 'Kaishun Xia', 'Pingping Liu', 'Tongshun Zhang', 'Zhiqiang Liu', 'Zhongan Bi', 'Chenyang Si', 'Tiansheng Sun', 'Caifeng Shan'], 'affiliations': ['Beijing Jiaotong University', 'Institute of Automation, Chinese Academy of Sciences', 'Jilin University', 'Nanjing University', 'Ningxia University', 'Stanford University', 'The General Hospital of the Peoples Liberation Army', 'Wuhan University', 'Zhejiang University', 'Ï€3 Lab'], 'pdf_title_img': 'assets/pdf/title_img/2510.03160.jpg', 'data': {'categories': ['#reasoning', '#training', '#healthcare', '#benchmark', '#dataset', '#multimodal', '#science'], 'emoji': 'ğŸ¦´', 'ru': {'title': 'SpineMed: AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SpineMed â€” ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ AI-Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SpineMed-450k Ñ 450 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº SpineBench. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½ĞºĞ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğµ, ĞšĞ¢ Ğ¸ ĞœĞ Ğ¢, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¿Ñ€Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºÑƒÑÑ‰Ğ¸Ñ… Ñ…Ğ¸Ñ€ÑƒÑ€Ğ³Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· LLM (Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸Ğº Ğ¸ Ñ€ĞµĞ²Ğ¸Ğ·Ğ¸Ñ). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ½ĞºĞ¾Ğ², Ğ½Ğ¾ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ½Ğ° SpineMed-450k Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Revolutionizing Spine Disorder Diagnosis with Level-Aware AI', 'desc': 'The paper introduces SpineMed, an innovative ecosystem designed to enhance AI-assisted diagnosis of spine disorders by providing level-aware, multimodal datasets and benchmarks. It features SpineMed-450k, a large-scale dataset with over 450,000 instances specifically curated for vertebral-level reasoning using a clinician-in-the-loop approach. The accompanying SpineBench framework allows for comprehensive evaluation of AI models on critical clinical tasks such as level identification and pathology assessment. Results show that models fine-tuned on SpineMed-450k significantly outperform existing large vision-language models in fine-grained reasoning, demonstrating improved diagnostic clarity and utility in clinical settings.'}, 'zh': {'title': 'è„ŠæŸ±ç–¾ç—…AIè¯Šæ–­çš„æ–°çªç ´', 'desc': 'SpineMedæ˜¯ä¸€ä¸ªé’ˆå¯¹è„ŠæŸ±ç–¾ç—…çš„äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ç”Ÿæ€ç³»ç»Ÿï¼ŒåŒ…å«SpineMed-450kæ•°æ®é›†å’ŒSpineBenchè¯„ä¼°æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿè§£å†³äº†ç¼ºä¹é’ˆå¯¹è„ŠæŸ±ç‰¹å®šå±‚æ¬¡çš„å¤šæ¨¡æ€æ•°æ®é›†çš„é—®é¢˜ï¼Œæä¾›äº†è¶…è¿‡45ä¸‡ä¸ªé«˜è´¨é‡çš„æŒ‡ä»¤å®ä¾‹ã€‚é€šè¿‡ä¸è„ŠæŸ±å¤–ç§‘åŒ»ç”Ÿåˆä½œï¼ŒSpineMedç¡®ä¿äº†æ•°æ®çš„ä¸´åºŠç›¸å…³æ€§å’Œå¯è¿½æº¯æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºSpineMed-450kå¾®è°ƒçš„æ¨¡å‹åœ¨å„é¡¹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.03120', 'title': 'SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?', 'url': 'https://huggingface.co/papers/2510.03120', 'abstract': "A new evaluation framework, SurveyBench, assesses the quality of automatically generated academic surveys using a quiz-driven approach, revealing deficiencies in current LLM4Survey methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Academic survey writing, which distills vast literature into a coherent and insightful narrative, remains a labor-intensive and intellectually demanding task. While recent approaches, such as general DeepResearch agents and survey-specialized methods, can generate surveys automatically (a.k.a. LLM4Survey), their outputs often fall short of human standards and there lacks a rigorous, reader-aligned benchmark for thoroughly revealing their deficiencies. To fill the gap, we propose a fine-grained, quiz-driven evaluation framework SurveyBench, featuring (1) typical survey topics source from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys; (2) a multifaceted metric hierarchy that assesses the outline quality (e.g., coverage breadth, logical coherence), content quality (e.g., synthesis granularity, clarity of insights), and non-textual richness; and (3) a dual-mode evaluation protocol that includes content-based and quiz-based answerability tests, explicitly aligned with readers' informational needs. Results show SurveyBench effectively challenges existing LLM4Survey approaches (e.g., on average 21% lower than human in content-based evaluation).", 'score': 1, 'issue_id': 6252, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}, 'hash': '9114023adb7490f9', 'authors': ['Zhaojun Sun', 'Xuzhou Zhu', 'Xuanhe Zhou', 'Xin Tong', 'Shuo Wang', 'Jie Fu', 'Guoliang Li', 'Zhiyuan Liu', 'Fan Wu'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.03120.jpg', 'data': {'categories': ['#survey', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'SurveyBench: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ AI-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ½Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ SurveyBench â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ quiz-driven Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°, ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ½ĞµÑ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 11,343 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ arXiv Ğ¸ 4,947 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM4Survey Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 21% Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'SurveyBench: Elevating AI-Generated Academic Surveys', 'desc': 'The paper introduces SurveyBench, a new evaluation framework designed to assess the quality of automatically generated academic surveys. It highlights the limitations of current LLM4Survey methods, which often fail to meet human standards in survey writing. SurveyBench utilizes a quiz-driven approach and a comprehensive metric hierarchy to evaluate both outline and content quality, ensuring alignment with reader needs. The results demonstrate that existing methods significantly underperform compared to human-generated surveys, with an average score 21% lower in content-based evaluations.'}, 'zh': {'title': 'SurveyBenchï¼šæå‡è‡ªåŠ¨ç”Ÿæˆå­¦æœ¯è°ƒæŸ¥çš„è¯„ä¼°æ ‡å‡†', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶SurveyBenchï¼Œç”¨äºè¯„ä¼°è‡ªåŠ¨ç”Ÿæˆçš„å­¦æœ¯è°ƒæŸ¥çš„è´¨é‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæµ‹éªŒçš„æ–¹æ³•ï¼Œæ­ç¤ºäº†å½“å‰LLM4Surveyæ–¹æ³•çš„ä¸è¶³ä¹‹å¤„ã€‚SurveyBenché€šè¿‡åˆ†ææ¥è‡ª11,343ç¯‡arXivè®ºæ–‡çš„å…¸å‹è°ƒæŸ¥ä¸»é¢˜å’Œ4,947ç¯‡é«˜è´¨é‡è°ƒæŸ¥ï¼Œå»ºç«‹äº†å¤šå±‚æ¬¡çš„è¯„ä¼°æŒ‡æ ‡ä½“ç³»ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒSurveyBenchåœ¨å†…å®¹è¯„ä¼°ä¸­å¹³å‡æ¯”äººç±»ä½21%ï¼Œæœ‰æ•ˆæŒ‘æˆ˜äº†ç°æœ‰çš„LLM4Surveyæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02571', 'title': 'How Confident are Video Models? Empowering Video Models to Express their\n  Uncertainty', 'url': 'https://huggingface.co/papers/2510.02571', 'abstract': 'A framework for uncertainty quantification in generative video models is introduced, including a metric for calibration, a black-box method called S-QUBED, and a benchmark dataset, demonstrating improved uncertainty estimates and task accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative video models demonstrate impressive text-to-video capabilities, spurring widespread adoption in many real-world applications. However, like large language models (LLMs), video generation models tend to hallucinate, producing plausible videos even when they are factually wrong. Although uncertainty quantification (UQ) of LLMs has been extensively studied in prior work, no UQ method for video models exists, raising critical safety concerns. To our knowledge, this paper represents the first work towards quantifying the uncertainty of video models. We present a framework for uncertainty quantification of generative video models, consisting of: (i) a metric for evaluating the calibration of video models based on robust rank correlation estimation with no stringent modeling assumptions; (ii) a black-box UQ method for video models (termed S-QUBED), which leverages latent modeling to rigorously decompose predictive uncertainty into its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate benchmarking calibration in video models. By conditioning the generation task in the latent space, we disentangle uncertainty arising due to vague task specifications from that arising from lack of knowledge. Through extensive experiments on benchmark video datasets, we demonstrate that S-QUBED computes calibrated total uncertainty estimates that are negatively correlated with the task accuracy and effectively computes the aleatoric and epistemic constituents.', 'score': 1, 'issue_id': 6252, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '6e5849ca43586c8a', 'authors': ['Zhiting Mei', 'Ola Shorinwa', 'Anirudha Majumdar'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2510.02571.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#hallucinations', '#dataset', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞšĞ¾Ğ³Ğ´Ğ° AI Ğ½Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½ Ğ² ÑĞ²Ğ¾Ñ‘Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ, ĞºĞ°Ğº Ğ¸ LLM, ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ S-QUBED, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ°Ğ»ĞµĞ°Ñ‚Ğ¾Ñ€Ğ½ÑƒÑ (Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµÑÑĞ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸) Ğ¸ ÑĞ¿Ğ¸ÑÑ‚ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ (Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸) ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ benchmark-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ°Ñ‘Ñ‚ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Quantifying Uncertainty in Generative Video Models for Safer AI', 'desc': "This paper introduces a new framework for measuring uncertainty in generative video models, which is crucial for ensuring their reliability in real-world applications. It presents a novel metric for assessing how well these models predict uncertainty, along with a black-box method called S-QUBED that separates different types of uncertainty. The framework also includes a benchmark dataset to evaluate the performance of video models in terms of their uncertainty calibration. Through experiments, the authors show that S-QUBED provides accurate uncertainty estimates that correlate with the models' task performance, addressing safety concerns in video generation."}, 'zh': {'title': 'ç”Ÿæˆè§†é¢‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºç”Ÿæˆè§†é¢‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€ä¸ªç”¨äºæ ¡å‡†çš„åº¦é‡æ ‡å‡†ã€ä¸€ç§ç§°ä¸ºS-QUBEDçš„é»‘ç®±æ–¹æ³•ï¼Œä»¥åŠä¸€ä¸ªåŸºå‡†æ•°æ®é›†ã€‚ç”Ÿæˆè§†é¢‘æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è§†é¢‘çš„èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ä¹Ÿå­˜åœ¨å¹»è§‰ç°è±¡ï¼Œå³ç”Ÿæˆçš„å†…å®¹å¯èƒ½åœ¨äº‹å®ä¸Šä¸€æ— æ˜¯å¤„ã€‚å°½ç®¡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–å·²æœ‰å¤§é‡ç ”ç©¶ï¼Œä½†ç›®å‰å°šæ— é’ˆå¯¹è§†é¢‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œè¿™å¼•å‘äº†å®‰å…¨éšæ‚£ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡é‡åŒ–äº†è§†é¢‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†S-QUBEDåœ¨æ ¡å‡†æ€»ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25771', 'title': 'Free Lunch Alignment of Text-to-Image Diffusion Models without\n  Preference Image Pairs', 'url': 'https://huggingface.co/papers/2509.25771', 'abstract': 'A new framework, Text Preference Optimization (TPO), aligns text-to-image models with human preferences without requiring paired image preference data, improving text-to-image alignment and human preference scores.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables "free-lunch" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.', 'score': 1, 'issue_id': 6253, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '6c93440c081695bd', 'authors': ['Jia Jun Cheng Xian', 'Muchen Li', 'Haotian Yang', 'Xin Tao', 'Pengfei Wan', 'Leonid Sigal', 'Renjie Liao'], 'affiliations': ['Canada CIFAR AI Chair', 'Kling Team, Kuaishou Technology', 'University of British Columbia', 'Vector Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.25771.jpg', 'data': {'categories': ['#open_source', '#alignment', '#benchmark', '#rlhf', '#multimodal', '#diffusion'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ‘ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Text Preference Optimization (TPO) Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸ÑĞºĞ°Ğ¶Ñ‘Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. TPO ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº DPO Ğ¸ KTO, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ Ğ¸Ñ… Ğ´Ğ¾ Ğ²ĞµÑ€ÑĞ¸Ğ¹ TDPO Ğ¸ TKTO. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Aligning Text and Images: A Free-Lunch Approach!', 'desc': 'The paper introduces a new framework called Text Preference Optimization (TPO) that enhances the alignment of text-to-image (T2I) models with human preferences without needing paired image preference data. This approach addresses the limitations of existing methods that rely on costly human annotations and reinforcement learning with human feedback (RLHF). TPO trains models to prefer correctly matched text-image pairs over mismatched ones, using perturbations generated by a large language model. The results demonstrate that TPO significantly improves human preference scores and T2I alignment compared to traditional methods, making it a scalable solution for better image generation.'}, 'zh': {'title': 'æ–‡æœ¬åå¥½ä¼˜åŒ–ï¼šæ— é¡»é…å¯¹æ•°æ®çš„å¯¹é½æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºæ–‡æœ¬åå¥½ä¼˜åŒ–ï¼ˆTPOï¼‰ï¼Œæ—¨åœ¨åœ¨ä¸éœ€è¦é…å¯¹å›¾åƒåå¥½æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä½¿æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸äººç±»åå¥½å¯¹é½ã€‚è¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒæ¨¡å‹æ›´å€¾å‘äºåŒ¹é…çš„æç¤ºï¼Œè€Œä¸æ˜¯é€šè¿‡æ‰°åŠ¨åŸå§‹æ ‡é¢˜ç”Ÿæˆçš„ä¸åŒ¹é…æç¤ºï¼Œä»è€Œå®ç°å¯¹é½ã€‚TPOä¸ç°æœ‰çš„åŸºäºåå¥½çš„ç®—æ³•å…¼å®¹ï¼Œå¹¶æ‰©å±•äº†DPOå’ŒKTOï¼Œå½¢æˆäº†TDPOå’ŒTKTOã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œæä¾›äº†æ›´å¥½çš„æ–‡æœ¬åˆ°å›¾åƒå¯¹é½å’Œäººç±»åå¥½è¯„åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.24002', 'title': 'MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\n  Use', 'url': 'https://huggingface.co/papers/2509.24002', 'abstract': 'MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of 127 high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only 52.56\\% pass@1 and 33.86\\% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below 30\\% pass@1 and 15\\% pass^4. On average, LLMs require 16.2 execution turns and 17.4 tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.', 'score': 118, 'issue_id': 6176, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 28', 'zh': '9æœˆ28æ—¥'}, 'hash': '9a5257700f81ad41', 'authors': ['Zijian Wu', 'Xiangyan Liu', 'Xinyuan Zhang', 'Lingjun Chen', 'Fanqing Meng', 'Lingxiao Du', 'Yiran Zhao', 'Fanshi Zhang', 'Yaoqi Ye', 'Jiawei Wang', 'Zirui Wang', 'Jinjie Ni', 'Yufan Yang', 'Arvin Xu', 'Michael Qizhe Shieh'], 'affiliations': ['EvalSys', 'Fudan University', 'LobeHub', 'National University of Singapore', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2509.24002.jpg', 'data': {'categories': ['#agi', '#agents', '#survey', '#benchmark'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'MCPMark: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚ÑŒ LLM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…', 'desc': 'MCPMark â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ MCP (Model Context Protocol) Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 127 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… CRUD-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½ĞµĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ¿Ğ»Ğ¾Ñ…Ğ¾: gpt-5-medium Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 52.56% ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ claude-sonnet-4 Ğ¸ o3 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 30%. Ğ’ ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ 16.2 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ 17.4 Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°.'}, 'en': {'title': 'MCPMark: Elevating LLMs to Real-World Challenges', 'desc': 'MCPMark is a new benchmark designed to evaluate the performance of Large Language Models (LLMs) in real-world workflows that require complex interactions with their environment. Unlike previous benchmarks that focused on simpler tasks, MCPMark includes 127 diverse tasks that involve a variety of create, read, update, and delete (CRUD) operations. The benchmark aims to standardize how LLMs interact with external systems, paving the way for the development of more capable general agents. Evaluation results show that even the best LLMs struggle with these tasks, indicating a significant gap in their ability to handle realistic scenarios.'}, 'zh': {'title': 'MCPMarkï¼šè¯„ä¼°çœŸå®å·¥ä½œæµç¨‹ä¸­çš„å¤§è¯­è¨€æ¨¡å‹', 'desc': 'MCPMarkæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®å·¥ä½œæµç¨‹ä¸­å¯¹å¤šç§ä»»åŠ¡çš„å¤„ç†èƒ½åŠ›ã€‚è¿™äº›ä»»åŠ¡è¦æ±‚ä¸ç¯å¢ƒè¿›è¡Œæ›´ä¸°å¯Œçš„äº¤äº’ï¼Œæ˜¾ç¤ºå‡ºå½“å‰çš„LLMåœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°è¾ƒå·®ã€‚MCPMarkåŒ…å«127ä¸ªé«˜è´¨é‡ä»»åŠ¡ï¼Œç”±é¢†åŸŸä¸“å®¶å’ŒAIä»£ç†å…±åŒåˆ›å»ºï¼Œæ—¨åœ¨æ›´çœŸå®å’Œå…¨é¢åœ°è¯„ä¼°MCPçš„ä½¿ç”¨ã€‚é€šè¿‡å¯¹å…ˆè¿›LLMçš„è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å³ä½¿æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œå…¶é€šè¿‡ç‡ä¹Ÿè¿œä½äºé¢„æœŸï¼Œçªæ˜¾äº†MCPMarkçš„æŒ‘æˆ˜æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26507', 'title': 'The Dragon Hatchling: The Missing Link between the Transformer and\n  Models of the Brain', 'url': 'https://huggingface.co/papers/2509.26507', 'abstract': "BDH, a biologically inspired Large Language Model, combines scale-free network architecture with Hebbian learning to achieve Transformer-like performance while maintaining interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.   We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \\n locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.   BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.   BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.   BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.", 'score': 102, 'issue_id': 6185, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'af365b084177f640', 'authors': ['Adrian Kosowski', 'PrzemysÅ‚aw UznaÅ„ski', 'Jan Chorowski', 'Zuzanna Stamirowska', 'MichaÅ‚ Bartoszkiewicz'], 'affiliations': ['Pathway, Palo Alto, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.26507.jpg', 'data': {'categories': ['#graphs', '#multimodal', '#architecture', '#reasoning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'BDH: Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¼Ğ¾Ñ‰ÑŒ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹', 'desc': 'BDH â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° LLM, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ğ°Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞµÑ‚ÑĞ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾-ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¥ĞµĞ±Ğ±Ñƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Transformer, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ. BDH Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚ĞµĞ¿ĞµĞ½ĞµĞ¹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞµÑ‚ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ğ°Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ°Ğ¹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹.'}, 'en': {'title': 'BDH: Bridging Biology and AI for Interpretability and Performance', 'desc': "BDH, or Dragon Hatchling, is a new Large Language Model that draws inspiration from biological networks, particularly the brain's scale-free architecture. It utilizes Hebbian learning, which mimics how neurons strengthen connections based on activity, to enhance its performance while remaining interpretable. This model achieves results comparable to Transformer models like GPT-2, using a similar number of parameters and training data. BDH's design allows for a clear understanding of its decision-making process, making it a significant step towards creating interpretable AI systems."}, 'zh': {'title': 'BDHï¼šç”Ÿç‰©å¯å‘çš„å¯è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹', 'desc': 'BDHæ˜¯ä¸€ç§å—ç”Ÿç‰©å¯å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†æ— æ ‡åº¦ç½‘ç»œæ¶æ„å’ŒHebbianå­¦ä¹ ï¼Œæ—¨åœ¨å®ç°ç±»ä¼¼Transformerçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå¯è§£é‡Šæ€§ã€‚è¯¥æ¨¡å‹é€šè¿‡å±€éƒ¨äº¤äº’çš„ç¥ç»ç²’å­æ„å»ºï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç†è®ºåŸºç¡€å’Œé«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚BDHåœ¨è¯­è¨€å’Œç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸GPT2ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”å‚æ•°æ•°é‡ç›¸åŒï¼Œè®­ç»ƒæ•°æ®ä¸€è‡´ã€‚BDHçš„å·¥ä½œè®°å¿†ä¾èµ–äºçªè§¦å¯å¡‘æ€§ï¼Œèƒ½å¤Ÿåœ¨å¤„ç†è¯­è¨€è¾“å…¥æ—¶åŠ å¼ºç‰¹å®šæ¦‚å¿µçš„è¿æ¥ï¼Œå±•ç°å‡ºç”Ÿç‰©å­¦ä¸Šçš„åˆç†æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25541', 'title': 'Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\n  Self-Play', 'url': 'https://huggingface.co/papers/2509.25541', 'abstract': 'Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in "Who Is the Spy"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model\'s reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.', 'score': 95, 'issue_id': 6175, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '1e4232d439e827c1', 'authors': ['Qinsi Wang', 'Bo Liu', 'Tianyi Zhou', 'Jing Shi', 'Yueqian Lin', 'Yiran Chen', 'Hai Helen Li', 'Kun Wan', 'Wentian Zhao'], 'affiliations': ['Adobe Inc.', 'Duke University', 'National University of Singapore', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2509.25541.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#optimization', '#games', '#cv', '#training', '#rl'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ VLM Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Vision-Zero â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ³Ñ€Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¸Ğ³Ñ€Ğ°Ñ Ğ² Ğ¸Ğ³Ñ€Ñ‹ Ñ‚Ğ¸Ğ¿Ğ° Â«ĞšÑ‚Ğ¾ ÑˆĞ¿Ğ¸Ğ¾Ğ½Â», Ğ³Ğ´Ğµ Ğ¾Ğ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸, Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Iterative Self-Play Policy Optimization Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾Ğ¸Ğ³Ñ€Ñƒ Ñ reinforcement learning, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¿Ğ»Ğ°Ñ‚Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¾ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… reasoning, Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸.'}, 'en': {'title': 'Empowering Vision-Language Models through Self-Play Games', 'desc': 'Vision-Zero is a new framework that improves vision-language models (VLMs) by allowing them to learn from playing competitive visual games without needing human-created datasets. It uses a method called Iterative Self-Play Policy Optimization, which helps models generate their own training data through gameplay, enhancing their reasoning skills. The framework can create games from any image, making it versatile across different domains and tasks. As a result, Vision-Zero achieves top performance in various reasoning tasks while avoiding the high costs of manual data annotation.'}, 'zh': {'title': 'Vision-Zeroï¼šæ— æ ‡æ³¨è‡ªæˆ‘æå‡çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶', 'desc': 'Vision-Zeroæ˜¯ä¸€ä¸ªé¢†åŸŸæ— å…³çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨ç«äº‰æ€§è§†è§‰æ¸¸æˆä¸­è‡ªæˆ‘æå‡ï¼Œå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è¿­ä»£è‡ªæˆ‘æ¸¸æˆç­–ç•¥ä¼˜åŒ–ï¼ˆIterative-SPOï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹ç”Ÿæˆè®­ç»ƒæ•°æ®ã€‚Vision-Zeroèƒ½å¤Ÿä»ä»»æ„å›¾åƒå¯¹ç”Ÿæˆæ¸¸æˆï¼Œæå‡æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶å±•ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€ç»ˆï¼ŒVision-Zeroåœ¨æ¨ç†ã€å›¾è¡¨é—®ç­”å’Œè§†è§‰ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–åŸºäºæ ‡æ³¨çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23873', 'title': 'Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token\n  Pruning for Efficient Supervised Fine-Tuning', 'url': 'https://huggingface.co/papers/2509.23873', 'abstract': 'Quadrant-based Tuning (Q-Tuning) optimizes both sample and token pruning in supervised fine-tuning of large language models, achieving superior performance with reduced data.  \t\t\t\t\tAI-generated summary \t\t\t\t As supervised fine-tuning (SFT) evolves from a lightweight post-training step into a compute-intensive phase rivaling mid-training in scale, data efficiency has become critical for aligning large language models (LLMs) under tight budgets. Existing data pruning methods suffer from a fragmented design: they operate either at the sample level or the token level in isolation, failing to jointly optimize both dimensions. This disconnect leads to significant inefficiencies--high-value samples may still contain redundant tokens, while token-level pruning often discards crucial instructional or corrective signals embedded in individual examples. To address this bottleneck, we introduce the Error-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes the heterogeneous utility of training data across samples and tokens. Guided by this insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework that strategically coordinates sample pruning and token pruning. Q-Tuning employs a two-stage strategy: first, it performs sample-level triage to retain examples rich in informative misconceptions or calibration signals; second, it applies an asymmetric token-pruning policy, using a context-aware scoring mechanism to trim less salient tokens exclusively from misconception samples while preserving calibration samples in their entirety. Our method sets a new state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B, Q-Tuning achieves a +38\\% average improvement over the full-data SFT baseline using only 12.5\\% of the original training data. As the first dynamic pruning approach to consistently outperform full-data training, Q-Tuning provides a practical and scalable blueprint for maximizing data utilization in budget-constrained LLM SFT.', 'score': 54, 'issue_id': 6184, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 28', 'zh': '9æœˆ28æ—¥'}, 'hash': 'e0fa1530a3055ba8', 'authors': ['Shaobo Wang', 'Jiaming Wang', 'Jiajun Zhang', 'Cong Wang', 'Yue Min', 'Zichen Wen', 'Fei Huang', 'Huiqiang Jiang', 'Junyang Lin', 'Dayiheng Liu', 'Linfeng Zhang'], 'affiliations': ['Alibaba Group', 'BJTU', 'EPIC Lab, SJTU', 'HKUST', 'NJU'], 'pdf_title_img': 'assets/pdf/title_img/2509.23873.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#data', '#training'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Q-Tuning â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ supervised fine-tuning Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Error-Uncertainty Plane â€” Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…. Q-Tuning Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ñ‚Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ñ€ĞµĞ·ĞºÑƒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°. ĞĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SmolLM2-1.7B Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 38% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 12.5% Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Maximizing Data Efficiency with Q-Tuning', 'desc': 'Quadrant-based Tuning (Q-Tuning) is a novel approach that enhances the efficiency of supervised fine-tuning for large language models by optimizing both sample and token pruning simultaneously. Traditional methods often focus on either samples or tokens separately, leading to inefficiencies where valuable data may be discarded. Q-Tuning introduces the Error-Uncertainty (EU) Plane, which helps identify the most useful training data by analyzing both samples and tokens together. This method has demonstrated significant improvements in performance, achieving a 38% increase in effectiveness while using only a fraction of the original training data.'}, 'zh': {'title': 'å››è±¡é™è°ƒä¼˜ï¼šé«˜æ•ˆåˆ©ç”¨æ•°æ®çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå››è±¡é™è°ƒä¼˜ï¼ˆQ-Tuningï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒè¿‡ç¨‹ä¸­çš„æ ·æœ¬å’Œæ ‡è®°å‰ªæã€‚é€šè¿‡å¼•å…¥è¯¯å·®-ä¸ç¡®å®šæ€§å¹³é¢ï¼ˆEU Planeï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶è¯„ä¼°è®­ç»ƒæ•°æ®åœ¨æ ·æœ¬å’Œæ ‡è®°å±‚é¢çš„æ•ˆç”¨ï¼Œä»è€Œå®ç°æ›´é«˜çš„æ•°æ®åˆ©ç”¨æ•ˆç‡ã€‚Q-Tuningé‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼Œé¦–å…ˆä¿ç•™å¯Œå«ä¿¡æ¯çš„æ ·æœ¬ï¼Œç„¶ååœ¨ç‰¹å®šæ ·æœ¬ä¸­è¿›è¡Œæ ‡è®°å‰ªæï¼Œç¡®ä¿é‡è¦ä¿¡æ¯ä¸è¢«ä¸¢å¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQ-Tuningåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è®¾ç«‹äº†æ–°çš„æ€§èƒ½æ ‡å‡†ï¼Œæ˜¾è‘—æé«˜äº†æ•°æ®åˆ©ç”¨ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25760', 'title': 'TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.25760', 'abstract': 'TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy -- models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.', 'score': 44, 'issue_id': 6175, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '58cf56a1a824c556', 'authors': ['Zhepei Wei', 'Xiao Yang', 'Kai Sun', 'Jiaqi Wang', 'Rulin Shao', 'Sean Chen', 'Mohammad Kachuee', 'Teja Gollapudi', 'Tony Liao', 'Nicolas Scheffer', 'Rakesh Wanga', 'Anuj Kumar', 'Yu Meng', 'Wen-tau Yih', 'Xin Luna Dong'], 'affiliations': ['FAIR at Meta', 'Meta Reality Labs', 'University of Virginia', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2509.25760.jpg', 'data': {'categories': ['#rlhf', '#hallucinations', '#reasoning', '#optimization', '#training', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ñƒ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ TruthRL â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµÑ€Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¿Ğ¾Ğ¾Ñ‰Ñ€ÑÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğµ, ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ ĞºĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸. TruthRL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 28.9% Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 21.1% Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'TruthRL: Balancing Accuracy and Abstention for Truthful AI', 'desc': "TruthRL is a novel reinforcement learning framework designed to enhance the truthfulness of large language models (LLMs) by effectively balancing accuracy and the ability to abstain from answering when uncertain. Traditional methods often lead to increased hallucinations or overly conservative responses, compromising the model's truthfulness. TruthRL addresses this by using a ternary reward system that differentiates between correct answers, hallucinations, and abstentions, encouraging models to provide accurate responses while also recognizing when to refrain from answering. Experimental results show that TruthRL significantly reduces hallucinations and improves overall truthfulness across various benchmarks and model architectures."}, 'zh': {'title': 'TruthRLï¼šæå‡è¯­è¨€æ¨¡å‹çœŸå®æ€§çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'TruthRLæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„çœŸå®æ€§ã€‚å®ƒé€šè¿‡å¹³è¡¡å‡†ç¡®æ€§å’Œæ”¾å¼ƒæ¥æ˜¾è‘—å‡å°‘å¹»è§‰ç°è±¡ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æå‡æ€§èƒ½ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ç®€å•æœ‰æ•ˆçš„ä¸‰å…ƒå¥–åŠ±æœºåˆ¶ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶é€‰æ‹©æ”¾å¼ƒï¼Œä»è€Œé¿å…é”™è¯¯å›ç­”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTruthRLç›¸æ¯”ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå‡å°‘äº†28.9%çš„å¹»è§‰ç°è±¡ï¼Œå¹¶æé«˜äº†21.1%çš„çœŸå®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26536', 'title': 'OceanGym: A Benchmark Environment for Underwater Embodied Agents', 'url': 'https://huggingface.co/papers/2509.26536', 'abstract': "OceanGym is a benchmark for underwater embodied agents using Multi-modal Large Language Models to address challenges in perception, planning, and adaptability in harsh ocean environments.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.", 'score': 29, 'issue_id': 6176, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '16228ef736074906', 'authors': ['Yida Xue', 'Mingjun Mao', 'Xiangyuan Ru', 'Yuqi Zhu', 'Baochang Ren', 'Shuofei Qiao', 'Mengru Wang', 'Shumin Deng', 'Xinyu An', 'Ningyu Zhang', 'Ying Chen', 'Huajun Chen'], 'affiliations': ['National University of Singapore', 'State Key Laboratory of Ocean Sensing, Zhejiang University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.26536.jpg', 'data': {'categories': ['#games', '#transfer_learning', '#agents', '#benchmark', '#multimodal'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'OceanGym: Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ĞºĞ° Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½ĞµĞ¸Ğ·Ğ²ĞµĞ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ°Ñ… Ğ¾ĞºĞµĞ°Ğ½Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ OceanGym â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ´Ğ½Ñ‹Ñ… embodied-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ² ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ĞºĞµĞ°Ğ½Ğ°. ĞŸĞ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞµĞ¼ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ ÑĞ¾Ğ½Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑ‡ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ MLLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ğ¾Ğ´Ğ²Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ.'}, 'en': {'title': 'OceanGym: Advancing AI for Underwater Exploration Challenges', 'desc': 'OceanGym is a new benchmark designed for testing underwater embodied agents using Multi-modal Large Language Models (MLLMs). It addresses the unique challenges of underwater environments, such as low visibility and dynamic currents, which complicate perception and decision-making. The benchmark includes eight realistic tasks that require agents to process both optical and sonar data while navigating complex scenarios. By highlighting the performance gaps between advanced AI agents and human experts, OceanGym aims to improve the adaptability and planning capabilities of AI in ocean exploration.'}, 'zh': {'title': 'OceanGymï¼šæ°´ä¸‹æ™ºèƒ½ä½“çš„æ–°åŸºå‡†æŒ‘æˆ˜', 'desc': 'OceanGymæ˜¯ä¸€ä¸ªé’ˆå¯¹æ°´ä¸‹å…·èº«æ™ºèƒ½ä½“çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è§£å†³åœ¨æ¶åŠ£æµ·æ´‹ç¯å¢ƒä¸­æ„ŸçŸ¥ã€è§„åˆ’å’Œé€‚åº”æ€§çš„é—®é¢˜ã€‚ä¸é™†åœ°æˆ–ç©ºä¸­ç¯å¢ƒä¸åŒï¼Œæ°´ä¸‹ç¯å¢ƒé¢ä¸´æç«¯çš„æ„ŸçŸ¥å’Œå†³ç­–æŒ‘æˆ˜ï¼Œå¦‚ä½èƒ½è§åº¦å’ŒåŠ¨æ€æµ·æµï¼Œä½¿å¾—æœ‰æ•ˆçš„æ™ºèƒ½ä½“éƒ¨ç½²å˜å¾—å¼‚å¸¸å›°éš¾ã€‚OceanGymåŒ…å«å…«ä¸ªç°å®ä»»åŠ¡é¢†åŸŸå’Œä¸€ä¸ªç»Ÿä¸€çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè¦æ±‚æ™ºèƒ½ä½“ç†è§£å…‰å­¦å’Œå£°çº³æ•°æ®ï¼Œèƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å¤æ‚ç¯å¢ƒï¼Œå¹¶åœ¨è¿™äº›æ¶åŠ£æ¡ä»¶ä¸‹å®Œæˆé•¿æœŸç›®æ ‡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œå‘ç°å½“å‰æœ€å…ˆè¿›çš„MLLMé©±åŠ¨æ™ºèƒ½ä½“ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œçªæ˜¾äº†åœ¨æ°´ä¸‹ç¯å¢ƒä¸­æ„ŸçŸ¥ã€è§„åˆ’å’Œé€‚åº”æ€§çš„æŒç»­å›°éš¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26625', 'title': 'Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training', 'url': 'https://huggingface.co/papers/2509.26625', 'abstract': "LLMs develop visual priors during language pre-training, which can be leveraged for vision tasks with minimal additional data, and these priors are composed of separable perception and reasoning components.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.", 'score': 28, 'issue_id': 6175, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '4ecd034d7f6a8060', 'authors': ['Junlin Han', 'Shengbang Tong', 'David Fan', 'Yufan Ren', 'Koustuv Sinha', 'Philip Torr', 'Filippos Kokkinos'], 'affiliations': ['Meta Superintelligence Labs', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2509.26625.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#alignment', '#dataset', '#transfer_learning'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°: ĞºĞ°Ğº LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ñ‹ ÑĞ¾ÑÑ‚Ğ¾ÑÑ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰ĞµĞ³Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡Ğ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ vision encoder. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 100 ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MLE-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Unlocking Visual Understanding in Language Models', 'desc': 'This paper explores how Large Language Models (LLMs) can develop visual understanding during their training on text data alone. It reveals that these models create visual priors, which are essential for performing vision tasks with minimal additional data. The study identifies two main components of these priors: perception and reasoning, each with distinct characteristics and scaling behaviors. By analyzing extensive experiments, the authors propose a method for enhancing LLMs with visual capabilities, setting a foundation for future multimodal AI systems.'}, 'zh': {'title': 'ä»è¯­è¨€é¢„è®­ç»ƒä¸­åŸ¹å…»è§†è§‰å…ˆéªŒçš„å…¨æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­è¨€é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æ„å¤–åœ°å‘å±•å‡ºä¸°å¯Œçš„è§†è§‰å…ˆéªŒã€‚è¿™äº›è§†è§‰å…ˆéªŒä½¿å¾—åœ¨è§†è§‰ä»»åŠ¡ä¸­èƒ½å¤Ÿä»¥ç›¸å¯¹è¾ƒå°‘çš„å¤šæ¨¡æ€æ•°æ®è§£é”æ½œåœ¨çš„è§†è§‰èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰å…ˆéªŒç”±å¯åˆ†ç¦»çš„æ„ŸçŸ¥å’Œæ¨ç†ç»„ä»¶ç»„æˆï¼Œä¸”è¿™ä¸¤è€…åœ¨è§„æ¨¡å’Œæ¥æºä¸Šå…·æœ‰ç‹¬ç‰¹çš„è¶‹åŠ¿ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨åŸ¹å…»è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œä»è€Œæ¨åŠ¨ä¸‹ä¸€ä»£å¤šæ¨¡æ€LLMsçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25848', 'title': 'More Thought, Less Accuracy? On the Dual Nature of Reasoning in\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2509.25848', 'abstract': "VAPO-Thinker-7B enhances multimodal reasoning by anchoring the process to visual information, improving performance on visual tasks while maintaining logical inference.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/", 'score': 28, 'issue_id': 6177, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '41251889e40e8e71', 'authors': ['Xinyu Tian', 'Shu Zou', 'Zhaoyuan Yang', 'Mengqi He', 'Fabian Waschkowski', 'Lukas Wesemann', 'Peter Tu', 'Jing Zhang'], 'affiliations': ['Australian National University', 'GE Research', 'University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2509.25848.jpg', 'data': {'categories': ['#rl', '#benchmark', '#cv', '#multimodal', '#reasoning'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'ĞĞµ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ¹ ÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ "Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ" Ğ² Vision-Language Models: Ğ¿Ñ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‚ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Vision-Anchored Policy Optimization (VAPO), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (Reinforcement Learning) Ğ¸ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ÑĞ²ÑĞ·ÑŒ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VAPO-Thinker-7B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Anchoring Reasoning to Visuals for Better Performance', 'desc': 'The paper introduces VAPO-Thinker-7B, a model that enhances multimodal reasoning by anchoring it to visual information. This approach improves performance on visual tasks while ensuring logical inference remains strong. The study reveals that while multimodal reasoning boosts problem-solving capabilities, it can lead to visual forgetting, where the model neglects visual input over time. To counteract this, the authors propose Vision-Anchored Policy Optimization (VAPO), which helps maintain a strong connection to visual data, resulting in state-of-the-art performance on various benchmarks.'}, 'zh': {'title': 'è§†è§‰é”šå®šï¼Œæ¨ç†æ›´ç²¾å‡†ï¼', 'desc': 'VAPO-Thinker-7Bé€šè¿‡å°†æ¨ç†è¿‡ç¨‹ä¸è§†è§‰ä¿¡æ¯ç›¸ç»“åˆï¼Œå¢å¼ºäº†å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œä»è€Œåœ¨è§†è§‰ä»»åŠ¡ä¸Šæé«˜äº†æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†é€»è¾‘æ¨ç†çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶å‘ç°ï¼Œå¤šæ¨¡æ€æ¨ç†å…·æœ‰åŒé‡ç‰¹æ€§ï¼Œè™½ç„¶å®ƒèƒ½æ˜¾è‘—æå‡é€»è¾‘æ¨ç†å’Œè§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´æ„ŸçŸ¥åŸºç¡€çš„é€æ¸å‰Šå¼±ï¼Œé€ æˆå¯¹åŸºæœ¬è§†è§‰é—®é¢˜çš„è¯†åˆ«å¤±è´¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†è§†è§‰é”šå®šç­–ç•¥ä¼˜åŒ–ï¼ˆVAPOï¼‰ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°å¼•å¯¼æ¨ç†è¿‡ç¨‹æœå‘è§†è§‰åŸºç¡€çš„è½¨è¿¹ã€‚æœ€ç»ˆï¼ŒVAPO-Thinker-7Båœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26226', 'title': 'Thinking-Free Policy Initialization Makes Distilled Reasoning Models\n  More Effective and Efficient Reasoners', 'url': 'https://huggingface.co/papers/2509.26226', 'abstract': 'TFPI, a simple adaptation to RLVR, improves performance and reduces token usage by discarding thinking content during training, accelerating RL convergence and achieving higher accuracy with less computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Reward (RLVR) effectively solves complex tasks but demands extremely long context lengths during training, leading to substantial computational costs. While multi-stage training can partially mitigate this, starting with overly short contexts often causes irreversible performance degradation, ultimately failing to reduce overall training compute significantly. In this paper, we introduce **T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. TFPI employs a simple *ThinkFree* operation, explicitly discarding the thinking content via a direct *</think>* append, to reduce token usage during inference. Training with *ThinkFree*-adapted inputs improves performance and lowers token consumption, even in the original slow-thinking mode. Extensive experiments across various benchmarks have shown that TFPI accelerates RL convergence, achieves a higher performance ceiling, and yields more token-efficient reasoning models without specialized rewards or complex training designs. With TFPI only, we train a 4B model to reach 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.', 'score': 25, 'issue_id': 6187, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '34e5a946d584992f', 'authors': ['Xin Xu', 'Cliveb AI', 'Kai Yang', 'Tianhao Chen', 'Yang Wang', 'Saiyong Yang', 'Can Yang'], 'affiliations': ['LLM Department, Tencent', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.26226.jpg', 'data': {'categories': ['#training', '#long_context', '#rl', '#rlhf', '#optimization'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'ĞÑ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ TFPI Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ (RLVR). ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ RLVR Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (Chain-of-Thought), Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². TFPI Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ ThinkFree, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Ñ‚ĞµĞ³ </think> Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ RL, Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 4B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° 89% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° AIME24, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµĞ½ĞµĞµ 4000 Ñ‡Ğ°ÑĞ¾Ğ² Ğ½Ğ° H20.'}, 'en': {'title': 'Streamlining RLVR: Boosting Performance with TFPI', 'desc': 'This paper presents Thinking-Free Policy Initialization (TFPI), an innovative approach that enhances Reinforcement Learning with Verifiable Reward (RLVR) by reducing token usage and improving training efficiency. TFPI simplifies the training process by discarding unnecessary thinking content, which accelerates the convergence of reinforcement learning models. The method allows for better performance with lower computational costs, even when using longer context lengths. Experimental results demonstrate that TFPI leads to significant improvements in accuracy and efficiency across various benchmarks, making it a valuable contribution to the field of machine learning.'}, 'zh': {'title': 'TFPIï¼šæå‡RLVRæ€§èƒ½çš„ç®€å•æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTFPIçš„ç®€å•é€‚åº”æ–¹æ³•ï¼Œæ—¨åœ¨æ”¹å–„å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„æ€§èƒ½ã€‚TFPIé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸¢å¼ƒæ€è€ƒå†…å®¹ï¼Œå‡å°‘äº†ä»¤ç‰Œçš„ä½¿ç”¨ï¼Œä»è€ŒåŠ é€Ÿäº†å¼ºåŒ–å­¦ä¹ çš„æ”¶æ•›ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨TFPIçš„è®­ç»ƒæ–¹æ³•å¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æœ€ç»ˆï¼ŒTFPIä½¿å¾—ä¸€ä¸ª4Bå‚æ•°çš„æ¨¡å‹åœ¨AIME24å’ŒLiveCodeBenchä¸Šåˆ†åˆ«è¾¾åˆ°äº†89.0%å’Œ65.5%çš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25182', 'title': 'DC-VideoGen: Efficient Video Generation with Deep Compression Video\n  Autoencoder', 'url': 'https://huggingface.co/papers/2509.25182', 'abstract': 'DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.', 'score': 25, 'issue_id': 6175, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '7de7ea8b15ae7048', 'authors': ['Junyu Chen', 'Wenkun He', 'Yuchao Gu', 'Yuyang Zhao', 'Jincheng Yu', 'Junsong Chen', 'Dongyun Zou', 'Yujun Lin', 'Zhekai Zhang', 'Muyang Li', 'Haocheng Xi', 'Ligeng Zhu', 'Enze Xie', 'Song Han', 'Han Cai'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2509.25182.jpg', 'data': {'categories': ['#inference', '#video', '#optimization', '#diffusion', '#architecture', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°', 'desc': 'DC-VideoGen â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ ÑĞ¶Ğ°Ñ‚Ğ¾Ğ¼Ñƒ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Deep Compression Video Autoencoder ÑĞ¾ ÑĞ¶Ğ°Ñ‚Ğ¸ĞµĞ¼ 32-64x Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ Ğ¸ 4x Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ AE-Adapt-V Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Wan-2.1-14B Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 10 GPU-Ğ´Ğ½ĞµĞ¹ Ğ½Ğ° NVIDIA H100, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ² 14.8 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 2160x3840 Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ GPU, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Accelerating Video Generation with Deep Compression', 'desc': 'DC-VideoGen is a framework designed to speed up video generation by modifying existing diffusion models to work in a compressed latent space. This approach allows for significant reductions in inference time while still producing high-quality, high-resolution videos. The framework utilizes a Deep Compression Video Autoencoder that efficiently compresses video data and an adaptation strategy called AE-Adapt-V for seamless integration of pre-trained models. As a result, DC-VideoGen can generate videos much faster, achieving up to 14.8 times lower latency compared to traditional methods.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'DC-VideoGen æ˜¯ä¸€ä¸ªåŠ é€Ÿè§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡å°†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°æ·±åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´æ¥å‡å°‘æ¨ç†å»¶è¿Ÿï¼Œä»è€Œå®ç°é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆã€‚è¯¥æ¡†æ¶å¯ä»¥åº”ç”¨äºä»»ä½•é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è½»é‡çº§å¾®è°ƒæé«˜æ•ˆç‡ã€‚å®ƒçš„ä¸¤ä¸ªå…³é”®åˆ›æ–°åŒ…æ‹¬ï¼šä¸€ç§å…·æœ‰æ–°é¢–å—å› æœæ—¶é—´è®¾è®¡çš„æ·±åº¦å‹ç¼©è§†é¢‘è‡ªç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé‡å»ºè´¨é‡çš„åŒæ—¶å®ç°32å€/64å€çš„ç©ºé—´å‹ç¼©å’Œ4å€çš„æ—¶é—´å‹ç¼©ï¼›ä»¥åŠAE-Adapt-Vï¼Œä¸€ç§ç¨³å¥çš„é€‚åº”ç­–ç•¥ï¼Œèƒ½å¤Ÿå¿«é€Ÿç¨³å®šåœ°å°†é¢„è®­ç»ƒæ¨¡å‹è½¬ç§»åˆ°æ–°çš„æ½œåœ¨ç©ºé—´ã€‚ä½¿ç”¨DC-VideoGenå¯¹é¢„è®­ç»ƒçš„Wan-2.1-14Bæ¨¡å‹è¿›è¡Œé€‚åº”åªéœ€10å¤©çš„GPUæ—¶é—´ï¼Œä¸”åŠ é€Ÿåçš„æ¨¡å‹åœ¨æ¨ç†å»¶è¿Ÿä¸Šæ¯”åŸºç¡€æ¨¡å‹ä½14.8å€ï¼Œä¸”èƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šç”Ÿæˆ2160x3840çš„è§†é¢‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25154', 'title': "Who's Your Judge? On the Detectability of LLM-Generated Judgments", 'url': 'https://huggingface.co/papers/2509.25154', 'abstract': "J-Detector, a neural detector with linguistic and LLM-enhanced features, effectively identifies LLM-generated judgments based on scores and candidate content, addressing biases and vulnerabilities in sensitive scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content -- an aspect crucial for effective judgment detection. Inspired by this, we introduce J-Detector, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of J-Detector and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios.", 'score': 24, 'issue_id': 6176, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '27f20852a9155cd5', 'authors': ['Dawei Li', 'Zhen Tan', 'Chengshuai Zhao', 'Bohan Jiang', 'Baixiang Huang', 'Pingchuan Ma', 'Abdullah Alnaibari', 'Kai Shu', 'Huan Liu'], 'affiliations': ['Arizona State University', 'Emory University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25154.jpg', 'data': {'categories': ['#hallucinations', '#data', '#ethics', '#dataset', '#interpretability', '#architecture', '#multimodal'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ”ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¾Ñ‚ LLM: Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ±Ğ°Ğ»Ğ»Ğ°Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ LLM-Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ»Ğ»Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ J-Detector â€” Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ñ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ LLM-ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ LLM-ÑÑƒĞ´ĞµĞ¹ ÑĞ¾ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ ĞµĞ³Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² LLM-ÑÑƒĞ´ÑŒÑÑ… Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'J-Detector: Unmasking Biases in LLM Judgments', 'desc': 'The paper introduces J-Detector, a neural network designed to identify judgments generated by Large Language Models (LLMs) based on their scores and the content of candidates. It highlights the challenges posed by biases and vulnerabilities in LLM-generated judgments, particularly in sensitive contexts like academic peer review. The authors emphasize that traditional LLM text detection methods are inadequate for this task, as they do not consider the relationship between judgment scores and candidate content. J-Detector addresses this gap by incorporating linguistic features and LLM-enhanced attributes, allowing for better detection and analysis of biases in LLM-generated judgments.'}, 'zh': {'title': 'J-Detectorï¼šç²¾å‡†è¯†åˆ«LLMç”Ÿæˆåˆ¤æ–­çš„åˆ©å™¨', 'desc': 'J-Detectoræ˜¯ä¸€ç§ç¥ç»æ£€æµ‹å™¨ï¼Œç»“åˆäº†è¯­è¨€å­¦å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºç‰¹æ€§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«åŸºäºLLMç”Ÿæˆçš„åˆ¤æ–­ã€‚è¯¥ç ”ç©¶æå‡ºäº†åˆ¤æ–­æ£€æµ‹çš„ä»»åŠ¡ï¼Œé‡ç‚¹å…³æ³¨åœ¨ç¼ºä¹æ–‡æœ¬åé¦ˆçš„æƒ…å†µä¸‹ï¼Œä»…ä¾èµ–åˆ¤æ–­åˆ†æ•°å’Œå€™é€‰å†…å®¹è¿›è¡Œæ£€æµ‹ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œå‘ç°ç°æœ‰çš„LLMç”Ÿæˆæ–‡æœ¬æ£€æµ‹æ–¹æ³•åœ¨æ•æ‰åˆ¤æ–­åˆ†æ•°ä¸å€™é€‰å†…å®¹ä¹‹é—´çš„äº’åŠ¨æ–¹é¢è¡¨ç°ä¸ä½³ã€‚J-Detectoré€šè¿‡æå–è¯­è¨€å­¦ç‰¹å¾å’ŒLLMå¢å¼ºç‰¹å¾ï¼ŒæˆåŠŸåœ°å°†LLMè¯„å®¡è€…çš„åè§ä¸å€™é€‰è€…çš„å±æ€§è”ç³»èµ·æ¥ï¼Œä»è€Œå®ç°å‡†ç¡®æ£€æµ‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25758', 'title': 'Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\n  Post Training', 'url': 'https://huggingface.co/papers/2509.25758', 'abstract': 'Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable capabilities of modern large reasoning models are largely unlocked through post-training techniques such as supervised fine-tuning and reinforcement learning. However, the architectural mechanisms behind such improvements remain largely opaque. In this work, we use circuit analysis to demonstrate that post-training for complex reasoning sparks the emergence of novel, functionally specialized attention heads. These heads collectively support structured reasoning and computation. Our comparative analysis across Qwen families and DeepSeek-distilled model reveals that these emergent heads evolve differently under different training regimes. Distillation and SFT foster a cumulative addition of stable reasoning heads. In contrast, group relative policy optimization operates in a dynamic search mode: relatively few attention heads are iteratively activated, evaluated, and pruned, with their survival closely tracking fluctuations in the task reward signal. Furthermore, we find that controllable think on/off models do not possess dedicated thinking heads. Instead, turning off explicit reasoning triggers a broader-but less efficient-set of compensatory heads. Through ablation and qualitative analyses, we connect these circuit-level dynamics to a crucial performance trade-off: strengthened heads enable sophisticated problem-solving strategies for difficult problems but can also introduce over-thinking failure modes, such as calculation errors or logical loops on simpler tasks. These findings connect circuit-level dynamics to macro-level performance, identifying an inherent tension where complex reasoning comes at the cost of elementary computations. More broadly, our work points to future directions for training policy design, emphasizing the need to balance the development of effective reasoning strategies with the assurance of reliable, flawless execution.', 'score': 18, 'issue_id': 6175, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'f7f61c1e3b1bdf7d', 'authors': ['Yein Park', 'Minbyul Jeong', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'Korea University', 'Upstage AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.25758.jpg', 'data': {'categories': ['#interpretability', '#optimization', '#reasoning', '#architecture', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ: ĞºĞ»ÑÑ‡ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ post-training Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº supervised fine-tuning Ğ¸ reinforcement learning, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… attention heads, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ: distillation Ğ¸ SFT ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ reasoning heads, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº group relative policy optimization Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ³Ğ¾Ğ»Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ÑĞ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ¸Ğ¼ĞµÑÑ‚ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… thinking heads, Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹, Ğ½Ğ¾ Ğ¼ĞµĞ½ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ°Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ: ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ½Ğ¾ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğº Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ·-Ğ·Ğ° Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Reasoning: The Power of Specialized Attention Heads', 'desc': 'This paper explores how post-training techniques like supervised fine-tuning and reinforcement learning enhance the performance of large reasoning models. It reveals that these techniques lead to the emergence of specialized attention heads that facilitate structured reasoning. The study shows that different training methods influence the evolution and effectiveness of these attention heads, with some fostering stable reasoning capabilities while others operate in a dynamic, adaptive manner. Ultimately, the research highlights a trade-off between advanced reasoning abilities and the risk of errors in simpler tasks, suggesting a need for careful design in training policies.'}, 'zh': {'title': 'åè®­ç»ƒæŠ€æœ¯åŠ©åŠ›ç»“æ„åŒ–æ¨ç†çš„æ¼”å˜', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åè®­ç»ƒæŠ€æœ¯å¦‚ä½•ä¿ƒè¿›ä¸“é—¨åŒ–æ³¨æ„åŠ›å¤´çš„å‡ºç°ï¼Œè¿™äº›æ³¨æ„åŠ›å¤´æ”¯æŒç»“æ„åŒ–æ¨ç†ã€‚é€šè¿‡ç”µè·¯åˆ†æï¼Œæˆ‘ä»¬å‘ç°ä¸åŒçš„è®­ç»ƒæ–¹å¼ä¼šå½±å“è¿™äº›æ³¨æ„åŠ›å¤´çš„æ¼”å˜å’Œæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯ï¼Œè’¸é¦å’Œç›‘ç£å¾®è°ƒä¿ƒè¿›äº†ç¨³å®šæ¨ç†å¤´çš„ç´¯ç§¯ï¼Œè€Œç›¸å¯¹ç­–ç•¥ä¼˜åŒ–åˆ™åœ¨åŠ¨æ€æœç´¢æ¨¡å¼ä¸‹å·¥ä½œã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å¤æ‚æ¨ç†ä¸åŸºæœ¬è®¡ç®—ä¹‹é—´çš„å†…åœ¨å¼ åŠ›ï¼Œå¼ºè°ƒäº†åœ¨è®­ç»ƒç­–ç•¥è®¾è®¡ä¸­å¹³è¡¡æœ‰æ•ˆæ¨ç†ä¸å¯é æ‰§è¡Œçš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26488', 'title': 'dParallel: Learnable Parallel Decoding for dLLMs', 'url': 'https://huggingface.co/papers/2509.26488', 'abstract': 'dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel', 'score': 16, 'issue_id': 6175, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '50b8e2e379343971', 'authors': ['Zigeng Chen', 'Gongfan Fang', 'Xinyin Ma', 'Ruonan Yu', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.26488.jpg', 'data': {'categories': ['#inference', '#optimization', '#benchmark', '#diffusion', '#open_source', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… LLM Ñ 10-ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ dParallel â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (dLLMs). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ ÑƒĞ·ĞºĞ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ¾ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° certainty-forcing distillation Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ 256 Ğ´Ğ¾ 24-30 Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… GSM8K Ğ¸ MBPP, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 8-10 Ñ€Ğ°Ğ· Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Unlocking Fast Parallel Decoding in Diffusion Models', 'desc': 'dParallel is a novel method designed to improve the efficiency of parallel decoding in diffusion large language models (dLLMs). It addresses the challenge of sequential certainty convergence for masked tokens, which has limited the speed of parallel decoding. By introducing certainty-forcing distillation, dParallel trains the model to quickly achieve high certainty on masked tokens while maintaining its original sampling paths. Experimental results show that dParallel significantly reduces decoding steps, achieving up to 10.5 times faster inference without sacrificing performance.'}, 'zh': {'title': 'dParallelï¼šåŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„å¹¶è¡Œè§£ç ', 'desc': 'dParallelæ˜¯ä¸€ç§å¢å¼ºæ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰å¹¶è¡Œè§£ç çš„æ–¹æ³•ï¼Œæ˜¾è‘—å‡å°‘äº†è§£ç æ­¥éª¤è€Œä¸å½±å“æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†dLLMsçš„å¹¶è¡Œæ€§ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹åœ¨è§£ç æ—¶éœ€è¦æ¥è¿‘ä»¤ç‰Œé•¿åº¦çš„æ­¥éª¤çš„é—®é¢˜ã€‚é€šè¿‡å¼•å…¥ç¡®å®šæ€§å¼ºåˆ¶è’¸é¦çš„è®­ç»ƒç­–ç•¥ï¼ŒdParallelèƒ½å¤Ÿæ›´å¿«åœ°å¹¶è¡Œå¤„ç†è¢«é®è”½çš„ä»¤ç‰Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒdParallelåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—å‡å°‘äº†è§£ç æ­¥éª¤ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26490', 'title': 'VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications', 'url': 'https://huggingface.co/papers/2509.26490', 'abstract': 'VitaBench is a benchmark for evaluating LLM-based agents in complex, real-world interactive tasks using a diverse set of tools and scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/', 'score': 15, 'issue_id': 6175, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '3ee3a7d4e39bff94', 'authors': ['Wei He', 'Yueqing Sun', 'Hongyan Hao', 'Xueyuan Hao', 'Zhikang Xia', 'Qi Gu', 'Chengcheng Han', 'Dengchang Zhao', 'Hui Su', 'Kefeng Zhang', 'Man Gao', 'Xi Su', 'Xiaodong Cai', 'Xunliang Cai', 'Yu Yang', 'Yunke Zhao'], 'affiliations': ['Meituan LongCat Team'], 'pdf_title_img': 'assets/pdf/title_img/2509.26490.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#games', '#survey'], 'emoji': 'ğŸ§­', 'ru': {'title': 'VitaBench: Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½ Ğ´Ğ»Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…', 'desc': 'VitaBench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 66 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ 400 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· ÑÑ„ĞµÑ€ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸ ĞµĞ´Ñ‹, Ñ€ĞµÑÑ‚Ğ¾Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ñ‚ÑƒÑ€Ğ¸Ğ·Ğ¼Ğ°, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ÑƒĞ±Ñ€Ğ¸Ğº ÑĞ¾ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼ Ğ¾ĞºĞ½Ğ¾Ğ¼, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ”Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 30% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ² ĞºÑ€Ğ¾ÑÑ-ÑÑ†ĞµĞ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'VitaBench: Advancing AI Agents in Real-World Complexity', 'desc': 'VitaBench is a new benchmark designed to test large language model (LLM)-based agents in complex, real-world tasks that require interaction with various tools. It addresses the limitations of existing benchmarks by providing a diverse set of scenarios that reflect daily applications, such as food delivery and travel services. The benchmark includes 66 tools and offers 100 cross-scenario tasks, challenging agents to manage dynamic user interactions and reason through complex instructions. The evaluation shows that even advanced models struggle, achieving only a 30% success rate on cross-scenario tasks, highlighting the need for further development in AI agents for practical use.'}, 'zh': {'title': 'VitaBenchï¼šè¯„ä¼°å¤æ‚äº’åŠ¨ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•', 'desc': 'VitaBenchæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤æ‚ç°å®äº’åŠ¨ä»»åŠ¡ä¸­çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒè§£å†³äº†ç°æœ‰åŸºå‡†æ— æ³•æ•æ‰ä»£ç†å¤„ç†å¤§é‡ä¿¡æ¯ã€åˆ©ç”¨å¤šæ ·èµ„æºå’Œç®¡ç†åŠ¨æ€ç”¨æˆ·äº¤äº’çš„å¤æ‚æ€§çš„é—®é¢˜ã€‚VitaBenchæä¾›äº†66ç§å·¥å…·å’Œå¤šç§åœºæ™¯ï¼Œè®¾è®¡äº†100ä¸ªè·¨åœºæ™¯ä»»åŠ¡å’Œ300ä¸ªå•åœºæ™¯ä»»åŠ¡ï¼Œè¦æ±‚ä»£ç†åœ¨å¤šè½®å¯¹è¯ä¸­æ¨ç†æ—¶é—´å’Œç©ºé—´ç»´åº¦ï¼Œä½¿ç”¨å¤æ‚å·¥å…·é›†ï¼Œå¹¶ä¸»åŠ¨æ¾„æ¸…æ¨¡ç³ŠæŒ‡ä»¤ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è·¨åœºæ™¯ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ä¹Ÿä»…ä¸º30%ï¼Œè¿™è¡¨æ˜VitaBenchå°†æ¨åŠ¨AIä»£ç†åœ¨å®é™…åº”ç”¨ä¸­çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.22646', 'title': 'Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal\n  LLMs', 'url': 'https://huggingface.co/papers/2509.22646', 'abstract': 'DeeptraceReward is a benchmark dataset that annotates human-perceived deepfake traces in videos, used to train multimodal language models for detecting AI-generated videos.  \t\t\t\t\tAI-generated summary \t\t\t\t Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.', 'score': 14, 'issue_id': 6178, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': '098fcc1c49c189c8', 'authors': ['Xingyu Fu', 'Siyi Liu', 'Yinuo Xu', 'Pan Lu', 'Guangqiuse Hu', 'Tianbo Yang', 'Taran Anantasagar', 'Christopher Shen', 'Yikai Mao', 'Yuanzhe Liu', 'Keyush Shah', 'Chung Un Lee', 'Yejin Choi', 'James Zou', 'Dan Roth', 'Chris Callison-Burch'], 'affiliations': ['Princeton University', 'Stanford University', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2509.22646.jpg', 'data': {'categories': ['#alignment', '#ethics', '#video', '#multimodal', '#benchmark', '#dataset', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞ»ĞµĞ´Ñ‹ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ² Ğ³Ğ»Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DeeptraceReward Ñ 4.3 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ»ÑĞ´Ğ¸. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· bounding box Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸ ĞºĞ¾Ğ½Ñ†Ğ° Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ°. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ multimodal language models ĞºĞ°Ğº reward Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° 7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° GPT-5 Ğ½Ğ° 34.7% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Detecting Deepfakes: Training Models with Human Insights', 'desc': 'DeeptraceReward is a new dataset designed to help train models that can detect deepfake videos by focusing on human-perceived traces. It includes 4.3K annotations from 3.3K high-quality generated videos, detailing where and when viewers notice signs of manipulation. The dataset categorizes these traces into nine major types, allowing models to learn how to identify and explain deepfake characteristics. By using this dataset, researchers can improve multimodal language models to better mimic human detection and reasoning about AI-generated content.'}, 'zh': {'title': 'æ­ç¤ºæ·±åº¦ä¼ªé€ ç—•è¿¹ï¼Œæå‡è§†é¢‘ç”Ÿæˆå¯ä¿¡åº¦', 'desc': 'DeeptraceRewardæ˜¯ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œä¸“æ³¨äºæ ‡æ³¨äººç±»æ„ŸçŸ¥çš„æ·±åº¦ä¼ªé€ è§†é¢‘ç—•è¿¹ï¼Œæ—¨åœ¨è®­ç»ƒå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä»¥æ£€æµ‹AIç”Ÿæˆçš„è§†é¢‘ã€‚è¯¥æ•°æ®é›†åŒ…å«4300ä¸ªè¯¦ç»†æ³¨é‡Šï¼Œæ¶µç›–3300ä¸ªé«˜è´¨é‡ç”Ÿæˆçš„è§†é¢‘ï¼Œæ¯ä¸ªæ³¨é‡Šæä¾›è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œå¹¶æ ‡è®°å‡ºåŒ…å«ä¼ªé€ ç—•è¿¹çš„åŒºåŸŸå’Œæ—¶é—´æˆ³ã€‚æˆ‘ä»¬å°†è¿™äº›æ³¨é‡Šæ•´åˆä¸º9ä¸ªä¸»è¦ç±»åˆ«ï¼Œå¸®åŠ©äººç±»è¯†åˆ«è§†é¢‘æ˜¯å¦ä¸ºAIç”Ÿæˆã€‚é€šè¿‡DeeptraceRewardï¼Œæˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹åœ¨ä¼ªé€ çº¿ç´¢è¯†åˆ«å’Œè§£é‡Šæ–¹é¢çš„è¡¨ç°è¶…è¶Šäº†GPT-5ï¼Œæ¨åŠ¨äº†ç¤¾ä¼šæ„è¯†å’Œå¯ä¿¡èµ–çš„è§†é¢‘ç”Ÿæˆç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26231', 'title': 'IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance', 'url': 'https://huggingface.co/papers/2509.26231', 'abstract': 'Implicit Multimodal Guidance (IMG) enhances multimodal alignment between diffusion-generated images and prompts without additional data or editing, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Ensuring precise multimodal alignment between diffusion-generated images and input prompts has been a long-standing challenge. Earlier works finetune diffusion weight using high-quality preference data, which tends to be limited and difficult to scale up. Recent editing-based methods further refine local regions of generated images but may compromise overall image quality. In this work, we propose Implicit Multimodal Guidance (IMG), a novel re-generation-based multimodal alignment framework that requires no extra data or editing operations. Specifically, given a generated image and its prompt, IMG a) utilizes a multimodal large language model (MLLM) to identify misalignments; b) introduces an Implicit Aligner that manipulates diffusion conditioning features to reduce misalignments and enable re-generation; and c) formulates the re-alignment goal into a trainable objective, namely Iteratively Updated Preference Objective. Extensive qualitative and quantitative evaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing alignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter, seamlessly enhancing prior finetuning-based alignment methods. Our code will be available at https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment.', 'score': 13, 'issue_id': 6176, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '2e5347092392a0bc', 'authors': ['Jiayi Guo', 'Chuanhao Yan', 'Xingqian Xu', 'Yulin Wang', 'Kai Wang', 'Gao Huang', 'Humphrey Shi'], 'affiliations': ['SHI Labs @ Georgia Tech', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.26231.jpg', 'data': {'categories': ['#multimodal', '#alignment', '#cv', '#diffusion'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞµÑĞ²Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Implicit Multimodal Guidance (IMG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ multimodal LLM Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ conditioning-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ñ… ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², IMG Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… SDXL Ğ¸ FLUX, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ IMG Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ plug-and-play Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Image-Prompt Alignment with Implicit Multimodal Guidance', 'desc': 'Implicit Multimodal Guidance (IMG) is a new framework designed to improve the alignment between images generated by diffusion models and their corresponding prompts. Unlike previous methods that rely on additional data or editing, IMG uses a multimodal large language model to identify and correct misalignments directly. It introduces an Implicit Aligner that adjusts the features used in the diffusion process to enhance image quality during re-generation. The framework not only surpasses existing alignment techniques but also integrates easily with previous methods, making it a versatile tool for multimodal tasks.'}, 'zh': {'title': 'éšå¼å¤šæ¨¡æ€å¼•å¯¼ï¼šæ— æ•°æ®å¯¹é½çš„åˆ›æ–°', 'desc': 'éšå¼å¤šæ¨¡æ€å¼•å¯¼ï¼ˆIMGï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€å¯¹é½æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ‰©æ•£ç”Ÿæˆå›¾åƒä¸è¾“å…¥æç¤ºä¹‹é—´çš„å¯¹é½ç²¾åº¦ï¼Œè€Œæ— éœ€é¢å¤–çš„æ•°æ®æˆ–ç¼–è¾‘æ“ä½œã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¯†åˆ«ç”Ÿæˆå›¾åƒä¸æç¤ºä¹‹é—´çš„é”™ä½ï¼Œå¹¶é€šè¿‡éšå¼å¯¹é½å™¨è°ƒæ•´æ‰©æ•£æ¡ä»¶ç‰¹å¾ä»¥å‡å°‘é”™ä½ã€‚IMGå°†é‡æ–°å¯¹é½ç›®æ ‡å…¬å¼åŒ–ä¸ºå¯è®­ç»ƒçš„ç›®æ ‡ï¼Œç§°ä¸ºè¿­ä»£æ›´æ–°åå¥½ç›®æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIMGåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„å¯¹é½æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥ä½œä¸ºçµæ´»çš„æ’ä»¶ï¼Œå¢å¼ºä¹‹å‰åŸºäºå¾®è°ƒçš„å¯¹é½æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26603', 'title': 'DeepScientist: Advancing Frontier-Pushing Scientific Findings\n  Progressively', 'url': 'https://huggingface.co/papers/2509.26603', 'abstract': 'DeepScientist autonomously conducts scientific discovery through Bayesian Optimization, surpassing human state-of-the-art methods on multiple AI tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of "hypothesize, verify, and analyze". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.', 'score': 12, 'issue_id': 6181, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '625bbc70427e5759', 'authors': ['Yixuan Weng', 'Minjun Zhu', 'Qiujie Xie', 'Qiyao Sun', 'Zhen Lin', 'Sifan Liu', 'Yue Zhang'], 'affiliations': ['Engineering School, Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2509.26603.jpg', 'data': {'categories': ['#science', '#agents', '#open_source', '#rl'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'AI-ÑƒÑ‡Ñ‘Ğ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'DeepScientist - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾ Ñ†Ğ¸ĞºĞ»Ñƒ Â«Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ°-Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ°-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Â», Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¾Ğº Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ´ĞµĞ¹ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ—Ğ° Ğ¼ĞµÑÑÑ†Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° Ğ¾ĞºĞ¾Ğ»Ğ¾ 5000 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸Ğ´ĞµĞ¹, ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ğ»Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 1100 Ğ¸Ğ· Ğ½Ğ¸Ñ…, Ğ¿Ğ¾Ñ‚Ñ€Ğ°Ñ‚Ğ¸Ğ² Ğ±Ğ¾Ğ»ĞµĞµ 20000 GPU-Ñ‡Ğ°ÑĞ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ DeepScientist Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆÑ‘Ğ» ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»ÑĞ´ÑŒĞ¼Ğ¸, Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… AI-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ° 183.7%, 1.9% Ğ¸ 7.9%, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾, Ñ‡Ñ‚Ğ¾ AI Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ½Ğ°ÑƒĞºĞ¸.'}, 'en': {'title': 'DeepScientist: AI Surpassing Human Discovery in Science', 'desc': 'DeepScientist is an advanced AI system that autonomously conducts scientific discovery using Bayesian Optimization. It addresses the limitations of previous AI systems by focusing on generating scientifically valuable contributions to real-world challenges. The system operates through a structured process of hypothesizing, verifying, and analyzing findings, while maintaining a cumulative memory of discoveries. By leveraging extensive computational resources, DeepScientist has generated thousands of unique ideas and validated many of them, outperforming human-designed methods in several AI tasks.'}, 'zh': {'title': 'DeepScientistï¼šè¶…è¶Šäººç±»çš„ç§‘å­¦å‘ç°æ–°çºªå…ƒ', 'desc': 'DeepScientist æ˜¯ä¸€ä¸ªé€šè¿‡è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œç§‘å­¦å‘ç°çš„ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªä¸»è¿›è¡Œç§‘å­¦ç ”ç©¶ï¼Œè¶…è¶Šäººç±»åœ¨å¤šä¸ªäººå·¥æ™ºèƒ½ä»»åŠ¡ä¸Šçš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å®ƒå°†ç§‘å­¦å‘ç°å½¢å¼åŒ–ä¸ºä¸€ä¸ªè´å¶æ–¯ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶é€šè¿‡â€œå‡è®¾ã€éªŒè¯å’Œåˆ†æâ€çš„åˆ†å±‚è¯„ä¼°è¿‡ç¨‹æ¥å®ç°ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨ç´¯ç§¯çš„å‘ç°è®°å¿†ï¼Œæ™ºèƒ½åœ°å¹³è¡¡æ–°å‡è®¾çš„æ¢ç´¢ä¸å·²æœ‰å‘ç°çš„åˆ©ç”¨ï¼Œé€‰æ‹©æ€§åœ°æå‡æœ€æœ‰å‰æ™¯çš„å‘ç°è¿›è¡Œæ›´é«˜ç²¾åº¦çš„éªŒè¯ã€‚æœ€ç»ˆï¼ŒDeepScientist ç”Ÿæˆäº†çº¦5000ä¸ªç‹¬ç‰¹çš„ç§‘å­¦æƒ³æ³•ï¼Œå¹¶æˆåŠŸéªŒè¯äº†çº¦1100ä¸ªï¼Œæ˜¾è‘—è¶…è¶Šäº†äººç±»è®¾è®¡çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26391', 'title': 'MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2509.26391', 'abstract': 'MotionRAG enhances video generation by integrating motion priors from reference videos using a retrieval-augmented framework, improving motion realism with negligible computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.', 'score': 12, 'issue_id': 6176, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '80a4fde692ceb5ab', 'authors': ['Chenhui Zhu', 'Yilu Wu', 'Shuai Wang', 'Gangshan Wu', 'Limin Wang'], 'affiliations': ['Shanghai AI Laboratory', 'State Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2509.26391.jpg', 'data': {'categories': ['#transfer_learning', '#video', '#rag', '#diffusion', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· retrieval motion-Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MotionRAG â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ retrieval-augmented Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ motion-Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¸Ğ· Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸ resamplers, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· causal transformer Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ. Motion-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· attention-based Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ zero-shot Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Video Realism with MotionRAG: Smart Retrieval for Realistic Motion Dynamics', 'desc': 'MotionRAG is a novel framework that improves video generation by incorporating motion priors from reference videos. It utilizes a retrieval-augmented approach to enhance the realism of motion in generated videos while maintaining low computational costs. The framework employs Context-Aware Motion Adaptation (CAMA) to adapt high-level motion features extracted from relevant videos, using a causal transformer for in-context learning. This method allows for significant improvements in motion realism across various domains and enables zero-shot generalization by simply updating the retrieval database.'}, 'zh': {'title': 'MotionRAGï¼šæå‡è§†é¢‘ç”Ÿæˆçš„è¿åŠ¨çœŸå®æ„Ÿ', 'desc': 'MotionRAGæ˜¯ä¸€ç§å¢å¼ºè§†é¢‘ç”Ÿæˆçš„æ¡†æ¶ï¼Œé€šè¿‡ä»å‚è€ƒè§†é¢‘ä¸­æ•´åˆè¿åŠ¨å…ˆéªŒæ¥æé«˜è¿åŠ¨çš„çœŸå®æ„Ÿã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¿åŠ¨é€‚åº”ï¼ˆCAMAï¼‰æŠ€æœ¯ï¼Œæå–é«˜å±‚æ¬¡çš„è¿åŠ¨ç‰¹å¾ï¼Œå¹¶é€šè¿‡å› æœå˜æ¢å™¨æ¶æ„è¿›è¡Œè¿åŠ¨é€‚åº”ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºä½¿ç”¨æ£€ç´¢åŸºç¡€çš„ç®¡é“å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†è½¬ç§»çš„è¿åŠ¨ç‰¹å¾æ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMotionRAGåœ¨å¤šä¸ªé¢†åŸŸå’ŒåŸºç¡€æ¨¡å‹ä¸Šéƒ½æ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è¿åŠ¨çœŸå®æ„Ÿï¼ŒåŒæ—¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‡ ä¹æ²¡æœ‰è®¡ç®—å¼€é”€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23610', 'title': 'Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and\n  Multi-Scale Global-Local Attention', 'url': 'https://huggingface.co/papers/2509.23610', 'abstract': 'Dolphin, an efficient AVSS method, uses a dual-path lightweight video encoder and a lightweight encoder-decoder separator with global-local attention blocks to achieve high separation quality and significant computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-visual speech separation (AVSS) methods leverage visual cues to extract target speech and have demonstrated strong separation quality in noisy acoustic environments. However, these methods usually involve a large number of parameters and require high computational cost, which is unacceptable in many applications where speech separation serves as only a preprocessing step for further speech processing. To address this issue, we propose an efficient AVSS method, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a dual-path lightweight video encoder that transforms lip-motion into discrete audio-aligned semantic tokens. For audio separation, we construct a lightweight encoder-decoder separator, in which each layer incorporates a global-local attention (GLA) block to efficiently capture multi-scale dependencies. Experiments on three benchmark datasets showed that Dolphin not only surpassed the current state-of-the-art (SOTA) model in separation quality but also achieved remarkable improvements in efficiency: over 50% fewer parameters, more than 2.4x reduction in MACs, and over 6x faster GPU inference speed. These results indicate that Dolphin offers a practical and deployable solution for high-performance AVSS in real-world scenarios. Our code and demo page are publicly available at http://cslikai.cn/Dolphin/.', 'score': 12, 'issue_id': 6175, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 28', 'zh': '9æœˆ28æ—¥'}, 'hash': 'd324759166979416', 'authors': ['Kai Li', 'Kejun Gao', 'Xiaolin Hu'], 'affiliations': ['Chinese Institute for Brain Research (CIBR), Beijing 100010, China', 'Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing 100084, China', 'Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing 100084, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.23610.jpg', 'data': {'categories': ['#video', '#benchmark', '#audio', '#inference'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Dolphin: Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞµĞ¿Ğ°Ñ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº', 'desc': 'Dolphin - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¿Ğ°Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸ (AVSS), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¿ÑƒÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ DP-LipCoder, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ± Ğ² Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ encoder-decoder ÑĞµĞ¿Ğ°Ñ€Ğ°Ñ‚Ğ¾Ñ€ Ñ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹. Dolphin Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ SOTA Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ ÑĞµĞ¿Ğ°Ñ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ğ¼ĞµÑ Ğ½Ğ° 50% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ² 2.4 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ² 6 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ inference Ğ½Ğ° GPU. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ, Ğ³Ğ´Ğµ ÑĞµĞ¿Ğ°Ñ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ÑˆÑŒ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾.'}, 'en': {'title': 'Dolphin: Efficient AVSS with Dual-Path Encoding and Global-Local Attention', 'desc': 'Dolphin is a novel audio-visual speech separation (AVSS) method that enhances speech extraction by utilizing visual cues from lip movements. It features a dual-path lightweight video encoder called DP-LipCoder, which converts lip motion into audio-aligned semantic tokens, improving the quality of speech separation. Additionally, Dolphin employs a lightweight encoder-decoder architecture with global-local attention blocks to efficiently manage multi-scale dependencies while significantly reducing computational costs. Experimental results demonstrate that Dolphin outperforms existing state-of-the-art models in both separation quality and efficiency, making it suitable for practical applications in noisy environments.'}, 'zh': {'title': 'Dolphinï¼šé«˜æ•ˆçš„éŸ³è§†é¢‘è¯­éŸ³åˆ†ç¦»æ–°æ–¹æ³•', 'desc': 'Dolphinæ˜¯ä¸€ç§é«˜æ•ˆçš„éŸ³è§†é¢‘è¯­éŸ³åˆ†ç¦»ï¼ˆAVSSï¼‰æ–¹æ³•ï¼Œé‡‡ç”¨åŒè·¯å¾„è½»é‡çº§è§†é¢‘ç¼–ç å™¨å’Œè½»é‡çº§ç¼–ç -è§£ç åˆ†ç¦»å™¨ï¼Œç»“åˆå…¨å±€-å±€éƒ¨æ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥å®ç°é«˜è´¨é‡çš„åˆ†ç¦»æ•ˆæœå’Œæ˜¾è‘—çš„è®¡ç®—æ•ˆç‡ã€‚è¯¥æ–¹æ³•é€šè¿‡DP-LipCoderæå–è§†è§‰ç‰¹å¾ï¼Œå°†å”‡éƒ¨è¿åŠ¨è½¬åŒ–ä¸ºä¸éŸ³é¢‘å¯¹é½çš„è¯­ä¹‰æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDolphinåœ¨åˆ†ç¦»è´¨é‡ä¸Šè¶…è¶Šäº†å½“å‰çš„æœ€å…ˆè¿›æ¨¡å‹ï¼ŒåŒæ—¶åœ¨å‚æ•°æ•°é‡ä¸Šå‡å°‘äº†50%ä»¥ä¸Šï¼ŒMACså‡å°‘äº†2.4å€ï¼ŒGPUæ¨ç†é€Ÿåº¦æé«˜äº†6å€ä»¥ä¸Šã€‚Dolphinä¸ºå®é™…åº”ç”¨ä¸­çš„é«˜æ€§èƒ½éŸ³è§†é¢‘è¯­éŸ³åˆ†ç¦»æä¾›äº†ä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26618', 'title': 'DA^2: Depth Anything in Any Direction', 'url': 'https://huggingface.co/papers/2509.26618', 'abstract': "DAÂ², a zero-shot generalizable and fully end-to-end panoramic depth estimator, addresses challenges in panoramic depth estimation by using a data curation engine and SphereViT to handle spherical distortions, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Panorama has a full FoV (360^circtimes180^circ), offering a more complete visual description than perspective images. Thanks to this characteristic, panoramic depth estimation is gaining increasing traction in 3D vision. However, due to the scarcity of panoramic data, previous methods are often restricted to in-domain settings, leading to poor zero-shot generalization. Furthermore, due to the spherical distortions inherent in panoramas, many approaches rely on perspective splitting (e.g., cubemaps), which leads to suboptimal efficiency. To address these challenges, we propose DA^{2}: Depth Anything in Any Direction, an accurate, zero-shot generalizable, and fully end-to-end panoramic depth estimator. Specifically, for scaling up panoramic data, we introduce a data curation engine for generating high-quality panoramic depth data from perspective, and create sim543K panoramic RGB-depth pairs, bringing the total to sim607K. To further mitigate the spherical distortions, we present SphereViT, which explicitly leverages spherical coordinates to enforce the spherical geometric consistency in panoramic image features, yielding improved performance. A comprehensive benchmark on multiple datasets clearly demonstrates DA^{2}'s SoTA performance, with an average 38% improvement on AbsRel over the strongest zero-shot baseline. Surprisingly, DA^{2} even outperforms prior in-domain methods, highlighting its superior zero-shot generalization. Moreover, as an end-to-end solution, DA^{2} exhibits much higher efficiency over fusion-based approaches. Both the code and the curated panoramic data will be released. Project page: https://depth-any-in-any-dir.github.io/.", 'score': 11, 'issue_id': 6179, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '4f4b4417e453eeeb', 'authors': ['Haodong Li', 'Wangguangdong Zheng', 'Jing He', 'Yuhao Liu', 'Xin Lin', 'Xin Yang', 'Ying-Cong Chen', 'Chunchao Guo'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'Tencent Hunyuan', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2509.26618.jpg', 'data': {'categories': ['#3d', '#optimization', '#data', '#dataset', '#benchmark', '#open_source'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ Ğ±ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DAÂ² â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ½Ğ° Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ¼ 360Ã—180 Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (zero-shot). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ² ~607K Ğ¿Ğ°Ñ€ RGB-Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° SphereViT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 38% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ AbsRel Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ zero-shot baseline Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ°Ğ¶Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'DAÂ²: Depth Estimation Anywhere, Anytime!', 'desc': 'DAÂ² is a novel panoramic depth estimator that operates without needing specific training data, making it capable of zero-shot generalization. It utilizes a data curation engine to create high-quality panoramic depth data from existing perspective images, significantly increasing the dataset size. To tackle the challenges posed by spherical distortions in panoramic images, DAÂ² employs SphereViT, which ensures geometric consistency in the features extracted from these images. The results show that DAÂ² achieves state-of-the-art performance, outperforming previous methods and demonstrating its efficiency as a fully end-to-end solution.'}, 'zh': {'title': 'DAÂ²ï¼šå…¨æ™¯æ·±åº¦ä¼°è®¡çš„æ–°çªç ´', 'desc': 'DAÂ²æ˜¯ä¸€ç§é›¶-shotå¯æ³›åŒ–çš„å…¨ç«¯åˆ°ç«¯å…¨æ™¯æ·±åº¦ä¼°è®¡å™¨ï¼Œæ—¨åœ¨è§£å†³å…¨æ™¯æ·±åº¦ä¼°è®¡ä¸­çš„æŒ‘æˆ˜ã€‚å®ƒé€šè¿‡æ•°æ®ç­–åˆ’å¼•æ“ç”Ÿæˆé«˜è´¨é‡çš„å…¨æ™¯æ·±åº¦æ•°æ®ï¼Œå¹¶åˆ©ç”¨SphereViTå¤„ç†çƒé¢å¤±çœŸï¼Œä»è€Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•ä¸­æ˜¾ç¤ºå‡º38%çš„å¹³å‡AbsRelæ”¹è¿›ï¼Œè¶…è¶Šäº†ä»¥å¾€çš„é›¶-shotåŸºçº¿å’Œé¢†åŸŸå†…æ–¹æ³•ã€‚DAÂ²ä½œä¸ºä¸€ä¸ªå…¨ç«¯åˆ°ç«¯çš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç°å‡ºæ¯”åŸºäºèåˆçš„æ–¹æ³•æ›´é«˜çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25911', 'title': 'Mem-Î±: Learning Memory Construction via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.25911', 'abstract': 'Mem-alpha, a reinforcement learning framework, enhances memory management in large language models through interaction and feedback, improving performance and generalization in long-term information understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it, especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-alpha, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract and store relevant content, then update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-alpha achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens, over 13x the training length, highlighting the robustness of Mem-alpha.', 'score': 10, 'issue_id': 6179, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '304daec7f10d72ea', 'authors': ['Yu Wang', 'Ryuichi Takanobu', 'Zhiqi Liang', 'Yuzhen Mao', 'Yuanzhe Hu', 'Julian McAuley', 'Xiaojian Wu'], 'affiliations': ['Anuttacon', 'Stanford University', 'University of California San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2509.25911.jpg', 'data': {'categories': ['#rl', '#optimization', '#agents', '#long_context', '#dataset', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mem-alpha â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ñ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ½Ğµ ÑƒĞ¼ĞµÑÑ‚ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ, ĞºĞ°ĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ. Mem-alpha Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸ feedback, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ reward signal Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. ĞĞ³ĞµĞ½Ñ‚Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ´Ğ¾ 30k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 400k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Empowering Memory Management in Language Models with Mem-alpha', 'desc': 'Mem-alpha is a reinforcement learning framework designed to improve memory management in large language models (LLMs). It addresses the limitations of current memory-augmented agents by allowing them to learn how to store, structure, and update information through interaction and feedback. By training on a diverse dataset of multi-turn interactions, Mem-alpha optimizes memory construction based on the accuracy of question-answering tasks. The framework demonstrates significant performance gains, enabling agents to generalize effectively to much longer sequences than they were trained on.'}, 'zh': {'title': 'Mem-alphaï¼šæå‡è®°å¿†ç®¡ç†çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'Mem-alpha æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡äº¤äº’å’Œåé¦ˆæ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„è®°å¿†ç®¡ç†èƒ½åŠ›ï¼Œä»è€Œæé«˜å…¶åœ¨é•¿æœŸä¿¡æ¯ç†è§£æ–¹é¢çš„è¡¨ç°å’Œæ³›åŒ–èƒ½åŠ›ã€‚å½“å‰çš„è®°å¿†å¢å¼ºä»£ç†é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰çš„æŒ‡ä»¤å’Œå·¥å…·æ¥æ›´æ–°è®°å¿†ï¼Œä½†è¯­è¨€æ¨¡å‹åœ¨å†³å®šå­˜å‚¨å“ªäº›ä¿¡æ¯ã€å¦‚ä½•æ„å»ºä¿¡æ¯ä»¥åŠä½•æ—¶æ›´æ–°æ—¶å¸¸å¸¸å­˜åœ¨ä¸è¶³ã€‚Mem-alpha é€šè¿‡è®­ç»ƒä»£ç†æœ‰æ•ˆç®¡ç†å¤æ‚çš„è®°å¿†ç³»ç»Ÿï¼Œä½¿ç”¨å¤šè½®äº¤äº’æ¨¡å¼çš„ä¸“é—¨è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶é€šè¿‡ä¸‹æ¸¸é—®ç­”å‡†ç¡®æ€§ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥ä¼˜åŒ–è®°å¿†æ„å»ºã€‚å®éªŒè¯æ˜ï¼ŒMem-alpha åœ¨ç°æœ‰è®°å¿†å¢å¼ºä»£ç†åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œä¸”åœ¨å¤„ç†è¶…è¿‡è®­ç»ƒé•¿åº¦çš„åºåˆ—æ—¶è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26495', 'title': 'OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!', 'url': 'https://huggingface.co/papers/2509.26495', 'abstract': "Operational safety, measured by OffTopicEval, is a critical issue for LLMs, with most models falling short, but prompt-based steering methods show promise in improving out-of-distribution refusal.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM's ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\% -- fall far short of reliable operational safety, while GPT models plateau in the 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma and Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23\\%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents.", 'score': 9, 'issue_id': 6176, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '4d40a6a3c67d3196', 'authors': ['Jingdi Lei', 'Varun Gumma', 'Rishabh Bhardwaj', 'Seok Min Lim', 'Chuan Li', 'Amir Zadeh', 'Soujanya Poria'], 'affiliations': ['IMDA', 'Lambda Labs', 'Nanyang Technological University', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2509.26495.jpg', 'data': {'categories': ['#security', '#alignment', '#training', '#ethics', '#agents', '#benchmark'], 'emoji': 'ğŸš¦', 'ru': {'title': 'ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ: Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ LLM Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ½ĞµĞ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ â€” ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ»ÑŒÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 20 Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… (Qwen-3 Ğ¸ Mistral) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 77-80% Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹: Q-ground Ğ¸ P-ground, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° 23-41%. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ°Ğ´ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒÑ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing LLM Safety with Prompt-Based Steering', 'desc': 'This paper addresses the critical issue of operational safety in Large Language Models (LLMs), which is their ability to safely accept or refuse user queries based on specific tasks. The authors introduce OffTopicEval, a new evaluation suite designed to measure this operational safety across various LLMs. Their findings reveal that most models, including top performers, exhibit significant operational safety shortcomings, with scores indicating high levels of operational unsafety. To mitigate these issues, the paper proposes prompt-based steering methods, which have shown to improve out-of-distribution refusal rates significantly, suggesting a pathway towards safer LLM applications.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ“ä½œå®‰å…¨æ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ“ä½œå®‰å…¨æ€§ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ ‡å‡†OffTopicEvalã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡ä¸åŒæ¨¡å‹çš„è¡¨ç°æœ‰æ‰€ä¸åŒï¼Œä½†æ‰€æœ‰æ¨¡å‹åœ¨æ“ä½œå®‰å…¨æ€§æ–¹é¢éƒ½å­˜åœ¨æ˜¾è‘—ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†åŸºäºæç¤ºçš„å¼•å¯¼æ–¹æ³•ï¼ŒåŒ…æ‹¬æŸ¥è¯¢å¼•å¯¼ï¼ˆQ-groundï¼‰å’Œç³»ç»Ÿæç¤ºå¼•å¯¼ï¼ˆP-groundï¼‰ï¼Œè¿™ä¸¤ç§æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æ‹’ç»èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œæ“ä½œå®‰å…¨æ€§æ˜¯æ¨¡å‹å¯¹é½çš„æ ¸å¿ƒé—®é¢˜ï¼Œæ€¥éœ€é‡‡å–å¹²é¢„æªæ–½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26030', 'title': 'Muon Outperforms Adam in Tail-End Associative Memory Learning', 'url': 'https://huggingface.co/papers/2509.26030', 'abstract': "Muon optimizer outperforms Adam in training LLMs by effectively optimizing associative memory parameters and balancing learning across classes in heavy-tailed data.  \t\t\t\t\tAI-generated summary \t\t\t\t The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.", 'score': 8, 'issue_id': 6188, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '494fca9d3962717d', 'authors': ['Shuche Wang', 'Fengzhuo Zhang', 'Jiaxiang Li', 'Cunxiao Du', 'Chao Du', 'Tianyu Pang', 'Zhuoran Yang', 'Mingyi Hong', 'Vincent Y. F. Tan'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'University of Minnesota', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2509.26030.jpg', 'data': {'categories': ['#math', '#training', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'Muon: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ»Ğ¸Ğ´ĞµÑ€ Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Muon, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Adam Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Muon Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²ĞµÑĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞµÑ‚Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºĞ»Ğ°ÑÑÑ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµĞ´ĞºĞ¾ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‚ÑÑ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ‚ÑĞ¶Ñ‘Ğ»Ñ‹Ğ¼Ğ¸ Ñ…Ğ²Ğ¾ÑÑ‚Ğ°Ğ¼Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Muon Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Adam, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¾ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ ĞºĞ»Ğ°ÑÑĞ¾Ğ².'}, 'en': {'title': 'Muon: The Optimizer for Balanced Learning in Heavy-Tailed Data', 'desc': "The Muon optimizer is a new method that improves the training of Large Language Models (LLMs) by optimizing specific parameters related to associative memory. It outperforms the widely used Adam optimizer, especially when dealing with heavy-tailed data where some classes are much less frequent than others. The paper explains that Muon's success comes from its ability to maintain a balanced learning process across all classes, particularly the less frequent ones, by using a unique update rule. Theoretical analysis supports these findings, showing that Muon achieves better performance in class-imbalanced scenarios compared to Adam."}, 'zh': {'title': 'Muonä¼˜åŒ–å™¨ï¼šé‡å°¾æ•°æ®ä¸­çš„å­¦ä¹ å¹³è¡¡è€…', 'desc': 'Muonä¼˜åŒ–å™¨åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶è¡¨ç°ä¼˜äºAdamï¼Œä¸»è¦é€šè¿‡æœ‰æ•ˆä¼˜åŒ–å…³è”è®°å¿†å‚æ•°å’Œåœ¨é‡å°¾æ•°æ®ä¸­å¹³è¡¡å­¦ä¹ æ¥å®ç°ã€‚æœ¬æ–‡æ­ç¤ºäº†MuonæˆåŠŸçš„æœºåˆ¶ï¼ŒæŒ‡å‡ºå…¶ä¼˜åŒ–çš„ä¸»è¦å› ç´ æ˜¯LLMsçš„å€¼å’Œè¾“å‡ºæ³¨æ„åŠ›æƒé‡ä»¥åŠå‰é¦ˆç½‘ç»œã€‚ç ”ç©¶è¡¨æ˜ï¼ŒMuonåœ¨é‡å°¾æ•°æ®ä¸Šèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä¼˜åŒ–å°¾ç±»ï¼Œè€Œå…¶æ›´æ–°è§„åˆ™ä½¿å¾—å­¦ä¹ è¿‡ç¨‹æ›´åŠ å‡åŒ€ã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†Muonåœ¨ç±»ä¸å¹³è¡¡æ•°æ®ä¸‹èƒ½å¤Ÿå®ç°æ›´å¹³è¡¡çš„å­¦ä¹ ï¼Œè€ŒAdamåˆ™å¯èƒ½å¯¼è‡´å­¦ä¹ è¯¯å·®çš„å·¨å¤§å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25189', 'title': 'InfoAgent: Advancing Autonomous Information-Seeking Agents', 'url': 'https://huggingface.co/papers/2509.25189', 'abstract': 'InfoAgent, a deep research agent using a custom data synthesis pipeline and search infrastructure, outperforms existing agents by improving tool use and reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Building Large Language Model agents that expand their capabilities by interacting with external tools represents a new frontier in AI research and applications. In this paper, we introduce InfoAgent, a deep research agent powered by an innovative data synthesis pipeline and orchestrated web search tools. To construct challenging, hard-to-find queries,we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty. Unlike prior work that relies heavily on commercial search tools, we develop a dedicated self-hosted search infrastructure, enhancing transparency of agent environments and facilitating further advancement of agent capacity. We evaluate the effectiveness of our data pipeline by measuring the average number of tool calls required to correctly answer a question, and also show that our agent yields better performance when equipped with our tools. Our InfoAgent is post-trained from Qwen3-14B using a two-stage recipe: cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy on BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming prior open-source deep research agents such as WebSailor-72B and DeepDive-32B.', 'score': 8, 'issue_id': 6178, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '96392fa5b9fec8c5', 'authors': ['Gongrui Zhang', 'Jialiang Zhu', 'Ruiqi Yang', 'Kai Qiu', 'Miaosen Zhang', 'Zhirong Wu', 'Qi Dai', 'Bei Liu', 'Chong Luo', 'Zhengyuan Yang', 'Linjie Li', 'Lijuan Wang', 'Weizhu Chen', 'Yuan Zhang', 'Xin Li', 'Zhaoyi Liu', 'Xin Geng', 'Baining Guo'], 'affiliations': ['Brown University', 'Microsoft', 'Southeast University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25189.jpg', 'data': {'categories': ['#reasoning', '#rl', '#agents', '#training', '#open_source', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'InfoAgent: ĞĞ³ĞµĞ½Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ InfoAgent â€” Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ pipeline ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ğ´Ğ»Ñ Ğ¾ÑĞ²Ğ¾ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. InfoAgent Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen3-14B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ open-source Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 15.3% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° BrowseComp Ğ¸ 40.4% Ğ½Ğ° Xbench-DS, Ğ¾Ğ¿ĞµÑ€ĞµĞ¶Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ WebSailor-72B Ğ¸ DeepDive-32B.'}, 'en': {'title': 'InfoAgent: Elevating Research with Enhanced Tool Use and Reasoning', 'desc': 'InfoAgent is a deep research agent that enhances its performance by utilizing a unique data synthesis pipeline and a self-hosted search infrastructure. It constructs complex queries through entity trees and sub-tree sampling, which increases the difficulty of questions systematically. This approach allows InfoAgent to outperform existing agents by improving its reasoning and tool usage capabilities. The agent is fine-tuned using a two-stage process that includes supervised learning and reinforcement learning, leading to significant improvements in accuracy on various benchmarks.'}, 'zh': {'title': 'InfoAgentï¼šæå‡å·¥å…·ä½¿ç”¨ä¸æ¨ç†èƒ½åŠ›çš„æ·±åº¦ç ”ç©¶ä»£ç†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºInfoAgentçš„æ·±åº¦ç ”ç©¶ä»£ç†ï¼Œå®ƒé€šè¿‡åˆ›æ–°çš„æ•°æ®åˆæˆç®¡é“å’Œæœç´¢å·¥å…·ï¼Œæå‡äº†å·¥å…·ä½¿ç”¨å’Œæ¨ç†èƒ½åŠ›ã€‚InfoAgentæ„å»ºäº†å®ä½“æ ‘å¹¶åº”ç”¨å­æ ‘é‡‡æ ·ï¼Œä»¥ç³»ç»Ÿæ€§åœ°å¢åŠ é—®é¢˜çš„éš¾åº¦ï¼Œä»è€Œç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§çš„æŸ¥è¯¢ã€‚ä¸ä»¥å¾€ä¾èµ–å•†ä¸šæœç´¢å·¥å…·çš„ç ”ç©¶ä¸åŒï¼ŒInfoAgentå¼€å‘äº†ä¸“ç”¨çš„è‡ªæ‰˜ç®¡æœç´¢åŸºç¡€è®¾æ–½ï¼Œå¢å¼ºäº†ä»£ç†ç¯å¢ƒçš„é€æ˜åº¦ã€‚é€šè¿‡æµ‹é‡æ­£ç¡®å›ç­”é—®é¢˜æ‰€éœ€çš„å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œè¯„ä¼°äº†æ•°æ®ç®¡é“çš„æœ‰æ•ˆæ€§ï¼Œç»“æœæ˜¾ç¤ºInfoAgentåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¹‹å‰çš„å¼€æºæ·±åº¦ç ”ç©¶ä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.24207', 'title': 'Humanline: Online Alignment as Perceptual Loss', 'url': 'https://huggingface.co/papers/2509.24207', 'abstract': 'Online alignment methods like GRPO outperform offline methods like DPO due to better approximation of human-perceived probability distributions, and introducing perceptual biases into offline training can achieve similar performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO) -- but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping -- originally introduced to just stabilize training -- recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating humanline variants of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks.', 'score': 8, 'issue_id': 6178, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '36a99560c909c06b', 'authors': ['Sijia Liu', 'Niklas Muennighoff', 'Kawin Ethayarajh'], 'affiliations': ['Princeton University', 'Stanford University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2509.24207.jpg', 'data': {'categories': ['#alignment', '#training', '#rl', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ§ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ (GRPO) Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² (DPO), Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ² Ğ¸Ğ· Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ¸. ĞĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞµ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ° ĞºĞ»Ğ¸Ğ¿Ğ¿Ğ¸Ğ½Ğ³ Ğ² PPO/GRPO Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ humanline, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ‚Ğ¸Ğ¿Ğ° DPO/KTO/GRPO. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ humanline Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ñ„Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ´ĞµÑˆĞµĞ²Ğ»Ğµ.'}, 'en': {'title': 'Aligning AI with Human Perception for Better Performance', 'desc': 'This paper discusses how online alignment methods, such as GRPO, are more effective than offline methods like DPO because they better reflect how humans perceive probabilities. It introduces the concept of perceptual biases, suggesting that incorporating these biases into offline training can yield similar performance to online methods. The authors argue that the traditional distinction between online and offline training is less important than aligning training with human perception. They propose a new design pattern that integrates perceptual distortions into training objectives, allowing offline methods to achieve results comparable to online methods.'}, 'zh': {'title': 'åœ¨çº¿å¯¹é½è¶…è¶Šç¦»çº¿å¯¹é½çš„ç§˜å¯†', 'desc': 'åœ¨çº¿å¯¹é½æ–¹æ³•ï¼ˆå¦‚GRPOï¼‰æ¯”ç¦»çº¿æ–¹æ³•ï¼ˆå¦‚DPOï¼‰è¡¨ç°æ›´å¥½ï¼Œå› ä¸ºå®ƒä»¬æ›´å¥½åœ°è¿‘ä¼¼äººç±»æ„ŸçŸ¥çš„æ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»¥äººä¸ºä¸­å¿ƒçš„è§£é‡Šï¼ŒåŸºäºè¡Œä¸ºç»æµå­¦çš„å‰æ™¯ç†è®ºï¼Œè¯æ˜åœ¨çº¿ç­–ç•¥é‡‡æ ·æ›´èƒ½æ¥è¿‘äººç±»æ„ŸçŸ¥çš„åˆ†å¸ƒã€‚PPO/GRPOé£æ ¼çš„å‰ªåˆ‡ä¸ä»…ç”¨äºç¨³å®šè®­ç»ƒï¼Œè¿˜æ¢å¤äº†äººç±»å¯¹æ¦‚ç‡çš„æ„ŸçŸ¥åå·®ã€‚é€šè¿‡é€‰æ‹©æ€§åœ°è®­ç»ƒä»»ä½•æ•°æ®ä»¥æ¨¡ä»¿äººç±»æ„ŸçŸ¥ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¿«ã€æ›´ä¾¿å®œå’Œçµæ´»åœ°è¿›è¡Œåè®­ç»ƒï¼Œè€Œä¸ç‰ºç‰²æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26628', 'title': 'Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models', 'url': 'https://huggingface.co/papers/2509.26628', 'abstract': 'A novel PSRL framework (AttnRL) enhances exploration efficiency in reasoning models by branching from high attention positions and using an adaptive sampling strategy, outperforming prior methods in mathematical reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.', 'score': 7, 'issue_id': 6175, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'ac8005b1bfc91f64', 'authors': ['Runze Liu', 'Jiakang Wang', 'Yuling Shi', 'Zhihui Xie', 'Chenxin An', 'Kaiyan Zhang', 'Jian Zhao', 'Xiaodong Gu', 'Lei Lin', 'Wenping Hu', 'Xiu Li', 'Fuzheng Zhang', 'Guorui Zhou', 'Kun Gai'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Kuaishou Technology', 'Shanghai Jiao Tong University', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.26628.jpg', 'data': {'categories': ['#math', '#optimization', '#reasoning', '#training', '#rl'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ AttnRL Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Process-Supervised Reinforcement Learning Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸ÑÑ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ attention scores, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑˆĞ°Ğ³Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ one-step off-policy Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Reasoning with Efficient Exploration in AttnRL', 'desc': 'The paper presents a new framework called AttnRL that improves exploration efficiency in reasoning models using Process-Supervised Reinforcement Learning (PSRL). It focuses on branching from positions in the model that have high attention scores, which are linked to better reasoning performance. Additionally, the authors introduce an adaptive sampling strategy that adjusts based on the difficulty of problems and the size of previous training batches. Experiments show that AttnRL outperforms existing methods in mathematical reasoning tasks, enhancing both performance and training efficiency.'}, 'zh': {'title': 'æå‡æ¨ç†æ¨¡å‹æ¢ç´¢æ•ˆç‡çš„æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ˆAttnRLï¼‰ï¼Œæ—¨åœ¨æé«˜æ¨ç†æ¨¡å‹çš„æ¢ç´¢æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»é«˜æ³¨æ„åŠ›ä½ç½®åˆ†æ”¯ï¼Œå¹¶é‡‡ç”¨è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„å±€é™æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé«˜æ³¨æ„åŠ›åˆ†æ•°çš„æ­¥éª¤ä¸æ¨ç†è¡Œä¸ºç›¸å…³ï¼Œå› æ­¤æˆ‘ä»¬é€‰æ‹©ä»è¿™äº›ä½ç½®è¿›è¡Œåˆ†æ”¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä¸€æ­¥ç¦»çº¿ç­–ç•¥è®­ç»ƒç®¡é“ï¼Œä»¥è¿›ä¸€æ­¥æé«˜é‡‡æ ·æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26542', 'title': 'Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap', 'url': 'https://huggingface.co/papers/2509.26542', 'abstract': 'VERA is a benchmark for evaluating reasoning ability in voice-interactive systems, revealing significant performance gaps compared to text models and highlighting challenges in real-time interaction.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for evaluating reasoning ability in voice-interactive systems under real-time conversational constraints. VERA comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual). Each item is adapted for speech interaction while preserving reasoning difficulty. VERA enables direct text-voice comparison within model families and supports analysis of how architectural choices affect reliability. We assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around ~10% accuracy, while approaching text performance requires sacrificing real-time interaction. Diagnostic experiments indicate that common mitigations are insufficient. Increasing "thinking time" yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors. Failure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs. VERA provides a reproducible testbed and targeted diagnostics for architectures that decouple thinking from speaking, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.', 'score': 7, 'issue_id': 6175, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '94e739b649ffcc6e', 'authors': ['Yueqian Lin', 'Zhengmian Hu', 'Qinsi Wang', 'Yudong Liu', 'Hengfan Zhang', 'Jayakumar Subramanian', 'Nikos Vlassis', 'Hai Helen Li', 'Yiran Chen'], 'affiliations': ['Adobe, San Jose, CA, USA', 'Duke University, Durham, NC, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.26542.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#architecture', '#long_context'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Ğ“Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ğµ AI-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ', 'desc': 'VERA â€” ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸: Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 74.8% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° ĞµÑ‘ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ â€” Ğ²ÑĞµĞ³Ğ¾ 6.1%. Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Â«Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµÂ» Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚, Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¾Ğ·Ğ²ÑƒÑ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ½Ğ¾ Ğ²ÑÑ‘ Ñ€Ğ°Ğ²Ğ½Ğ¾ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ñƒ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2,931 Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ¾Ğ¹ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´ Ğ¿Ğ¾ Ğ¿ÑÑ‚Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ, ĞºĞ°Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ… Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Bridging the Gap: Evaluating Voice Reasoning with VERA', 'desc': 'VERA is a benchmark designed to evaluate the reasoning capabilities of voice-interactive systems, highlighting the performance differences between voice and text models. It includes 2,931 voice-native episodes adapted from existing text benchmarks, organized into five distinct tracks. The study reveals significant accuracy gaps, with text models outperforming voice models in reasoning tasks, particularly in mathematics and factual contexts. VERA serves as a tool for analyzing how different architectural choices impact the reliability of voice systems, aiming to improve real-time interaction without sacrificing reasoning quality.'}, 'zh': {'title': 'VERAï¼šè¯­éŸ³äº¤äº’æ¨ç†èƒ½åŠ›çš„è¯„ä¼°åŸºå‡†', 'desc': 'VERAæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è¯­éŸ³äº¤äº’ç³»ç»Ÿæ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œæ­ç¤ºäº†ä¸æ–‡æœ¬æ¨¡å‹ç›¸æ¯”çš„æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œå¹¶å¼ºè°ƒäº†å®æ—¶äº¤äº’ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥åŸºå‡†åŒ…å«2931ä¸ªè¯­éŸ³åŸç”Ÿçš„æ¡ˆä¾‹ï¼Œæ¶µç›–æ•°å­¦ã€ç½‘ç»œã€ç§‘å­¦ã€é•¿ä¸Šä¸‹æ–‡å’Œäº‹å®äº”ä¸ªé¢†åŸŸï¼Œé€‚åº”è¯­éŸ³äº¤äº’çš„åŒæ—¶ä¿æŒæ¨ç†éš¾åº¦ã€‚é€šè¿‡å¯¹12ä¸ªç°ä»£è¯­éŸ³ç³»ç»Ÿä¸å¼ºå¤§çš„æ–‡æœ¬åŸºçº¿è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°è¯­éŸ³ç³»ç»Ÿåœ¨å‡†ç¡®æ€§ä¸Šå­˜åœ¨è¾ƒå¤§çš„å·®è·ã€‚VERAä¸ºè§£è€¦æ€è€ƒä¸è¡¨è¾¾çš„æ¶æ„æä¾›äº†å¯é‡å¤çš„æµ‹è¯•å¹³å°å’Œé’ˆå¯¹æ€§çš„è¯Šæ–­ï¼Œå¸®åŠ©è¡¡é‡å®æ—¶è¯­éŸ³åŠ©æ‰‹åœ¨æµç•…æ€§å’Œå¯é æ¨ç†æ–¹é¢çš„è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25397', 'title': 'A Cartography of Open Collaboration in Open Source AI: Mapping\n  Practices, Motivations, and Governance in 14 Open Large Language Model\n  Projects', 'url': 'https://huggingface.co/papers/2509.25397', 'abstract': 'Research explores collaboration in open large language models, identifying diverse motivations and organizational models among developers from various sectors.  \t\t\t\t\tAI-generated summary \t\t\t\t The proliferation of open large language models (LLMs) is fostering a vibrant ecosystem of research and innovation in artificial intelligence (AI). However, the methods of collaboration used to develop open LLMs both before and after their public release have not yet been comprehensively studied, limiting our understanding of how open LLM projects are initiated, organized, and governed as well as what opportunities there are to foster this ecosystem even further. We address this gap through an exploratory analysis of open collaboration throughout the development and reuse lifecycle of open LLMs, drawing on semi-structured interviews with the developers of 14 open LLMs from grassroots projects, research institutes, startups, and Big Tech companies in North America, Europe, Africa, and Asia. We make three key contributions to research and practice. First, collaboration in open LLM projects extends far beyond the LLMs themselves, encompassing datasets, benchmarks, open source frameworks, leaderboards, knowledge sharing and discussion forums, and compute partnerships, among others. Second, open LLM developers have a variety of social, economic, and technological motivations, from democratizing AI access and promoting open science to building regional ecosystems and expanding language representation. Third, the sampled open LLM projects exhibit five distinct organizational models, ranging from single company projects to non-profit-sponsored grassroots projects, which vary in their centralization of control and community engagement strategies used throughout the open LLM lifecycle. We conclude with practical recommendations for stakeholders seeking to support the global community building a more open future for AI.', 'score': 7, 'issue_id': 6181, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': 'c2c88df55d808e37', 'authors': ['Johan LinÃ¥ker', 'Cailean Osborne', 'Jennifer Ding', 'Ben Burtenshaw'], 'affiliations': ['Boundary Object Studio London, UK', 'Hugging Face Antwerp, Belgium', 'RISE Research Institutes of Sweden Lund, Sweden', 'University of Oxford Oxford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2509.25397.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#multilingual'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Ğ­ĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼Ğ¸ 14 Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½ Ğ¸ ÑĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞšĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM-Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾ Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸, Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸, Ğ»Ğ¸Ğ´ĞµÑ€Ğ±Ğ¾Ñ€Ğ´Ñ‹ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€ÑÑ‚Ğ²Ğ°. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº AI Ğ¸ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ Ğ½Ğ°ÑƒĞºĞ¸ Ğ´Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¿ÑÑ‚ÑŒ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… LLM-Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾ Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Fostering Collaboration for Open AI Innovation', 'desc': 'This paper investigates how collaboration occurs in the development of open large language models (LLMs) and identifies various motivations and organizational structures among developers. It highlights that collaboration extends beyond just the models to include datasets, benchmarks, and community engagement. The study reveals that developers are driven by diverse goals such as democratizing AI and enhancing language representation. Additionally, it categorizes open LLM projects into five organizational models, providing insights into how these projects can be better supported.'}, 'zh': {'title': 'å¼€æ”¾å¤§å‹è¯­è¨€æ¨¡å‹çš„åä½œä¸åˆ›æ–°', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¼€æ”¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼€å‘ä¸­çš„åä½œï¼Œè¯†åˆ«äº†æ¥è‡ªä¸åŒé¢†åŸŸå¼€å‘è€…çš„å¤šæ ·åŒ–åŠ¨æœºå’Œç»„ç»‡æ¨¡å¼ã€‚ç ”ç©¶å‘ç°ï¼Œå¼€æ”¾LLMé¡¹ç›®çš„åä½œä¸ä»…é™äºæ¨¡å‹æœ¬èº«ï¼Œè¿˜åŒ…æ‹¬æ•°æ®é›†ã€åŸºå‡†æµ‹è¯•ã€å¼€æºæ¡†æ¶ç­‰å¤šä¸ªæ–¹é¢ã€‚å¼€å‘è€…çš„åŠ¨æœºå¤šç§å¤šæ ·ï¼ŒåŒ…æ‹¬ä¿ƒè¿›äººå·¥æ™ºèƒ½çš„æ°‘ä¸»åŒ–ã€æ¨åŠ¨å¼€æ”¾ç§‘å­¦ä»¥åŠæ‰©å±•è¯­è¨€è¡¨ç°ç­‰ã€‚æœ€åï¼Œç ”ç©¶æå‡ºäº†å¯¹åˆ©ç›Šç›¸å…³è€…çš„å®ç”¨å»ºè®®ï¼Œä»¥æ”¯æŒå…¨çƒç¤¾åŒºå»ºè®¾æ›´å¼€æ”¾çš„äººå·¥æ™ºèƒ½æœªæ¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25339', 'title': 'VisualOverload: Probing Visual Understanding of VLMs in Really Dense\n  Scenes', 'url': 'https://huggingface.co/papers/2509.25339', 'abstract': "VisualOverload is a VQA benchmark that challenges models with simple vision tasks in densely populated scenes, revealing gaps in current VLMs' performance and offering insights into their failure modes.  \t\t\t\t\tAI-generated summary \t\t\t\t Is basic visual understanding really solved in state-of-the-art VLMs? We present VisualOverload, a slightly different visual question answering (VQA) benchmark comprising 2,720 question-answer pairs, with privately held ground-truth responses. Unlike prior VQA datasets that typically focus on near global image understanding, VisualOverload challenges models to perform simple, knowledge-free vision tasks in densely populated (or, overloaded) scenes. Our dataset consists of high-resolution scans of public-domain paintings that are populated with multiple figures, actions, and unfolding subplots set against elaborately detailed backdrops. We manually annotated these images with questions across six task categories to probe for a thorough understanding of the scene. We hypothesize that current benchmarks overestimate the performance of VLMs, and encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes. Indeed, we observe that even the best model (o3) out of 37 tested models only achieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on all questions. Beyond a thorough evaluation, we complement our benchmark with an error analysis that reveals multiple failure modes, including a lack of counting skills, failure in OCR, and striking logical inconsistencies under complex tasks. Altogether, VisualOverload exposes a critical gap in current vision models and offers a crucial resource for the community to develop better models.   Benchmark: http://paulgavrikov.github.io/visualoverload", 'score': 7, 'issue_id': 6175, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': 'be3aca0a0807cc94', 'authors': ['Paul Gavrikov', 'Wei Lin', 'M. Jehanzeb Mirza', 'Soumya Jahagirdar', 'Muhammad Huzaifa', 'Sivan Doveh', 'Serena Yeung-Levy', 'James Glass', 'Hilde Kuehne'], 'affiliations': ['Independent Researcher', 'JKU Linz', 'MIT CSAIL', 'MIT-IBM Watson AI Lab', 'Stanford', 'TÃ¼bingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2509.25339.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#benchmark', '#dataset', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'VisualOverload: ĞºĞ¾Ğ³Ğ´Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ VLM Ğ² Ñ‚ÑƒĞ¿Ğ¸Ğº', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº VisualOverload Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿ĞµÑ€ĞµĞ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ñ…. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 2720 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ¸ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ”Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (o3) Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 69.5% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ½Ğ° ÑĞ°Ğ¼Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼ ÑĞ¿Ğ»Ğ¸Ñ‚Ğµ â€” Ğ²ÑĞµĞ³Ğ¾ 19.6%, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° (OCR) Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unveiling Gaps in Visual Understanding with VisualOverload', 'desc': 'VisualOverload is a new benchmark for visual question answering (VQA) that tests the capabilities of vision-language models (VLMs) in complex scenes filled with details. It includes 2,720 question-answer pairs based on high-resolution images of public-domain paintings, focusing on simple tasks that require understanding of crowded environments. The study reveals that existing VLMs struggle with basic visual comprehension, achieving only 19.6% accuracy on the most challenging questions. This benchmark not only highlights the limitations of current models but also provides a resource for improving their performance through targeted error analysis.'}, 'zh': {'title': 'æ­ç¤ºè§†è§‰æ¨¡å‹çš„å…³é”®ç¼ºé™·', 'desc': 'VisualOverloadæ˜¯ä¸€ä¸ªè§†è§‰é—®ç­”ï¼ˆVQAï¼‰åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡å¯†é›†åœºæ™¯ä¸­çš„ç®€å•è§†è§‰ä»»åŠ¡æ¥æŒ‘æˆ˜æ¨¡å‹ï¼Œæ­ç¤ºå½“å‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æ€§èƒ½çš„ä¸è¶³ã€‚è¯¥åŸºå‡†åŒ…å«2720ä¸ªé—®ç­”å¯¹ï¼Œä¸»è¦å…³æ³¨åœ¨å¤æ‚èƒŒæ™¯ä¸‹çš„å›¾åƒç†è§£èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰åŸºå‡†å¯èƒ½é«˜ä¼°äº†VLMsçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¯†é›†åœºæ™¯æ—¶ï¼Œæ¨¡å‹åœ¨ç»†èŠ‚ç¼–ç å’Œæ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚é€šè¿‡é”™è¯¯åˆ†æï¼ŒVisualOverloadæ­ç¤ºäº†å¤šä¸ªå¤±è´¥æ¨¡å¼ï¼ŒåŒ…æ‹¬è®¡æ•°èƒ½åŠ›ä¸è¶³ã€å…‰å­¦å­—ç¬¦è¯†åˆ«å¤±è´¥å’Œå¤æ‚ä»»åŠ¡ä¸‹çš„é€»è¾‘ä¸ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.22613', 'title': 'Benefits and Pitfalls of Reinforcement Learning for Language Model\n  Planning: A Theoretical Perspective', 'url': 'https://huggingface.co/papers/2509.22613', 'abstract': "Theoretical analysis of reinforcement learning methods in enhancing LLM planning reveals that while RL improves generalization through exploration, policy gradient suffers from diversity collapse, whereas Q-learning maintains diversity and requires careful reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.", 'score': 7, 'issue_id': 6175, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': '8899a479ee5856c0', 'authors': ['Siwei Wang', 'Yifei Shen', 'Haoran Sun', 'Shi Feng', 'Shang-Hua Teng', 'Li Dong', 'Yaru Hao', 'Wei Chen'], 'affiliations': ['Harvard University', 'Microsoft Research Asia', 'Peking University', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2509.22613.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#benchmark', '#training', '#rl'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Q-learning Ğ»ÑƒÑ‡ÑˆĞµ policy gradient Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ reinforcement learning ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. Supervised fine-tuning Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğº Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº RL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€ĞµĞ´Ñ‹. Policy gradient Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Q-learning ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ off-policy. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ reward hacking Ğ² Q-learning.'}, 'en': {'title': 'Exploration Enhances Planning: Balancing Diversity in RL for LLMs', 'desc': 'This paper analyzes how reinforcement learning (RL) methods can improve the planning abilities of Large Language Models (LLMs). It highlights that while RL enhances generalization through exploration, policy gradient methods face a problem called diversity collapse, where the variety of outputs decreases over time. In contrast, Q-learning maintains output diversity and allows for off-policy learning, but it requires careful design of rewards to avoid issues like reward hacking. The findings are validated through experiments on the Blocksworld planning benchmark, demonstrating the practical implications of these theoretical insights.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡è¯­è¨€æ¨¡å‹è§„åˆ’èƒ½åŠ›çš„ç†è®ºåˆ†æ', 'desc': 'æœ¬ç ”ç©¶åˆ†æäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§„åˆ’èƒ½åŠ›ä¸­çš„ä½œç”¨ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶RLé€šè¿‡æ¢ç´¢æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šå‡ºç°å¤šæ ·æ€§å´©æºƒçš„é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒQå­¦ä¹ æ–¹æ³•èƒ½å¤Ÿä¿æŒå¤šæ ·æ€§ï¼Œå¹¶ä¸”åœ¨æ”¶æ•›æ—¶å…·æœ‰ç¦»çº¿å­¦ä¹ çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†å¥–åŠ±è®¾è®¡çš„é‡è¦æ€§ï¼Œä»¥é˜²æ­¢Qå­¦ä¹ ä¸­çš„å¥–åŠ±æ“æ§é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26476', 'title': 'Regression Language Models for Code', 'url': 'https://huggingface.co/papers/2509.26476', 'abstract': 'A unified Regression Language Model (RLM) predicts numeric outcomes of code executions, including memory footprint, latency, and neural network performance, across multiple languages and hardware platforms.  \t\t\t\t\tAI-generated summary \t\t\t\t We study code-to-metric regression: predicting numeric outcomes of code executions, a challenging task due to the open-ended nature of programming languages. While prior methods have resorted to heavy and domain-specific feature engineering, we show that a single unified Regression Language Model (RLM) can simultaneously predict directly from text, (i) the memory footprint of code across multiple high-level languages such as Python and C++, (ii) the latency of Triton GPU kernels, and (iii) the accuracy and speed of trained neural networks represented in ONNX. In particular, a relatively small 300M parameter RLM initialized from T5Gemma, obtains > 0.9 Spearman-rank on competitive programming submissions from APPS, and a single unified model achieves > 0.5 average Spearman-rank across 17 separate languages from CodeNet. Furthermore, the RLM can obtain the highest average Kendall-Tau of 0.46 on five classic NAS design spaces previously dominated by graph neural networks, and simultaneously predict architecture latencies on numerous hardware platforms.', 'score': 6, 'issue_id': 6176, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '4c9c21b6cd24bc71', 'authors': ['Yash Akhauri', 'Xingyou Song', 'Arissa Wongpanich', 'Bryan Lewandowski', 'Mohamed S. Abdelfattah'], 'affiliations': ['Cornell University', 'Google'], 'pdf_title_img': 'assets/pdf/title_img/2509.26476.jpg', 'data': {'categories': ['#games', '#data', '#optimization', '#training', '#dataset', '#multilingual', '#small_models'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞµĞ´Ğ¸Ğ½Ğ°Ñ Regression Language Model (RLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (Python, C++, Triton, ONNX) Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². RLM Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ T5Gemma Ñ 300M Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Spearman > 0.9 Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ > 0.5 Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ¿Ğ¾ 17 ÑĞ·Ñ‹ĞºĞ°Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ graph neural networks Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Neural Architecture Search, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Kendall-Tau 0.46 Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ….'}, 'en': {'title': 'Unified Regression Model: Predicting Code Performance Across Languages and Hardware', 'desc': 'The paper introduces a unified Regression Language Model (RLM) that predicts numeric outcomes from code executions, such as memory usage and latency, across various programming languages and hardware. Unlike previous methods that relied on extensive feature engineering, the RLM directly analyzes code text to make predictions. It demonstrates strong performance, achieving high Spearman-rank scores on competitive programming tasks and across multiple languages. Additionally, the model excels in predicting architecture latencies in neural architecture search, outperforming traditional graph neural networks.'}, 'zh': {'title': 'ç»Ÿä¸€å›å½’è¯­è¨€æ¨¡å‹ï¼šè·¨è¯­è¨€ä¸å¹³å°çš„æ€§èƒ½é¢„æµ‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å›å½’è¯­è¨€æ¨¡å‹ï¼ˆRLMï¼‰ï¼Œç”¨äºé¢„æµ‹ä»£ç æ‰§è¡Œçš„æ•°å€¼ç»“æœï¼ŒåŒ…æ‹¬å†…å­˜å ç”¨ã€å»¶è¿Ÿå’Œç¥ç»ç½‘ç»œæ€§èƒ½ã€‚ä¸ä»¥å¾€éœ€è¦å¤§é‡é¢†åŸŸç‰¹å®šç‰¹å¾å·¥ç¨‹çš„æ–¹æ³•ä¸åŒï¼ŒRLMèƒ½å¤Ÿç›´æ¥ä»æ–‡æœ¬ä¸­è¿›è¡Œé¢„æµ‹ï¼Œé€‚ç”¨äºå¤šç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚å®éªŒè¡¨æ˜ï¼ŒRLMåœ¨å¤šä¸ªç¼–ç¨‹è¯­è¨€çš„ç«äº‰æ€§æäº¤ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°äº†è¶…è¿‡0.9çš„Spearmanç­‰çº§ç›¸å…³ç³»æ•°ã€‚è¯¥æ¨¡å‹è¿˜åœ¨ç»å…¸çš„ç¥ç»æ¶æ„æœç´¢è®¾è®¡ç©ºé—´ä¸­å–å¾—äº†æœ€é«˜çš„Kendall-Tauå¹³å‡å€¼ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸åŒç¡¬ä»¶å¹³å°ä¸Šçš„å¹¿æ³›é€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26645', 'title': 'TTT3R: 3D Reconstruction as Test-Time Training', 'url': 'https://huggingface.co/papers/2509.26645', 'abstract': 'TTT3R, a test-time training intervention, enhances length generalization in 3D reconstruction by dynamically adjusting memory updates based on alignment confidence, improving global pose estimation and processing efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern Recurrent Neural Networks have become a competitive architecture for 3D reconstruction due to their linear-time complexity. However, their performance degrades significantly when applied beyond the training context length, revealing limited length generalization. In this work, we revisit the 3D reconstruction foundation models from a Test-Time Training perspective, framing their designs as an online learning problem. Building on this perspective, we leverage the alignment confidence between the memory state and incoming observations to derive a closed-form learning rate for memory updates, to balance between retaining historical information and adapting to new observations. This training-free intervention, termed TTT3R, substantially improves length generalization, achieving a 2times improvement in global pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU memory to process thousands of images. Code available in https://rover-xingyu.github.io/TTT3R', 'score': 4, 'issue_id': 6181, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '4706713900f12d39', 'authors': ['Xingyu Chen', 'Yue Chen', 'Yuliang Xiu', 'Andreas Geiger', 'Anpei Chen'], 'affiliations': ['University of Tubingen, Tubingen AI Center', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2509.26645.jpg', 'data': {'categories': ['#3d', '#training', '#optimization', '#long_context'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ»ĞµÑ‚Ñƒ Ğ´Ğ»Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TTT3R â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ test-time training Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½ĞµĞµ Ñ‚ĞµÑ…, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ¸ÑÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ñƒ Ğ´Ğ»Ñ learning rate. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ â€” Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¿Ñ€Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ 20 FPS Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 6 Ğ“Ğ‘ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚Ñ‹ÑÑÑ‡ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'TTT3R: Boosting 3D Reconstruction with Smart Memory Updates', 'desc': "The paper introduces TTT3R, a novel approach that enhances length generalization in 3D reconstruction tasks. It addresses the limitations of Recurrent Neural Networks (RNNs) when they encounter input sequences longer than those seen during training. By applying Test-Time Training, TTT3R dynamically adjusts memory updates based on the confidence of alignment between the model's memory and new observations. This method significantly improves global pose estimation and processing efficiency, achieving a twofold increase in performance while maintaining a fast processing speed."}, 'zh': {'title': 'TTT3Rï¼šæå‡3Dé‡å»ºé•¿åº¦æ³›åŒ–çš„åˆ›æ–°æ–¹æ³•', 'desc': 'TTT3Ræ˜¯ä¸€ç§æµ‹è¯•æ—¶è®­ç»ƒçš„å¹²é¢„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ ¹æ®å¯¹é½ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´è®°å¿†æ›´æ–°ï¼Œå¢å¼º3Dé‡å»ºä¸­çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†3Dé‡å»ºè§†ä¸ºåœ¨çº¿å­¦ä¹ é—®é¢˜ï¼Œåˆ©ç”¨è®°å¿†çŠ¶æ€ä¸æ–°è§‚å¯Ÿä¹‹é—´çš„å¯¹é½ç½®ä¿¡åº¦æ¥æ¨å¯¼é—­å¼å­¦ä¹ ç‡ï¼Œä»è€Œåœ¨ä¿ç•™å†å²ä¿¡æ¯ä¸é€‚åº”æ–°è§‚å¯Ÿä¹‹é—´å–å¾—å¹³è¡¡ã€‚é€šè¿‡è¿™ç§è®­ç»ƒæ— å…³çš„å¹²é¢„ï¼ŒTTT3Ræ˜¾è‘—æé«˜äº†é•¿åº¦æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å…¨å±€å§¿æ€ä¼°è®¡ä¸Šå®ç°äº†2å€çš„æå‡ï¼ŒåŒæ—¶ä»¥20å¸§æ¯ç§’çš„é€Ÿåº¦å¤„ç†æ•°åƒå¼ å›¾åƒï¼Œä»…éœ€6GBçš„GPUå†…å­˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26539', 'title': 'Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents', 'url': 'https://huggingface.co/papers/2509.26539', 'abstract': 'Ferret-UI Lite, a compact end-to-end GUI agent, achieves competitive performance across diverse platforms using chain-of-thought reasoning, visual tool-use, and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale GUI agents. In GUI grounding, Ferret-UI Lite attains scores of 91.6%, 53.3%, and 61.2% on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of 28.0% on AndroidWorld and 19.8% on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents.', 'score': 4, 'issue_id': 6175, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '4f42c990da9b21fe', 'authors': ['Zhen Yang', 'Zi-Yi Dou', 'Di Feng', 'Forrest Huang', 'Anh Nguyen', 'Keen You', 'Omar Attia', 'Yuhao Yang', 'Michael Feng', 'Haotian Zhang', 'Ram Ramrakhya', 'Chao Jia', 'Jeffrey Nichols', 'Alexander Toshev', 'Yinfei Yang', 'Zhe Gan'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2509.26539.jpg', 'data': {'categories': ['#inference', '#agents', '#reasoning', '#small_models', '#synthetic', '#data', '#dataset', '#rl'], 'emoji': 'ğŸ“±', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ', 'desc': 'Ferret-UI Lite â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 3B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ…, Ğ²ĞµĞ± Ğ¸ Ğ´ĞµÑĞºÑ‚Ğ¾Ğ¿ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ chain-of-thought Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ reinforcement learning ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 91.6% Ğ½Ğ° ScreenSpot-V2, Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ 28% Ğ½Ğ° AndroidWorld. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… GUI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Compact GUI Agent with Competitive Performance', 'desc': 'Ferret-UI Lite is a small, end-to-end agent designed to interact with Graphic User Interfaces (GUIs) across various platforms like mobile and desktop. It employs chain-of-thought reasoning and visual tool-use to enhance its performance, making it effective even with limited resources. The agent is trained using a mix of real and synthetic GUI data, and it utilizes reinforcement learning to optimize its actions based on specific rewards. Overall, Ferret-UI Lite demonstrates competitive results compared to other small-scale GUI agents, showcasing its potential for on-device applications.'}, 'zh': {'title': 'ç´§å‡‘é«˜æ•ˆçš„GUIä»£ç†ï¼šFerret-UI Lite', 'desc': 'Ferret-UI Lite æ˜¯ä¸€ç§ç´§å‡‘çš„ç«¯åˆ°ç«¯å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ï¼Œèƒ½å¤Ÿåœ¨å¤šç§å¹³å°ä¸Šå®ç°ç«äº‰åŠ›çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†é“¾å¼æ€ç»´æ¨ç†ã€è§†è§‰å·¥å…·ä½¿ç”¨å’Œå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯ï¼Œä¸“ä¸ºå°å‹è®¾å¤‡ä¼˜åŒ–ã€‚é€šè¿‡ä»çœŸå®å’Œåˆæˆæ¥æºä¸­ç­–åˆ’å¤šæ ·åŒ–çš„GUIæ•°æ®ï¼ŒFerret-UI Lite åœ¨æ¨ç†æ—¶çš„è¡¨ç°å¾—åˆ°äº†å¢å¼ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFerret-UI Lite åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸç‡ä¸å…¶ä»–å°å‹GUIä»£ç†ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23166', 'title': 'Test-Time Policy Adaptation for Enhanced Multi-Turn Interactions with\n  LLMs', 'url': 'https://huggingface.co/papers/2509.23166', 'abstract': 'ROSA, a lightweight algorithm, enhances multi-turn interactions in LLMs by adapting to user feedback in real-time, improving both task effectiveness and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) employ multi-turn interaction as a fundamental paradigm for completing complex tasks. However, their performance often degrades in extended interactions, as they are typically trained on static, single-turn data, which hinders their ability to adapt to real-time user feedback. To address this limitation, we first propose a new paradigm: Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which utilizes user feedback from the ongoing interaction as a reward signal to estimate a latent optimal policy aligned with user preferences, then updates a small subset of parameters to steer the model toward this policy, ultimately enabling efficient in-conversation self-correction. We then introduce Optimum-Referenced One-Step Adaptation (ROSA), a lightweight algorithm that operationalizes T2PAM. ROSA guides the model parameters toward a theoretical optimal policy in a single, efficient update step, avoiding costly iterative gradient-based optimization and minimizing computational overhead. We provide a rigorous theoretical analysis guaranteeing that the policy of ROSA converges to the preference of user as the number of interactions increases. Extensive experiments on challenging benchmark demonstrate that ROSA achieves significant improvements in both task effectiveness and efficiency.', 'score': 4, 'issue_id': 6178, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': '08ae55e74db47a69', 'authors': ['Chenxing Wei', 'Hong Wang', 'Ying He', 'Fei Yu', 'Yao Shu'], 'affiliations': ['College of Computer Science and Software Engineering, Shenzhen University, China', 'Guangdong Lab of AI and Digital Economy (SZ), China', 'Hong Kong University of Science and Technology (Guangzhou), China', 'School of Information Technology, Carleton University, Canada', 'University of Science and Technology of China, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.23166.jpg', 'data': {'categories': ['#alignment', '#training', '#benchmark', '#rlhf', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ AI Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ROSA â€” Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ LLM. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğº Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ROSA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ğ¸Ğ´Ğ±ĞµĞº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° ROSA ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğº Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¿Ğ¾ Ğ¼ĞµÑ€Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‡Ğ¸ÑĞ»Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ° ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'ROSA: Real-Time Adaptation for Enhanced Multi-Turn Interactions', 'desc': "This paper introduces ROSA, a lightweight algorithm designed to improve multi-turn interactions in Large Language Models (LLMs) by incorporating real-time user feedback. It addresses the challenge of LLMs degrading in performance during extended interactions due to their training on static data. The authors propose a new approach called Test-Time Policy Adaptation for Multi-Turn Interactions (T2PAM), which uses user feedback as a reward signal to adjust the model's parameters towards an optimal policy. ROSA operationalizes this approach with a single-step update, ensuring efficient adaptation while minimizing computational costs and enhancing task effectiveness."}, 'zh': {'title': 'å®æ—¶åé¦ˆï¼Œæå‡å¤šè½®äº¤äº’çš„æ™ºèƒ½ç®—æ³•', 'desc': 'ROSAæ˜¯ä¸€ç§è½»é‡çº§ç®—æ³•ï¼Œæ—¨åœ¨é€šè¿‡å®æ—¶é€‚åº”ç”¨æˆ·åé¦ˆæ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„å¤šè½®äº¤äº’ã€‚ä¼ ç»Ÿçš„LLMsåœ¨å¤šè½®äº¤äº’ä¸­è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸åŸºäºé™æ€çš„å•è½®æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹å®æ—¶åé¦ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼ï¼šå¤šè½®äº¤äº’çš„æµ‹è¯•æ—¶ç­–ç•¥é€‚åº”ï¼ˆT2PAMï¼‰ï¼Œåˆ©ç”¨ç”¨æˆ·åé¦ˆä½œä¸ºå¥–åŠ±ä¿¡å·æ¥ä¼°è®¡ä¸ç”¨æˆ·åå¥½ä¸€è‡´çš„æ½œåœ¨æœ€ä¼˜ç­–ç•¥ã€‚ROSAç®—æ³•é€šè¿‡ä¸€æ¬¡é«˜æ•ˆçš„æ›´æ–°æ­¥éª¤å¼•å¯¼æ¨¡å‹å‚æ•°æœå‘ç†è®ºæœ€ä¼˜ç­–ç•¥ï¼Œä»è€Œæé«˜ä»»åŠ¡çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25716', 'title': 'DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation', 'url': 'https://huggingface.co/papers/2509.25716', 'abstract': 'A novel technique for predicting APIs and generating code in real-time using a compact reranker outperforms larger models with reduced latency, addressing API leaks and unclear usage intent in enterprise code.  \t\t\t\t\tAI-generated summary \t\t\t\t Current search techniques are limited to standard RAG query-document applications. In this paper, we propose a novel technique to expand the code and index for predicting the required APIs, directly enabling high-quality, end-to-end code generation for auto-completion and agentic AI applications. We address the problem of API leaks in current code-to-code benchmark datasets by introducing a new dataset built from real-world ServiceNow Script Includes that capture the challenge of unclear API usage intent in the code. Our evaluation metrics show that this method achieves 87.86% top-40 retrieval accuracy, allowing the critical context with APIs needed for successful downstream code generation. To enable real-time predictions, we develop a comprehensive post-training pipeline that optimizes a compact 0.6B reranker through synthetic dataset generation, supervised fine-tuning, and reinforcement learning. This approach enables our compact reranker to outperform a much larger 8B model while maintaining 2.5x reduced latency, effectively addressing the nuances of enterprise-specific code without the computational overhead of larger models.', 'score': 3, 'issue_id': 6189, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '1cecea53e2439864', 'authors': ['Esakkivel Esakkiraja', 'Denis Akhiyarov', 'Aditya Shanmugham', 'Chitra Ganapathy'], 'affiliations': ['ServiceNow, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2509.25716.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#leakage', '#dataset', '#synthetic', '#rl', '#rag'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ reranker Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ API Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ API Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ API Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° ServiceNow. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ reranker Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 0.6B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 87.86% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° top-40 Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, supervised fine-tuning Ğ¸ reinforcement learning. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½ÑƒÑ 8B Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ² 2.5 Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ enterprise-Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Real-Time API Prediction with Compact Reranker', 'desc': 'This paper presents a new method for predicting application programming interfaces (APIs) and generating code in real-time using a compact reranker model. The technique improves upon existing methods by addressing issues like API leaks and ambiguous usage intent in enterprise code, utilizing a specially created dataset from real-world examples. The proposed model achieves high retrieval accuracy and significantly reduces latency compared to larger models, making it suitable for real-time applications. By employing a post-training pipeline that includes synthetic data generation and reinforcement learning, the compact reranker demonstrates superior performance while being computationally efficient.'}, 'zh': {'title': 'å®æ—¶APIé¢„æµ‹ä¸ä»£ç ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æŠ€æœ¯ï¼Œç”¨äºå®æ—¶é¢„æµ‹APIå¹¶ç”Ÿæˆä»£ç ï¼Œä½¿ç”¨ç´§å‡‘çš„é‡æ’åºå™¨ï¼Œå…¶æ€§èƒ½ä¼˜äºæ›´å¤§çš„æ¨¡å‹ä¸”å»¶è¿Ÿæ›´ä½ã€‚æˆ‘ä»¬é€šè¿‡æ„å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œè§£å†³äº†å½“å‰ä»£ç åŸºå‡†æ•°æ®é›†ä¸­APIæ³„æ¼å’Œä¸æ˜ç¡®ä½¿ç”¨æ„å›¾çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨è¯„ä¼°ä¸­æ˜¾ç¤ºå‡º87.86%çš„å‰40ä¸ªæ£€ç´¢å‡†ç¡®ç‡ï¼Œèƒ½å¤Ÿæä¾›æˆåŠŸç”Ÿæˆä»£ç æ‰€éœ€çš„APIä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå…¨é¢çš„åè®­ç»ƒç®¡é“ï¼Œé€šè¿‡åˆæˆæ•°æ®é›†ç”Ÿæˆã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–ç´§å‡‘çš„é‡æ’åºå™¨ï¼Œä»è€Œåœ¨ä¸å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹å®ç°å®æ—¶é¢„æµ‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26329', 'title': 'TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics', 'url': 'https://huggingface.co/papers/2509.26329', 'abstract': 'TAU, a benchmark of culturally specific Taiwanese soundmarks, reveals that state-of-the-art large audio-language models underperform compared to local humans, highlighting the need for localized evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large audio-language models are advancing rapidly, yet most evaluations emphasize speech or globally sourced sounds, overlooking culturally distinctive cues. This gap raises a critical question: can current models generalize to localized, non-semantic audio that communities instantly recognize but outsiders do not? To address this, we present TAU (Taiwan Audio Understanding), a benchmark of everyday Taiwanese "soundmarks." TAU is built through a pipeline combining curated sources, human editing, and LLM-assisted question generation, producing 702 clips and 1,794 multiple-choice items that cannot be solved by transcripts alone. Experiments show that state-of-the-art LALMs, including Gemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates the need for localized benchmarks to reveal cultural blind spots, guide more equitable multimodal evaluation, and ensure models serve communities beyond the global mainstream.', 'score': 2, 'issue_id': 6176, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'f898fff4a933cddb', 'authors': ['Yi-Cheng Lin', 'Yu-Hua Chen', 'Jia-Kai Dong', 'Yueh-Hsuan Huang', 'Szu-Chi Chen', 'Yu-Chen Chen', 'Chih-Yao Chen', 'Yu-Jung Lin', 'Yu-Ling Chen', 'Zih-Yu Chen', 'I-Ning Tsai', 'Hsiu-Hsuan Wang', 'Ho-Lam Chung', 'Ke-Han Lu', 'Hung-yi Lee'], 'affiliations': ['National Taiwan University', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2509.26329.jpg', 'data': {'categories': ['#alignment', '#ethics', '#audio', '#benchmark', '#multimodal'], 'emoji': 'ğŸ””', 'ru': {'title': 'ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¸ ĞºĞ°Ğº Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-AI: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑĞ»Ñ‹ÑˆĞ°Ñ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TAU Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¸ Ğ¢Ğ°Ğ¹Ğ²Ğ°Ğ½Ñ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 702 Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ¸Ğ¿Ğ° Ğ¸ 1794 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LALM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Gemini 2.5 Ğ¸ Qwen2-Audio, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ¶Ğµ Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¶Ğ¸Ñ‚ĞµĞ»ĞµĞ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¿Ñ‹Ñ… Ğ·Ğ¾Ğ½ AI-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ñ€Ğ°Ğ²ĞµĞ´Ğ»Ğ¸Ğ²Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Bridging the Gap: Localized Audio Understanding with TAU', 'desc': "The paper introduces TAU, a benchmark designed to evaluate audio-language models using culturally specific sounds from Taiwan, known as 'soundmarks.' It highlights that current state-of-the-art large audio-language models (LALMs) struggle to recognize these localized audio cues, performing significantly worse than local human listeners. The study emphasizes the importance of localized evaluations, as existing benchmarks often focus on globally sourced sounds, neglecting unique cultural audio. By showcasing the limitations of LALMs in understanding culturally distinctive sounds, TAU aims to promote more equitable and relevant multimodal evaluations."}, 'zh': {'title': 'æœ¬åœ°åŒ–è¯„ä¼°ï¼Œæå‡éŸ³é¢‘ç†è§£èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†TAUï¼ˆå°æ¹¾éŸ³é¢‘ç†è§£ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å°æ¹¾ç‰¹æœ‰å£°éŸ³æ ‡è®°çš„åŸºå‡†æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰æœ€å…ˆè¿›çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è¿™äº›åœ°æ–¹æ€§éŸ³é¢‘æ—¶ï¼Œè¡¨ç°è¿œä½äºå½“åœ°äººã€‚æ­¤ç ”ç©¶å¼ºè°ƒäº†å¯¹æœ¬åœ°åŒ–è¯„ä¼°çš„éœ€æ±‚ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£å’ŒæœåŠ¡äºç‰¹å®šæ–‡åŒ–çš„ç¤¾åŒºã€‚é€šè¿‡ç»“åˆç­–åˆ’æ¥æºå’Œäººç±»ç¼–è¾‘ï¼ŒTAUæä¾›äº†702ä¸ªéŸ³é¢‘ç‰‡æ®µå’Œ1794ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨å¤„ç†éè¯­ä¹‰éŸ³é¢‘æ—¶çš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26157', 'title': 'EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series\n  Forecasting', 'url': 'https://huggingface.co/papers/2509.26157', 'abstract': 'EntroPE, a temporally informed framework using entropy-guided dynamic patching, enhances time series forecasting by preserving temporal coherence and improving accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), a novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by a global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as a promising new paradigm for time series modeling. Code is available at: https://github.com/Sachithx/EntroPE.', 'score': 2, 'issue_id': 6184, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '3341e033dcf26e80', 'authors': ['Sachith Abeywickrama', 'Emadeldeen Eldele', 'Min Wu', 'Xiaoli Li', 'Chau Yuen'], 'affiliations': ['Department of Computer Science, Khalifa University, UAE', 'Information Systems Technology and Design, Singapore University of Technology and Design, Singapore', 'Institute for Infocomm Research, A*STAR, Singapore', 'School of Electrical and Electronics Engineering, Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.26157.jpg', 'data': {'categories': ['#training', '#long_context', '#benchmark', '#optimization', '#data', '#architecture'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ğ¿Ğ¾ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EntroPE â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ°Ñ‚Ñ‡Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¿Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ² Ñ‚Ğ¾Ñ‡ĞºĞ°Ñ… ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ÑƒÑˆĞ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Time Series Forecasting with Dynamic Patching', 'desc': 'EntroPE is a new framework designed to improve time series forecasting by using a method called entropy-guided dynamic patching. This approach helps maintain the natural flow of time in data, which is often disrupted by traditional methods that cut data into fixed segments. By identifying key transition points in the data, EntroPE ensures that important short-term relationships are preserved, leading to better learning and predictions. The framework includes two main components: one that finds where to cut the data and another that processes these segments to capture important patterns, resulting in enhanced accuracy and efficiency in forecasting tasks.'}, 'zh': {'title': 'ç†µå¼•å¯¼çš„åŠ¨æ€è¡¥ä¸ç¼–ç ï¼Œæå‡æ—¶é—´åºåˆ—é¢„æµ‹', 'desc': 'EntroPEæ˜¯ä¸€ç§åŸºäºç†µå¼•å¯¼çš„åŠ¨æ€è¡¥ä¸ç¼–ç å™¨ï¼Œæ—¨åœ¨æé«˜æ—¶é—´åºåˆ—é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®ƒé€šè¿‡åŠ¨æ€æ£€æµ‹æ—¶é—´è½¬å˜ç‚¹ï¼Œä¿æŒæ—¶é—´ç»“æ„çš„ä¸€è‡´æ€§ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•ä¸­æ—¶é—´æ— å…³çš„è¡¥ä¸æ„å»ºå¸¦æ¥çš„é—®é¢˜ã€‚EntroPEåŒ…å«ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šç†µåŸºåŠ¨æ€è¡¥ä¸å™¨å’Œè‡ªé€‚åº”è¡¥ä¸ç¼–ç å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è¡¥ä¸å†…çš„ä¾èµ–å…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEntroPEåœ¨é•¿æœŸé¢„æµ‹åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†é¢„æµ‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25085', 'title': 'jina-reranker-v3: Last but Not Late Interaction for Document Reranking', 'url': 'https://huggingface.co/papers/2509.25085', 'abstract': 'A multilingual document reranker using causal self-attention achieves state-of-the-art performance with a compact architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t jina-reranker-v3 is a 0.6B parameter multilingual document reranker that introduces a novel last but not late interaction. Unlike late interaction models such as ColBERT that perform separate encoding followed by multi-vector matching, our approach conducts causal self-attention between query and documents within the same context window, enabling rich cross-document interactions before extracting contextual embeddings from the last token of each document. This compact architecture achieves state-of-the-art BEIR performance with 61.94 nDCG@10 while being ten times smaller than generative listwise rerankers.', 'score': 2, 'issue_id': 6184, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '3f9f7f40e445d045', 'authors': ['Feng Wang', 'Yuqing Li', 'Han Xiao'], 'affiliations': ['Jina AI GmbH', 'University of Pittsburgh'], 'pdf_title_img': 'assets/pdf/title_img/2509.25085.jpg', 'data': {'categories': ['#machine_translation', '#multilingual', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ñ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‘ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ğ³Ñ€Ğ¾Ğ¼Ğ¾Ğ·Ğ´ĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ jina-reranker-v3 Ñ 0.6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ self-attention Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼ Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¼ Ğ¾ĞºĞ½Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ´ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ° Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ "last but not late interaction" Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° ColBERT, Ğ³Ğ´Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° 61.94 nDCG@10 Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BEIR, Ğ±ÑƒĞ´ÑƒÑ‡Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… listwise Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ².'}, 'en': {'title': 'Compact Multilingual Reranking with Causal Self-Attention', 'desc': 'This paper presents a multilingual document reranker called jina-reranker-v3, which utilizes a compact architecture with only 0.6 billion parameters. It introduces a novel approach known as last but not late interaction, which allows for causal self-attention between the query and documents within the same context window. This method enables more effective cross-document interactions before generating contextual embeddings from the last token of each document. As a result, the model achieves state-of-the-art performance on the BEIR benchmark with an nDCG@10 score of 61.94, while being significantly smaller than traditional generative listwise rerankers.'}, 'zh': {'title': 'å› æœè‡ªæ³¨æ„åŠ›ï¼šå¤šè¯­è¨€æ–‡æ¡£é‡æ’åºçš„æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§å¤šè¯­è¨€æ–‡æ¡£é‡æ’åºæ¨¡å‹ï¼Œåä¸ºjina-reranker-v3ï¼Œå…·æœ‰0.6äº¿å‚æ•°ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´è¿›è¡Œäº¤äº’ï¼Œå…è®¸åœ¨åŒä¸€ä¸Šä¸‹æ–‡çª—å£å†…è¿›è¡Œä¸°å¯Œçš„è·¨æ–‡æ¡£äº¤äº’ã€‚ä¸ä¼ ç»Ÿçš„æ™šæœŸäº¤äº’æ¨¡å‹ä¸åŒï¼Œå®ƒåœ¨æå–æ¯ä¸ªæ–‡æ¡£æœ€åä¸€ä¸ªtokençš„ä¸Šä¸‹æ–‡åµŒå…¥ä¹‹å‰ï¼Œå…ˆè¿›è¡Œäº¤äº’ã€‚è¯¥ç´§å‡‘çš„æ¶æ„åœ¨BEIRåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†61.94çš„nDCG@10ï¼Œä¸”ä½“ç§¯æ¯”ç”Ÿæˆå¼åˆ—è¡¨é‡æ’åºå™¨å°åå€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23773', 'title': 'Knowledge Homophily in Large Language Models', 'url': 'https://huggingface.co/papers/2509.23773', 'abstract': 'Graph Neural Network regression models estimate entity-level knowledgeability in Large Language Models to improve active labeling and multi-hop reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.', 'score': 2, 'issue_id': 6176, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 28', 'zh': '9æœˆ28æ—¥'}, 'hash': '9842b9fef9ea6d7d', 'authors': ['Utkarsh Sahu', 'Zhisheng Qi', 'Mahantesh Halappanavar', 'Nedim Lipka', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Yu Zhang', 'Yao Ma', 'Yu Wang'], 'affiliations': ['Adobe Research', 'Pacific Northwest National Laboratory', 'Rensselaer Polytechnic Institute', 'Texas A&M University', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2509.23773.jpg', 'data': {'categories': ['#data', '#reasoning', '#training', '#dataset', '#agents', '#graphs', '#multimodal'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ“Ñ€Ğ°Ñ„ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ LLM: ÑĞ¾ÑĞµĞ´Ğ¸ Ğ·Ğ½Ğ°ÑÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ² Ğ¸Ñ… Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ³Ñ€Ğ°Ñ„Ğ°. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ³Ğ¾Ğ¼Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ ÑÑ…Ğ¾Ğ¶Ğ¸Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ² Ğ³Ñ€Ğ°Ñ„Ğµ, Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°Ğ¼ Ğ² ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°ÑƒĞºĞµ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Graph Neural Network Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ„Ğ°ĞºÑ‚Ğ°Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… ÑƒĞ·Ğ»Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ… Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹.'}, 'en': {'title': 'Enhancing Knowledgeability in LLMs through Graph Neural Networks', 'desc': 'This paper explores how Large Language Models (LLMs) can be represented as graphs to better understand their knowledge structure. It introduces a Graph Neural Network (GNN) regression model that estimates the knowledgeability of entities based on their relationships with neighboring entities in the graph. By identifying knowledge homophily, where similar knowledge levels are found among closely related entities, the model helps prioritize which facts to verify for efficient labeling. This approach enhances the active labeling process and improves multi-hop reasoning in applications like question answering.'}, 'zh': {'title': 'åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œæå‡çŸ¥è¯†è¯„ä¼°ä¸æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å›¾ç¥ç»ç½‘ç»œå›å½’æ¨¡å‹æ¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å®ä½“çš„çŸ¥è¯†æ°´å¹³ï¼Œä»¥æé«˜ä¸»åŠ¨æ ‡æ³¨å’Œå¤šè·³æ¨ç†çš„æ•ˆæœã€‚ç ”ç©¶å‘ç°ï¼ŒLLMsçš„çŸ¥è¯†åœ¨å›¾ç»“æ„ä¸­å‘ˆç°å‡ºç›¸ä¼¼æ€§ï¼Œå³ç›¸é‚»å®ä½“çš„çŸ¥è¯†æ°´å¹³å¾€å¾€ç›¸ä¼¼ã€‚é€šè¿‡å°†LLMsçš„çŸ¥è¯†æ˜ å°„ä¸ºå›¾è¡¨ç¤ºï¼Œæœ¬æ–‡åˆ†æäº†å®ä½“ä¸å…¶é‚»å±…ä¹‹é—´çš„çŸ¥è¯†å…³ç³»ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºé‚»åŸŸè¯„åˆ†çš„çŸ¥è¯†æ°´å¹³ä¼°è®¡æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†ä¸»åŠ¨æ ‡æ³¨çš„æ•ˆç‡ï¼Œè¿˜å¢å¼ºäº†åœ¨æ¨ç†å¯†é›†å‹é—®ç­”ä¸­çš„å¤šè·³è·¯å¾„æ£€ç´¢èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23094', 'title': 'd^2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching', 'url': 'https://huggingface.co/papers/2509.23094', 'abstract': 'Dual aDaptive Cache (dÂ²Cache) accelerates diffusion-based large language model inference by selectively updating key-value states and enabling quasi left-to-right generation, improving both speed and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce Dual aDaptive Cache (d^2Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d^2Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d^2Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d^2Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.', 'score': 2, 'issue_id': 6184, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': '2379e9c6764ecd0f', 'authors': ['Yuchu Jiang', 'Yue Cai', 'Xiangzhong Luo', 'Jiale Fu', 'Jiarui Wang', 'Chonghan Liu', 'Xu Yang'], 'affiliations': ['Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education', 'Qiyuan Tech', 'Southeast University'], 'pdf_title_img': 'assets/pdf/title_img/2509.23094.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#diffusion', '#architecture'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ dÂ²Cache â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ LLM Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ KV-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡-Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºÑÑˆ Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ ĞºĞ²Ğ°Ğ·Ğ¸-ÑĞ»ĞµĞ²Ğ°-Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLaDA Ğ¸ Dream Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Accelerating dLLM Inference with Dual aDaptive Cache', 'desc': 'The paper introduces Dual aDaptive Cache (dÂ²Cache), a novel framework designed to enhance the efficiency of diffusion-based large language models (dLLMs) during inference. Unlike autoregressive models, dLLMs face challenges due to their bidirectional attention mechanism, which limits the effectiveness of traditional key-value (KV) caching. dÂ²Cache employs a two-stage selection strategy to selectively update KV states for certain tokens while caching others, allowing for faster and more reliable text generation. Experimental results show that dÂ²Cache significantly accelerates inference speed and improves the quality of generated text in models like LLaDA and Dream.'}, 'zh': {'title': 'åŒè‡ªé€‚åº”ç¼“å­˜ï¼šåŠ é€Ÿæ¨ç†ä¸æå‡è´¨é‡çš„åˆ©å™¨', 'desc': 'åŒè‡ªé€‚åº”ç¼“å­˜ï¼ˆdÂ²Cacheï¼‰é€šè¿‡é€‰æ‹©æ€§æ›´æ–°é”®å€¼çŠ¶æ€å’Œå®ç°å‡†å·¦åˆ°å³ç”Ÿæˆï¼ŒåŠ é€Ÿäº†åŸºäºæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ï¼Œæé«˜äº†é€Ÿåº¦å’Œè´¨é‡ã€‚åŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ•ˆç‡ä¸Šå­˜åœ¨ä¸è¶³ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–åŒå‘æ³¨æ„åŠ›ï¼Œæ— æ³•åƒè‡ªå›å½’æ¨¡å‹é‚£æ ·ç›´æ¥åˆ©ç”¨æ ‡å‡†çš„é”®å€¼ç¼“å­˜ã€‚dÂ²Cacheå¼•å…¥äº†ä¸€ç§æ— è®­ç»ƒçš„è¿‘ä¼¼é”®å€¼ç¼“å­˜æ¡†æ¶ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µçš„ç»†ç²’åº¦é€‰æ‹©ç­–ç•¥ï¼Œåœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­è¯†åˆ«ä»¤ç‰Œå¹¶è‡ªé€‚åº”æ›´æ–°å…¶é”®å€¼çŠ¶æ€ï¼ŒåŒæ—¶ç¼“å­˜å…¶ä½™ä»¤ç‰Œçš„é”®å€¼çŠ¶æ€ä»¥ä¾›é‡ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒdÂ²Cacheä¸ä»…æ˜¾è‘—åŠ å¿«äº†æ¨ç†é€Ÿåº¦ï¼Œè¿˜åœ¨ç”Ÿæˆè´¨é‡ä¸Šå–å¾—äº†ä¸€è‡´çš„æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25248', 'title': 'BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source\n  Software', 'url': 'https://huggingface.co/papers/2509.25248', 'abstract': "A new benchmark, BUILD-BENCH, and an LLM-based agent, OSS-BUILD-AGENT, address the complexities of compiling diverse open-source software projects.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatically compiling open-source software (OSS) projects is a vital, labor-intensive, and complex task, which makes it a good challenge for LLM Agents. Existing methods rely on manually curated rules and workflows, which cannot adapt to OSS that requires customized configuration or environment setup. Recent attempts using Large Language Models (LLMs) used selective evaluation on a subset of highly rated OSS, a practice that underestimates the realistic challenges of OSS compilation. In practice, compilation instructions are often absent, dependencies are undocumented, and successful builds may even require patching source files or modifying build scripts. We propose a more challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more diverse in quality, scale, and characteristics. Furthermore, we propose a strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with enhanced build instruction retrieval module that achieves state-of-the-art performance on BUILD-BENCH and is adaptable to heterogeneous OSS characteristics. We also provide detailed analysis regarding different compilation method design choices and their influence to the whole task, offering insights to guide future advances. We believe performance on BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as a complex software engineering tasks, and, as such, our benchmark will spur innovation with a significant impact on downstream applications in the fields of software development and software security.", 'score': 2, 'issue_id': 6183, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': '951d4237714a02d7', 'authors': ['Zehua Zhang', 'Ati Priya Bajaj', 'Divij Handa', 'Siyu Liu', 'Arvind S Raj', 'Hongkai Chen', 'Hulin Wang', 'Yibo Liu', 'Zion Leonahenahe Basque', 'Souradip Nath', 'Vishal Juneja', 'Nikhil Chapre', 'Yan Shoshitaishvili', 'Adam DoupÃ©', 'Chitta Baral', 'Ruoyu Wang'], 'affiliations': ['School of Computing and Augmented Intelligence, Arizona State University, Tempe, AZ 85281, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.25248.jpg', 'data': {'categories': ['#agents', '#benchmark', '#open_source', '#security'], 'emoji': 'ğŸ”¨', 'ru': {'title': 'LLM-Ğ°Ğ³ĞµĞ½Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ open-source Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BUILD-BENCH Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ open-source Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ñ€ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ñ… Ğ¸ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸Ğ½Ğ³Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ OSS-BUILD-AGENT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑĞ±Ğ¾Ñ€ĞºĞµ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ software engineering.'}, 'en': {'title': 'Revolutionizing OSS Compilation with BUILD-BENCH and OSS-BUILD-AGENT', 'desc': 'The paper introduces BUILD-BENCH, a new benchmark designed to evaluate the performance of agents in compiling diverse open-source software (OSS) projects. It highlights the limitations of existing methods that rely on fixed rules and workflows, which fail to adapt to the unique requirements of various OSS. The authors present OSS-BUILD-AGENT, a large language model (LLM)-based agent that excels in retrieving build instructions and demonstrates superior performance on the BUILD-BENCH. This work aims to enhance the understanding of compilation challenges in software engineering and promote advancements in software development and security.'}, 'zh': {'title': 'åº”å¯¹å¼€æºè½¯ä»¶ç¼–è¯‘çš„æŒ‘æˆ˜æ–°åŸºå‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•BUILD-BENCHå’Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†OSS-BUILD-AGENTï¼Œæ—¨åœ¨è§£å†³ç¼–è¯‘å¤šæ ·åŒ–å¼€æºè½¯ä»¶é¡¹ç›®çš„å¤æ‚æ€§ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨ç¼–åˆ¶çš„è§„åˆ™å’Œå·¥ä½œæµç¨‹ï¼Œæ— æ³•é€‚åº”éœ€è¦å®šåˆ¶é…ç½®æˆ–ç¯å¢ƒè®¾ç½®çš„å¼€æºè½¯ä»¶ã€‚æˆ‘ä»¬æå‡ºçš„BUILD-BENCHåŸºå‡†æµ‹è¯•åŒ…å«äº†è´¨é‡ã€è§„æ¨¡å’Œç‰¹å¾æ›´ä¸ºå¤šæ ·çš„å¼€æºè½¯ä»¶ï¼Œæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§å’Œç°å®æ€§çš„è¯„ä¼°ã€‚OSS-BUILD-AGENTåˆ™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ä»£ç†ï¼Œå…·å¤‡å¢å¼ºçš„æ„å»ºæŒ‡ä»¤æ£€ç´¢æ¨¡å—ï¼Œåœ¨BUILD-BENCHä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„å¼€æºè½¯ä»¶ç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21361', 'title': 'Context Is What You Need: The Maximum Effective Context Window for Real\n  World Limits of LLMs', 'url': 'https://huggingface.co/papers/2509.21361', 'abstract': "Research reveals significant discrepancies between reported and effective context window sizes in large language models, impacting accuracy and hallucination rates across different problem types.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM) providers boast big numbers for maximum context window sizes. To test the real world use of context windows, we 1) define a concept of maximum effective context window, 2) formulate a testing method of a context window's effectiveness over various sizes and problem types, and 3) create a standardized way to compare model efficacy for increasingly larger context window sizes to find the point of failure. We collected hundreds of thousands of data points across several models and found significant differences between reported Maximum Context Window (MCW) size and Maximum Effective Context Window (MECW) size. Our findings show that the MECW is, not only, drastically different from the MCW but also shifts based on the problem type. A few top of the line models in our test group failed with as little as 100 tokens in context; most had severe degradation in accuracy by 1000 tokens in context. All models fell far short of their Maximum Context Window by as much as 99 percent. Our data reveals the Maximum Effective Context Window shifts based on the type of problem provided, offering clear and actionable insights into how to improve model accuracy and decrease model hallucination rates.", 'score': 2, 'issue_id': 6186, 'pub_date': '2025-09-21', 'pub_date_card': {'ru': '21 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 21', 'zh': '9æœˆ21æ—¥'}, 'hash': '4a71ea7177e7bc3a', 'authors': ['Norman Paulsen'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2509.21361.jpg', 'data': {'categories': ['#long_context', '#training', '#hallucinations', '#benchmark', '#data'], 'emoji': 'ğŸ“‰', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ LLM: Ğ¾Ğ±ĞµÑ‰Ğ°Ğ½Ğ¸Ñ vs Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° (MECW) Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ĞµĞ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‚ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ¶Ğµ Ğ¿Ñ€Ğ¸ 100 Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ… Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğº 1000 Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼. Ğ’ÑĞµ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ°, Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ñ Ğ´Ğ¾ 99%, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ‚Ğ¸Ğ¿Ğ° Ñ€ĞµÑˆĞ°ĞµĞ¼Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Unveiling the True Limits of Context in Language Models', 'desc': 'This paper investigates the difference between the reported maximum context window sizes and the actual effective context window sizes in large language models (LLMs). It introduces a new concept called Maximum Effective Context Window (MECW) and presents a method to evaluate its effectiveness across various problem types. The research shows that many models perform poorly with context sizes much smaller than their claimed maximums, leading to significant accuracy issues and increased hallucination rates. The findings suggest that understanding the MECW can help improve model performance and reliability in real-world applications.'}, 'zh': {'title': 'æ­ç¤ºä¸Šä¸‹æ–‡çª—å£çš„çœŸå®æœ‰æ•ˆæ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æŠ¥å‘Šçš„æœ€å¤§ä¸Šä¸‹æ–‡çª—å£å¤§å°ä¸å®é™…æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£å¤§å°ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚è¿™äº›å·®å¼‚å½±å“äº†æ¨¡å‹åœ¨ä¸åŒé—®é¢˜ç±»å‹ä¸Šçš„å‡†ç¡®æ€§å’Œå¹»è§‰ç‡ã€‚ç ”ç©¶è€…å®šä¹‰äº†æœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£çš„æ¦‚å¿µï¼Œå¹¶æå‡ºäº†ä¸€ç§æµ‹è¯•æ–¹æ³•æ¥è¯„ä¼°ä¸Šä¸‹æ–‡çª—å£åœ¨ä¸åŒå¤§å°å’Œé—®é¢˜ç±»å‹ä¸‹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ€å¤§æœ‰æ•ˆä¸Šä¸‹æ–‡çª—å£ä¸æŠ¥å‘Šçš„æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¹¶ä¸”æ ¹æ®é—®é¢˜ç±»å‹çš„ä¸åŒè€Œå˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26604', 'title': 'Video Object Segmentation-Aware Audio Generation', 'url': 'https://huggingface.co/papers/2509.26604', 'abstract': 'SAGANet, a multimodal generative model, enhances audio generation by using object-level segmentation maps, improving control and fidelity in professional Foley workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing multimodal audio generation models often lack precise user control, which limits their applicability in professional Foley workflows. In particular, these models focus on the entire video and do not provide precise methods for prioritizing a specific object within a scene, generating unnecessary background sounds, or focusing on the wrong objects. To address this gap, we introduce the novel task of video object segmentation-aware audio generation, which explicitly conditions sound synthesis on object-level segmentation maps. We present SAGANet, a new multimodal generative model that enables controllable audio generation by leveraging visual segmentation masks along with video and textual cues. Our model provides users with fine-grained and visually localized control over audio generation. To support this task and further research on segmentation-aware Foley, we propose Segmented Music Solos, a benchmark dataset of musical instrument performance videos with segmentation information. Our method demonstrates substantial improvements over current state-of-the-art methods and sets a new standard for controllable, high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are available at https://saganet.notion.site', 'score': 1, 'issue_id': 6186, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'ba66403735003951', 'authors': ['Ilpo Viertola', 'Vladimir Iashin', 'Esa Rahtu'], 'affiliations': ['Tampere University, Tampere, Finland', 'University of Oxford, Oxford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2509.26604.jpg', 'data': {'categories': ['#audio', '#synthetic', '#games', '#benchmark', '#dataset', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ·Ğ²ÑƒĞºĞ° Ñ‡ĞµÑ€ĞµĞ· ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° SAGANet â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ°Ñ€Ñ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Foley-Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ñ… Ğ² ÑÑ†ĞµĞ½Ğµ. SAGANet Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°ÑĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼ Ğ·Ğ²ÑƒĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Segmented Music Solos Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Precision Audio Generation with Object-Level Control', 'desc': 'SAGANet is a new multimodal generative model designed to improve audio generation in Foley workflows by using object-level segmentation maps. This model allows for precise user control by focusing on specific objects within a video scene, thus avoiding unwanted background sounds. By conditioning audio synthesis on visual segmentation masks along with video and text inputs, SAGANet enhances the fidelity and relevance of generated sounds. The introduction of the Segmented Music Solos dataset further supports research in this area, showcasing significant advancements over existing methods.'}, 'zh': {'title': 'SAGANetï¼šç²¾å‡†æ§åˆ¶éŸ³é¢‘ç”Ÿæˆçš„æ–°æ ‡å‡†', 'desc': 'SAGANetæ˜¯ä¸€ç§å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨å¯¹è±¡çº§åˆ†å‰²å›¾æ¥å¢å¼ºéŸ³é¢‘ç”Ÿæˆï¼Œæ”¹å–„äº†ä¸“ä¸šFoleyå·¥ä½œæµç¨‹ä¸­çš„æ§åˆ¶å’Œä¿çœŸåº¦ã€‚ç°æœ‰çš„å¤šæ¨¡æ€éŸ³é¢‘ç”Ÿæˆæ¨¡å‹é€šå¸¸ç¼ºä¹ç²¾ç¡®çš„ç”¨æˆ·æ§åˆ¶ï¼Œå¯¼è‡´åœ¨ç”ŸæˆéŸ³é¢‘æ—¶æ— æ³•ä¼˜å…ˆè€ƒè™‘ç‰¹å®šå¯¹è±¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»»åŠ¡ï¼Œå³è§†é¢‘å¯¹è±¡åˆ†å‰²æ„ŸçŸ¥éŸ³é¢‘ç”Ÿæˆï¼Œæ˜ç¡®åœ°å°†å£°éŸ³åˆæˆä¸å¯¹è±¡çº§åˆ†å‰²å›¾ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨æˆ·èƒ½å¤Ÿå¯¹éŸ³é¢‘ç”Ÿæˆè¿›è¡Œç»†ç²’åº¦å’Œè§†è§‰å®šä½çš„æ§åˆ¶ï¼Œå¹¶åœ¨å¯æ§æ€§å’Œé«˜ä¿çœŸåº¦çš„Foleyåˆæˆæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26574', 'title': 'Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark', 'url': 'https://huggingface.co/papers/2509.26574', 'abstract': 'CritPt, a benchmark for evaluating LLMs on research-level physics tasks, reveals significant gaps between current model capabilities and the demands of physics research.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced "critical point"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.', 'score': 1, 'issue_id': 6176, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'c743f2941f645607', 'authors': ['Minhui Zhu', 'Minyang Tian', 'Xiaocheng Yang', 'Tianci Zhou', 'Penghao Zhu', 'Eli Chertkov', 'Shengyan Liu', 'Yufeng Du', 'Lifan Yuan', 'Ziming Ji', 'Indranil Das', 'Junyi Cao', 'Yufeng Du', 'Jinchen He', 'Yifan Su', 'Jiabin Yu', 'Yikun Jiang', 'Yujie Zhang', 'Chang Liu', 'Ze-Min Huang', 'Weizhen Jia', 'Xinan Chen', 'Peixue Wu', 'Yunkai Wang', 'Juntai Zhou', 'Yong Zhao', 'Farshid Jafarpour', 'Jessie Shelton', 'Aaron Young', 'John Bartolotta', 'Wenchao Xu', 'Yue Sun', 'Anjun Chu', 'Victor Colussi', 'Chris Akers', 'Nathan Brooks', 'Wenbo Fu', 'Christopher Wilson', 'Jinchao Zhao', 'Marvin Qi', 'Anqi Mu', 'Yubo Yang', 'Allen Zang', 'Yang Lyu', 'Peizhi Mai', 'Xuefei Guo', 'Luyu Gao', 'Ze Yang', 'Chi Xue', 'Dmytro Bandak', 'YaÃ¯r Hein', 'Yonatan Kahn', 'Kevin Zhou', 'John Drew Wilson Jarrod T. Reilly', 'Di Luo', 'Daniel Inafuku', 'Hao Tong', 'Liang Yang', 'Ruixing Zhang', 'Xueying Wang', 'Ofir Press', 'Nicolas Chia', 'Eliu Huerta', 'Hao Peng'], 'affiliations': ['Argonne National Laboratory', 'Caltech', 'Carnegie Mellon University', 'Chi 3 Optics', 'Columbia University', 'ETH ZÃ¼rich', 'Harvard University', 'Hofstra University', 'Hong Kong University of Science and Technology', 'Independent', 'National Institute of Theory and Mathematics', 'Northeastern University', 'Ohio State University', 'Paul Scherrer Institute', 'Perimeter Institute for Theoretical Physics', 'The Chinese University of Hong Kong', 'University of California San Diego', 'University of California, Berkeley', 'University of California, Los Angeles', 'University of Chicago', 'University of Cologne', 'University of Colorado Boulder', 'University of Connecticut', 'University of Florida', 'University of Illinois Urbana-Champaign', 'University of Maryland, College Park', 'University of Tennessee Knoxville', 'University of Toronto', 'University of Washington Seattle', 'University of Waterloo', 'Utrecht University', 'Vector Institute', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2509.26574.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#dataset'], 'emoji': 'âš›ï¸', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ğ»Ğ¸ LLM Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… â€” Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ»Ğ¸ÑÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CritPt Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 71 ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¸ 190 Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²ÑĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¾Ñ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ¾ Ğ±Ğ¸Ğ¾Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 50 Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ°Ğ¼Ğ¸-Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµĞ³Ğ¾ 4% Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 10% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ AI Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ.'}, 'en': {'title': 'Bridging the Gap: Evaluating LLMs in Advanced Physics Research', 'desc': 'The paper introduces CritPt, a benchmark specifically designed to evaluate large language models (LLMs) on complex, research-level physics tasks. It highlights the significant gap between the capabilities of current LLMs and the requirements of advanced physics research, as evidenced by low accuracy rates on full-scale challenges. CritPt includes 71 composite research challenges and 190 simpler tasks, all created by active physics researchers to ensure relevance and rigor. The findings suggest that while LLMs show potential in isolated tasks, they struggle with comprehensive research problems, indicating a need for further development in AI tools for scientific applications.'}, 'zh': {'title': 'è¯„ä¼°LLMsåœ¨ç‰©ç†ç ”ç©¶ä¸­çš„èƒ½åŠ›å·®è·', 'desc': 'CritPtæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç ”ç©¶çº§ç‰©ç†ä»»åŠ¡ä¸Šçš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹èƒ½åŠ›ä¸ç‰©ç†ç ”ç©¶éœ€æ±‚ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†ç°ä»£ç‰©ç†ç ”ç©¶çš„å¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬é‡å­ç‰©ç†ã€å¤©ä½“ç‰©ç†å’Œæµä½“åŠ¨åŠ›å­¦ç­‰ã€‚CritPtåŒ…å«71ä¸ªå¤åˆç ”ç©¶æŒ‘æˆ˜ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå…¥é—¨çº§çš„å®Œæ•´ç ”ç©¶é¡¹ç›®ï¼Œå¹¶åˆ†è§£ä¸º190ä¸ªæ›´ç®€å•çš„æ£€æŸ¥ç‚¹ä»»åŠ¡ã€‚å°½ç®¡å½“å‰æœ€å…ˆè¿›çš„LLMsåœ¨å­¤ç«‹çš„æ£€æŸ¥ç‚¹ä¸Šè¡¨ç°å‡ºä¸€å®šçš„æ½œåŠ›ï¼Œä½†åœ¨è§£å†³å®Œæ•´çš„ç ”ç©¶è§„æ¨¡æŒ‘æˆ˜æ—¶ä»ç„¶è¿œè¿œä¸å¤Ÿï¼Œæœ€é«˜å‡†ç¡®ç‡ä»…ä¸º4.0%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25666', 'title': 'Nudging the Boundaries of LLM Reasoning', 'url': 'https://huggingface.co/papers/2509.25666', 'abstract': 'NuRL, a nudging method using self-generated hints, enhances the upper limit of LLM reasoning in online reinforcement learning by enabling learning from previously unsolvable problems.  \t\t\t\t\tAI-generated summary \t\t\t\t Current online reinforcement learning (RL) algorithms like GRPO share a key limitation in LLM reasoning: they cannot learn from problems that are "unsolvable" to the model. In other words, they can only improve performance on problems where the model is capable of exploring the correct answer. Consequently, the model\'s "upper limit" remains unchanged after RL training, even though the likelihood of solving easier, solvable problems may increase. These hard samples cannot contribute to training, as no rollouts yield rewards and thus no gradients are produced. To unlock learning from these hard samples, we propose NuRL, a "nudging" method that aims to push the upper bound of LLM reasoning using self-generated hints, i.e., abstract cues that help reduce the problem difficulty for the model. Given a question and its gold answer, the model generates a CoT and then produces a hint containing the core knowledge needed to solve the problem. During training, we generate G rollouts from the base policy and use the pass rate to decide whether the hint should be injected. For hard samples with a 0% pass rate, we inject the hint and regenerate a new batch of trajectories. This yields two benefits: (1) the hint boosts pass rates (from 0% to non-zero), thereby introducing training signals for previously unsolvable samples, and (2) the hints are self-generated, avoiding distributional shift and do not rely on external models. NuRL achieves consistent improvements across 6 benchmarks and 3 models, while remaining complementary to test-time scaling. Notably, NuRL can raise the model\'s upper limit, whereas GRPO leaves pass@1024 unchanged from the base model. Furthermore, we present a systematic study of what makes an effective hint and when hints are most useful. Interestingly, the best hints are abstract and high-level, and are most beneficial when applied necessarily and after GRPO has converged.', 'score': 1, 'issue_id': 6188, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '4b96b0d8255bff52', 'authors': ['Justin Chih-Yao Chen', 'Becky Xiangyu Peng', 'Prafulla Kumar Choubey', 'Kung-Hsiang Huang', 'Jiaxin Zhang', 'Mohit Bansal', 'Chien-Sheng Wu'], 'affiliations': ['Salesforce AI Research', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2509.25666.jpg', 'data': {'categories': ['#training', '#rlhf', '#reasoning', '#optimization', '#benchmark', '#rl'], 'emoji': 'ğŸ’¡', 'ru': {'title': 'ĞŸĞ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ ÑĞµĞ±Ğµ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ LLM Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ NuRL, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ½ÑŒÑˆĞµ Ğ½Ğµ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ reinforcement learning, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GRPO, Ğ¼Ğ¾Ğ³ÑƒÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°Ñ Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğ¹ Ğ¿Ñ€ĞµĞ´ĞµĞ» Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. NuRL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ (hints), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ Ğ½ĞµĞ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ¾Ğ¹ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 6 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ 3 Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»Ğµ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ GRPO.'}, 'en': {'title': 'Unlocking Learning with Self-Generated Hints in Reinforcement Learning', 'desc': 'NuRL is a novel nudging method designed to enhance the reasoning capabilities of large language models (LLMs) in online reinforcement learning. It addresses the limitation of existing algorithms, like GRPO, which struggle to learn from unsolvable problems by generating self-created hints that simplify these challenges. By injecting these hints during training, NuRL allows the model to produce meaningful gradients from previously unsolvable samples, effectively raising its upper limit of reasoning. The method has shown consistent improvements across multiple benchmarks and models, demonstrating the importance of high-level, abstract hints in the learning process.'}, 'zh': {'title': 'NuRLï¼šæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä¸Šé™çš„è‡ªç”Ÿæˆæç¤ºæ–¹æ³•', 'desc': 'NuRLæ˜¯ä¸€ç§ä½¿ç”¨è‡ªç”Ÿæˆæç¤ºçš„å¼•å¯¼æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¸­å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„ä¸Šé™ã€‚ç°æœ‰çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•å¦‚GRPOå­˜åœ¨ä¸€ä¸ªå…³é”®é™åˆ¶ï¼Œå³æ— æ³•ä»æ¨¡å‹è®¤ä¸ºâ€œä¸å¯è§£â€çš„é—®é¢˜ä¸­å­¦ä¹ ã€‚NuRLé€šè¿‡ç”ŸæˆæŠ½è±¡æç¤ºï¼Œå¸®åŠ©æ¨¡å‹é™ä½é—®é¢˜éš¾åº¦ï¼Œä»è€Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ ä»¥å‰æ— æ³•è§£å†³çš„æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒNuRLåœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªæ¨¡å‹ä¸Šå‡å–å¾—äº†ä¸€è‡´çš„æ”¹è¿›ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25631', 'title': 'Swift: An Autoregressive Consistency Model for Efficient Weather\n  Forecasting', 'url': 'https://huggingface.co/papers/2509.25631', 'abstract': 'Swift, a single-step consistency model, enables efficient and skillful probabilistic weather forecasting by autoregressive finetuning of a probability flow model with CRPS, outperforming diffusion models and competitive with IFS ENS.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models offer a physically grounded framework for probabilistic weather forecasting, but their typical reliance on slow, iterative solvers during inference makes them impractical for subseasonal-to-seasonal (S2S) applications where long lead-times and domain-driven calibration are essential. To address this, we introduce Swift, a single-step consistency model that, for the first time, enables autoregressive finetuning of a probability flow model with a continuous ranked probability score (CRPS) objective. This eliminates the need for multi-model ensembling or parameter perturbations. Results show that Swift produces skillful 6-hourly forecasts that remain stable for up to 75 days, running 39times faster than state-of-the-art diffusion baselines while achieving forecast skill competitive with the numerical-based, operational IFS ENS. This marks a step toward efficient and reliable ensemble forecasting from medium-range to seasonal-scales.', 'score': 1, 'issue_id': 6195, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '6195dc6ffadb0e6e', 'authors': ['Jason Stock', 'Troy Arcomano', 'Rao Kotamarthi'], 'affiliations': ['Allen Institute for AI', 'Argonne National Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2509.25631.jpg', 'data': {'categories': ['#training', '#inference', '#data', '#diffusion', '#optimization'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ· Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³', 'desc': 'Swift - ÑÑ‚Ğ¾ consistency model, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CRPS Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, Ñ‡Ñ‚Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Swift Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 39 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ½Ğ° ÑÑ€Ğ¾Ğº Ğ´Ğ¾ 75 Ğ´Ğ½ĞµĞ¹. ĞŸĞ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ IFS ENS, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Swift: Revolutionizing Weather Forecasting with Speed and Precision', 'desc': 'The paper introduces Swift, a novel single-step consistency model designed for efficient probabilistic weather forecasting. It utilizes autoregressive finetuning of a probability flow model with a continuous ranked probability score (CRPS) objective, which enhances forecasting accuracy without the need for complex multi-model ensembling. Swift significantly improves the speed of generating forecasts, running 39 times faster than traditional diffusion models while maintaining competitive skill levels with established numerical forecasting systems. This advancement allows for reliable weather predictions over extended periods, making it suitable for subseasonal-to-seasonal applications.'}, 'zh': {'title': 'Swiftï¼šé«˜æ•ˆçš„å¤©æ°”é¢„æŠ¥æ–°æ–¹æ³•', 'desc': 'Swiftæ˜¯ä¸€ç§å•æ­¥ä¸€è‡´æ€§æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡è‡ªå›å½’å¾®è°ƒæ¦‚ç‡æµæ¨¡å‹ï¼Œä½¿ç”¨è¿ç»­æ’åæ¦‚ç‡è¯„åˆ†ï¼ˆCRPSï¼‰ç›®æ ‡æ¥å®ç°é«˜æ•ˆçš„å¤©æ°”é¢„æŠ¥ã€‚ä¸ä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼ŒSwiftåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸å†ä¾èµ–ç¼“æ…¢çš„è¿­ä»£æ±‚è§£å™¨ï¼Œä»è€Œé€‚ç”¨äºå­£èŠ‚æ€§å¤©æ°”é¢„æŠ¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSwiftèƒ½å¤Ÿç”Ÿæˆç¨³å®šçš„6å°æ—¶é¢„æŠ¥ï¼Œä¸”åœ¨75å¤©å†…ä¿æŒå‡†ç¡®æ€§ï¼Œé€Ÿåº¦æ¯”ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¿«39å€ã€‚è¿™æ ‡å¿—ç€åœ¨ä¸­æœŸåˆ°å­£èŠ‚æ€§å¤©æ°”é¢„æŠ¥ä¸­å®ç°é«˜æ•ˆå’Œå¯é çš„é›†æˆé¢„æŠ¥è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25134', 'title': 'LayerD: Decomposing Raster Graphic Designs into Layers', 'url': 'https://huggingface.co/papers/2509.25134', 'abstract': 'LayerD decomposes raster images into editable layers using iterative extraction and refinement, outperforming existing methods and enabling use with advanced image generators.  \t\t\t\t\tAI-generated summary \t\t\t\t Designers craft and edit graphic designs in a layer representation, but layer-based editing becomes impossible once composited into a raster image. In this work, we propose LayerD, a method to decompose raster graphic designs into layers for re-editable creative workflow. LayerD addresses the decomposition task by iteratively extracting unoccluded foreground layers. We propose a simple yet effective refinement approach taking advantage of the assumption that layers often exhibit uniform appearance in graphic designs. As decomposition is ill-posed and the ground-truth layer structure may not be reliable, we develop a quality metric that addresses the difficulty. In experiments, we show that LayerD successfully achieves high-quality decomposition and outperforms baselines. We also demonstrate the use of LayerD with state-of-the-art image generators and layer-based editing.', 'score': 1, 'issue_id': 6178, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '2f81e58539182440', 'authors': ['Tomoyuki Suzuki', 'Kang-Jun Liu', 'Naoto Inoue', 'Kota Yamaguchi'], 'affiliations': ['CyberAgent', 'Tohoku University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25134.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºÑƒ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'LayerD â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ ÑĞ»Ğ¾Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ°, Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ñ Ğ½ĞµĞ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑĞ»Ğ¾Ğ¸ Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. LayerD Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ baseline-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'LayerD: Transforming Raster Images into Editable Layers', 'desc': 'LayerD is a novel method designed to decompose raster images into editable layers, facilitating a more flexible graphic design workflow. It employs an iterative process to extract unoccluded foreground layers, which enhances the quality of the decomposition. The method incorporates a refinement strategy that leverages the uniform appearance of layers in graphic designs, addressing the challenges of the ill-posed nature of decomposition. Experimental results indicate that LayerD surpasses existing techniques, making it compatible with advanced image generators and improving layer-based editing capabilities.'}, 'zh': {'title': 'LayerDï¼šå›¾åƒåˆ†è§£çš„æ–°æ–¹æ³•', 'desc': 'LayerDæ˜¯ä¸€ç§å°†å…‰æ …å›¾åƒåˆ†è§£ä¸ºå¯ç¼–è¾‘å±‚çš„æ–¹æ³•ï¼Œé‡‡ç”¨è¿­ä»£æå–å’Œç²¾ç‚¼çš„æŠ€æœ¯ã€‚è¯¥æ–¹æ³•é€šè¿‡é€æ­¥æå–æœªè¢«é®æŒ¡çš„å‰æ™¯å±‚ï¼Œè§£å†³äº†å›¾åƒåˆ†è§£çš„éš¾é¢˜ã€‚LayerDè¿˜å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç²¾ç‚¼æ–¹æ³•ï¼Œåˆ©ç”¨å›¾å½¢è®¾è®¡ä¸­å±‚çš„å¤–è§‚é€šå¸¸æ˜¯å‡åŒ€çš„å‡è®¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLayerDåœ¨åˆ†è§£è´¨é‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¸å…ˆè¿›çš„å›¾åƒç”Ÿæˆå™¨å’ŒåŸºäºå±‚çš„ç¼–è¾‘å·¥å…·ç»“åˆä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25082', 'title': 'MANI-Pure: Magnitude-Adaptive Noise Injection for Adversarial\n  Purification', 'url': 'https://huggingface.co/papers/2509.25082', 'abstract': 'MANI-Pure, a magnitude-adaptive purification framework using diffusion models, effectively suppresses high-frequency adversarial perturbations while preserving low-frequency content, enhancing robust accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Adversarial purification with diffusion models has emerged as a promising defense strategy, but existing methods typically rely on uniform noise injection, which indiscriminately perturbs all frequencies, corrupting semantic structures and undermining robustness. Our empirical study reveals that adversarial perturbations are not uniformly distributed: they are predominantly concentrated in high-frequency regions, with heterogeneous magnitude intensity patterns that vary across frequencies and attack types. Motivated by this observation, we introduce MANI-Pure, a magnitude-adaptive purification framework that leverages the magnitude spectrum of inputs to guide the purification process. Instead of injecting homogeneous noise, MANI-Pure adaptively applies heterogeneous, frequency-targeted noise, effectively suppressing adversarial perturbations in fragile high-frequency, low-magnitude bands while preserving semantically critical low-frequency content. Extensive experiments on CIFAR-10 and ImageNet-1K validate the effectiveness of MANI-Pure. It narrows the clean accuracy gap to within 0.59 of the original classifier, while boosting robust accuracy by 2.15, and achieves the top-1 robust accuracy on the RobustBench leaderboard, surpassing the previous state-of-the-art method.', 'score': 1, 'issue_id': 6180, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': 'a6343f8b608f4cb0', 'authors': ['Xiaoyi Huang', 'Junwei Wu', 'Kejia Zhang', 'Carl Yang', 'Zhiming Luo'], 'affiliations': ['Emory University', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25082.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#security'], 'emoji': 'ğŸ”Š', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¾Ñ‚ adversarial Ğ°Ñ‚Ğ°Ğº Ñ‡ĞµÑ€ĞµĞ· Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MANI-Pure â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ adversarial Ğ°Ñ‚Ğ°Ğº Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ adversarial Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ°, MANI-Pure Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ°Ñ… Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° RobustBench, ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ² robust accuracy Ğ½Ğ° 2.15% Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Adaptive Purification for Enhanced Robustness Against Adversarial Attacks', 'desc': 'MANI-Pure is a new framework designed to improve the robustness of machine learning models against adversarial attacks by using diffusion models. It recognizes that adversarial perturbations are mostly found in high-frequency areas and vary in intensity, rather than being evenly distributed. By applying targeted noise that adapts to the magnitude of these perturbations, MANI-Pure effectively reduces harmful high-frequency noise while keeping important low-frequency information intact. This approach has shown significant improvements in robust accuracy on datasets like CIFAR-10 and ImageNet-1K, outperforming previous methods.'}, 'zh': {'title': 'MANI-Pureï¼šè‡ªé€‚åº”å‡€åŒ–ï¼Œæå‡é²æ£’æ€§', 'desc': 'MANI-Pureæ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¹…åº¦è‡ªé€‚åº”å‡€åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæŠ‘åˆ¶é«˜é¢‘å¯¹æŠ—æ‰°åŠ¨ï¼ŒåŒæ—¶ä¿ç•™ä½é¢‘å†…å®¹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚ç°æœ‰çš„å¯¹æŠ—å‡€åŒ–æ–¹æ³•é€šå¸¸ä¾èµ–äºå‡åŒ€å™ªå£°æ³¨å…¥ï¼Œè¿™ä¼šæ— å·®åˆ«åœ°æ‰°åŠ¨æ‰€æœ‰é¢‘ç‡ï¼Œç ´åè¯­ä¹‰ç»“æ„ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå¯¹æŠ—æ‰°åŠ¨å¹¶ä¸æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œè€Œæ˜¯ä¸»è¦é›†ä¸­åœ¨é«˜é¢‘åŒºåŸŸï¼Œå¹¶ä¸”åœ¨ä¸åŒé¢‘ç‡å’Œæ”»å‡»ç±»å‹ä¸‹å…·æœ‰ä¸åŒçš„å¹…åº¦å¼ºåº¦æ¨¡å¼ã€‚MANI-Pureé€šè¿‡åˆ©ç”¨è¾“å…¥çš„å¹…åº¦è°±æ¥æŒ‡å¯¼å‡€åŒ–è¿‡ç¨‹ï¼Œé€‚åº”æ€§åœ°æ–½åŠ é’ˆå¯¹ç‰¹å®šé¢‘ç‡çš„å¼‚è´¨å™ªå£°ï¼Œæœ‰æ•ˆæŠ‘åˆ¶è„†å¼±çš„é«˜é¢‘ä½å¹…åº¦å¸¦ä¸­çš„å¯¹æŠ—æ‰°åŠ¨ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰ä¸Šé‡è¦çš„ä½é¢‘å†…å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.24732', 'title': 'Who invented deep residual learning?', 'url': 'https://huggingface.co/papers/2509.24732', 'abstract': 'A timeline of the evolution of deep residual learning, a key advancement in neural network architecture.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern AI is based on deep artificial neural networks (NNs). As of 2025, the most cited scientific article of the 21st century is an NN paper on deep residual learning with residual connections. Who invented this? We present a timeline of the evolution of deep residual learning.', 'score': 1, 'issue_id': 6180, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '0bc8de443fa1708e', 'authors': ['Juergen Schmidhuber'], 'affiliations': ['IDSIA'], 'pdf_title_img': 'assets/pdf/title_img/2509.24732.jpg', 'data': {'categories': ['#architecture'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ĞºÑ‚Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ñ‘Ğ» residual connections', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ…Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (deep residual learning) â€” ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ residual connections, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾ residual learning ÑÑ‚Ğ°Ğ»Ğ° ÑĞ°Ğ¼Ğ¾Ğ¹ Ñ†Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒÑ‘Ğ¹ XXI Ğ²ĞµĞºĞ° Ğ¿Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ½Ğ° 2025 Ğ³Ğ¾Ğ´. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¾ Ñ‚Ğ¾Ğ¼, ĞºÑ‚Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ğ°Ğ» ÑÑ‚Ğ¾Ñ‚ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ AI.'}, 'en': {'title': 'Tracing the Evolution of Deep Residual Learning', 'desc': 'This paper outlines the historical development of deep residual learning, a significant breakthrough in neural network architecture. It highlights the importance of residual connections, which help in training deeper networks by mitigating the vanishing gradient problem. The authors trace the contributions of various researchers and key milestones that led to the widespread adoption of this technique. By 2025, deep residual learning is recognized as a foundational element in modern AI, influencing numerous applications in machine learning.'}, 'zh': {'title': 'æ·±åº¦æ®‹å·®å­¦ä¹ çš„æ¼”å˜å†ç¨‹', 'desc': 'æ·±åº¦æ®‹å·®å­¦ä¹ æ˜¯ç¥ç»ç½‘ç»œæ¶æ„ä¸­çš„ä¸€ä¸ªé‡è¦è¿›å±•ã€‚æœ¬æ–‡æä¾›äº†æ·±åº¦æ®‹å·®å­¦ä¹ çš„å‘å±•æ—¶é—´çº¿ï¼Œå±•ç¤ºäº†å…¶æ¼”å˜è¿‡ç¨‹ã€‚æ®‹å·®è¿æ¥çš„å¼•å…¥ä½¿å¾—è®­ç»ƒæ›´æ·±å±‚æ¬¡çš„ç¥ç»ç½‘ç»œæˆä¸ºå¯èƒ½ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚åˆ°2025å¹´ï¼Œæ·±åº¦æ®‹å·®å­¦ä¹ çš„ç›¸å…³è®ºæ–‡å°†æˆä¸º21ä¸–çºªè¢«å¼•ç”¨æœ€å¤šçš„ç§‘å­¦æ–‡ç« ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.24088', 'title': 'CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems', 'url': 'https://huggingface.co/papers/2509.24088', 'abstract': 'CORRECT is a lightweight, training-free framework that uses an online cache of distilled error schemata to improve error localization in multi-agent systems with minimal overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems (MAS) are increasingly capable of tackling complex real-world tasks, yet their reliance on inter-agent coordination, tool use, and long-horizon reasoning makes error recognition particularly challenging. Minor errors can propagate across agents, escalating into task failures while producing long, intertwined execution trajectories that impose significant costs for both human developers and automated systems to debug and analyze. Our key insight is that, despite surface differences in failure trajectories (e.g., logs), MAS errors often recur with similar structural patterns. This paper presents CORRECT, the first lightweight, training-free framework that leverages an online cache of distilled error schemata to recognize and transfer knowledge of failure structures across new requests. This cache-based reuse allows LLMs to perform targeted error localization at inference time, avoiding the need for expensive retraining while adapting to dynamic MAS deployments in subseconds. To support rigorous study in this domain, we also introduce CORRECT-Error, a large-scale dataset of over 2,000 annotated trajectories collected through a novel error-injection pipeline guided by real-world distributions, and further validated through human evaluation to ensure alignment with natural failure patterns. Experiments across seven diverse MAS applications show that CORRECT improves step-level error localization up to 19.8% over existing advances while at near-zero overhead, substantially narrowing the gap between automated and human-level error recognition.', 'score': 1, 'issue_id': 6191, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 28', 'zh': '9æœˆ28æ—¥'}, 'hash': '3728d1ca311982ad', 'authors': ['Yifan Yu', 'Moyan Li', 'Shaoyuan Xu', 'Jinmiao Fu', 'Xinhai Hou', 'Fan Lai', 'Bryan Wang'], 'affiliations': ['Amazon', 'University of Illinois Urbana-Champaign', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2509.24088.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#inference', '#data', '#agents', '#reasoning', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ…ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'CORRECT â€” ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… (MAS). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ĞºĞµÑˆ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… ÑĞ±Ğ¾ĞµĞ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ CORRECT-Error Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 2000 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, ÑĞ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¸Ğ½ÑŠĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´Ğ¾ 19.8% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ñ… Ğ½Ğ°ĞºĞ»Ğ°Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ°Ñ….'}, 'en': {'title': 'CORRECT: Smart Error Localization for Multi-Agent Systems', 'desc': 'The paper introduces CORRECT, a novel framework designed to enhance error localization in multi-agent systems (MAS) without the need for extensive training. It utilizes an online cache of distilled error schemata to identify recurring structural patterns in errors, which helps in recognizing failures more efficiently. By leveraging this cache, CORRECT allows for quick adaptation to new tasks and environments, significantly reducing the overhead typically associated with error recognition. The framework is validated with the CORRECT-Error dataset, demonstrating improved error localization performance compared to existing methods, thus bridging the gap between automated systems and human error recognition capabilities.'}, 'zh': {'title': 'CORRECTï¼šæå‡å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé”™è¯¯å®šä½çš„è½»é‡çº§æ¡†æ¶', 'desc': 'CORRECTæ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„é”™è¯¯å®šä½èƒ½åŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œè®­ç»ƒã€‚è¯¥æ¡†æ¶åˆ©ç”¨åœ¨çº¿ç¼“å­˜çš„æç‚¼é”™è¯¯æ¨¡å¼ï¼Œè¯†åˆ«å’Œè½¬ç§»é”™è¯¯ç»“æ„çš„çŸ¥è¯†ï¼Œä»è€Œåœ¨æ¨ç†æ—¶å®ç°é’ˆå¯¹æ€§çš„é”™è¯¯å®šä½ã€‚é€šè¿‡è¿™ç§ç¼“å­˜é‡ç”¨ï¼ŒCORRECTèƒ½å¤Ÿåœ¨å‡ ç§’é’Ÿå†…é€‚åº”åŠ¨æ€çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿéƒ¨ç½²ï¼Œé¿å…äº†æ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCORRECTåœ¨ä¸ƒä¸ªä¸åŒçš„å¤šæ™ºèƒ½ä½“åº”ç”¨ä¸­ï¼Œé”™è¯¯å®šä½çš„å‡†ç¡®æ€§æé«˜äº†19.8%ï¼Œå‡ ä¹æ²¡æœ‰é¢å¤–å¼€é”€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23695', 'title': 'Estimating Time Series Foundation Model Transferability via In-Context\n  Learning', 'url': 'https://huggingface.co/papers/2509.23695', 'abstract': "TimeTic is a transferability estimation framework that predicts the performance of time series foundation models after fine-tuning on unseen datasets, using tabular foundation models and entropy evolution for model characterization.  \t\t\t\t\tAI-generated summary \t\t\t\t Time series foundation models (TSFMs) offer strong zero-shot forecasting via large-scale pre-training, yet fine-tuning remains critical for boosting performance in domains with limited public data. With the growing number of TSFMs, efficiently identifying the best model for downstream fine-tuning becomes increasingly challenging. In this work, we introduce TimeTic, a transferability estimation framework that recasts model selection as an in-context-learning problem: given observations on known (source) datasets, it predicts how a TSFM will perform after fine-tuning on a downstream (target) dataset. TimeTic flexibly organizes the observed model-data relationships as contextual information, allowing it to adapt seamlessly to various test-time scenarios. Leveraging the natural tabular structure formed by dataset meta-features, model characteristics, and fine-tuned performance, we employ tabular foundation models to serve as in-context learners. We further introduce a novel model characterization based on entropy evolution across model layers, capturing embedding-space distinctions and enabling TimeTic to generalize across arbitrary model sets. We establish a comprehensive benchmark for transferability estimation including 10 datasets, 10 foundation models, and 3 forecasting tasks. On this benchmark, TimeTic's estimation demonstrates strong alignment with actual fine-tuned performance for previously unseen datasets, achieving a mean rank correlation of approximately 0.6 and a 30% improvement compared to using zero-shot performance as the transferability score.", 'score': 1, 'issue_id': 6184, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 28', 'zh': '9æœˆ28æ—¥'}, 'hash': '6753046e5d9fabfa', 'authors': ['Qingren Yao', 'Ming Jin', 'Chengqi Zhang', 'Chao-Han Huck Yang', 'Jun Qi', 'Shirui Pan'], 'affiliations': ['Griffith University', 'Hong Kong Baptist University', 'NVIDIA Research', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2509.23695.jpg', 'data': {'categories': ['#training', '#benchmark', '#transfer_learning', '#dataset'], 'emoji': 'â°', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ time series Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'TimeTic â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ foundation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ fine-tuning Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº in-context learning: Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ performance Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ. Ğ”Ğ»Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸, Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ tabular foundation models. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¸Ğ· 10 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ 10 foundation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ TimeTic Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¾ĞºĞ¾Ğ»Ğ¾ 0.6 Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ baseline Ğ½Ğ° 30%.'}, 'en': {'title': 'TimeTic: Predicting Performance of Time Series Models with Smart Estimation', 'desc': 'TimeTic is a framework designed to estimate how well time series foundation models (TSFMs) will perform after being fine-tuned on new datasets. It treats the model selection process as an in-context learning problem, using data from known datasets to predict outcomes for unknown ones. By organizing model and dataset relationships into a tabular format, TimeTic can adapt to different scenarios effectively. The framework also introduces a unique method of characterizing models through entropy evolution, which helps it generalize across various models and improve transferability estimation significantly.'}, 'zh': {'title': 'TimeTicï¼šæå‡æ—¶é—´åºåˆ—æ¨¡å‹å¾®è°ƒæ€§èƒ½çš„è½¬ç§»æ€§ä¼°è®¡æ¡†æ¶', 'desc': 'TimeTicæ˜¯ä¸€ä¸ªè½¬ç§»æ€§ä¼°è®¡æ¡†æ¶ï¼Œæ—¨åœ¨é¢„æµ‹æ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹åœ¨æœªè§æ•°æ®é›†ä¸Šå¾®è°ƒåçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ¨¡å‹é€‰æ‹©é‡æ–°å®šä¹‰ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ é—®é¢˜ï¼Œåˆ©ç”¨å·²çŸ¥æ•°æ®é›†çš„è§‚å¯Ÿç»“æœæ¥é¢„æµ‹æ¨¡å‹åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚TimeTicçµæ´»åœ°ç»„ç»‡è§‚å¯Ÿåˆ°çš„æ¨¡å‹ä¸æ•°æ®ä¹‹é—´çš„å…³ç³»ï¼Œé€‚åº”ä¸åŒçš„æµ‹è¯•åœºæ™¯ã€‚é€šè¿‡å¼•å…¥åŸºäºç†µæ¼”åŒ–çš„æ¨¡å‹ç‰¹å¾åŒ–ï¼ŒTimeTicèƒ½å¤Ÿåœ¨ä»»æ„æ¨¡å‹é›†ä¸Šè¿›è¡Œæ³›åŒ–ï¼Œæ˜¾è‘—æé«˜äº†è½¬ç§»æ€§ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23019', 'title': 'LLM Watermark Evasion via Bias Inversion', 'url': 'https://huggingface.co/papers/2509.23019', 'abstract': 'The Bias-Inversion Rewriting Attack (BIRA) effectively evades watermarking in large language models by suppressing specific logits, highlighting a significant vulnerability in watermarking techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Watermarking for large language models (LLMs) embeds a statistical signal during generation to enable detection of model-produced text. While watermarking has proven effective in benign settings, its robustness under adversarial evasion remains contested. To advance a rigorous understanding and evaluation of such vulnerabilities, we propose the Bias-Inversion Rewriting Attack (BIRA), which is theoretically motivated and model-agnostic. BIRA weakens the watermark signal by suppressing the logits of likely watermarked tokens during LLM-based rewriting, without any knowledge of the underlying watermarking scheme. Across recent watermarking methods, BIRA achieves over 99\\% evasion while preserving the semantic content of the original text. Beyond demonstrating an attack, our results reveal a systematic vulnerability, emphasizing the need for stress testing and robust defenses.', 'score': 1, 'issue_id': 6189, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': '4442b22fb44496b3', 'authors': ['Jeongyeon Hwang', 'Sangdon Park', 'Jungseul Ok'], 'affiliations': ['Pohang University of Science and Technology (POSTECH), South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2509.23019.jpg', 'data': {'categories': ['#benchmark', '#inference', '#security'], 'emoji': 'ğŸ’§', 'ru': {'title': 'ĞÑ‚Ğ°ĞºĞ° Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ñ‚Ğ°ĞºÑƒ BIRA (Bias-Inversion Rewriting Attack), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ğº, Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. BIRA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 99% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ñ‹ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½ÑƒÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ².'}, 'en': {'title': 'BIRA: Unmasking Vulnerabilities in LLM Watermarking', 'desc': 'The paper introduces the Bias-Inversion Rewriting Attack (BIRA), a method that successfully bypasses watermarking in large language models (LLMs) by targeting and suppressing specific logits associated with watermarked tokens. This attack highlights a critical weakness in current watermarking techniques, which are designed to identify AI-generated text. BIRA operates without needing to know the details of the watermarking scheme, making it a versatile and powerful adversarial strategy. The findings indicate that while watermarking can be effective, it is not robust against sophisticated evasion tactics like BIRA, underscoring the importance of developing stronger defenses.'}, 'zh': {'title': 'æ­ç¤ºæ°´å°æŠ€æœ¯çš„è„†å¼±æ€§ï¼šåå·®åè½¬é‡å†™æ”»å‡»', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåå·®åè½¬é‡å†™æ”»å‡»ï¼ˆBIRAï¼‰çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§„é¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ°´å°æŠ€æœ¯ã€‚BIRAé€šè¿‡æŠ‘åˆ¶ç‰¹å®šçš„logitsï¼Œå‰Šå¼±äº†æ°´å°ä¿¡å·ï¼Œä»è€Œåœ¨é‡å†™è¿‡ç¨‹ä¸­é¿å…è¢«æ£€æµ‹ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºå…·ä½“çš„æ°´å°æ–¹æ¡ˆï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒBIRAåœ¨å¤šç§æ°´å°æ–¹æ³•ä¸‹å®ç°äº†è¶…è¿‡99%çš„è§„é¿ç‡ï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹æ–‡æœ¬çš„è¯­ä¹‰å†…å®¹ï¼Œæ­ç¤ºäº†æ°´å°æŠ€æœ¯çš„ç³»ç»Ÿæ€§è„†å¼±æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.22889', 'title': 'Convolutional Set Transformer', 'url': 'https://huggingface.co/papers/2509.22889', 'abstract': 'The Convolutional Set Transformer (CST) processes image sets directly, combining feature extraction and contextual modeling for improved performance in set classification and anomaly detection, with compatibility for CNN explainability methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the Convolutional Set Transformer (CST), a novel neural architecture designed to process image sets of arbitrary cardinality that are visually heterogeneous yet share high-level semantics - such as a common category, scene, or concept. Existing set-input networks, e.g., Deep Sets and Set Transformer, are limited to vector inputs and cannot directly handle 3D image tensors. As a result, they must be cascaded with a feature extractor, typically a CNN, which encodes images into embeddings before the set-input network can model inter-image relationships. In contrast, CST operates directly on 3D image tensors, performing feature extraction and contextual modeling simultaneously, thereby enabling synergies between the two processes. This design yields superior performance in tasks such as Set Classification and Set Anomaly Detection and further provides native compatibility with CNN explainability methods such as Grad-CAM, unlike competing approaches that remain opaque. Finally, we show that CSTs can be pre-trained on large-scale datasets and subsequently adapted to new domains and tasks through standard Transfer Learning schemes. To support further research, we release CST-15, a CST backbone pre-trained on ImageNet (https://github.com/chinefed/convolutional-set-transformer).', 'score': 1, 'issue_id': 6186, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': '9683d53f1c23ea13', 'authors': ['Federico Chinello', 'Giacomo Boracchi'], 'affiliations': ['Dep. of Computing Sciences, Bocconi University, Italy', 'Dep. of Electronics, Information and Bioengineering, Politecnico di Milano, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2509.22889.jpg', 'data': {'categories': ['#training', '#transfer_learning', '#interpretability', '#games', '#open_source', '#dataset', '#cv', '#architecture'], 'emoji': 'ğŸ´', 'ru': {'title': 'ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Convolutional Set Transformer (CST) â€” Ğ½Ğ¾Ğ²Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° Deep Sets, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ CNN Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², CST Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 3D Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹. CST ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ CNN (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Grad-CAM) Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ transfer learning Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ImageNet.'}, 'en': {'title': 'Revolutionizing Image Set Processing with CST', 'desc': 'The Convolutional Set Transformer (CST) is a new neural network architecture that processes sets of images directly, allowing it to handle varying numbers of images that share common themes. Unlike previous models that require images to be converted into fixed-size vectors before processing, CST works with 3D image tensors, enabling simultaneous feature extraction and contextual modeling. This leads to better performance in tasks like set classification and anomaly detection, while also being compatible with CNN explainability techniques. Additionally, CST can be pre-trained on large datasets and adapted to new tasks using transfer learning, making it a versatile tool for researchers.'}, 'zh': {'title': 'å·ç§¯é›†å˜æ¢å™¨ï¼šç›´æ¥å¤„ç†å›¾åƒé›†åˆçš„åˆ›æ–°æ¶æ„', 'desc': 'å·ç§¯é›†å˜æ¢å™¨ï¼ˆCSTï¼‰æ˜¯ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ï¼Œèƒ½å¤Ÿç›´æ¥å¤„ç†å…·æœ‰ä»»æ„æ•°é‡çš„å›¾åƒé›†åˆã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†ç‰¹å¾æå–å’Œä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œæå‡äº†é›†åˆåˆ†ç±»å’Œå¼‚å¸¸æ£€æµ‹çš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦å…ˆæå–ç‰¹å¾çš„ç½‘ç»œä¸åŒï¼ŒCSTå¯ä»¥ç›´æ¥åœ¨3Då›¾åƒå¼ é‡ä¸Šæ“ä½œï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„å¤„ç†ã€‚CSTè¿˜ä¸CNNå¯è§£é‡Šæ€§æ–¹æ³•å…¼å®¹ï¼Œæ”¯æŒåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé¢„è®­ç»ƒå¹¶é€šè¿‡è¿ç§»å­¦ä¹ é€‚åº”æ–°ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26555', 'title': 'Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional\n  Video Generation', 'url': 'https://huggingface.co/papers/2509.26555', 'abstract': 'Stable Cinemetrics introduces a structured evaluation framework for professional video generation, using taxonomies to assess models across specific filmmaking controls.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have enabled high-fidelity video synthesis from user provided prompts. However, existing models and benchmarks fail to capture the complexity and requirements of professional video generation. Towards that goal, we introduce Stable Cinemetrics, a structured evaluation framework that formalizes filmmaking controls into four disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera. Together, these taxonomies define 76 fine-grained control nodes grounded in industry practices. Using these taxonomies, we construct a benchmark of prompts aligned with professional use cases and develop an automated pipeline for prompt categorization and question generation, enabling independent evaluation of each control dimension. We conduct a large-scale human study spanning 10+ models and 20K videos, annotated by a pool of 80+ film professionals. Our analysis, both coarse and fine-grained reveal that even the strongest current models exhibit significant gaps, particularly in Events and Camera-related controls. To enable scalable evaluation, we train an automatic evaluator, a vision-language model aligned with expert annotations that outperforms existing zero-shot baselines. SCINE is the first approach to situate professional video generation within the landscape of video generative models, introducing taxonomies centered around cinematic controls and supporting them with structured evaluation pipelines and detailed analyses to guide future research.', 'score': 0, 'issue_id': 6178, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '6d557fb5b6516154', 'authors': ['Agneet Chatterjee', 'Rahim Entezari', 'Maksym Zhuravinskyi', 'Maksim Lapin', 'Reshinth Adithyan', 'Amit Raj', 'Chitta Baral', 'Yezhou Yang', 'Varun Jampani'], 'affiliations': ['Arizona State University', 'Google DeepMind', 'Stability AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.26555.jpg', 'data': {'categories': ['#benchmark', '#games', '#video', '#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Stable Cinemetrics â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AI Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ (Setup, Event, Lighting, Camera) Ñ 76 Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ñ… ĞºĞ¸Ğ½Ğ¾Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸. Ğ‘Ñ‹Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ 80+ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»Ğ¾Ğ² ĞºĞ¸Ğ½Ğ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ 20 Ñ‚Ñ‹ÑÑÑ‡ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ 10+ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑĞµÑ€ÑŒÑ‘Ğ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ´Ğ°Ğ¶Ğµ Ñƒ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ evaluator Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Revolutionizing Video Evaluation with Cinematic Taxonomies', 'desc': 'Stable Cinemetrics presents a new framework for evaluating AI-generated videos, focusing on professional filmmaking standards. It introduces four hierarchical taxonomiesâ€”Setup, Event, Lighting, and Cameraâ€”that break down the complex aspects of video generation into 76 specific control nodes. The framework includes a benchmark of prompts based on real-world filmmaking scenarios and an automated system for categorizing these prompts and generating evaluation questions. A large-scale study with film professionals shows that current models still struggle with certain filmmaking controls, particularly in Events and Camera, highlighting the need for improved evaluation methods in video generation.'}, 'zh': {'title': 'ç¨³å®šç”µå½±åº¦é‡ï¼šä¸“ä¸šè§†é¢‘ç”Ÿæˆçš„æ–°æ ‡å‡†', 'desc': 'Stable Cinemetrics æ˜¯ä¸€ä¸ªç»“æ„åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºä¸“ä¸šè§†é¢‘ç”Ÿæˆã€‚å®ƒé€šè¿‡å››ä¸ªå±‚æ¬¡åˆ†æ˜çš„åˆ†ç±»æ³•ï¼ˆè®¾ç½®ã€äº‹ä»¶ã€ç…§æ˜å’Œæ‘„åƒï¼‰æ¥è¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šç”µå½±åˆ¶ä½œæ§åˆ¶æ–¹é¢çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å®šä¹‰äº†76ä¸ªåŸºäºè¡Œä¸šå®è·µçš„ç»†ç²’åº¦æ§åˆ¶èŠ‚ç‚¹ï¼Œå¹¶æ„å»ºäº†ä¸ä¸“ä¸šç”¨ä¾‹å¯¹é½çš„åŸºå‡†æµ‹è¯•ã€‚é€šè¿‡å¤§è§„æ¨¡çš„äººç±»ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°å½“å‰æœ€å¼ºæ¨¡å‹åœ¨äº‹ä»¶å’Œæ‘„åƒæ§åˆ¶æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26278', 'title': 'ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency\n  Estimation', 'url': 'https://huggingface.co/papers/2509.26278', 'abstract': 'ProfVLM, a compact vision-language model, uses generative reasoning to estimate skill proficiency and generate expert feedback from multi-view videos, outperforming existing methods with fewer parameters and faster training.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing approaches to skill proficiency estimation often rely on black-box video classifiers, ignoring multi-view context and lacking explainability. We present ProfVLM, a compact vision-language model that reformulates this task as generative reasoning: it jointly predicts skill level and generates expert-like feedback from egocentric and exocentric videos. Central to our method is an AttentiveGatedProjector that dynamically fuses multi-view features, projected from a frozen TimeSformer backbone into a language model tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60%. Our approach not only achieves superior accuracy across diverse activities, but also outputs natural language critiques aligned with performance, offering transparent reasoning. These results highlight generative vision-language modeling as a powerful new direction for skill assessment.', 'score': 0, 'issue_id': 6182, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '542e2b1ee1e15307', 'authors': ['Edoardo Bianchi', 'Jacopo Staiano', 'Antonio Liotta'], 'affiliations': ['Free University of Bozen-Bolzano, Via Bruno Buozzi 1, Bozen-Bolzano, 39100, Italy', 'University of Trento, Via Inama 5, Trento, 38122, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2509.26278.jpg', 'data': {'categories': ['#multimodal', '#training', '#interpretability', '#optimization', '#reasoning', '#cv'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ VLM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'ProfVLM â€” ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ²Ğ»Ğ°Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ AttentiveGatedProjector, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ TimeSformer Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 20 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 60%, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Revolutionizing Skill Assessment with Generative Vision-Language Modeling', 'desc': 'ProfVLM is a compact vision-language model designed to assess skill proficiency by utilizing generative reasoning. Unlike traditional video classifiers, it leverages multi-view video data to provide both skill level predictions and expert feedback. The model employs an AttentiveGatedProjector to effectively combine features from different video perspectives, enhancing its understanding of the task. With significant improvements in accuracy and efficiency, ProfVLM demonstrates the potential of generative vision-language modeling in skill assessment applications.'}, 'zh': {'title': 'ç”Ÿæˆæ¨ç†ï¼šæŠ€èƒ½è¯„ä¼°çš„æ–°æ–¹å‘', 'desc': 'ProfVLMæ˜¯ä¸€ç§ç´§å‡‘çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨ç†æ¥è¯„ä¼°æŠ€èƒ½æ°´å¹³å¹¶ä»å¤šè§†è§’è§†é¢‘ä¸­ç”Ÿæˆä¸“å®¶åé¦ˆã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨å‚æ•°æ›´å°‘å’Œè®­ç»ƒæ›´å¿«çš„æƒ…å†µä¸‹è¡¨ç°æ›´ä¼˜ã€‚è¯¥æ¨¡å‹é€šè¿‡åŠ¨æ€èåˆå¤šè§†è§’ç‰¹å¾ï¼Œç»“åˆè‡ªæˆ‘ä¸­å¿ƒå’Œå¤–éƒ¨ä¸­å¿ƒçš„è§†é¢‘ï¼Œæ¥å…±åŒé¢„æµ‹æŠ€èƒ½æ°´å¹³å¹¶ç”Ÿæˆç±»ä¼¼ä¸“å®¶çš„åé¦ˆã€‚è®­ç»ƒåœ¨EgoExo4Dæ•°æ®é›†ä¸Šï¼ŒProfVLMä¸ä»…åœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè¿˜æä¾›äº†ä¸è¡¨ç°ç›¸ç¬¦çš„è‡ªç„¶è¯­è¨€è¯„è®ºï¼Œå±•ç°äº†ç”Ÿæˆè§†è§‰-è¯­è¨€å»ºæ¨¡åœ¨æŠ€èƒ½è¯„ä¼°ä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25810', 'title': 'Learning to Reason as Action Abstractions with Scalable Mid-Training RL', 'url': 'https://huggingface.co/papers/2509.25810', 'abstract': 'Mid-training with action abstractions enhances reinforcement learning in large language models, improving performance and convergence in code generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models excel with reinforcement learning (RL), but fully unlocking this potential requires a mid-training stage. An effective mid-training phase should identify a compact set of useful actions and enable fast selection among them through online RL. We formalize this intuition by presenting the first theoretical result on how mid-training shapes post-training: it characterizes an action subspace that minimizes both the value approximation error from pruning and the RL error during subsequent planning. Our analysis reveals two key determinants of mid-training effectiveness: pruning efficiency, which shapes the prior of the initial RL policy, and its impact on RL convergence, which governs the extent to which that policy can be improved via online interactions. These results suggest that mid-training is most effective when the decision space is compact and the effective horizon is short, highlighting the importance of operating in the space of action abstractions rather than primitive actions. Building on these insights, we propose Reasoning as Action Abstractions (RA3), a scalable mid-training algorithm. Specifically, we derive a sequential variational lower bound and optimize it by iteratively discovering temporally-consistent latent structures via RL, followed by fine-tuning on the bootstrapped data. Experiments on code generation tasks demonstrate the effectiveness of our approach. Across multiple base models, RA3 improves the average performance on HumanEval and MBPP by 8 and 4 points over the base model and the next-token prediction baseline. Furthermore, RA3 achieves faster convergence and higher asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and Codeforces.', 'score': 0, 'issue_id': 6189, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'ec4f3fd30226fba2', 'authors': ['Shenao Zhang', 'Donghan Yu', 'Yihao Feng', 'Bowen Jin', 'Zhaoran Wang', 'John Peebles', 'Zirui Wang'], 'affiliations': ['Apple', 'Northwestern University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2509.25810.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#rlhf', '#rl', '#games'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ĞºĞ¾Ğ´', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ°Ñ ÑÑ‚Ğ°Ğ´Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (mid-training) ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ reinforcement learning Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼. ĞĞ½Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ mid-training Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ RA3, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· RL Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 8 Ğ¸ 4 Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… HumanEval Ğ¸ MBPP, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Unlocking RL Potential with Mid-Training Action Abstractions', 'desc': 'This paper discusses how mid-training with action abstractions can improve reinforcement learning (RL) in large language models, particularly for code generation tasks. It introduces a new algorithm called Reasoning as Action Abstractions (RA3), which optimizes the selection of useful actions during a mid-training phase. The authors provide a theoretical framework that shows how this mid-training process can reduce errors in value approximation and enhance RL convergence. Experiments demonstrate that RA3 significantly boosts performance and speeds up learning in various code generation benchmarks.'}, 'zh': {'title': 'ä¸­æœŸè®­ç»ƒä¸åŠ¨ä½œæŠ½è±¡æå‡å¼ºåŒ–å­¦ä¹ æ•ˆæœ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨ä¸­æœŸè®­ç»ƒä¸åŠ¨ä½œæŠ½è±¡çš„ç»“åˆï¼Œä»¥å¢å¼ºå¼ºåŒ–å­¦ä¹ çš„æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­ã€‚ä¸­æœŸè®­ç»ƒé˜¶æ®µé€šè¿‡è¯†åˆ«ä¸€ç»„ç´§å‡‘çš„æœ‰ç”¨åŠ¨ä½œï¼Œä¿ƒè¿›äº†å¿«é€Ÿé€‰æ‹©ï¼Œå¹¶é€šè¿‡åœ¨çº¿å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–äº†å†³ç­–è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸­æœŸè®­ç»ƒçš„æœ‰æ•ˆæ€§å—ä¸¤ä¸ªå…³é”®å› ç´ çš„å½±å“ï¼šå‰ªææ•ˆç‡å’Œå¯¹å¼ºåŒ–å­¦ä¹ æ”¶æ•›æ€§çš„å½±å“ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæå‡ºäº†ä¸€ç§åä¸ºReasoning as Action Abstractionsï¼ˆRA3ï¼‰çš„å¯æ‰©å±•ä¸­æœŸè®­ç»ƒç®—æ³•ï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å¤šä¸ªä»£ç ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.24510', 'title': 'Specialization after Generalization: Towards Understanding Test-Time\n  Training in Foundation Models', 'url': 'https://huggingface.co/papers/2509.24510', 'abstract': "Test-time training (TTT) improves performance by allowing foundation models to specialize on test tasks, reducing in-distribution test error through a mechanism of focusing on relevant concepts.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent empirical studies have explored the idea of continuing to train a model at test-time for a given task, known as test-time training (TTT), and have found it to yield significant performance improvements. However, there is limited understanding of why and when TTT is effective. Earlier explanations mostly focused on the observation that TTT may help when applied to out-of-distribution adaptation or used with privileged data. However, the growing scale of foundation models with most test data being in-distribution questions these explanations. We instead posit that foundation models remain globally underparameterized, with TTT providing a mechanism for specialization after generalization, focusing capacity on concepts relevant to the test task. Specifically, under the linear representation hypothesis, we propose a model in which TTT achieves a substantially smaller in-distribution test error than global training. We empirically validate our model's key assumptions by training a sparse autoencoder on ImageNet, showing that semantically related data points are explained by only a few shared concepts. Finally, we perform scaling studies across image and language tasks that confirm the practical implications of our model, identifying the regimes where specialization is most effective.", 'score': 0, 'issue_id': 6185, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '6361987e96ad9f9b', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚, Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ test-time training (TTT) â€” Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ â€” Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ foundation models. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ´Ğ¾Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ° TTT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°Ñ…, Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ğ¾Ğ¹ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ½Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¾Ğ±ÑŠÑÑĞ½ÑÑÑ‚ÑÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ ÑĞ¾ sparse autoencoder Ğ½Ğ° ImageNet. ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ğ² ĞºĞ°ĞºĞ¸Ñ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· TTT Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°.'}, 'en': {'title': 'Specializing Models at Test-Time for Better Performance', 'desc': 'This paper discusses test-time training (TTT), a method that enhances the performance of foundation models by allowing them to adapt to specific tasks during testing. The authors argue that TTT helps models focus on relevant concepts, leading to reduced errors on in-distribution test data. They propose that foundation models are underparameterized, and TTT enables them to specialize after initial generalization. Through experiments with a sparse autoencoder on ImageNet, they validate their hypothesis and identify conditions under which TTT is most beneficial across various tasks.'}, 'zh': {'title': 'æµ‹è¯•æ—¶è®­ç»ƒï¼šæå‡æ¨¡å‹ä¸“é—¨åŒ–çš„å…³é”®', 'desc': 'æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTTTï¼‰é€šè¿‡å…è®¸åŸºç¡€æ¨¡å‹åœ¨æµ‹è¯•ä»»åŠ¡ä¸Šè¿›è¡Œä¸“é—¨åŒ–ï¼Œä»è€Œæé«˜æ€§èƒ½ï¼Œå‡å°‘åˆ†å¸ƒå†…æµ‹è¯•è¯¯å·®ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒTTTåœ¨ç‰¹å®šä»»åŠ¡ä¸Šç»§ç»­è®­ç»ƒæ¨¡å‹å¯ä»¥æ˜¾è‘—æå‡æ•ˆæœï¼Œä½†å¯¹å…¶æœ‰æ•ˆæ€§åŸå› çš„ç†è§£ä»ç„¶æœ‰é™ã€‚æˆ‘ä»¬æå‡ºï¼ŒåŸºç¡€æ¨¡å‹åœ¨å…¨å±€ä¸Šä»ç„¶æ˜¯æ¬ å‚æ•°åŒ–çš„ï¼ŒTTTä¸ºåœ¨æ³›åŒ–åè¿›è¡Œä¸“é—¨åŒ–æä¾›äº†ä¸€ç§æœºåˆ¶ï¼Œä¸“æ³¨äºä¸æµ‹è¯•ä»»åŠ¡ç›¸å…³çš„æ¦‚å¿µã€‚é€šè¿‡å¯¹å›¾åƒå’Œè¯­è¨€ä»»åŠ¡çš„æ‰©å±•ç ”ç©¶ï¼Œæˆ‘ä»¬ç¡®è®¤äº†æ¨¡å‹çš„å®é™…åº”ç”¨ï¼Œè¯†åˆ«å‡ºä¸“é—¨åŒ–æœ€æœ‰æ•ˆçš„æƒ…å†µã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2509.18538', 'title': 'GeoRemover: Removing Objects and Their Causal Visual Artifacts', 'url': 'https://huggingface.co/papers/2509.18538', 'abstract': "A geometry-aware two-stage framework for intelligent image editing effectively removes objects and their causal visual artifacts by decoupling geometry removal and appearance rendering.  \t\t\t\t\tAI-generated summary \t\t\t\t Towards intelligent image editing, object removal should eliminate both the target object and its causal visual artifacts, such as shadows and reflections. However, existing image appearance-based methods either follow strictly mask-aligned training and fail to remove these causal effects which are not explicitly masked, or adopt loosely mask-aligned strategies that lack controllability and may unintentionally over-erase other objects. We identify that these limitations stem from ignoring the causal relationship between an object's geometry presence and its visual effects. To address this limitation, we propose a geometry-aware two-stage framework that decouples object removal into (1) geometry removal and (2) appearance rendering. In the first stage, we remove the object directly from the geometry (e.g., depth) using strictly mask-aligned supervision, enabling structure-aware editing with strong geometric constraints. In the second stage, we render a photorealistic RGB image conditioned on the updated geometry, where causal visual effects are considered implicitly as a result of the modified 3D geometry. To guide learning in the geometry removal stage, we introduce a preference-driven objective based on positive and negative sample pairs, encouraging the model to remove objects as well as their causal visual artifacts while avoiding new structural insertions. Extensive experiments demonstrate that our method achieves state-of-the-art performance in removing both objects and their associated artifacts on two popular benchmarks. The code is available at https://github.com/buxiangzhiren/GeoRemover.", 'score': 0, 'issue_id': 6187, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 23', 'zh': '9æœˆ23æ—¥'}, 'hash': 'a7bec14cbc682e85', 'authors': ['Zixin Zhu', 'Haoxiang Li', 'Xuelu Feng', 'He Wu', 'Chunming Qiao', 'Junsong Yuan'], 'affiliations': ['Pixocial Technology', 'University at Buffalo'], 'pdf_title_img': 'assets/pdf/title_img/2509.18538.jpg', 'data': {'categories': ['#cv', '#3d', '#benchmark', '#optimization'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ“ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ñ‚ĞµĞ½ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ°Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚, Ğ½Ğ¾ Ğ¸ ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ‚ĞµĞ½Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑƒĞ´Ğ°Ğ»ÑĞµÑ‚ÑÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¸Ğ· ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğ³Ğ¾ mask-aligned Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ RGB Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ¿Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ preference-driven Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ±ĞµĞ· Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ´Ğ¾Ğ².'}, 'en': {'title': 'Decoupling Geometry and Appearance for Flawless Object Removal', 'desc': 'This paper presents a two-stage framework for intelligent image editing that focuses on effectively removing objects and their visual artifacts. The first stage involves geometry removal, where the object is taken out based on its geometric structure, ensuring that the editing respects the spatial relationships in the image. The second stage is appearance rendering, which creates a realistic image by considering the updated geometry and the causal visual effects like shadows and reflections. The proposed method outperforms existing techniques by addressing the limitations of previous approaches that either fail to remove artifacts or unintentionally alter other elements in the image.'}, 'zh': {'title': 'å‡ ä½•æ„ŸçŸ¥çš„æ™ºèƒ½å›¾åƒç¼–è¾‘æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å‡ ä½•æ„ŸçŸ¥çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºæ™ºèƒ½å›¾åƒç¼–è¾‘ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå»é™¤ç›®æ ‡ç‰©ä½“åŠå…¶å› æœè§†è§‰ä¼ªå½±ã€‚è¯¥æ–¹æ³•å°†ç‰©ä½“å»é™¤è¿‡ç¨‹åˆ†ä¸ºå‡ ä½•å»é™¤å’Œå¤–è§‚æ¸²æŸ“ä¸¤ä¸ªé˜¶æ®µï¼Œç¡®ä¿åœ¨å»é™¤ç‰©ä½“æ—¶è€ƒè™‘å…¶å‡ ä½•ç»“æ„ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ä¸¥æ ¼çš„æ©è†œå¯¹é½ç›‘ç£ç›´æ¥ä»å‡ ä½•ä¿¡æ¯ä¸­å»é™¤ç‰©ä½“ï¼Œç¬¬äºŒé˜¶æ®µåˆ™åŸºäºæ›´æ–°åçš„å‡ ä½•ä¿¡æ¯æ¸²æŸ“å‡ºé€¼çœŸçš„RGBå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å»é™¤ç‰©ä½“åŠå…¶ç›¸å…³ä¼ªå½±æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00446', 'title': 'LongCodeZip: Compress Long Context for Code Language Models', 'url': 'https://huggingface.co/papers/2510.00446', 'abstract': 'LongCodeZip is a code compression framework for LLMs that uses dual-stage compression to reduce context size without degrading performance, improving efficiency in code intelligence applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.', 'score': 86, 'issue_id': 6221, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'b9bb4e93a2d263ea', 'authors': ['Yuling Shi', 'Yichun Qian', 'Hongyu Zhang', 'Beijun Shen', 'Xiaodong Gu'], 'affiliations': ['Chongqing University, Chongqing, China', 'Shanghai Jiao Tong University, Shanghai, China', 'Stanford University, Stanford, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.00446.jpg', 'data': {'categories': ['#training', '#long_context', '#data', '#optimization', '#plp'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LongCodeZip â€” ÑÑ‚Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ LLM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ğ¸. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ³Ñ€ÑƒĞ±ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹, Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ğ¾ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ ĞºĞ¾Ğ¼Ğ¿Ñ€ĞµÑÑĞ¸Ñ: Ğ¾ÑÑ‚Ğ°Ğ²ÑˆĞ¸ĞµÑÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ»Ğ¾ĞºĞ¸, Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½-Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ¾ 5.6x Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ ĞºĞ¾Ğ´Ñƒ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ñ€Ğ¾Ğ´Ğµ LLMLingua.'}, 'en': {'title': 'Efficient Code Compression for LLMs with LongCodeZip', 'desc': 'LongCodeZip is a specialized framework designed to compress code for Large Language Models (LLMs) while maintaining performance. It utilizes a dual-stage compression approach, first applying coarse-grained compression to identify and prioritize relevant function-level chunks based on their importance. Then, it employs fine-grained compression to further refine these functions into optimal segments, ensuring that only the most pertinent information is retained. This method significantly reduces context size, achieving up to a 5.6x compression ratio, which enhances the efficiency of code-related tasks without sacrificing output quality.'}, 'zh': {'title': 'æå‡ä»£ç æ™ºèƒ½çš„å‹ç¼©æ•ˆç‡', 'desc': 'LongCodeZip æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®¾è®¡çš„ä»£ç å‹ç¼©æ¡†æ¶ï¼Œé‡‡ç”¨åŒé˜¶æ®µå‹ç¼©ç­–ç•¥æ¥å‡å°‘ä¸Šä¸‹æ–‡å¤§å°è€Œä¸é™ä½æ€§èƒ½ã€‚å®ƒé¦–å…ˆé€šè¿‡æ¡ä»¶å›°æƒ‘åº¦å¯¹å‡½æ•°çº§å—è¿›è¡Œç²—ç²’åº¦å‹ç¼©ï¼Œä¿ç•™æœ€ç›¸å…³çš„å‡½æ•°ï¼›ç„¶åè¿›è¡Œç»†ç²’åº¦å‹ç¼©ï¼Œæ ¹æ®å›°æƒ‘åº¦å°†ä¿ç•™çš„å‡½æ•°åˆ†å—ï¼Œå¹¶åœ¨è‡ªé€‚åº”ä»¤ç‰Œé¢„ç®—ä¸‹é€‰æ‹©æœ€ä½³å­é›†ã€‚é€šè¿‡åœ¨ä»£ç è¡¥å…¨ã€æ‘˜è¦å’Œé—®ç­”ç­‰å¤šä¸ªä»»åŠ¡ä¸Šçš„è¯„ä¼°ï¼ŒLongCodeZip æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•çš„æ€§èƒ½ï¼Œå‹ç¼©æ¯”é«˜è¾¾ 5.6 å€ã€‚è¯¥æ¡†æ¶æœ‰æ•ˆå‡å°‘äº†ä¸Šä¸‹æ–‡å¤§å°ï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®ä¿¡æ¯ï¼Œä»è€Œæå‡äº†ä»£ç æ™ºèƒ½åº”ç”¨çš„æ•ˆç‡å’Œèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02283', 'title': 'Self-Forcing++: Towards Minute-Scale High-Quality Video Generation', 'url': 'https://huggingface.co/papers/2510.02283', 'abstract': "A method is proposed to enhance long-horizon video generation by using sampled segments from self-generated long videos to guide student models, maintaining quality and consistency without additional supervision or retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/", 'score': 68, 'issue_id': 6221, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '6c012f635291b12f', 'authors': ['Justin Cui', 'Jie Wu', 'Ming Li', 'Tao Yang', 'Xiaojie Li', 'Rui Wang', 'Andrew Bai', 'Yuanhao Ban', 'Cho-Jui Hsieh'], 'affiliations': ['ByteDance Seed', 'UCLA', 'University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2510.02283.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#long_context', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¿Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² 20 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ±Ğ¾Ğ»ĞµĞµ 4 Ğ¼Ğ¸Ğ½ÑƒÑ‚. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµÑÑ‡Ñ‘Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ¾Ğ² Ğ¸ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°ĞµÑ‚ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¿ĞµÑ€ĞµÑĞºÑĞ¿Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Long Video Generation with Self-Sampled Guidance', 'desc': 'This paper presents a novel method for improving the generation of long videos using segments from self-generated long videos to guide student models. By leveraging the knowledge of teacher models, the approach maintains high video quality and temporal consistency without the need for additional supervision or retraining. The method allows for scaling video lengths significantly, achieving up to 20 times the length of what teacher models can produce. Experimental results show that this technique outperforms existing methods in both fidelity and consistency, enabling the generation of videos lasting over 4 minutes.'}, 'zh': {'title': 'æå‡é•¿è§†é¢‘ç”Ÿæˆè´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºé•¿æ—¶é—´è§†é¢‘ç”Ÿæˆçš„æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨è‡ªç”Ÿæˆé•¿è§†é¢‘çš„é‡‡æ ·ç‰‡æ®µæ¥æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œåœ¨ä¸éœ€è¦é¢å¤–ç›‘ç£æˆ–é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ä¿æŒè´¨é‡å’Œä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œä¸ºå­¦ç”Ÿæ¨¡å‹æä¾›æŒ‡å¯¼ï¼Œé¿å…äº†å¸¸è§çš„é—®é¢˜ï¼Œå¦‚è¿‡åº¦æ›å…‰å’Œé”™è¯¯ç´¯ç§¯ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå°†è§†é¢‘é•¿åº¦æ‰©å±•åˆ°æ•™å¸ˆæ¨¡å‹èƒ½åŠ›çš„20å€ï¼Œç”Ÿæˆæ—¶é•¿å¯è¾¾4åˆ†15ç§’ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†é¢‘ç”Ÿæˆçš„ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02245', 'title': 'ExGRPO: Learning to Reason from Experience', 'url': 'https://huggingface.co/papers/2510.02245', 'abstract': 'ExGRPO, a framework that prioritizes valuable reasoning experiences, improves and stabilizes reinforcement learning from verifiable rewards for large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR.', 'score': 62, 'issue_id': 6222, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '2d2caae66ebc961d', 'authors': ['Runzhe Zhan', 'Yafu Li', 'Zhi Wang', 'Xiaoye Qu', 'Dongrui Liu', 'Jing Shao', 'Derek F. Wong', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2510.02245.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ÑÑ Ğ½Ğ° Ñ†ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ¿Ñ‹Ñ‚Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ExGRPO â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… on-policy Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ExGRPO Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 1.5B Ğ´Ğ¾ 8B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 3.5-7.6 Ğ±Ğ°Ğ»Ğ»Ğ¾Ğ². ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ°Ğ¼, Ğ³Ğ´Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ on-policy Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ‚ĞµÑ€Ğ¿ÑÑ‚ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ñƒ.'}, 'en': {'title': 'Prioritizing Valuable Experiences for Better Reinforcement Learning', 'desc': 'ExGRPO is a new framework designed to enhance reinforcement learning from verifiable rewards (RLVR) for large language models. It addresses the inefficiencies of traditional on-policy training by prioritizing valuable reasoning experiences, which helps stabilize the learning process. The framework identifies key indicators of experience value, such as rollout correctness and entropy, to optimize the learning dynamics. Experiments demonstrate that ExGRPO significantly improves reasoning performance across various models, making it a crucial advancement in efficient RLVR.'}, 'zh': {'title': 'ä¼˜å…ˆè€ƒè™‘æœ‰ä»·å€¼ç»éªŒçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'ExGRPOæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜å…ˆè€ƒè™‘æœ‰ä»·å€¼çš„æ¨ç†ç»éªŒï¼Œä»è€Œæ”¹å–„å’Œç¨³å®šåŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚ä¼ ç»Ÿçš„åœ¨çº¿è®­ç»ƒæ–¹æ³•åœ¨æ¯æ¬¡æ›´æ–°åä¼šä¸¢å¼ƒç»éªŒï¼Œå¯¼è‡´è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œä¸ç¨³å®šã€‚æœ¬æ–‡é¦–æ¬¡æ¢è®¨äº†ä»€ä¹ˆæ ·çš„æ¨ç†ç»éªŒæ˜¯æœ‰ä»·å€¼çš„ï¼Œå¹¶ç¡®å®šäº†å›æ»šæ­£ç¡®æ€§å’Œç†µä½œä¸ºæœ‰æ•ˆçš„ç»éªŒä»·å€¼æŒ‡æ ‡ã€‚é€šè¿‡è¿™äº›è§è§£ï¼ŒExGRPOç»„ç»‡å’Œä¼˜å…ˆè€ƒè™‘æœ‰ä»·å€¼çš„ç»éªŒï¼Œå¹¶é‡‡ç”¨æ··åˆç­–ç•¥ç›®æ ‡æ¥å¹³è¡¡æ¢ç´¢ä¸ç»éªŒåˆ©ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02314', 'title': 'StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions', 'url': 'https://huggingface.co/papers/2510.02314', 'abstract': "A novel density-guided poisoning method for 3D Gaussian Splatting enhances attack effectiveness by strategically injecting Gaussian points and disrupting multi-view consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/", 'score': 52, 'issue_id': 6223, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '334776e6e757ace4', 'authors': ['Bo-Hsu Ke', 'You-Zhe Xie', 'Yu-Lun Liu', 'Wei-Chen Chiu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.02314.jpg', 'data': {'categories': ['#security', '#benchmark', '#3d'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ° Ğ½Ğ° 3D Gaussian Splatting Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° 3D Gaussian Splatting (3DGS) Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ñ‚Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· Kernel Density Estimation (KDE), ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¸Ğ»Ğ»ÑĞ·Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑˆÑƒĞ¼ Ğ´Ğ»Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ multi-view consistency, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´Ñ‹. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ KDE Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Attack Effectiveness in 3D Gaussian Splatting with Density-Guided Poisoning', 'desc': 'This paper presents a new method for attacking 3D Gaussian Splatting (3DGS) by using a density-guided poisoning technique. The approach involves injecting Gaussian points into areas with low density, which creates misleading visual artifacts that are noticeable from certain viewpoints. Additionally, the method employs an adaptive noise strategy to further disrupt the consistency of views across the 3D scene. The authors also introduce a new evaluation protocol based on Kernel Density Estimation (KDE) to measure the effectiveness of these attacks, showing that their method outperforms existing techniques.'}, 'zh': {'title': 'å¢å¼º3Dé«˜æ–¯ç‚¹äº‘æ”»å‡»æ•ˆæœçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¯†åº¦å¼•å¯¼ä¸­æ¯’æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼º3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3D Gaussian Splattingï¼‰çš„æ”»å‡»æ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨ä½å¯†åº¦åŒºåŸŸæ³¨å…¥é«˜æ–¯ç‚¹ï¼Œç ´åå¤šè§†å›¾ä¸€è‡´æ€§ï¼Œä»è€Œåœ¨å—å½±å“çš„è§†è§’ä¸­åµŒå…¥æ˜æ˜¾çš„è™šå‡ç‰©ä½“ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å™ªå£°ç­–ç•¥ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ç³»ç»Ÿçš„KDEè¯„ä¼°åè®®ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®¢è§‚åœ°è¯„ä¼°æ”»å‡»éš¾åº¦ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02297', 'title': 'Interactive Training: Feedback-Driven Neural Network Optimization', 'url': 'https://huggingface.co/papers/2510.02297', 'abstract': 'Interactive Training is a framework that allows real-time, feedback-driven intervention during neural network training, improving stability and adaptability.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional neural network training typically follows fixed, predefined optimization recipes, lacking the flexibility to dynamically respond to instabilities or emerging training issues. In this paper, we introduce Interactive Training, an open-source framework that enables real-time, feedback-driven intervention during neural network training by human experts or automated AI agents. At its core, Interactive Training uses a control server to mediate communication between users or agents and the ongoing training process, allowing users to dynamically adjust optimizer hyperparameters, training data, and model checkpoints. Through three case studies, we demonstrate that Interactive Training achieves superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needs, paving the way toward a future training paradigm where AI agents autonomously monitor training logs, proactively resolve instabilities, and optimize training dynamics.', 'score': 36, 'issue_id': 6221, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'a5b5b6a9b4ca0924', 'authors': ['Wentao Zhang', 'Yang Young Lu', 'Yuntian Deng'], 'affiliations': ['University of Waterloo', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2510.02297.jpg', 'data': {'categories': ['#training', '#open_source', '#optimization'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾Ğ¼ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Interactive Training â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼ Ğ¸Ğ»Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ¢Ñ€Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ¸Ğ´ÑÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Empowering Neural Networks with Real-Time Interactive Training', 'desc': 'This paper presents Interactive Training, a novel framework that enhances neural network training by allowing real-time interventions. It addresses the limitations of traditional training methods, which often lack the flexibility to adapt to issues as they arise. The framework facilitates communication between users or AI agents and the training process, enabling dynamic adjustments to hyperparameters, training data, and model checkpoints. The results from case studies show that Interactive Training leads to better stability, less sensitivity to initial settings, and greater adaptability to user requirements.'}, 'zh': {'title': 'å®æ—¶åé¦ˆï¼Œæå‡è®­ç»ƒçµæ´»æ€§', 'desc': 'äº’åŠ¨è®­ç»ƒæ˜¯ä¸€ç§æ¡†æ¶ï¼Œå…è®¸åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶çš„åé¦ˆé©±åŠ¨å¹²é¢„ï¼Œä»è€Œæé«˜è®­ç»ƒçš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œè®­ç»ƒé€šå¸¸éµå¾ªå›ºå®šçš„ä¼˜åŒ–æµç¨‹ï¼Œç¼ºä¹åŠ¨æ€åº”å¯¹ä¸ç¨³å®šæ€§æˆ–æ–°å‡ºç°é—®é¢˜çš„çµæ´»æ€§ã€‚æœ¬æ–‡ä»‹ç»çš„äº’åŠ¨è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒäººç±»ä¸“å®¶æˆ–è‡ªåŠ¨åŒ–AIä»£ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶å¹²é¢„ï¼Œç”¨æˆ·å¯ä»¥åŠ¨æ€è°ƒæ•´ä¼˜åŒ–å™¨è¶…å‚æ•°ã€è®­ç»ƒæ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚é€šè¿‡ä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†äº’åŠ¨è®­ç»ƒåœ¨è®­ç»ƒç¨³å®šæ€§ã€å¯¹åˆå§‹è¶…å‚æ•°çš„æ•æ„Ÿæ€§é™ä½ä»¥åŠå¯¹ç”¨æˆ·éœ€æ±‚çš„é€‚åº”æ€§æé«˜æ–¹é¢çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02209', 'title': 'StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?', 'url': 'https://huggingface.co/papers/2510.02209', 'abstract': 'StockBench evaluates large language models in realistic stock trading environments, revealing challenges and opportunities in developing LLM-powered financial agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.', 'score': 34, 'issue_id': 6221, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '6d2362d30dbb6925', 'authors': ['Yanxu Chen', 'Zijun Yao', 'Yantao Liu', 'Jin Ye', 'Jianing Yu', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.02209.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#benchmark', '#agents'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°ĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ğ½Ğ¾ Ğ¿Ğ¾ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼', 'desc': 'StockBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ¾Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğ¸ Ğ°ĞºÑ†Ğ¸ÑĞ¼Ğ¸. ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑÑ‚ ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ - Ñ†ĞµĞ½Ñ‹, Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸ - Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞµ, Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğµ Ğ¸Ğ»Ğ¸ ÑƒĞ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ğ¸ Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼ĞµÑÑÑ†ĞµĞ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-5 Ğ¸ Claude-4, Ñ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ "ĞºÑƒĞ¿Ğ¸ Ğ¸ Ğ´ĞµÑ€Ğ¶Ğ¸", Ñ…Ğ¾Ñ‚Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¿ĞµÑ… Ğ² ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ.'}, 'en': {'title': 'StockBench: Evaluating LLMs in Real-World Stock Trading', 'desc': 'This paper introduces StockBench, a new benchmark for evaluating large language models (LLMs) in realistic stock trading scenarios. Unlike previous benchmarks that focus on static knowledge, StockBench assesses LLMs on their ability to make dynamic trading decisions based on daily market signals. The evaluation uses financial metrics to measure performance, revealing that while many LLMs struggle to outperform a basic buy-and-hold strategy, some show promise in generating higher returns and managing risk. This research highlights the complexities of applying LLMs in finance and aims to foster further exploration in developing effective financial agents.'}, 'zh': {'title': 'StockBenchï¼šè¯„ä¼°é‡‘èä»£ç†çš„æœªæ¥æ½œåŠ›', 'desc': 'StockBench æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®è‚¡ç¥¨äº¤æ˜“ç¯å¢ƒä¸­çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚å®ƒè§£å†³äº†ç°æœ‰é‡‘èåŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰äº¤æ˜“åŠ¨æ€å’Œè¿­ä»£ç‰¹æ€§çš„ä¸è¶³ã€‚é€šè¿‡æä¾›æ¯æ—¥å¸‚åœºä¿¡å·ï¼ŒLLM ä»£ç†éœ€è¦åšå‡ºä¹°å…¥ã€å–å‡ºæˆ–æŒæœ‰çš„å†³ç­–ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å¤§å¤šæ•° LLM ä»£ç†æœªèƒ½è¶…è¶Šç®€å•çš„ä¹°å…¥æŒæœ‰ç­–ç•¥ï¼Œä½†ä¸€äº›æ¨¡å‹æ˜¾ç¤ºå‡ºæ›´é«˜çš„å›æŠ¥æ½œåŠ›å’Œæ›´æœ‰æ•ˆçš„é£é™©ç®¡ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01149', 'title': 'ModernVBERT: Towards Smaller Visual Document Retrievers', 'url': 'https://huggingface.co/papers/2510.01149', 'abstract': 'ModernVBERT, a compact vision-language encoder, outperforms larger models in document retrieval by optimizing attention masking, image resolution, modality alignment, and contrastive objectives.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal embedding models are gaining prevalence, notably for document retrieval as efficient alternatives to text-only pipelines. These models are typically built by finetuning large vision-language decoders (VLMs) with contrastive losses on text-image pairs. In this work, we show that, while cost-efficient, this repurposing approach often bottlenecks retrieval performance. Through controlled experiments, we establish a principled recipe for improving visual document retrieval models. We notably measure the impact of attention masking, image resolution, modality alignment data regimes, and late interaction centered contrastive objectives which emerge as central performance factors. Building on these insights, we release ModernVBERT, a compact 250M-parameter vision-language encoder that outperforms models up to 10 times larger when finetuned on document retrieval tasks. Models and code are made available at https://huggingface.co/ModernVBERT.', 'score': 26, 'issue_id': 6231, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '676d94db77731b89', 'authors': ['Paul Teiletche', 'Quentin MacÃ©', 'Max Conti', 'Antonio Loison', 'Gautier Viaud', 'Pierre Colombo', 'Manuel Faysse'], 'affiliations': ['CentraleSupelec, Paris-Saclay', 'EPFL', 'Equall.ai', 'Illuin Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.01149.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#open_source', '#training', '#optimization', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹, Ğ½Ğ¾ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹: ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ encoder Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ModernVBERT â€” ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ vision-language encoder Ñ 250 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… VLM Ñ contrastive losses Ğ½Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ retrieval. Ğ§ĞµÑ€ĞµĞ· ÑĞµÑ€Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ½Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ attention masking, Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ contrastive objectives Ñ late interaction. ModernVBERT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² 10 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Compact Power: ModernVBERT Revolutionizes Document Retrieval', 'desc': 'ModernVBERT is a new vision-language encoder designed to improve document retrieval tasks. It achieves better performance than larger models by optimizing key factors such as attention masking, image resolution, and modality alignment. The research highlights that traditional methods of fine-tuning large vision-language models can limit retrieval effectiveness. By implementing a more efficient approach with only 250 million parameters, ModernVBERT demonstrates superior results in retrieving documents compared to models that are ten times its size.'}, 'zh': {'title': 'ModernVBERTï¼šå°å·§è€Œå¼ºå¤§çš„è§†è§‰-è¯­è¨€ç¼–ç å™¨', 'desc': 'ModernVBERTæ˜¯ä¸€ç§ç´§å‡‘çš„è§†è§‰-è¯­è¨€ç¼–ç å™¨ï¼Œé€šè¿‡ä¼˜åŒ–æ³¨æ„åŠ›æ©ç ã€å›¾åƒåˆ†è¾¨ç‡ã€æ¨¡æ€å¯¹é½å’Œå¯¹æ¯”ç›®æ ‡ï¼Œè¶…è¶Šäº†æ›´å¤§çš„æ¨¡å‹åœ¨æ–‡æ¡£æ£€ç´¢ä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼Œè™½ç„¶å¯¹å¤§å‹è§†è§‰-è¯­è¨€è§£ç å™¨è¿›è¡Œå¾®è°ƒæ˜¯ä¸€ç§æˆæœ¬æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½†è¿™ç§æ–¹æ³•å¾€å¾€ä¼šé™åˆ¶æ£€ç´¢æ€§èƒ½ã€‚é€šè¿‡æ§åˆ¶å®éªŒï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ”¹è¿›è§†è§‰æ–‡æ¡£æ£€ç´¢æ¨¡å‹çš„åŸåˆ™æ€§æ–¹æ³•ï¼Œå¹¶æµ‹é‡äº†å¤šä¸ªå…³é”®å› ç´ çš„å½±å“ã€‚æœ€ç»ˆï¼ŒModernVBERTä»¥250Må‚æ•°çš„è§„æ¨¡ï¼Œåœ¨æ–‡æ¡£æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†é«˜è¾¾10å€æ›´å¤§çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01265', 'title': 'RLP: Reinforcement as a Pretraining Objective', 'url': 'https://huggingface.co/papers/2510.01265', 'abstract': 'RLP, an information-driven reinforcement pretraining objective, enhances reasoning models by integrating exploration into pretraining, leading to significant performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes.', 'score': 26, 'issue_id': 6222, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': '80c397f193259627', 'authors': ['Ali Hatamizadeh', 'Syeda Nahida Akter', 'Shrimai Prabhumoye', 'Jan Kautz', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Yejin Choi'], 'affiliations': ['Boston University', 'Carnegie Mellon University', 'NVIDIA', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2510.01265.jpg', 'data': {'categories': ['#training', '#reasoning', '#rl', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ñ‡Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ RLP â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ reinforcement learning Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ pretraining, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ post-training. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ·Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ chain-of-thought Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ñˆ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ verifier Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Â«Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ¼Â» ÑƒĞ¶Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ pretraining. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-1.7B-Base ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¾ 19% Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ° Ğ´Ğ»Ñ Nemotron-Nano-12B-v2 ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ²Ñ‹Ñ€Ğ¾Ñ Ñ 42.81% Ğ´Ğ¾ 61.32%.'}, 'en': {'title': 'Reinforcement Learning for Smarter Pretraining', 'desc': 'This paper introduces RLP, a novel reinforcement pretraining objective that enhances reasoning models by incorporating exploration during the pretraining phase. Unlike traditional methods that only apply reinforcement learning after initial training, RLP encourages models to engage in exploratory reasoning earlier, treating chain-of-thought as an action that provides valuable information for future predictions. The reward system is designed to measure the improvement in predicting the next token based on both context and a reasoning chain, promoting independent thinking in models. The results show significant performance boosts across various benchmarks, particularly in reasoning-heavy tasks, demonstrating the effectiveness of integrating reinforcement learning into the pretraining process.'}, 'zh': {'title': 'æ¢ç´¢é©±åŠ¨çš„å¼ºåŒ–é¢„è®­ç»ƒï¼Œæå‡æ¨ç†æ¨¡å‹è¡¨ç°', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¿¡æ¯é©±åŠ¨çš„å¼ºåŒ–é¢„è®­ç»ƒç›®æ ‡RLPï¼Œæ—¨åœ¨é€šè¿‡å°†æ¢ç´¢èå…¥é¢„è®­ç»ƒæ¥å¢å¼ºæ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚RLPå°†æ€ç»´é“¾è§†ä¸ºä¸€ç§æ¢ç´¢è¡Œä¸ºï¼Œå¹¶æ ¹æ®å…¶å¯¹æœªæ¥æ ‡è®°é¢„æµ‹çš„ä¿¡æ¯å¢ç›Šæ¥è®¡ç®—å¥–åŠ±ä¿¡å·ã€‚è¿™ç§æ–¹æ³•é¼“åŠ±æ¨¡å‹åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ä¹‹å‰ç‹¬ç«‹æ€è€ƒï¼Œä»è€Œåœ¨é¢„è®­ç»ƒé˜¶æ®µæ›´æ—©åœ°åŸ¹å…»ç‹¬ç«‹æ€è€ƒèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨RLPè¿›è¡Œé¢„è®­ç»ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01591', 'title': 'CLUE: Non-parametric Verification from Experience via Hidden-State\n  Clustering', 'url': 'https://huggingface.co/papers/2510.01591', 'abstract': "Hidden states in Large Language Models encode correctness as a separable signature, enabling a minimalist verifier (CLUE) to outperform text-level and confidence-based methods in reranking and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Assessing the quality of Large Language Model (LLM) outputs presents a critical challenge. Previous methods either rely on text-level information (e.g., reward models, majority voting), which can overfit to superficial cues, or on calibrated confidence from token probabilities, which would fail on less-calibrated models. Yet both of these signals are, in fact, partial projections of a richer source of information: the model's internal hidden states. Early layers, closer to token embeddings, preserve semantic and lexical features that underpin text-based judgments, while later layers increasingly align with output logits, embedding confidence-related information. This paper explores hidden states directly as a unified foundation for verification. We show that the correctness of a solution is encoded as a geometrically separable signature within the trajectory of hidden activations. To validate this, we present Clue (Clustering and Experience-based Verification), a deliberately minimalist, non-parametric verifier. With no trainable parameters, CLUE only summarizes each reasoning trace by an hidden state delta and classifies correctness via nearest-centroid distance to ``success'' and ``failure'' clusters formed from past experience. The simplicity of this method highlights the strength of the underlying signal. Empirically, CLUE consistently outperforms LLM-as-a-judge baselines and matches or exceeds modern confidence-based methods in reranking candidates, improving both top-1 and majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24 with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0% (top-maj@16).", 'score': 20, 'issue_id': 6221, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '5fb07bdb1a94a1b3', 'authors': ['Zhenwen Liang', 'Ruosen Li', 'Yujun Zhou', 'Linfeng Song', 'Dian Yu', 'Xinya Du', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Lab', 'University of Notre Dame', 'University of Texas at Dallas'], 'pdf_title_img': 'assets/pdf/title_img/2510.01591.jpg', 'data': {'categories': ['#rlhf', '#training', '#reasoning', '#interpretability'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ LLM ĞºĞ°Ğº Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ (hidden states) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ğ¼ÑƒÑ ÑĞ¸Ğ³Ğ½Ğ°Ñ‚ÑƒÑ€Ñƒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ CLUE â€” Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ´Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸Ğ· Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´ĞµĞ»ÑŒÑ‚Ñƒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. CLUE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ confidence-based Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ 56.7% Ğ´Ğ¾ 70.0% Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ AIME 24.'}, 'en': {'title': 'Unlocking Hidden States for Accurate LLM Verification', 'desc': 'This paper investigates how hidden states in Large Language Models (LLMs) can be used to assess the correctness of model outputs more effectively than traditional methods. It introduces CLUE, a minimalist verifier that leverages the geometric separability of hidden activations to classify outputs as correct or incorrect without needing trainable parameters. By summarizing reasoning traces with hidden state deltas and using nearest-centroid distance to classify correctness, CLUE demonstrates superior performance over existing text-level and confidence-based approaches. The results show significant improvements in accuracy, highlighting the potential of hidden states as a rich source of information for verification tasks.'}, 'zh': {'title': 'åˆ©ç”¨éšè—çŠ¶æ€æå‡è¯­è¨€æ¨¡å‹çš„éªŒè¯å‡†ç¡®æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…éƒ¨éšè—çŠ¶æ€å¦‚ä½•ç¼–ç æ­£ç¡®æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºCLUEçš„ç®€çº¦éªŒè¯å™¨ã€‚CLUEåˆ©ç”¨éšè—çŠ¶æ€çš„å‡ ä½•å¯åˆ†ç¦»ç‰¹å¾ï¼Œèƒ½å¤Ÿåœ¨é‡æ’åºå’Œå‡†ç¡®æ€§æ–¹é¢è¶…è¶Šä¼ ç»Ÿçš„æ–‡æœ¬çº§å’ŒåŸºäºç½®ä¿¡åº¦çš„æ–¹æ³•ã€‚é€šè¿‡å¯¹éšè—çŠ¶æ€çš„ç›´æ¥åˆ†æï¼ŒCLUEä¸éœ€è¦å¯è®­ç»ƒå‚æ•°ï¼Œä»…é€šè¿‡æ€»ç»“æ¨ç†è½¨è¿¹çš„éšè—çŠ¶æ€å˜åŒ–æ¥è¿›è¡Œåˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLUEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.22067', 'title': 'The Rogue Scalpel: Activation Steering Compromises LLM Safety', 'url': 'https://huggingface.co/papers/2509.22067', 'abstract': "Activation steering, intended to control LLM behavior, can instead increase harmful compliance and undermine model alignment safeguards.  \t\t\t\t\tAI-generated summary \t\t\t\t Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a model's hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign features from a sparse autoencoder (SAE), a common source of interpretable directions, increases these rates by a further 2-4%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.", 'score': 20, 'issue_id': 6230, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': '71a298f2089d76c2', 'authors': ['Anton Korznikov', 'Andrey Galichin', 'Alexey Dontsov', 'Oleg Y. Rogov', 'Ivan Oseledets', 'Elena Tutubalina'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2509.22067.jpg', 'data': {'categories': ['#inference', '#hallucinations', '#interpretability', '#alignment', '#rlhf'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ activation steering â€” Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â€” Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½Ğ°Ñ€ÑƒÑˆĞ°Ñ‚ÑŒ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¸Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ€Ğ¸Ğ½Ğ³Ğ¾Ğ² ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ 0% Ğ´Ğ¾ 27%, Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· sparse autoencoder ÑƒÑÑƒĞ³ÑƒĞ±Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞµÑ‰Ñ‘ Ğ½Ğ° 2-4%. ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ 20 ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ‚Ğ°ĞºÑƒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞµÑ‘ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Activation Steering: A Risky Path to Model Control', 'desc': "This paper investigates the technique of activation steering, which aims to control the behavior of large language models (LLMs) by modifying their hidden states during inference. Contrary to its intended purpose, the study finds that activation steering can actually lead to increased harmful compliance, undermining the safeguards designed to align model behavior with ethical standards. Through various experiments, the authors demonstrate that even random steering can significantly raise the likelihood of the model complying with harmful requests. The findings suggest that having precise control over a model's internal mechanisms does not ensure safe or desirable outcomes, challenging the notion that interpretability equates to safety in AI systems."}, 'zh': {'title': 'æ¿€æ´»å¼•å¯¼ï¼šå®‰å…¨æ€§çš„è¯¯åŒº', 'desc': 'æ¿€æ´»å¼•å¯¼æ˜¯ä¸€ç§æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡Œä¸ºçš„æŠ€æœ¯ï¼Œé€šè¿‡åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„å‘é‡ç›´æ¥æ·»åŠ åˆ°æ¨¡å‹çš„éšè—çŠ¶æ€ä¸­ã€‚å°½ç®¡å®ƒè¢«è§†ä¸ºä¸€ç§ç²¾ç¡®ã€å¯è§£é‡Šä¸”å¯èƒ½æ›´å®‰å…¨çš„æ›¿ä»£å¾®è°ƒçš„æ–¹æ³•ï¼Œä½†æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ¿€æ´»å¼•å¯¼å®é™…ä¸Šä¼šç ´åæ¨¡å‹çš„å¯¹é½å®‰å…¨æœºåˆ¶ï¼Œå¯¼è‡´æ¨¡å‹å¯¹æœ‰å®³è¯·æ±‚çš„é¡ºä»æ€§å¢åŠ ã€‚é€šè¿‡å¯¹ä¸åŒæ¨¡å‹å®¶æ—çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯éšæœºæ–¹å‘çš„å¼•å¯¼ä¹Ÿèƒ½å°†æœ‰å®³é¡ºä»çš„æ¦‚ç‡ä»0%æé«˜åˆ°2-27%ã€‚è¿™äº›ç»“æœæŒ‘æˆ˜äº†é€šè¿‡å¯è§£é‡Šæ€§å®ç°å®‰å…¨æ€§çš„ä¼ ç»Ÿè§‚å¿µï¼Œè¡¨æ˜å¯¹æ¨¡å‹å†…éƒ¨çš„ç²¾ç¡®æ§åˆ¶å¹¶ä¸ä¿è¯å¯¹æ¨¡å‹è¡Œä¸ºçš„ç²¾ç¡®æ§åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02286', 'title': 'Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks', 'url': 'https://huggingface.co/papers/2510.02286', 'abstract': 'DialTree-RPO, an on-policy reinforcement learning framework with tree search, autonomously discovers diverse multi-turn attack strategies against large language models, achieving higher attack success rates and uncovering new attack trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns.', 'score': 19, 'issue_id': 6225, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'fa89fbe7ee7a8552', 'authors': ['Ruohao Guo', 'Afshin Oroojlooy', 'Roshan Sridhar', 'Miguel Ballesteros', 'Alan Ritter', 'Dan Roth'], 'affiliations': ['Georgia Institute of Technology', 'Oracle AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.02286.jpg', 'data': {'categories': ['#rlhf', '#rl', '#security'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° DialTree-RPO, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ reinforcement learning Ğ¸ tree search Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ ÑƒĞ³Ñ€Ğ¾Ğ·Ñƒ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ·Ğ»Ğ¾ÑƒĞ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ½Ğ¸ĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ²Ğ¾Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ‚Ğ°ĞºĞ°Ñ… Ğ¸ Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°. DialTree-RPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°Ğº.'}, 'en': {'title': 'Unleashing Multi-Turn Attack Strategies with DialTree-RPO', 'desc': 'DialTree-RPO is a novel reinforcement learning framework designed to autonomously identify multi-turn attack strategies against large language models (LLMs). By treating dialogue as a sequential decision-making problem, it utilizes tree search to explore diverse attack trajectories without relying on pre-defined templates or human-curated data. This approach significantly enhances the attack success rate, achieving over 25.9% improvement compared to existing methods focused on single-turn attacks. The framework reveals new strategies by learning optimal dialogue policies, addressing a critical gap in AI safety research.'}, 'zh': {'title': 'DialTree-RPOï¼šè‡ªä¸»å‘ç°å¤šè½®æ”»å‡»ç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'DialTree-RPOæ˜¯ä¸€ç§åŸºäºç­–ç•¥çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†æ ‘æœç´¢æŠ€æœ¯ï¼Œèƒ½å¤Ÿè‡ªä¸»å‘ç°å¤šè½®æ”»å‡»ç­–ç•¥ï¼Œé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ”»å‡»ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¯¹è¯è§†ä¸ºä¸€ä¸ªåºåˆ—å†³ç­–é—®é¢˜ï¼Œç³»ç»Ÿæ€§åœ°æ¢ç´¢å¤šè½®æ”»å‡»çš„å¯èƒ½æ€§ï¼Œè€Œæ— éœ€ä¾èµ–äººå·¥æ ‡æ³¨çš„æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDialTree-RPOåœ¨10ä¸ªç›®æ ‡æ¨¡å‹ä¸Šå®ç°äº†è¶…è¿‡25.9%çš„æ”»å‡»æˆåŠŸç‡æå‡ï¼Œå¹¶æœ‰æ•ˆå‘ç°äº†æ–°çš„æ”»å‡»ç­–ç•¥ã€‚æ­¤ç ”ç©¶å¡«è¡¥äº†ç°æœ‰æ–¹æ³•åœ¨å¤šè½®æ”»å‡»é¢†åŸŸçš„ç©ºç™½ï¼Œå±•ç¤ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¯¹è¯ä¸­çš„è„†å¼±æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01444', 'title': 'VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.01444', 'abstract': 'VOGUE, a method that shifts exploration to the visual input space by quantifying policy sensitivity to visual perturbations, enhances multimodal reasoning in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) improves reasoning in large language models (LLMs) but struggles with exploration, an issue that still persists for multimodal LLMs (MLLMs). Current methods treat the visual input as a fixed, deterministic condition, overlooking a critical source of ambiguity and struggling to build policies robust to plausible visual variations. We introduce VOGUE (Visual Uncertainty Guided Exploration), a novel method that shifts exploration from the output (text) to the input (visual) space. By treating the image as a stochastic context, VOGUE quantifies the policy\'s sensitivity to visual perturbations using the symmetric KL divergence between a "raw" and "noisy" branch, creating a direct signal for uncertainty-aware exploration. This signal shapes the learning objective via an uncertainty-proportional bonus, which, combined with a token-entropy bonus and an annealed sampling schedule, effectively balances exploration and exploitation. Implemented within GRPO on two model scales (Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three visual math benchmarks and 3.7% on three general-domain reasoning benchmarks, while simultaneously increasing pass@4 performance and mitigating the exploration decay commonly observed in RL fine-tuning. Our work shows that grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for improving multimodal reasoning.', 'score': 19, 'issue_id': 6221, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '827fe718c8df2c9c', 'authors': ['Rui Liu', 'Dian Yu', 'Tong Zheng', 'Runpeng Dai', 'Zongxia Li', 'Wenhao Yu', 'Zhenwen Liang', 'Linfeng Song', 'Haitao Mi', 'Pratap Tokekar', 'Dong Yu'], 'affiliations': ['Tencent AI Lab, Bellevue, WA', 'University of Maryland, College Park', 'University of North Carolina, Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2510.01444.jpg', 'data': {'categories': ['#training', '#multimodal', '#rl', '#games', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ VOGUE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (exploration) Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ…Ğ¾Ğ´, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚Ñ€Ğ°ĞºÑ‚ÑƒĞµÑ‚ ĞµĞ³Ğ¾ ĞºĞ°Ğº ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½ÑƒÑ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ±Ğ¾Ğ½ÑƒÑ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Qwen2.5-VL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 2.6% Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 3.7% Ğ´Ğ»Ñ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Multimodal Reasoning through Visual Uncertainty Exploration', 'desc': 'The paper introduces VOGUE, a method that enhances multimodal reasoning in large language models by focusing on the visual input space. It addresses the exploration challenges in reinforcement learning with verifiable rewards by quantifying how sensitive a policy is to changes in visual inputs. VOGUE treats images as stochastic contexts, using symmetric KL divergence to measure policy sensitivity and create an uncertainty-aware exploration signal. This approach leads to improved accuracy in reasoning tasks by balancing exploration and exploitation through an uncertainty-proportional bonus and other techniques.'}, 'zh': {'title': 'åŸºäºè§†è§‰ä¸ç¡®å®šæ€§çš„æ¢ç´¢æå‡å¤šæ¨¡æ€æ¨ç†', 'desc': 'VOGUEæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡é‡åŒ–ç­–ç•¥å¯¹è§†è§‰æ‰°åŠ¨çš„æ•æ„Ÿæ€§ï¼Œå°†æ¢ç´¢è½¬ç§»åˆ°è§†è§‰è¾“å…¥ç©ºé—´ï¼Œä»è€Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å°†å›¾åƒè§†ä¸ºéšæœºä¸Šä¸‹æ–‡ï¼Œåˆ©ç”¨å¯¹ç§°KLæ•£åº¦æ¥é‡åŒ–ç­–ç•¥çš„æ•æ„Ÿæ€§ï¼Œåˆ›å»ºä¸€ä¸ªç›´æ¥çš„ä¿¡å·ä»¥æ”¯æŒä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ¢ç´¢ã€‚VOGUEé€šè¿‡ä¸ç¡®å®šæ€§æ¯”ä¾‹å¥–åŠ±ã€ä»¤ç‰Œç†µå¥–åŠ±å’Œé€æ­¥é‡‡æ ·è®¡åˆ’æœ‰æ•ˆå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨è§†è§‰æ•°å­¦åŸºå‡†å’Œä¸€èˆ¬é¢†åŸŸæ¨ç†åŸºå‡†ä¸Šçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°†æ¢ç´¢ä¸è§†è§‰è¾“å…¥çš„å›ºæœ‰ä¸ç¡®å®šæ€§ç»“åˆæ˜¯ä¸€ç§æœ‰æ•ˆçš„å¤šæ¨¡æ€æ¨ç†æ”¹è¿›ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01284', 'title': 'Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation', 'url': 'https://huggingface.co/papers/2510.01284', 'abstract': 'Ovi is a unified audio-video generation model using twin-DiT modules with blockwise cross-modal fusion, enabling natural synchronization and high-quality multimodal outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-video generation has often relied on complex multi-stage architectures or sequential synthesis of sound and visuals. We introduce Ovi, a unified paradigm for audio-video generation that models the two modalities as a single generative process. By using blockwise cross-modal fusion of twin-DiT modules, Ovi achieves natural synchronization and removes the need for separate pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion modeling, we initialize an audio tower with an architecture identical to that of a strong pretrained video model. Trained from scratch on hundreds of thousands of hours of raw audio, the audio tower learns to generate realistic sound effects, as well as speech that conveys rich speaker identity and emotion. Fusion is obtained by jointly training the identical video and audio towers via blockwise exchange of timing (via scaled-RoPE embeddings) and semantics (through bidirectional cross-attention) on a vast video corpus. Our model enables cinematic storytelling with natural speech and accurate, context-matched sound effects, producing movie-grade video clips. All the demos, code and model weights are published at https://aaxwaz.github.io/Ovi', 'score': 19, 'issue_id': 6222, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'ee7ea259a6e9fd1a', 'authors': ['Chetwin Low', 'Weimin Wang', 'Calder Katyal'], 'affiliations': ['Character AI', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2510.01284.jpg', 'data': {'categories': ['#video', '#audio', '#multimodal', '#open_source', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ovi â€” ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… DiT-Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ (twin-DiT) Ñ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ²ÑƒĞºĞ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞÑƒĞ´Ğ¸Ğ¾-Ğ±Ğ°ÑˆĞ½Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ½ÑƒĞ»Ñ Ğ½Ğ° ÑĞ¾Ñ‚Ğ½ÑÑ… Ñ‚Ñ‹ÑÑÑ‡ Ñ‡Ğ°ÑĞ¾Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ²ÑƒĞºĞ¾Ğ²Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ¸ Ñ€ĞµÑ‡ÑŒ Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡ĞµĞ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾. Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ±Ğ°ÑˆĞµĞ½ Ñ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ñ‚Ğ°Ğ¹Ğ¼Ğ¸Ğ½Ğ³Ğµ Ñ‡ĞµÑ€ĞµĞ· scaled-RoPE ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¾Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Ovi: Seamless Audio-Video Generation for Cinematic Storytelling', 'desc': 'Ovi is a novel model designed for generating audio and video together in a seamless way. It uses twin-DiT modules that allow for blockwise cross-modal fusion, which means it can combine sound and visuals more effectively than previous methods. This model learns from a large amount of raw audio to create realistic sounds and speech that match the emotions and identities of speakers. By training both audio and video components together, Ovi produces high-quality, synchronized outputs suitable for cinematic storytelling.'}, 'zh': {'title': 'Oviï¼šéŸ³è§†é¢‘ç”Ÿæˆçš„æ–°èŒƒå¼', 'desc': 'Oviæ˜¯ä¸€ç§ç»Ÿä¸€çš„éŸ³è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨åŒé‡DiTæ¨¡å—å’Œå—çº§è·¨æ¨¡æ€èåˆæŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°è‡ªç„¶çš„åŒæ­¥å’Œé«˜è´¨é‡çš„å¤šæ¨¡æ€è¾“å‡ºã€‚ä¸ä¼ ç»Ÿçš„å¤šé˜¶æ®µæ¶æ„ä¸åŒï¼ŒOviå°†éŸ³é¢‘å’Œè§†é¢‘è§†ä¸ºå•ä¸€çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œç®€åŒ–äº†ç”Ÿæˆæµç¨‹ã€‚è¯¥æ¨¡å‹é€šè¿‡è”åˆè®­ç»ƒéŸ³é¢‘å’Œè§†é¢‘å¡”ï¼Œåˆ©ç”¨æ—¶é—´å’Œè¯­ä¹‰çš„å—çº§äº¤æ¢ï¼Œæå‡äº†å¤šæ¨¡æ€èåˆçš„ç²¾ç»†å»ºæ¨¡èƒ½åŠ›ã€‚æœ€ç»ˆï¼ŒOvièƒ½å¤Ÿç”Ÿæˆå…·æœ‰ç”µå½±çº§åˆ«è´¨é‡çš„éŸ³è§†é¢‘ç‰‡æ®µï¼Œå±•ç°è‡ªç„¶çš„è¯­éŸ³å’Œå‡†ç¡®çš„å£°éŸ³æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02250', 'title': 'The Unreasonable Effectiveness of Scaling Agents for Computer Use', 'url': 'https://huggingface.co/papers/2510.02250', 'abstract': "Behavior Best-of-N (bBoN) improves the reliability and success rates of computer-use agents by generating and selecting among multiple rollouts using behavior narratives, achieving state-of-the-art performance on OSWorld and strong generalization to different operating systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-use agents (CUAs) hold promise for automating everyday digital tasks, but their unreliability and high variance hinder their application to long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method that scales over agents by generating multiple rollouts and selecting among them using behavior narratives that describe the agents' rollouts. It enables both wide exploration and principled trajectory selection, substantially improving robustness and success rates. On OSWorld, our bBoN scaling method establishes a new state of the art (SoTA) at 69.9%, significantly outperforming prior methods and approaching human-level performance at 72%, with comprehensive ablations validating key design choices. We further demonstrate strong generalization results to different operating systems on WindowsAgentArena and AndroidWorld. Crucially, our results highlight the unreasonable effectiveness of scaling CUAs, when you do it right: effective scaling requires structured trajectory understanding and selection, and bBoN provides a practical framework to achieve this.", 'score': 18, 'issue_id': 6222, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'aa05336e77565d70', 'authors': ['Gonzalo Gonzalez-Pumariega', 'Vincent Tu', 'Chih-Lun Lee', 'Jiachen Yang', 'Ang Li', 'Xin Eric Wang'], 'affiliations': ['Simular Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.02250.jpg', 'data': {'categories': ['#agents', '#optimization', '#games'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¸ ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Behavior Best-of-N (bBoN) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ² â€” Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ OSWorld Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° 69.9%, Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑÑŒ Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ 72%. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ â€” ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ bBoN.'}, 'en': {'title': 'Scaling Success: Behavior Best-of-N for Reliable Computer-Use Agents', 'desc': 'The paper presents Behavior Best-of-N (bBoN), a novel approach to enhance the performance of computer-use agents (CUAs) by generating multiple rollouts and selecting the best ones based on behavior narratives. This method allows for extensive exploration of possible actions while ensuring that the most effective trajectories are chosen, leading to improved reliability and success rates in complex tasks. The results show that bBoN achieves state-of-the-art performance on the OSWorld benchmark, nearing human-level effectiveness. Additionally, the method demonstrates strong generalization capabilities across different operating systems, emphasizing the importance of structured trajectory understanding in scaling CUAs effectively.'}, 'zh': {'title': 'è¡Œä¸ºæœ€ä½³é€‰æ‹©ï¼šæå‡è®¡ç®—æœºä»£ç†çš„å¯é æ€§ä¸æˆåŠŸç‡', 'desc': 'è¡Œä¸ºæœ€ä½³é€‰æ‹©ï¼ˆbBoNï¼‰é€šè¿‡ç”Ÿæˆå’Œé€‰æ‹©å¤šä¸ªå›æ»šï¼Œåˆ©ç”¨è¡Œä¸ºå™è¿°æé«˜äº†è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„å¯é æ€§å’ŒæˆåŠŸç‡ã€‚è¯¥æ–¹æ³•åœ¨OSWorldä¸Šè¾¾åˆ°äº†69.9%çš„æ–°çŠ¶æ€ï¼Œæ˜¾è‘—ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼Œå¹¶æ¥è¿‘äººç±»æ°´å¹³çš„72%ã€‚bBoNæ–¹æ³•å…è®¸å¹¿æ³›æ¢ç´¢å’Œæœ‰åŸåˆ™çš„è½¨è¿¹é€‰æ‹©ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†é²æ£’æ€§å’ŒæˆåŠŸç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜å±•ç¤ºäº†bBoNåœ¨ä¸åŒæ“ä½œç³»ç»Ÿä¸Šçš„å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†æœ‰æ•ˆæ‰©å±•è®¡ç®—æœºä½¿ç”¨ä»£ç†çš„åˆç†æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02240', 'title': 'RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.02240', 'abstract': "RewardMap, a multi-stage reinforcement learning framework, enhances multimodal large language models' visual understanding and reasoning skills through dense reward signals and a difficulty-aware reward design.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities.", 'score': 15, 'issue_id': 6223, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'd80bedb4d445e906', 'authors': ['Sicheng Feng', 'Kaiwen Tuo', 'Song Wang', 'Lingdong Kong', 'Jianke Zhu', 'Huan Wang'], 'affiliations': ['National University of Singapore', 'Tongji University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.02240.jpg', 'data': {'categories': ['#reasoning', '#rag', '#dataset', '#rl', '#optimization', '#multimodal', '#benchmark'], 'emoji': 'ğŸ—ºï¸', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RewardMap â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ñ‡ĞµÑ€ĞµĞ· reinforcement learning. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ReasonMap-Plus Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· VQA-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ½Ğ¾Ğ²ÑˆĞµÑÑ‚Ğ²Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 3.47% Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Boosting Visual Reasoning in MLLMs with RewardMap', 'desc': "RewardMap is a multi-stage reinforcement learning framework that improves the visual understanding and reasoning abilities of multimodal large language models (MLLMs). It addresses the challenge of fine-grained visual reasoning by introducing dense reward signals through Visual Question Answering (VQA) tasks, which helps in effective training. The framework features a difficulty-aware reward design that provides richer supervision and tackles the issue of sparse rewards. Experiments show that RewardMap significantly enhances performance across various benchmarks, demonstrating its effectiveness in boosting MLLMs' capabilities in complex reasoning tasks."}, 'zh': {'title': 'æå‡è§†è§‰ç†è§£ä¸æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'RewardMapæ˜¯ä¸€ä¸ªå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¯†é›†å¥–åŠ±ä¿¡å·å’Œéš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡ï¼Œæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ åœ¨å¤æ‚ä»»åŠ¡ä¸­é¢ä¸´çš„ç¨€ç–å¥–åŠ±å’Œä¸ç¨³å®šä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡æ„å»ºReasonMap-Plusæ•°æ®é›†ï¼ŒRewardMapèƒ½å¤Ÿæœ‰æ•ˆåœ°è¿›è¡Œå†·å¯åŠ¨è®­ç»ƒï¼Œå¢å¼ºç»†ç²’åº¦çš„è§†è§‰ç†è§£æŠ€èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRewardMapçš„å„ä¸ªç»„ä»¶å‡èƒ½å¸¦æ¥ä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œç»“åˆä½¿ç”¨æ—¶æ•ˆæœæœ€ä½³ï¼Œæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†3.47%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02294', 'title': 'F2LLM Technical Report: Matching SOTA Embedding Performance with 6\n  Million Open-Source Data', 'url': 'https://huggingface.co/papers/2510.02294', 'abstract': 'F2LLM, a suite of large language models, achieves high embedding performance with efficient fine-tuning from foundation models using open-source datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works.', 'score': 13, 'issue_id': 6221, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'f50d032bbcffbe10', 'authors': ['Ziyin Zhang', 'Zihan Liao', 'Hang Yu', 'Peng Di', 'Rui Wang'], 'affiliations': ['Ant Group', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.02294.jpg', 'data': {'categories': ['#training', '#open_source', '#dataset', '#small_models', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ¸Ğ· foundation Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'F2LLM â€” ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ language models Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 0.6B, 1.7B Ğ¸ 4B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, F2LLM Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ contrastive pretraining Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â€” Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½ÑÑ‚ÑÑ Ğ½Ğ° 6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ñ€Ğ¾ĞµĞº query-document-negative Ğ¸Ğ· Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MTEB Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ F2LLM-4B Ğ·Ğ°Ğ½ÑĞ»Ğ° 2-Ğµ Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ€ĞµĞ´Ğ¸ 4B Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ 7-Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ·Ğ°Ñ‡Ñ‘Ñ‚Ğµ, Ğ° F2LLM-1.7B ÑÑ‚Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ² ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ 1B-2B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»Ğ¸ ĞºĞ¾Ğ´, Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ² Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ baseline Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ².'}, 'en': {'title': 'F2LLM: Efficient Embedding Models for Cost-Effective Performance', 'desc': 'F2LLM is a new suite of large language models designed for efficient embedding performance. It fine-tunes foundation models using a curated dataset of 6 million query-document-negative tuples, avoiding the need for expensive pretraining and synthetic data. The models come in three sizes: 0.6B, 1.7B, and 4B parameters, with the largest model achieving high rankings on the MTEB English leaderboard. By releasing the models and training data, F2LLM aims to provide a cost-effective and reproducible baseline for future research in machine learning.'}, 'zh': {'title': 'F2LLMï¼šé«˜æ•ˆåµŒå…¥çš„åŸºç¡€æ¨¡å‹å¾®è°ƒ', 'desc': 'F2LLMæ˜¯ä¸€å¥—å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¸“æ³¨äºé«˜æ•ˆçš„åµŒå…¥æ€§èƒ½ã€‚å®ƒé€šè¿‡å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨å¼€æ”¾æºä»£ç æ•°æ®é›†ï¼Œé¿å…äº†ä»¥å¾€æ¨¡å‹éœ€è¦çš„å¤§è§„æ¨¡å¯¹æ¯”é¢„è®­ç»ƒå’Œå¤æ‚çš„è®­ç»ƒæµç¨‹ã€‚F2LLMæä¾›äº†ä¸‰ç§ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼Œåˆ†åˆ«ä¸º0.6Bã€1.7Bå’Œ4Bï¼Œå¹¶åœ¨MTEBè‹±è¯­æ’è¡Œæ¦œä¸Šè¡¨ç°ä¼˜å¼‚ã€‚ä¸ºäº†æ¨åŠ¨æœªæ¥çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æ¨¡å‹ã€è®­ç»ƒæ•°æ®é›†å’Œä»£ç ï¼Œæ—¨åœ¨ä¸ºåç»­å·¥ä½œæä¾›ä¸€ä¸ªå¼ºå¤§ä¸”ç»æµå®æƒ çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02190', 'title': 'A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports', 'url': 'https://huggingface.co/papers/2510.02190', 'abstract': 'A benchmark and evaluation framework for Deep Research Agents (DRAs) assesses their performance on complex tasks with multidimensional metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable of external perception and information integration. As a representative embodiment, Deep Research Agents (DRAs) systematically exhibit the capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output, which markedly enhance performance on complex and open-ended tasks. However, existing benchmarks remain deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to assess such systems effectively. This paper introduces a rigorous benchmark and a multidimensional evaluation framework tailored to DRAs and report-style responses. The benchmark comprises 214 expert-curated challenging queries distributed across 10 broad thematic domains, each accompanied by manually constructed reference bundles to support composite evaluation. The framework enables comprehensive evaluation of long-form reports generated by DRAs, incorporating integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness. Extensive experimentation confirms the superior performance of mainstream DRAs over web-search-tool-augmented reasoning models, yet reveals considerable scope for further improvement. This study provides a robust foundation for capability assessment, architectural refinement, and paradigm advancement in DRA systems.', 'score': 13, 'issue_id': 6221, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '98260d7be8c3395a', 'authors': ['Yang Yao', 'Yixu Wang', 'Yuxuan Zhang', 'Yi Lu', 'Tianle Gu', 'Lingyu Li', 'Dingyi Zhao', 'Keming Wu', 'Haozhe Wang', 'Ping Nie', 'Yan Teng', 'Yingchun Wang'], 'affiliations': ['Fudan University', 'Hong Kong University of Science and Technology', 'Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The University of Hong Kong', 'Tsinghua University', 'University of British Columbia', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2510.02190.jpg', 'data': {'categories': ['#benchmark', '#agents', '#evaluation', '#optimization', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Deep Research Agents (DRA) â€” AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¿Ğ¾Ğ¸ÑĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 214 ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸Ğ· 10 Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ, Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ LLM Ñ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, Ğ½Ğ¾ Ğ²ÑÑ‘ ĞµÑ‰Ñ‘ Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Evaluation for Deep Research Agents', 'desc': 'This paper presents a new benchmark and evaluation framework specifically designed for Deep Research Agents (DRAs), which are advanced AI systems capable of handling complex tasks. The framework includes 214 challenging queries across various themes and offers a multidimensional approach to assess the performance of DRAs based on metrics like semantic quality and retrieval trustworthiness. It highlights the limitations of existing benchmarks in evaluating DRAs and proposes a structured method for comprehensive assessment. The findings indicate that while DRAs outperform traditional web-search models, there is still significant room for improvement in their capabilities.'}, 'zh': {'title': 'æ·±åº¦ç ”ç©¶ä»£ç†çš„è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹æ·±åº¦ç ”ç©¶ä»£ç†ï¼ˆDRAï¼‰çš„åŸºå‡†å’Œè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤šç»´åº¦æŒ‡æ ‡è¯„ä¼°å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚DRAèƒ½å¤Ÿè¿›è¡Œä»»åŠ¡åˆ†è§£ã€è·¨æºæ£€ç´¢ã€å¤šé˜¶æ®µæ¨ç†å’Œç»“æ„åŒ–è¾“å‡ºï¼Œæ˜¾è‘—æå‡äº†åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç°æœ‰çš„è¯„ä¼°åŸºå‡†åœ¨è¯„ä¼°ç»´åº¦ã€å“åº”æ ¼å¼å’Œè¯„åˆ†æœºåˆ¶ä¸Šå­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å¯¹è¿™äº›ç³»ç»Ÿçš„æœ‰æ•ˆè¯„ä¼°ã€‚æœ¬æ–‡çš„æ¡†æ¶åŒ…å«214ä¸ªä¸“å®¶ç­–åˆ’çš„æŒ‘æˆ˜æ€§æŸ¥è¯¢ï¼Œæ”¯æŒå¯¹DRAç”Ÿæˆçš„é•¿æ ¼å¼æŠ¥å‘Šè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæä¾›è¯­ä¹‰è´¨é‡ã€ä¸»é¢˜èšç„¦å’Œæ£€ç´¢å¯ä¿¡åº¦çš„ç»¼åˆè¯„åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02173', 'title': 'Learning to Reason for Hallucination Span Detection', 'url': 'https://huggingface.co/papers/2510.02173', 'abstract': 'A reinforcement learning framework with span-level rewards improves hallucination span detection in large language models by incentivizing reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.', 'score': 12, 'issue_id': 6225, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '671b472906bfa9c1', 'authors': ['Hsuan Su', 'Ting-Yao Hu', 'Hema Swetha Koppula', 'Kundan Krishna', 'Hadi Pouransari', 'Cheng-Yu Hsieh', 'Cem Koc', 'Joseph Yitan Cheng', 'Oncel Tuzel', 'Raviteja Vemulapalli'], 'affiliations': ['Apple', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.02173.jpg', 'data': {'categories': ['#rlhf', '#optimization', '#hallucinations', '#reasoning', '#benchmark', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Reinforcement learning ÑƒÑ‡Ğ¸Ñ‚ LLM Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ', 'desc': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ - Ğ½ĞµĞ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ñ‘Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ RL4HS - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Chain-of-Thought Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Class-Aware Policy Optimization Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ RAGTruth Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ reasoning.'}, 'en': {'title': 'Reinforcement Learning for Better Hallucination Span Detection', 'desc': 'This paper presents a reinforcement learning framework called RL4HS that enhances the detection of hallucination spans in large language models (LLMs). Unlike traditional binary detection methods, RL4HS focuses on identifying specific spans of hallucinated content, which requires more complex reasoning. The framework uses a span-level reward system to encourage better reasoning processes, leveraging Chain-of-Thought (CoT) techniques. Experimental results indicate that RL4HS outperforms existing models, highlighting the effectiveness of reinforcement learning in improving hallucination detection.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ åŠ©åŠ›å¹»è§‰æ£€æµ‹çš„çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è·¨åº¦çº§å¥–åŠ±æ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰è·¨åº¦æ£€æµ‹ã€‚ä¼ ç»Ÿçš„å¹»è§‰æ£€æµ‹é€šå¸¸è¢«è§†ä¸ºäºŒå…ƒä»»åŠ¡ï¼Œä½†è®¸å¤šå®é™…åº”ç”¨éœ€è¦è¯†åˆ«å¹»è§‰çš„å…·ä½“éƒ¨åˆ†ï¼Œè¿™æ˜¯ä¸€ç§å¤šæ­¥éª¤çš„å†³ç­–è¿‡ç¨‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†èƒ½å¤Ÿåœ¨å¤šæ¬¡é‡‡æ ·ä¸­ç”Ÿæˆè‡³å°‘ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†RL4HSæ¡†æ¶ï¼Œä»¥æ¿€åŠ±æ¨ç†å¹¶è§£å†³å¥–åŠ±ä¸å¹³è¡¡é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRL4HSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†é¢„è®­ç»ƒæ¨ç†æ¨¡å‹å’Œç›‘ç£å¾®è°ƒï¼Œè¯æ˜äº†ä½¿ç”¨è·¨åº¦çº§å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ åœ¨å¹»è§‰è·¨åº¦æ£€æµ‹ä¸­çš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01179', 'title': 'TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP\n  Environments', 'url': 'https://huggingface.co/papers/2510.01179', 'abstract': 'Toucan, a large publicly available tool-agentic dataset, enhances the performance of LLM agents by providing diverse, realistic, and complex multi-tool and multi-turn interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench.', 'score': 12, 'issue_id': 6221, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '30b0ec9c798b87c6', 'authors': ['Zhangchen Xu', 'Adriana Meza Soria', 'Shawn Tan', 'Anurag Roy', 'Ashish Sunil Agrawal', 'Radha Poovendran', 'Rameswar Panda'], 'affiliations': ['MIT-IBM Watson AI Lab', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2510.01179.jpg', 'data': {'categories': ['#open_source', '#synthetic', '#agents', '#benchmark', '#dataset'], 'emoji': 'ğŸ¦œ', 'ru': {'title': 'Toucan: ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Toucan â€” ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1,5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ 500 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Model Context Protocols (MCP) Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ñ‚Ñ€ĞµĞ¼Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Toucan, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ BFCL V3 Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° MCP-Universe Bench.'}, 'en': {'title': 'Toucan: Elevating LLM Agents with Diverse Tool Interactions', 'desc': 'Toucan is a large dataset designed to improve the performance of Large Language Model (LLM) agents by providing a wide variety of realistic and complex interactions involving multiple tools and turns. It addresses the limitations of existing datasets, which often lack diversity and realism, by synthesizing 1.5 million trajectories from nearly 500 real-world Model Context Protocols (MCPs). The dataset generation process includes quality filtering and the use of multiple models to ensure high-quality outputs, along with mechanisms to diversify tasks and simulate multi-turn conversations. Models trained on Toucan have shown superior performance compared to larger closed-source models on established benchmarks, demonstrating its effectiveness in advancing the capabilities of LLM agents.'}, 'zh': {'title': 'Toucanï¼šæå‡LLMä»£ç†æ€§èƒ½çš„å…³é”®æ•°æ®é›†', 'desc': 'Toucanæ˜¯ä¸€ä¸ªå¤§å‹çš„å…¬å¼€å¯ç”¨å·¥å…·ä»£ç†æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ€§èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«150ä¸‡ä¸ªè½¨è¿¹ï¼Œæ¥æºäºè¿‘500ä¸ªçœŸå®çš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ï¼Œæä¾›å¤šæ ·åŒ–ã€çœŸå®ä¸”å¤æ‚çš„å¤šå·¥å…·å’Œå¤šè½®äº¤äº’ã€‚Toucané€šè¿‡çœŸå®çš„MCPç¯å¢ƒç”Ÿæˆä»»åŠ¡ï¼Œç¡®ä¿äº†æ•°æ®çš„å¤šæ ·æ€§å’ŒæŒ‘æˆ˜æ€§ã€‚ç»è¿‡ä¸¥æ ¼çš„è§„åˆ™å’Œæ¨¡å‹éªŒè¯ï¼ŒToucanç”Ÿæˆçš„é«˜è´¨é‡è¾“å‡ºä½¿å¾—åœ¨BFCL V3åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ›´å¤§çš„å°é—­æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02253', 'title': 'DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing', 'url': 'https://huggingface.co/papers/2510.02253', 'abstract': "DragFlow leverages FLUX's strong generative priors and region-based editing with affine transformations to achieve state-of-the-art performance in drag-based image editing.  \t\t\t\t\tAI-generated summary \t\t\t\t Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.", 'score': 10, 'issue_id': 6222, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'd10d645e2e301186', 'authors': ['Zihan Zhou', 'Shilin Lu', 'Shuli Leng', 'Shaocong Zhang', 'Zhuming Lian', 'Xinlei Yu', 'Adams Wai-Kin Kong'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2510.02253.jpg', 'data': {'categories': ['#dataset', '#cv', '#multimodal', '#benchmark', '#open_source', '#architecture'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ FLUX: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': "DragFlow â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ (drag-editing), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ prior'Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FLUX Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ DiT Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ DiT, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ pretrained Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ LLM Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… DragBench-DR Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ReD Bench Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² drag-based Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹."}, 'en': {'title': 'Revolutionizing Drag-Based Image Editing with DragFlow', 'desc': "DragFlow is a new framework that improves drag-based image editing by using advanced generative models called FLUX. Traditional methods struggled with distortions because earlier models like Stable Diffusion couldn't accurately map edited images back to their original forms. DragFlow introduces a region-based editing approach that uses affine transformations for better feature supervision, making the editing process more reliable. By integrating personalization adapters and multimodal language models, DragFlow achieves significant improvements over previous methods, setting a new standard in the field of image editing."}, 'zh': {'title': 'DragFlowï¼šæ‹–æ‹½å¼å›¾åƒç¼–è¾‘çš„æ–°çªç ´', 'desc': 'DragFlow æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œåˆ©ç”¨ FLUX çš„å¼ºç”Ÿæˆå…ˆéªŒå’ŒåŸºäºåŒºåŸŸçš„ç¼–è¾‘æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ‹–æ‹½å¼å›¾åƒç¼–è¾‘çš„æ•ˆæœã€‚ä¼ ç»Ÿçš„æ‹–æ‹½ç¼–è¾‘å¸¸å¸¸å¯¼è‡´ç›®æ ‡åŒºåŸŸçš„å¤±çœŸï¼Œå› ä¸ºæ—©æœŸæ¨¡å‹çš„å…ˆéªŒä¸è¶³ä»¥å°†ä¼˜åŒ–åçš„æ½œåœ¨è¡¨ç¤ºå‡†ç¡®æ˜ å°„åˆ°è‡ªç„¶å›¾åƒä¸Šã€‚DragFlow é€šè¿‡å¼•å…¥ä»¿å°„å˜æ¢çš„åŒºåŸŸç¼–è¾‘èŒƒå¼ï¼Œæä¾›äº†æ›´ä¸°å¯Œå’Œä¸€è‡´çš„ç‰¹å¾ç›‘ç£ï¼Œä»è€Œå…‹æœäº†ç‚¹åŸºæ‹–æ‹½ç¼–è¾‘çš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDragFlow åœ¨æ‹–æ‹½å¼å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„åŸºçº¿ï¼Œè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01817', 'title': 'Sparse Query Attention (SQA): A Computationally Efficient Attention\n  Mechanism with Query Heads Reduction', 'url': 'https://huggingface.co/papers/2510.01817', 'abstract': 'Sparse Query Attention (SQA) reduces computational complexity in Transformer models by decreasing the number of Query heads, leading to significant throughput improvements with minimal impact on model quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents a significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains a critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), a novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by a factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and a family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only a minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, suggesting its potential as a powerful tool for building more efficient and scalable models', 'score': 10, 'issue_id': 6236, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '2733653ecf786568', 'authors': ['Adam Filipek'], 'affiliations': ['Reactive AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.01817.jpg', 'data': {'categories': ['#long_context', '#training', '#architecture', '#optimization', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞœĞµĞ½ÑŒÑˆĞµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² â€” Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Transformer Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Sparse Query Attention (SQA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Query-Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Key/Value Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹ (FLOPs), Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… (32-200 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²) Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 3 Ñ€Ğ°Ğ· Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. SQA Ğ±Ñ‹Ğ» Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Reactive Transformer Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Boosting Transformer Efficiency with Sparse Query Attention', 'desc': "Sparse Query Attention (SQA) is a new approach that enhances Transformer models by reducing the number of Query heads, which helps lower the computational complexity of the attention mechanism. This reduction leads to significant improvements in throughput, especially for tasks involving long sequences, without greatly affecting the model's performance. Unlike previous methods that focused on sharing Key and Value projections, SQA directly decreases the number of floating-point operations (FLOPs) needed for attention score calculations. Empirical results show that SQA can improve computation speed by up to 3 times in demanding scenarios, making it a promising solution for more efficient AI models."}, 'zh': {'title': 'ç¨€ç–æŸ¥è¯¢æ³¨æ„åŠ›ï¼šæå‡Transformeræ•ˆç‡çš„åˆ©å™¨', 'desc': 'ç¨€ç–æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆSQAï¼‰é€šè¿‡å‡å°‘æŸ¥è¯¢å¤´çš„æ•°é‡æ¥é™ä½Transformeræ¨¡å‹çš„è®¡ç®—å¤æ‚æ€§ï¼Œä»è€Œåœ¨ä¿æŒæ¨¡å‹è´¨é‡çš„åŒæ—¶æ˜¾è‘—æé«˜äº†ååé‡ã€‚ä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†é•¿åºåˆ—æ—¶é¢ä¸´äºŒæ¬¡è®¡ç®—å¤æ‚åº¦çš„æŒ‘æˆ˜ï¼Œè€ŒSQAé€šè¿‡ä¼˜åŒ–æŸ¥è¯¢å¤´çš„æ•°é‡ï¼Œç›´æ¥é™ä½äº†æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è´Ÿæ‹…ã€‚è¯¥è®ºæ–‡æä¾›äº†SQAçš„ç†è®ºåŸºç¡€ã€æ•°å­¦å…¬å¼ä»¥åŠä¸€ç³»åˆ—æ¶æ„å˜ä½“çš„ä»‹ç»ã€‚å®éªŒè¯æ˜ï¼Œåœ¨é•¿åºåˆ—å¤„ç†æ—¶ï¼ŒSQAåœ¨è®¡ç®—å¯†é›†å‹åœºæ™¯ä¸­å¯å®ç°é«˜è¾¾3å€çš„ååé‡æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01346', 'title': 'Aristotle: IMO-level Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2510.01346', 'abstract': 'Aristotle, an AI system combining formal verification and informal reasoning, achieves top performance on International Mathematical Olympiad problems using Lean proof search, lemma generation, and a geometry solver.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Aristotle, an AI system that combines formal verification with informal reasoning, achieving gold-medal-equivalent performance on the 2025 International Mathematical Olympiad problems. Aristotle integrates three main components: a Lean proof search system, an informal reasoning system that generates and formalizes lemmas, and a dedicated geometry solver. Our system demonstrates state-of-the-art performance with favorable scaling properties for automated theorem proving.', 'score': 9, 'issue_id': 6237, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'b8737df2c245ef48', 'authors': ['Tudor Achim', 'Alex Best', 'Kevin Der', 'MathÃ¯s FÃ©dÃ©rico', 'Sergei Gukov', 'Daniel Halpern-Leister', 'Kirsten Henningsgard', 'Yury Kudryashov', 'Alexander Meiburg', 'Martin Michelsen', 'Riley Patterson', 'Eric Rodriguez', 'Laura Scharff', 'Vikram Shanker', 'Vladmir Sicca', 'Hari Sowrirajan', 'Aidan Swope', 'Matyas Tamas', 'Vlad Tenev', 'Jonathan Thomm', 'Harold Williams', 'Lawrence Wu'], 'affiliations': ['ByteDance', 'Google Deepmind', 'OpenAI', 'harmonic.fun'], 'pdf_title_img': 'assets/pdf/title_img/2510.01346.jpg', 'data': {'categories': ['#optimization', '#architecture', '#math', '#reasoning'], 'emoji': 'ğŸ¥‡', 'ru': {'title': 'Aristotle: AI-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Aristotle, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€Ñ‘Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ğ¸ĞºĞ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ² Lean, Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»ĞµĞ¼Ğ¼, Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Aristotle Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ Ğ¼ĞµĞ´Ğ°Ğ»Ğ¸ Ğ½Ğ° Ğ¾Ğ»Ğ¸Ğ¼Ğ¿Ğ¸Ğ°Ğ´Ğµ IMO 2025. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼.'}, 'en': {'title': 'Aristotle: Bridging Formal Verification and Informal Reasoning for Mathematical Excellence', 'desc': 'Aristotle is an advanced AI system that merges formal verification techniques with informal reasoning strategies. It excels in solving complex problems, achieving results comparable to gold medalists in the International Mathematical Olympiad. The system utilizes a Lean proof search for rigorous verification, generates and formalizes lemmas for informal reasoning, and includes a specialized geometry solver. This combination allows Aristotle to perform exceptionally well in automated theorem proving, showcasing its scalability and efficiency.'}, 'zh': {'title': 'é˜¿é‡Œå£«å¤šå¾·ï¼šç»“åˆå½¢å¼éªŒè¯ä¸éæ­£å¼æ¨ç†çš„AIç³»ç»Ÿ', 'desc': 'Aristotleæ˜¯ä¸€ä¸ªç»“åˆäº†å½¢å¼éªŒè¯å’Œéæ­£å¼æ¨ç†çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚å®ƒåœ¨2025å¹´å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹é—®é¢˜ä¸Šè¾¾åˆ°äº†é‡‘ç‰Œæ°´å¹³çš„è¡¨ç°ã€‚è¯¥ç³»ç»Ÿé›†æˆäº†ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šLeanè¯æ˜æœç´¢ç³»ç»Ÿã€ç”Ÿæˆå’Œå½¢å¼åŒ–å¼•ç†çš„éæ­£å¼æ¨ç†ç³»ç»Ÿï¼Œä»¥åŠä¸“é—¨çš„å‡ ä½•æ±‚è§£å™¨ã€‚Aristotleå±•ç¤ºäº†åœ¨è‡ªåŠ¨å®šç†è¯æ˜æ–¹é¢çš„æœ€å…ˆè¿›æ€§èƒ½ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02295', 'title': 'VideoNSA: Native Sparse Attention Scales Video Understanding', 'url': 'https://huggingface.co/papers/2510.02295', 'abstract': 'VideoNSA, an adaptation of Native Sparse Attention to video-language models, enhances long-video understanding and temporal reasoning through end-to-end training and a hardware-aware hybrid attention approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks.', 'score': 8, 'issue_id': 6226, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'fa989de618e49801', 'authors': ['Enxin Song', 'Wenhao Chai', 'Shusheng Yang', 'Ethan Armand', 'Xiaojun Shan', 'Haiyang Xu', 'Jianwen Xie', 'Zhuowen Tu'], 'affiliations': ['Lambda, Inc', 'New York University', 'Princeton University', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2510.02295.jpg', 'data': {'categories': ['#video', '#reasoning', '#long_context', '#architecture', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VideoNSA â€” Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Native Sparse Attention Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° end-to-end Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¸Ğ· 216K Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Qwen2.5-VL. VideoNSA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ baseline-Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ¾ ÑĞ¶Ğ°Ñ‚Ğ¸ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'Enhancing Long-Video Understanding with VideoNSA', 'desc': 'VideoNSA is a novel approach that enhances video-language models by integrating Native Sparse Attention (NSA) for better long-video comprehension and temporal reasoning. It utilizes end-to-end training on a large dataset of video instructions, allowing the model to learn effectively from diverse video content. The method combines dense attention for text with sparse attention for video, optimizing performance on tasks that require understanding over extended time frames. Through various experiments, VideoNSA demonstrates significant improvements in handling long videos and maintaining coherence, outperforming traditional methods in several benchmarks.'}, 'zh': {'title': 'è§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šVideoNSA', 'desc': 'VideoNSAæ˜¯ä¸€ç§å°†åŸç”Ÿç¨€ç–æ³¨æ„åŠ›ï¼ˆNSAï¼‰åº”ç”¨äºè§†é¢‘è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºé•¿è§†é¢‘ç†è§£å’Œæ—¶é—´æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒï¼ŒVideoNSAåœ¨ä¸€ä¸ªåŒ…å«216Kè§†é¢‘æŒ‡ä»¤çš„æ•°æ®é›†ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œé‡‡ç”¨ç¡¬ä»¶æ„ŸçŸ¥çš„æ··åˆæ³¨æ„åŠ›ç­–ç•¥ã€‚è¯¥æ–¹æ³•åœ¨æ–‡æœ¬ä¸Šä¿æŒå¯†é›†æ³¨æ„åŠ›ï¼Œè€Œåœ¨è§†é¢‘ä¸Šä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›ï¼Œä»è€Œæé«˜äº†é•¿è§†é¢‘ç†è§£å’Œæ—¶é—´æ¨ç†çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoNSAåœ¨å¤„ç†128Kæ ‡è®°æ—¶è¡¨ç°å¯é ï¼Œå¹¶ä¸”åœ¨å…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›åˆ†é…ä¸Šè¾¾åˆ°äº†æœ€ä½³æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26376', 'title': 'Go with Your Gut: Scaling Confidence for Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2509.26376', 'abstract': 'ScalingAR enhances next-token prediction in autoregressive image generation by using token entropy and adaptive scaling, improving model performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as a novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams a calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios.', 'score': 8, 'issue_id': 6222, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'd720c352a15d85b1', 'authors': ['Harold Haodong Chen', 'Xianfeng Wu', 'Wen-Jie Shu', 'Rongjin Guo', 'Disen Lan', 'Harry Yang', 'Ying-Cong Chen'], 'affiliations': ['CityUHK', 'FDU', 'HKUST', 'HKUST(GZ)', 'PolyU'], 'pdf_title_img': 'assets/pdf/title_img/2509.26376.jpg', 'data': {'categories': ['#data', '#optimization', '#training', '#benchmark', '#architecture'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ScalingAR â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº test-time scaling Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ°Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…: ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ (ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸) Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ (Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹). ScalingAR Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… reward-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ NTP-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° 12.5% Ğ½Ğ° GenEval Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 62% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Enhancing Image Generation with Adaptive Scaling and Token Entropy', 'desc': 'ScalingAR is a novel framework that enhances next-token prediction in autoregressive image generation by utilizing token entropy and adaptive scaling techniques. It addresses the limitations of existing test-time scaling methods that are not suitable for visual autoregressive tasks, which often struggle with incomplete intermediate results. By operating at two levelsâ€”Profile Level and Policy Levelâ€”ScalingAR effectively manages confidence states and optimizes the generation process. Experimental results demonstrate significant improvements in model performance and efficiency, showcasing its ability to reduce token consumption while increasing robustness in challenging scenarios.'}, 'zh': {'title': 'ScalingARï¼šæå‡è‡ªå›å½’å›¾åƒç”Ÿæˆçš„ä¸‹ä¸€æ ‡è®°é¢„æµ‹', 'desc': 'ScalingAR æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ã€‚å®ƒé€šè¿‡åˆ©ç”¨æ ‡è®°ç†µä½œä¸ºä¿¡å·ï¼Œå¹¶åœ¨ä¸¤ä¸ªäº’è¡¥çš„ç¼©æ”¾å±‚é¢ä¸Šæ“ä½œï¼Œæ¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•æ¶ˆé™¤äº†å¯¹æ—©æœŸè§£ç å’Œå¤–éƒ¨å¥–åŠ±çš„éœ€æ±‚ï¼Œä¸“é—¨é’ˆå¯¹åŸºäºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹çš„å›¾åƒç”Ÿæˆè¿›è¡Œä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒScalingAR åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ï¼ŒåŒæ—¶æœ‰æ•ˆå‡å°‘äº†è§†è§‰æ ‡è®°çš„æ¶ˆè€—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.22582', 'title': 'Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs', 'url': 'https://huggingface.co/papers/2509.22582', 'abstract': "A study evaluates the effectiveness of large language models in identifying context-grounded hallucinations using a newly constructed benchmark and free-form textual descriptions, revealing challenges in distinguishing between missing details and unverifiable information.  \t\t\t\t\tAI-generated summary \t\t\t\t Context-grounded hallucinations are cases where model outputs contain information not verifiable against the source text. We study the applicability of LLMs for localizing such hallucinations, as a more practical alternative to existing complex evaluation pipelines. In the absence of established benchmarks for meta-evaluation of hallucinations localization, we construct one tailored to LLMs, involving a challenging human annotation of over 1,000 examples. We complement the benchmark with an LLM-based evaluation protocol, verifying its quality in a human evaluation. Since existing representations of hallucinations limit the types of errors that can be expressed, we propose a new representation based on free-form textual descriptions, capturing the full range of possible errors. We conduct a comprehensive study, evaluating four large-scale LLMs, which highlights the benchmark's difficulty, as the best model achieves an F1 score of only 0.67. Through careful analysis, we offer insights into optimal prompting strategies for the task and identify the main factors that make it challenging for LLMs: (1) a tendency to incorrectly flag missing details as inconsistent, despite being instructed to check only facts in the output; and (2) difficulty with outputs containing factually correct information absent from the source - and thus not verifiable - due to alignment with the model's parametric knowledge.", 'score': 8, 'issue_id': 6238, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': '8f7449cbd0e3d9c0', 'authors': ['Yehonatan Peisakhovsky', 'Zorik Gekhman', 'Yosi Mass', 'Liat Ein-Dor', 'Roi Reichart'], 'affiliations': ['IBM Research', 'Technion - Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.22582.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#hallucinations', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞ¾Ğ¸ÑĞº Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: ĞºĞ¾Ğ³Ğ´Ğ° LLM Ğ¿ÑƒÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ â€” ÑĞ»ÑƒÑ‡Ğ°Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ÑƒÑ Ğ¿Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 1000 Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° F1-score Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 0.67. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ â€” Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ğ¾Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ĞºĞ°Ğº Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ñ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½ÑƒÑ, Ğ½Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ÑƒÑ Ğ¿Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºÑƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ÑĞ²Ğ¾Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unveiling the Limits of LLMs in Detecting Hallucinations', 'desc': "This study investigates how well large language models (LLMs) can detect context-grounded hallucinations, which are inaccuracies in generated text that cannot be verified against the original source. The researchers created a new benchmark with over 1,000 examples to evaluate LLMs' performance in identifying these hallucinations, as existing methods were found to be too complex. They also introduced a novel representation of hallucinations using free-form textual descriptions to better capture various types of errors. The findings reveal that even the best-performing model only achieved an F1 score of 0.67, highlighting significant challenges in distinguishing between missing details and unverifiable information."}, 'zh': {'title': 'è¯†åˆ«ä¸Šä¸‹æ–‡å¹»è§‰çš„æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«ä¸Šä¸‹æ–‡åŸºç¡€å¹»è§‰æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨æ–°æ„å»ºçš„åŸºå‡†å’Œè‡ªç”±å½¢å¼æ–‡æœ¬æè¿°ã€‚ä¸Šä¸‹æ–‡åŸºç¡€å¹»è§‰æ˜¯æŒ‡æ¨¡å‹è¾“å‡ºåŒ…å«æ— æ³•ä¸æºæ–‡æœ¬æ ¸å®çš„ä¿¡æ¯ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªä¸“é—¨é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºå‡†ï¼Œå¹¶è¿›è¡Œäº†è¶…è¿‡1000ä¸ªç¤ºä¾‹çš„äººç±»æ³¨é‡Šï¼Œä»¥éªŒè¯å…¶è´¨é‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹çš„F1åˆ†æ•°ä»…ä¸º0.67ï¼Œæ­ç¤ºäº†è¯†åˆ«å¹»è§‰çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¼˜åŒ–æç¤ºç­–ç•¥çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01304', 'title': 'Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and\n  Reasoning in Vision-Language Models', 'url': 'https://huggingface.co/papers/2510.01304', 'abstract': 'AGILE, an interactive jigsaw-solving framework, enhances visual perception and reasoning in Vision-Language Models through iterative action and feedback, improving performance on jigsaw tasks and general vision tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 times 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .', 'score': 7, 'issue_id': 6224, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '1b3bf32d6b7a84f7', 'authors': ['Yu Zeng', 'Wenxuan Huang', 'Shiting Huang', 'Xikun Bao', 'Yukun Qi', 'Yiming Zhao', 'Qiuchen Wang', 'Lin Chen', 'Zehui Chen', 'Huaian Chen', 'Wanli Ouyang', 'Feng Zhao'], 'affiliations': ['East China Normal University', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.01304.jpg', 'data': {'categories': ['#cv', '#rl', '#agents', '#reasoning', '#optimization', '#multimodal'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ VLM Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ±Ğ¾Ñ€ĞºÑƒ Ğ¿Ğ°Ğ·Ğ»Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AGILE â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Vision-Language Models Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡-Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº (jigsaw puzzles) Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ â€” Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… 2Ã—2 Ğ²Ñ‹Ñ€Ğ¾ÑĞ»Ğ° Ñ 9.5% Ğ´Ğ¾ 82.8%. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° 9 Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ¾ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° 3.1%, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ´ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'AGILE: Enhancing Vision-Language Models through Interactive Learning', 'desc': "AGILE is a framework designed to improve the visual perception and reasoning skills of Vision-Language Models (VLMs) through an interactive jigsaw-solving process. It allows models to engage with their environment iteratively, generating actions based on their current state and receiving detailed visual feedback. This method significantly enhances the model's performance on jigsaw tasks, with accuracy improvements from 9.5% to 82.8%, and also boosts generalization across various vision tasks. By addressing the limitations of existing VLMs, AGILE provides a scalable solution to enhance multimodal learning capabilities."}, 'zh': {'title': 'AGILEï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ„ŸçŸ¥ä¸æ¨ç†èƒ½åŠ›', 'desc': 'AGILEæ˜¯ä¸€ä¸ªäº¤äº’å¼æ‹¼å›¾è§£å†³æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¿­ä»£çš„è¡ŒåŠ¨å’Œåé¦ˆæ¥å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†æ‹¼å›¾è§£å†³è¿‡ç¨‹è§†ä¸ºä¸€ä¸ªäº’åŠ¨è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ­¥ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚æ¯ä¸€æ­¥ï¼Œæ¨¡å‹æ ¹æ®å½“å‰çŠ¶æ€ç”Ÿæˆå¯æ‰§è¡Œä»£ç ä»¥æ‰§è¡ŒåŠ¨ä½œï¼ŒåŒæ—¶ç¯å¢ƒæä¾›ç»†è‡´çš„è§†è§‰åé¦ˆä»¥æŒ‡å¯¼ä»»åŠ¡å®Œæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAGILEæ˜¾è‘—æé«˜äº†æ‹¼å›¾ä»»åŠ¡çš„è¡¨ç°ï¼Œå¹¶åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21789', 'title': 'Visual Multi-Agent System: Mitigating Hallucination Snowballing via\n  Visual Flow', 'url': 'https://huggingface.co/papers/2509.21789', 'abstract': 'ViF mitigates visual hallucination snowballing in Multi-Agent Systems by enhancing visual attention and message relay through selected visual tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in a single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information. Through turn-, layer-, and token-wise attention analyses, we provide detailed insights into the essence of hallucination snowballing regarding the reduction of visual attention allocation. It leads us to identify a subset of vision tokens with a unimodal attention peak in middle layers that best preserve visual evidence but gradually diminish in deeper agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we propose ViF, a lightweight, plug-and-play mitigation paradigm that relays inter-agent messages with Visual Flow powered by the selected visual relay tokens and applies attention reallocation to amplify this pattern. The experiment results demonstrate that our method markedly reduces hallucination snowballing, consistently improving the performance across eight benchmarks based on four common MAS structures and ten base models. The source code will be available at: https://github.com/YU-deep/ViF.git.', 'score': 7, 'issue_id': 6225, 'pub_date': '2025-09-26', 'pub_date_card': {'ru': '26 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 26', 'zh': '9æœˆ26æ—¥'}, 'hash': 'f05b2844cf6eaf03', 'authors': ['Xinlei Yu', 'Chengming Xu', 'Guibin Zhang', 'Yongbo He', 'Zhangquan Chen', 'Zhucun Xue', 'Jiangning Zhang', 'Yue Liao', 'Xiaobin Hu', 'Yu-Gang Jiang', 'Shuicheng Yan'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Tencent Youtu Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21789.jpg', 'data': {'categories': ['#hallucinations', '#agents', '#benchmark', '#cv'], 'emoji': 'â„ï¸', 'ru': {'title': 'ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞ½ĞµĞ¶Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Â«ÑĞ½ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹Â» Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Visual Language Models, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ·-Ğ·Ğ° Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ Ğ¿Ğ¸ĞºĞ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ViF Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµÑ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½Ğ¸Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ´ĞµÑÑÑ‚ÑŒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Mitigating Visual Hallucination in Multi-Agent Systems with ViF', 'desc': "This paper introduces ViF, a method designed to reduce visual hallucination snowballing in Multi-Agent Systems (MAS) that utilize Visual Language Models (VLMs). The issue arises when one agent's visual hallucination is amplified by subsequent agents due to excessive reliance on textual information. Through detailed attention analyses, the authors identify specific visual tokens that maintain visual integrity but lose effectiveness in deeper agent interactions. ViF enhances message relay and visual attention by focusing on these selected tokens, leading to significant improvements in performance across various MAS benchmarks."}, 'zh': {'title': 'ViFï¼šå‡è½»å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§†è§‰å¹»è§‰', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºViFçš„æ–¹æ³•ï¼Œç”¨äºç¼“è§£å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„è§†è§‰å¹»è§‰é›ªçƒæ•ˆåº”ã€‚è¯¥æ•ˆåº”æ˜¯ç”±äºå•ä¸ªæ™ºèƒ½ä½“çš„å¹»è§‰è¢«åç»­æ™ºèƒ½ä½“æ”¾å¤§ï¼Œå¯¼è‡´ä¿¡æ¯ä¼ é€’å¤±çœŸã€‚é€šè¿‡å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°æŸäº›è§†è§‰æ ‡è®°åœ¨ä¸­é—´å±‚å…·æœ‰æœ€ä½³çš„è§†è§‰è¯æ®ä¿ç•™èƒ½åŠ›ã€‚ViFé€šè¿‡é€‰æ‹©è¿™äº›è§†è§‰æ ‡è®°å¹¶é‡æ–°åˆ†é…æ³¨æ„åŠ›ï¼Œæœ‰æ•ˆåœ°æ”¹å–„äº†ä¿¡æ¯ä¼ é€’ï¼Œæ˜¾è‘—é™ä½äº†å¹»è§‰é›ªçƒæ•ˆåº”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00428', 'title': 'Automated Structured Radiology Report Generation with Rich Clinical\n  Context', 'url': 'https://huggingface.co/papers/2510.00428', 'abstract': 'Incorporating clinical context into automated structured radiology report generation improves report quality by addressing temporal hallucinations and utilizing comprehensive patient data.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated structured radiology report generation (SRRG) from chest X-ray images offers significant potential to reduce workload of radiologists by generating reports in structured formats that ensure clarity, consistency, and adherence to clinical reporting standards. While radiologists effectively utilize available clinical contexts in their diagnostic reasoning, existing SRRG systems overlook these essential elements. This fundamental gap leads to critical problems including temporal hallucinations when referencing non-existent clinical contexts. To address these limitations, we propose contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical context for SRRG. We curate C-SRRG dataset by integrating comprehensive clinical context encompassing 1) multi-view X-ray images, 2) clinical indication, 3) imaging techniques, and 4) prior studies with corresponding comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, we demonstrate that incorporating clinical context with the proposed C-SRRG significantly improves report generation quality. We publicly release dataset, code, and checkpoints to facilitate future research for clinically-aligned automated RRG at https://github.com/vuno/contextualized-srrg.', 'score': 6, 'issue_id': 6222, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '01935fd7ce2849e2', 'authors': ['Seongjae Kang', 'Dong Bok Lee', 'Juho Jung', 'Dongseop Kim', 'Won Hwa Kim', 'Sunghoon Joo'], 'affiliations': ['KAIST', 'POSTECH', 'VUNO Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2510.00428.jpg', 'data': {'categories': ['#data', '#hallucinations', '#science', '#dataset', '#healthcare', '#multimodal', '#benchmark', '#open_source'], 'emoji': '\U0001fa7b', 'ru': {'title': 'ĞšĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ğ¼ ÑĞ½Ğ¸Ğ¼ĞºĞ°Ğ¼ Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Â«Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Â» â€” Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼, ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑÑ‹Ğ»Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½ĞµÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ C-SRRG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸ÑÑ…, ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ, Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ multimodal LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Radiology Reports with Clinical Context', 'desc': 'This paper presents a new approach to automated structured radiology report generation (SRRG) that incorporates clinical context to enhance report quality. The authors identify that existing SRRG systems often ignore important clinical information, leading to issues like temporal hallucinations, where reports reference non-existent contexts. To solve this, they introduce contextualized SRRG (C-SRRG), which integrates various clinical data such as multi-view X-ray images and patient histories. Their experiments show that using C-SRRG significantly improves the clarity and accuracy of generated reports, making them more useful for radiologists.'}, 'zh': {'title': 'æ•´åˆä¸´åºŠèƒŒæ™¯ï¼Œæå‡æ”¾å°„å­¦æŠ¥å‘Šè´¨é‡', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–ç»“æ„åŒ–æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡åŒ–ç»“æ„åŒ–æŠ¥å‘Šç”Ÿæˆï¼ˆC-SRRGï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æ•´åˆä¸°å¯Œçš„ä¸´åºŠèƒŒæ™¯ä¿¡æ¯ï¼Œè§£å†³äº†ç°æœ‰ç³»ç»Ÿåœ¨ç”ŸæˆæŠ¥å‘Šæ—¶å¿½è§†ä¸´åºŠä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œä»è€Œå‡å°‘äº†æ—¶é—´å¹»è§‰çš„å‘ç”Ÿã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šè§†è§’Xå…‰å›¾åƒã€ä¸´åºŠæŒ‡ç¤ºã€æˆåƒæŠ€æœ¯å’Œæ‚£è€…å†å²çš„C-SRRGæ•°æ®é›†ã€‚é€šè¿‡ä¸å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜ï¼ŒC-SRRGæ˜¾è‘—æé«˜äº†æŠ¥å‘Šç”Ÿæˆçš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02315', 'title': 'Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity', 'url': 'https://huggingface.co/papers/2510.02315', 'abstract': 'A theoretical framework and algorithms for improving multi-subject fidelity in text-to-image models through control over sampling dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.', 'score': 5, 'issue_id': 6223, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '279a167c4dc7509b', 'authors': ['Eric Tillmann Bill', 'Enis Simsar', 'Thomas Hofmann'], 'affiliations': ['ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2510.02315.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#cv', '#diffusion'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² text-to-image Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² flow matching Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ test-time inference Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Adjoint Matching Ğ´Ğ»Ñ Ğ»Ñ‘Ğ³ĞºĞ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Stable Diffusion 3.5, FLUX Ğ¸ SDXL Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ FOCUS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Multi-Subject Fidelity in Text-to-Image Models', 'desc': 'This paper presents a new framework and algorithms aimed at enhancing the performance of text-to-image (T2I) models when generating images from multi-subject prompts. The authors identify common issues such as attribute leakage and identity entanglement that occur in these models and propose a method to control sampling dynamics to improve fidelity. They introduce two algorithms: a test-time controller that adjusts the sampling process on-the-fly and a fine-tuning method called Adjoint Matching that optimizes a control network. The results show that these approaches significantly enhance multi-subject alignment while preserving the original style of the base models, demonstrating their effectiveness across various T2I architectures.'}, 'zh': {'title': 'æå‡å¤šä¸»ä½“ä¸€è‡´æ€§çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç†è®ºæ¡†æ¶å’Œç®—æ³•ï¼Œä»¥æ”¹å–„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¤šä¸»ä½“åœºæ™¯ä¸­çš„è¡¨ç°ã€‚ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨å¤„ç†å•ä¸€å®ä½“æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤šä¸»ä½“æè¿°ä¸­å¸¸å¸¸å‡ºç°å±æ€§æ³„æ¼å’Œèº«ä»½çº ç¼ ç­‰é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡éšæœºæœ€ä¼˜æ§åˆ¶çš„æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§ä¼˜åŒ–é‡‡æ ·åŠ¨æ€çš„ç›®æ ‡ï¼Œä»è€Œå®ç°å¤šä¸»ä½“çš„è§£è€¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„ç®—æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šå‡èƒ½æœ‰æ•ˆæé«˜å¤šä¸»ä½“çš„ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€æ¨¡å‹çš„é£æ ¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01670', 'title': 'Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness', 'url': 'https://huggingface.co/papers/2510.01670', 'abstract': 'Computer-Use Agents consistently exhibit Blind Goal-Directedness, a bias that leads to risky behavior regardless of feasibility or context, as demonstrated by the BLIND-ACT benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take actions on GUIs to accomplish user goals. In this paper, we show that CUAs consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals regardless of feasibility, safety, reliability, or context. We characterize three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii) assumptions and decisions under ambiguity, and (iii) contradictory or infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement with human annotations. We use BLIND-ACT to evaluate nine frontier models, including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing high average BGD rates (80.8%) across them. We show that BGD exposes subtle risks that arise even when inputs are not directly harmful. While prompting-based interventions lower BGD levels, substantial risk persists, highlighting the need for stronger training- or inference-time interventions. Qualitative analysis reveals observed failure modes: execution-first bias (focusing on how to act over whether to act), thought-action disconnect (execution diverging from reasoning), and request-primacy (justifying actions due to user request). Identifying BGD and introducing BLIND-ACT establishes a foundation for future research on studying and mitigating this fundamental risk and ensuring safe CUA deployment.', 'score': 5, 'issue_id': 6222, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '03b47e25c6ec7bd1', 'authors': ['Erfan Shayegani', 'Keegan Hines', 'Yue Dong', 'Nael Abu-Ghazaleh', 'Roman Lutz', 'Spencer Whitehead', 'Vidhisha Balachandran', 'Besmira Nushi', 'Vibhav Vineet'], 'affiliations': ['Microsoft AI Red Team', 'Microsoft Research AI Frontiers', 'NVIDIA', 'University of California, Riverside'], 'pdf_title_img': 'assets/pdf/title_img/2510.01670.jpg', 'data': {'categories': ['#reasoning', '#alignment', '#training', '#benchmark', '#agents', '#security'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡Ğ»ĞµĞ¿Ğ¾Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»Ğ¸: ĞºĞ°Ğº AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ» Ñ€Ğ°Ğ´Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ (Computer-Use Agents), ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Â«ÑĞ»ĞµĞ¿ÑƒÑ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÂ» â€” ÑÑ‚Ñ€ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ»ÑĞ±Ğ¾Ğ¹ Ñ†ĞµĞ½Ğ¾Ğ¹, Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒÑ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ». Ğ”Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BLIND-ACT Ñ 90 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ LLM-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Claude Sonnet 4 Ğ¸ GPT-5 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ‚Ğ°ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ² 80.8% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². ĞĞ³ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ½ĞµĞ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸. Ğ¥Ğ¾Ñ‚Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ Ñ€Ğ¸ÑĞº, Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ğ¾Ğ¹ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ inference Ğ´Ğ»Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Addressing Blind Goal-Directedness in AI Agents for Safer Interactions', 'desc': 'This paper discusses a problem called Blind Goal-Directedness (BGD) in Computer-Use Agents (CUAs), which are AI systems that perform tasks on graphical user interfaces. BGD causes these agents to pursue goals without considering if they are safe or feasible, leading to risky behaviors. The authors introduce a benchmark called BLIND-ACT, which consists of 90 tasks designed to evaluate BGD in CUAs, revealing that many advanced models exhibit high rates of this bias. The study emphasizes the importance of addressing BGD to ensure the safe deployment of CUAs, suggesting that while some interventions can reduce BGD, significant risks remain.'}, 'zh': {'title': 'è¯†åˆ«ç›²ç›®ç›®æ ‡å¯¼å‘ï¼Œç¡®ä¿å®‰å…¨çš„è®¡ç®—æœºä½¿ç”¨ä»£ç†', 'desc': 'è®¡ç®—æœºä½¿ç”¨ä»£ç†ï¼ˆCUAsï¼‰åœ¨æ‰§è¡Œç”¨æˆ·ç›®æ ‡æ—¶ï¼Œè¡¨ç°å‡ºä¸€ç§ç§°ä¸ºç›²ç›®ç›®æ ‡å¯¼å‘ï¼ˆBGDï¼‰çš„åå·®ã€‚è¿™ç§åå·®ä½¿å¾—ä»£ç†åœ¨è¿½æ±‚ç›®æ ‡æ—¶ï¼Œä¸è€ƒè™‘å¯è¡Œæ€§ã€å®‰å…¨æ€§ã€å¯é æ€§æˆ–ä¸Šä¸‹æ–‡ã€‚æœ¬æ–‡é€šè¿‡BLIND-ACTåŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†BGDçš„ä¸‰ç§å¸¸è§æ¨¡å¼ï¼Œå¹¶è¯„ä¼°äº†å¤šç§å‰æ²¿æ¨¡å‹çš„è¡¨ç°ï¼Œå‘ç°å®ƒä»¬æ™®éå­˜åœ¨é«˜è¾¾80.8%çš„BGDç‡ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨ä»£ç†è¡Œä¸ºä¸­è¯†åˆ«BGDçš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†åŸºç¡€ï¼Œä»¥ç¡®ä¿CUAçš„å®‰å…¨éƒ¨ç½²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01623', 'title': 'VLA-R1: Enhancing Reasoning in Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2510.01623', 'abstract': 'VLA-R1 enhances VLA models with RLVR and GRPO to improve reasoning and execution, achieving better generalization and real-world performance using a new dataset with chain-of-thought supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.', 'score': 5, 'issue_id': 6222, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '07cfd503342b7106', 'authors': ['Angen Ye', 'Zeyu Zhang', 'Boyuan Wang', 'Xiaofeng Wang', 'Dapeng Zhang', 'Zheng Zhu'], 'affiliations': ['CASIA', 'GigaAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.01623.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#dataset', '#rl', '#training', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'VLA Ñ ÑĞ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'VLA-R1 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ (chain-of-thought) Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Reinforcement Learning from Verifiable Rewards (RLVR) Ğ¸ Group Relative Policy Optimization (GRPO) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ¼. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ VLA-CoT-13K Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ (affordances) Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ VLA Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Reasoning in Vision-Language-Action Models with VLA-R1', 'desc': "The paper introduces VLA-R1, an advanced Vision-Language-Action model that enhances reasoning and execution capabilities. It combines Reinforcement Learning from Verifiable Rewards (RLVR) and Group Relative Policy Optimization (GRPO) to improve the model's ability to reason step-by-step and generate actions that consider environmental constraints. A new dataset, VLA-CoT-13K, is created to provide chain-of-thought supervision, which helps the model learn better reasoning aligned with real-world tasks. Evaluations show that VLA-R1 outperforms previous models in both generalization and practical applications, making it a significant advancement in embodied AI."}, 'zh': {'title': 'VLA-R1ï¼šæ¨ç†ä¸æ‰§è¡Œçš„å®Œç¾ç»“åˆ', 'desc': 'VLA-R1æ˜¯ä¸€ç§å¢å¼ºçš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ï¼Œç»“åˆäº†å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæ—¨åœ¨æ”¹å–„æ¨ç†å’Œæ‰§è¡Œèƒ½åŠ›ã€‚è¯¥æ¨¡å‹é€šè¿‡è®¾è®¡åŸºäºRLVRçš„åè®­ç»ƒç­–ç•¥ï¼Œå¼ºåŒ–äº†åŒºåŸŸå¯¹é½ã€è½¨è¿¹ä¸€è‡´æ€§å’Œè¾“å‡ºæ ¼å¼ï¼Œä»è€Œæé«˜äº†æ¨ç†çš„ç¨³å¥æ€§å’Œæ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼ŒVLA-R1ä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„é«˜è´¨é‡æ•°æ®é›†VLA-CoT-13Kï¼Œæä¾›äº†ä¸å¯ç”¨æ€§å’Œè½¨è¿¹æ³¨é‡Šæ˜ç¡®å¯¹é½çš„æ€ç»´é“¾ç›‘ç£ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒVLA-R1åœ¨å¤šä¸ªå¹³å°ä¸Šå±•ç°å‡ºä¼˜äºä»¥å¾€VLAæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å’Œç°å®ä¸–ç•Œè¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02263', 'title': 'RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems', 'url': 'https://huggingface.co/papers/2510.02263', 'abstract': 'Introducing reasoning abstractions in reinforcement learning improves structured exploration and generalization for complex problem-solving.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement "algorithmic procedures" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration.', 'score': 4, 'issue_id': 6227, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '52cc5d424b5e4b27', 'authors': ['Yuxiao Qu', 'Anikait Singh', 'Yoonho Lee', 'Amrith Setlur', 'Ruslan Salakhutdinov', 'Chelsea Finn', 'Aviral Kumar'], 'affiliations': ['Carnegie Mellon University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2510.02263.jpg', 'data': {'categories': ['#reasoning', '#training', '#rlhf', '#rl'], 'emoji': 'ğŸ§©', 'ru': {'title': 'ĞĞ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· reasoning abstractions â€” ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´ RLAD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ¸Ğ³Ñ€Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ reinforcement learning, Ğ³Ğ´Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğº Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ñ‡ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Reinforcement Learning with Reasoning Abstractions', 'desc': 'This paper presents a new approach in reinforcement learning (RL) called reasoning abstractions, which helps models explore complex problems more effectively. By using concise natural language descriptions of procedures and knowledge, the model can better identify relevant information and build solutions. The authors propose a two-player RL training method, where one part generates abstractions and the other generates solutions, leading to improved generalization and structured exploration. The findings suggest that focusing on generating abstractions during testing enhances performance more than simply generating more solutions.'}, 'zh': {'title': 'æ¨ç†æŠ½è±¡æå‡å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢ä¸æ³›åŒ–èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¼•å…¥æ¨ç†æŠ½è±¡çš„æ–¹æ³•ï¼Œä»¥æ”¹å–„å¤æ‚é—®é¢˜è§£å†³çš„ç»“æ„åŒ–æ¢ç´¢å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ¨ç†ä¸ä»…ä»…æ˜¯æ¨¡å¼åŒ¹é…æˆ–è®°å¿†è§£å†³æ–¹æ¡ˆï¼Œè€Œæ˜¯éœ€è¦è¯†åˆ«å’Œå®æ–½â€œç®—æ³•è¿‡ç¨‹â€ï¼Œä»¥æ¨å¯¼å‡ºå›°éš¾é—®é¢˜çš„ç­”æ¡ˆã€‚æˆ‘ä»¬å¼•å…¥äº†æ¨ç†æŠ½è±¡ï¼Œå³å¯¹ç¨‹åºæ€§å’Œäº‹å®çŸ¥è¯†çš„ç®€æ´è‡ªç„¶è¯­è¨€æè¿°ï¼ŒæŒ‡å¯¼æ¨¡å‹å­¦ä¹ æˆåŠŸçš„æ¨ç†ã€‚é€šè¿‡è®­ç»ƒæ¨¡å‹æå‡ºå¤šä¸ªæŠ½è±¡ï¼Œå¹¶ç»“åˆå¼ºåŒ–å­¦ä¹ ï¼Œæœ€ç»ˆå®ç°äº†ä¸€ä¸ªæœ‰æ•ˆçš„ä¸¤ç©å®¶å¼ºåŒ–å­¦ä¹ è®­ç»ƒèŒƒå¼ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨æ›´å¤æ‚é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02259', 'title': 'Transformers Discover Molecular Structure Without Graph Priors', 'url': 'https://huggingface.co/papers/2510.02259', 'abstract': 'Transformers trained directly on Cartesian coordinates can achieve competitive performance in molecular energy and force prediction without predefined graphs, demonstrating adaptability and scalability.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinatesx2013without predefined graphs or physical priorsx2013can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patternsx2013such as attention weights that decay inversely with interatomic distancex2013and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.', 'score': 4, 'issue_id': 6221, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '2f48683b9886f234', 'authors': ['Tobias Kreiman', 'Yutong Bai', 'Fadi Atieh', 'Elizabeth Weaver', 'Eric Qu', 'Aditi S. Krishnapriyan'], 'affiliations': ['LBNL', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2510.02259.jpg', 'data': {'categories': ['#graphs', '#architecture', '#dataset', '#optimization', '#science'], 'emoji': 'âš›ï¸', 'ru': {'title': 'Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ÑÑ‚ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Transformer-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ½Ğ° Ğ´ĞµĞºĞ°Ñ€Ñ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ… Ğ°Ñ‚Ğ¾Ğ¼Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ², Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸ Ğ¸ ÑĞ¸Ğ» Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ». Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Graph Neural Networks (GNN), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ğ¼Ğ¸, Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼, Ñ‚Ğ°ĞºĞ¸Ğ¼ ĞºĞ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ¼ĞµĞ¶Ğ°Ñ‚Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ AI. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ²ÑÑ‚ Ğ¿Ğ¾Ğ´ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Transformers: A New Era for Molecular Predictions Without Graphs', 'desc': 'This paper explores the use of Transformers, a type of neural network, for predicting molecular energies and forces directly from Cartesian coordinates, without relying on predefined graphs. Traditionally, Graph Neural Networks (GNNs) have been used for these tasks, but they can be limited by their fixed graph structures. The authors show that Transformers can achieve competitive performance while being more adaptable and scalable, as they learn patterns based on the data rather than hard-coded rules. This research suggests that Transformers can effectively replace GNNs in molecular modeling, offering a more flexible approach to machine learning in this field.'}, 'zh': {'title': 'å˜æ¢å™¨ï¼šåˆ†å­å»ºæ¨¡çš„æ–°é€‰æ‹©', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†ç›´æ¥åœ¨ç¬›å¡å°”åæ ‡ä¸Šè®­ç»ƒçš„å˜æ¢å™¨ï¼ˆTransformersï¼‰åœ¨åˆ†å­èƒ½é‡å’ŒåŠ›é¢„æµ‹ä¸­çš„è¡¨ç°ã€‚ä¸ä¼ ç»Ÿçš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸åŒï¼Œå˜æ¢å™¨ä¸ä¾èµ–äºé¢„å®šä¹‰çš„å›¾ç»“æ„ï¼Œå› æ­¤å…·æœ‰æ›´å¥½çš„é€‚åº”æ€§å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå˜æ¢å™¨èƒ½å¤Ÿå­¦ä¹ åˆ°ç‰©ç†ä¸€è‡´çš„æ¨¡å¼ï¼Œå¹¶åœ¨ä¸åŒçš„åˆ†å­ç¯å¢ƒä¸­çµæ´»é€‚åº”ã€‚æˆ‘ä»¬çš„å‘ç°æŒ‘æˆ˜äº†ç¡¬ç¼–ç å›¾ç»“æ„çš„å¿…è¦æ€§ï¼ŒæŒ‡å‘äº†åˆ†å­å»ºæ¨¡ä¸­æ ‡å‡†åŒ–å’Œå¯æ‰©å±•çš„æ¶æ„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01538', 'title': 'TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis', 'url': 'https://huggingface.co/papers/2510.01538', 'abstract': 'TimeSeriesScientist (TSci) is an LLM-driven framework that automates time series forecasting with minimal human intervention, outperforming statistical and LLM-based methods and providing transparent reports.  \t\t\t\t\tAI-generated summary \t\t\t\t Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. A general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics and self-planning over the input; Forecaster performs model fitting and validation and, based on the results, adaptively selects the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into a comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into a white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces a clear and rigorous report that makes the forecasting workflow more transparent and interpretable.', 'score': 4, 'issue_id': 6223, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'b1cf0979cd51af28', 'authors': ['Haokun Zhao', 'Xiang Zhang', 'Jiaqi Wei', 'Yiwei Xu', 'Yuting He', 'Siqi Sun', 'Chenyu You'], 'affiliations': ['Case Western Reserve University', 'Fudan University', 'Stony Brook University', 'University of British Columbia', 'University of California, Los Angeles', 'University of California, San Diego', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.01538.jpg', 'data': {'categories': ['#data', '#optimization', '#agents', '#interpretability', '#training', '#benchmark'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¾Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'TimeSeriesScientist (TSci) â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ÑĞ´Ğ¾Ğ² Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Curator, Planner, Forecaster Ğ¸ Reporter, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºÑƒ, Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞµÑ‘ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°. TSci Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ LLM-Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ½Ğ° 10,4% Ğ¸ 38,2% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°Ğ¼ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.'}, 'en': {'title': 'Automating Time Series Forecasting with Transparency and Precision', 'desc': 'TimeSeriesScientist (TSci) is an innovative framework that automates the process of time series forecasting using large language models (LLMs) with minimal human input. It addresses the challenges of handling numerous short and noisy time series by employing four specialized agents: Curator for data diagnostics, Planner for model selection, Forecaster for fitting and validation, and Reporter for generating transparent reports. TSci outperforms traditional statistical and LLM-based methods, achieving significant reductions in forecast error while providing interpretable results. This framework not only enhances the efficiency of forecasting but also ensures that the process is understandable and adaptable across various domains.'}, 'zh': {'title': 'æ—¶é—´åºåˆ—é¢„æµ‹çš„æ™ºèƒ½åŒ–è§£å†³æ–¹æ¡ˆ', 'desc': 'TimeSeriesScientist (TSci) æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œå‡å°‘äººå·¥å¹²é¢„ã€‚è¯¥æ¡†æ¶åŒ…å«å››ä¸ªä¸“é—¨çš„ä»£ç†ï¼Œåˆ†åˆ«è´Ÿè´£æ•°æ®è¯Šæ–­ã€æ¨¡å‹é€‰æ‹©ã€æ¨¡å‹æ‹Ÿåˆå’Œç»“æœæŠ¥å‘Šã€‚TSci åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé¢„æµ‹è¯¯å·®å¹³å‡é™ä½äº†10.4%åˆ°38.2%ã€‚é€šè¿‡æä¾›é€æ˜çš„è‡ªç„¶è¯­è¨€è§£é‡Šå’Œå…¨é¢çš„æŠ¥å‘Šï¼ŒTSci ä½¿é¢„æµ‹è¿‡ç¨‹å˜å¾—æ›´åŠ å¯è§£é‡Šå’Œå¯æ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00523', 'title': 'VIRTUE: Visual-Interactive Text-Image Universal Embedder', 'url': 'https://huggingface.co/papers/2510.00523', 'abstract': 'VIRTUE, a novel Visual-InteRactive Text-Image Universal Embedder, integrates segmentation and vision-language models to enable visual interactions and localized grounding, achieving state-of-the-art performance in representation learning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.', 'score': 4, 'issue_id': 6221, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '8379081e03e82135', 'authors': ['Wei-Yao Wang', 'Kazuya Tateishi', 'Qiyu Wu', 'Shusuke Takahashi', 'Yuki Mitsufuji'], 'affiliations': ['Sony AI', 'Sony Group Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2510.00523.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#interpretability', '#games', '#optimization', '#cv'], 'emoji': 'ğŸ‘†', 'ru': {'title': 'Embeddings Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼: ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ VIRTUE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ embeddings Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼. ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¸ (Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹, bounding box Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑĞºĞ¾Ğ¹), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ benchmark SCaR Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹. VIRTUE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 3-8% Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ½Ğ° 15-20% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Empowering Visual Interaction with VIRTUE', 'desc': "VIRTUE is a new model that combines segmentation and vision-language techniques to enhance how machines understand and interact with images and text. It allows users to specify areas of interest in images, improving the model's ability to learn detailed information about specific objects. This capability enables more precise responses to user queries and enhances the model's performance in various representation learning tasks. The paper introduces a benchmark to test VIRTUE's effectiveness, showing it outperforms existing models in multiple tasks."}, 'zh': {'title': 'VIRTUEï¼šå¼€å¯è§†è§‰äº¤äº’çš„æ–°çºªå…ƒ', 'desc': 'VIRTUEæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰äº¤äº’æ–‡æœ¬-å›¾åƒé€šç”¨åµŒå…¥æ¨¡å‹ï¼Œå®ƒç»“åˆäº†åˆ†å‰²æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°è§†è§‰äº¤äº’å’Œå±€éƒ¨å®šä½ã€‚è¯¥æ¨¡å‹é€šè¿‡å¤„ç†ç”¨æˆ·æŒ‡å®šçš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆå¦‚ç‚¹ã€è¾¹ç•Œæ¡†ã€æ©ç ï¼‰ï¼Œæå‡äº†åµŒå…¥æ¨¡å‹çš„äº¤äº’èƒ½åŠ›ã€‚VIRTUEåœ¨è¡¨ç¤ºå­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¤æ‚å’Œæ¨¡ç³Šåœºæ™¯ä¸‹çš„å¤„ç†èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥å¤§è§„æ¨¡çš„åˆ†å‰²å’Œåœºæ™¯æè¿°æ£€ç´¢åŸºå‡†ï¼ŒVIRTUEåœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.24203', 'title': 'Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm:\n  Demystifying Some Myths About GRPO and Its Friends', 'url': 'https://huggingface.co/papers/2509.24203', 'abstract': 'Off-policy reinforcement learning for large language models is explored through a new derivation of group-relative REINFORCE, offering insights into importance sampling, clipping, and data-weighting strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.', 'score': 4, 'issue_id': 6222, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': 'e7096714c253809b', 'authors': ['Chaorui Yao', 'Yanxi Chen', 'Yuchang Sun', 'Yushuo Chen', 'Wenhao Zhang', 'Xuchen Pan', 'Yaliang Li', 'Bolin Ding'], 'affiliations': ['Alibaba Group', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2509.24203.jpg', 'data': {'categories': ['#optimization', '#rl', '#agi', '#training', '#rlhf'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Off-policy Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ LLM: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° REINFORCE', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ REINFORCE Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½ Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ off-policy Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¾ÑĞ¼Ñ‹ÑĞ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GRPO, OPMD Ğ¸ AsymRE, Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ´Ğ²ÑƒÑ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ²: Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ²ĞµĞ½Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¸Ñ„Ñ‹ Ğ¾ Ñ€Ğ¾Ğ»Ğ¸ importance sampling Ğ¸ clipping Ğ² ÑÑ‚Ğ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² off-policy RL Ğ´Ğ»Ñ LLM.'}, 'en': {'title': 'Unlocking Off-Policy Learning for Large Language Models', 'desc': 'This paper investigates off-policy reinforcement learning (RL) techniques specifically for large language models (LLMs). It introduces a new derivation of group-relative REINFORCE, allowing for a better understanding of how importance sampling, clipping, and data-weighting can be effectively utilized in off-policy settings. The authors provide insights into regularizing policy updates and shaping data distributions, which enhance the adaptability of REINFORCE algorithms. Their findings are supported by empirical studies, paving the way for improved algorithm design in off-policy RL applications for LLMs.'}, 'zh': {'title': 'ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼šå¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°æœºé‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç¾¤ä½“ç›¸å¯¹REINFORCEçš„æ¨å¯¼æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¼ ç»Ÿçš„REINFORCEç®—æ³•å¯ä»¥åœ¨ä¸å‡è®¾ç‰¹å®šè®­ç»ƒæ•°æ®åˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œç¦»çº¿è§£é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé€‚åº”ç¦»çº¿è®¾ç½®çš„åŸåˆ™ï¼šè§„èŒƒåŒ–ç­–ç•¥æ›´æ–°å’Œä¸»åŠ¨è°ƒæ•´æ•°æ®åˆ†å¸ƒã€‚é€šè¿‡å¯¹é‡è¦æ€§é‡‡æ ·å’Œå‰ªåˆ‡çš„è§’è‰²è¿›è¡Œåˆ†æï¼Œæœ¬æ–‡ä¸ºç¦»çº¿å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›äº†æ–°çš„ç†è®ºä¾æ®å’Œå®è¯æ”¯æŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02272', 'title': 'Parallel Scaling Law: Unveiling Reasoning Generalization through A\n  Cross-Linguistic Perspective', 'url': 'https://huggingface.co/papers/2510.02272', 'abstract': 'Research investigates cross-linguistic transferability of reasoning capabilities in Large Reasoning Models, revealing significant variations and proposing a parallel training approach to improve generalization across languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: Does the reasoning capability achieved from English RPT effectively transfer to other languages? We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: First-Parallel Leap, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable Parallel Scaling Law, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as Monolingual Generalization Gap, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs.', 'score': 3, 'issue_id': 6222, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'ad37f939671428fb', 'authors': ['Wen Yang', 'Junhong Wu', 'Chong Li', 'Chengqing Zong', 'Jiajun Zhang'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Wuhan AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.02272.jpg', 'data': {'categories': ['#multilingual', '#reasoning', '#low_resource', '#rlhf', '#transfer_learning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ AI Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (LRM), Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ñ‡Ñ‚Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ½Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… AI-ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Enhancing Multilingual Reasoning in Large Models', 'desc': "This paper explores how reasoning abilities in Large Reasoning Models (LRMs) can transfer between different languages. It highlights that the effectiveness of this transfer varies based on the model's initial training, the target language, and the training methods used. The authors introduce a parallel training approach to enhance cross-lingual generalization, revealing that models trained primarily on English often struggle with other languages. Their findings suggest that improving multilingual reasoning requires a deeper understanding of how language-specific patterns affect model performance."}, 'zh': {'title': 'è·¨è¯­è¨€æ¨ç†èƒ½åŠ›çš„æå‡ä¹‹é“', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨è·¨è¯­è¨€æ¨ç†èƒ½åŠ›çš„è½¬ç§»æ€§ï¼Œå‘ç°å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¹³è¡Œè®­ç»ƒæ–¹æ³•ä»¥æé«˜è·¨è¯­è¨€çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè‹±è¯­ä¸ºä¸­å¿ƒçš„LRMsåœ¨å¤šè¯­è¨€æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ä¸å°½ç›¸åŒï¼Œä¸”åˆå§‹æ¨¡å‹ã€ç›®æ ‡è¯­è¨€å’Œè®­ç»ƒèŒƒå¼éƒ½ä¼šå½±å“è·¨è¯­è¨€è½¬ç§»æ€§ã€‚é€šè¿‡å¹²é¢„ç ”ç©¶å‘ç°ï¼Œåˆå§‹è‹±è¯­èƒ½åŠ›è¾ƒå¼ºçš„æ¨¡å‹å¾€å¾€è¿‡åº¦ä¾èµ–è‹±è¯­ç‰¹å®šæ¨¡å¼ï¼Œå¯¼è‡´è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ­ç¤ºäº†å¹³è¡Œè®­ç»ƒçš„æ˜¾è‘—æ•ˆæœï¼Œå¹¶æå‡ºäº†å•è¯­è¨€ä¸å¹³è¡Œè¯­è¨€ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼ŒæŒ‘æˆ˜äº†LRMæ¨ç†ä¸äººç±»è®¤çŸ¥ç›¸ä¼¼çš„å‡è®¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01796', 'title': 'Rethinking the shape convention of an MLP', 'url': 'https://huggingface.co/papers/2510.01796', 'abstract': 'Hourglass MLP blocks, with skip connections in expanded dimensions and narrow bottlenecks, outperform conventional narrow-wide-narrow MLPs in generative tasks across image datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.', 'score': 3, 'issue_id': 6221, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'cb9db736774b09f8', 'authors': ['Meng-Hsi Chen', 'Yu-Ang Lee', 'Feng-Ting Liao', 'Da-shan Shiu'], 'affiliations': ['MediaTek Research', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.01796.jpg', 'data': {'categories': ['#dataset', '#architecture', '#optimization'], 'emoji': 'â³', 'ru': {'title': 'ĞŸĞµÑĞ¾Ñ‡Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‹ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹: skip connections Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Hourglass MLP, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ¾Ğ²: skip connections Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ² Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ÑÑ‚ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ·ĞºĞ¸Ğµ bottleneck-ÑĞ»Ğ¾Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾ÑÑ‚Ğ°Ğ²Ğ°Ñ‚ÑŒÑÑ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Hourglass MLP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ° ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ñ„Ñ€Ğ¾Ğ½Ñ‚Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ³Ğ»ÑƒĞ±Ğ¶Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¼Ğ¸ skip connections Ğ¸ ÑƒĞ·ĞºĞ¸Ğ¼Ğ¸ bottlenecks.'}, 'en': {'title': 'Revolutionizing MLPs: Hourglass Architecture for Superior Generative Performance', 'desc': 'This paper introduces Hourglass MLP blocks, which utilize a wide-narrow-wide architecture with skip connections in expanded dimensions, contrasting with the traditional narrow-wide-narrow design. By allowing residual computations to flow through narrow bottlenecks, the model enhances performance in generative tasks while maintaining computational efficiency. The authors demonstrate that fixed random initialization for input signal projections can streamline training and inference processes. Their experiments reveal that Hourglass architectures outperform conventional MLPs, suggesting a need to rethink skip connection strategies in various neural network designs.'}, 'zh': {'title': 'é‡æ–°å®šä¹‰è·³è·ƒè¿æ¥ï¼šHourglass MLPçš„ä¼˜åŠ¿', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ç»“æ„ï¼Œç§°ä¸ºHourglass MLPå—ï¼Œé‡‡ç”¨å®½-çª„-å®½çš„è®¾è®¡ï¼Œè·³è·ƒè¿æ¥åœ¨æ‰©å±•ç»´åº¦ä¸Šæ“ä½œï¼Œè€Œæ®‹å·®è®¡ç®—åˆ™é€šè¿‡çª„ç“¶é¢ˆæµåŠ¨ã€‚è¿™ç§è®¾è®¡æŒ‘æˆ˜äº†ä¼ ç»Ÿçš„çª„-å®½-çª„ç»“æ„ï¼Œåˆ©ç”¨é«˜ç»´ç©ºé—´è¿›è¡Œå¢é‡ä¼˜åŒ–ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒHourglass MLPåœ¨ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºä¼ ç»Ÿè®¾è®¡ï¼Œå°¤å…¶æ˜¯åœ¨å‚æ•°é¢„ç®—å¢åŠ æ—¶ï¼Œæœ€ä½³é…ç½®å€¾å‘äºæ›´æ·±çš„ç½‘ç»œå’Œæ›´å®½çš„è·³è·ƒè¿æ¥ã€‚æˆ‘ä»¬çš„å‘ç°æç¤ºåœ¨ç°ä»£æ¶æ„ä¸­é‡æ–°è€ƒè™‘è·³è·ƒè¿æ¥çš„æ”¾ç½®ï¼Œå¯èƒ½å¯¹å˜æ¢å™¨å’Œå…¶ä»–æ®‹å·®ç½‘ç»œæœ‰å¹¿æ³›çš„åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01143', 'title': 'Generalized Parallel Scaling with Interdependent Generations', 'url': 'https://huggingface.co/papers/2510.01143', 'abstract': 'Bridge enhances parallel LLM inference by generating interdependent responses, improving accuracy and consistency with minimal additional parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel LLM inference scaling involves sampling a set of N>1 responses for a single input prompt. However, these N parallel responses tend to be generated independently from each other, partitioning compute resources and leaving potentially useful information in one generation untapped by others. This is in contrast to response length scaling where past computation is used in all future steps. For higher quality responses and response sets, we propose Bridge to generate interdependent responses in parallel by rethinking batched LLM hidden states as holistic tensors rather than independent slices. With only a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 50% and boosts consistency of correct responses. Trained once, Bridge scales to any generation width, all with greater performance than independent generations, unlocking a more general mode of parallel scaling that effectively leverages information between sequences, compatible with any post-generation aggregation technique.', 'score': 3, 'issue_id': 6227, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '2a2384f425b56994', 'authors': ['Harry Dong', 'David Brandfonbrener', 'Eryk Helenowski', 'Yun He', 'Mrinal Kumar', 'Han Fang', 'Yuejie Chi', 'Karthik Abinav Sankararaman'], 'affiliations': ['Carnegie Mellon University', 'Meta', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2510.01143.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#rl'], 'emoji': 'ğŸŒ‰', 'ru': {'title': 'Bridge: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ´Ñ€ÑƒĞ³ Ñƒ Ğ´Ñ€ÑƒĞ³Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Bridge, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ½Ğ¸ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ N Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ²ÑĞµÑ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ´Ñ€ÑƒĞ³ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ°. Bridge Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 2.8-5.1% Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 50% Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ reinforcement learning Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Bridge: Enhancing Parallel LLM Inference with Interdependent Responses', 'desc': 'This paper introduces Bridge, a method that enhances the accuracy and consistency of parallel large language model (LLM) inference by generating interdependent responses. Instead of treating each response as an independent output, Bridge views the hidden states of batched LLMs as holistic tensors, allowing for better information sharing among responses. By using only a small increase in parameters, Bridge significantly improves response quality and consistency, achieving up to 50% gains in accuracy through reinforcement learning. This approach allows for scalable generation widths and is compatible with various post-generation aggregation techniques, making it a versatile solution for parallel LLM tasks.'}, 'zh': {'title': 'Bridgeï¼šæå‡å¹¶è¡Œæ¨ç†çš„å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºBridgeçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¹¶è¡Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚é€šè¿‡ç”Ÿæˆç›¸äº’ä¾èµ–çš„å“åº”ï¼ŒBridgeèƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºï¼Œé¿å…ä¿¡æ¯å­¤å²›ç°è±¡ã€‚ä¸ä¼ ç»Ÿçš„ç‹¬ç«‹ç”Ÿæˆæ–¹å¼ä¸åŒï¼ŒBridgeå°†éšè—çŠ¶æ€è§†ä¸ºæ•´ä½“å¼ é‡ï¼Œä»è€Œæå‡å“åº”è´¨é‡ã€‚ä»…éœ€å°‘é‡æ–°å¢å‚æ•°ï¼ŒBridgeå°±èƒ½æ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œé€‚ç”¨äºä»»ä½•ç”Ÿæˆå®½åº¦çš„åœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01123', 'title': 'Rethinking Thinking Tokens: LLMs as Improvement Operators', 'url': 'https://huggingface.co/papers/2510.01123', 'abstract': 'Parallel-Distill-Refine (PDR) and Sequential Refinement (SR) improve the performance of LLMs by optimizing accuracy and latency through metacognitive strategies, with PDR showing significant gains on math tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning training incentivizes LLMs to produce long chains of thought (long CoT), which among other things, allows them to explore solution strategies with self-checking. This results in higher accuracy, but inflates context length, token/compute cost, and answer latency. We ask: Can current models leverage their metacognition to provide other combinations on this Pareto frontier, e.g., better accuracy with lower context length and/or latency? Abstractly, we view the model as an improvement operator on its own "thoughts" with a continuum of possible strategies. We identify an interesting inference family Parallel-Distill-Refine (PDR), which performs the following: (i) generate diverse drafts in parallel; (ii) distill them into a bounded, textual workspace; and (iii) refine conditioned on this workspace, producing an output that seeds the next round. Importantly, context length (hence compute cost) is controllable via degree of parallelism, and is no longer conflated with the total number of generated tokens. We report PDR instantiations of current models that give better accuracy than long CoT while incurring lower latency. Setting degree of parallelism to 1 yields an interesting subcase, Sequential Refinement (SR) (iteratively improve a single candidate answer) which provides performance superior to long CoT. Success of such model orchestrations raises the question whether further training could shift the Pareto frontier. To this end, we train an 8B thinking model with Reinforcement Learning (RL) to make it consistent with PDR as the inference method. On math tasks with verifiable answers, iterative pipelines surpass single-pass baselines at matched sequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME 2024 and +9% on AIME 2025).', 'score': 3, 'issue_id': 6241, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'a44e5102e9317c19', 'authors': ['Lovish Madaan', 'Aniket Didolkar', 'Suchin Gururangan', 'John Quan', 'Ruan Silva', 'Ruslan Salakhutdinov', 'Manzil Zaheer', 'Sanjeev Arora', 'Anirudh Goyal'], 'affiliations': ['Anthropic', 'Meta Superintelligence Labs', 'Mila, University of Montreal', 'Princeton University', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2510.01123.jpg', 'data': {'categories': ['#optimization', '#math', '#long_context', '#reasoning', '#training', '#inference', '#rl'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ñ‹ÑĞ»ĞµĞ¹: Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Parallel-Distill-Refine (PDR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ LLM, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾, Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¶Ğ¸Ğ¼Ğ°Ñ Ğ¸Ñ… Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (long CoT), PDR Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ£Ğ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Sequential Refinement (SR) Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ 8B Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ reinforcement learning Ğ¿Ğ¾Ğ´ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ PDR Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…: +11% Ğ½Ğ° AIME 2024 Ğ¸ +9% Ğ½Ğ° AIME 2025.'}, 'en': {'title': 'Optimizing LLMs: Accuracy Meets Efficiency with PDR', 'desc': 'The paper introduces two strategies, Parallel-Distill-Refine (PDR) and Sequential Refinement (SR), to enhance the performance of large language models (LLMs) by balancing accuracy and latency. PDR operates by generating multiple drafts in parallel, distilling them into a concise workspace, and refining the output based on this workspace, which allows for better control over context length and computational costs. The authors demonstrate that PDR can achieve higher accuracy than traditional long chains of thought (CoT) while reducing latency, particularly in math tasks. Additionally, they explore the potential of training models with Reinforcement Learning to further optimize these strategies, showing significant performance improvements in specific math competitions.'}, 'zh': {'title': 'å¹¶è¡Œè’¸é¦ç²¾ç‚¼ï¼šæå‡LLMæ€§èƒ½çš„æ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ–¹æ³•ï¼Œç§°ä¸ºå¹¶è¡Œè’¸é¦ç²¾ç‚¼ï¼ˆPDRï¼‰ï¼Œæ—¨åœ¨é€šè¿‡å…ƒè®¤çŸ¥ç­–ç•¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡†ç¡®æ€§å’Œå“åº”é€Ÿåº¦ã€‚PDRé€šè¿‡å¹¶è¡Œç”Ÿæˆå¤šæ ·åŒ–è‰ç¨¿ï¼Œæç‚¼æˆä¸€ä¸ªæœ‰é™çš„æ–‡æœ¬å·¥ä½œåŒºï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œç²¾ç‚¼ï¼Œä»è€Œä¼˜åŒ–äº†æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„é•¿é“¾æ¨ç†ï¼ˆlong CoTï¼‰ç›¸æ¯”ï¼ŒPDRåœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„å»¶è¿Ÿã€‚ç ”ç©¶è¿˜æ¢è®¨äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶ä¸PDRæ¨ç†æ–¹æ³•ä¿æŒä¸€è‡´ï¼Œä»¥è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26313', 'title': 'One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy\n  Gradient', 'url': 'https://huggingface.co/papers/2509.26313', 'abstract': "One-token rollout (OTR) enhances supervised fine-tuning of large language models by incorporating policy gradient methods to improve generalization using on-policy data.  \t\t\t\t\tAI-generated summary \t\t\t\t Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by sampling multiple candidate tokens from the current policy's distribution. The ground-truth token from the supervised data is then used to provide a reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization, offering a promising new direction for fine-tuning LLMs.", 'score': 3, 'issue_id': 6232, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '27f70b348211f788', 'authors': ['Rui Ming', 'Haoyuan Wu', 'Shoubo Hu', 'Zhuolun He', 'Bei Yu'], 'affiliations': ['ChatEDA Tech', 'Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.26313.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#reasoning', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğµ: ĞºĞ¾Ğ³Ğ´Ğ° supervised learning Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ reinforcement learning', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ One-Token Rollout (OTR) Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ supervised fine-tuning Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ policy gradient Ğ¸Ğ· reinforcement learning. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ RL-Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ², Ğ° ground-truth Ñ‚Ğ¾ĞºĞµĞ½ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ off-policy Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ on-policy ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° online Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ OTR Ğ½Ğ°Ğ´ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ SFT Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ reasoning.'}, 'en': {'title': 'Enhancing Fine-Tuning with One-Token Rollout', 'desc': 'One-token rollout (OTR) is a new method for improving the fine-tuning of large language models (LLMs) by using techniques from reinforcement learning (RL). Unlike traditional supervised fine-tuning (SFT), which relies on a fixed dataset, OTR dynamically samples data from the current policy to enhance generalization. It treats each token generation as a reinforcement learning step, using a Monte Carlo approach to evaluate multiple token candidates and provide rewards based on ground-truth data. Our experiments show that OTR significantly outperforms standard SFT across various challenging tasks, highlighting the importance of on-policy data for better model performance.'}, 'zh': {'title': 'å•æ­¥å›æ»šï¼šæå‡è¯­è¨€æ¨¡å‹å¾®è°ƒçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå•æ­¥å›æ»šï¼ˆOTRï¼‰çš„æ–°ç®—æ³•ï¼Œç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒã€‚OTRé€šè¿‡å¼•å…¥ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œåˆ©ç”¨å½“å‰ç­–ç•¥çš„åœ¨çº¿æ•°æ®æ¥æ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸åŒï¼ŒOTRå°†æ¯ä¸ªæ ‡è®°ç”Ÿæˆè§†ä¸ºå•æ­¥å¼ºåŒ–å­¦ä¹ è½¨è¿¹ï¼Œå¹¶é€šè¿‡è’™ç‰¹å¡æ´›å›æ»šé‡‡æ ·å¤šä¸ªå€™é€‰æ ‡è®°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOTRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºæ ‡å‡†çš„ç›‘ç£å¾®è°ƒï¼Œè¯æ˜äº†åœ¨çº¿æ•°æ®åœ¨æé«˜æ³›åŒ–èƒ½åŠ›æ–¹é¢çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.24304', 'title': 'FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame\n  Spotlighting', 'url': 'https://huggingface.co/papers/2509.24304', 'abstract': 'FrameThinker, a novel framework, enhances video reasoning by iteratively interrogating video content through supervised fine-tuning and reinforcement learning, achieving significant improvements and efficiency over existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Vision-Language Models (LVLMs) have achieved substantial progress in video understanding, their application to long video reasoning is hindered by uniform frame sampling and static textual reasoning, which are inefficient and struggle to handle visually intensive video tasks. To overcome these challenges, in this paper, we introduce the concept of thinking with long videos and propose a novel framework FrameThinker. Within this framework, LVLMs are able to iteratively interrogate video content. Developing such video reasoning capabilities in LVLMs presents notable challenges, particularly in adapting the model to new video actions (e.g. select frame), and designing reward functions to guide LVLMs to adopt the newly introduced action. To solve these challenges, we propose a two-phase training strategy, first employing Supervised Fine-Tuning (SFT) to instill fundamental action capabilities, followed by Reinforcement Learning (RL) to optimize a strategic decision-making policy. Notably, in this RL phase, we conduct an in-depth and comprehensive exploration of the reward design for each action and format reward. Extensive experiments on reasoning benchmarks like Video-Holmes, LongVideo-Reason, and long-video understanding benchmarks such as LongVideoBench, MLVU, VideoMME, and LVBench, demonstrate that FrameThinker achieves a significant average improvement of +10.4% over baselines while drastically reducing the number of processed frames. Most notably, our 7B model, FrameThinker establishes a new state-of-the-art on LongVideo-Reason, achieving 76.1% accuracy using an average of only 20.6 frames. This not only outperforms the competitive LongVILA-R1 (72.0%) but does so with over 20x fewer frames (vs. 512), demonstrating unparalleled efficiency and effectiveness.', 'score': 3, 'issue_id': 6222, 'pub_date': '2025-09-29', 'pub_date_card': {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'}, 'hash': '8f267884ff3ee32b', 'authors': ['Zefeng He', 'Xiaoye Qu', 'Yafu Li', 'Siyuan Huang', 'Daizong Liu', 'Yu Cheng'], 'affiliations': ['Nanjing University', 'Peking University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.24304.jpg', 'data': {'categories': ['#reasoning', '#video', '#long_context', '#rl', '#training', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ²', 'desc': 'FrameThinker - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Large Vision-Language Models (LVLMs). Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ´Ğ²Ğµ Ñ„Ğ°Ğ·Ñ‹: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° supervised fine-tuning Ğ´Ğ»Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ´Ñ€Ğ°), Ğ·Ğ°Ñ‚ĞµĞ¼ reinforcement learning Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 10.4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 20 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² - Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 76.1% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° LongVideo-Reason, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ² ÑÑ€ĞµĞ´Ğ½ĞµĞ¼ Ğ²ÑĞµĞ³Ğ¾ 20.6 ĞºĞ°Ğ´Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Video Reasoning with FrameThinker', 'desc': 'FrameThinker is a new framework designed to improve video reasoning by allowing Large Vision-Language Models (LVLMs) to interactively analyze video content. It addresses the limitations of traditional methods that rely on uniform frame sampling and static reasoning, which are inefficient for complex video tasks. The framework employs a two-phase training strategy, starting with Supervised Fine-Tuning to develop basic action skills, followed by Reinforcement Learning to refine decision-making processes. Experimental results show that FrameThinker significantly enhances performance on various benchmarks, achieving state-of-the-art accuracy while processing far fewer frames than previous models.'}, 'zh': {'title': 'FrameThinkerï¼šé«˜æ•ˆçš„è§†é¢‘æ¨ç†æ–°æ¡†æ¶', 'desc': 'FrameThinkeræ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œé€æ­¥è¯¢é—®è§†é¢‘å†…å®¹ï¼Œä»è€Œå¢å¼ºè§†é¢‘æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘æ¨ç†ä¸­çš„æ•ˆç‡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†è§†è§‰å¯†é›†å‹ä»»åŠ¡æ—¶ã€‚é€šè¿‡ä¸¤é˜¶æ®µçš„è®­ç»ƒç­–ç•¥ï¼Œé¦–å…ˆè¿›è¡Œç›‘ç£å¾®è°ƒä»¥å»ºç«‹åŸºæœ¬åŠ¨ä½œèƒ½åŠ›ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å†³ç­–ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFrameThinkeråœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨ç†å‡†ç¡®ç‡ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘äº†å¤„ç†çš„å¸§æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01241', 'title': 'SKYLENAGE Technical Report: Mathematical Reasoning and\n  Contest-Innovation Benchmarks for Multi-Level Math Evaluation', 'url': 'https://huggingface.co/papers/2510.01241', 'abstract': 'SKYLENAGE benchmarks evaluate LLMs on math reasoning, revealing performance gaps and ceiling effects across different educational levels.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) now perform strongly on many public math suites, yet frontier separation within mathematics increasingly suffers from ceiling effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a 100-item, structure-aware diagnostic set with per-item metadata on length, numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item contest-style suite spanning four stages from high school to doctoral under a seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a single setup and analyze subject x model and grade x model performance. On the contest suite, the strongest model reaches 44% while the runner-up reaches 37%; accuracy declines from high school to doctoral, and top systems exhibit a doctoral-to-high-school retention near 79%. On the reasoning set, the best model attains 81% overall, and hardest-slice results reveal clear robustness gaps between leaders and the mid-tier. In summary, we release SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH; together, SKYLENAGE provides a hard, reasoning-centered and broadly covering math benchmark with calibrated difficulty and rich metadata, serving as a reference benchmark for future evaluations of mathematical reasoning.', 'score': 3, 'issue_id': 6222, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': 'a5e2f851fdccce5f', 'authors': ['Hu Wei', 'Ze Xu', 'Boyu Yang', 'Linlin Miao', 'Weiqi Zhai', 'Yihan Li', 'Zixuan Li', 'Zhijun Wang', 'Boya Wang', 'Jianwei Yu', 'Jialing Yuan', 'Xiaoyue Zhang', 'Cheng He', 'Minglei Chen', 'Zifan Zhang', 'Qianhui Li', 'Wei Wang', 'Xiang Xu'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.01241.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#math', '#survey'], 'emoji': 'ğŸ“', 'ru': {'title': 'SKYLENAGE: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ğ±Ğ½Ğ°Ğ¶Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ reasoning Ğ² LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° SKYLENAGE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM: ReasoningMATH Ñ 100 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, Ğ¸ MATH ÑĞ¾ 150 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾Ñ‚ ÑˆĞºĞ¾Ğ»Ñ‹ Ğ´Ğ¾ Ğ´Ğ¾ĞºÑ‚Ğ¾Ñ€Ğ°Ğ½Ñ‚ÑƒÑ€Ñ‹. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 15 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 44% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµÑÑ‚Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑˆĞºĞ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾ Ğ´Ğ¾ĞºÑ‚Ğ¾Ñ€ÑĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ. ĞĞ° Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° 81% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ°Ğ¼Ñ‹Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° reasoning Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ AI.'}, 'en': {'title': 'SKYLENAGE: Benchmarking Math Reasoning in LLMs', 'desc': 'The SKYLENAGE benchmarks are designed to evaluate large language models (LLMs) on their mathematical reasoning abilities, highlighting performance gaps across different educational levels. The benchmarks consist of two parts: SKYLENAGE-ReasoningMATH, which includes a diagnostic set with detailed metadata, and SKYLENAGE-MATH, a contest-style suite that covers a range of subjects from high school to doctoral levels. The evaluation of fifteen LLM variants shows that while the best model achieves 81% accuracy on reasoning tasks, there are significant declines in performance from high school to doctoral levels. Overall, SKYLENAGE aims to provide a comprehensive and challenging benchmark for assessing mathematical reasoning in LLMs, with a focus on calibrated difficulty and detailed performance metrics.'}, 'zh': {'title': 'SKYLENAGEï¼šæ•°å­¦æ¨ç†çš„æ–°åŸºå‡†', 'desc': 'SKYLENAGEåŸºå‡†æµ‹è¯•è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„è¡¨ç°ï¼Œæ­ç¤ºäº†ä¸åŒæ•™è‚²æ°´å¹³ä¹‹é—´çš„æ€§èƒ½å·®è·å’Œå¤©èŠ±æ¿æ•ˆåº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªäº’è¡¥çš„åŸºå‡†ï¼šSKYLENAGE-ReasoningMATHå’ŒSKYLENAGE-MATHï¼Œå‰è€…æ˜¯ä¸€ä¸ªåŒ…å«100ä¸ªé¡¹ç›®çš„ç»“æ„åŒ–è¯Šæ–­é›†ï¼Œåè€…æ˜¯ä¸€ä¸ªåŒ…å«150ä¸ªé¡¹ç›®çš„ç«èµ›é£æ ¼å¥—ä»¶ï¼Œæ¶µç›–ä»é«˜ä¸­åˆ°åšå£«çš„å››ä¸ªé˜¶æ®µã€‚é€šè¿‡å¯¹åäº”ç§ç°ä»£LLMå˜ä½“çš„è¯„ä¼°ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒå­¦ç§‘å’Œå¹´çº§çš„æ¨¡å‹è¡¨ç°ï¼Œå‘ç°å‡†ç¡®ç‡ä»é«˜ä¸­åˆ°åšå£«é€æ¸ä¸‹é™ã€‚SKYLENAGEæä¾›äº†ä¸€ä¸ªä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„æ•°å­¦åŸºå‡†ï¼Œå…·æœ‰æ ¡å‡†çš„éš¾åº¦å’Œä¸°å¯Œçš„å…ƒæ•°æ®ï¼Œä¸ºæœªæ¥çš„æ•°å­¦æ¨ç†è¯„ä¼°æä¾›äº†å‚è€ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02306', 'title': 'Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation', 'url': 'https://huggingface.co/papers/2510.02306', 'abstract': 'Ignoring rating updates for draws in arena-style evaluations of large language models improves battle outcome prediction accuracy by 1-3% across different rating systems.  \t\t\t\t\tAI-generated summary \t\t\t\t In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and the user chooses the winning response or deems the "battle" a draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether a draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields a 1-3% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates.', 'score': 2, 'issue_id': 6234, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '79d6d15b77cd3ecb', 'authors': ['Raphael Tang', 'Crystina Zhang', 'Wenyan Li', 'Carmen Lai', 'Pontus Stenetorp', 'Yao Lu'], 'affiliations': ['Centre for Artificial Intelligence, University College London', 'Independent Researcher', 'Research and Development Center for Large Language Models, National Institute of Informatics', 'University of Copenhagen', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2510.02306.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#games'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞĞ¸Ñ‡ÑŒÑ Ğ² Ğ°Ñ€ĞµĞ½Ğµ LLM â€” ÑÑ‚Ğ¾ Ğ½Ğµ Ñ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ğ»Ñ‘Ğ³ĞºĞ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² arena-style Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…, Ğ³Ğ´Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑŠÑĞ²Ğ»ÑÑÑ‚ Ğ½Ğ¸Ñ‡ÑŒÑ. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ Ğ½Ğ¸Ñ‡ÑŒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ² ÑˆĞ°Ñ…Ğ¼Ğ°Ñ‚Ğ½Ğ¾Ğ¼ Elo-Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğµ â€” ÑƒÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¸Ñ‡ÑŒÑ ÑĞºĞ¾Ñ€ĞµĞµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ. Ğ˜Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€Ğ¸ Ğ½Ğ¸Ñ‡ÑŒĞ¸Ñ… ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° 1-3% Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ½Ğ¸Ñ‡ÑŒĞ¸Ñ… Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Rethinking Draws: Enhancing LLM Battle Predictions', 'desc': 'This paper investigates the effectiveness of the Elo rating system in arena-style evaluations of large language models (LLMs). It challenges the assumption that a draw in model responses indicates equal performance, suggesting instead that draws may reflect the difficulty of the query posed. By ignoring rating updates for draws, the authors demonstrate a 1-3% improvement in predicting battle outcomes across various rating systems. The findings indicate that draws are more frequent for easier and more objective queries, prompting a reevaluation of how draws are treated in rating updates.'}, 'zh': {'title': 'å¿½ç•¥å¹³å±€æ›´æ–°ï¼Œæå‡æ¨¡å‹æˆ˜æ–—é¢„æµ‹å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ç«æŠ€è¯„ä¼°ä¸­ï¼Œå¿½ç•¥å¹³å±€çš„è¯„åˆ†æ›´æ–°å¦‚ä½•æé«˜æˆ˜æ–—ç»“æœé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è´¨ç–‘å¹³å±€æ˜¯å¦çœŸæ­£æ„å‘³ç€ä¸¤ä¸ªæ¨¡å‹ç›¸ç­‰ï¼Œå› æ­¤æ˜¯å¦åº”è¯¥å°†å®ƒä»¬çš„è¯„åˆ†ç›¸ç­‰åŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¹³å±€æ›´å¯èƒ½åæ˜ æŸ¥è¯¢çš„éš¾åº¦ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„å¹³ç­‰è¡¨ç°ã€‚é€šè¿‡å¯¹ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å¿½ç•¥å¹³å±€çš„è¯„åˆ†æ›´æ–°å¯ä»¥æé«˜1-3%çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00137', 'title': 'Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval', 'url': 'https://huggingface.co/papers/2510.00137', 'abstract': 'A new training objective, MW loss, is introduced to improve retriever calibration and ranking quality by directly optimizing the Area under the ROC Curve (AUC), outperforming Contrastive Loss in retrieval-augmented generation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Dual-encoder retrievers depend on the principle that relevant documents should score higher than irrelevant ones for a given query. Yet the dominant Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss, optimizes a softened ranking surrogate that we rigorously prove is fundamentally oblivious to score separation quality and unrelated to AUC. This mismatch leads to poor calibration and suboptimal performance in downstream tasks like retrieval-augmented generation (RAG). To address this fundamental limitation, we introduce the MW loss, a new training objective that maximizes the Mann-Whitney U statistic, which is mathematically equivalent to the Area under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be correctly ranked by minimizing binary cross entropy over score differences. We provide theoretical guarantees that MW loss directly upper-bounds the AoC, better aligning optimization with retrieval goals. We further promote ROC curves and AUC as natural threshold free diagnostics for evaluating retriever calibration and ranking quality. Empirically, retrievers trained with MW loss consistently outperform contrastive counterparts in AUC and standard retrieval metrics. Our experiments show that MW loss is an empirically superior alternative to Contrastive Loss, yielding better-calibrated and more discriminative retrievers for high-stakes applications like RAG.', 'score': 2, 'issue_id': 6232, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '7d01b141bdfa87b9', 'authors': ['Nima Sheikholeslami', 'Erfan Hosseini', 'Patrice Bechard', 'Srivatsava Daruru', 'Sai Rajeswar'], 'affiliations': ['ServiceNow'], 'pdf_title_img': 'assets/pdf/title_img/2510.00137.jpg', 'data': {'categories': ['#training', '#optimization', '#rag', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'MW loss: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ AUC Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Contrastive Loss Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ MW loss Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ dual-encoder retriever Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ¿Ğ¾Ğ´ ROC-ĞºÑ€Ğ¸Ğ²Ğ¾Ğ¹ (AUC) Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ĞœĞ°Ğ½Ğ½Ğ°-Ğ£Ğ¸Ñ‚Ğ½Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Contrastive Loss Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Noise Contrastive Estimation Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ AUC, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ Ñ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹. MW loss Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½ÑƒÑ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞºĞ¾Ñ€Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ retriever-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ MW loss, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ñ Contrastive Loss Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… retrieval-augmented generation.'}, 'en': {'title': 'MW Loss: A New Standard for Retriever Calibration and Ranking', 'desc': 'This paper introduces a new training objective called MW loss, which aims to enhance the calibration and ranking quality of retrievers in retrieval-augmented generation tasks. Unlike the traditional Contrastive Loss that relies on Noise Contrastive Estimation, MW loss directly optimizes the Area under the ROC Curve (AUC), ensuring better score separation between relevant and irrelevant documents. The authors demonstrate that MW loss minimizes binary cross entropy over score differences, leading to improved performance in downstream tasks. Empirical results show that retrievers trained with MW loss outperform those trained with Contrastive Loss, making it a more effective choice for high-stakes applications.'}, 'zh': {'title': 'MWæŸå¤±ï¼šæå‡æ£€ç´¢å™¨æ ¡å‡†ä¸æ’åè´¨é‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒç›®æ ‡ï¼Œç§°ä¸ºMWæŸå¤±ï¼Œæ—¨åœ¨é€šè¿‡ç›´æ¥ä¼˜åŒ–ROCæ›²çº¿ä¸‹çš„é¢ç§¯ï¼ˆAUCï¼‰æ¥æ”¹å–„æ£€ç´¢å™¨çš„æ ¡å‡†å’Œæ’åè´¨é‡ã€‚ä¼ ç»Ÿçš„å¯¹æ¯”æŸå¤±ï¼ˆContrastive Lossï¼‰ä¾èµ–çš„å™ªå£°å¯¹æ¯”ä¼°è®¡ï¼ˆNCEï¼‰ç›®æ ‡æœªèƒ½æœ‰æ•ˆåŒºåˆ†ç›¸å…³å’Œä¸ç›¸å…³æ–‡æ¡£çš„å¾—åˆ†ï¼Œå¯¼è‡´æ ¡å‡†ä¸ä½³å’Œä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ä¸ç†æƒ³ã€‚MWæŸå¤±é€šè¿‡æœ€å¤§åŒ–Mann-Whitney Uç»Ÿè®¡é‡ï¼Œç¡®ä¿æ¯å¯¹æ­£è´Ÿæ ·æœ¬çš„å¾—åˆ†å·®å¼‚è¢«æ­£ç¡®æ’åï¼Œä»è€Œæé«˜äº†æ£€ç´¢æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨MWæŸå¤±è®­ç»ƒçš„æ£€ç´¢å™¨åœ¨AUCå’Œæ ‡å‡†æ£€ç´¢æŒ‡æ ‡ä¸Šå‡ä¼˜äºå¯¹æ¯”æŸå¤±çš„æ£€ç´¢å™¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.25729', 'title': 'Controlled Generation for Private Synthetic Text', 'url': 'https://huggingface.co/papers/2509.25729', 'abstract': 'A novel methodology for privacy-preserving synthetic text generation using entity-aware control codes and HIPS theory achieves a balance between privacy and utility in sensitive domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Text anonymization is essential for responsibly developing and deploying AI in high-stakes domains such as healthcare, social services, and law. In this work, we propose a novel methodology for privacy-preserving synthetic text generation that leverages the principles of de-identification and the Hiding In Plain Sight (HIPS) theory. Our approach introduces entity-aware control codes to guide controllable generation using either in-context learning (ICL) or prefix tuning. The ICL variant ensures privacy levels consistent with the underlying de-identification system, while the prefix tuning variant incorporates a custom masking strategy and loss function to support scalable, high-quality generation. Experiments on legal and clinical datasets demonstrate that our method achieves a strong balance between privacy protection and utility, offering a practical and effective solution for synthetic text generation in sensitive domains.', 'score': 2, 'issue_id': 6236, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '8fa0aefa8e1a5246', 'authors': ['Zihao Zhao', 'Anjalie Field'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2509.25729.jpg', 'data': {'categories': ['#data', '#ethics', '#dataset', '#healthcare', '#synthetic'], 'emoji': 'ğŸ”’', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ´Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ° Ğ¸ ÑÑ€Ğ¸ÑĞ¿Ñ€ÑƒĞ´ĞµĞ½Ñ†Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ´ĞµĞ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ HIPS (Hiding In Plain Sight), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ´Ñ‹, Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ¾ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ğ² Ğ´Ğ²ÑƒÑ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ñ…: Ñ‡ĞµÑ€ĞµĞ· in-context learning (ICL) Ğ¸ Ñ‡ĞµÑ€ĞµĞ· prefix tuning Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Balancing Privacy and Utility in Synthetic Text Generation', 'desc': 'This paper presents a new method for generating synthetic text that protects privacy while maintaining usefulness, especially in sensitive areas like healthcare and law. It uses entity-aware control codes to guide the text generation process, ensuring that sensitive information is de-identified. The methodology incorporates two techniques: in-context learning (ICL) for privacy consistency and prefix tuning with a custom masking strategy for high-quality output. Experiments show that this approach effectively balances privacy and utility, making it a valuable tool for responsible AI development.'}, 'zh': {'title': 'éšç§ä¿æŠ¤ä¸å®ç”¨æ€§çš„å®Œç¾å¹³è¡¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œç”¨äºåœ¨æ•æ„Ÿé¢†åŸŸå®ç°éšç§ä¿æŠ¤çš„åˆæˆæ–‡æœ¬ç”Ÿæˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†å»æ ‡è¯†åŒ–åŸåˆ™å’ŒHIPSç†è®ºï¼Œä½¿ç”¨å®ä½“æ„ŸçŸ¥æ§åˆ¶ä»£ç æ¥æŒ‡å¯¼å¯æ§ç”Ÿæˆã€‚é€šè¿‡åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œå‰ç¼€è°ƒä¼˜çš„å˜ä½“ä¸­ï¼Œç¡®ä¿ç”Ÿæˆçš„æ–‡æœ¬åœ¨éšç§ä¿æŠ¤å’Œå®ç”¨æ€§ä¹‹é—´è¾¾åˆ°è‰¯å¥½çš„å¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ³•å¾‹å’Œä¸´åºŠæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„åˆæˆæ–‡æœ¬ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01691', 'title': 'MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment\n  Abilities in MLLMs', 'url': 'https://huggingface.co/papers/2510.01691', 'abstract': 'MedQ-Bench introduces a benchmark for language-based evaluation of medical image quality using Multi-modal Large Language Models, focusing on both perceptual and reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descriptive, human-like reasoning process central to expert evaluation. To address this gap, we introduce MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning paradigm for language-based evaluation of medical image quality with Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary tasks: (1) MedQ-Perception, which probes low-level perceptual capability via human-curated questions on fundamental visual attributes; and (2) MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks, aligning model evaluation with human-like reasoning on image quality. The benchmark spans five imaging modalities and over forty quality attributes, totaling 2,600 perceptual queries and 708 reasoning assessments, covering diverse image sources including authentic clinical acquisitions, images with simulated degradations via physics-based reconstructions, and AI-generated images. To evaluate reasoning ability, we propose a multi-dimensional judging protocol that assesses model outputs along four complementary axes. We further conduct rigorous human-AI alignment validation by comparing LLM-based judgement with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates that models exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical use. These findings highlight the need for targeted optimization of MLLMs in medical IQA. We hope that MedQ-Bench will catalyze further exploration and unlock the untapped potential of MLLMs for medical image quality evaluation.', 'score': 1, 'issue_id': 6221, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '1aec9b0a96bf1d83', 'authors': ['Jiyao Liu', 'Jinjie Wei', 'Wanying Qu', 'Chenglong Ma', 'Junzhi Ning', 'Yunheng Li', 'Ying Chen', 'Xinzhe Luo', 'Pengcheng Chen', 'Xin Gao', 'Ming Hu', 'Huihui Xu', 'Xin Wang', 'Shujian Gao', 'Dingkang Yang', 'Zhongying Deng', 'Jin Ye', 'Lihao Liu', 'Junjun He', 'Ningsheng Xu'], 'affiliations': ['Fudan University', 'Imperial College London', 'Shanghai Artificial Intelligence Laboratory', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2510.01691.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#healthcare', '#optimization', '#reasoning'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'MedQ-Bench â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ benchmark Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº. Benchmark Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡: MedQ-Perception Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ MedQ-Reasoning Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑƒĞ¼ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼-Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ°Ğ¼. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ 40 Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ½Ğ¸Ğ¼ĞºĞ¸, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµÑ„ĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ AI-generated ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ â€” Ğ²ÑĞµĞ³Ğ¾ 2600 Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ 708 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 14 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Medical Image Quality Assessment with Language Models', 'desc': 'MedQ-Bench is a new benchmark designed to evaluate the quality of medical images using Multi-modal Large Language Models (MLLMs). It addresses the limitations of traditional methods that rely on simple score-based metrics, which do not capture the complex reasoning that human experts use. The benchmark includes two main tasks: MedQ-Perception for assessing basic visual attributes and MedQ-Reasoning for evaluating more complex reasoning about image quality. By providing a comprehensive set of tasks and a robust evaluation framework, MedQ-Bench aims to improve the performance of MLLMs in medical image quality assessment and encourage further research in this area.'}, 'zh': {'title': 'åŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°çš„æ–°åŸºå‡†ï¼šMedQ-Bench', 'desc': 'MedQ-Benchæ˜¯ä¸€ä¸ªç”¨äºåŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°çš„åŸºå‡†ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œè¯­è¨€åŸºç¡€çš„è¯„ä¼°ã€‚è¯¥åŸºå‡†å…³æ³¨æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œå®šä¹‰äº†ä¸¤ä¸ªäº’è¡¥çš„ä»»åŠ¡ï¼šMedQ-Perceptionå’ŒMedQ-Reasoningã€‚MedQ-Perceptioné€šè¿‡äººç±»ç­–åˆ’çš„é—®é¢˜æ¢æµ‹ä½çº§æ„ŸçŸ¥èƒ½åŠ›ï¼Œè€ŒMedQ-Reasoningåˆ™åŒ…æ‹¬æ— å‚è€ƒå’Œæ¯”è¾ƒæ¨ç†ä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿æ¨¡å‹è¯„ä¼°ä¸äººç±»ä¸“å®¶çš„æ¨ç†è¿‡ç¨‹ç›¸ä¸€è‡´ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„MLLMåœ¨åŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°ä¸­è¡¨ç°å‡ºåˆæ­¥ä½†ä¸ç¨³å®šçš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œå¼ºè°ƒäº†å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–çš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00537', 'title': 'Spectral Scaling Laws in Language Models: How Effectively Do\n  Feed-Forward Networks Use Their Latent Space?', 'url': 'https://huggingface.co/papers/2510.00537', 'abstract': 'Research on large language models reveals an asymmetric spectral scaling law in feed-forward networks, indicating that increasing width primarily adds low-energy directions while dominant modes saturate early, leading to underutilized latent space.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. We study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) -- we quantify how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design.', 'score': 1, 'issue_id': 6223, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'ce99291350b941ce', 'authors': ['Nandan Kumar Jha', 'Brandon Reagen'], 'affiliations': ['New York University'], 'pdf_title_img': 'assets/pdf/title_img/2510.00537.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¨Ğ¸Ñ€Ğ¾ĞºĞ¸Ğµ ÑĞµÑ‚Ğ¸ Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ: Ğ·Ğ°ĞºĞ¾Ğ½ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ feed-forward ÑĞµÑ‚ĞµĞ¹ Ğ² LLM Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ¸Ğ·ĞºĞ¾ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ñ‹ Ğ½Ğ°ÑÑ‹Ñ‰Ğ°ÑÑ‚ÑÑ Ñ€Ğ°Ğ½Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Hard Rank, Soft Rank Ğ¸ Spectral Utilization Index Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… LLaMA, GPT-2 Ğ¸ nGPT. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: soft rank Ñ€Ğ°ÑÑ‚Ñ‘Ñ‚ Ğ¿Ğ¾ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ğ°ĞºĞ¾Ğ½Ñƒ, Ğ° hard rank â€” Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑÑƒĞ±Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€Ñƒ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ FFN ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‘Ğ¼ĞºĞ¾ÑÑ‚ÑŒÑ Ñ…Ğ²Ğ¾ÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° LLM.'}, 'en': {'title': 'Unlocking Latent Space: The Asymmetric Scaling of Feed-Forward Networks', 'desc': 'This paper investigates how the width of feed-forward networks (FFNs) in large language models (LLMs) affects their performance and utilization of latent space. It introduces an asymmetric spectral scaling law, showing that increasing the width mainly enhances low-energy directions while the dominant modes reach saturation quickly. The authors use various metrics to analyze the activation of latent directions in models like LLaMA and GPT-2, revealing that much of the latent space remains under-utilized as width increases. This research provides insights into optimizing FFN width for better efficiency in LLM design by balancing tail capacity and dominant-mode capacity.'}, 'zh': {'title': 'ä¼˜åŒ–å‰é¦ˆç½‘ç»œå®½åº¦ï¼Œæå‡æ½œåœ¨ç©ºé—´åˆ©ç”¨ç‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å‰é¦ˆç½‘ç»œï¼ˆFFNsï¼‰çš„å®½åº¦é€‰æ‹©é—®é¢˜ï¼Œæ­ç¤ºäº†ä¸å¯¹ç§°çš„è°±ç¼©æ”¾æ³•åˆ™ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¢åŠ ç½‘ç»œå®½åº¦ä¸»è¦å¢åŠ ä½èƒ½é‡æ–¹å‘ï¼Œè€Œä¸»å¯¼æ¨¡å¼åœ¨æ—©æœŸå°±è¾¾åˆ°é¥±å’Œï¼Œå¯¼è‡´æ½œåœ¨ç©ºé—´çš„åˆ©ç”¨ä¸è¶³ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€å¥—è½»é‡çº§çš„è¯Šæ–­å·¥å…·æ¥é‡åŒ–ä¸åŒæ¨¡å‹ï¼ˆå¦‚LLaMAã€GPT-2å’ŒnGPTï¼‰ä¸­æœ‰æ•ˆæ¿€æ´»çš„æ½œåœ¨æ–¹å‘æ•°é‡ã€‚ç ”ç©¶ç»“æœä¸ºFFNå®½åº¦é€‰æ‹©æä¾›äº†åŸåˆ™æ€§çš„æƒè¡¡æŒ‡å¯¼ï¼Œå¸®åŠ©è®¾è®¡æ›´é«˜æ•ˆçš„æ¨ç†æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26330', 'title': 'SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking\n  for Training-free Zero-Shot Composed Image Retrieval', 'url': 'https://huggingface.co/papers/2509.26330', 'abstract': "SQUARE is a two-stage training-free framework that uses Multimodal Large Language Models to enhance zero-shot Composed Image Retrieval by enriching query embeddings and performing efficient batch reranking.  \t\t\t\t\tAI-generated summary \t\t\t\t Composed Image Retrieval (CIR) aims to retrieve target images that preserve the visual content of a reference image while incorporating user-specified textual modifications. Training-free zero-shot CIR (ZS-CIR) approaches, which require no task-specific training or labeled data, are highly desirable, yet accurately capturing user intent remains challenging. In this paper, we present SQUARE, a novel two-stage training-free framework that leverages Multimodal Large Language Models (MLLMs) to enhance ZS-CIR. In the Semantic Query-Augmented Fusion (SQAF) stage, we enrich the query embedding derived from a vision-language model (VLM) such as CLIP with MLLM-generated captions of the target image. These captions provide high-level semantic guidance, enabling the query to better capture the user's intent and improve global retrieval quality. In the Efficient Batch Reranking (EBR) stage, top-ranked candidates are presented as an image grid with visual marks to the MLLM, which performs joint visual-semantic reasoning across all candidates. Our reranking strategy operates in a single pass and yields more accurate rankings. Experiments show that SQUARE, with its simplicity and effectiveness, delivers strong performance on four standard CIR benchmarks. Notably, it maintains high performance even with lightweight pre-trained, demonstrating its potential applicability.", 'score': 1, 'issue_id': 6226, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '5b35a1137cf9fd79', 'authors': ['Ren-Di Wu', 'Yu-Yen Lin', 'Huei-Fang Yang'], 'affiliations': ['National Sun Yat-sen University, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2509.26330.jpg', 'data': {'categories': ['#cv', '#reasoning', '#benchmark', '#multimodal', '#games'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'SQUARE â€” ÑÑ‚Ğ¾ framework Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ (Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ + Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Multimodal LLM, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ€Ğ°Ğ½ĞºĞ¸Ğ½Ğ³ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ²: MLLM Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ¿ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ² Ğ²Ğ¸Ğ´Ğµ ÑĞµÑ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸, Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… benchmark-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ´Ğ°Ğ¶Ğµ Ñ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Image Retrieval with Multimodal Language Models', 'desc': 'SQUARE is a novel framework designed to improve zero-shot Composed Image Retrieval (ZS-CIR) by utilizing Multimodal Large Language Models (MLLMs). It operates in two stages: first, it enhances query embeddings through Semantic Query-Augmented Fusion (SQAF) by adding MLLM-generated captions that clarify user intent. Next, it employs Efficient Batch Reranking (EBR) to refine the retrieval results by performing visual-semantic reasoning on the top candidates. This approach allows SQUARE to achieve high accuracy without the need for task-specific training, making it effective across various benchmarks.'}, 'zh': {'title': 'SQUAREï¼šæ— è®­ç»ƒçš„ç»„åˆå›¾åƒæ£€ç´¢æ–°æ¡†æ¶', 'desc': 'SQUAREæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ— è®­ç»ƒæ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥å¢å¼ºé›¶-shotç»„åˆå›¾åƒæ£€ç´¢ï¼ˆZS-CIRï¼‰ã€‚åœ¨è¯­ä¹‰æŸ¥è¯¢å¢å¼ºèåˆï¼ˆSQAFï¼‰é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡MLLMç”Ÿæˆçš„ç›®æ ‡å›¾åƒæ ‡é¢˜æ¥ä¸°å¯Œæ¥è‡ªè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æŸ¥è¯¢åµŒå…¥ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰ç”¨æˆ·æ„å›¾ã€‚æ¥ç€ï¼Œåœ¨é«˜æ•ˆæ‰¹é‡é‡æ’åºï¼ˆEBRï¼‰é˜¶æ®µï¼ŒMLLMå¯¹æ’åé å‰çš„å€™é€‰å›¾åƒè¿›è¡Œè”åˆè§†è§‰-è¯­ä¹‰æ¨ç†ï¼Œæä¾›æ›´å‡†ç¡®çš„æ’åã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSQUAREåœ¨å¤šä¸ªæ ‡å‡†CIRåŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸”åœ¨è½»é‡çº§é¢„è®­ç»ƒæ¨¡å‹ä¸‹ä»èƒ½ä¿æŒé«˜æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01581', 'title': 'Think Right: Learning to Mitigate Under-Over Thinking via Adaptive,\n  Attentive Compression', 'url': 'https://huggingface.co/papers/2510.01581', 'abstract': "TRAAC, an online post-training RL method, improves model accuracy and efficiency by adaptively adjusting reasoning steps based on task difficulty using self-attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.", 'score': 0, 'issue_id': 6234, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': 'e3cc40d449f365df', 'authors': ['Joykirat Singh', 'Justin Chih-Yao Chen', 'Archiki Prasad', 'Elias Stengel-Eskin', 'Akshay Nambi', 'Mohit Bansal'], 'affiliations': ['Microsoft Research', 'The University of Texas at Austin', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2510.01581.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ”ÑƒĞ¼Ğ°Ğ¹ ÑÑ‚Ğ¾Ğ»ÑŒĞºĞ¾, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'TRAAC â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³Ğ° Ñ reinforcement learning, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ self-attention Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ°. ĞĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3-4B Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 8.4% Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 36.8% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GPQA-D Ğ¸ BBEH.'}, 'en': {'title': 'Think Right: Adaptive Reasoning for Enhanced Efficiency', 'desc': 'TRAAC is an innovative online post-training reinforcement learning method designed to enhance model performance by dynamically adjusting reasoning steps according to the difficulty of tasks. It addresses the problem of under-adaptivity, where models either underthink or overthink, leading to inefficiencies in reasoning. By utilizing self-attention mechanisms, TRAAC identifies crucial reasoning steps and eliminates unnecessary ones, optimizing the reasoning process. The method not only improves accuracy and reduces reasoning length but also generalizes well across various tasks, demonstrating its effectiveness in adapting to different levels of task complexity.'}, 'zh': {'title': 'TRAACï¼šè‡ªé€‚åº”æ¨ç†çš„æ™ºèƒ½é€‰æ‹©', 'desc': 'TRAACæ˜¯ä¸€ç§åœ¨çº¿åè®­ç»ƒå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ ¹æ®ä»»åŠ¡éš¾åº¦è‡ªé€‚åº”è°ƒæ•´æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­ï¼ŒçŸ­æ¨ç†å¯¼è‡´çš„é”™è¯¯å’Œé•¿æ¨ç†é€ æˆçš„èµ„æºæµªè´¹é—®é¢˜ã€‚TRAACé€šè¿‡è¯†åˆ«é‡è¦æ­¥éª¤å¹¶ä¿®å‰ªå†—ä½™æ­¥éª¤ï¼Œä¼˜åŒ–äº†æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å°†ä»»åŠ¡éš¾åº¦çº³å…¥è®­ç»ƒå¥–åŠ±ä¸­ï¼Œä»¥åˆç†åˆ†é…æ¨ç†é¢„ç®—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTRAACåœ¨å¤šç§ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘äº†æ¨ç†æ­¥éª¤ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00352', 'title': 'AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with\n  Multi-Objective Guidance', 'url': 'https://huggingface.co/papers/2510.00352', 'abstract': 'AReUReDi, a discrete optimization algorithm, achieves Pareto optimality in multi-objective biomolecule sequence design, outperforming evolutionary and diffusion-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation.', 'score': 0, 'issue_id': 6222, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': '9d15b447c9342ce6', 'authors': ['Tong Chen', 'Yinuo Zhang', 'Pranam Chatterjee'], 'affiliations': ['Centre for Computational Biology, Duke-NUS Medical School, Singapore', 'Department of Bioengineering, University of Pennsylvania', 'Department of Computer and Information Science, University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2510.00352.jpg', 'data': {'categories': ['#dataset', '#optimization', '#math'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ñ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ğ¾Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AReUReDi â€” Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¸Ğ¾Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ»ÑĞ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Rectified Discrete Flows Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞ°Ğ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ§ĞµĞ±Ñ‹ÑˆÑ‘Ğ²Ğ° Ñ Ğ¾Ñ‚Ğ¶Ğ¸Ğ³Ğ¾Ğ¼ Ğ¿Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñƒ ĞœĞµÑ‚Ñ€Ğ¾Ğ¿Ğ¾Ğ»Ğ¸ÑĞ°-Ğ“Ğ°ÑÑ‚Ğ¸Ğ½Ğ³ÑĞ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾-Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. AReUReDi ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾ Ğ¿ÑÑ‚Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ (Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ°ÑÑ‚Ğ²Ğ¾Ñ€Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ Ğ¿Ğ¾Ğ»ÑƒĞ²Ñ‹Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ) Ğ´Ğ»Ñ Ğ¿ĞµĞ¿Ñ‚Ğ¸Ğ´Ğ¾Ğ² Ğ¸ SMILES-Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğº Ñ„Ñ€Ğ¾Ğ½Ñ‚Ñƒ ĞŸĞ°Ñ€ĞµÑ‚Ğ¾.'}, 'en': {'title': 'AReUReDi: Optimizing Biomolecule Sequences for Multiple Objectives', 'desc': 'AReUReDi is a novel discrete optimization algorithm designed for multi-objective biomolecule sequence design, ensuring Pareto optimality. Unlike traditional methods that often focus on single objectives or continuous spaces, AReUReDi effectively handles multiple conflicting objectives by utilizing Tchebycheff scalarization and locally balanced proposals. The algorithm employs annealed Metropolis-Hastings updates to enhance sampling towards optimal solutions while maintaining distributional invariance. When tested on peptide and SMILES sequence design, AReUReDi demonstrated superior performance compared to existing evolutionary and diffusion-based methods, optimizing several therapeutic properties simultaneously.'}, 'zh': {'title': 'AReUReDiï¼šå¤šç›®æ ‡ä¼˜åŒ–çš„æ–°é€‰æ‹©', 'desc': 'AReUReDiæ˜¯ä¸€ç§ç¦»æ•£ä¼˜åŒ–ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤šç›®æ ‡ç”Ÿç‰©åˆ†å­åºåˆ—è®¾è®¡ä¸­å®ç°å¸•ç´¯æ‰˜æœ€ä¼˜ã€‚ä¸ç°æœ‰çš„è¿›åŒ–å’Œæ‰©æ•£æ–¹æ³•ç›¸æ¯”ï¼ŒAReUReDiåœ¨ä¼˜åŒ–å¤šä¸ªç›¸äº’å†²çªçš„ç›®æ ‡æ–¹é¢è¡¨ç°æ›´ä½³ã€‚è¯¥ç®—æ³•ç»“åˆäº†Tchebycheffæ ‡é‡åŒ–ã€å±€éƒ¨å¹³è¡¡ææ¡ˆå’Œé€€ç«Metropolis-Hastingsæ›´æ–°ï¼Œç¡®ä¿äº†æ”¶æ•›åˆ°å¸•ç´¯æ‰˜å‰æ²¿çš„ç†è®ºä¿è¯ã€‚åº”ç”¨äºè‚½å’ŒSMILESåºåˆ—è®¾è®¡æ—¶ï¼ŒAReUReDièƒ½å¤ŸåŒæ—¶ä¼˜åŒ–å¤šè¾¾äº”ç§æ²»ç–—ç‰¹æ€§ï¼Œå±•ç°å‡ºå…¶åœ¨å¤šå±æ€§ç”Ÿç‰©åˆ†å­ç”Ÿæˆä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.01260', 'title': 'IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol', 'url': 'https://huggingface.co/papers/2510.01260', 'abstract': "IoT-MCP, a framework using the Model Context Protocol, enables seamless communication between Large Language Models and IoT devices, achieving high task success rates and low resource usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Large Language Models (LLMs) with Internet-of-Things (IoT) systems faces significant challenges in hardware heterogeneity and control complexity. The Model Context Protocol (MCP) emerges as a critical enabler, providing standardized communication between LLMs and physical devices. We propose IoT-MCP, a novel framework that implements MCP through edge-deployed servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g., ``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100% task success rate to generate tool calls that fully meet expectations and obtain completely accurate results, 205ms average response time, and 74KB peak memory footprint. This work delivers both an open-source integration framework (https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized evaluation methodology for LLM-IoT systems.", 'score': 0, 'issue_id': 6230, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '3ff1f92430757d75', 'authors': ['Ningyuan Yang', 'Guanliang Lyu', 'Mingchen Ma', 'Yiyi Lu', 'Yiming Li', 'Zhihui Gao', 'Hancheng Ye', 'Jianyi Zhang', 'Tingjun Chen', 'Yiran Chen'], 'affiliations': ['Department of Electrical and Computer Engineering, Duke University'], 'pdf_title_img': 'assets/pdf/title_img/2510.01260.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#open_source', '#benchmark', '#low_resource'], 'emoji': 'ğŸŒ‰', 'ru': {'title': 'ĞœĞ¾ÑÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ LLM Ğ¸ IoT ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ IoT-MCP â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ IoT-ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Model Context Protocol. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ MCP-ÑĞµÑ€Ğ²ĞµÑ€Ñ‹ Ğ½Ğ° edge-ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº IoT-MCP Bench Ñ 114 Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¸ 1140 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ½Ğ° 22 Ñ‚Ğ¸Ğ¿Ğ°Ñ… ÑĞµĞ½ÑĞ¾Ñ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ 100% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ñ‚ĞºĞ»Ğ¸ĞºĞ° 205Ğ¼Ñ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 74ĞšĞ‘.'}, 'en': {'title': 'Seamless LLM-IoT Integration with IoT-MCP Framework', 'desc': 'The paper presents IoT-MCP, a framework that utilizes the Model Context Protocol (MCP) to facilitate effective communication between Large Language Models (LLMs) and Internet-of-Things (IoT) devices. It addresses challenges such as hardware diversity and control complexity by standardizing interactions, allowing for seamless integration. The authors introduce IoT-MCP Bench, a benchmark with 114 Basic Tasks and 1,140 Complex Tasks to evaluate LLM performance in IoT contexts. Experimental results show that IoT-MCP achieves a 100% task success rate with low latency and memory usage, providing a robust solution for LLM-IoT integration.'}, 'zh': {'title': 'æ— ç¼è¿æ¥LLMä¸ç‰©è”ç½‘çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºIoT-MCPçš„æ¡†æ¶ï¼Œåˆ©ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å®ç°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ç‰©è”ç½‘ï¼ˆIoTï¼‰è®¾å¤‡ä¹‹é—´çš„æ— ç¼é€šä¿¡ã€‚è¯¥æ¡†æ¶é€šè¿‡è¾¹ç¼˜æœåŠ¡å™¨éƒ¨ç½²ï¼Œè§£å†³äº†ç¡¬ä»¶å¼‚æ„æ€§å’Œæ§åˆ¶å¤æ‚æ€§çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†IoT-MCP Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«114ä¸ªåŸºæœ¬ä»»åŠ¡å’Œ1140ä¸ªå¤æ‚ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°IoTæ”¯æŒçš„LLMã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒIoT-MCPåœ¨22ç§ä¼ æ„Ÿå™¨å’Œ6ç§å¾®æ§åˆ¶å™¨ä¸Šå®ç°äº†100%çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹³å‡å“åº”æ—¶é—´ä¸º205æ¯«ç§’ï¼Œå³°å€¼å†…å­˜å ç”¨ä¸º74KBã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents (33)', '#agi (5)', '#alignment (13)', '#architecture (27)', '#audio (5)', '#benchmark (74)', '#cv (19)', '#data (23)', '#dataset (39)', '#diffusion (14)', '#ethics (6)', '#games (17)', '#graphs (3)', '#hallucinations (11)', '#healthcare (4)', '#inference (15)', '#interpretability (17)', '#leakage (1)', '#long_context (16)', '#low_resource (3)', '#machine_translation (1)', '#math (9)', '#multilingual (5)', '#multimodal (36)', '#open_source (29)', '#optimization (80)', '#plp (1)', '#rag (6)', '#reasoning (58)', '#rl (43)', '#rlhf (17)', '#robotics (2)', '#science (5)', '#security (9)', '#small_models (5)', '#story_generation', '#survey (5)', '#synthetic (8)', '#training (78)', '#transfer_learning (7)', '#video (12)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-06 05:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-06 05:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-06 05:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    