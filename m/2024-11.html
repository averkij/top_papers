
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 133 papers. November 2024.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 0;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }
        
        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ноябрь 2024</span> | <span id="title-articles-count">133 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/m/2024-10.html">⬅️ <span id="prev-date">10.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2024-12.html">➡️ <span id="next-date">12.2024</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ноябрь 2024', 'en': 'November 2024', 'zh': '11月2024年'};
        let feedDateNext = {'ru': '12.2024', 'en': '12/2024', 'zh': '12月2024年'};
        let feedDatePrev = {'ru': '10.2024', 'en': '10/2024', 'zh': '10月2024年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.02959', 'title': 'HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems', 'url': 'https://huggingface.co/papers/2411.02959', 'abstract': 'Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems.', 'score': 32, 'issue_id': 437, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '6fb8684374e5fdcb', 'data': {'categories': ['#rag', '#hallucinations', '#inference', '#data', '#retrieval_augmented_generation'], 'emoji': '🌐', 'ru': {'title': 'HtmlRAG: Улучшение RAG-систем с помощью структурированной веб-информации', 'desc': 'Статья представляет новый подход к генерации с извлечением информации (RAG), названный HtmlRAG. В отличие от традиционных систем RAG, использующих простой текст, HtmlRAG сохраняет структурную и семантическую информацию HTML-документов. Авторы предлагают методы очистки, сжатия и обрезки HTML для уменьшения шума и сокращения входных токенов. Эксперименты на шести наборах данных для вопросно-ответных систем подтверждают превосходство использования HTML в системах RAG.'}, 'en': {'title': 'Harnessing HTML for Enhanced Knowledge Retrieval in RAG Systems', 'desc': 'This paper introduces HtmlRAG, a novel approach to Retrieval-Augmented Generation (RAG) that utilizes HTML instead of plain text for knowledge retrieval. By leveraging the structural and semantic information present in HTML, HtmlRAG aims to enhance the performance of large language models (LLMs) and reduce the hallucination problem. The authors address the challenges posed by HTML, such as excess tokens and noise, by implementing cleaning, compression, and pruning techniques to streamline the input. Experimental results demonstrate that HtmlRAG outperforms traditional plain-text-based RAG systems across multiple question-answering datasets.'}, 'zh': {'title': '用HTML提升检索增强生成的能力', 'desc': '本文提出了一种新的检索增强生成（RAG）方法，称为HtmlRAG，旨在改善大语言模型（LLMs）的知识能力并减少幻觉问题。HtmlRAG使用HTML格式而非纯文本来增强生成过程，从而保留更多的结构和语义信息。为了应对HTML中多余内容带来的挑战，本文提出了HTML清理、压缩和修剪策略，以减少输入的冗余信息。实验结果表明，HtmlRAG在六个问答数据集上的表现优于传统的纯文本RAG系统。'}}}, {'id': 'https://huggingface.co/papers/2411.00871', 'title': 'LLaMo: Large Language Model-based Molecular Graph Assistant', 'url': 'https://huggingface.co/papers/2411.00871', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable generalization and instruction-following capabilities with instruction tuning. The advancements in LLMs and instruction tuning have led to the development of Large Vision-Language Models (LVLMs). However, the competency of the LLMs and instruction tuning have been less explored in the molecular domain. Thus, we propose LLaMo: Large Language Model-based Molecular graph assistant, which is an end-to-end trained large molecular graph-language model. To bridge the discrepancy between the language and graph modalities, we present the multi-level graph projector that transforms graph representations into graph tokens by abstracting the output representations of each GNN layer and motif representations with the cross-attention mechanism. We also introduce machine-generated molecular graph instruction data to instruction-tune the large molecular graph-language model for general-purpose molecule and language understanding. Our extensive experiments demonstrate that LLaMo shows the best performance on diverse tasks, such as molecular description generation, property prediction, and IUPAC name prediction. The code of LLaMo is available at https://github.com/mlvlab/LLaMo.', 'score': 13, 'issue_id': 439, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'd1284691dab4e739', 'data': {'categories': ['#cv', '#graphs', '#multimodal', '#training', '#transfer_learning', '#open_source', '#architecture'], 'emoji': '🧬', 'ru': {'title': 'LLaMo: Революция в понимании молекулярных структур с помощью ИИ', 'desc': 'Исследователи представили LLaMo - ассистента для работы с молекулярными графами, основанного на большой языковой модели. LLaMo использует многоуровневый графовый проектор для преобразования графовых представлений в токены, что позволяет объединить языковую и графовую модальности. Модель обучена на инструкциях, сгенерированных машинным способом, для понимания молекул и языка. Эксперименты показывают, что LLaMo превосходит существующие решения в задачах генерации описаний молекул, предсказания свойств и определения названий по IUPAC.'}, 'en': {'title': 'Bridging Language and Molecules with LLaMo', 'desc': 'This paper introduces LLaMo, a Large Language Model-based Molecular graph assistant designed to enhance understanding in the molecular domain. It utilizes a multi-level graph projector to convert molecular graph representations into tokens, facilitating better interaction between language and graph data. The model is instruction-tuned using machine-generated molecular graph instruction data, enabling it to perform various tasks like molecular description generation and property prediction. Experimental results show that LLaMo outperforms existing models in these tasks, highlighting its effectiveness in bridging language and molecular graph understanding.'}, 'zh': {'title': 'LLaMo：连接语言与分子的桥梁', 'desc': '大型语言模型（LLMs）在指令调优方面展现了出色的泛化能力和遵循指令的能力。本文提出了LLaMo，一个基于大型语言模型的分子图助手，旨在填补语言和图形模态之间的差距。我们引入了多层图投影器，通过跨注意力机制将图表示转换为图标记，并使用机器生成的分子图指令数据对模型进行指令调优。实验结果表明，LLaMo在分子描述生成、属性预测和IUPAC名称预测等多项任务中表现最佳。'}}}, {'id': 'https://huggingface.co/papers/2411.02359', 'title': 'DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution', 'url': 'https://huggingface.co/papers/2411.02359', 'abstract': 'MLLMs have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we propose a Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (i.e., power consumption), as well as peak computational consumption (i.e., latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.', 'score': 10, 'issue_id': 437, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '08c45469caff5fa0', 'data': {'categories': ['#reasoning', '#benchmark', '#inference', '#agi', '#optimization', '#robotics', '#open_source', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Эффективные языковые модели для роботов: меньше ресурсов, та же мощность', 'desc': 'Статья представляет Dynamic Early-Exit Framework для робототехнических моделей зрения, языка и действия (DeeR-VLA). Эта система автоматически регулирует размер активированной мультимодальной языковой модели (MLLM) в зависимости от ситуации, используя архитектуру с множественными выходами. DeeR разработан для эффективной работы в условиях ограниченных вычислительных ресурсов роботов. На бенчмарке CALVIN система показала значительное снижение вычислительных затрат и использования памяти GPU без ущерба для производительности.'}, 'en': {'title': 'Efficient Robotic Intelligence with Dynamic Early-Exit MLLMs', 'desc': 'This paper introduces a new framework called DeeR for improving the efficiency of robotic vision-language-action models (MLLMs). DeeR uses a Dynamic Early-Exit approach that allows the model to adjust its size based on the specific task, reducing unnecessary computations. By implementing a multi-exit architecture, the model can stop processing once it has enough information, which helps save power and memory. The results show that DeeR can significantly lower computational costs and memory usage while still performing well on tasks, making it suitable for real-world robotic applications.'}, 'zh': {'title': '动态调整，智能机器人更高效！', 'desc': '本论文提出了一种动态早期退出框架（DeeR-VLA），旨在解决机器人视觉-语言-动作模型（MLLM）在实际应用中的计算和内存限制问题。该框架通过多出口架构，能够根据具体情况自动调整激活的模型大小，从而避免冗余计算。我们还开发了新算法，设定早期终止标准，以满足预定义的计算需求，如功耗和延迟。实验结果表明，DeeR在CALVIN机器人操作基准上显著降低了计算成本和GPU内存使用，同时保持了竞争力的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.01493', 'title': 'Sample-Efficient Alignment for LLMs', 'url': 'https://huggingface.co/papers/2411.01493', 'abstract': "We study methods for efficiently aligning large language models (LLMs) with human preferences given budgeted online feedback. We first formulate the LLM alignment problem in the frame of contextual dueling bandits. This formulation, subsuming recent paradigms such as online RLHF and online DPO, inherently quests for sample-efficient algorithms that incorporate online active exploration. Leveraging insights from bandit theory, we introduce a unified algorithm based on Thompson sampling and highlight its applications in two distinct LLM alignment scenarios. The practical agent that efficiently implements this algorithm, named SEA (Sample-Efficient Alignment), is empirically validated through extensive experiments across three model scales (1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The results demonstrate that SEA achieves highly sample-efficient alignment with oracle's preferences, outperforming recent active exploration methods for LLMs. Additionally, we release the implementation of SEA together with an efficient codebase designed for online alignment of LLMs, aiming to accelerate future research in this field.", 'score': 7, 'issue_id': 440, 'pub_date': '2024-11-03', 'pub_date_card': {'ru': '3 ноября', 'en': 'November 3', 'zh': '11月3日'}, 'hash': 'c5bb9727a6ba6119', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#optimization', '#training', '#open_source', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Эффективное согласование языковых моделей с помощью контекстных дуэльных бандитов', 'desc': 'Исследователи изучают методы эффективного согласования больших языковых моделей (LLM) с человеческими предпочтениями при ограниченной онлайн-обратной связи. Они формулируют проблему согласования LLM в рамках контекстных дуэльных бандитов и предлагают унифицированный алгоритм на основе выборки Томпсона. Практический агент SEA (Sample-Efficient Alignment), реализующий этот алгоритм, показывает высокую эффективность выборки при согласовании с предпочтениями оракула. Исследователи также выпускают реализацию SEA вместе с эффективной кодовой базой для онлайн-согласования LLM.'}, 'en': {'title': 'Efficiently Aligning LLMs with Human Preferences Using SEA', 'desc': 'This paper explores how to align large language models (LLMs) with human preferences using limited online feedback. It frames the alignment challenge as a contextual dueling bandits problem, which seeks efficient algorithms that can learn from active exploration. The authors propose a new algorithm called SEA (Sample-Efficient Alignment) based on Thompson sampling, which is tested across various model sizes and preference learning methods. The results show that SEA is highly effective in aligning LLMs with human preferences while using fewer samples compared to existing methods.'}, 'zh': {'title': '高效对齐大型语言模型与人类偏好', 'desc': '本文研究了如何在有限的在线反馈下高效地将大型语言模型（LLMs）与人类偏好对齐。我们将LLM对齐问题框定为上下文对抗赌博者问题，提出了一种基于汤普森采样的统一算法，并在两个不同的LLM对齐场景中进行了应用。通过大量实验验证，名为SEA（样本高效对齐）的实用代理在不同规模的模型和偏好学习算法中表现出色，显示出其在对齐方面的高样本效率。我们还发布了SEA的实现和高效代码库，以促进该领域未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2410.23054', 'title': 'Controlling Language and Diffusion Models by Transporting Activations', 'url': 'https://huggingface.co/papers/2410.23054', 'abstract': 'The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output. In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.', 'score': 6, 'issue_id': 444, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '5238611006cc8b68', 'data': {'categories': ['#diffusion', '#hallucinations', '#cv', '#optimization', '#multimodal', '#interpretability', '#ethics', '#training', '#security', '#architecture', '#alignment'], 'emoji': '🎛️', 'ru': {'title': 'AcT: точное управление генеративными моделями через активации нейронов', 'desc': 'Эта статья представляет AcT (Activation Transport) - новый фреймворк для управления генеративными моделями, основанный на теории оптимального транспорта. AcT позволяет контролировать поведение модели путем управления активациями нейронов, что обобщает предыдущие подходы в этой области. Фреймворк применим к различным модальностям и обеспечивает точный контроль с минимальными вычислительными затратами. Эксперименты показали эффективность AcT для снижения токсичности, индукции концепций и повышения правдивости языковых моделей, а также для управления стилем и отрицания концепций в моделях генерации изображений.'}, 'en': {'title': 'Steering Generative Models with Activation Transport', 'desc': 'This paper presents Activation Transport (AcT), a new framework designed to control the behavior of large generative models by steering their activations. Using principles from optimal transport theory, AcT allows for precise manipulation of model outputs without significantly increasing computational costs. The framework is applicable across different modalities, including large language models (LLMs) and text-to-image diffusion models (T2Is). Experimental results demonstrate that AcT can reduce toxicity in LLMs, induce specific concepts, and enhance truthfulness, while also providing style control and concept negation in T2Is.'}, 'zh': {'title': '激活传输：控制生成模型的新方法', 'desc': '本文提出了一种名为激活传输（AcT）的新框架，旨在通过最优传输理论来引导模型的激活，从而控制生成模型的输出。该方法具有通用性，可以在不同的模态中应用，且对模型的计算开销影响极小。实验结果表明，AcT在大型语言模型（LLMs）中能够有效减少有害内容、引入任意概念并提高生成内容的真实性。在文本到图像的扩散模型（T2Is）中，AcT则实现了对风格的精细控制和概念的否定。'}}}, {'id': 'https://huggingface.co/papers/2411.01602', 'title': 'DreamPolish: Domain Score Distillation With Progressive Geometry Generation', 'url': 'https://huggingface.co/papers/2411.01602', 'abstract': 'We introduce DreamPolish, a text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on a view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add a surface polishing stage with only a few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find a suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce a novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such a domain. We draw inspiration from the classifier-free guidance (CFG) in textconditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods.', 'score': 6, 'issue_id': 439, 'pub_date': '2024-11-03', 'pub_date_card': {'ru': '3 ноября', 'en': 'November 3', 'zh': '11月3日'}, 'hash': '403fd08e60540a0a', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#3d', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'DreamPolish: Революция в генерации 3D-объектов с идеальной геометрией и текстурами', 'desc': 'DreamPolish - это модель генерации 3D-объектов из текста, которая превосходит существующие методы в создании утонченной геометрии и высококачественных текстур. Модель использует множественные нейронные представления и дополнительный оценщик нормалей для улучшения стабильности синтеза геометрии. Для генерации текстур авторы предлагают новый метод дистилляции оценок в определенном домене (DSD), используя предобученные модели text-to-image. Эксперименты показывают, что DreamPolish создает 3D-объекты с отполированными поверхностями и фотореалистичными текстурами, превосходя современные методы.'}, 'en': {'title': 'DreamPolish: Elevating 3D Generation with Refined Geometry and Textures', 'desc': 'DreamPolish is a text-to-3D generation model that focuses on creating high-quality 3D objects with refined geometry and textures. It improves the geometry construction by using multiple neural representations and an additional normal estimator to reduce artifacts caused by limited guidance. The model also introduces a surface polishing stage that requires minimal training to enhance the geometric details further. For texture generation, it employs a novel domain score distillation (DSD) method, inspired by classifier-free guidance, to achieve photorealistic and consistent textures in the generated 3D assets.'}, 'zh': {'title': 'DreamPolish：生成高质量3D资产的创新模型', 'desc': '本文介绍了一种名为DreamPolish的文本到3D生成模型，能够生成精细的几何形状和高质量的纹理。在几何构建阶段，我们利用多种神经表示来增强合成过程的稳定性，并引入额外的法线估计器来改善几何细节。纹理生成阶段采用了一种新的评分蒸馏目标，称为领域评分蒸馏（DSD），以引导神经表示朝向包含真实感和一致性渲染的适当领域。实验表明，我们的模型能够生成表面光滑且具有真实感纹理的3D资产，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2411.03047', 'title': 'GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details', 'url': 'https://huggingface.co/papers/2411.03047', 'abstract': 'Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches. Project page: https://garverselod.github.io/', 'score': 3, 'issue_id': 444, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': 'eadbb8f8b93d4818', 'data': {'categories': ['#diffusion', '#synthetic', '#benchmark', '#inference', '#graphs', '#data', '#training', '#dataset', '#open_source', '#3d'], 'emoji': '👚', 'ru': {'title': 'GarVerseLOD: Революция в 3D-реконструкции одежды по одному изображению', 'desc': 'GarVerseLOD - это новый набор данных и фреймворк для реконструкции 3D-одежды по одному изображению. Он включает 6000 высококачественных моделей одежды с детализированной геометрией, созданных профессиональными художниками. Набор данных имеет иерархическую структуру с уровнями детализации, от стилизованных форм до одежды с учетом позы и пиксельно-точными деталями. Для генерации реалистичных парных изображений для каждой модели одежды используется условная диффузионная модель.'}, 'en': {'title': 'Revolutionizing 3D Garment Reconstruction with GarVerseLOD', 'desc': "This paper introduces GarVerseLOD, a new dataset and framework aimed at improving 3D garment reconstruction from single images. The authors highlight the challenges faced by existing methods in generalizing to unseen images with complex clothing and poses. By collecting 6,000 high-quality cloth models and organizing them into a hierarchical dataset with varying levels of detail, they enhance the model's ability to learn and generalize. Additionally, they employ a novel labeling approach using conditional diffusion models to create realistic paired images, resulting in superior garment reconstruction quality compared to previous methods."}, 'zh': {'title': '高保真3D服装重建的新突破', 'desc': '神经隐式函数在从多张或单张图像中数字化穿衣人类方面取得了显著进展。然而，当前的方法在处理复杂的布料变形和身体姿势的未见图像时仍然面临挑战。为了解决这一问题，我们提出了GarVerseLOD，一个新的数据集和框架，旨在从单张无约束图像中实现高保真度的3D服装重建。通过收集6000个高质量的布料模型，并采用分层细节的方式，我们的研究显著提高了模型的泛化能力和推理准确性。'}}}, {'id': 'https://huggingface.co/papers/2411.02657', 'title': 'Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge', 'url': 'https://huggingface.co/papers/2411.02657', 'abstract': "Rare diseases present unique challenges in healthcare, often suffering from delayed diagnosis and fragmented information landscapes. The scarcity of reliable knowledge in these conditions poses a distinct challenge for Large Language Models (LLMs) in supporting clinical management and delivering precise patient information underscoring the need for focused training on these 'zebra' cases. We present Zebra-Llama, a specialized context-aware language model with high precision Retrieval Augmented Generation (RAG) capability, focusing on Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000 individuals, exemplifies the complexities of rare diseases with its diverse symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing a novel context-aware fine-tuning methodology trained on questions derived from medical literature, patient experiences, and clinical resources, along with expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities in handling EDS-related queries. On a test set of real-world questions collected from EDS patients and clinicians, medical experts evaluated the responses generated by both models, revealing Zebra-Llama's substantial improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama not only provides more accessible and reliable EDS information but also establishes a framework for developing specialized AI solutions for other rare conditions. This work represents a crucial step towards democratizing expert-level knowledge in rare disease management, potentially transforming how healthcare providers and patients navigate the complex landscape of rare diseases.", 'score': 3, 'issue_id': 439, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '26c0b7bc39488945', 'data': {'categories': ['#science', '#rag', '#healthcare', '#training', '#open_source'], 'emoji': '🦓', 'ru': {'title': 'Zebra-Llama: ИИ-эксперт по редким заболеваниям', 'desc': 'Статья представляет Zebra-Llama - специализированную языковую модель для редких заболеваний, используя синдром Элерса-Данлоса (EDS) как пример. Модель использует контекстно-зависимую настройку и усиленную генерацию с извлечением (RAG) для обработки запросов, связанных с EDS. Zebra-Llama показала значительные улучшения по сравнению с базовой моделью в полноте, точности, ясности и надежности цитирования при ответе на реальные вопросы пациентов и врачей. Эта работа открывает путь к созданию специализированных ИИ-решений для других редких заболеваний, демократизируя экспертные знания в этой области.'}, 'en': {'title': 'Zebra-Llama: Transforming Rare Disease Management with AI', 'desc': 'This paper introduces Zebra-Llama, a specialized language model designed to improve the management of rare diseases, specifically Ehlers-Danlos Syndrome (EDS). It addresses the challenges posed by limited information and delayed diagnoses in rare conditions by utilizing a context-aware fine-tuning approach. The model employs Retrieval Augmented Generation (RAG) to enhance the precision of responses to EDS-related queries, outperforming the base model in various evaluation metrics. By making Zebra-Llama an open-source resource, the authors aim to democratize access to expert knowledge in rare disease management, paving the way for similar advancements in other rare conditions.'}, 'zh': {'title': 'Zebra-Llama：罕见疾病管理的新突破', 'desc': '罕见疾病在医疗保健中面临独特挑战，常常导致诊断延迟和信息碎片化。针对这些罕见病例，本文提出了一种名为Zebra-Llama的专门语言模型，具备高精度的检索增强生成能力，重点关注Ehlers-Danlos综合症（EDS）。通过一种新颖的上下文感知微调方法，Zebra-Llama在处理与EDS相关的问题时表现出前所未有的能力，显著提高了回答的全面性、准确性和清晰度。该模型作为开源资源发布，不仅提供了更易获取和可靠的EDS信息，还为其他罕见疾病开发专门的人工智能解决方案奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2411.02844', 'title': 'Correlation of Object Detection Performance with Visual Saliency and Depth Estimation', 'url': 'https://huggingface.co/papers/2411.02844', 'abstract': "As object detection techniques continue to evolve, understanding their relationships with complementary visual tasks becomes crucial for optimising model architectures and computational resources. This paper investigates the correlations between object detection accuracy and two fundamental visual tasks: depth prediction and visual saliency prediction. Through comprehensive experiments using state-of-the-art models (DeepGaze IIE, Depth Anything, DPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that visual saliency shows consistently stronger correlations with object detection accuracy (mArho up to 0.459 on Pascal VOC) compared to depth prediction (mArho up to 0.283). Our analysis reveals significant variations in these correlations across object categories, with larger objects showing correlation values up to three times higher than smaller objects. These findings suggest incorporating visual saliency features into object detection architectures could be more beneficial than depth information, particularly for specific object categories. The observed category-specific variations also provide insights for targeted feature engineering and dataset design improvements, potentially leading to more efficient and accurate object detection systems.", 'score': 2, 'issue_id': 444, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': 'd17ec72bcba1cd05', 'data': {'categories': ['#cv', '#graphs', '#optimization', '#dataset', '#architecture'], 'emoji': '👁️', 'ru': {'title': 'Визуальная значимость превосходит глубину в улучшении обнаружения объектов', 'desc': 'Исследование изучает связь между точностью обнаружения объектов и задачами предсказания глубины и визуальной значимости. Эксперименты с современными моделями на наборах данных COCO и Pascal VOC показали, что визуальная значимость имеет более сильную корреляцию с точностью обнаружения объектов по сравнению с предсказанием глубины. Обнаружены значительные различия в корреляциях между категориями объектов, причем для более крупных объектов корреляция до трех раз выше. Результаты предполагают, что включение признаков визуальной значимости в архитектуры обнаружения объектов может быть более полезным, чем информация о глубине.'}, 'en': {'title': 'Enhancing Object Detection with Visual Saliency Insights', 'desc': 'This paper explores how object detection accuracy relates to two other visual tasks: depth prediction and visual saliency prediction. The authors conducted experiments with advanced models on popular datasets and found that visual saliency has a stronger correlation with object detection accuracy than depth prediction. They noted that larger objects tend to show much higher correlation values compared to smaller ones. The results suggest that integrating visual saliency features into object detection models could enhance performance, especially for certain object categories.'}, 'zh': {'title': '视觉显著性助力物体检测精度提升', 'desc': '本文研究了物体检测精度与深度预测和视觉显著性预测这两种基本视觉任务之间的关系。通过在COCO和Pascal VOC数据集上使用先进模型进行全面实验，发现视觉显著性与物体检测精度的相关性明显高于深度预测。研究还表明，不同物体类别之间的相关性存在显著差异，大型物体的相关性值可高达小型物体的三倍。结果表明，将视觉显著性特征融入物体检测架构可能比深度信息更有利，尤其是在特定物体类别中。'}}}, {'id': 'https://huggingface.co/papers/2411.02393', 'title': 'Adaptive Length Image Tokenization via Recurrent Allocation', 'url': 'https://huggingface.co/papers/2411.02393', 'abstract': 'Current vision systems typically assign fixed-length representations to images, regardless of the information content. This contrasts with human intelligence - and even large language models - which allocate varying representational capacities based on entropy, context and familiarity. Inspired by this, we propose an approach to learn variable-length token representations for 2D images. Our encoder-decoder architecture recursively processes 2D image tokens, distilling them into 1D latent tokens over multiple iterations of recurrent rollouts. Each iteration refines the 2D tokens, updates the existing 1D latent tokens, and adaptively increases representational capacity by adding new tokens. This enables compression of images into a variable number of tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction loss and FID metrics, demonstrating that token count aligns with image entropy, familiarity and downstream task requirements. Recurrent token processing with increasing representational capacity in each iteration shows signs of token specialization, revealing potential for object / part discovery.', 'score': 1, 'issue_id': 448, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '1c3553b38b491652', 'data': {'categories': ['#optimization', '#architecture', '#interpretability', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Адаптивное токенизированное представление изображений', 'desc': 'Статья предлагает новый подход к представлению изображений с помощью токенов переменной длины. Авторы разработали архитектуру энкодер-декодер, которая рекурсивно обрабатывает 2D-токены изображения, преобразуя их в 1D-латентные токены. Процесс включает несколько итераций, на каждой из которых уточняются 2D-токены, обновляются существующие 1D-токены и адаптивно увеличивается емкость представления путем добавления новых токенов. Эксперименты показали, что количество токенов соответствует энтропии изображения и требованиям downstream-задач.'}, 'en': {'title': 'Adaptive Tokenization for Enhanced Image Representation', 'desc': "This paper introduces a novel method for creating variable-length token representations of images, which contrasts with traditional fixed-length approaches. The proposed encoder-decoder architecture processes 2D image tokens recursively, refining them into 1D latent tokens through multiple iterations. Each iteration not only updates the existing tokens but also allows for the addition of new tokens, enabling a flexible representation that adapts to the complexity of the image. The effectiveness of this method is validated through reconstruction loss and FID metrics, showing that the number of tokens correlates with the image's information content and can enhance downstream tasks."}, 'zh': {'title': '可变长度标记表示：提升图像理解能力', 'desc': '当前的视觉系统通常为图像分配固定长度的表示，这与人类智能和大型语言模型的动态表示能力形成对比。我们提出了一种方法，通过编码器-解码器架构学习二维图像的可变长度标记表示。该方法通过递归处理二维图像标记，将其提炼为一维潜在标记，并在多个迭代中逐步更新和增加表示能力。我们的实验表明，标记数量与图像的熵、熟悉度和下游任务需求相一致，显示出标记的专业化潜力。'}}}, {'id': 'https://huggingface.co/papers/2411.03312', 'title': 'Inference Optimal VLMs Need Only One Visual Token but Larger Models', 'url': 'https://huggingface.co/papers/2411.03312', 'abstract': 'Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5-10times), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.', 'score': 1, 'issue_id': 446, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '60954fb0c9d4b5fb', 'data': {'categories': ['#reasoning', '#cv', '#inference', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Меньше токенов, больше модель: новый подход к оптимизации VLM', 'desc': 'Эта статья исследует оптимальный баланс между количеством визуальных токенов и параметрами языковой модели (LLM) в vision-language моделях (VLM). Авторы установили закономерности масштабирования, которые показывают, как производительность VLM меняется в зависимости от этих двух факторов. Результаты демонстрируют, что для задач визуального рассуждения оптимальная производительность достигается при использовании самой большой LLM, которая помещается в бюджет вычислений, при минимизации количества визуальных токенов. Исследование предлагает новый взгляд на оптимизацию VLM для эффективного вывода.'}, 'en': {'title': 'Maximizing VLM Efficiency with Token Compression', 'desc': 'This paper explores the efficiency of Vision Language Models (VLMs) in visual understanding tasks, focusing on the trade-off between the number of visual tokens and the size of the language model (LLM). The authors establish scaling laws to determine how performance varies with these two factors, revealing that optimal performance is achieved with minimal visual tokens, often reducing to just one token. They highlight that traditional token reduction methods may not be sufficient for achieving the best inference performance, suggesting that higher compression ratios are necessary. The paper also proposes initial strategies for adapting VLMs to work effectively under these high token compression conditions.'}, 'zh': {'title': '优化推理：最大化LLM与最小化视觉标记的平衡', 'desc': '视觉语言模型（VLMs）在视觉理解和推理任务中表现出色，但在实际应用中，由于处理大量输入标记（主要来自图像）所需的计算量大，推理延迟较高。为了降低推理成本，可以选择缩小大型语言模型（LLM）或减少输入图像标记的数量，后者是许多近期研究的重点。我们通过建立缩放法则来描述视觉标记数量与LLM参数之间的最佳权衡，发现对于视觉推理任务，最佳推理行为是在推理预算内使用最大的LLM，同时将视觉标记数量减少到最小，通常只需一个标记。我们的研究表明，计算最优的推理模式需要在更高的标记压缩比下进行操作。'}}}, {'id': 'https://huggingface.co/papers/2411.03823', 'title': 'Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination', 'url': 'https://huggingface.co/papers/2411.03823', 'abstract': 'The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced.', 'score': 31, 'issue_id': 457, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '3f0a02ee67213e17', 'data': {'categories': ['#leakage', '#benchmark', '#multimodal', '#training', '#dataset', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Чистота данных - залог надёжности мультимодальных ИИ-моделей', 'desc': 'Статья посвящена проблеме загрязнения данных в мультимодальных больших языковых моделях (MLLM). Авторы представляют фреймворк MM-Detect для обнаружения загрязнения в MLLM. Экспериментальные результаты показывают, что MM-Detect чувствителен к разным степеням загрязнения и может выявлять значительные улучшения производительности из-за утечки обучающего набора. Исследование также рассматривает возможность загрязнения на этапах предобучения и тонкой настройки моделей.'}, 'en': {'title': 'Detecting Contamination in Multimodal Language Models', 'desc': 'This paper addresses the challenge of data contamination in multimodal large language models (MLLMs), which can affect their performance evaluation. The authors propose a new framework called MM-Detect, specifically designed to identify contamination in MLLMs across different modalities and training phases. Their experiments show that MM-Detect can effectively detect varying levels of contamination and reveal how training set leakage impacts performance. Additionally, the study investigates contamination sources during both the pre-training and fine-tuning phases of MLLMs, providing valuable insights into potential contamination points.'}, 'zh': {'title': '多模态数据污染检测的新突破', 'desc': '本研究介绍了一种针对多模态大语言模型（MLLMs）数据污染检测的新框架MM-Detect。该框架能够有效识别在训练过程中可能出现的数据污染问题，尤其是在多模态基准测试中。实验结果表明，MM-Detect对不同程度的数据污染非常敏感，并能显著提升性能评估的准确性。此外，我们还探讨了数据污染可能源自LLMs的预训练阶段和MLLMs的微调阶段，为理解污染引入的时机提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2411.03562', 'title': 'Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level', 'url': 'https://huggingface.co/papers/2411.03562', 'abstract': "We introduce Agent K v1.0, an end-to-end autonomous data science agent designed to automate, optimise, and generalise across diverse data science tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle by learning from experience. It leverages a highly flexible structured reasoning framework to enable it to dynamically process memory in a nested structure, effectively learning from accumulated experience stored to handle complex reasoning tasks. It optimises long- and short-term memory by selectively storing and retrieving key information, guiding future decisions based on environmental rewards. This iterative approach allows it to refine decisions without fine-tuning or backpropagation, achieving continuous improvement through experiential learning. We evaluate our agent's apabilities using Kaggle competitions as a case study. Following a fully automated protocol, Agent K v1.0 systematically addresses complex and multimodal data science tasks, employing Bayesian optimisation for hyperparameter tuning and feature engineering. Our new evaluation framework rigorously assesses Agent K v1.0's end-to-end capabilities to generate and send submissions starting from a Kaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\\% success rate across tasks, spanning tabular, computer vision, NLP, and multimodal domains. When benchmarking against 5,856 human Kaggle competitors by calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\\%, demonstrating an overall skill level comparable to Expert-level users. Notably, its Elo-MMR score falls between the first and third quartiles of scores achieved by human Grandmasters. Furthermore, our results indicate that Agent K v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a record of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's progression system.", 'score': 23, 'issue_id': 457, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '1db584382b826315', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#agi', '#optimization', '#multimodal', '#training', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Agent K v1.0: Автономный ИИ-агент для решения задач в области науки о данных', 'desc': 'Представлен Agent K v1.0 - автономный агент для автоматизации задач в области науки о данных. Он использует структурированную систему рассуждений для обработки памяти и обучения на основе опыта. Agent K v1.0 оптимизирует долгосрочную и краткосрочную память, что позволяет ему улучшать решения без дополнительной настройки. Оценка возможностей агента проводилась на соревнованиях Kaggle, где он продемонстрировал 92.5% успешность в решении разнообразных задач.'}, 'en': {'title': 'Agent K v1.0: Your Autonomous Data Science Expert!', 'desc': "Agent K v1.0 is an autonomous data science agent that automates the entire data science life cycle, learning from its experiences to improve over time. It uses a structured reasoning framework to manage memory and handle complex tasks without the need for traditional fine-tuning methods. By employing Bayesian optimization for hyperparameter tuning and feature engineering, it effectively addresses a variety of data science challenges. The agent's performance has been validated through Kaggle competitions, where it achieved a high success rate and ranked competitively against human experts, demonstrating its advanced capabilities in multiple domains."}, 'zh': {'title': 'Agent K v1.0：自动化数据科学的未来', 'desc': '我们介绍了Agent K v1.0，这是一个端到端的自主数据科学代理，旨在自动化、优化和泛化各种数据科学任务。Agent K v1.0 完全自动化，能够管理整个数据科学生命周期，并通过经验学习来提升能力。它利用灵活的结构化推理框架，动态处理嵌套结构中的记忆，有效地从积累的经验中学习，以应对复杂的推理任务。通过选择性存储和检索关键信息，Agent K v1.0 优化了短期和长期记忆，基于环境奖励指导未来决策，展现出与人类专家相当的技能水平。'}}}, {'id': 'https://huggingface.co/papers/2411.03884', 'title': 'Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models', 'url': 'https://huggingface.co/papers/2411.03884', 'abstract': 'Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.', 'score': 5, 'issue_id': 459, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '6ed1524392784244', 'data': {'categories': ['#optimization', '#math', '#training', '#open_source', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'PolyCom: Революция в активационных функциях для трансформеров', 'desc': 'Статья представляет новую категорию активационных функций для трансформеров, называемую PolyCom (полиномиальные композитные активации). Авторы теоретически обосновывают, что PolyCom обладает улучшенной выразительностью и эффективностью по сравнению с другими активационными функциями. Эксперименты показывают, что использование PolyCom в больших языковых моделях (LLM) позволяет им лучше улавливать взаимодействия высокого порядка в данных. Результаты демонстрируют значительные улучшения в точности и скорости сходимости по сравнению с традиционными активационными функциями.'}, 'en': {'title': 'Unlocking Transformer Potential with Polynomial Activations', 'desc': 'This paper introduces a new type of activation function called Polynomial Composition Activations (PolyCom) for transformer models. The authors argue that PolyCom enhances the nonlinearity of transformers, which is crucial for improving their representational capacity. Through mathematical analysis, they show that networks using PolyCom can approximate complex functions more efficiently than those using traditional activation functions. Empirical tests on large language models reveal that PolyCom leads to better performance in terms of accuracy and convergence, demonstrating its potential as a superior alternative in machine learning applications.'}, 'zh': {'title': '多项式组合激活函数：提升变换器性能的新方法', 'desc': '本文提出了一种新型的多项式组合激活函数（PolyCom），旨在优化变换器的动态性能。我们通过数学分析证明了PolyCom在表达能力和效率方面优于其他激活函数。实验结果表明，使用PolyCom的网络在逼近光滑函数时所需的参数更少，且在大型语言模型的预训练配置中表现出更高的准确性和收敛速度。我们的研究表明，PolyCom能够有效捕捉数据中的高阶交互，从而显著提升模型性能。'}}}, {'id': 'https://huggingface.co/papers/2411.03590', 'title': 'From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond', 'url': 'https://huggingface.co/papers/2411.03590', 'abstract': "Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI's o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1's performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.", 'score': 1, 'issue_id': 460, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': 'c43efbb9c2b1c150', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#inference', '#optimization', '#healthcare', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Новая эра моделей с встроенным reasoning: o1-preview превосходит GPT-4 в медицине', 'desc': 'Статья исследует эффективность модели o1-preview от OpenAI на различных медицинских тестах в сравнении с GPT-4 и стратегией Medprompt. Авторы обнаружили, что o1-preview превосходит GPT-4 даже без специальных техник промптинга, но традиционные методы обучения на нескольких примерах (few-shot learning) снижают её производительность. Исследование выявило, что o1-preview достигает наилучших результатов при более высокой стоимости, в то время как GPT-4 с Medprompt остаётся ценной альтернативой в определённых контекстах. Авторы также отмечают необходимость создания новых, более сложных медицинских тестов, так как o1-preview близка к насыщению на существующих бенчмарках.'}, 'en': {'title': 'Unlocking Medical Mastery: The Power of Run-Time Reasoning in LLMs', 'desc': "This paper explores the effectiveness of run-time steering strategies, particularly focusing on the Medprompt technique, in enhancing the performance of large language models (LLMs) in specialized fields like medicine. It introduces OpenAI's o1-preview model, which employs a new approach to reasoning before generating responses, showing superior performance compared to the GPT-4 series even without additional prompting. The study evaluates various medical benchmarks and finds that traditional few-shot prompting may not be beneficial for reasoning-native models like o1-preview, while ensembling remains a viable but resource-intensive option. The analysis reveals a trade-off between cost and accuracy, highlighting the need for new benchmarks to further challenge these advanced models."}, 'zh': {'title': '运行时推理：提升大型语言模型的关键', 'desc': '本文探讨了运行时引导策略（如Medprompt）在提升大型语言模型（LLMs）在医学等专业领域表现中的重要性。研究表明，o1-preview模型在生成最终响应之前进行运行时推理，能够在多种医学挑战基准上超越GPT-4系列。尽管o1-preview在没有提示技术的情况下表现优异，但经典的提示工程策略在新推理模型中可能不再有效。最后，研究指出o1-preview在现有医学基准上已接近饱和，强调了开发新基准的必要性。'}}}, {'id': 'https://huggingface.co/papers/2411.04109', 'title': 'Self-Consistency Preference Optimization', 'url': 'https://huggingface.co/papers/2411.04109', 'abstract': 'Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. In this work, we extend the self-consistency concept to help train models. We thus introduce self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. We show ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku.', 'score': 1, 'issue_id': 459, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '213f2796c0bc72ae', 'data': {'categories': ['#small_models', '#reasoning', '#rl', '#optimization', '#training', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Самосогласованность как ключ к улучшению ИИ без учителя', 'desc': 'Статья представляет новый метод самосогласованной оптимизации предпочтений (ScPO) для улучшения моделей машинного обучения без использования аннотированных данных. Этот подход расширяет концепцию самосогласованности, применяемую обычно на этапе вывода, на процесс обучения модели. ScPO итеративно обучает модель предпочитать согласованные ответы над несогласованными на новых задачах без учителя. Метод показывает значительные улучшения в задачах рассуждения, таких как GSM8K и MATH, сокращая разрыв с обучением с учителем.'}, 'en': {'title': 'Empowering Models with Self-Consistency for Better Reasoning', 'desc': "This paper introduces a new method called self-consistency preference optimization (ScPO) to enhance the training of machine learning models without human annotations. ScPO focuses on improving the consistency of answers by iteratively training the model to prefer consistent responses over inconsistent ones. The authors demonstrate that ScPO significantly outperforms traditional reward model training on complex reasoning tasks, such as GSM8K and MATH, and even approaches the performance of supervised training. Additionally, when combined with standard supervised learning, ScPO further boosts the model's performance, achieving superior results on various benchmarks."}, 'zh': {'title': '自我一致性优化，提升模型推理能力！', 'desc': '自我对齐是指模型在没有人工标注的情况下自我改进的过程，近年来这一研究领域迅速发展。然而，现有技术在复杂推理任务中常常无法有效提升性能，因为很难分配正确的奖励。本文提出了一种新的方法——自我一致性偏好优化（ScPO），它通过迭代训练一致的答案，使其优于不一致的答案，从而解决无监督新问题。实验结果表明，ScPO在推理任务上显著优于传统的奖励模型训练，且与标准监督学习结合后效果更佳。'}}}, {'id': 'https://huggingface.co/papers/2410.23218', 'title': 'OS-ATLAS: A Foundation Action Model for Generalist GUI Agents', 'url': 'https://huggingface.co/papers/2410.23218', 'abstract': 'Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.', 'score': 41, 'issue_id': 410, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'd7a3f0fd08f934d5', 'data': {'categories': ['#synthetic', '#benchmark', '#cv', '#graphs', '#multiplatform', '#data', '#training', '#dataset', '#open_source', '#agents'], 'emoji': '🖥️', 'ru': {'title': 'OS-Atlas: Открытая модель для универсального взаимодействия с GUI', 'desc': 'Исследователи разработали OS-Atlas - основополагающую модель для взаимодействия с графическим интерфейсом пользователя (GUI). Модель использует инновационный подход к данным и моделированию, что позволяет ей эффективно работать с GUI и решать задачи вне распределения (OOD). Авторы создали открытый набор инструментов для синтеза данных о GUI на различных платформах и выпустили крупнейший открытый кросс-платформенный корпус, содержащий более 13 миллионов элементов GUI. OS-Atlas демонстрирует значительное улучшение производительности по сравнению с предыдущими моделями на шести тестовых наборах, охватывающих мобильные, настольные и веб-платформы.'}, 'en': {'title': 'Empowering Open-Source GUI Agents with OS-Atlas', 'desc': "This paper introduces OS-Atlas, an open-source foundational model designed for GUI grounding and Out-Of-Distribution (OOD) tasks. It addresses the performance gap between commercial Vision-Language Models (VLMs) and open-source alternatives by providing a comprehensive toolkit for synthesizing GUI grounding data across various platforms. The authors present the largest open-source cross-platform GUI grounding dataset, featuring over 13 million GUI elements, which enhances the model's ability to understand and generalize from GUI screenshots. Extensive evaluations show that OS-Atlas outperforms previous models, offering insights for further advancements in open-source VLM capabilities."}, 'zh': {'title': '开源GUI模型OS-Atlas：提升界面理解能力的创新之路', 'desc': '本论文介绍了OS-Atlas，一个开源的GUI动作模型，专注于GUI定位和超出分布（OOD）任务。我们开发了一个工具包，可以在多个平台上合成GUI定位数据，包括Windows、Linux、MacOS、Android和网页。OS-Atlas利用超过1300万个GUI元素的数据集，结合创新的模型训练方法，显著提高了对GUI截图的理解能力。通过在六个基准测试中进行广泛评估，OS-Atlas在移动、桌面和网页平台上表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2411.00027', 'title': 'Personalization of Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2411.00027', 'abstract': 'Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.', 'score': 23, 'issue_id': 409, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'a190b2e727d2d0ad', 'data': {'categories': ['#benchmark', '#training', '#dataset', '#survey', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Объединяя подходы: комплексный взгляд на персонализацию больших языковых моделей', 'desc': 'Статья посвящена персонализации больших языковых моделей (LLM) и объединяет два основных направления исследований в этой области. Авторы предлагают таксономию использования персонализированных LLM, формализуют основы и расширяют понятия персонализации. Они систематизируют литературу по различным аспектам, включая методы персонализации, наборы данных и способы оценки. В работе также выделяются нерешенные проблемы и открытые вопросы в данной области исследований.'}, 'en': {'title': 'Bridging Personalization Gaps in Large Language Models', 'desc': 'This paper addresses the growing need for personalization in Large Language Models (LLMs) by creating a comprehensive framework that connects personalized text generation with applications like recommendation systems. It introduces a taxonomy that categorizes various aspects of personalized LLMs, including techniques, datasets, and evaluation methods. The authors formalize the concept of personalization in LLMs, discussing its different dimensions and the challenges faced in this area. By synthesizing existing research and identifying open problems, the paper serves as a guide for researchers and practitioners interested in the personalization of LLMs.'}, 'zh': {'title': '统一个性化大型语言模型的研究', 'desc': '本文探讨了大型语言模型（LLMs）个性化的重要性及其应用。我们首次将个性化文本生成与个性化相关的下游应用（如推荐系统）结合起来，提出了个性化LLMs的分类法。文章对个性化LLMs的基础进行了形式化定义，并讨论了个性化的不同方面、使用场景和需求。最后，我们总结了现有文献，并指出了个性化LLMs面临的挑战和未解决的问题，以帮助研究人员和从业者更好地理解这一领域。'}}}, {'id': 'https://huggingface.co/papers/2411.00322', 'title': 'Constant Acceleration Flow', 'url': 'https://huggingface.co/papers/2411.00322', 'abstract': 'Rectified flow and reflow procedures have significantly advanced fast generation by progressively straightening ordinary differential equation (ODE) flows. They operate under the assumption that image and noise pairs, known as couplings, can be approximated by straight trajectories with constant velocity. However, we observe that modeling with constant velocity and using reflow procedures have limitations in accurately learning straight trajectories between pairs, resulting in suboptimal performance in few-step generation. To address these limitations, we introduce Constant Acceleration Flow (CAF), a novel framework based on a simple constant acceleration equation. CAF introduces acceleration as an additional learnable variable, allowing for more expressive and accurate estimation of the ODE flow. Moreover, we propose two techniques to further improve estimation accuracy: initial velocity conditioning for the acceleration model and a reflow process for the initial velocity. Our comprehensive studies on toy datasets, CIFAR-10, and ImageNet 64x64 demonstrate that CAF outperforms state-of-the-art baselines for one-step generation. We also show that CAF dramatically improves few-step coupling preservation and inversion over Rectified flow. Code is available at https://github.com/mlvlab/CAF{https://github.com/mlvlab/CAF}.', 'score': 20, 'issue_id': 412, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'edca1b3005d37bab', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'CAF: Ускоряем генерацию изображений с помощью постоянного ускорения', 'desc': 'Статья представляет новый подход к ускорению генерации изображений в машинном обучении, называемый Constant Acceleration Flow (CAF). В отличие от существующих методов, CAF использует модель постоянного ускорения вместо постоянной скорости для более точного моделирования траекторий между парами изображений и шума. Авторы вводят ускорение как дополнительную обучаемую переменную и предлагают техники улучшения точности оценки, включая обусловливание начальной скорости и процесс рефлоу. Эксперименты на различных наборах данных показывают превосходство CAF над современными методами в одношаговой генерации и сохранении связей при генерации за несколько шагов.'}, 'en': {'title': 'Accelerating Image Generation with Constant Acceleration Flow', 'desc': 'This paper presents a new method called Constant Acceleration Flow (CAF) to improve the generation of images using ordinary differential equations (ODEs). Traditional methods assume that the flow between images and noise can be modeled as straight lines moving at a constant speed, which can lead to inaccuracies. CAF enhances this by introducing acceleration as a learnable variable, allowing for more flexible and precise modeling of the flow. The authors demonstrate that CAF outperforms existing methods in generating images with fewer steps while maintaining better quality and accuracy.'}, 'zh': {'title': '常加速度流：提升图像生成的新方法', 'desc': '本文提出了一种新的框架，称为常加速度流（CAF），用于改进图像生成过程。CAF通过引入加速度作为可学习变量，能够更准确地估计常微分方程（ODE）流。与传统的直线轨迹假设不同，CAF允许在生成过程中考虑加速度，从而提高生成质量。实验结果表明，CAF在少步生成和耦合保持方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2410.23266', 'title': 'TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models', 'url': 'https://huggingface.co/papers/2410.23266', 'abstract': "Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, how well do the models truly perform visual temporal reasoning? Our study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. To systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame Information Disparity. Following these principles, we introduce TOMATO, Temporal Reasoning Multimodal Evaluation, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. TOMATO comprises 1,484 carefully curated, human-annotated questions spanning six tasks (i.e., action count, direction, rotation, shape & trend, velocity & frequency, and visual cues), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. Our comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model. Moreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence. We believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending human world dynamics through the video modality.", 'score': 19, 'issue_id': 416, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '2743c77af808246f', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#graphs', '#video', '#multimodal', '#survey'], 'emoji': '🍅', 'ru': {'title': 'TOMATO: Новый стандарт для оценки временного рассуждения в видеоанализе', 'desc': 'Исследователи разработали новый бенчмарк TOMATO для оценки способностей мультимодальных фундаментальных моделей (МФМ) к временному рассуждению при анализе видео. Бенчмарк включает 1484 тщательно отобранных вопроса по шести задачам, применяемых к 1417 видео различных сценариев. Авторы выявили значительный разрыв в производительности между человеком и лучшей моделью, а также фундаментальные ограничения существующих МФМ в интерпретации последовательности кадров. TOMATO призван стать важным инструментом для оценки следующего поколения МФМ и стимулировать развитие систем ИИ, способных понимать динамику человеческого мира через видеомодальность.'}, 'en': {'title': 'TOMATO: A New Benchmark for Evaluating Temporal Reasoning in Video Understanding', 'desc': 'This paper investigates the true capabilities of Multimodal Foundation Models (MFMs) in visual temporal reasoning for video understanding. The authors argue that existing benchmarks may overstate the performance of these models, as many tasks can be solved using only a few frames or even out-of-order frames. To address this, they introduce TOMATO, a new benchmark designed to rigorously evaluate MFMs based on three principles: Multi-Frame Gain, Frame Order Sensitivity, and Frame Information Disparity. Their findings reveal a significant performance gap between human understanding and model capabilities, highlighting the need for improved temporal reasoning in future AI systems.'}, 'zh': {'title': '评估多模态模型的时间推理能力', 'desc': '本研究探讨了多模态基础模型（MFM）在视频理解中的视觉时间推理能力。我们发现，现有基准测试可能高估了这些模型的表现，因为许多问题可以通过少量或无序的帧来解决。为此，我们提出了三个原则和相应的评估指标，并引入了TOMATO基准，以系统评估MFM在视频理解中的时间推理能力。我们的评估显示，当前模型与人类表现之间存在57.3%的差距，揭示了MFM在处理连续帧时的基本局限性。'}}}, {'id': 'https://huggingface.co/papers/2411.00776', 'title': 'Randomized Autoregressive Visual Generation', 'url': 'https://huggingface.co/papers/2411.00776', 'abstract': "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer", 'score': 16, 'issue_id': 408, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '0cc2c0f19f735f79', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#optimization', '#open_source', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Случайная перестановка для улучшения генерации изображений', 'desc': 'Статья представляет метод Randomized AutoRegressive modeling (RAR) для генерации изображений. RAR использует случайную перестановку входной последовательности во время обучения авторегрессионной модели, что позволяет учитывать двунаправленный контекст. Метод сохраняет совместимость с языковыми моделями и достигает нового state-of-the-art результата на бенчмарке ImageNet-256 с показателем FID 1.48. RAR превосходит как авторегрессионные, так и диффузионные методы генерации изображений.'}, 'en': {'title': 'Revolutionizing Image Generation with Randomized AutoRegressive Modeling', 'desc': 'This paper introduces Randomized AutoRegressive modeling (RAR), a novel approach for generating images that enhances performance while remaining compatible with existing language modeling techniques. RAR employs a unique training method where the input sequence is randomly shuffled during the autoregressive training process, allowing the model to learn from various factorization orders. This strategy helps the model to better understand and utilize bidirectional contexts, leading to improved image generation capabilities. The results show that RAR achieves a remarkable FID score of 1.48 on the ImageNet-256 benchmark, outperforming previous state-of-the-art methods in both autoregressive and diffusion-based image generation.'}, 'zh': {'title': '随机自回归建模：图像生成的新突破', 'desc': '本文提出了一种随机自回归建模（RAR）方法用于视觉生成，在图像生成任务上设定了新的最先进性能，同时与语言建模框架完全兼容。RAR方法简单：在标准的自回归训练过程中，输入序列通常按光栅形式排列，但以概率r随机打乱为不同的因子化顺序，r从1开始，随着训练线性衰减到0。这种退火训练策略使模型能够学习最大化所有因子化顺序的期望似然，从而有效提高模型建模双向上下文的能力。RAR保持了自回归建模框架的完整性，确保与语言建模的完全兼容，同时在图像生成中显著提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2411.00660', 'title': 'Physics in Next-token Prediction', 'url': 'https://huggingface.co/papers/2411.00660', 'abstract': "We discovered the underlying physics in Next-token Prediction (NTP). We identified the law of information conservation within NTP and proposed the First Law of Information Capacity (IC-1), demonstrating that the essence of intelligence emergence in auto-regressive models is fundamentally a process of information transfer. We also introduced Landauer's Principle into NTP, formulating the Second Law of Information Capacity (IC-2), which establishes the relationship between auto-regressive model training and energy consumption. Additionally, we presented several corollaries, which hold practical significance for production practices. Finally, we validated the compatibility and complementarity of our findings with existing theories.", 'score': 11, 'issue_id': 416, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '9114a8de1a432c2d', 'data': {'categories': ['#science', '#optimization', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Физика информации раскрывает тайны искусственного интеллекта', 'desc': 'Исследователи обнаружили фундаментальные физические принципы в задаче предсказания следующего токена (NTP). Они сформулировали Первый закон информационной емкости, показывающий, что возникновение интеллекта в авторегрессионных моделях по сути является процессом передачи информации. Введя принцип Ландауэра в NTP, ученые также сформулировали Второй закон информационной емкости, связывающий обучение авторегрессионных моделей с энергопотреблением. Исследование предлагает ряд практических следствий и согласуется с существующими теориями в области машинного обучения.'}, 'en': {'title': 'Unveiling the Physics of Next-Token Prediction', 'desc': "This paper explores the physics behind Next-token Prediction (NTP) in machine learning. It introduces the First Law of Information Capacity (IC-1), which highlights how intelligence in auto-regressive models arises from information transfer. The authors also apply Landauer's Principle to NTP, leading to the Second Law of Information Capacity (IC-2), linking model training to energy consumption. The findings are supported by practical corollaries and align with existing theories in the field."}, 'zh': {'title': '揭示自回归模型中的信息与能量关系', 'desc': '我们发现了下一步预测（NTP）中的基本物理原理。我们识别了NTP中的信息守恒定律，并提出了信息容量第一定律（IC-1），证明了自回归模型中智能出现的本质是信息传递的过程。我们还将朗道原理引入NTP，制定了信息容量第二定律（IC-2），建立了自回归模型训练与能量消耗之间的关系。此外，我们提出了几个具有实际意义的推论，验证了我们的发现与现有理论的兼容性和互补性。'}}}, {'id': 'https://huggingface.co/papers/2410.24159', 'title': 'GPT or BERT: why not both?', 'url': 'https://huggingface.co/papers/2410.24159', 'abstract': 'We present a simple way to merge masked language modeling with causal language modeling. This hybrid training objective results in a model that combines the strengths of both modeling paradigms within a single transformer stack: GPT-BERT can be transparently used like any standard causal or masked language model. We test the pretraining process that enables this flexible behavior on the BabyLM Challenge 2024. The results show that the hybrid pretraining outperforms masked-only or causal-only models. We openly release the models, training corpora and code.', 'score': 10, 'issue_id': 415, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'f46bbe7c538f7a87', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Гибридное языковое моделирование: лучшее из двух миров', 'desc': 'Исследователи представили новый метод объединения маскированного и причинно-следственного языкового моделирования. Результатом стала модель GPT-BERT, сочетающая преимущества обоих подходов в единой трансформерной архитектуре. Эксперименты в рамках BabyLM Challenge 2024 показали, что гибридное предобучение превосходит модели, использующие только один из методов. Авторы открыто публикуют модели, обучающие корпуса и код.'}, 'en': {'title': 'Merging Masked and Causal Language Models for Enhanced Performance', 'desc': 'This paper introduces a novel approach that merges masked language modeling (MLM) with causal language modeling (CLM) into a single transformer architecture. The resulting model, named GPT-BERT, leverages the advantages of both paradigms, allowing it to function effectively as either a standard masked or causal language model. The authors evaluate this hybrid training method on the BabyLM Challenge 2024, demonstrating that it outperforms models trained exclusively with either MLM or CLM. Additionally, they provide open access to the models, training data, and code to facilitate further research.'}, 'zh': {'title': '混合预训练，模型性能双赢！', 'desc': '本文提出了一种将掩码语言建模与因果语言建模相结合的简单方法。这种混合训练目标使得模型能够在单个变换器架构中结合两种建模范式的优点。我们在2024年BabyLM挑战赛中测试了这种灵活行为的预训练过程。结果表明，混合预训练的性能优于仅使用掩码或仅使用因果的模型。'}}}, {'id': 'https://huggingface.co/papers/2410.22370', 'title': 'Survey of User Interface Design and Interaction Techniques in Generative AI Applications', 'url': 'https://huggingface.co/papers/2410.22370', 'abstract': 'The applications of generative AI have become extremely impressive, and the interplay between users and AI is even more so. Current human-AI interaction literature has taken a broad look at how humans interact with generative AI, but it lacks specificity regarding the user interface designs and patterns used to create these applications. Therefore, we present a survey that comprehensively presents taxonomies of how a human interacts with AI and the user interaction patterns designed to meet the needs of a variety of relevant use cases. We focus primarily on user-guided interactions, surveying interactions that are initiated by the user and do not include any implicit signals given by the user. With this survey, we aim to create a compendium of different user-interaction patterns that can be used as a reference for designers and developers alike. In doing so, we also strive to lower the entry barrier for those attempting to learn more about the design of generative AI applications.', 'score': 10, 'issue_id': 409, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '9701ceb4e85eeeba', 'data': {'categories': ['#agents', '#survey', '#architecture', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Путеводитель по взаимодействию человека и ИИ', 'desc': 'Статья представляет обзор таксономий взаимодействия человека с генеративным ИИ и паттернов пользовательского интерфейса для различных сценариев использования. Авторы фокусируются на взаимодействиях, инициируемых пользователем, без учета неявных сигналов. Цель работы - создать справочник паттернов взаимодействия для дизайнеров и разработчиков приложений генеративного ИИ. Исследование направлено на снижение входного барьера для тех, кто хочет изучить дизайн приложений генеративного ИИ.'}, 'en': {'title': 'Enhancing User Interaction with Generative AI', 'desc': 'This paper surveys the ways users interact with generative AI, focusing specifically on user-guided interactions. It identifies and categorizes various user interface designs and interaction patterns that cater to different use cases. By providing a comprehensive taxonomy, the authors aim to serve as a reference for designers and developers in creating more effective generative AI applications. The goal is to make it easier for newcomers to understand and engage with the design aspects of these technologies.'}, 'zh': {'title': '提升人机交互，设计更智能的生成式AI应用', 'desc': '本论文探讨了生成式人工智能（AI）与用户之间的互动，强调了用户界面设计的重要性。我们提供了一份全面的调查，分类了人类与AI的互动方式，特别关注用户主导的交互模式。通过这项调查，我们希望为设计师和开发者提供不同的用户交互模式参考，降低学习生成式AI应用设计的门槛。最终目标是提升人机交互的效率和用户体验。'}}}, {'id': 'https://huggingface.co/papers/2410.23775', 'title': 'In-Context LoRA for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2410.23775', 'abstract': 'Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20sim 100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA', 'score': 9, 'issue_id': 407, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '748dab03a37a21a4', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Раскрытие скрытого потенциала DiT для многозадачной генерации изображений', 'desc': 'Исследование предлагает новый подход к использованию диффузионных трансформеров (DiT) для генерации изображений в различных задачах. Авторы обнаружили, что существующие модели DiT для преобразования текста в изображение уже обладают способностью к контекстной генерации без дополнительной настройки. На основе этого наблюдения они разработали простой pipeline, включающий конкатенацию изображений, совместное создание подписей и применение LoRA-тюнинга на небольших датасетах. Предложенный метод, названный IC-LoRA, позволяет генерировать высококачественные наборы изображений, лучше соответствующие заданным промптам.'}, 'en': {'title': 'Unlocking In-Context Generation with IC-LoRA', 'desc': 'This paper investigates the use of diffusion transformers (DiTs) for generating images without being tied to specific tasks. The authors propose that DiTs can generate images effectively with minimal adjustments, leveraging their inherent in-context generation capabilities. They introduce a new method called In-Context LoRA (IC-LoRA), which simplifies the process by concatenating images and using joint captioning, along with small dataset tuning. This approach enhances the quality of generated images while maintaining a flexible architecture that can adapt to various tasks without extensive retraining.'}, 'zh': {'title': '激活上下文生成能力，提升图像生成质量', 'desc': '本研究探讨了扩散变换器（DiTs）在无任务特定的图像生成中的应用。我们提出，文本到图像的DiTs本身具备上下文生成能力，只需少量调整即可激活。通过实验，我们展示了现有的文本到图像DiTs能够在不调整的情况下有效进行上下文生成。我们提出了一种简单的流程，利用DiTs的上下文能力，生成高保真度的图像集，且不需要对原始模型进行修改。'}}}, {'id': 'https://huggingface.co/papers/2411.00412', 'title': 'Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation', 'url': 'https://huggingface.co/papers/2411.00412', 'abstract': "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but often produce hallucinations for complex ones. While integrating LLMs with tools can increase reliability, this approach typically results in over-reliance on tools, diminishing the model's ability to solve simple problems through basic reasoning. In contrast, human experts first assess problem complexity using domain knowledge before choosing an appropriate solution approach. Inspired by this human problem-solving process, we propose a novel two-component fine-tuning method. In the first component World Knowledge Distillation (WKD), LLMs learn directly from solutions generated using tool's information to internalize domain knowledge. In the second component Tool Usage Adaptation (TUA), we partition problems into easy and hard categories based on the model's direct answering accuracy. While maintaining the same alignment target for easy problems as in WKD, we train the model to intelligently switch to tool usage for more challenging problems. We validate our method on six scientific benchmark datasets, spanning mathematics, climate science and epidemiology. On average, our models demonstrate a 28.18% improvement in answer accuracy and a 13.89% increase in tool usage precision across all datasets, surpassing state-of-the-art models including GPT-4o and Claude-3.5.", 'score': 9, 'issue_id': 407, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '27e4deefc7d09df0', 'data': {'categories': ['#science', '#reasoning', '#rl', '#hallucinations', '#benchmark', '#multilingual', '#math', '#training', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Умное переключение: как научить ИИ эффективно решать задачи разной сложности', 'desc': 'Исследование посвящено улучшению способности больших языковых моделей (LLM) решать научные задачи. Авторы предлагают двухкомпонентный метод дообучения: дистилляция мировых знаний и адаптация использования инструментов. Этот подход позволяет LLM эффективно решать простые задачи с помощью базовых рассуждений, а для сложных - прибегать к инструментам. Метод показал значительное улучшение точности ответов и точности использования инструментов на шести научных наборах данных.'}, 'en': {'title': 'Enhancing LLMs: Smart Tool Use for Complex Problems', 'desc': "This paper addresses the limitations of Large Language Models (LLMs) in solving complex scientific problems, which often lead to inaccuracies or 'hallucinations'. The authors propose a two-component fine-tuning method that mimics human problem-solving strategies by first assessing problem complexity. The first component, World Knowledge Distillation (WKD), allows LLMs to learn from solutions that utilize external tools, while the second component, Tool Usage Adaptation (TUA), helps the model categorize problems as easy or hard and decide when to use tools. The proposed method shows significant improvements in accuracy and tool usage precision across various scientific datasets, outperforming existing models."}, 'zh': {'title': '智能切换，提升模型解决问题的能力', 'desc': '大型语言模型（LLMs）在解决简单科学问题方面表现出色，但在复杂问题上常常出现幻觉。我们提出了一种新颖的双组件微调方法，模仿人类专家的解决问题过程。第一个组件是世界知识蒸馏（WKD），使LLMs从工具生成的解决方案中学习领域知识。第二个组件是工具使用适应（TUA），根据模型的直接回答准确性将问题分为简单和困难两类，从而提高模型在复杂问题上的工具使用能力。'}}}, {'id': 'https://huggingface.co/papers/2411.00771', 'title': 'CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes', 'url': 'https://huggingface.co/papers/2411.00771', 'abstract': 'Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10times compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs. The project page is available at https://dekuliutesla.github.io/CityGaussianV2/.', 'score': 8, 'issue_id': 415, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '00c1f9c65cf89cfe', 'data': {'categories': ['#benchmark', '#graphs', '#optimization', '#training', '#3d'], 'emoji': '🏙️', 'ru': {'title': 'CityGaussianV2: Эффективная реконструкция крупномасштабных сцен с помощью улучшенного Gaussian Splatting', 'desc': 'Статья представляет CityGaussianV2 - новый подход к реконструкции крупномасштабных сцен, решающий проблемы геометрической точности и эффективности в 3D Gaussian Splatting. Авторы внедряют технику уплотнения на основе декомпозированного градиента и регрессии глубины для устранения размытых артефактов и ускорения сходимости. Для масштабирования вводится фильтр удлинения, снижающий взрывной рост количества гауссианов. Оптимизированный конвейер CityGaussian позволяет достичь 10-кратного сжатия, экономии времени обучения на 25% и снижения использования памяти на 50%.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with CityGaussianV2', 'desc': 'This paper introduces CityGaussianV2, a new method for reconstructing large-scale 3D scenes using Gaussian splatting techniques. It addresses challenges in geometric accuracy and efficiency that arise from the unstructured nature of traditional 3D Gaussian Splatting. The authors implement a novel densification and depth regression technique to improve image clarity and speed up the training process. Additionally, they optimize the training pipeline to reduce memory usage and training time significantly while maintaining high visual quality and accuracy.'}, 'zh': {'title': '高效精准的大规模场景重建新方法', 'desc': '3D高斯点云（3DGS）在辐射场重建中取得了显著进展，但在复杂场景中准确表示表面仍然是一个挑战。本文提出了CityGaussianV2，一种针对大规模场景重建的新方法，解决了几何精度和效率的问题。我们采用分解梯度的密集化和深度回归技术，消除模糊伪影并加速收敛。通过引入延伸滤波器，我们有效地减少了高斯数量的爆炸，同时优化了CityGaussian管道，实现了训练时间和内存使用的显著节省。'}}}, {'id': 'https://huggingface.co/papers/2411.00233', 'title': 'SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models', 'url': 'https://huggingface.co/papers/2411.00233', 'abstract': 'The state of health (SOH) of a Li-ion battery is a critical parameter that determines the remaining capacity and the remaining lifetime of the battery. In this paper, we propose SambaMixer a novel structured state space model (SSM) for predicting the state of health of Li-ion batteries. The proposed SSM is based on the MambaMixer architecture, which is designed to handle multi-variate time signals. We evaluate our model on the NASA battery discharge dataset and show that our model outperforms the state-of-the-art on this dataset. We further introduce a novel anchor-based resampling method which ensures time signals are of the expected length while also serving as augmentation technique. Finally, we condition prediction on the sample time and the cycle time difference using positional encodings to improve the performance of our model and to learn recuperation effects. Our results proof that our model is able to predict the SOH of Li-ion batteries with high accuracy and robustness.', 'score': 7, 'issue_id': 410, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '6361ca66f5ca137f', 'data': {'categories': ['#science', '#optimization', '#healthcare', '#training', '#dataset', '#architecture'], 'emoji': '🔋', 'ru': {'title': 'Точное прогнозирование срока службы аккумуляторов с помощью глубокого обучения', 'desc': 'Статья представляет SambaMixer - новую модель структурированного пространства состояний для прогнозирования состояния здоровья литий-ионных аккумуляторов. Модель основана на архитектуре MambaMixer и способна обрабатывать многомерные временные сигналы. Авторы предлагают метод ресэмплинга на основе якорей для нормализации длины сигналов и аугментации данных. Использование позиционного кодирования времени выборки и разницы циклов позволяет модели учитывать эффекты восстановления аккумуляторов.'}, 'en': {'title': 'SambaMixer: Revolutionizing Li-ion Battery Health Prediction', 'desc': 'This paper introduces SambaMixer, a new structured state space model (SSM) designed to predict the state of health (SOH) of Li-ion batteries. The model utilizes the MambaMixer architecture to effectively process multi-variate time signals, enhancing prediction accuracy. It is evaluated against the NASA battery discharge dataset, demonstrating superior performance compared to existing methods. Additionally, the paper presents an innovative anchor-based resampling technique and employs positional encodings to improve predictions by accounting for time-related factors.'}, 'zh': {'title': '高效预测锂离子电池健康状态的新方法', 'desc': '本文提出了一种新颖的结构化状态空间模型（SSM），用于预测锂离子电池的健康状态（SOH）。该模型基于MambaMixer架构，能够处理多变量时间信号。我们在NASA电池放电数据集上评估了该模型，结果显示其性能优于现有的最先进方法。此外，我们引入了一种新颖的基于锚点的重采样方法，以确保时间信号的预期长度，并作为数据增强技术。'}}}, {'id': 'https://huggingface.co/papers/2410.22901', 'title': 'HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models', 'url': 'https://huggingface.co/papers/2410.22901', 'abstract': 'We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (https://songkey.github.io/hellomeme).', 'score': 7, 'issue_id': 408, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '801963cbdcf75d7b', 'data': {'categories': ['#cv', '#video', '#training', '#open_source', '#games', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Адаптеры для текст-в-изображение моделей: новый подход к генерации мемов', 'desc': 'Статья представляет эффективный метод внедрения адаптеров в базовые модели преобразования текста в изображение. Этот подход оптимизирует механизм внимания, связанный с двумерными картами признаков, что улучшает производительность адаптера. Метод был успешно применен для генерации мемов в видеоформате. Авторы отмечают совместимость метода с производными моделями SD1.5, что делает его ценным для сообщества открытого исходного кода.'}, 'en': {'title': 'Enhancing Text-to-Image Models with Adapter Integration', 'desc': 'This paper presents a novel method for integrating adapters into text-to-image foundation models, allowing them to perform complex tasks while maintaining their ability to generalize. The method focuses on optimizing the attention mechanism associated with 2D feature maps, which significantly boosts the performance of the adapters. The effectiveness of this approach was demonstrated through the task of meme video generation, yielding impressive results. The authors aim to contribute to the open-source community by sharing their code and providing insights for post-training tasks in large text-to-image models.'}, 'zh': {'title': '适配器插入：提升文本到图像模型的能力', 'desc': '本文提出了一种有效的方法，将适配器插入文本到图像的基础模型中，从而在执行复杂的下游任务时保持基础模型的泛化能力。该方法的核心思想是优化与二维特征图相关的注意力机制，从而增强适配器的性能。我们在生成表情包视频的任务上验证了该方法，并取得了显著的结果。希望这项工作能为大型文本到图像模型的后训练任务提供一些见解，并为开源社区带来价值。'}}}, {'id': 'https://huggingface.co/papers/2411.00225', 'title': 'Fashion-VDM: Video Diffusion Model for Virtual Try-On', 'url': 'https://huggingface.co/papers/2411.00225', 'abstract': "We present Fashion-VDM, a video diffusion model (VDM) for generating virtual try-on videos. Given an input garment image and person video, our method aims to generate a high-quality try-on video of the person wearing the given garment, while preserving the person's identity and motion. Image-based virtual try-on has shown impressive results; however, existing video virtual try-on (VVT) methods are still lacking garment details and temporal consistency. To address these issues, we propose a diffusion-based architecture for video virtual try-on, split classifier-free guidance for increased control over the conditioning inputs, and a progressive temporal training strategy for single-pass 64-frame, 512px video generation. We also demonstrate the effectiveness of joint image-video training for video try-on, especially when video data is limited. Our qualitative and quantitative experiments show that our approach sets the new state-of-the-art for video virtual try-on. For additional results, visit our project page: https://johannakarras.github.io/Fashion-VDM.", 'score': 6, 'issue_id': 418, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'a412815c3df113c6', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#video', '#optimization', '#training', '#games', '#architecture'], 'emoji': '👚', 'ru': {'title': 'Виртуальная примерка одежды на видео с помощью диффузионных моделей', 'desc': 'Fashion-VDM - это модель видеодиффузии для создания виртуальных примерочных видео. Модель генерирует высококачественное видео человека, одетого в заданный предмет одежды, сохраняя при этом личность и движения человека. Авторы предлагают архитектуру на основе диффузии, разделенное беcклассификаторное управление и прогрессивную стратегию временного обучения для генерации 64-кадровых видео разрешением 512 пикселей за один проход. Эксперименты показывают, что подход устанавливает новый уровень качества в задаче виртуальной примерки одежды на видео.'}, 'en': {'title': 'Revolutionizing Virtual Try-Ons with Fashion-VDM!', 'desc': 'Fashion-VDM is a novel video diffusion model designed to create realistic virtual try-on videos by combining garment images with person videos. The model focuses on maintaining the identity and motion of the person while accurately displaying the garment. It addresses challenges in existing video virtual try-on methods, such as lack of detail and temporal consistency, by utilizing a diffusion-based architecture and a progressive training strategy. Our experiments demonstrate that Fashion-VDM achieves state-of-the-art results in video virtual try-on, even with limited video data.'}, 'zh': {'title': 'Fashion-VDM：视频虚拟试穿的新突破', 'desc': '我们提出了Fashion-VDM，这是一种用于生成虚拟试穿视频的视频扩散模型。该方法可以根据输入的服装图像和人物视频，生成高质量的试穿视频，同时保持人物的身份和动作。与现有的视频虚拟试穿方法相比，我们的方法在服装细节和时间一致性方面有显著提升。我们的实验结果表明，Fashion-VDM在视频虚拟试穿领域达到了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2411.00680', 'title': 'Zipfian Whitening', 'url': 'https://huggingface.co/papers/2411.00680', 'abstract': "The word embedding space in neural models is skewed, and correcting this can improve task performance. We point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are uniform; in reality, word frequencies follow a highly non-uniform distribution, known as Zipf's law. Surprisingly, simply performing PCA whitening weighted by the empirical word frequency that follows Zipf's law significantly improves task performance, surpassing established baselines. From a theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures. By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective, and in terms of the loss functions for imbalanced classification. Additionally, our theory corroborates that popular natural language processing methods, such as skip-gram negative sampling, WhiteningBERT, and headless language models, work well just because their word embeddings encode the empirical word frequency into the underlying probabilistic model.", 'score': 6, 'issue_id': 413, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '26e33c177a131240', 'data': {'categories': ['#reasoning', '#optimization', '#math', '#data', '#architecture'], 'emoji': '📊', 'ru': {'title': 'Улучшение векторных представлений слов с учетом закона Ципфа', 'desc': 'Статья посвящена проблеме асимметрии пространства векторных представлений слов в нейронных моделях. Авторы предлагают метод коррекции этой асимметрии с учетом закона Ципфа о распределении частот слов. Простое применение PCA отбеливания, взвешенного по эмпирической частоте слов, значительно улучшает производительность модели на различных задачах. Теоретический анализ показывает, что этот подход естественным образом подчеркивает информативные низкочастотные слова и объясняет эффективность популярных методов обработки естественного языка.'}, 'en': {'title': 'Correcting Skewness in Word Embeddings for Better Performance', 'desc': "This paper discusses how the word embedding space in neural models can be improved by addressing its skewness. It highlights that many existing methods assume uniform word frequencies, while in reality, word frequencies follow Zipf's law, which is highly non-uniform. The authors propose using PCA whitening that is weighted by empirical word frequencies, leading to significant improvements in task performance. Their theoretical framework categorizes word representations based on exponential families, emphasizing the importance of low-frequency words and providing insights into popular NLP methods."}, 'zh': {'title': '校正词嵌入空间，提升任务性能！', 'desc': '这篇论文探讨了神经模型中的词嵌入空间偏斜问题，并提出了通过校正这一偏斜来提高任务性能的方法。研究表明，现有的许多模型假设词频是均匀分布的，但实际上，词频遵循一种称为齐夫定律的高度非均匀分布。通过对词频进行加权的主成分分析（PCA）白化，显著提升了任务性能，超越了已有的基准。理论上，我们的方法与现有方法可以清晰地分类，强调了低频词的重要性，并解释了流行的自然语言处理方法为何有效。'}}}, {'id': 'https://huggingface.co/papers/2411.00369', 'title': 'GRS-QA -- Graph Reasoning-Structured Question Answering Dataset', 'url': 'https://huggingface.co/papers/2411.00369', 'abstract': 'Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.', 'score': 6, 'issue_id': 409, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'b3e4773e065d1bc1', 'data': {'categories': ['#rag', '#reasoning', '#cv', '#graphs', '#dataset', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Графы рассуждений раскрывают возможности языковых моделей', 'desc': 'Статья представляет новый набор данных GRS-QA для оценки способностей больших языковых моделей (LLM) в многоходовых вопросно-ответных задачах. GRS-QA включает в себя как семантические контексты, так и структуры рассуждений для пар вопрос-ответ. Набор данных использует графы рассуждений, где узлы представляют текстовые контексты, а ребра обозначают логические связи. Эмпирический анализ показывает, что LLM по-разному справляются с вопросами, имеющими различные структуры рассуждений.'}, 'en': {'title': 'Unlocking LLMs: Understanding Reasoning Structures in Multi-Hop QA', 'desc': 'This paper discusses the performance of Large Language Models (LLMs) in multi-hop question-answering (M-QA) tasks, focusing on their reasoning abilities. The authors identify a gap in existing datasets that do not provide detailed reasoning structures, which are crucial for evaluating LLM performance. To fill this gap, they introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which features reasoning graphs that clearly outline the logical pathways for answering questions. Their analysis shows that LLMs exhibit varying performance based on the complexity of the reasoning structures involved, highlighting the importance of understanding both textual and semantic elements in M-QA.'}, 'zh': {'title': '揭示推理结构对LLM表现的影响', 'desc': '大型语言模型（LLMs）在多跳问答（M-QA）中表现出色，主要得益于其先进的推理能力。然而，LLM在多跳问答中的表现受固有推理结构的影响尚不明确，主要是因为缺乏提供细粒度推理结构的问答数据集。为了解决这个问题，我们引入了图推理结构问答数据集（GRS-QA），该数据集为问答对提供了语义上下文和推理结构。与现有的M-QA数据集不同，GRS-QA通过构建推理图来明确捕捉复杂的推理路径，节点表示文本上下文，边表示逻辑流，从而实现对LLM推理能力的细粒度评估。'}}}, {'id': 'https://huggingface.co/papers/2411.00762', 'title': 'Face Anonymization Made Simple', 'url': 'https://huggingface.co/papers/2411.00762', 'abstract': 'Current face anonymization techniques often depend on identity loss calculated by face recognition models, which can be inaccurate and unreliable. Additionally, many methods require supplementary data such as facial landmarks and masks to guide the synthesis process. In contrast, our approach uses diffusion models with only a reconstruction loss, eliminating the need for facial landmarks or masks while still producing images with intricate, fine-grained details. We validated our results on two public benchmarks through both quantitative and qualitative evaluations. Our model achieves state-of-the-art performance in three key areas: identity anonymization, facial attribute preservation, and image quality. Beyond its primary function of anonymization, our model can also perform face swapping tasks by incorporating an additional facial image as input, demonstrating its versatility and potential for diverse applications. Our code and models are available at https://github.com/hanweikung/face_anon_simple .', 'score': 5, 'issue_id': 415, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '0948ed1ff23fd58b', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#open_source', '#architecture'], 'emoji': '🎭', 'ru': {'title': 'Простая и эффективная анонимизация лиц с помощью диффузионных моделей', 'desc': 'Эта статья представляет новый метод анонимизации лиц с использованием диффузионных моделей. В отличие от существующих подходов, метод не требует дополнительных данных вроде лицевых ориентиров или масок. Модель достигает наилучших результатов в анонимизации личности, сохранении атрибутов лица и качестве изображения. Кроме того, она может выполнять задачи по замене лиц, демонстрируя свою универсальность.'}, 'en': {'title': 'Revolutionizing Face Anonymization with Diffusion Models', 'desc': "This paper presents a novel face anonymization technique that utilizes diffusion models, focusing solely on reconstruction loss without the need for additional data like facial landmarks or masks. The method achieves high-quality image generation while effectively anonymizing identities and preserving facial attributes. The authors demonstrate the model's performance through rigorous evaluations on public benchmarks, showcasing its state-of-the-art results in identity anonymization and image quality. Additionally, the model's versatility is highlighted by its capability to perform face swapping tasks, making it suitable for various applications."}, 'zh': {'title': '创新的面部匿名化与交换技术', 'desc': '当前的面部匿名化技术通常依赖于面部识别模型计算的身份损失，这可能不够准确和可靠。我们的方法使用扩散模型，仅依赖重建损失，省去了面部特征点或面具的需求，同时仍能生成细致的图像。我们的模型在身份匿名化、面部属性保留和图像质量三个关键领域达到了最先进的性能。除了匿名化功能外，我们的模型还可以通过添加额外的面部图像作为输入来执行面部交换任务，展示了其多样性和潜在应用。'}}}, {'id': 'https://huggingface.co/papers/2410.21157', 'title': 'M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation', 'url': 'https://huggingface.co/papers/2410.21157', 'abstract': 'Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.', 'score': 5, 'issue_id': 408, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'd6a0779456870cae', 'data': {'categories': ['#benchmark', '#multilingual', '#plp', '#data', '#dataset', '#transfer_learning', '#open_source'], 'emoji': '🖥️', 'ru': {'title': 'Многоязычный бенчмарк для оценки LLM в автодополнении кода', 'desc': 'Статья представляет новый бенчмарк M2RC-EVAL для оценки способностей больших языковых моделей (LLM) в задаче автодополнения кода на уровне репозитория. Бенчмарк охватывает 18 языков программирования и включает детальные аннотации для различных сценариев дополнения. Авторы также создали набор данных M2RC-INSTRUCT для улучшения возможностей существующих LLM в этой задаче. Эксперименты подтверждают эффективность предложенных инструментов.'}, 'en': {'title': 'Empowering Multilingual Code Completion with M2RC-EVAL and M2RC-INSTRUCT', 'desc': 'This paper introduces a new benchmark called M2RC-EVAL for repository-level code completion that supports 18 programming languages, addressing the limitations of existing benchmarks that only cover a few languages. It provides fine-grained annotations based on abstract syntax trees, allowing for a more detailed evaluation of code completion scenarios. Additionally, the authors present the M2RC-INSTRUCT dataset to enhance the performance of code Large Language Models (LLMs) in multilingual contexts. Experimental results show that both M2RC-EVAL and M2RC-INSTRUCT significantly improve the capabilities of existing code LLMs.'}, 'zh': {'title': '多语言代码补全的新基准测试', 'desc': '本论文提出了一种新的多语言代码补全基准测试，称为M2RC-EVAL，涵盖了18种编程语言。现有的基准测试通常只关注少数几种语言，无法全面评估大型语言模型在不同语言中的代码智能能力。此外，M2RC-EVAL提供了细粒度的注释，帮助研究人员更好地理解模型在不同补全场景下的表现。为了进一步提升代码补全能力，我们还构建了一个多语言指令数据集M2RC-INSTRUCT。'}}}, {'id': 'https://huggingface.co/papers/2411.00030', 'title': 'WikiNER-fr-gold: A Gold-Standard NER Corpus', 'url': 'https://huggingface.co/papers/2411.00030', 'abstract': 'We address in this article the the quality of the WikiNER corpus, a multilingual Named Entity Recognition corpus, and provide a consolidated version of it. The annotation of WikiNER was produced in a semi-supervised manner i.e. no manual verification has been carried out a posteriori. Such corpus is called silver-standard. In this paper we propose WikiNER-fr-gold which is a revised version of the French proportion of WikiNER. Our corpus consists of randomly sampled 20% of the original French sub-corpus (26,818 sentences with 700k tokens). We start by summarizing the entity types included in each category in order to define an annotation guideline, and then we proceed to revise the corpus. Finally we present an analysis of errors and inconsistency observed in the WikiNER-fr corpus, and we discuss potential future work directions.', 'score': 4, 'issue_id': 411, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '5f3e739256c5e800', 'data': {'categories': ['#synthetic', '#multilingual', '#data', '#dataset', '#low_resource'], 'emoji': '🏷️', 'ru': {'title': 'Золотой стандарт для французского NER: улучшение WikiNER', 'desc': 'Статья посвящена улучшению качества корпуса WikiNER для распознавания именованных сущностей на нескольких языках. Авторы создали WikiNER-fr-gold - вручную проверенную версию французской части корпуса, состоящую из 20% оригинальных данных. Они разработали руководство по аннотации, пересмотрели корпус и проанализировали ошибки в исходном WikiNER-fr. Исследование направлено на повышение точности обучения моделей NER на французском языке.'}, 'en': {'title': 'Enhancing Named Entity Recognition with WikiNER-fr-gold', 'desc': 'This paper focuses on improving the quality of the WikiNER corpus, which is used for Named Entity Recognition (NER) in multiple languages. The original WikiNER corpus was created using a semi-supervised approach, resulting in a silver-standard dataset without manual verification. The authors introduce WikiNER-fr-gold, a refined version of the French subset of WikiNER, based on a carefully sampled 20% of the original data. They outline the types of entities included, establish annotation guidelines, and analyze errors in the original corpus to suggest future improvements.'}, 'zh': {'title': '提升WikiNER语料库质量的探索', 'desc': '本文讨论了WikiNER语料库的质量，这是一个多语言命名实体识别语料库，并提供了一个整合版本。WikiNER的标注是以半监督的方式生成的，即没有进行后期的人工验证，因此该语料库被称为银标准。我们提出了WikiNER-fr-gold，这是WikiNER法语部分的修订版本，包含了原法语子语料库的20%随机抽样（26,818个句子，700k个标记）。最后，我们分析了WikiNER-fr语料库中观察到的错误和不一致，并讨论了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2410.24024', 'title': 'AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents', 'url': 'https://huggingface.co/papers/2410.24024', 'abstract': 'Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.', 'score': 40, 'issue_id': 422, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '4ba16ad433c7511f', 'data': {'categories': ['#benchmark', '#multimodal', '#training', '#dataset', '#open_source', '#games', '#agents'], 'emoji': '🤖', 'ru': {'title': 'AndroidLab: Революция в обучении Android-агентов', 'desc': 'Статья представляет AndroidLab - систематическую среду для разработки и оценки агентов на базе Android. Эта среда включает в себя операционное окружение с различными модальностями, пространством действий и воспроизводимым эталоном. AndroidLab поддерживает как большие языковые модели (LLM), так и мультимодальные модели (LMM) в одном пространстве действий. С помощью AndroidLab авторы разработали набор данных Android Instruction и обучили шесть моделей с открытым исходным кодом, значительно повысив их эффективность.'}, 'en': {'title': 'Empowering Android Agents with AndroidLab: A New Benchmark Framework', 'desc': 'This paper introduces AndroidLab, a comprehensive framework designed for training and evaluating Android agents. It provides a structured environment that includes various modalities and a defined action space, allowing for systematic testing of both open-source and closed-source models. The framework supports large language models (LLMs) and multimodal models (LMMs), facilitating their development and benchmarking across 138 tasks in nine different applications. By utilizing AndroidLab, the authors significantly improved the success rates of LLMs and LMMs, demonstrating its effectiveness in enhancing the performance of Android agents.'}, 'zh': {'title': 'AndroidLab：提升安卓代理的训练与评估', 'desc': '自主代理在与现实世界互动中变得越来越重要，尤其是安卓代理。现有的安卓代理训练和评估研究缺乏系统性，无法全面比较开源和闭源模型。我们提出了AndroidLab，这是一个系统化的安卓代理框架，支持多种操作环境和动作空间。通过AndroidLab，我们开发了安卓指令数据集，并显著提高了大语言模型和多模态模型的成功率。'}}}, {'id': 'https://huggingface.co/papers/2411.02355', 'title': '"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization', 'url': 'https://huggingface.co/papers/2411.02355', 'abstract': 'Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the "best" format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous "continuous batching" deployment of mid- and large-size models on high-end GPUs. Our results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements.', 'score': 28, 'issue_id': 431, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '8cc3629c4f7e76a1', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#training', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Оптимальное квантование LLM: баланс между точностью и производительностью', 'desc': 'Это исследование посвящено квантованию больших языковых моделей (LLM) для ускорения вывода. Авторы провели комплексный эмпирический анализ различных форматов квантования (FP8, INT8, INT4) на семействе моделей Llama-3.1, оценивая их точность на академических тестах и реальных задачах. Результаты показывают, что квантование FP8 практически безошибочно для всех размеров моделей, а INT8 при правильной настройке дает снижение точности всего на 1-3%. Исследование также включает анализ производительности на различных GPU архитектурах, предоставляя практические рекомендации по развертыванию квантованных LLM.'}, 'en': {'title': 'Optimizing Quantization for Efficient Large Language Model Deployment', 'desc': 'This paper investigates the accuracy and performance trade-offs of different quantization formats for large language models (LLMs), specifically focusing on FP8, INT8, and INT4 quantization methods. The authors conducted extensive evaluations on the Llama-3.1 model family, analyzing over 500,000 instances to determine the impact of quantization on model accuracy and text generation. Key findings indicate that FP8 quantization is lossless, while INT8 can achieve minimal accuracy loss with proper tuning, and INT4 quantization remains competitive. The study also provides practical guidelines for selecting the optimal quantization format based on deployment scenarios and GPU architectures.'}, 'zh': {'title': '量化模型的最佳选择与性能优化', 'desc': '本文研究了大语言模型（LLM）量化对推理加速的影响，特别关注不同量化格式（如FP8、INT8、INT4）在准确性和性能之间的权衡。我们对Llama-3.1模型系列进行了全面的实证研究，评估了量化模型在学术基准和实际任务中的表现。研究发现，FP8量化在所有模型规模上都是无损的，而经过适当调优的INT8量化仅有1-3%的准确性下降。此外，我们还提出了一些量化改进方法，帮助实现了最先进的准确性恢复结果。'}}}, {'id': 'https://huggingface.co/papers/2411.02337', 'title': 'WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning', 'url': 'https://huggingface.co/papers/2411.02337', 'abstract': "Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.", 'score': 25, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'a9af7d20e52c1f39', 'data': {'categories': ['#small_models', '#reasoning', '#rl', '#benchmark', '#optimization', '#training', '#open_source', '#agents', '#architecture'], 'emoji': '🕸️', 'ru': {'title': 'WebRL: Открытые языковые модели превосходят проприетарные в веб-задачах', 'desc': 'Статья представляет WebRL - фреймворк обучения с подкреплением для создания веб-агентов на основе открытых языковых моделей. WebRL решает проблемы нехватки обучающих задач, разреженной обратной связи и смещения распределения политики в онлайн-обучении. Фреймворк включает самоэволюционирующий учебный план, модель вознаграждения на основе результатов и адаптивные стратегии обучения с подкреплением. Применение WebRL значительно улучшило производительность открытых моделей Llama-3.1 и GLM-4 в задачах веб-взаимодействия, превзойдя даже проприетарные модели GPT-4.'}, 'en': {'title': 'Empowering Open LLMs for Superior Web Performance with WebRL', 'desc': 'This paper presents WebRL, a novel framework that enhances open large language models (LLMs) for web-based tasks through reinforcement learning. It tackles challenges such as limited training tasks, sparse feedback, and policy drift by implementing a self-evolving curriculum that generates new tasks from failures, a robust outcome-supervised reward model, and adaptive learning strategies. The framework significantly improves the performance of open models like Llama-3.1 and GLM-4, achieving success rates that surpass proprietary models like GPT-4-Turbo. Overall, WebRL demonstrates a promising approach to making powerful web agents more accessible by leveraging open-source LLMs.'}, 'zh': {'title': 'WebRL：开放LLM的自我进化网络代理训练框架', 'desc': '大型语言模型（LLMs）在网络任务中展现了出色的潜力，但现有的LLM网络代理依赖昂贵的专有API，而开放的LLM缺乏决策能力。本文提出了WebRL，一个自我进化的在线课程强化学习框架，旨在利用开放的LLM训练高性能的网络代理。WebRL解决了构建LLM网络代理的三个关键挑战，包括训练任务稀缺、反馈信号稀疏和在线学习中的策略分布漂移。通过WebRL，我们将开放的Llama-3.1和GLM-4模型转变为高效的网络代理，显著提高了它们的成功率，超越了现有的专有模型。'}}}, {'id': 'https://huggingface.co/papers/2411.02336', 'title': 'MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D', 'url': 'https://huggingface.co/papers/2411.02336', 'abstract': 'Texturing is a crucial step in the 3D asset production workflow, which enhances the visual appeal and diversity of 3D assets. Despite recent advancements in Text-to-Texture (T2T) generation, existing methods often yield subpar results, primarily due to local discontinuities, inconsistencies across multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle these challenges, we propose a novel generation-refinement 3D texturing framework called MVPaint, which can generate high-resolution, seamless textures while emphasizing multi-view consistency. MVPaint mainly consists of three key modules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model, MVPaint first simultaneously generates multi-view images by employing an SMG model, which leads to coarse texturing results with unpainted parts due to missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete 3D texturing, we introduce the S3I method, specifically designed to effectively texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore, MVPaint employs a UVR module to improve the texture quality in the UV space, which first performs a UV-space Super-Resolution, followed by a Spatial-aware Seam-Smoothing algorithm for revising spatial texturing discontinuities caused by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the Objaverse T2T benchmark and the GSO T2T benchmark, based on selected high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset, respectively. Extensive experimental results demonstrate that MVPaint surpasses existing state-of-the-art methods. Notably, MVPaint could generate high-fidelity textures with minimal Janus issues and highly enhanced cross-view consistency.', 'score': 20, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '0f7de0fb0e5cdd86', 'data': {'categories': ['#benchmark', '#cv', '#optimization', '#dataset', '#games', '#3d'], 'emoji': '🎨', 'ru': {'title': 'MVPaint: революция в автоматическом текстурировании 3D-моделей', 'desc': 'В статье представлена новая система MVPaint для генерации высококачественных текстур для 3D-моделей. Система состоит из трех ключевых модулей: синхронизированной многоракурсной генерации, пространственно-ориентированного 3D-инпейнтинга и уточнения UV-развертки. MVPaint решает проблемы локальных разрывов, несогласованности между ракурсами и зависимости от UV-развертки, характерные для существующих методов текстурирования. Авторы также создали два новых бенчмарка для оценки качества генерации текстур по тексту.'}, 'en': {'title': 'MVPaint: Revolutionizing 3D Texturing with Multi-View Consistency', 'desc': 'This paper introduces MVPaint, a new framework for generating high-quality textures for 3D models. It addresses common issues in Text-to-Texture (T2T) generation, such as local discontinuities and inconsistencies across different views. MVPaint consists of three main components: Synchronized Multi-view Generation for initial texture creation, Spatial-aware 3D Inpainting for filling in unpainted areas, and UV Refinement for enhancing texture quality in UV space. The framework outperforms existing methods, providing seamless textures with improved visual consistency across multiple perspectives.'}, 'zh': {'title': 'MVPaint：提升3D纹理生成的一体化解决方案', 'desc': '本文提出了一种新的3D纹理生成与优化框架MVPaint，旨在解决现有文本到纹理生成方法中的局部不连续性和多视图一致性问题。MVPaint包含三个主要模块：同步多视图生成（SMG）、空间感知3D修补（S3I）和UV优化（UVR），能够生成高分辨率、无缝的纹理。通过SMG模块，MVPaint可以同时生成多视图图像，而S3I模块则专注于填补未观察到的区域。最后，UVR模块通过超分辨率和缝合平滑算法来提升UV空间中的纹理质量，实验结果表明MVPaint在纹理生成方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2411.00860', 'title': 'Survey of Cultural Awareness in Language Models: Text and Beyond', 'url': 'https://huggingface.co/papers/2411.00860', 'abstract': 'Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology. In this paper, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking the definitions of culture from anthropology and psychology as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of Human-Computer Interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature.', 'score': 19, 'issue_id': 425, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '8e4a6348db0215e3', 'data': {'categories': ['#science', '#benchmark', '#multilingual', '#multimodal', '#ethics', '#training', '#dataset', '#survey', '#architecture', '#alignment'], 'emoji': '🌍', 'ru': {'title': 'Культурная инклюзивность в эпоху больших языковых моделей', 'desc': 'Эта статья посвящена внедрению культурной осведомленности в крупномасштабные языковые модели (LLM). Авторы рассматривают методологии создания кросс-культурных датасетов и стратегии включения культурных аспектов в задачи обработки естественного языка. Обсуждаются этические последствия культурной адаптации LLM и роль взаимодействия человека с компьютером в этом процессе. Статья также предлагает направления для будущих исследований в области культурной осведомленности искусственного интеллекта.'}, 'en': {'title': 'Enhancing Cultural Sensitivity in Language Models', 'desc': 'This paper explores the importance of cultural sensitivity in large language models (LLMs) used in applications like chatbots. It reviews existing research on how to make LLMs more inclusive by integrating insights from psychology and anthropology. The authors define cultural awareness in LLMs and discuss methods for creating diverse datasets and evaluating cultural inclusivity. Additionally, they highlight ethical considerations and suggest future research directions to enhance cultural alignment in LLMs.'}, 'zh': {'title': '让大型语言模型更具文化敏感性', 'desc': '本论文探讨了在大型语言模型（LLMs）中融入文化敏感性的重要性，以确保用户的包容性。我们首先定义了LLMs中的文化意识，并基于人类学和心理学的定义进行讨论。接着，我们分析了创建跨文化数据集的方法、在下游任务中实现文化包容的策略，以及用于评估LLMs文化意识的方法论。最后，我们讨论了文化对齐的伦理影响、人机交互在推动文化包容中的作用，以及未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2411.02395', 'title': 'Training-free Regional Prompting for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02395', 'abstract': 'Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.', 'score': 19, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '2a0401bfd2cb136b', 'data': {'categories': ['#diffusion', '#architecture', '#open_source', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Точная генерация изображений по сложным текстам: новый метод для DiT', 'desc': 'Статья представляет новый метод регионального промптинга для архитектуры Diffusion Transformer (DiT), применимый к моделям FLUX.1. Этот подход позволяет улучшить генерацию изображений по сложным текстовым описаниям без дополнительного обучения модели. Авторы реализовали метод манипуляции вниманием, что позволяет DiT более точно следовать детальным композиционным промптам. Работа направлена на преодоление ограничений существующих моделей в обработке длинных текстовых описаний с множеством объектов и пространственных отношений.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Regional Prompting in Diffusion Transformers', 'desc': 'This paper discusses the limitations of current diffusion models in generating images from long and complex text prompts. It highlights the challenges these models face when dealing with multiple objects and their attributes in a spatial context. The authors propose a new method called regional prompting for the Diffusion Transformer (DiT) architecture, specifically for the FLUX.1 model, which enhances its ability to generate images based on detailed text descriptions without requiring additional training. This approach utilizes attention manipulation to achieve fine-grained compositional generation.'}, 'zh': {'title': '区域提示提升扩散模型的文本到图像生成能力', 'desc': '扩散模型在文本到图像生成方面表现出色，尤其是在理解语义方面得到了很大提升。尽管已有许多区域提示方法被提出，但现有模型仍无法完美处理长且复杂的文本提示。本文提出了一种基于注意力操作的区域提示方法，专门针对FLUX.1架构进行实现。该方法使得扩散变换器能够在无需训练的情况下，实现细粒度的组合文本到图像生成能力。'}}}, {'id': 'https://huggingface.co/papers/2411.02385', 'title': 'How Far is Video Generation from World Model: A Physical Law Perspective', 'url': 'https://huggingface.co/papers/2411.02385', 'abstract': 'OpenAI\'s Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit "case-based" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora\'s broader success. See our project page at https://phyworld.github.io', 'score': 19, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '771395452deb397f', 'data': {'categories': ['#diffusion', '#synthetic', '#benchmark', '#cv', '#video', '#training', '#open_source', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Генеративные видеомодели не раскрывают физические законы при масштабировании', 'desc': 'Исследование оценивает способность моделей генерации видео изучать фундаментальные физические законы из визуальных данных. Авторы разработали 2D симуляцию для создания видео, управляемых законами классической механики. Эксперименты показали, что модели достигают идеальной генерализации в рамках распределения, но не справляются с экстраполяцией на новые сценарии. Результаты указывают на то, что модели не абстрагируют общие физические правила, а скорее демонстрируют обобщение на основе конкретных примеров.'}, 'en': {'title': 'Unlocking Video Generation: Beyond Scaling to Understand Physics', 'desc': 'This paper investigates the ability of video generation models to learn and predict physical laws from visual data without human input. The authors created a 2D simulation environment to generate videos based on classical mechanics, allowing for extensive testing of model performance. They found that while the models excelled in familiar scenarios, they struggled with new, unseen situations, indicating a reliance on specific examples rather than generalizing physical principles. The study concludes that simply increasing model size is not enough for these systems to grasp fundamental laws of physics, highlighting the need for improved generalization techniques.'}, 'zh': {'title': '视频生成模型与物理法则的探索', 'desc': '本文探讨了视频生成模型在学习物理法则方面的潜力。我们开发了一个二维模拟测试平台，用于生成受经典力学法则支配的视频数据。通过对模型在不同场景下的表现进行评估，我们发现模型在已知分布内表现良好，但在未知分布中则出现失败。研究表明，模型在推广新案例时，优先考虑的因素依次为颜色、大小、速度和形状，而不是抽象出一般的物理规则。'}}}, {'id': 'https://huggingface.co/papers/2411.02265', 'title': 'Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent', 'url': 'https://huggingface.co/papers/2411.02265', 'abstract': "In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.   Codes: https://github.com/Tencent/Hunyuan-Large   Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large", 'score': 16, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '775afc5ff4d7fbdf', 'data': {'categories': ['#long_context', '#reasoning', '#synthetic', '#benchmark', '#optimization', '#math', '#plp', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Hunyuan-Large: Гигантский шаг вперед в области моделей смеси экспертов', 'desc': 'Статья представляет Hunyuan-Large - крупнейшую открытую модель на основе трансформеров с архитектурой смеси экспертов, содержащую 389 миллиардов параметров. Модель демонстрирует превосходную производительность в различных задачах, включая понимание и генерацию языка, логические рассуждения и программирование. Ключевые особенности Hunyuan-Large включают использование масштабных синтетических данных и смешанную стратегию маршрутизации экспертов. Авторы также исследуют законы масштабирования и графики скорости обучения для моделей смеси экспертов.'}, 'en': {'title': 'Unlocking New Frontiers in AI with Hunyuan-Large', 'desc': 'Hunyuan-Large is a groundbreaking Transformer-based mixture of experts model with an impressive 389 billion parameters, designed to process up to 256K tokens. It demonstrates exceptional performance in various tasks such as language understanding, logical reasoning, and coding, surpassing the capabilities of LLama3.1-70B and rivaling the larger LLama3.1-405B. The model employs innovative techniques like large-scale synthetic data generation, a mixed expert routing strategy, and expert-specific learning rates to enhance its efficiency. Furthermore, the paper explores scaling laws and learning rate schedules, offering insights for future advancements in model optimization.'}, 'zh': {'title': 'Hunyuan-Large：超大规模专家混合模型的创新之路', 'desc': '本文介绍了Hunyuan-Large，这是目前最大的开源基于Transformer的专家混合模型，拥有3890亿个参数和520亿个激活参数，能够处理多达256K的token。我们对Hunyuan-Large在语言理解与生成、逻辑推理、数学问题解决、编码、长上下文和聚合任务等多个基准上的优越性能进行了全面评估，结果显示其优于LLama3.1-70B，并且在与更大模型LLama3.1-405B的比较中表现相当。Hunyuan-Large的关键实践包括大规模合成数据、混合专家路由策略、键值缓存压缩技术和专家特定学习率策略。此外，我们还研究了专家混合模型的扩展规律和学习率调度，为未来模型的开发和优化提供了宝贵的见解和指导。'}}}, {'id': 'https://huggingface.co/papers/2411.02319', 'title': 'GenXD: Generating Any 3D and 4D Scenes', 'url': 'https://huggingface.co/papers/2411.02319', 'abstract': "Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, GenXD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, GenXD employs masked latent conditions to support a variety of conditioning views. GenXD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating GenXD's effectiveness and versatility compared to previous methods in 3D and 4D generation.", 'score': 13, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '51444eddaf6bbbe7', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#video', '#data', '#dataset', '#3d', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'GenXD: Универсальный генератор 3D и 4D сцен', 'desc': 'Исследователи представили новый подход к генерации 3D и 4D сцен под названием GenXD. Они создали большой набор данных реальных 4D сцен CamVid-30K, используя специальный конвейер для извлечения информации о движении камеры и объектов из видео. GenXD использует мультиракурсно-временные модули для разделения движений камеры и объектов, а также маскированные латентные условия для гибкого задания входных ракурсов. Метод позволяет генерировать видео с заданной траекторией камеры и согласованные 3D виды, которые можно преобразовать в 3D-представления.'}, 'en': {'title': 'Unlocking 3D and 4D Generation with GenXD', 'desc': 'This paper addresses the challenges of generating 3D and 4D visual content, which are more complex than 2D generation due to limited data and model design issues. The authors introduce a new dataset called CamVid-30K, created by extracting camera poses and object movements from videos, which helps in training models effectively. They present a framework named GenXD that utilizes multiview-temporal modules to differentiate between camera and object movements, enabling the generation of realistic 3D and 4D scenes. Extensive evaluations show that GenXD outperforms existing methods, demonstrating its capability to create coherent videos and 3D representations from diverse inputs.'}, 'zh': {'title': '突破3D与4D生成的瓶颈', 'desc': '本文探讨了3D和4D视觉生成的挑战，尤其是在缺乏大规模4D数据的情况下。我们提出了一种数据整理流程，从视频中获取相机姿态和物体运动强度，并引入了一个大型的4D场景数据集CamVid-30K。基于这些数据，我们开发了GenXD框架，能够生成任意3D或4D场景。通过多视角时间模块，GenXD能够有效地学习相机和物体的运动，并生成与相机轨迹一致的视频和可提升为3D表示的视图。'}}}, {'id': 'https://huggingface.co/papers/2411.00836', 'title': 'DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models', 'url': 'https://huggingface.co/papers/2411.00836', 'abstract': "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.", 'score': 13, 'issue_id': 420, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'e73ae00a5621a2b9', 'data': {'categories': ['#reasoning', '#synthetic', '#benchmark', '#cv', '#graphs', '#optimization', '#math', '#dataset', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'DynaMath: Новый подход к оценке математических способностей ИИ', 'desc': 'Статья представляет DynaMath - динамический визуальный математический бенчмарк для оценки робастности рассуждений Vision-Language Models (VLM) в задачах математики. Исследователи обнаружили, что современные VLM, такие как GPT-4V, часто не способны применять шаги решения к похожим задачам с небольшими изменениями. DynaMath включает 501 исходный вопрос в виде Python-программ, позволяющих генерировать множество вариаций для тестирования обобщающей способности моделей. Результаты оценки 14 современных VLM показали, что их точность в худшем случае значительно ниже средней точности.'}, 'en': {'title': 'Enhancing VLMs: Unveiling Mathematical Reasoning Limitations with DynaMath', 'desc': "This paper explores the limitations of Vision-Language Models (VLMs) in performing mathematical reasoning tasks that involve visual elements. It highlights that while humans can adapt their problem-solving strategies to slight changes in problems, current state-of-the-art VLMs like GPT-4o struggle with this adaptability. To address this issue, the authors introduce DynaMath, a dynamic benchmark that generates a wide variety of mathematical questions to rigorously test VLMs' reasoning capabilities. The findings reveal that VLMs exhibit significantly lower accuracy in worst-case scenarios compared to average cases, underscoring the importance of evaluating their robustness in mathematical reasoning."}, 'zh': {'title': '提升视觉语言模型的数学推理能力', 'desc': '本文探讨了视觉语言模型（VLMs）在数学推理任务中的表现，尤其是在视觉上下文的影响下。研究发现，尽管人类能够灵活应对相似问题的变化，当前的最先进模型如GPT-4o在面对这些变化时却表现不佳，显示出其数学推理能力的局限性。为了解决这一问题，本文提出了DynaMath，一个动态视觉数学基准，旨在深入评估VLMs的推理稳健性。通过对501个高质量种子问题的自动生成，DynaMath能够评估模型在不同输入条件下的泛化能力，结果显示模型在最坏情况下的准确率显著低于平均情况。'}}}, {'id': 'https://huggingface.co/papers/2411.01747', 'title': 'DynaSaur: Large Language Agents Beyond Predefined Actions', 'url': 'https://huggingface.co/papers/2411.01747', 'abstract': 'Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in https://github.com/adobe-research/dynasaur{https://github.com/adobe-research/dynasaur}.', 'score': 12, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '772b11b15cab80a0', 'data': {'categories': ['#reasoning', '#benchmark', '#agi', '#plp', '#open_source', '#agents', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Динамическое создание действий для агентов на базе LLM: шаг к адаптивному ИИ', 'desc': 'Эта статья представляет новую структуру агентов на основе больших языковых моделей (LLM), которая позволяет динамически создавать и комбинировать действия в режиме реального времени. В отличие от существующих систем с фиксированным набором действий, предлагаемый подход генерирует и выполняет программы на языке общего назначения на каждом шаге взаимодействия с окружающей средой. Авторы демонстрируют, что их метод обеспечивает большую гибкость и превосходит предыдущие подходы в экспериментах на бенчмарке GAIA. Особенно эффективно система работает в сценариях, где предопределенные действия отсутствуют или неприменимы.'}, 'en': {'title': 'Empowering LLM Agents with Dynamic Action Generation', 'desc': 'This paper introduces a new framework for large language model (LLM) agents that allows them to dynamically create and execute actions in real-time, rather than relying on a fixed set of predefined actions. This approach enhances the planning and acting capabilities of LLM agents, making them more adaptable to complex and unpredictable environments. By generating programs in a general-purpose programming language, the agents can respond to unforeseen situations and accumulate useful actions for future tasks. The experiments conducted on the GAIA benchmark show that this framework significantly outperforms traditional methods, providing greater flexibility and effectiveness in real-world applications.'}, 'zh': {'title': '动态创建与组合动作的LLM代理框架', 'desc': '现有的大型语言模型（LLM）代理系统通常在每一步从固定的预定义动作集中选择动作。这种方法在封闭的、狭窄的环境中有效，但在现实世界中应用时面临两个主要挑战：一是从固定动作集中选择限制了LLM代理的规划和执行能力，二是需要大量人力来列举和实现所有可能的动作，这在复杂环境中变得不切实际。我们提出了一种LLM代理框架，允许在线动态创建和组合动作，代理通过生成和执行通用编程语言编写的程序与环境互动。我们的实验表明，该框架提供了更大的灵活性，并在GAIA基准测试中表现优于以往方法。'}}}, {'id': 'https://huggingface.co/papers/2411.02397', 'title': 'Adaptive Caching for Faster Video Generation with Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02397', 'abstract': 'Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that "not all videos are created equal": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.', 'score': 12, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'd7fa22d791789900', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Адаптивное кэширование ускоряет генерацию видео без потери качества', 'desc': 'Статья представляет метод AdaCache для ускорения видео-диффузионных трансформеров (DiT) без переобучения. AdaCache адаптивно кэширует вычисления в процессе диффузии, учитывая, что разным видео требуется разное количество шагов шумоподавления. Также предложена схема регуляризации движения (MoReg) для оптимизации распределения вычислений на основе содержания движения в видео. Метод значительно ускоряет генерацию видео (до 4.7 раз на Open-Sora 720p) без потери качества.'}, 'en': {'title': 'Accelerating Video Generation with Adaptive Caching!', 'desc': 'This paper presents a method called Adaptive Caching (AdaCache) to improve the speed of video generation using Diffusion Transformers (DiTs). The authors argue that different videos have varying requirements for denoising steps, allowing for a more efficient computation process. By caching computations and creating a tailored caching schedule for each video, they enhance the balance between quality and speed. Additionally, they introduce a Motion Regularization (MoReg) technique to optimize resource allocation based on the motion content of the video, achieving significant speed improvements without compromising quality.'}, 'zh': {'title': '加速视频生成，提升质量与效率！', 'desc': '本论文提出了一种名为自适应缓存（AdaCache）的方法，用于加速视频生成中的扩散变换器（DiTs）。该方法通过缓存计算过程，针对每个视频生成制定缓存计划，从而优化质量与延迟的平衡。我们还引入了运动正则化（MoReg）方案，根据视频中的运动内容来控制计算分配。整体而言，这些创新显著提高了推理速度，同时保持了生成质量。'}}}, {'id': 'https://huggingface.co/papers/2411.02335', 'title': 'Sparsing Law: Towards Large Language Models with Greater Activation Sparsity', 'url': 'https://huggingface.co/papers/2411.02335', 'abstract': 'Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-p% sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1-sparsity ratio) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable.', 'score': 8, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '3c9152f4d267fc7b', 'data': {'categories': ['#optimization', '#training', '#architecture', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Разреженность активаций в LLM: ключ к эффективности и интерпретируемости', 'desc': 'Статья исследует разреженность активаций в больших языковых моделях (LLM). Авторы предлагают новую метрику PPL-p% для измерения разреженности активаций. Исследование показывает, что различные функции активации демонстрируют противоположные тренды разреженности во время обучения. Обнаружено, что соотношение ширины и глубины модели влияет на разреженность активаций, а масштаб параметров модели оказывает слабое влияние.'}, 'en': {'title': 'Unlocking Efficiency: The Power of Activation Sparsity in LLMs', 'desc': 'This paper investigates activation sparsity in decoder-only Transformer-based large language models (LLMs), focusing on how weakly-contributed activation outputs can be reduced for efficiency. The authors introduce a new metric called PPL-p% sparsity to quantitatively measure activation sparsity across different activation functions. Their experiments reveal that while ReLU and SiLU functions perform similarly, they exhibit opposite trends in training-time sparsity, with ReLU being more efficient. Additionally, the study finds that activation sparsity is largely unaffected by the parameter scale, suggesting that deeper architectures may enhance efficiency without requiring more parameters.'}, 'zh': {'title': '激活稀疏性：提升大型语言模型效率的关键', 'desc': '激活稀疏性指的是在激活输出中存在大量贡献较弱的元素，这些元素可以被消除，从而对大型语言模型（LLMs）的许多重要应用有益。本文对基于解码器的Transformer LLM中的激活稀疏性进行了全面的定量研究，提出了一种新的激活稀疏性度量标准PPL-p%稀疏性，适用于任何激活函数。研究发现，不同的激活函数在性能上相似，但在训练时间的稀疏性趋势上却相反，ReLU激活函数在利用更多训练数据方面更为高效。最后，研究表明，激活稀疏性的极限值与参数规模的变化关系不大，这为提高LLMs的效率和可解释性提供了重要的启示。'}}}, {'id': 'https://huggingface.co/papers/2411.02394', 'title': 'AutoVFX: Physically Realistic Video Editing from Natural Language Instructions', 'url': 'https://huggingface.co/papers/2411.02394', 'abstract': "Modern visual effects (VFX) software has made it possible for skilled artists to create imagery of virtually anything. However, the creation process remains laborious, complex, and largely inaccessible to everyday users. In this work, we present AutoVFX, a framework that automatically creates realistic and dynamic VFX videos from a single video and natural language instructions. By carefully integrating neural scene modeling, LLM-based code generation, and physical simulation, AutoVFX is able to provide physically-grounded, photorealistic editing effects that can be controlled directly using natural language instructions. We conduct extensive experiments to validate AutoVFX's efficacy across a diverse spectrum of videos and instructions. Quantitative and qualitative results suggest that AutoVFX outperforms all competing methods by a large margin in generative quality, instruction alignment, editing versatility, and physical plausibility.", 'score': 7, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '96ed53359fbc24a5', 'data': {'categories': ['#reasoning', '#cv', '#video', '#multimodal', '#games', '#architecture', '#alignment'], 'emoji': '🎬', 'ru': {'title': 'AutoVFX: Создание спецэффектов силой мысли', 'desc': 'AutoVFX - это новая система, которая автоматически создает реалистичные видео с визуальными эффектами на основе одного видео и текстовых инструкций на естественном языке. Она объединяет нейронное моделирование сцен, генерацию кода с помощью больших языковых моделей и физическое моделирование. AutoVFX позволяет создавать фотореалистичные эффекты с физически корректным поведением. Эксперименты показали, что система значительно превосходит аналоги по качеству генерации, соответствию инструкциям, универсальности редактирования и физической достоверности.'}, 'en': {'title': 'Transforming VFX Creation with Natural Language and AI', 'desc': 'AutoVFX is a novel framework designed to simplify the creation of visual effects (VFX) by allowing users to generate realistic videos from a single input video and natural language commands. It combines advanced techniques such as neural scene modeling, large language model (LLM)-based code generation, and physical simulation to produce high-quality, dynamic effects. The framework is evaluated through extensive experiments, demonstrating superior performance in generative quality, alignment with user instructions, versatility in editing, and adherence to physical realism. Overall, AutoVFX makes sophisticated VFX creation accessible to a broader audience, reducing the complexity of traditional methods.'}, 'zh': {'title': '自动化视觉特效，轻松创作真实影像', 'desc': '现代视觉特效软件使得熟练的艺术家能够创造几乎任何图像，但创作过程仍然繁琐且复杂，普通用户难以接触。本文提出了AutoVFX，一个框架可以根据单个视频和自然语言指令自动创建逼真且动态的视觉特效视频。通过精心整合神经场景建模、基于大语言模型的代码生成和物理仿真，AutoVFX能够提供物理基础的、照片级真实感的编辑效果，并可以通过自然语言指令直接控制。大量实验表明，AutoVFX在生成质量、指令对齐、编辑多样性和物理合理性方面显著优于所有竞争方法。'}}}, {'id': 'https://huggingface.co/papers/2411.00785', 'title': 'IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI', 'url': 'https://huggingface.co/papers/2411.00785', 'abstract': 'We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can "migrate" the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.', 'score': 7, 'issue_id': 434, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'a7e2824d0e474c0b', 'data': {'categories': ['#science', '#cv', '#graphs', '#video', '#multimodal', '#training', '#dataset', '#robotics', '#transfer_learning', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Единое пространство действий для людей и роботов', 'desc': 'Исследователи представляют IGOR - систему для создания единого семантически согласованного пространства действий для людей и роботов. IGOR сжимает визуальные изменения между начальным и целевым изображением в латентные действия, что позволяет генерировать метки действий для масштабных видеоданных. Это единое латентное пространство действий позволяет обучать базовые политики и модели мира для разнообразных задач, выполняемых как роботами, так и людьми. IGOR демонстрирует возможность переноса движений объектов между видео, даже между людьми и роботами, а также интеграцию с естественным языком и низкоуровневым управлением роботами.'}, 'en': {'title': 'Unified Action Space for Human-Robot Interaction', 'desc': 'The paper presents Image-GOal Representations (IGOR), a framework designed to create a unified action space that is semantically consistent for both humans and robots. By compressing the visual changes between an initial image and its goal state into latent actions, IGOR facilitates knowledge transfer across diverse datasets of human and robot activities. This approach allows for the generation of latent action labels from large-scale video data, enabling the training of foundational policy and world models for various tasks. Ultimately, IGOR enhances the ability to transfer movement knowledge between humans and robots, aligning actions with natural language for improved robot control.'}, 'zh': {'title': '统一动作空间，连接人类与机器人', 'desc': '本文介绍了图像目标表示（IGOR），旨在学习一个统一且语义一致的动作空间，适用于人类和各种机器人。通过这个统一的潜在动作空间，IGOR能够在大规模机器人和人类活动数据之间进行知识转移。我们通过压缩初始图像与目标状态之间的视觉变化来实现这一点，从而生成潜在动作标签。IGOR为机器人控制和人机交互开辟了新的可能性。'}}}, {'id': 'https://huggingface.co/papers/2411.02327', 'title': 'PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance', 'url': 'https://huggingface.co/papers/2411.02327', 'abstract': "The past year has witnessed the significant advancement of video-based large language models. However, the challenge of developing a unified model for both short and long video understanding remains unresolved. Most existing video LLMs cannot handle hour-long videos, while methods custom for long videos tend to be ineffective for shorter videos and images. In this paper, we identify the key issue as the redundant content in videos. To address this, we propose a novel pooling strategy that simultaneously achieves token compression and instruction-aware visual feature aggregation. Our model is termed Prompt-guided Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three core components: the CLIP-based visual-prompt alignment that extracts visual information relevant to the user's instructions, the prompt-guided pooling that compresses the visual sequence to arbitrary scales using convolution-style pooling, and the clip context extension designed for lengthy prompt common in visual dialogue. Moreover, our codebase also integrates the most advanced video Direct Preference Optimization (DPO) and visual interleave training. Extensive experiments have validated the performance of our model. With superior throughput and only 1024 visual context, PPLLaVA achieves better results on image benchmarks as a video LLM, while achieving state-of-the-art performance across various video benchmarks, excelling in tasks ranging from caption generation to multiple-choice questions, and handling video lengths from seconds to hours. Codes have been available at https://github.com/farewellthree/PPLLaVA.", 'score': 7, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'ed9d356ccf75d780', 'data': {'categories': ['#long_context', '#rlhf', '#benchmark', '#video', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Универсальная обработка видео любой длины с помощью инновационного пулинга', 'desc': 'Статья представляет новую модель PPLLaVA для обработки как коротких, так и длинных видео. Ключевая инновация заключается в использовании стратегии пулинга, которая сжимает токены и агрегирует визуальные признаки с учетом инструкций. Модель включает выравнивание визуальных подсказок на основе CLIP, пулинг с учетом подсказок и расширение контекста клипов. PPLLaVA демонстрирует высокую производительность на различных бенчмарках для изображений и видео разной длительности.'}, 'en': {'title': 'Unified Video Understanding with PPLLaVA', 'desc': 'This paper presents a new model called Prompt-guided Pooling LLaVA (PPLLaVA) that improves video understanding for both short and long videos. The authors identify redundant content in videos as a key challenge and propose a pooling strategy that compresses tokens while aggregating visual features based on user instructions. PPLLaVA includes components for visual-prompt alignment, convolution-style pooling, and context extension for lengthy prompts. The model demonstrates superior performance across various benchmarks, effectively handling video lengths from seconds to hours.'}, 'zh': {'title': '视频理解的新突破：PPLLaVA模型', 'desc': '本论文提出了一种新的视频大语言模型，称为PPLLaVA，旨在解决短视频和长视频理解的统一模型问题。我们发现视频中的冗余内容是主要挑战，因此提出了一种新的池化策略，实现了令牌压缩和指令感知的视觉特征聚合。PPLLaVA包含三个核心组件，分别是基于CLIP的视觉提示对齐、提示引导池化和剪辑上下文扩展。经过广泛实验验证，PPLLaVA在处理从几秒到几小时的视频时，表现出色，超越了现有的视频基准。'}}}, {'id': 'https://huggingface.co/papers/2411.00918', 'title': 'LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models', 'url': 'https://huggingface.co/papers/2411.00918', 'abstract': 'Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.', 'score': 7, 'issue_id': 420, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'a406640433a3de34', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'LibMoE: Демократизация исследований Mixture of Experts в больших языковых моделях', 'desc': 'Статья представляет LibMoE - комплексную модульную систему для исследования, обучения и оценки алгоритмов Mixture of Experts (MoE) в больших языковых моделях (LLM). LibMoE основан на трех принципах: модульный дизайн, эффективное обучение и всесторонняя оценка. Используя LibMoE, авторы провели обширное сравнение пяти современных алгоритмов MoE на трех различных LLM и 11 наборах данных в режиме zero-shot. Результаты показали, что, несмотря на уникальные характеристики, все алгоритмы MoE показывают примерно одинаковые результаты при усреднении по широкому спектру задач.'}, 'en': {'title': 'LibMoE: Streamlining Mixture of Experts for Large Language Models', 'desc': 'This paper introduces LibMoE, a framework designed to facilitate research on Mixture of Experts (MoE) algorithms for large language models (LLMs). It emphasizes a modular design, efficient training processes, and comprehensive evaluation methods to make MoE more accessible to researchers. The authors benchmark five leading MoE algorithms across three LLMs and 11 datasets, revealing that their performance is generally similar across various tasks. LibMoE aims to standardize the training and evaluation of MoE, helping researchers advance the development of future LLMs.'}, 'zh': {'title': 'LibMoE：让混合专家算法更易于研究和应用', 'desc': '混合专家（MoE）在大型语言模型（LLMs）的高效和有效发展中扮演着重要角色。由于资源需求巨大，许多研究者难以研究大规模的MoE算法。本文开发了LibMoE，这是一个全面且模块化的框架，旨在简化MoE算法的研究、训练和评估。通过模块化设计、高效训练和全面评估，LibMoE使得MoE在LLMs中的应用对更多研究者变得可及。'}}}, {'id': 'https://huggingface.co/papers/2411.00743', 'title': 'Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models', 'url': 'https://huggingface.co/papers/2411.00743', 'abstract': 'Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and L_0 sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5\\% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.', 'score': 6, 'issue_id': 421, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'af234e3c99f935ec', 'data': {'categories': ['#optimization', '#interpretability', '#ethics', '#data', '#training', '#dataset', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Специализированные автоэнкодеры раскрывают скрытые концепты в фундаментальных моделях', 'desc': 'Статья представляет новый метод интерпретации фундаментальных моделей - Специализированные разреженные автоэнкодеры (SSAE). SSAE фокусируются на конкретных поддоменах и эффективно выявляют редкие, но важные концепты в данных. Авторы демонстрируют преимущества плотного поиска для выбора данных и использования Tilted Empirical Risk Minimization в качестве целевой функции обучения. Эффективность SSAE показана на стандартных метриках и в практическом исследовании на наборе данных Bias in Bios.'}, 'en': {'title': 'Illuminating Hidden Concepts in Foundation Models with SSAEs', 'desc': 'This paper discusses the challenges of understanding and mitigating risks in foundation models (FMs) through effective interpretability methods. It introduces Specialized Sparse Autoencoders (SSAEs), which are designed to better identify rare but important concepts in data by focusing on specific subdomains. The authors provide a training approach that utilizes dense retrieval for data selection and Tilted Empirical Risk Minimization to enhance concept recall. The results show that SSAEs outperform traditional Sparse Autoencoders in capturing these critical features, as demonstrated in a case study that improved classification accuracy by addressing bias in gender information.'}, 'zh': {'title': '专注子领域的稀疏自编码器：揭示基础模型的潜在特征', 'desc': '本论文探讨了基础模型（FMs）潜在风险的理解与缓解，强调了有效的可解释性方法的重要性。我们提出了一种新的稀疏自编码器（SSAEs），旨在揭示数据中稀有但重要的概念，特别关注特定子领域。通过密集检索和倾斜经验风险最小化等方法，我们展示了SSAEs在捕捉子领域尾部概念方面的优势。案例研究表明，SSAEs在去除虚假性别信息时，分类准确率提高了12.5%。'}}}, {'id': 'https://huggingface.co/papers/2411.01798', 'title': 'SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF', 'url': 'https://huggingface.co/papers/2411.01798', 'abstract': "In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.", 'score': 5, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'e5a60efe079c6136', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#benchmark', '#optimization', '#training', '#open_source', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'SALSA: Гибкое обучение для лучшей адаптации языковых моделей', 'desc': 'Статья представляет новый подход SALSA для обучения больших языковых моделей с подкреплением от обратной связи человека (RLHF). В отличие от традиционного метода PPO, использующего KL-дивергенцию с замороженной исходной моделью, SALSA создает более гибкую опорную модель путем усреднения весов двух независимо обученных моделей. Это позволяет исследовать более широкую область пространства параметров, не жертвуя стабильностью. Эксперименты показывают, что SALSA превосходит PPO по многим метрикам, включая награды, обобщение вне распределения и общую производительность.'}, 'en': {'title': 'SALSA: Enhancing LLM Alignment through Flexible Exploration', 'desc': "This paper introduces SALSA, a new method for improving the alignment of Large Language Models (LLMs) with human preferences using Reinforcement Learning from Human Feedback (RLHF). Traditional RLHF methods use a fixed reference policy, which can restrict the model's ability to explore and find better solutions due to the penalty imposed by Kullback-Leibler (KL) divergence. SALSA addresses this issue by averaging weights from two independently fine-tuned models, creating a more flexible reference that allows for greater exploration of the reward landscape. The results show that SALSA enhances model performance, robustness, and generalization across various benchmarks compared to traditional methods like Proximal Policy Optimization (PPO)."}, 'zh': {'title': 'SALSA：提升大型语言模型对齐与探索的创新方法', 'desc': '在大型语言模型（LLM）的开发中，基于人类反馈的强化学习（RLHF）对于使模型与人类价值观和偏好保持一致至关重要。传统的RLHF依赖于当前策略与冻结初始策略之间的Kullback-Leibler（KL）散度作为参考，这在策略优化算法中作为惩罚项使用。本文提出了一种新方法SALSA（基于模型集合的对齐学习），通过对两个独立的监督微调模型进行权重空间平均，创建一个更灵活的参考模型，从而克服了传统方法的局限性。SALSA通过更好的探索能力，显著提高了模型的鲁棒性和性能，验证了其在多个基准测试中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.01106', 'title': 'LoRA-Contextualizing Adaptation of Large Multimodal Models for Long Document Understanding', 'url': 'https://huggingface.co/papers/2411.01106', 'abstract': 'Large multimodal models (LMMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page, visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to LMMs leads to inefficiencies, especially with lengthy documents. In this work, we present a novel framework named LoRA-Contextualizing Adaptation of Large multimodal models (LoCAL), which broadens the capabilities of any LMM to support long-document understanding. We demonstrate that LMMs can effectively serve as multimodal retrievers, fetching relevant pages to answer user questions based on these pages. LoCAL is implemented with two specific LMM adapters: one for evidence page retrieval and another for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of LoCAL.', 'score': 3, 'issue_id': 433, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 ноября', 'en': 'November 2', 'zh': '11月2日'}, 'hash': '9ac41ff2238a6f8d', 'data': {'categories': ['#science', '#rag', '#benchmark', '#multimodal', '#architecture', '#long_context'], 'emoji': '📄', 'ru': {'title': 'LoCAL: Революция в понимании длинных документов с помощью LMM', 'desc': 'Статья представляет новый фреймворк LoCAL для улучшения понимания длинных документов большими мультимодальными моделями (LMM). LoCAL использует два адаптера LMM: один для поиска релевантных страниц, другой для ответов на вопросы. Этот подход позволяет эффективно обрабатывать сложные многостраничные документы, преодолевая ограничения традиционных методов. Эмпирические результаты показывают превосходную производительность LoCAL на публичных бенчмарках.'}, 'en': {'title': 'Enhancing Long-Document Understanding with LoCAL', 'desc': 'This paper introduces LoCAL, a new framework designed to enhance large multimodal models (LMMs) for understanding long and complex documents. Traditional methods struggle with efficiency when processing multiple pages, but LoCAL allows LMMs to act as multimodal retrievers, selecting relevant pages to answer questions. The framework includes two specialized adapters: one for retrieving evidence pages and another for answering questions based on those pages. Empirical results indicate that LoCAL achieves state-of-the-art performance on public benchmarks, showcasing its effectiveness in long-document comprehension.'}, 'zh': {'title': '提升大型多模态模型的长文档理解能力', 'desc': '大型多模态模型（LMMs）在理解文本丰富的图像方面取得了显著进展，但在处理复杂的多页视觉文档时仍然面临挑战。传统的文档解析方法在检索增强生成中存在性能和效率的限制，而直接将所有页面呈现给LMMs则导致效率低下，尤其是在处理较长文档时。我们提出了一种新框架，称为LoRA-上下文适应的大型多模态模型（LoCAL），它扩展了任何LMM支持长文档理解的能力。实验结果表明，LoCAL在公共基准测试中表现出色，证明了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.01192', 'title': 'Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks', 'url': 'https://huggingface.co/papers/2411.01192', 'abstract': 'We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research.', 'score': 3, 'issue_id': 427, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 ноября', 'en': 'November 2', 'zh': '11月2日'}, 'hash': 'df042a726644eac5', 'data': {'categories': ['#small_models', '#benchmark', '#multilingual', '#dataset', '#open_source', '#low_resource', '#architecture'], 'emoji': '🦢', 'ru': {'title': 'Swan: Прорыв в обработке арабского языка', 'desc': 'Представлены модели семейства Swan для встраивания текстов на арабском языке. Разработаны два варианта: Swan-Small на основе ARBERTv2 и Swan-Large на базе ArMistral. Для оценки создан бенчмарк ArabicMTEB, охватывающий 8 задач и 94 набора данных. Swan-Large превосходит Multilingual-E5-large по большинству арабских задач, демонстрируя диалектную и культурную осведомленность.'}, 'en': {'title': 'Swan: Advancing Arabic Language Embeddings for Diverse Applications', 'desc': 'This paper presents Swan, a new family of embedding models specifically designed for the Arabic language, which includes two versions: Swan-Small and Swan-Large. Swan-Small is based on ARBERTv2, while Swan-Large utilizes the ArMistral model, a large pretrained Arabic language model. The authors introduce ArabicMTEB, a benchmark suite that evaluates the performance of these models across various Arabic text tasks, ensuring they are effective in different dialects and cultural contexts. The results show that Swan-Large outperforms existing models in most Arabic tasks, highlighting its efficiency and effectiveness in Arabic natural language processing.'}, 'zh': {'title': 'Swan：阿拉伯语嵌入模型的创新之路', 'desc': '本文介绍了Swan，一个以阿拉伯语为中心的嵌入模型家族，适用于小规模和大规模的应用场景。Swan包括两个变体：基于ARBERTv2的Swan-Small和基于预训练阿拉伯大型语言模型ArMistral的Swan-Large。我们提出了ArabicMTEB，一个全面的基准套件，用于评估阿拉伯文本嵌入在跨语言、多方言、多领域和多文化方面的表现，涵盖八个不同任务和94个数据集。Swan-Large在大多数阿拉伯任务中表现优异，超越了Multilingual-E5-large，而Swan-Small也始终优于Multilingual-E5 base。'}}}, {'id': 'https://huggingface.co/papers/2411.00359', 'title': 'Constrained Diffusion Implicit Models', 'url': 'https://huggingface.co/papers/2411.00359', 'abstract': 'This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose constrained diffusion implicit models (CDIM) that modify the diffusion updates to enforce a constraint upon the final output. For noiseless inverse problems, CDIM exactly satisfies the constraints; in the noisy case, we generalize CDIM to satisfy an exact constraint on the residual distribution of the noise. Experiments across a variety of tasks and metrics show strong performance of CDIM, with analogous inference acceleration to unconstrained DDIM: 10 to 50 times faster than previous conditional diffusion methods. We demonstrate the versatility of our approach on many problems including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reconstruction.', 'score': 2, 'issue_id': 434, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'e85bdeb4e14858fd', 'data': {'categories': ['#diffusion', '#cv', '#inference', '#optimization', '#3d', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Ускоренное решение обратных задач с помощью ограниченных диффузионных моделей', 'desc': 'Статья описывает эффективный алгоритм для решения зашумленных линейных обратных задач с использованием предобученных диффузионных моделей. Авторы предлагают constrained diffusion implicit models (CDIM), которые модифицируют обновления диффузии для обеспечения ограничений на конечный результат. CDIM точно удовлетворяет ограничениям для задач без шума, а для зашумленных задач обобщается для удовлетворения точного ограничения на распределение остаточного шума. Эксперименты показывают высокую производительность CDIM с ускорением вывода в 10-50 раз по сравнению с предыдущими условными диффузионными методами.'}, 'en': {'title': 'Accelerating Inverse Problem Solutions with Constrained Diffusion Models', 'desc': "This paper introduces a new algorithm called Constrained Diffusion Implicit Models (CDIM) for tackling noisy linear inverse problems. CDIM builds on the existing Denoising Diffusion Implicit Models (DDIM) by incorporating constraints into the diffusion process, ensuring that the final output adheres to specific requirements. In scenarios without noise, CDIM perfectly meets these constraints, while in noisy situations, it adapts to maintain an exact constraint on the noise's residual distribution. The results demonstrate that CDIM not only performs well across various tasks like super-resolution and denoising but also accelerates inference significantly, achieving speeds 10 to 50 times faster than traditional methods."}, 'zh': {'title': '高效解决带噪声线性逆问题的约束扩散模型', 'desc': '本文提出了一种高效的算法，用于解决带噪声的线性逆问题，利用预训练的扩散模型。我们扩展了去噪扩散隐式模型（DDIM）的范式，提出了约束扩散隐式模型（CDIM），通过修改扩散更新来强制最终输出满足约束条件。在无噪声的逆问题中，CDIM能够精确满足约束；而在有噪声的情况下，我们将CDIM推广到满足噪声残差分布的精确约束。实验结果表明，CDIM在多种任务和指标上表现出色，其推理速度比之前的条件扩散方法快10到50倍，展示了我们方法在超分辨率、去噪、修复、去模糊和3D点云重建等问题上的多样性。'}}}, {'id': 'https://huggingface.co/papers/2411.00492', 'title': 'Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models', 'url': 'https://huggingface.co/papers/2411.00492', 'abstract': 'We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.', 'score': 2, 'issue_id': 433, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'd4857ddaa86c9914', 'data': {'categories': ['#reasoning', '#benchmark', '#interpretability', '#architecture', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Коллективный разум экспертов для улучшения ответов ИИ', 'desc': 'В статье представлен метод Multi-expert Prompting, улучшающий генерацию текста большими языковыми моделями (LLM). Этот подход симулирует работу нескольких экспертов, объединяет их ответы и выбирает лучший результат. Процесс реализуется в виде единой цепочки рассуждений с использованием семи специально разработанных подзадач, основанных на методике Nominal Group Technique. Эксперименты показывают, что Multi-expert Prompting значительно превосходит базовые методы по ряду критериев, включая достоверность и информативность ответов.'}, 'en': {'title': 'Harnessing Collective Expertise for Superior Language Model Responses', 'desc': 'Multi-expert Prompting is an advanced technique that enhances the performance of large language models (LLMs) by simulating the input of multiple experts. It aggregates responses from these simulated experts and selects the best one, ensuring that the output is more accurate and informative. This method is structured around seven subtasks inspired by the Nominal Group Technique, which aids in effective decision-making. Evaluations show that this approach significantly improves the truthfulness and usefulness of LLM responses while minimizing negative outputs like toxicity.'}, 'zh': {'title': '多专家提示：提升语言模型生成的全新方法', 'desc': '本文提出了一种新的增强方法，称为多专家提示（Multi-expert Prompting），旨在改善大型语言模型（LLM）的生成效果。该方法通过模拟多个专家来指导LLM执行输入指令，聚合各个专家的响应，并选择最佳的单个和聚合响应。我们设计了七个子任务，基于成熟的决策框架——名义小组技术（Nominal Group Technique），以实现这一过程。评估结果表明，多专家提示在提高响应的真实性、事实性、信息量和实用性方面显著优于专家提示（ExpertPrompting）及其他基线，同时减少了有害性和攻击性。'}}}, {'id': 'https://huggingface.co/papers/2410.22366', 'title': 'Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2410.22366', 'abstract': "Sparse autoencoders (SAEs) have become a core ingredient in the reverse engineering of large-language models (LLMs). For LLMs, they have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigated the possibility of using SAEs to learn interpretable features for a few-step text-to-image diffusion models, such as SDXL Turbo. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net. We find that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find one block that deals mainly with image composition, one that is mainly responsible for adding local details, and one for color, illumination, and style. Therefore, our work is an important first step towards better understanding the internals of generative text-to-image models like SDXL Turbo and showcases the potential of features learned by SAEs for the visual domain.   Code is available at https://github.com/surkovv/sdxl-unbox", 'score': 71, 'issue_id': 366, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'bb45ca9d1b89342a', 'data': {'categories': ['#diffusion', '#cv', '#interpretability', '#open_source', '#3d', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая секреты SDXL Turbo: разреженные автоэнкодеры в действии', 'desc': 'Исследователи применили разреженные автоэнкодеры (SAE) для анализа внутренних представлений модели генерации изображений по тексту SDXL Turbo. Они обнаружили, что SAE способны выделять интерпретируемые признаки из промежуточных слоев модели. Анализ показал специализацию различных блоков модели на композиции изображения, локальных деталях и цвете/освещении. Это исследование открывает путь к лучшему пониманию и контролю генеративных моделей изображений.'}, 'en': {'title': 'Unlocking Interpretability in Text-to-Image Models with Sparse Autoencoders', 'desc': "This paper explores the use of Sparse Autoencoders (SAEs) to analyze and interpret the inner workings of text-to-image diffusion models, specifically SDXL Turbo. By training SAEs on the updates from transformer blocks in the model's denoising U-net, the authors demonstrate that these autoencoders can extract interpretable features that influence the image generation process. The study reveals that different transformer blocks specialize in various aspects of image creation, such as composition, local details, and color. This research marks a significant advancement in understanding generative models and highlights the utility of SAEs in the visual domain."}, 'zh': {'title': '稀疏自编码器助力文本到图像模型的理解', 'desc': '稀疏自编码器（SAEs）在大型语言模型（LLMs）的逆向工程中发挥了重要作用。本文探讨了将SAEs应用于文本到图像模型的可能性，特别是针对SDXL Turbo的几步扩散模型。我们发现，SAEs学习到的特征具有可解释性，并且对生成过程有因果影响，揭示了模型内部块之间的专业化。我们的研究为更好地理解生成文本到图像模型的内部机制迈出了重要的一步。'}}}, {'id': 'https://huggingface.co/papers/2410.23743', 'title': 'What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective', 'url': 'https://huggingface.co/papers/2410.23743', 'abstract': 'What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. Our code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer_Gradient.', 'score': 56, 'issue_id': 364, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '6af756426d4b0064', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training', '#open_source', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Градиенты раскрывают тайны обучения языковых моделей', 'desc': 'Исследование анализирует паттерны обучения различных слоев больших языковых моделей (LLM) через призму градиентов. Авторы изучают влияние быстрого и медленного мышления на градиенты слоев, учитывая популярность обучения LLM на цепочках рассуждений (CoT) и процессных вознаграждениях. Результаты показывают, что медленное мышление с детальным CoT приводит к большей стабильности обучения по сравнению с быстрым мышлением. Исследование также выявляет различия в градиентных паттернах между предобученными и инструктированными LLM, а также между задачами рассуждения и обучения знаниям.'}, 'en': {'title': 'Unlocking Stability: The Power of Slow Thinking in LLM Training', 'desc': 'This paper explores how different training patterns in large language models (LLMs) affect their learning stability and efficiency. It focuses on the impact of fast versus slow thinking on layer-wise gradients during training, particularly when using reasoning techniques like chain-of-thoughts (CoT). The findings reveal that fast thinking generates larger gradients and more variability across layers, while slow thinking promotes stability and better distinguishes correct reasoning paths. Additionally, the study highlights that pre-trained LLMs are more resilient to the instability caused by fast thinking compared to instruction-tuned models.'}, 'zh': {'title': '快速与慢速思维对大语言模型训练的影响', 'desc': '本研究探讨了大语言模型（LLMs）在后训练阶段的不同层次的训练模式，特别关注快速思维与慢速思维对梯度的影响。研究发现，快速思维在没有链式思维（CoT）的情况下，会导致更大的梯度和层间梯度差异，而慢速思维则带来了更好的学习稳定性。预训练的LLMs对快速思维的不稳定性影响较小，而指令调优的LLMs则更为敏感。此外，慢速思维的梯度模式能够有效区分正确与无关的推理路径，增强了对LLM训练的基本理解。'}}}, {'id': 'https://huggingface.co/papers/2410.22476', 'title': 'A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents', 'url': 'https://huggingface.co/papers/2410.22476', 'abstract': 'In task-oriented dialogue systems, intent detection is crucial for interpreting user queries and providing appropriate responses. Existing research primarily addresses simple queries with a single intent, lacking effective systems for handling complex queries with multiple intents and extracting different intent spans. Additionally, there is a notable absence of multilingual, multi-intent datasets. This study addresses three critical tasks: extracting multiple intent spans from queries, detecting multiple intents, and developing a multi-lingual multi-label intent dataset. We introduce a novel multi-label multi-class intent detection dataset (MLMCID-dataset) curated from existing benchmark datasets. We also propose a pointer network-based architecture (MLMCID) to extract intent spans and detect multiple intents with coarse and fine-grained labels in the form of sextuplets. Comprehensive analysis demonstrates the superiority of our pointer network-based system over baseline approaches in terms of accuracy and F1-score across various datasets.', 'score': 24, 'issue_id': 370, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '22f1775d93baf7e1', 'data': {'categories': ['#synthetic', '#benchmark', '#multilingual', '#graphs', '#optimization', '#dataset', '#transfer_learning', '#games', '#low_resource', '#machine_translation', '#architecture'], 'emoji': '🎯', 'ru': {'title': 'Точное определение множественных намерений в диалоговых системах', 'desc': 'Статья представляет новый подход к обнаружению намерений в диалоговых системах. Авторы предлагают архитектуру на основе указательной сети для извлечения нескольких намерений из запросов пользователей. Также создан новый многоязычный датасет для задачи многометочной классификации намерений. Эксперименты показывают превосходство предложенного метода над базовыми подходами по точности и F1-мере.'}, 'en': {'title': 'Enhancing Intent Detection for Complex Queries in Multilingual Dialogue Systems', 'desc': 'This paper focuses on improving task-oriented dialogue systems by enhancing intent detection, especially for complex queries that involve multiple intents. The authors identify a gap in existing research, which often only addresses simple queries, and they highlight the need for multilingual datasets that can handle multiple intents. To tackle this, they introduce the MLMCID-dataset, a new multi-label multi-class intent detection dataset, and a pointer network-based architecture designed to extract intent spans and detect multiple intents effectively. Their experiments show that this new approach outperforms traditional methods, achieving better accuracy and F1-scores across different datasets.'}, 'zh': {'title': '多意图检测的新突破', 'desc': '在任务导向对话系统中，意图检测对于理解用户查询和提供合适的响应至关重要。现有研究主要集中于处理单一意图的简单查询，缺乏有效的系统来处理复杂的多意图查询和提取不同的意图范围。此外，缺乏多语言和多意图的数据集。本研究提出了一个新的多标签多类意图检测数据集（MLMCID-dataset），并提出了一种基于指针网络的架构（MLMCID），能够提取意图范围并检测多种意图，分析结果显示该系统在准确性和F1分数上优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2410.24198', 'title': 'SelfCodeAlign: Self-Alignment for Code Generation', 'url': 'https://huggingface.co/papers/2410.24198', 'abstract': "Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component's effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance.", 'score': 19, 'issue_id': 372, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '76a150925181ec52', 'data': {'categories': ['#small_models', '#synthetic', '#benchmark', '#plp', '#data', '#training', '#dataset', '#open_source', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'SelfCodeAlign: самообучение ИИ-программиста без человеческих аннотаций', 'desc': 'SelfCodeAlign - это новый метод самонастройки языковых моделей для программирования без использования обширных человеческих аннотаций. Он генерирует разнообразные задачи программирования, создает и проверяет решения, а затем использует успешные примеры для дообучения модели. Эксперименты показывают, что SelfCodeAlign значительно улучшает производительность моделей разного размера в задачах программирования. Метод превосходит предыдущие подходы к инструктивной настройке без участия человека и дистилляции.'}, 'en': {'title': 'SelfCodeAlign: Revolutionizing Code LLMs with Minimal Human Input', 'desc': "This paper introduces SelfCodeAlign, a novel method for improving large language models (LLMs) specifically for coding tasks without needing extensive human input. The approach involves generating diverse coding tasks from high-quality code snippets and validating the model's responses in a controlled environment. By fine-tuning the model on a dataset of 74,000 instruction-response pairs, SelfCodeAlign significantly enhances the model's ability to follow instructions, achieving superior performance compared to larger models. The results demonstrate that this method is effective across various model sizes and establishes a new standard for self-aligned code LLMs."}, 'zh': {'title': '自我对齐，提升代码模型性能！', 'desc': '本论文提出了一种名为SelfCodeAlign的自我对齐代码大语言模型（LLM）的方法，旨在提高模型遵循人类指令的能力。该方法无需大量人工标注或蒸馏，利用相同的基础模型在数据生成过程中进行推理。SelfCodeAlign通过从高质量的种子代码片段中提取多样化的编码概念，生成新的任务，并在沙箱环境中验证每个任务的多个响应。实验结果表明，使用SelfCodeAlign生成的数据集进行微调后，模型在多个基准测试中表现优于之前的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2410.23918', 'title': 'BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments', 'url': 'https://huggingface.co/papers/2410.23918', 'abstract': 'Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from capability to availability, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce BitStack, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code is available at https://github.com/xinghaow99/BitStack.', 'score': 17, 'issue_id': 371, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '7699f83913665aca', 'data': {'categories': ['#inference', '#optimization', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'BitStack: эффективное сжатие весов для гибкого развертывания языковых моделей', 'desc': 'В статье представлен новый метод сжатия весов моделей машинного обучения под названием BitStack. Этот подход позволяет динамически регулировать размер модели с минимальной передачей данных между оперативной памятью и устройствами хранения. BitStack использует декомпозицию весов, учитывая значимость каждого параметра, что приводит к созданию резидуальных блоков размером примерно 1 бит на параметр в каждой итерации декомпозиции. Эксперименты показывают, что BitStack соответствует или превосходит сильные базовые линии квантования, особенно при экстремальных коэффициентах сжатия.'}, 'en': {'title': 'Dynamic Memory Management for Large Language Models with BitStack', 'desc': 'This paper presents BitStack, a new method for compressing large language models (LLMs) without the need for prior training. It addresses the challenge of deploying LLMs on devices with limited memory by allowing dynamic adjustments to model size based on available memory. BitStack uses weight decomposition to create small, efficient blocks of model parameters, which can be easily transmitted and loaded as needed. The results show that BitStack not only provides fine control over memory usage but also performs as well or better than traditional quantization methods, especially under high compression scenarios.'}, 'zh': {'title': 'BitStack：高效的权重压缩方法', 'desc': '大型语言模型（LLMs）在许多应用中取得了革命性进展，但在本地设备上的部署仍面临内存限制的挑战。本文提出了一种名为BitStack的新方法，它是一种无训练的权重压缩技术，能够在内存使用和模型性能之间实现灵活的权衡。通过权重分解，BitStack可以动态调整模型大小，减少运行内存与存储设备之间的传输。实验表明，尽管提供了细粒度的大小控制，BitStack在极端压缩比下的性能仍然与强量化基线相当或更优。'}}}, {'id': 'https://huggingface.co/papers/2410.23933', 'title': 'Language Models can Self-Lengthen to Generate Long Texts', 'url': 'https://huggingface.co/papers/2410.23933', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to process long contexts, yet a notable gap remains in generating long, aligned outputs. This limitation stems from a training gap where pre-training lacks effective instructions for long-text generation, and post-training data primarily consists of short query-response pairs. Current approaches, such as instruction backtranslation and behavior imitation, face challenges including data quality, copyright issues, and constraints on proprietary model usage. In this paper, we introduce an innovative iterative training framework called Self-Lengthen that leverages only the intrinsic knowledge and skills of LLMs without the need for auxiliary data or proprietary models. The framework consists of two roles: the Generator and the Extender. The Generator produces the initial response, which is then split and expanded by the Extender. This process results in a new, longer response, which is used to train both the Generator and the Extender iteratively. Through this process, the models are progressively trained to handle increasingly longer responses. Experiments on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long-text generation, when applied to top open-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at https://github.com/QwenLM/Self-Lengthen.', 'score': 15, 'issue_id': 363, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '2ba3bbe4b8a9836d', 'data': {'categories': ['#small_models', '#benchmark', '#optimization', '#training', '#open_source', '#architecture', '#long_context'], 'emoji': '📏', 'ru': {'title': 'Самоудлинение: революция в генерации длинных текстов для LLM', 'desc': 'Статья представляет новый метод обучения больших языковых моделей (LLM) для генерации длинных текстов, называемый Self-Lengthen. Этот подход использует итеративное обучение с двумя ролями: Генератор и Расширитель, которые работают вместе для создания и улучшения длинных ответов. Метод не требует дополнительных данных или проприетарных моделей, опираясь только на внутренние знания и навыки LLM. Эксперименты показывают, что Self-Lengthen превосходит существующие методы в генерации длинных текстов при применении к ведущим открытым LLM.'}, 'en': {'title': 'Empowering LLMs to Generate Longer, Coherent Texts with Self-Lengthen', 'desc': "This paper addresses the challenge of generating long, coherent outputs from Large Language Models (LLMs), which often struggle due to a lack of effective training for long-text generation. The authors propose a novel iterative training framework called Self-Lengthen, which utilizes the models' existing capabilities without relying on external data or proprietary models. The framework involves two components: a Generator that creates an initial response and an Extender that expands this response into a longer format. Through iterative training, the models improve their ability to generate longer, aligned outputs, demonstrating superior performance compared to existing methods in experiments and human evaluations."}, 'zh': {'title': '自我延长：提升长文本生成的创新框架', 'desc': '最近，大型语言模型（LLMs）的进步显著提升了处理长文本的能力，但在生成长且一致的输出方面仍存在不足。这一限制源于训练过程中的缺陷，预训练缺乏有效的长文本生成指令，而后期训练的数据主要是短问答对。本文提出了一种创新的迭代训练框架Self-Lengthen，利用LLMs的内在知识和技能，无需辅助数据或专有模型。该框架包括生成器和扩展器两个角色，通过生成初始响应并进行扩展，逐步训练模型以处理更长的响应。'}}}, {'id': 'https://huggingface.co/papers/2410.24175', 'title': 'Constraint Back-translation Improves Complex Instruction Following of Large Language Models', 'url': 'https://huggingface.co/papers/2410.24175', 'abstract': "Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.", 'score': 15, 'issue_id': 363, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '6550f79d46b1945c', 'data': {'categories': ['#synthetic', '#benchmark', '#optimization', '#data', '#training', '#dataset', '#open_source'], 'emoji': '🦀', 'ru': {'title': 'Улучшение следования сложным инструкциям в LLM через обратный перевод ограничений', 'desc': "Эта статья представляет новый метод улучшения способности больших языковых моделей (LLM) следовать сложным инструкциям. Авторы предлагают технику 'обратного перевода ограничений', которая использует существующие наборы данных для генерации высококачественных пар инструкция-ответ со сложными ограничениями. Они создали набор данных CRAB, используя эту технику, и показали, что дообучение на нем улучшает способность LLM следовать сложным инструкциям. Исследователи также обнаружили, что обратный перевод ограничений может служить полезной вспомогательной целью обучения."}, 'en': {'title': 'Enhancing LLMs with Constraint Back-Translation for Complex Instructions', 'desc': 'This paper addresses the challenges that large language models (LLMs) face when following complex instructions. It critiques traditional instruction-tuning methods that rely on generating complex instruction-response pairs, which often leads to poor performance due to the limitations of LLMs. The authors introduce a new technique called constraint back-translation, which enhances existing high-quality instruction-response pairs by adding implicit complex constraints. Their experiments show that using this method to create a dataset, named CRAB, significantly improves the ability of various LLMs to follow complex instructions, demonstrating its effectiveness as a training strategy.'}, 'zh': {'title': '提升复杂指令跟随能力的新方法', 'desc': '大型语言模型（LLMs）在处理复杂指令时表现不佳，尤其是在格式和长度等约束方面。以往的研究通过对复杂指令-响应对进行后训练来改进模型，但效果有限。本文提出了一种新颖的数据生成技术，称为约束反向翻译，利用现有数据集中的高质量指令-响应对，添加已满足的复杂约束，从而提高数据质量。实验表明，在新生成的数据集CRAB上进行后训练，可以显著提升多个基础LLM在复杂指令跟随能力上的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.24213', 'title': 'Learning Video Representations without Natural Videos', 'url': 'https://huggingface.co/papers/2410.24213', 'abstract': 'In this paper, we show that useful video representations can be learned from synthetic videos and natural images, without incorporating natural videos in the training. We propose a progression of video datasets synthesized by simple generative processes, that model a growing set of natural video properties (e.g. motion, acceleration, and shape transformations). The downstream performance of video models pre-trained on these generated datasets gradually increases with the dataset progression. A VideoMAE model pre-trained on our synthetic videos closes 97.2% of the performance gap on UCF101 action classification between training from scratch and self-supervised pre-training from natural videos, and outperforms the pre-trained model on HMDB51. Introducing crops of static images to the pre-training stage results in similar performance to UCF101 pre-training and outperforms the UCF101 pre-trained model on 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the low-level properties of the datasets, we identify correlations between frame diversity, frame similarity to natural data, and downstream performance. Our approach provides a more controllable and transparent alternative to video data curation processes for pre-training.', 'score': 13, 'issue_id': 362, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'a01960d8f855aede', 'data': {'categories': ['#synthetic', '#cv', '#video', '#training', '#dataset', '#transfer_learning'], 'emoji': '🎞️', 'ru': {'title': 'Синтетические видео для эффективного предобучения видеомоделей', 'desc': 'Исследователи показали, что полезные представления видео можно обучить на синтетических видео и естественных изображениях, без использования реальных видео. Они предложили последовательность синтетических видеонаборов данных, моделирующих различные свойства естественных видео. Модель VideoMAE, предобученная на этих синтетических данных, показала результаты, близкие к предобучению на реальных видео для задачи классификации действий. Анализ свойств наборов данных выявил корреляции между разнообразием кадров, их схожестью с естественными данными и эффективностью на целевых задачах.'}, 'en': {'title': 'Synthetic Videos: A New Path to Effective Video Representation Learning', 'desc': 'This paper demonstrates that effective video representations can be learned using synthetic videos and natural images, without needing natural videos for training. The authors introduce a series of progressively complex synthetic video datasets that capture essential characteristics of natural videos, such as motion and shape changes. They show that a VideoMAE model pre-trained on these synthetic datasets significantly narrows the performance gap compared to models trained on natural videos. Additionally, incorporating static image crops during pre-training enhances performance on various action classification tasks, suggesting a strong link between dataset properties and model effectiveness.'}, 'zh': {'title': '合成视频助力视频表示学习的突破', 'desc': '本文展示了如何从合成视频和自然图像中学习有用的视频表示，而无需在训练中使用自然视频。我们提出了一系列通过简单生成过程合成的视频数据集，这些数据集模拟了越来越多的自然视频特性，如运动、加速度和形状变换。通过在这些合成数据集上预训练的VideoMAE模型，在UCF101动作分类任务中，性能差距缩小了97.2%，并在HMDB51上表现优于预训练模型。我们的研究表明，数据集的帧多样性和与自然数据的相似性与下游性能之间存在相关性，为视频数据的预训练提供了更可控和透明的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2410.20650', 'title': 'NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks', 'url': 'https://huggingface.co/papers/2410.20650', 'abstract': 'The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available.', 'score': 12, 'issue_id': 380, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '2878f8bac8da005d', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#open_source'], 'emoji': '🗜️', 'ru': {'title': 'NeuZip: эффективное сжатие нейросетей без потери качества', 'desc': 'NeuZip - это новая схема сжатия весов нейронных сетей, основанная на энтропии чисел с плавающей запятой. Метод позволяет значительно уменьшить объем памяти, необходимой для обучения и вывода крупных моделей, без потери производительности. Например, NeuZip сокращает использование памяти при обучении модели Llama-3 8B с 31 ГБ до менее чем 16 ГБ. При выводе метод может уменьшить использование памяти более чем вдвое, сохраняя почти безупречную производительность.'}, 'en': {'title': 'NeuZip: Compressing Neural Networks Without Compromise', 'desc': 'This paper presents NeuZip, a novel weight compression technique designed for neural networks. NeuZip leverages the entropy of floating-point numbers to compress model weights, allowing for more efficient use of on-device memory during both training and inference. The method significantly reduces the memory requirements of large models, such as the Llama-3 8B, without compromising their performance. By implementing NeuZip, users can achieve a memory footprint reduction of over 50% while maintaining the integrity of the training dynamics and inference results.'}, 'zh': {'title': 'NeuZip：高效内存压缩，性能不打折！', 'desc': '本论文介绍了一种新的权重压缩方案，称为NeuZip，旨在提高神经网络的内存效率。通过利用浮点数的熵，NeuZip能够在不牺牲性能的情况下，显著减少模型的内存占用。具体来说，我们将Llama-3 8B模型的训练内存需求从31GB降低到16GB以下，同时保持训练动态不变。在推理阶段，我们的方法可以将内存使用减少一半以上，同时保持接近无损的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.22394', 'title': "AAAR-1.0: Assessing AI's Potential to Assist Research", 'url': 'https://huggingface.co/papers/2410.22394', 'abstract': 'Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new versions.', 'score': 12, 'issue_id': 379, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'cbb8d9bd8efa2823', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#plp', '#dataset', '#open_source', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'AAAR-1.0: Новый рубеж в оценке ИИ для научных исследований', 'desc': 'Представлен новый бенчмарк AAAR-1.0 для оценки производительности больших языковых моделей (LLM) в исследовательских задачах. Он включает четыре задания: вывод уравнений, разработку экспериментов, выявление слабых мест в статьях и анализ рецензий. AAAR-1.0 отличается от существующих бенчмарков ориентацией на исследовательскую деятельность и необходимостью глубоких экспертных знаний. Оценка различных LLM на этом бенчмарке выявила их потенциал и ограничения в выполнении сложных исследовательских задач.'}, 'en': {'title': 'Empowering Researchers with AI: Introducing AAAR-1.0', 'desc': 'This paper presents AAAR-1.0, a new benchmark dataset aimed at evaluating the performance of large language models (LLMs) in research-related tasks. The dataset focuses on four key tasks: assessing equation correctness, designing experiments, identifying weaknesses in papers, and critiquing reviews. Unlike previous benchmarks, AAAR-1.0 is specifically tailored for research activities that require specialized knowledge. The study highlights both the capabilities and limitations of current LLMs in handling complex research tasks, with plans for future iterations of the benchmark.'}, 'zh': {'title': 'AAAR-1.0：提升研究效率的语言模型基准', 'desc': '本研究介绍了AAAR-1.0，这是一个用于评估大型语言模型（LLMs）在研究任务中的表现的基准数据集。该数据集专注于三个关键任务：方程推理、实验设计和论文弱点识别，旨在帮助研究人员更好地利用LLMs。与以往的基准不同，AAAR-1.0强调研究导向，要求深厚的领域专业知识。通过对开源和专有LLMs的评估，我们揭示了它们在复杂研究任务中的潜力和局限性。'}}}, {'id': 'https://huggingface.co/papers/2410.24032', 'title': 'Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks', 'url': 'https://huggingface.co/papers/2410.24032', 'abstract': "The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface. CARE's interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In a within-subject user study with 22 participants, CARE was consistently preferred over a baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CARE's potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration.", 'score': 8, 'issue_id': 368, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'b4fbb07af8c8665c', 'data': {'categories': ['#reasoning', '#multimodal', '#interpretability', '#agents', '#architecture', '#alignment'], 'emoji': '🤝', 'ru': {'title': 'CARE: Персонализированный ИИ-помощник для исследовательских задач', 'desc': 'Статья представляет систему CARE (Collaborative Assistant for Personalized Exploration), которая улучшает персонализацию в исследовательских задачах, сочетая мультиагентную архитектуру на основе больших языковых моделей (LLM) со структурированным пользовательским интерфейсом. CARE состоит из панелей чата, решений и потребностей, что позволяет итеративно уточнять запросы и динамически генерировать решения. Система использует коллаборативный подход для выявления явных и неявных потребностей пользователя, предоставляя индивидуальные, действенные решения. Пользовательское исследование показало, что CARE предпочтительнее базового чат-бота на основе LLM, снижая когнитивную нагрузку и стимулируя креативность.'}, 'en': {'title': 'Transforming LLMs into Proactive Personal Assistants', 'desc': 'This paper presents the Collaborative Assistant for Personalized Exploration (CARE), a system that improves how users interact with large language models (LLMs) during exploratory tasks. CARE uses a multi-agent framework that works together to understand both the explicit and implicit needs of users, allowing for more personalized assistance. The system features a structured user interface with different panels that help users refine their queries and generate solutions dynamically. A user study showed that participants preferred CARE over traditional LLM chatbots, noting its effectiveness in reducing cognitive load and providing tailored support.'}, 'zh': {'title': '个性化探索的协作助手CARE', 'desc': '这篇论文介绍了一种名为CARE的系统，旨在提高大型语言模型（LLM）在探索性任务中的个性化支持。CARE结合了多代理LLM框架和结构化用户界面，帮助用户在模糊查询时更好地获取信息。系统的界面包括聊天面板、解决方案面板和需求面板，支持用户迭代优化查询和动态生成解决方案。研究表明，CARE在用户体验上优于传统的LLM聊天机器人，能够减轻认知负担，激发创造力，并提供更具针对性的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2410.24211', 'title': 'DELTA: Dense Efficient Long-range 3D Tracking for any video', 'url': 'https://huggingface.co/papers/2410.24211', 'abstract': 'Tracking dense 3D motion from monocular videos remains challenging, particularly when aiming for pixel-level precision over long sequences. We introduce \\Approach, a novel method that efficiently tracks every pixel in 3D space, enabling accurate motion estimation across entire videos. Our approach leverages a joint global-local attention mechanism for reduced-resolution tracking, followed by a transformer-based upsampler to achieve high-resolution predictions. Unlike existing methods, which are limited by computational inefficiency or sparse tracking, \\Approach delivers dense 3D tracking at scale, running over 8x faster than previous methods while achieving state-of-the-art accuracy. Furthermore, we explore the impact of depth representation on tracking performance and identify log-depth as the optimal choice. Extensive experiments demonstrate the superiority of \\Approach on multiple benchmarks, achieving new state-of-the-art results in both 2D and 3D dense tracking tasks. Our method provides a robust solution for applications requiring fine-grained, long-term motion tracking in 3D space.', 'score': 8, 'issue_id': 368, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'e9fe68be8b772de2', 'data': {'categories': ['#long_context', '#benchmark', '#cv', '#video', '#optimization', '#3d', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Революция в плотном 3D-трекинге: быстро, точно, масштабируемо', 'desc': 'Статья представляет новый метод для точного отслеживания движения каждого пикселя в 3D-пространстве на протяжении длинных видеопоследовательностей. Авторы используют механизм совместного глобально-локального внимания для отслеживания на пониженном разрешении, а затем применяют трансформер для повышения разрешения предсказаний. Метод работает в 8 раз быстрее предыдущих подходов, достигая при этом наилучшей точности. Эксперименты показывают превосходство предложенного метода на нескольких эталонных наборах данных для задач плотного отслеживания в 2D и 3D.'}, 'en': {'title': 'Efficient Dense 3D Motion Tracking with State-of-the-Art Precision', 'desc': 'This paper presents a new method called \\Approach for tracking dense 3D motion from monocular videos with high precision. It utilizes a joint global-local attention mechanism to perform efficient tracking at reduced resolution, followed by a transformer-based upsampler to enhance the predictions to high resolution. The method significantly improves computational efficiency, running over 8 times faster than previous techniques while achieving state-of-the-art accuracy in both 2D and 3D tracking tasks. Additionally, the study identifies log-depth as the best depth representation for enhancing tracking performance.'}, 'zh': {'title': '高效稠密三维运动追踪的新方法', 'desc': '本论文介绍了一种新方法\textit{Approach}，用于从单目视频中高效追踪每个像素的三维运动。该方法结合了全局和局部注意力机制，先进行低分辨率追踪，然后使用基于变换器的上采样器实现高分辨率预测。与现有方法相比，\textit{Approach}在计算效率和稠密追踪方面表现出色，速度比之前的方法快8倍，同时保持了最先进的准确性。通过广泛的实验，我们证明了\textit{Approach}在多个基准测试中的优越性，提供了在三维空间中进行精细、长期运动追踪的强大解决方案。'}}}, {'id': 'https://huggingface.co/papers/2410.21969', 'title': 'BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays', 'url': 'https://huggingface.co/papers/2410.21969', 'abstract': 'Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.', 'score': 7, 'issue_id': 362, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'f3fe2798fa53bad0', 'data': {'categories': ['#benchmark', '#cv', '#graphs', '#healthcare', '#transfer_learning', '#dataset', '#open_source', '#survey'], 'emoji': '〰️', 'ru': {'title': 'Единый стандарт для оценки моделей анализа медицинских данных', 'desc': 'Статья представляет BenchX - унифицированную систему оценки методов предобучения моделей для анализа медицинских изображений и текстов (MedVLP). BenchX включает обширные наборы данных рентгеновских снимков грудной клетки, стандартизированные протоколы предобработки и разделения данных, а также унифицированные методы дообучения моделей. С помощью BenchX авторы провели сравнительный анализ девяти современных методов MedVLP, выявив неожиданные результаты и необходимость пересмотра некоторых выводов предыдущих исследований.'}, 'en': {'title': 'Standardizing Medical Vision-Language Pretraining Evaluation with BenchX', 'desc': 'The paper introduces MedVLP, a method for learning visual representations from medical images and reports, which can be applied to various medical tasks. It highlights the challenges in evaluating MedVLP methods due to inconsistencies in datasets and implementations. To address this, the authors propose BenchX, a benchmark framework that standardizes evaluation across multiple MedVLP methods using public chest X-ray datasets. BenchX includes comprehensive datasets, standardized preprocessing, and unified finetuning protocols, allowing for systematic comparisons and improved performance assessments of MedVLP techniques.'}, 'zh': {'title': '统一基准，提升医疗视觉语言预训练的比较与评估', 'desc': '本文提出了一个名为BenchX的统一基准框架，旨在评估医疗视觉语言预训练（MedVLP）方法的性能。BenchX包含三个主要组成部分：涵盖九个数据集和四个医疗任务的综合数据集、标准化的数据预处理和训练测试划分的基准套件，以及适应不同MedVLP方法的一致微调协议。通过使用BenchX，我们为九种最先进的MedVLP方法建立了基线，并发现一些早期的MedVLP方法的性能可以提升，超过更近期的方法。这一发现促使我们重新审视MedVLP领域的研究进展和结论。'}}}, {'id': 'https://huggingface.co/papers/2410.21666', 'title': 'Minimum Entropy Coupling with Bottleneck', 'url': 'https://huggingface.co/papers/2410.21666', 'abstract': 'This paper investigates a novel lossy compression framework operating under logarithmic loss, designed to handle situations where the reconstruction distribution diverges from the source distribution. This framework is especially relevant for applications that require joint compression and retrieval, and in scenarios involving distributional shifts due to processing. We show that the proposed formulation extends the classical minimum entropy coupling framework by integrating a bottleneck, allowing for a controlled degree of stochasticity in the coupling. We explore the decomposition of the Minimum Entropy Coupling with Bottleneck (MEC-B) into two distinct optimization problems: Entropy-Bounded Information Maximization (EBIM) for the encoder, and Minimum Entropy Coupling (MEC) for the decoder. Through extensive analysis, we provide a greedy algorithm for EBIM with guaranteed performance, and characterize the optimal solution near functional mappings, yielding significant theoretical insights into the structural complexity of this problem. Furthermore, we illustrate the practical application of MEC-B through experiments in Markov Coding Games (MCGs) under rate limits. These games simulate a communication scenario within a Markov Decision Process, where an agent must transmit a compressed message from a sender to a receiver through its actions. Our experiments highlight the trade-offs between MDP rewards and receiver accuracy across various compression rates, showcasing the efficacy of our method compared to conventional compression baseline.', 'score': 4, 'issue_id': 379, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '9e1b09f8eff55094', 'data': {'categories': ['#rl', '#benchmark', '#optimization', '#math', '#data', '#training', '#games'], 'emoji': '🗜️', 'ru': {'title': 'Сжатие данных с контролируемыми потерями для задач с изменением распределения', 'desc': "Статья представляет новую систему сжатия с потерями, работающую в условиях логарифмических потерь, когда распределение реконструкции отличается от исходного распределения. Это расширение классической задачи минимального энтропийного связывания, включающее 'бутылочное горлышко' для контроля стохастичности. Авторы разделяют задачу на две оптимизационные подзадачи: максимизацию информации с ограничением энтропии (EBIM) для кодировщика и минимальное энтропийное связывание (MEC) для декодера. Практическое применение метода демонстрируется на примере марковских игр кодирования (MCG) с ограничениями скорости передачи данных."}, 'en': {'title': 'Revolutionizing Compression with Controlled Stochasticity', 'desc': 'This paper presents a new lossy compression framework that uses logarithmic loss to effectively manage cases where the output distribution differs from the original input distribution. It is particularly useful for tasks that involve both compression and retrieval, especially when there are changes in the data distribution. The authors enhance the traditional minimum entropy coupling method by adding a bottleneck, which allows for a controlled level of randomness in the data coupling process. They break down the new framework, called Minimum Entropy Coupling with Bottleneck (MEC-B), into two optimization tasks: one for maximizing information during encoding and another for minimizing entropy during decoding, providing a greedy algorithm with proven performance.'}, 'zh': {'title': '新型有损压缩框架：控制随机性的最小熵耦合', 'desc': '本文研究了一种新颖的有损压缩框架，该框架在对数损失下运行，旨在处理重建分布与源分布不一致的情况。该框架特别适用于需要联合压缩和检索的应用，以及由于处理导致的分布变化场景。我们展示了所提出的公式如何通过集成瓶颈扩展经典的最小熵耦合框架，从而在耦合中允许控制的随机性程度。通过对最小熵耦合与瓶颈（MEC-B）的分解，我们提出了编码器的熵约束信息最大化（EBIM）和解码器的最小熵耦合（MEC）两个优化问题，并提供了保证性能的贪婪算法。'}}}, {'id': 'https://huggingface.co/papers/2410.24218', 'title': 'Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use', 'url': 'https://huggingface.co/papers/2410.24218', 'abstract': "In real-world scenarios, it is desirable for embodied agents to have the ability to leverage human language to gain explicit or implicit knowledge for learning tasks. Despite recent progress, most previous approaches adopt simple low-level instructions as language inputs, which may not reflect natural human communication. It's not clear how to incorporate rich language use to facilitate task learning. To address this question, this paper studies different types of language inputs in facilitating reinforcement learning (RL) embodied agents. More specifically, we examine how different levels of language informativeness (i.e., feedback on past behaviors and future guidance) and diversity (i.e., variation of language expressions) impact agent learning and inference. Our empirical results based on four RL benchmarks demonstrate that agents trained with diverse and informative language feedback can achieve enhanced generalization and fast adaptation to new tasks. These findings highlight the pivotal role of language use in teaching embodied agents new tasks in an open world. Project website: https://github.com/sled-group/Teachable_RL", 'score': 4, 'issue_id': 375, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '3f784be6816fb87a', 'data': {'categories': ['#reasoning', '#rl', '#rlhf', '#benchmark', '#transfer_learning', '#games', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Язык как ключ к обучению ИИ-агентов в реальном мире', 'desc': 'Статья исследует влияние различных типов языковых входных данных на обучение агентов с подкреплением в воплощенных средах. Авторы изучают, как информативность и разнообразие языка влияют на обучение и вывод агентов. Эксперименты на четырех эталонных задачах показывают, что агенты, обученные с разнообразной и информативной языковой обратной связью, достигают лучшей обобщаемости и быстрой адаптации к новым задачам. Результаты подчеркивают ключевую роль использования естественного языка в обучении воплощенных агентов новым задачам.'}, 'en': {'title': 'Empowering Agents with Rich Language for Smarter Learning', 'desc': 'This paper explores how embodied agents can use human language to improve their learning in real-world tasks. It focuses on the impact of different types of language inputs, specifically looking at how informative and diverse language can enhance reinforcement learning (RL) processes. The study shows that agents receiving varied and detailed language feedback perform better in adapting to new tasks and generalizing their knowledge. These findings emphasize the importance of natural language communication in training intelligent agents to operate effectively in dynamic environments.'}, 'zh': {'title': '语言助力智能体学习新任务', 'desc': '本论文研究了如何利用人类语言来帮助具身智能体学习任务。我们探讨了不同类型的语言输入对强化学习（RL）智能体的影响，特别是语言信息量和多样性如何影响学习效果。实验证明，使用多样且信息丰富的语言反馈训练的智能体，能够更好地适应新任务并提高泛化能力。这些发现强调了语言在教导具身智能体新任务中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2410.23825', 'title': 'GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages', 'url': 'https://huggingface.co/papers/2410.23825', 'abstract': 'The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it - including the pipeline, language identification model, and filters - available to the research community. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1, Pipeline v. 3.0 https://github.com/cisnlp/GlotCC.', 'score': 3, 'issue_id': 375, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '7d1bf1bef5a430e0', 'data': {'categories': ['#multilingual', '#data', '#dataset', '#open_source', '#low_resource'], 'emoji': '🌐', 'ru': {'title': 'GlotCC: Многоязычный корпус для обучения языковых моделей', 'desc': 'Статья представляет GlotCC - чистый корпус текстов объемом 2 ТБ, охватывающий более 1000 языков, включая миноритарные. Корпус создан на основе CommonCrawl с использованием воспроизводимого открытого конвейера обработки данных. GlotCC тщательно очищен от шума, что делает его надежным для использования в обучении языковых моделей. Авторы предоставляют доступ к корпусу и системе его создания для исследовательского сообщества.'}, 'en': {'title': 'Empowering Minority Languages with GlotCC: A Clean, Large-Scale Corpus', 'desc': 'This paper introduces GlotCC, a large and clean text corpus designed for minority languages, addressing the gap in available data for these languages in the context of pretrained language models. The corpus is derived from CommonCrawl and spans over 2TB, covering more than 1000 languages, making it a valuable resource for researchers. The authors emphasize the importance of having a reproducible and open-source pipeline for generating such corpora, which includes a language identification model and noise filters. By providing GlotCC and its generation system to the research community, the authors aim to enhance the development of language models for underrepresented languages.'}, 'zh': {'title': 'GlotCC：多语言研究的新资源', 'desc': '随着预训练语言模型的发展，对大规模文本语料库的需求不断增加。现有的语料库主要集中在大型语言社区，缺乏覆盖广泛少数语言的资源。我们提出了GlotCC，这是一个干净的、文档级的2TB通用语料库，来源于CommonCrawl，涵盖了1000多种语言。我们将GlotCC及其生成系统（包括管道、语言识别模型和过滤器）提供给研究社区，以促进多语言研究。'}}}, {'id': 'https://huggingface.co/papers/2411.08147', 'title': 'Large Language Models Can Self-Improve in Long-context Reasoning', 'url': 'https://huggingface.co/papers/2411.08147', 'abstract': 'Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose SEALONG, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of SEALONG, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, SEALONG achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs.', 'score': 39, 'issue_id': 567, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': 'ea4232d9ddd5ef31', 'data': {'categories': ['#training', '#synthetic', '#long_context', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Самообучение LLMs для длинных контекстов', 'desc': 'В статье рассматривается проблема улучшения способности больших языковых моделей (LLMs) к рассуждению в длинных контекстах. Авторы предлагают метод SEALONG, который позволяет моделям самостоятельно улучшаться без необходимости в данных от экспертов или продвинутых моделей. Метод заключается в генерации нескольких ответов на вопрос, их оценке с помощью минимального байесовского риска и последующей оптимизации. Эксперименты показывают, что SEALONG значительно улучшает производительность моделей, таких как Llama-3.1-8B-Instruct.'}, 'en': {'title': 'Empowering LLMs to Self-Improve Long-Context Reasoning', 'desc': 'This paper addresses the challenges that large language models (LLMs) face in reasoning over long contexts. The authors propose a new method called SEALONG, which allows LLMs to enhance their long-context reasoning capabilities without relying on human-annotated data. Instead, the approach involves generating multiple outputs for each question, scoring them using Minimum Bayes Risk, and then fine-tuning the model based on these scores. Experimental results show that SEALONG significantly improves performance on Llama-3.1-8B-Instruct, outperforming previous methods that depend on expert-generated data.'}, 'zh': {'title': '让大型语言模型自我提升长文本推理能力', 'desc': '大型语言模型（LLMs）在处理长文本方面取得了显著进展，但在长文本推理上仍然存在困难。现有的方法通常依赖于人工专家或先进模型（如GPT-4）提供的合成数据进行微调，这限制了进一步的发展。为了解决这个问题，我们提出了一种名为SEALONG的方法，旨在让LLMs在长文本推理中自我改进。通过对每个问题采样多个输出，并使用最小贝叶斯风险进行评分，我们可以基于这些输出进行监督微调或偏好优化，从而显著提高模型性能。'}}}, {'id': 'https://huggingface.co/papers/2411.08380', 'title': 'EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation', 'url': 'https://huggingface.co/papers/2411.08380', 'abstract': 'Video generation has emerged as a promising tool for world simulation, leveraging visual data to replicate real-world environments. Within this context, egocentric video generation, which centers on the human perspective, holds significant potential for enhancing applications in virtual reality, augmented reality, and gaming. However, the generation of egocentric videos presents substantial challenges due to the dynamic nature of egocentric viewpoints, the intricate diversity of actions, and the complex variety of scenes encountered. Existing datasets are inadequate for addressing these challenges effectively. To bridge this gap, we present EgoVid-5M, the first high-quality dataset specifically curated for egocentric video generation. EgoVid-5M encompasses 5 million egocentric video clips and is enriched with detailed action annotations, including fine-grained kinematic control and high-level textual descriptions. To ensure the integrity and usability of the dataset, we implement a sophisticated data cleaning pipeline designed to maintain frame consistency, action coherence, and motion smoothness under egocentric conditions. Furthermore, we introduce EgoDreamer, which is capable of generating egocentric videos driven simultaneously by action descriptions and kinematic control signals. The EgoVid-5M dataset, associated action annotations, and all data cleansing metadata will be released for the advancement of research in egocentric video generation.', 'score': 14, 'issue_id': 565, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '7c82b1fc5a785fe0', 'data': {'categories': ['#data', '#synthetic', '#games', '#video', '#dataset'], 'emoji': '👀', 'ru': {'title': 'EgoVid-5M: революция в генерации эгоцентрических видео', 'desc': 'Статья представляет EgoVid-5M - первый крупномасштабный датасет для генерации эгоцентрических видео, содержащий 5 миллионов клипов с подробными аннотациями действий. Авторы разработали сложный процесс очистки данных для обеспечения целостности и удобства использования датасета. Также представлена модель EgoDreamer, способная генерировать эгоцентрические видео на основе описаний действий и кинематических сигналов управления. Датасет, аннотации и метаданные будут опубликованы для продвижения исследований в области генерации эгоцентрических видео.'}, 'en': {'title': 'EgoVid-5M: Revolutionizing Egocentric Video Generation', 'desc': 'This paper introduces EgoVid-5M, a new dataset designed for generating egocentric videos, which are videos captured from a first-person perspective. The dataset contains 5 million video clips with detailed annotations that describe the actions and movements occurring in each clip. The authors also present EgoDreamer, a model that can create egocentric videos based on both action descriptions and kinematic controls. This work aims to address the challenges of existing datasets and improve the quality and usability of egocentric video generation for applications in virtual and augmented reality.'}, 'zh': {'title': 'EgoVid-5M：自我视角视频生成的新突破', 'desc': '视频生成是一种有前景的世界模拟工具，可以利用视觉数据复制现实环境。以自我视角为中心的视频生成在虚拟现实、增强现实和游戏应用中具有重要潜力。然而，自我视角视频的生成面临着动态视角、复杂动作和多样场景的挑战。为了解决这些问题，我们提出了EgoVid-5M，这是第一个专门为自我视角视频生成而创建的高质量数据集，包含500万个视频片段和详细的动作注释。'}}}, {'id': 'https://huggingface.co/papers/2411.07618', 'title': 'Direct Preference Optimization Using Sparse Feature-Level Constraints', 'url': 'https://huggingface.co/papers/2411.07618', 'abstract': 'The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.', 'score': 10, 'issue_id': 565, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': 'b7ab0d7ebab29360', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': '🎯', 'ru': {'title': 'Эффективное выравнивание языковых моделей с помощью разреженных признаков', 'desc': 'Статья представляет новый метод для выравнивания больших языковых моделей (LLM) с человеческими предпочтениями - Feature-level constrained Preference Optimization (FPO). FPO использует предобученные разреженные автоэнкодеры и вводит ограничения на уровне признаков для эффективного выравнивания. Метод показывает улучшение на 5.08% по сравнению с современными базовыми методами при значительно меньших вычислительных затратах. FPO предлагает перспективное решение для эффективного и контролируемого выравнивания LLM.'}, 'en': {'title': 'Efficient Alignment of Language Models with Human Preferences', 'desc': 'This paper addresses the challenge of aligning large language models (LLMs) with human preferences. It introduces a new method called Feature-level constrained Preference Optimization (FPO), which aims to improve the alignment process while maintaining stability and efficiency. FPO utilizes pre-trained Sparse Autoencoders (SAEs) and applies feature-level constraints to enhance the alignment without incurring high computational costs. Experimental results show that FPO outperforms existing methods, achieving a significant improvement in performance while being more resource-efficient.'}, 'zh': {'title': '高效稳定的语言模型对齐新方法', 'desc': '本文提出了一种新的方法，称为特征级约束偏好优化（FPO），旨在简化大型语言模型（LLM）与人类偏好的对齐过程。FPO利用预训练的稀疏自编码器（SAE）并引入特征级约束，从而实现高效且稳定的对齐。通过激活稀疏特征和使用特征级离线参考，FPO在计算成本上显著降低，同时提高了模型的性能。实验结果表明，FPO在基准数据集上相较于最先进的基线方法，赢率提高了5.08%。'}}}, {'id': 'https://huggingface.co/papers/2411.08868', 'title': 'CamemBERT 2.0: A Smarter French Language Model Aged to Perfection', 'url': 'https://huggingface.co/papers/2411.08868', 'abstract': 'French language models, such as CamemBERT, have been widely adopted across industries for natural language processing (NLP) tasks, with models like CamemBERT seeing over 4 million downloads per month. However, these models face challenges due to temporal concept drift, where outdated training data leads to a decline in performance, especially when encountering new topics and terminology. This issue emphasizes the need for updated models that reflect current linguistic trends. In this paper, we introduce two new versions of the CamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these challenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use of the Replaced Token Detection (RTD) objective for better contextual understanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked Language Modeling (MLM) objective. Both models are trained on a significantly larger and more recent dataset with longer context length and an updated tokenizer that enhances tokenization performance for French. We evaluate the performance of these models on both general-domain NLP tasks and domain-specific applications, such as medical field tasks, demonstrating their versatility and effectiveness across a range of use cases. Our results show that these updated models vastly outperform their predecessors, making them valuable tools for modern NLP systems. All our new models, as well as intermediate checkpoints, are made openly available on Huggingface.', 'score': 8, 'issue_id': 569, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '6be618c24defa5fb', 'data': {'categories': ['#training', '#multilingual', '#healthcare', '#long_context', '#architecture', '#dataset', '#low_resource', '#open_source'], 'emoji': '🇫🇷', 'ru': {'title': 'Обновленные французские языковые модели для современных задач NLP', 'desc': 'В статье представлены две новые версии французской языковой модели CamemBERT - CamemBERTav2 и CamemBERTv2. Эти модели призваны решить проблему временного дрейфа концепций, из-за которого устаревшие данные для обучения приводят к снижению производительности. CamemBERTav2 основана на архитектуре DeBERTaV3 и использует метод Replaced Token Detection, а CamemBERTv2 построена на RoBERTa с применением Masked Language Modeling. Обе модели обучены на более обширном и современном наборе данных с увеличенной длиной контекста и улучшенным токенизатором для французского языка.'}, 'en': {'title': 'Revamping CamemBERT for Modern NLP Challenges', 'desc': 'This paper presents two updated versions of the French language model CamemBERT, named CamemBERTav2 and CamemBERTv2, to tackle the issue of temporal concept drift in natural language processing. CamemBERTav2 utilizes the DeBERTaV3 architecture with a Replaced Token Detection objective, while CamemBERTv2 is based on RoBERTa and employs Masked Language Modeling. Both models are trained on a larger, more recent dataset, improving their contextual understanding and tokenization performance for French. The evaluation shows that these models significantly outperform previous versions in various NLP tasks, including general and domain-specific applications.'}, 'zh': {'title': '更新模型，提升法语NLP性能', 'desc': '本文介绍了两种新的法语语言模型CamemBERTav2和CamemBERTv2，旨在解决由于过时训练数据导致的性能下降问题。这些模型基于DeBERTaV3和RoBERTa架构，分别采用了替换标记检测（RTD）和掩蔽语言建模（MLM）目标，以提高上下文理解能力。它们在更大且更新的数据集上进行训练，具有更长的上下文长度和改进的分词器，提升了法语的分词性能。实验结果表明，这些更新的模型在通用和特定领域的自然语言处理任务中表现优异，成为现代NLP系统的重要工具。'}}}, {'id': 'https://huggingface.co/papers/2411.08790', 'title': 'Can sparse autoencoders be used to decompose and interpret steering vectors?', 'url': 'https://huggingface.co/papers/2411.08790', 'abstract': 'Steering vectors are a promising approach to control the behaviour of large language models. However, their underlying mechanisms remain poorly understood. While sparse autoencoders (SAEs) may offer a potential method to interpret steering vectors, recent findings show that SAE-reconstructed vectors often lack the steering properties of the original vectors. This paper investigates why directly applying SAEs to steering vectors yields misleading decompositions, identifying two reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in feature directions, which SAEs are not designed to accommodate. These limitations hinder the direct use of SAEs for interpreting steering vectors.', 'score': 4, 'issue_id': 570, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '68e55764db274419', 'data': {'categories': ['#architecture', '#interpretability', '#training'], 'emoji': '🧭', 'ru': {'title': 'Ограничения автоэнкодеров в расшифровке векторов управления ИИ', 'desc': 'Статья исследует проблемы применения разреженных автоэнкодеров (SAE) для интерпретации векторов управления в больших языковых моделях. Авторы выявляют два ключевых ограничения: несоответствие векторов управления входному распределению SAE и наличие значимых отрицательных проекций. Эти факторы препятствуют прямому использованию SAE для анализа векторов управления. Исследование подчеркивает необходимость разработки новых методов для понимания механизмов работы векторов управления в языковых моделях.'}, 'en': {'title': 'Understanding Steering Vectors: Limitations of Sparse Autoencoders', 'desc': 'This paper explores the use of steering vectors in controlling large language models and highlights the challenges in understanding their mechanisms. It specifically examines the limitations of sparse autoencoders (SAEs) when applied to these steering vectors. The authors identify that steering vectors do not fit the input distribution that SAEs are designed for, and they can have significant negative projections that SAEs cannot handle. These findings suggest that using SAEs for interpreting steering vectors may lead to inaccurate results, indicating a need for alternative methods.'}, 'zh': {'title': '揭示引导向量的奥秘', 'desc': '本论文探讨了引导向量在控制大型语言模型行为中的应用潜力，但其机制尚不清楚。研究发现，稀疏自编码器（SAE）在解释引导向量时存在问题，重构的向量往往缺乏原始向量的引导特性。论文指出，直接应用SAE于引导向量会导致误导性的分解，原因包括引导向量超出了SAE设计的输入分布，以及引导向量在特征方向上可能具有有意义的负投影。由于这些限制，SAE在解释引导向量时的直接应用受到阻碍。'}}}, {'id': 'https://huggingface.co/papers/2411.08307', 'title': 'PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation', 'url': 'https://huggingface.co/papers/2411.08307', 'abstract': 'Music generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains a significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io.', 'score': 1, 'issue_id': 575, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '1f1b5e7081062b6f', 'data': {'categories': ['#audio', '#architecture', '#story_generation'], 'emoji': '🎼', 'ru': {'title': 'PerceiverS: Новый подход к генерации структурированной и выразительной музыки', 'desc': 'Статья представляет PerceiverS - новую архитектуру для генерации символической музыки. Модель использует механизмы эффективной сегментации и многомасштабного внимания для улучшения структуры и выразительности генерируемой музыки. PerceiverS сочетает кросс-внимание и самовнимание для захвата как долгосрочных музыкальных структур, так и кратковременных нюансов исполнения. Эксперименты на наборах данных вроде Maestro показывают улучшение в генерации связной и разнообразной музыки.'}, 'en': {'title': 'Enhancing Symbolic Music Generation with PerceiverS', 'desc': 'This paper introduces PerceiverS, a new architecture aimed at improving symbolic music generation. It utilizes Effective Segmentation and Multi-Scale attention mechanisms to learn both long-term structures and short-term expressive details in music. By integrating cross-attention and self-attention, the model effectively captures the overall musical structure while maintaining nuanced performance elements. Evaluations on datasets like Maestro show that PerceiverS generates more coherent and diverse music, achieving better structural consistency and expressive variation.'}, 'zh': {'title': 'PerceiverS：生成富有表现力的符号音乐新方法', 'desc': '本文提出了一种新的音乐生成架构，称为PerceiverS（分段与尺度），旨在解决生成长结构和富有表现力的符号音乐的挑战。该模型结合了有效的分段和多尺度注意力机制，能够同时学习长期结构依赖和短期表现细节。通过在多尺度设置中结合交叉注意力和自注意力，PerceiverS能够捕捉长距离的音乐结构，同时保留演奏的细微差别。实验结果表明，该模型在生成连贯且多样化的音乐方面表现出色，具有结构一致性和表现变化。'}}}, {'id': 'https://huggingface.co/papers/2411.08328', 'title': 'Motion Control for Enhanced Complex Action Video Generation', 'url': 'https://huggingface.co/papers/2411.08328', 'abstract': "Existing text-to-video (T2V) models often struggle with generating videos with sufficiently pronounced or complex actions. A key limitation lies in the text prompt's inability to precisely convey intricate motion details. To address this, we propose a novel framework, MVideo, designed to produce long-duration videos with precise, fluid actions. MVideo overcomes the limitations of text prompts by incorporating mask sequences as an additional motion condition input, providing a clearer, more accurate representation of intended actions. Leveraging foundational vision models such as GroundingDINO and SAM2, MVideo automatically generates mask sequences, enhancing both efficiency and robustness. Our results demonstrate that, after training, MVideo effectively aligns text prompts with motion conditions to produce videos that simultaneously meet both criteria. This dual control mechanism allows for more dynamic video generation by enabling alterations to either the text prompt or motion condition independently, or both in tandem. Furthermore, MVideo supports motion condition editing and composition, facilitating the generation of videos with more complex actions. MVideo thus advances T2V motion generation, setting a strong benchmark for improved action depiction in current video diffusion models. Our project page is available at https://mvideo-v1.github.io/.", 'score': 0, 'issue_id': 575, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': 'ff99307fadb40120', 'data': {'categories': ['#multimodal', '#diffusion', '#video', '#benchmark', '#games'], 'emoji': '🎬', 'ru': {'title': 'Точные движения в генерируемых видео с помощью масок', 'desc': 'MVideo - это новая система для генерации видео по текстовому описанию, которая решает проблему недостаточно выраженных или сложных действий в существующих моделях. Она использует последовательности масок в качестве дополнительного входа для более точного представления движений. MVideo автоматически генерирует эти маски с помощью моделей компьютерного зрения, что повышает эффективность и надежность. Система позволяет независимо изменять текстовое описание и условия движения, а также поддерживает редактирование и композицию условий движения.'}, 'en': {'title': 'MVideo: Revolutionizing Text-to-Video with Precise Motion Control', 'desc': 'The paper introduces MVideo, a new framework for text-to-video (T2V) generation that enhances the depiction of complex actions in videos. It addresses the limitations of traditional text prompts by using mask sequences as an additional input, which provides clearer motion details. MVideo utilizes advanced vision models to automatically create these mask sequences, improving the efficiency and accuracy of video generation. The framework allows for independent or combined adjustments to text prompts and motion conditions, enabling the creation of more dynamic and intricate videos.'}, 'zh': {'title': 'MVideo：提升文本到视频生成的动态性与精确性', 'desc': '现有的文本到视频（T2V）模型在生成复杂动作的视频时常常面临挑战。主要问题在于文本提示无法准确传达复杂的运动细节。为了解决这个问题，我们提出了一种新框架MVideo，旨在生成具有精确流畅动作的长时视频。MVideo通过引入掩码序列作为额外的运动条件输入，克服了文本提示的局限性，从而实现更动态的视频生成。'}}}, {'id': 'https://huggingface.co/papers/2411.07232', 'title': 'Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models', 'url': 'https://huggingface.co/papers/2411.07232', 'abstract': 'Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models\' attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed "Additing Affordance Benchmark" for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics.', 'score': 41, 'issue_id': 523, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': '5e344b551de578a9', 'data': {'categories': ['#diffusion', '#dataset', '#cv', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Add-it: Умное добавление объектов в изображения без дополнительного обучения', 'desc': 'В статье представлен метод Add-it для добавления объектов в изображения на основе текстовых инструкций. Он использует расширенный механизм внимания в диффузионных моделях, учитывая информацию из сцены, текстового запроса и генерируемого изображения. Add-it превосходит современные методы на реальных и сгенерированных бенчмарках без специального обучения. Метод предпочтителен в более чем 80% случаев по оценкам людей.'}, 'en': {'title': 'Seamless Object Insertion with Add-it: No Fine-Tuning Needed!', 'desc': "This paper presents Add-it, a novel approach for adding objects to images based on text instructions, addressing the challenge of maintaining the original scene's integrity while ensuring the new object is placed naturally. The method leverages diffusion models' attention mechanisms, integrating information from the scene image, text prompt, and generated image to achieve seamless object insertion. By employing a weighted extended-attention mechanism, Add-it preserves structural consistency and fine details, resulting in more plausible object placements. Remarkably, Add-it does not require task-specific fine-tuning and outperforms existing supervised methods on various benchmarks, including a new evaluation standard for object placement plausibility."}, 'zh': {'title': '无缝图像编辑的新突破', 'desc': '这篇论文介绍了一种名为Add-it的方法，用于根据文本指令将物体添加到图像中。该方法利用扩散模型的注意力机制，结合场景图像、文本提示和生成图像的信息，以实现自然的物体放置。Add-it在不进行特定任务微调的情况下，达到了图像插入基准测试的最先进结果，并在80%以上的情况下被人类评估者所偏好。该方法保持了结构一致性和细节，同时确保了物体的自然位置。'}}}, {'id': 'https://huggingface.co/papers/2411.07199', 'title': 'OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision', 'url': 'https://huggingface.co/papers/2411.07199', 'abstract': 'Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \\omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \\omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \\omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/', 'score': 37, 'issue_id': 522, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': '89d7bedc1b5241ac', 'data': {'categories': ['#dataset', '#data', '#optimization', '#cv', '#architecture', '#open_source', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'OmniEdit: универсальный редактор изображений', 'desc': 'В статье рассматриваются методы редактирования изображений с использованием моделей диффузии, которые обучаются на автоматически синтезированных или вручную аннотированных парах изображений. Основные проблемы существующих моделей включают ограниченные навыки редактирования, шумные и артефактные данные, а также ограничение по разрешению и соотношению сторон изображений. Представлена новая модель OmniEdit, способная выполнять семь различных задач редактирования изображений с любым соотношением сторон. OmniEdit использует обучение от семи специализированных моделей, улучшает качество данных с помощью важностной выборки и предлагает новую архитектуру EditNet для повышения успешности редактирования.'}, 'en': {'title': 'OmniEdit: The All-in-One Image Editing Solution', 'desc': "This paper introduces \textit{omniedit}, a versatile image editing model designed to tackle multiple editing tasks with varying aspect ratios. The authors address key challenges in existing methods, such as biased synthesis, noisy datasets, and fixed resolutions, which limit practical applications. By leveraging supervision from multiple specialist models and employing advanced importance sampling techniques, \textit{omniedit} enhances data quality and editing performance. The proposed EditNet architecture further improves the model's success rate, making it a powerful tool for real-world image editing scenarios."}, 'zh': {'title': '全能图像编辑，打破现实应用的限制', 'desc': '本文介绍了一种名为\textit{omniedit}的全能图像编辑器，旨在解决现有图像编辑方法在实际应用中的局限性。我们识别出三个主要挑战，包括偏见合成过程导致的编辑能力有限、训练数据集中的噪声和伪影问题，以及数据集的低分辨率和固定宽高比限制。通过利用七个不同专业模型的监督，\textit{omniedit}能够处理七种不同的图像编辑任务，并且支持任意宽高比。我们的实验结果表明，\textit{omniedit}在自动评估和人工评估中均显著优于现有模型。'}}}, {'id': 'https://huggingface.co/papers/2411.07140', 'title': 'Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models', 'url': 'https://huggingface.co/papers/2411.07140', 'abstract': 'New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.', 'score': 29, 'issue_id': 522, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': 'ffca97b13123516b', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#low_resource', '#multilingual'], 'emoji': '🇨🇳', 'ru': {'title': 'Новый китайский бенчмарк для оценки фактологических способностей языковых моделей', 'desc': 'Статья представляет новый бенчмарк Chinese SimpleQA для оценки фактологических способностей языковых моделей на китайском языке. Этот бенчмарк охватывает 6 основных тем и 99 подтем, с коротким форматом вопросов и ответов. Авторы провели тщательный контроль качества для обеспечения высокого уровня вопросов и ответов. Chinese SimpleQA позволяет легко оценивать модели с помощью API OpenAI и призван помочь разработчикам лучше понять возможности своих моделей в работе с китайским языком.'}, 'en': {'title': 'Empowering Chinese LLMs with SimpleQA Factuality Benchmark', 'desc': 'This paper introduces Chinese SimpleQA, a new benchmark designed to evaluate the factuality of Large Language Models (LLMs) specifically for the Chinese language. It features a diverse set of questions across six major topics, ensuring high-quality and static reference answers for consistency in evaluation. The benchmark emphasizes short questions and answers, making the grading process straightforward and efficient, particularly using the OpenAI API. The authors aim for Chinese SimpleQA to help developers assess and improve the factuality capabilities of their models in the Chinese context.'}, 'zh': {'title': '中文SimpleQA：提升语言模型事实能力的基准', 'desc': '本文介绍了中文SimpleQA，这是第一个全面评估语言模型回答短问题的事实能力的基准。该基准专注于中文，涵盖六个主要主题和99个多样化的子主题。我们通过严格的质量控制过程，确保问题和答案的高质量，并且参考答案是静态的，不会随时间变化。希望中文SimpleQA能够帮助开发者更好地理解其模型的中文事实能力，促进基础模型的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.06176', 'title': 'M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework', 'url': 'https://huggingface.co/papers/2411.06176', 'abstract': 'The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io.', 'score': 28, 'issue_id': 521, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 ноября', 'en': 'November 9', 'zh': '11月9日'}, 'hash': '950719af940fd8d0', 'data': {'categories': ['#benchmark', '#long_context', '#dataset', '#multimodal', '#open_source', '#training'], 'emoji': '📄', 'ru': {'title': 'M-LongDoc: прорыв в понимании длинных мультимодальных документов', 'desc': 'Статья представляет M-LongDoc - новый набор данных и фреймворк для оценки мультимодальных моделей в задаче понимания длинных документов. Авторы предлагают метод дообучения, учитывающий особенности поиска в мультимодальных документах. Набор данных включает 851 образец современных многостраничных документов, требующих генеративных, а не только экстрактивных ответов. Эксперименты показывают, что предложенный подход улучшает корректность ответов модели на 4.6% по сравнению с базовыми моделями с открытым исходным кодом.'}, 'en': {'title': 'Enhancing Multimodal Document Understanding with M-LongDoc', 'desc': 'This paper presents M-LongDoc, a benchmark designed to evaluate large multimodal models on lengthy documents that include text, figures, and tables. The authors introduce a retrieval-aware tuning method that enhances the efficiency and effectiveness of multimodal document reading, particularly for open-ended question-answering tasks. Unlike previous benchmarks, M-LongDoc features more recent and extensive documents, requiring models to generate comprehensive answers rather than just extractive responses. Experimental results indicate that the proposed tuning approach improves the accuracy of model responses by 4.6% compared to existing baseline models.'}, 'zh': {'title': '提升多模态文档理解的效率与效果', 'desc': '本文介绍了一种名为M-LongDoc的基准数据集，包含851个样本，旨在评估大型多模态模型在文档理解和问答任务中的表现。由于文档通常包含文本、图形和表格等多种内容，人工阅读耗时较长，因此需要开发有效的自动化方法来辅助人类。我们提出了一种检索感知的调优方法，以提高多模态文档阅读的效率和效果。实验结果表明，该方法在模型响应的正确性上相较于基线开源模型有4.6%的相对提升。'}}}, {'id': 'https://huggingface.co/papers/2411.05830', 'title': 'GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models', 'url': 'https://huggingface.co/papers/2411.05830', 'abstract': "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\GitChameleon{}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.  is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, GPT-4o achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries,  serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.", 'score': 18, 'issue_id': 528, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '86b9530c6bdf431e', 'data': {'categories': ['#benchmark', '#optimization', '#open_source', '#plp', '#dataset'], 'emoji': '🦎', 'ru': {'title': 'GitChameleon: Тест на адаптивность моделей к версиям библиотек', 'desc': 'Статья представляет GitChameleon - новый набор данных для оценки способности языковых моделей генерировать код, совместимый с конкретными версиями библиотек. Датасет содержит 116 задач по автодополнению кода на Python с исполняемыми юнит-тестами. Эксперименты показали, что современные большие языковые модели (LLM) испытывают трудности с этой задачей - GPT-4 достигает лишь 39.9% успешных решений из 10 попыток. GitChameleon призван стимулировать разработку более адаптивных моделей генерации кода.'}, 'en': {'title': 'Adapting Code Generation to Evolving Libraries with GitChameleon', 'desc': 'This paper addresses the challenges faced by code generation models due to the rapid evolution of software libraries. It introduces \textit{GitChameleon}, a new dataset with 116 Python code completion problems that are specifically tied to different library versions and include executable unit tests. The dataset allows for a more rigorous evaluation of large language models (LLMs) in generating code that is both syntactically correct and functionally accurate. The findings indicate that current state-of-the-art models, like GPT-4o, struggle with this task, underscoring the need for improved adaptability in code generation systems.'}, 'zh': {'title': '应对软件库版本更新的代码生成挑战', 'desc': '随着软件库的快速发展，代码生成模型面临着适应频繁版本更新的挑战，同时还需保持与旧版本的兼容性。现有的代码补全基准测试往往忽视了这一动态特性，而考虑到这一点的测试又依赖于静态代码预测任务，缺乏基于执行的评估，限制了模型的实际可用性。为了解决这个问题，我们引入了\textit{GitChameleon}，这是一个新颖的手动整理数据集，包含116个Python代码补全问题，每个问题都基于特定的库版本，并附有可执行的单元测试。我们的评估显示，当前最先进的大型语言模型在生成版本特定代码方面表现不佳，强调了这一问题的复杂性和现有模型的局限性。'}}}, {'id': 'https://huggingface.co/papers/2411.07126', 'title': 'Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models', 'url': 'https://huggingface.co/papers/2411.07126', 'abstract': 'We introduce Edify Image, a family of diffusion models capable of generating photorealistic image content with pixel-perfect accuracy. Edify Image utilizes cascaded pixel-space diffusion models trained using a novel Laplacian diffusion process, in which image signals at different frequency bands are attenuated at varying rates. Edify Image supports a wide range of applications, including text-to-image synthesis, 4K upsampling, ControlNets, 360 HDR panorama generation, and finetuning for image customization.', 'score': 18, 'issue_id': 521, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': 'a7486a925b416669', 'data': {'categories': ['#3d', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Фотореалистичная генерация изображений с пиксельной точностью', 'desc': 'Edify Image - это семейство диффузионных моделей, способных генерировать фотореалистичный контент с пиксельной точностью. Модель использует каскадные диффузионные модели в пространстве пикселей, обученные с помощью нового процесса лапласовской диффузии. В этом процессе сигналы изображения на разных частотных диапазонах затухают с разной скоростью. Edify Image поддерживает широкий спектр приложений, включая синтез изображений по тексту, апскейлинг до 4K, ControlNets и генерацию панорам HDR 360°. Модель также позволяет осуществлять тонкую настройку для кастомизации изображений.'}, 'en': {'title': 'Edify Image: Revolutionizing Photorealistic Image Generation with Precision', 'desc': 'Edify Image is a new set of diffusion models designed to create highly realistic images with precise detail. It employs a unique Laplacian diffusion process that adjusts the diffusion rates for different frequency bands of image signals. This allows for versatile applications such as generating images from text, enhancing image resolution to 4K, and creating panoramic images. Additionally, it offers customization options through finetuning, making it adaptable for various image generation tasks.'}, 'zh': {'title': 'Edify Image：生成真实感图像的新突破', 'desc': 'Edify Image是一种扩散模型，能够生成像素级精确的真实感图像内容。它采用级联像素空间扩散模型，并使用新颖的拉普拉斯扩散过程进行训练，能够以不同的速率衰减不同频率带的图像信号。该模型支持多种应用，包括文本到图像合成、4K超分辨率、ControlNets、360 HDR全景生成以及图像定制的微调。Edify Image在图像生成领域展现了强大的灵活性和高质量的输出。'}}}, {'id': 'https://huggingface.co/papers/2411.06208', 'title': 'IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization', 'url': 'https://huggingface.co/papers/2411.06208', 'abstract': 'In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.', 'score': 14, 'issue_id': 521, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 ноября', 'en': 'November 9', 'zh': '11月9日'}, 'hash': 'de83b5a8e14da36e', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'TRACE и IOPO: новый подход к обучению языковых моделей сложным инструкциям', 'desc': 'Эта статья представляет TRACE - новый эталонный тест для улучшения и оценки способности языковых моделей следовать сложным инструкциям. Авторы также предлагают метод IOPO (оптимизация предпочтений ввода-вывода) для более эффективного обучения моделей. TRACE включает 120 тысяч тренировочных и 1000 оценочных примеров. Эксперименты показывают значительное улучшение результатов по сравнению с существующими методами как на целевых, так и на сторонних данных.'}, 'en': {'title': 'Enhancing LLMs with TRACE and IOPO for Complex Instructions', 'desc': 'This paper addresses the challenge of large language models (LLMs) in following complex instructions, which is becoming increasingly important as their applications grow. It introduces TRACE, a benchmark designed to enhance and evaluate the ability of LLMs to handle complex instructions, featuring a substantial dataset of 120K training examples and 1K evaluation cases. The authors propose a novel alignment method called IOPO (Input-Output Preference Optimization), which focuses on both input and output preferences to improve LLM responses. Experimental results demonstrate that IOPO significantly enhances performance on both in-domain and out-of-domain datasets, outperforming existing methods like SFT and DPO.'}, 'zh': {'title': '提升复杂指令跟随能力的创新方法', 'desc': '在大型语言模型（LLMs）领域，模型准确遵循指令的能力至关重要，尤其是在指令复杂性迅速增加的情况下。本文提出了TRACE，一个用于提高和评估复杂指令跟随能力的基准，包含12万条训练数据和1000条评估数据。我们还提出了IOPO（输入-输出偏好优化）对齐方法，考虑了输入和输出偏好对，帮助LLMs快速对齐响应偏好并深入探索指令偏好。通过在领域内和领域外数据集上的广泛实验，验证了IOPO的有效性，显示出在领域内数据上分别提高了8.15%和2.18%，在领域外数据上提高了6.29%和3.13%。'}}}, {'id': 'https://huggingface.co/papers/2411.07231', 'title': 'Watermark Anything with Localized Messages', 'url': 'https://huggingface.co/papers/2411.07231', 'abstract': 'Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. We introduce a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). The WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked. The models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks. Experiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. Moreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions - no larger than 10% of the image surface - even for small 256times 256 images.', 'score': 11, 'issue_id': 524, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': '0f19ada656cd9116', 'data': {'categories': ['#training', '#cv', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'WAM: Локализованные водяные знаки для любой части изображения', 'desc': 'Эта статья представляет новую модель глубокого обучения для локализованного водяного знака изображений, названную Watermark Anything Model (WAM). WAM способна незаметно встраивать водяные знаки в определенные области изображения и извлекать их, даже если изображение было отредактировано или объединено с другими. Модель обучается сначала на изображениях низкого разрешения, а затем дообучается для улучшения незаметности водяных знаков и работы с несколькими метками. Эксперименты показывают, что WAM конкурентоспособна с современными методами по незаметности и устойчивости, особенно против вставок и склеек изображений.'}, 'en': {'title': 'Watermark Anything: Revolutionizing Localized Image Watermarking', 'desc': 'The paper presents a novel deep-learning model called the Watermark Anything Model (WAM) designed for localized image watermarking. Unlike traditional methods, WAM effectively handles small watermarked areas, making it suitable for images with diverse sources or edits. The model consists of an embedder that subtly alters the image and an extractor that identifies watermarked regions to recover hidden messages. Experimental results demonstrate that WAM achieves high imperceptibility and robustness against common image manipulations, outperforming existing techniques, especially in challenging scenarios involving splicing and inpainting.'}, 'zh': {'title': '局部水印的新突破：水印任何模型', 'desc': '本文提出了一种新的深度学习模型，称为水印任何模型（WAM），用于局部图像水印。该模型能够在不显著改变图像的情况下，嵌入水印并提取隐藏信息。WAM通过联合训练和后期训练，确保水印的隐蔽性和多重水印的能力。实验结果表明，WAM在隐蔽性和鲁棒性方面与最先进的方法相当，尤其在处理拼接和修复时表现优异。'}}}, {'id': 'https://huggingface.co/papers/2411.07180', 'title': 'Counterfactual Generation from Language Models', 'url': 'https://huggingface.co/papers/2411.07180', 'abstract': "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.", 'score': 4, 'issue_id': 521, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': '6b57fa07bdf242ce', 'data': {'categories': ['#math', '#interpretability', '#reasoning', '#training', '#data'], 'emoji': '🔀', 'ru': {'title': 'Контрфактическое рассуждение в языковых моделях: новый взгляд на причинно-следственные связи', 'desc': 'Данная статья представляет новый подход к генерации контрфактических примеров в языковых моделях. Авторы предлагают рассматривать языковые модели как обобщенные структурные уравнения, используя трюк Гумбеля-макса. Это позволяет моделировать совместное распределение оригинальных строк и их контрфактических вариантов. Разработанный алгоритм, основанный на выборке Гумбеля с учетом последствий, позволяет выводить скрытые переменные шума и генерировать контрфактические примеры для наблюдаемых строк.'}, 'en': {'title': 'Harnessing Counterfactuals for Better Control of Language Models', 'desc': 'This paper focuses on understanding how to control language models by manipulating their causal generation mechanisms. It critiques existing methods like representation surgery, which alter model behavior but may not provide precise insights into their effects. The authors introduce a new framework that uses counterfactual reasoning to generate true string counterfactuals, distinguishing it from traditional interventions. Their approach employs Generalized Structural-equation Models and Gumbel-max sampling to effectively model the relationship between original strings and their counterfactuals, revealing the limitations of current intervention techniques.'}, 'zh': {'title': '掌握语言模型的因果生成机制', 'desc': '本文探讨了在语言模型中理解和操控因果生成机制的重要性。以往的研究主要依赖于表示手术等技术来干预模型，但我们强调反事实推理与干预是不同的概念。我们提出了一种框架，通过将语言模型重构为广义结构方程模型，生成真实的字符串反事实。实验表明，该方法能够生成有意义的反事实，同时揭示了常用干预技术的显著副作用。'}}}, {'id': 'https://huggingface.co/papers/2411.05902', 'title': 'Autoregressive Models in Vision: A Survey', 'url': 'https://huggingface.co/papers/2411.05902', 'abstract': 'Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the strategy of representation. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multi-faceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multi-modal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.', 'score': 3, 'issue_id': 532, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': '49d4d6e55fb35ca2', 'data': {'categories': ['#healthcare', '#survey', '#video', '#3d', '#open_source', '#multimodal', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Авторегрессия в компьютерном зрении: от пикселей до 3D', 'desc': 'Эта статья представляет собой обзор авторегрессионных моделей в компьютерном зрении. Авторы рассматривают различные стратегии представления визуальных данных: на уровне пикселей, токенов и масштабов. В работе анализируются связи между авторегрессионными моделями и другими генеративными моделями, а также их применение в различных областях, включая воплощенный ИИ и 3D медицинский ИИ. Статья завершается обсуждением текущих проблем и потенциальных направлений исследований авторегрессионных моделей в компьютерном зрении.'}, 'en': {'title': 'Exploring Autoregressive Models: Bridging NLP and Computer Vision', 'desc': 'This paper surveys the application of autoregressive models in computer vision, highlighting their success in generating high-quality visual content. It categorizes these models into three main types: pixel-based, token-based, and scale-based, reflecting the hierarchical nature of visual data. The authors also discuss the relationship between autoregressive models and other generative models, as well as their applications in various fields such as embodied AI and 3D medical AI. Additionally, the paper addresses current challenges and suggests future research directions, providing a comprehensive resource for researchers in the area.'}, 'zh': {'title': '自回归模型：视觉生成的新前沿', 'desc': '自回归建模在自然语言处理领域取得了巨大成功，最近在计算机视觉中也成为了一个重要的研究方向。自回归模型在视觉内容生成方面表现出色，能够生成高质量的图像和视频。本文综述了自回归模型在视觉中的应用，探讨了像素级、标记级和尺度级等不同的表示策略。我们还分析了自回归模型与其他生成模型的关系，并提出了在新兴领域中的应用和未来研究方向。'}}}, {'id': 'https://huggingface.co/papers/2411.06481', 'title': 'KMM: Key Frame Mask Mamba for Extended Motion Generation', 'url': 'https://huggingface.co/papers/2411.06481', 'abstract': "Human motion generation is a cut-edge area of research in generative computer vision, with promising applications in video creation, game development, and robotic manipulation. The recent Mamba architecture shows promising results in efficiently modeling long and complex sequences, yet two significant challenges remain: Firstly, directly applying Mamba to extended motion generation is ineffective, as the limited capacity of the implicit memory leads to memory decay. Secondly, Mamba struggles with multimodal fusion compared to Transformers, and lack alignment with textual queries, often confusing directions (left or right) or omitting parts of longer text queries. To address these challenges, our paper presents three key contributions: Firstly, we introduce KMM, a novel architecture featuring Key frame Masking Modeling, designed to enhance Mamba's focus on key actions in motion segments. This approach addresses the memory decay problem and represents a pioneering method in customizing strategic frame-level masking in SSMs. Additionally, we designed a contrastive learning paradigm for addressing the multimodal fusion problem in Mamba and improving the motion-text alignment. Finally, we conducted extensive experiments on the go-to dataset, BABEL, achieving state-of-the-art performance with a reduction of more than 57% in FID and 70% parameters compared to previous state-of-the-art methods. See project website: https://steve-zeyu-zhang.github.io/KMM", 'score': 3, 'issue_id': 524, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 ноября', 'en': 'November 10', 'zh': '11月10日'}, 'hash': '970f532cd0007e6c', 'data': {'categories': ['#cv', '#optimization', '#games', '#multimodal', '#long_context', '#architecture', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'KMM: Прорыв в генерации движений человека с помощью улучшенной архитектуры Mamba', 'desc': 'Статья представляет новую архитектуру KMM для генерации движений человека, основанную на модели Mamba. KMM решает проблемы затухания памяти и мультимодального слияния, характерные для Mamba, с помощью маскирования ключевых кадров и контрастивного обучения. Эксперименты на датасете BABEL показали значительное улучшение качества генерации по сравнению с предыдущими методами. Предложенный подход демонстрирует перспективы в области компьютерного зрения и генеративных моделей для создания реалистичных движений человека.'}, 'en': {'title': 'Enhancing Motion Generation with Key Frame Masking', 'desc': "This paper focuses on improving human motion generation using a new architecture called KMM, which stands for Key frame Masking Modeling. The KMM architecture enhances the Mamba model's ability to handle long motion sequences by addressing memory decay and improving focus on key actions. Additionally, it introduces a contrastive learning approach to better fuse multimodal data and align motion with textual queries, overcoming limitations seen in previous models. The authors demonstrate the effectiveness of their method through extensive experiments on the BABEL dataset, achieving significant performance improvements over existing techniques."}, 'zh': {'title': '提升人类动作生成的关键技术', 'desc': '人类动作生成是生成计算机视觉中的前沿研究领域，具有视频创作、游戏开发和机器人操作等应用前景。本文提出了一种新的KMM架构，通过关键帧掩蔽建模来增强Mamba在动作片段中的关键动作关注能力，从而解决了内存衰减问题。我们还设计了一种对比学习范式，以改善Mamba的多模态融合和运动-文本对齐问题。通过在BABEL数据集上的广泛实验，我们的模型在FID指标上减少了超过57%，并且参数量减少了70%，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.05990', 'title': 'Game-theoretic LLM: Agent Workflow for Negotiation Games', 'url': 'https://huggingface.co/papers/2411.05990', 'abstract': "This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.   To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.   Our research contributes to a deeper understanding of LLMs' decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.", 'score': 3, 'issue_id': 522, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': 'aae23469f2886f4c', 'data': {'categories': ['#games', '#agents', '#rl', '#math', '#reasoning'], 'emoji': '🎲', 'ru': {'title': 'Повышение рациональности языковых моделей в теории игр', 'desc': 'Статья исследует рациональность больших языковых моделей (LLM) в контексте теории игр. Авторы оценивают современные LLM в различных играх с полной и неполной информацией, обнаруживая, что модели часто отклоняются от рациональных стратегий. Для решения этой проблемы предложены специальные рабочие процессы, улучшающие способность моделей вычислять равновесия Нэша и принимать рациональные решения. Результаты показывают значительное повышение рациональности и устойчивости LLM в игровых задачах при использовании этих методов.'}, 'en': {'title': 'Enhancing LLM Rationality in Strategic Decision-Making', 'desc': "This paper examines how large language models (LLMs) make decisions in strategic situations using game theory. It finds that LLMs often do not follow rational strategies, especially in complex games with larger payoff matrices. To improve their decision-making, the authors propose game-theoretic workflows that help LLMs better compute Nash Equilibria and make rational choices under uncertainty. The results show that these workflows significantly enhance the models' ability to identify optimal strategies and perform better in negotiation scenarios."}, 'zh': {'title': '提升大型语言模型的博弈理性', 'desc': '本文研究了大型语言模型（LLMs）在战略决策中的理性，特别是在博弈论框架下。我们评估了多种先进的LLMs在完全信息和不完全信息博弈中的表现，发现随着博弈复杂性的增加，LLMs常常偏离理性策略。为了解决这些问题，我们设计了多种博弈论工作流程，以指导LLMs的推理和决策过程，从而提高其计算纳什均衡和在不确定条件下做出理性选择的能力。实验结果表明，采用这些工作流程显著提高了LLMs在博弈任务中的理性和稳健性。'}}}, {'id': 'https://huggingface.co/papers/2411.05945', 'title': 'NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts', 'url': 'https://huggingface.co/papers/2411.05945', 'abstract': "Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an ``expert'' of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset's tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.", 'score': 2, 'issue_id': 528, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': '1306ade09b2fc5c5', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#machine_translation', '#transfer_learning', '#multimodal', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Мультизадачная коррекция ошибок: один MoE для всех доменов', 'desc': 'Статья представляет новый подход к обучению модели для коррекции ошибок распознавания текста в различных областях. Авторы предлагают использовать архитектуру Mixture-of-Experts (MoE) для создания мультизадачной модели коррекции. Эксперты в MoE обучаются на специфических для каждого набора данных особенностях, что позволяет эффективно обрабатывать различные типы входных данных. Эксперименты показывают, что предложенная модель NeKo достигает нового уровня производительности в задачах распознавания речи и машинного перевода, превосходя существующие модели.'}, 'en': {'title': 'Harnessing Mixture-of-Experts for Superior Post-Recognition Error Correction', 'desc': "This paper introduces a novel approach to post-recognition error correction using a Mixture-of-Experts (MoE) model. The authors propose a Multi-Task Correction MoE that effectively learns from diverse datasets, such as speech-to-text and language-to-text, by routing tokens to specialized experts. This method reduces the need for separate correction models, leading to fewer parameters while maintaining high performance. Experimental results demonstrate significant improvements in word error rate (WER) and BLEU scores, showcasing the model's effectiveness across various tasks, including zero-shot evaluations against leading models."}, 'zh': {'title': '混合专家模型：提升多任务学习的利器', 'desc': '本文探讨了如何有效地训练一个通用的后识别错误修正模型，特别是在处理多种领域数据集时。我们提出了一种混合专家模型（Mixture-of-Experts），通过让专家学习特定数据集的特征，从而在一个模型中整合这些知识。实验结果显示，该模型在语音转文本、语言转文本和视觉转文本任务中表现出色，显著降低了错误率。我们的研究在多个基准测试中超越了现有的最先进技术，展示了混合专家模型的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2411.06424', 'title': 'Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction', 'url': 'https://huggingface.co/papers/2411.06424', 'abstract': 'Safety fine-tuning algorithms are commonly used to fine-tune language models to reduce harmful outputs, but the exact internal mechanisms of how those models achieve this remain unclear. In studying direct preference optimisation (DPO) for toxicity reduction, current explanations claim that DPO works by dampening the most toxic MLP neurons to learn an offset to avert toxic regions in the residual stream. However, by ablating the most toxic neurons and applying activation patching, we find this explanation incomplete. By projecting neuron activation changes onto a toxicity probe, we find that only 31.8\\% of toxicity reduction comes from dampened toxic neurons. Instead, DPO reduces toxicity by accumulating effects across multiple neuron groups, both reducing writing in the toxic direction and promoting anti-toxicity in the residual stream. Moreover, DPO gives noisy adjustments to neuron activations, with many neurons actually increasing toxicity. This indicates that DPO is a balancing process between opposing neuron effects to achieve toxicity reduction.', 'score': 2, 'issue_id': 524, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 ноября', 'en': 'November 10', 'zh': '11月10日'}, 'hash': '045698e0e102aa83', 'data': {'categories': ['#ethics', '#training', '#hallucinations', '#rlhf', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'DPO: тонкая настройка нейронов для снижения токсичности', 'desc': 'Статья исследует механизмы работы алгоритмов оптимизации предпочтений (DPO) для снижения токсичности языковых моделей. Авторы опровергают существующие объяснения, согласно которым DPO работает путем подавления наиболее токсичных нейронов. Исследование показывает, что только 31.8% снижения токсичности происходит за счет подавленных токсичных нейронов. DPO снижает токсичность, накапливая эффекты в разных группах нейронов, как уменьшая токсичные активации, так и усиливая анти-токсичные в остаточном потоке.'}, 'en': {'title': 'DPO: Balancing Neuron Effects for Safer Language Models', 'desc': 'This paper investigates how direct preference optimization (DPO) helps reduce harmful outputs in language models. It challenges the common belief that DPO primarily works by dampening the most toxic neurons. Instead, the study reveals that only a small portion of toxicity reduction is due to this dampening effect, with most of the reduction coming from a complex interplay of multiple neuron groups. The findings suggest that DPO functions as a balancing act, where both toxic and anti-toxic neuron activations are adjusted to achieve an overall decrease in toxicity.'}, 'zh': {'title': 'DPO：在对立神经元效应中实现毒性减少的平衡过程', 'desc': '安全微调算法常用于调整语言模型，以减少有害输出，但其内部机制仍不清楚。本文研究了直接偏好优化（DPO）在减少毒性方面的作用，发现现有解释不够全面。通过对最毒神经元的消融和激活修补，我们发现只有31.8%的毒性减少来自于抑制毒性神经元。DPO通过多个神经元组的累积效应来减少毒性，同时在残差流中促进反毒性，表明DPO是一个在对立神经元效应之间平衡的过程。'}}}, {'id': 'https://huggingface.co/papers/2411.06272', 'title': 'Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models', 'url': 'https://huggingface.co/papers/2411.06272', 'abstract': 'As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose "Golden Touchstone", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models\' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.', 'score': 2, 'issue_id': 520, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 ноября', 'en': 'November 9', 'zh': '11月9日'}, 'hash': '2559c023f673c9b4', 'data': {'categories': ['#low_resource', '#optimization', '#open_source', '#multilingual', '#benchmark'], 'emoji': '💹', 'ru': {'title': 'Эталон для оценки финансовых языковых моделей', 'desc': 'В статье обсуждается необходимость создания стандартизированного метода оценки производительности больших языковых моделей (LLM) в финансовом секторе. Авторы предлагают "Golden Touchstone", первый двуязычный эталон для финансовых LLM, который включает в себя наборы данных на китайском и английском языках для восьми основных финансовых задач NLP. Этот эталон позволяет более полно оценивать способности моделей в понимании и генерации финансовой информации. Исследование также включает открытый доступ к коду и весам модели Touchstone-GPT, что способствует дальнейшему развитию и оптимизации финансовых LLM.'}, 'en': {'title': 'Golden Touchstone: Elevating Financial LLM Evaluation', 'desc': "This paper introduces 'Golden Touchstone', a new bilingual benchmark designed to evaluate the performance of large language models (LLMs) in the financial sector. It addresses the shortcomings of existing benchmarks by providing a comprehensive assessment across eight key financial NLP tasks in both Chinese and English. The benchmark is built from high-quality datasets and reflects industry needs, allowing for a thorough evaluation of models like GPT-4o, Llama3, FinGPT, and FinMA. Additionally, the paper presents Touchstone-GPT, a financial LLM that has been fine-tuned for better performance on this benchmark, while also making the resources publicly available to support further research in financial LLMs."}, 'zh': {'title': '金融领域的标准化评估工具——金色基准', 'desc': '随着大型语言模型在金融领域的广泛应用，评估其性能的标准化方法变得尤为重要。现有的金融基准测试存在语言和任务覆盖面有限、数据集质量低以及适应性不足等问题。为了解决这些问题，我们提出了“金色基准”，这是第一个全面的双语金融基准，涵盖了中英文的八个核心金融自然语言处理任务。通过对主要模型的比较分析，我们揭示了它们在处理复杂金融信息时的优缺点，并开源了Touchstone-GPT模型，以促进未来的研究和优化。'}}}, {'id': 'https://huggingface.co/papers/2411.05966', 'title': 'Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation', 'url': 'https://huggingface.co/papers/2411.05966', 'abstract': 'Large language models (LLMs) have demonstrated significant success in natural language processing (NLP) tasks and have shown promising results in other domains such as protein sequence generation. However, there remain salient differences between LLMs used for NLP, which effectively handle multiple tasks and are available in small sizes, and protein language models that are often specialized for specific tasks and only exist in larger sizes. In this work, we introduce two small protein language models, based on Llama-3-8B and Phi-3-mini, that are capable of both uncontrollable and controllable protein generation. For the uncontrollable generation task, our best model achieves an average pLDDT score of 69.75, demonstrating robust performance in generating viable protein structures. For the controllable generation task, in which the model generates proteins according to properties specified in the prompt, we achieve a remarkable average TM-Score of 0.84, indicating high structural similarity to target proteins. We chose 10 properties, including six classes of enzymes, to extend the capabilities of prior protein language models. Our approach utilizes the Low-Rank Adaptor (LoRA) technique, reducing trainable parameters to just 4% of the original model size, lowering computational requirements. By using a subset of the UniRef50 dataset and small models, we reduced the overall training time by 70% without compromising performance. Notably, Phi-3-mini reduced trainable parameters by 60%, decreasing training cost by 30% compared to Llama 3. Consequently, Phi-3 achieved a comparable TM-Score of 0.81, demonstrating that smaller models can match the performance of larger ones, like Llama 3. We also demonstrate the deployment of our models on the energy efficient ET-SoC-1 chip, significantly improving the TPS/W by a factor of 3.', 'score': 1, 'issue_id': 534, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': '7cf32991005092d0', 'data': {'categories': ['#low_resource', '#optimization', '#science', '#dataset', '#small_models', '#training', '#inference'], 'emoji': '🧬', 'ru': {'title': 'Маленькие модели - большие возможности в генерации белков', 'desc': 'В этой работе представлены две небольшие языковые модели для работы с белками, основанные на Llama-3-8B и Phi-3-mini. Модели способны как к неконтролируемой, так и к контролируемой генерации белков, достигая высоких показателей pLDDT и TM-Score соответственно. Исследователи использовали технику Low-Rank Adaptor (LoRA) для уменьшения количества обучаемых параметров и сокращения вычислительных требований. Результаты показывают, что небольшие модели могут достигать производительности, сравнимой с более крупными моделями, при значительном снижении затрат на обучение и развертывание.'}, 'en': {'title': 'Small Models, Big Impact: Efficient Protein Generation', 'desc': 'This paper presents two small protein language models, Llama-3-8B and Phi-3-mini, which excel in both uncontrollable and controllable protein generation tasks. The models achieve impressive performance metrics, with an average pLDDT score of 69.75 for generating viable protein structures and a TM-Score of 0.84 for generating proteins based on specified properties. By employing the Low-Rank Adaptor (LoRA) technique, the models significantly reduce the number of trainable parameters, leading to a 70% decrease in training time and a 30% reduction in training costs. The results indicate that smaller models can achieve comparable performance to larger models while being more efficient, especially when deployed on energy-efficient hardware.'}, 'zh': {'title': '小型模型也能生成高质量蛋白质', 'desc': '本研究介绍了两种小型蛋白质语言模型，基于Llama-3-8B和Phi-3-mini，能够进行不可控和可控的蛋白质生成。我们的最佳模型在不可控生成任务中达到了69.75的平均pLDDT分数，显示出生成可行蛋白质结构的强大性能。在可控生成任务中，模型根据提示生成特定属性的蛋白质，平均TM-Score达到了0.84，表明与目标蛋白质的结构相似性很高。通过使用低秩适配器（LoRA）技术，我们将可训练参数减少到原模型的4%，显著降低了计算需求。'}}}, {'id': 'https://huggingface.co/papers/2411.04997', 'title': 'LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation', 'url': 'https://huggingface.co/papers/2411.04997', 'abstract': "CLIP is one of the most important multimodal foundational models today. What powers CLIP's capabilities? The rich supervision signals provided by natural language, the carrier of human knowledge, shape a powerful cross-modal representation space. However, with the rapid advancements in large language models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs' strong textual understanding can fundamentally improve CLIP's ability to handle image captions, drastically enhancing its ability to process long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP's visual encoder. Thanks to the LLM's presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP's text encoder's context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks.", 'score': 25, 'issue_id': 513, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '5e5b851688791e8a', 'data': {'categories': ['#long_context', '#multimodal', '#games', '#training'], 'emoji': '🔗', 'ru': {'title': 'Усиление CLIP с помощью LLM для лучшего понимания изображений и текста', 'desc': 'LLM2CLIP - это новый подход к улучшению мультимодальной модели CLIP с помощью больших языковых моделей (LLM). Авторы предлагают дообучать LLM в пространстве подписей к изображениям с помощью контрастивного обучения, чтобы улучшить текстовые возможности CLIP. Затем обученная LLM выступает в роли учителя для визуального энкодера CLIP. Этот метод позволяет использовать более длинные и сложные подписи к изображениям, преодолевая ограничения стандартного текстового энкодера CLIP.'}, 'en': {'title': "Unlocking CLIP's Potential with LLMs", 'desc': "This paper introduces LLM2CLIP, a new method that enhances the CLIP model by integrating large language models (LLMs) like GPT-4. By fine-tuning the LLM in the caption space using contrastive learning, the model improves its ability to understand and generate complex image captions. The LLM acts as a teacher for CLIP's visual encoder, allowing it to process longer and more intricate texts than the original CLIP could handle. The results show significant advancements in cross-modal tasks, demonstrating the effectiveness of combining LLMs with multimodal representation learning."}, 'zh': {'title': '利用大型语言模型提升CLIP的多模态学习能力', 'desc': 'CLIP是一个重要的多模态基础模型，本文提出了一种新方法LLM2CLIP，旨在利用大型语言模型（LLMs）来增强CLIP的能力。通过对LLM进行微调并结合对比学习，我们能够提取其文本能力，从而显著提高CLIP在处理图像标题时的表现。LLM的强大文本理解能力使得CLIP能够处理更长和更复杂的文本，克服了传统CLIP的局限性。实验结果表明，这种方法在跨模态任务中带来了显著的改进。'}}}, {'id': 'https://huggingface.co/papers/2411.04282', 'title': 'Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding', 'url': 'https://huggingface.co/papers/2411.04282', 'abstract': 'Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO.', 'score': 22, 'issue_id': 518, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': 'e566070395107bc0', 'data': {'categories': ['#architecture', '#dataset', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие скрытого потенциала рассуждений в языковых моделях', 'desc': 'Статья представляет новый метод LaTRO для улучшения способностей больших языковых моделей (LLM) к рассуждению. LaTRO формулирует процесс рассуждения как выборку из латентного распределения и оптимизирует его с помощью вариационных подходов. Метод позволяет LLM одновременно улучшать процесс рассуждения и способность оценивать качество рассуждений без необходимости внешней обратной связи. Эксперименты на наборах данных GSM8K и ARC-Challenge показали значительное улучшение точности по сравнению с базовыми моделями и обычной дообучением.'}, 'en': {'title': 'Unlocking Reasoning Potential in Large Language Models with LaTRO', 'desc': 'This paper presents LaTent Reasoning Optimization (LaTRO), a new framework designed to enhance the reasoning abilities of large language models (LLMs) during training. LaTRO treats reasoning as a process of sampling from a latent distribution and uses variational methods to optimize this process. The framework allows LLMs to improve their reasoning skills and assess the quality of their reasoning simultaneously, without needing external feedback. Experimental results show that LaTRO significantly boosts the performance of LLMs on reasoning tasks, indicating that pre-trained models have untapped reasoning potential that can be further developed through this method.'}, 'zh': {'title': '解锁大型语言模型的潜在推理能力', 'desc': '大型语言模型（LLMs）在处理复杂推理任务时仍然面临挑战。我们提出了一种新的框架，称为LaTent推理优化（LaTRO），它通过变分方法将推理过程视为从潜在分布中采样。LaTRO可以在训练过程中同时提高模型的推理能力和评估推理质量的能力，而无需外部反馈。实验结果表明，LaTRO显著提高了模型在GSM8K和ARC-Challenge数据集上的表现，证明了预训练LLMs的潜在推理能力可以通过我们的优化方法得到增强。'}}}, {'id': 'https://huggingface.co/papers/2411.05288', 'title': 'Balancing Pipeline Parallelism with Vocabulary Parallelism', 'url': 'https://huggingface.co/papers/2411.05288', 'abstract': 'Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .', 'score': 16, 'issue_id': 509, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': '19accdb712f507d9', 'data': {'categories': ['#architecture', '#inference', '#open_source', '#optimization', '#training'], 'emoji': '⚡', 'ru': {'title': 'Эффективное распараллеливание словарных слоев для ускорения обучения больших языковых моделей', 'desc': 'Статья предлагает новый метод распараллеливания обучения больших языковых моделей, фокусируясь на эффективном распределении слоев словаря. Авторы разработали алгоритмы для равномерного распределения вычислений и памяти между устройствами в конвейере, что позволяет уменьшить простои и оптимизировать использование памяти. Метод интегрируется с существующими схемами конвейерного параллелизма и особенно эффективен в сочетании с методами балансировки активационной памяти. Эксперименты показывают значительное улучшение пропускной способности и снижение пикового использования памяти, особенно для моделей с большими словарями.'}, 'en': {'title': 'Balancing Memory and Computation in Pipeline Parallelism for Language Models', 'desc': 'This paper focuses on improving the efficiency of training large language models by addressing the imbalance caused by vocabulary layers in pipeline parallelism. The authors propose a method to evenly distribute vocabulary layers across different pipeline devices, which helps to balance computation and memory usage. They introduce algorithms to minimize communication delays within these layers and integrate their approach with existing pipeline schedules. The results show significant improvements in throughput and reduced memory usage, making the training process more efficient, especially for models with large vocabularies.'}, 'zh': {'title': '优化词汇层以平衡计算与内存', 'desc': '本文探讨了在训练大型语言模型时，词汇层导致的计算和内存不平衡问题。我们提出了一种方法，将词汇层均匀分配到管道设备上，并将计算分组到管道传递中。为了减少激活内存开销，我们设计了几种算法来降低词汇层内的通信障碍。通过结合这些技术，我们的方法在计算和参数内存之间实现了有效平衡，并在大词汇场景下显著降低了峰值内存使用。'}}}, {'id': 'https://huggingface.co/papers/2411.05738', 'title': 'StdGEN: Semantic-Decomposed 3D Character Generation from Single Images', 'url': 'https://huggingface.co/papers/2411.05738', 'abstract': 'We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io', 'score': 13, 'issue_id': 507, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': 'b23d3650ace21f86', 'data': {'categories': ['#3d', '#diffusion', '#games'], 'emoji': '🎭', 'ru': {'title': 'StdGEN: Революция в создании 3D-персонажей с семантическим разделением', 'desc': 'StdGEN - это инновационный конвейер для генерации семантически декомпозированных трехмерных персонажей высокого качества из одиночных изображений. В основе StdGEN лежит предложенная авторами Semantic-aware Large Reconstruction Model (S-LRM), трансформер-модель, которая реконструирует геометрию, цвет и семантику из многоракурсных изображений. Конвейер включает дифференцируемую схему извлечения многослойной семантической поверхности, специализированную эффективную многоракурсную диффузионную модель и модуль итеративного уточнения многослойной поверхности. Эксперименты показывают значительное превосходство StdGEN над существующими методами в генерации 3D-персонажей аниме по качеству геометрии, текстур и возможностям декомпозиции.'}, 'en': {'title': 'Revolutionizing 3D Character Generation with StdGEN', 'desc': 'StdGEN is a novel pipeline designed to create high-quality 3D characters from single images, focusing on semantic decomposition. It overcomes limitations of previous methods by generating detailed characters with distinct components like body, clothing, and hair in just three minutes. The core of StdGEN is the Semantic-aware Large Reconstruction Model (S-LRM), which uses a transformer architecture to reconstruct geometry, color, and semantics efficiently. With additional features like a multi-layer surface extraction and a diffusion model, StdGEN achieves superior performance in 3D character generation, particularly for anime, allowing for easy customization and broad application.'}, 'zh': {'title': 'StdGEN：高效生成可分解3D角色的创新管道', 'desc': 'StdGEN是一种创新的管道，能够从单张图像生成语义分解的高质量3D角色，广泛应用于虚拟现实、游戏和电影制作等领域。与以往方法相比，StdGEN在可分解性、有效性和效率上表现出色，能够在三分钟内生成细致的3D角色，且各个语义组件如身体、衣服和头发分离。其核心是语义感知的大型重建模型（S-LRM），该模型基于变换器，能够从多视图图像中联合重建几何、颜色和语义。通过引入可微分的多层语义表面提取方案，StdGEN实现了高质量、可分解的3D角色生成，实验结果显示其在3D动漫角色生成方面的性能超越了现有基准。'}}}, {'id': 'https://huggingface.co/papers/2411.02462', 'title': 'Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study', 'url': 'https://huggingface.co/papers/2411.02462', 'abstract': "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.", 'score': 9, 'issue_id': 508, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '38beaabd86eeaa88', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#plp'], 'emoji': '🧪', 'ru': {'title': 'Эффективная настройка языковых моделей для генерации модульных тестов', 'desc': 'Эта статья исследует применение методов эффективной настройки параметров (PEFT) для больших языковых моделей (LLM) в задаче генерации модульных тестов. Авторы сравнивают полную тонкую настройку с различными методами PEFT, включая LoRA, (IA)^3 и настройку промптов, на разных архитектурах и размерах моделей. Результаты показывают, что методы PEFT могут обеспечить производительность, сравнимую с полной тонкой настройкой, при этом значительно снижая вычислительные затраты. Особенно эффективной оказалась настройка промптов, а LoRA в некоторых случаях приближается к эффективности полной тонкой настройки.'}, 'en': {'title': 'Unlocking Cost-Effective Fine-Tuning for Unit Test Generation', 'desc': 'This paper explores the use of parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) in the context of unit test generation. It highlights the challenges of full fine-tuning, which can be costly and resource-intensive, especially as LLMs increase in size. The authors evaluate various PEFT techniques, such as LoRA and prompt tuning, to determine their effectiveness compared to full fine-tuning. The results indicate that PEFT methods can achieve performance similar to full fine-tuning, with prompt tuning being the most efficient option for resource utilization.'}, 'zh': {'title': '参数高效微调：提升单元测试生成的经济性与有效性', 'desc': '这篇论文探讨了大型语言模型（LLMs）在单元测试生成中的应用，特别是参数高效微调（PEFT）方法。传统的全量微调虽然有效，但成本高昂，PEFT方法通过只微调部分参数来降低计算开销。研究表明，PEFT方法在单元测试生成中能够达到与全量微调相当的性能，尤其是提示微调在成本和资源利用上最为有效。论文还比较了不同模型架构和大小下的多种PEFT方法，展示了其在特定任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.04425', 'title': 'DELIFT: Data Efficient Language model Instruction Fine Tuning', 'url': 'https://huggingface.co/papers/2411.04425', 'abstract': "Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.", 'score': 8, 'issue_id': 509, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '397d7c5c26dfad0f', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': '🔍', 'ru': {'title': 'Эффективный файн-тюнинг языковых моделей с меньшими данными', 'desc': 'Статья представляет новый алгоритм DELIFT для оптимизации выбора данных при файн-тюнинге больших языковых моделей (LLM). DELIFT использует попарную метрику полезности для оценки информативности образцов данных относительно текущих возможностей модели. Алгоритм эффективно работает на всех этапах файн-тюнинга: инструктирование, обучение специфичным задачам и непрерывное обучение. Эксперименты показали, что DELIFT может сократить объем данных для файн-тюнинга на 70% без ущерба для производительности.'}, 'en': {'title': 'Optimize Data, Maximize Performance with DELIFT!', 'desc': 'This paper presents DELIFT, a new algorithm designed to improve the fine-tuning process of large language models (LLMs) by optimizing data selection. DELIFT focuses on three stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning, making it more efficient than traditional methods. It uses a pairwise utility metric to evaluate the informational value of data samples, ensuring that only the most beneficial data is selected. The results show that DELIFT can significantly reduce the amount of data needed for fine-tuning by up to 70%, while maintaining or even enhancing model performance.'}, 'zh': {'title': '高效微调：DELIFT算法的创新之路', 'desc': '本论文提出了一种名为DELIFT的新算法，用于提高大型语言模型（LLMs）在特定任务上的性能，同时减少冗余和无效数据的使用。DELIFT通过优化数据选择，系统性地改进了三个关键的微调阶段：指令微调、任务特定微调和持续微调。该方法使用了一种成对效用度量，量化数据样本对模型响应其他样本的改善程度，从而有效评估信息价值。实验结果表明，DELIFT能够在不降低性能的情况下，将微调数据量减少多达70%，显著提高了计算效率。'}}}, {'id': 'https://huggingface.co/papers/2411.04954', 'title': 'CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM', 'url': 'https://huggingface.co/papers/2411.04954', 'abstract': "This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user's inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models' vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points. The project page and more visualizations can be found at: https://cad-mllm.github.io/", 'score': 7, 'issue_id': 518, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'f3ddd073293c6b26', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#multimodal', '#optimization', '#open_source'], 'emoji': '🏗️', 'ru': {'title': 'CAD-MLLM: Революция в генерации CAD-моделей с помощью мультимодального ИИ', 'desc': 'CAD-MLLM - это первая система, способная генерировать параметрические CAD-модели на основе мультимодальных входных данных, включая текст, изображения и облака точек. Система использует последовательности команд CAD-моделей и продвинутые большие языковые модели для согласования пространства признаков между различными модальностями и векторными представлениями CAD-моделей. Для обучения модели был создан набор данных Omni-CAD, содержащий около 450 тысяч CAD-моделей с соответствующими мультимодальными данными. Экспериментальные результаты показывают, что CAD-MLLM значительно превосходит существующие методы условной генерации и остается устойчивой к шумам и отсутствующим точкам.'}, 'en': {'title': 'Revolutionizing CAD Generation with Multimodal Inputs', 'desc': 'This paper presents CAD-MLLM, a novel system designed to generate Computer-Aided Design (CAD) models from various user inputs, including text, images, and point clouds. It utilizes large language models (LLMs) to effectively align different types of input data with the vectorized representations of CAD models. The authors introduce the Omni-CAD dataset, which is the first of its kind, containing around 450,000 instances of multimodal data paired with CAD construction sequences. The evaluation of the generated models includes new metrics for topology and surface quality, showing that CAD-MLLM outperforms existing methods and is resilient to data noise.'}, 'zh': {'title': '统一CAD生成系统：多模态输入的创新应用', 'desc': '本文旨在设计一个统一的计算机辅助设计（CAD）生成系统，能够根据用户的文本描述、图像、点云或其组合轻松生成CAD模型。我们介绍了CAD-MLLM，这是第一个能够基于多模态输入生成参数化CAD模型的系统。该框架利用CAD模型的命令序列，并采用先进的大型语言模型（LLMs）对不同模态数据和CAD模型的向量表示进行特征空间对齐。我们构建了一个名为Omni-CAD的综合数据集，包含文本描述、多视图图像、点和命令序列，约有45万个实例，评估结果显示CAD-MLLM在生成质量上显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2411.04097', 'title': 'RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models', 'url': 'https://huggingface.co/papers/2411.04097', 'abstract': 'Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RaVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RaVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.', 'score': 5, 'issue_id': 516, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '62101343e5b62784', 'data': {'categories': ['#optimization', '#multimodal', '#cv', '#training', '#healthcare', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Точное обнаружение и устранение ложных корреляций в мультимодальных моделях', 'desc': 'Эта статья представляет метод RaVL для обнаружения и смягчения ложных корреляций в мультимодальных моделях компьютерного зрения и языка (VLM). RaVL использует кластеризацию на уровне регионов изображения для выявления конкретных визуальных признаков, вызывающих ошибки классификации. Затем применяется новая функция потерь, учитывающая регионы, чтобы модель фокусировалась на релевантных областях при дообучении. Эксперименты на 654 VLM показали значительное улучшение в обнаружении и смягчении ложных корреляций по сравнению с базовыми методами.'}, 'en': {'title': 'Enhancing VLM Robustness by Targeting Spurious Correlations at the Local Level', 'desc': 'This paper introduces RaVL, a method designed to improve the robustness of fine-tuned vision-language models (VLMs) by addressing spurious correlations between image features and text attributes. Unlike existing methods that focus on global image-level features, RaVL operates on fine-grained local image features to identify and mitigate these correlations. It employs a region-level clustering approach to pinpoint specific image features that lead to classification errors in zero-shot scenarios. The results demonstrate that RaVL significantly enhances the discovery and mitigation of spurious correlations, leading to improved classification accuracy across various VLM architectures and domains.'}, 'zh': {'title': '提升视觉语言模型鲁棒性的RaVL方法', 'desc': '本文提出了一种名为RaVL的模型，旨在提高视觉语言模型（VLM）的鲁棒性。RaVL通过局部图像特征来发现和减轻虚假相关性，而不是仅在全局图像层面进行干预。该方法利用区域级聚类来识别导致零-shot分类错误的精确图像特征，并通过新的区域感知损失函数来减轻这些虚假相关性。实验结果表明，RaVL在发现和减轻虚假相关性方面均显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2411.04986', 'title': 'The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities', 'url': 'https://huggingface.co/papers/2411.04986', 'abstract': 'Modern language models can process inputs across diverse languages and modalities. We hypothesize that models acquire this capability through learning a shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. We term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through a transmodal semantic "hub" which integrates information from various modality-specific "spokes" regions. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the model\'s dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply a vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing.', 'score': 4, 'issue_id': 515, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '9a27aef11b630a04', 'data': {'categories': ['#multilingual', '#multimodal', '#transfer_learning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Единое семантическое пространство как ключ к мультимодальности ИИ', 'desc': 'Статья рассматривает гипотезу семантического хаба в современных языковых моделях. Исследователи предполагают, что модели обрабатывают разнородные входные данные через общее семантическое пространство представлений. Эксперименты показывают, что семантически эквивалентные входы на разных языках имеют схожие представления в промежуточных слоях модели. Это свойство распространяется на различные типы данных, включая арифметические выражения, код и аудиовизуальные входы.'}, 'en': {'title': 'Unlocking Multimodal Understanding: The Semantic Hub Hypothesis', 'desc': "This paper explores how modern language models can understand and process different languages and types of data by learning a shared representation space. The authors propose the 'semantic hub hypothesis', which suggests that these models organize information similarly to how the human brain integrates knowledge from various sources. They demonstrate that representations of semantically similar inputs, even from different languages or modalities, are closely aligned in the model's intermediate layers. Additionally, they show that changes in one type of data representation can influence outputs in other types, indicating that this shared space is actively used by the model rather than being a mere artifact of training."}, 'zh': {'title': '共享表示空间：跨模态理解的关键', 'desc': '现代语言模型能够处理多种语言和模态的输入。我们假设模型通过学习一个共享的表示空间来获得这一能力，这个空间将语义相似的输入放在一起，即使它们来自不同的模态或语言。我们称之为语义中心假设，类似于神经科学中的中心-辐射模型，认为人脑中的语义知识是通过一个跨模态的语义“中心”组织的。研究表明，不同语言中语义等价的输入在模型的中间层具有相似的表示，这种共享表示空间在处理输入时被模型积极利用。'}}}, {'id': 'https://huggingface.co/papers/2411.05457', 'title': 'Improving the detection of technical debt in Java source code with an enriched dataset', 'url': 'https://huggingface.co/papers/2411.05457', 'abstract': 'Technical debt (TD) is a term used to describe the additional work and costs that emerge when developers have opted for a quick and easy solution to a problem, rather than a more effective and well-designed, but time-consuming approach. Self-Admitted Technical Debts (SATDs) are a specific type of technical debts that developers intentionally document and acknowledge, typically via textual comments. While these self-admitted comments are a useful tool for identifying technical debts, most of the existing approaches focus on capturing crucial tokens associated with various categories of TD, neglecting the rich information embedded within the source code itself. Recent research has focused on detecting SATDs by analyzing comments embedded in source code, and there has been little work dealing with technical debts contained in the source code. To fill such a gap, in this study, through the analysis of comments and their associated source code from 974 Java projects hosted in the Stack corpus, we curated the first ever dataset of TD identified by code comments, coupled with its associated source code. Through an empirical evaluation, we found out that the comments of the resulting dataset help enhance the prediction performance of state-of-the-art SATD detection models. More importantly, including the classified source code significantly improves the accuracy in predicting various types of technical debt. In this respect, our work is two-fold: (i) We believe that our dataset will catalyze future work in the domain, inspiring various research issues related to the recognition of technical debt; (ii) The proposed classifiers may serve as baselines for other studies on the detection of TD by means of the curated dataset.', 'score': 2, 'issue_id': 510, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': 'bc9c84b19f317115', 'data': {'categories': ['#data', '#training', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Новый взгляд на технический долг: анализ кода и комментариев', 'desc': 'Эта статья посвящена исследованию технического долга (ТД) в разработке программного обеспечения. Авторы создали первый датасет, содержащий примеры ТД, идентифицированного в комментариях к коду, вместе с соответствующим исходным кодом. Исследование показало, что включение классифицированного исходного кода значительно улучшает точность предсказания различных типов технического долга. Работа предлагает новый подход к обнаружению ТД, который может служить основой для будущих исследований в этой области.'}, 'en': {'title': 'Bridging Comments and Code: Enhancing Technical Debt Detection', 'desc': 'This paper addresses the issue of Technical Debt (TD) in software development, particularly focusing on Self-Admitted Technical Debts (SATDs) documented by developers. It highlights the limitations of existing methods that primarily analyze comment tokens, overlooking the valuable information within the source code itself. The authors present a novel dataset created from 974 Java projects, linking SATD comments to their corresponding source code, which enhances the detection of technical debts. Their empirical evaluation demonstrates that incorporating both comments and source code significantly improves the performance of SATD detection models, paving the way for future research in this area.'}, 'zh': {'title': '技术债务识别的新视角', 'desc': '技术债务（TD）是指开发者为了快速解决问题而选择的简单方案所带来的额外工作和成本。自我承认的技术债务（SATD）是开发者通过文本注释主动记录和承认的一种特定类型的技术债务。本文通过分析来自974个Java项目的注释和相关源代码，创建了首个由代码注释识别的技术债务数据集，并发现这些注释能显著提高现有SATD检测模型的预测性能。我们的研究不仅为技术债务的识别提供了新的数据集，还提出了可作为基线的分类器，推动未来相关研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.04905', 'title': 'OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models', 'url': 'https://huggingface.co/papers/2411.04905', 'abstract': "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems.While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an ``open cookbook'' for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.", 'score': 85, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '799dedd6597ce7ab', 'data': {'categories': ['#dataset', '#data', '#training', '#synthetic', '#agents', '#plp'], 'emoji': '🧑\u200d💻', 'ru': {'title': 'OpenCoder: открытая книга рецептов для создания топовых языковых моделей кода', 'desc': 'OpenCoder - это высококачественная языковая модель для работы с кодом, сопоставимая по производительности с ведущими моделями. Авторы не только предоставляют веса модели и код для инференса, но и полный набор воспроизводимых данных для обучения, конвейер обработки данных и подробные протоколы обучения. Ключевыми ингредиентами для создания модели такого уровня являются оптимизированные эвристические правила очистки данных, методы дедупликации, использование текстового корпуса, связанного с кодом, и высококачественные синтетические данные. Эта открытость призвана ускорить исследования и обеспечить воспроизводимый прогресс в области ИИ для работы с кодом.'}, 'en': {'title': 'OpenCoder: Unlocking Code AI with Transparency and Reproducibility', 'desc': 'This paper introduces OpenCoder, a high-performance large language model (LLM) specifically designed for code generation and reasoning tasks. It addresses the lack of open-access models that provide reproducible data processing and transparent training protocols, which are essential for scientific research. OpenCoder not only matches the performance of proprietary models but also shares its model weights, inference code, and detailed training methodologies. By emphasizing data cleaning, corpus recall, and synthetic data generation, OpenCoder aims to enhance accessibility and foster reproducible advancements in the field of code AI.'}, 'zh': {'title': 'OpenCoder：开放的顶级代码大语言模型', 'desc': '本文介绍了OpenCoder，一个高质量的代码大语言模型（LLM），旨在为科学研究提供开放的资源。与其他模型不同，OpenCoder不仅提供模型权重和推理代码，还包括可重复的训练数据和完整的数据处理流程。我们识别出构建顶级代码LLM的关键要素，包括数据清洗的启发式规则、与代码相关的文本语料库的回忆以及高质量的合成数据。通过这种开放性，我们希望加速代码人工智能的研究和可重复的进展。'}}}, {'id': 'https://huggingface.co/papers/2411.05003', 'title': 'ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning', 'url': 'https://huggingface.co/papers/2411.05003', 'abstract': 'Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique.', 'score': 59, 'issue_id': 464, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'f71f2e0f1addbe57', 'data': {'categories': ['#video', '#diffusion', '#hallucinations'], 'emoji': '🎥', 'ru': {'title': 'Переснимаем реальность: новые ракурсы для любого видео', 'desc': 'ReCapture - это метод генерации новых видео с измененными траекториями камеры на основе одного пользовательского видео. Он позволяет перегенерировать исходное видео с другими углами обзора и кинематографическим движением камеры, включая правдоподобное восстановление невидимых частей сцены. Метод работает в два этапа: сначала создается шумное опорное видео с новой траекторией камеры, а затем оно очищается и делается временно согласованным. ReCapture использует мультивидовые диффузионные модели и рендеринг облака точек на основе глубины.'}, 'en': {'title': 'ReCapture: Transforming User Videos with New Camera Perspectives', 'desc': 'This paper introduces ReCapture, a novel method for generating new videos with different camera angles from a single user-provided video. It leverages multiview diffusion models and depth-based point cloud rendering to create an initial noisy video with a new camera trajectory. The method then refines this video using a masked video fine-tuning technique to ensure temporal consistency and clarity. Additionally, ReCapture can convincingly generate parts of the scene that were not visible in the original video, enhancing the overall viewing experience.'}, 'zh': {'title': 'ReCapture：从用户视频生成新视角的魔法', 'desc': '最近在视频建模方面取得了突破，使得生成视频中的相机轨迹可控。然而，这些方法无法直接应用于用户提供的非生成视频。本文提出了一种名为ReCapture的方法，可以从单个用户提供的视频生成具有新相机轨迹的新视频。该方法不仅能够从不同角度重新生成参考视频，还能合理地幻觉出参考视频中不可见的场景部分。'}}}, {'id': 'https://huggingface.co/papers/2411.04965', 'title': 'BitNet a4.8: 4-bit Activations for 1-bit LLMs', 'url': 'https://huggingface.co/papers/2411.04965', 'abstract': 'Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.', 'score': 54, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'dcfd440f9caf6714', 'data': {'categories': ['#inference', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'BitNet a4.8: Эффективность и производительность в мире языковых моделей', 'desc': 'Статья представляет BitNet a4.8 - усовершенствованную версию 1-битной языковой модели. Эта модель использует 4-битные активации для входных данных в слоях внимания и прямой связи, а также применяет разреживание и 8-битное квантование для промежуточных состояний. Эксперименты показывают, что BitNet a4.8 достигает производительности, сравнимой с BitNet b1.58, при эквивалентных затратах на обучение, но с более быстрым выводом. Модель активирует только 55% параметров и поддерживает 3-битный KV-кэш, повышая эффективность развертывания и вывода крупномасштабных языковых моделей.'}, 'en': {'title': 'Efficient Inference with 4-bit Activations in LLMs', 'desc': 'This paper introduces BitNet a4.8, a new model that enhances the efficiency of 1-bit Large Language Models (LLMs) by using 4-bit activations. It combines hybrid quantization and sparsification techniques to reduce errors from outlier channels, allowing for better performance. The model uses 4-bit activations in key layers while applying 8-bit quantization to intermediate states, resulting in faster inference times. Experiments show that BitNet a4.8 matches the performance of its predecessor, BitNet b1.58, while being more efficient in terms of parameter activation and supporting a 3-bit key-value cache.'}, 'zh': {'title': '提升大型语言模型推理效率的新方法', 'desc': '最近关于1位大型语言模型（LLMs）的研究，如BitNet b1.58，展示了在保持性能的同时降低推理成本的前景。本文介绍了BitNet a4.8，支持1位LLMs的4位激活。BitNet a4.8采用混合量化和稀疏化策略，以减轻由异常通道引入的量化误差。实验表明，BitNet a4.8在训练成本相当的情况下，推理速度更快，且仅激活55%的参数，支持3位KV缓存，进一步提高了大规模LLM的部署和推理效率。'}}}, {'id': 'https://huggingface.co/papers/2411.04996', 'title': 'Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models', 'url': 'https://huggingface.co/papers/2411.04996', 'abstract': "The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. We evaluate MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8\\% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2\\% of the wall-clock time and text quality in 75.6\\% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs).", 'score': 35, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '53d29fd65eda072e', 'data': {'categories': ['#architecture', '#multimodal', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективные мультимодальные трансформеры: меньше вычислений, та же мощность', 'desc': 'Эта статья представляет Mixture-of-Transformers (MoT) - новую архитектуру для мультимодальных языковых моделей, которая значительно снижает вычислительные затраты при предобучении. MoT разделяет параметры модели по модальностям, позволяя эффективно обрабатывать текст, изображения и речь в единой системе. Эксперименты показывают, что MoT достигает производительности плотных базовых моделей, используя значительно меньше вычислительных ресурсов. Это позволяет создавать более эффективные мультимодальные системы искусственного интеллекта.'}, 'en': {'title': 'Efficient Multi-Modal Processing with Mixture-of-Transformers', 'desc': 'This paper presents Mixture-of-Transformers (MoT), a novel sparse multi-modal transformer architecture designed to efficiently handle text, images, and speech. By decoupling non-embedding parameters by modality, MoT allows for specialized processing while maintaining global self-attention across the entire input. The architecture significantly reduces computational costs, achieving comparable performance to dense models with fewer floating point operations (FLOPs). Evaluations show that MoT not only matches but often outperforms dense baselines in various settings, demonstrating its effectiveness in multi-modal tasks.'}, 'zh': {'title': '混合变换器：高效的多模态学习新方案', 'desc': '本论文介绍了一种新的稀疏多模态变换器架构，称为混合变换器（MoT），旨在降低大语言模型的预训练计算成本。MoT通过模态解耦模型的非嵌入参数，使得不同模态（如文本、图像和语音）可以进行特定处理，同时保持全局自注意力机制。实验结果表明，MoT在多个设置下表现出色，能够以更少的计算资源达到与密集基线相当的性能。该模型在图像生成和语音处理任务中均展现了显著的效率优势，证明了其在多模态学习中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2411.04928', 'title': 'DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion', 'url': 'https://huggingface.co/papers/2411.04928', 'abstract': 'In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively represented through sequences of video frames. While recent video diffusion models have shown remarkable success in producing vivid visuals, they face limitations in directly recovering 3D/4D scenes due to limited spatial and temporal controllability during generation. To overcome this, we propose ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant data. This controllable video diffusion approach enables precise manipulation of spatial structure and temporal dynamics, allowing us to reconstruct both 3D and 4D representations from sequential frames with the combination of spatial and temporal dimensions. Additionally, to bridge the gap between generated videos and real-world scenes, we introduce a trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation. Extensive experiments on various real-world and synthetic datasets demonstrate that DimensionX achieves superior results in controllable video generation, as well as in 3D and 4D scene generation, compared with previous methods.', 'score': 34, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'b0958f934bc56a95', 'data': {'categories': ['#3d', '#video', '#synthetic'], 'emoji': '🎭', 'ru': {'title': 'От 2D к 4D: DimensionX раздвигает границы генеративных моделей', 'desc': 'DimensionX - это фреймворк для генерации фотореалистичных 3D и 4D сцен из одного изображения с помощью видео-диффузии. Ключевым компонентом является ST-Director, который разделяет пространственные и временные факторы в видео-диффузии, обучая размерно-зависимые LoRA на данных с различными измерениями. Для 3D генерации используется механизм, учитывающий траекторию, а для 4D - стратегия шумоподавления, сохраняющая идентичность. Эксперименты показывают превосходство DimensionX в контролируемой генерации видео и создании 3D/4D сцен по сравнению с существующими методами.'}, 'en': {'title': 'Transforming Single Images into Stunning 3D and 4D Worlds!', 'desc': 'DimensionX is a novel framework that generates photorealistic 3D and 4D scenes from a single image using video diffusion techniques. It addresses the limitations of existing video diffusion models by introducing ST-Director, which separates spatial and temporal factors, allowing for better control over the generated scenes. By learning dimension-aware Low-Rank Adaptations (LoRAs) from diverse data, DimensionX enhances the manipulation of both spatial structures and temporal dynamics. The framework also incorporates a trajectory-aware mechanism and an identity-preserving denoising strategy to improve the realism of generated scenes, outperforming previous methods in extensive experiments.'}, 'zh': {'title': 'DimensionX：从单图像生成真实3D和4D场景的创新框架', 'desc': '本文介绍了DimensionX，一个框架，旨在通过单张图像和视频扩散生成逼真的3D和4D场景。我们的方法利用视频帧序列有效表示3D场景的空间结构和4D场景的时间演变。为了解决现有视频扩散模型在生成过程中空间和时间可控性不足的问题，我们提出了ST-Director，通过从维度变化的数据中学习维度感知的LoRA，解耦空间和时间因素。实验结果表明，DimensionX在可控视频生成以及3D和4D场景生成方面优于以往的方法。'}}}, {'id': 'https://huggingface.co/papers/2411.04709', 'title': 'TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2411.04709', 'abstract': 'Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io.', 'score': 21, 'issue_id': 464, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': 'fcc8e4daf79a82b9', 'data': {'categories': ['#dataset', '#video'], 'emoji': '🎬', 'ru': {'title': 'TIP-I2V: Революция в изучении промптов для генерации видео из изображений', 'desc': 'Исследователи представили TIP-I2V - первый крупномасштабный набор данных, содержащий более 1,70 миллиона уникальных пользовательских текстовых и изображений-промптов для генерации видео из изображений. Датасет также включает соответствующие сгенерированные видео от пяти современных моделей преобразования изображений в видео. TIP-I2V позволяет анализировать предпочтения пользователей, оценивать многомерную производительность обученных моделей и решать проблемы безопасности, связанные с дезинформацией. Этот набор данных подчеркивает важность специализированного датасета промптов для генерации видео из изображений и открывает новые возможности для исследований в этой области.'}, 'en': {'title': 'Empowering Image-to-Video Generation with TIP-I2V Dataset', 'desc': 'This paper presents TIP-I2V, the first large-scale dataset containing over 1.70 million unique user-provided text and image prompts for image-to-video generation. The dataset aims to enhance the controllability and visual consistency of video generation models by providing a rich source of prompts. It also includes generated videos from five advanced image-to-video models, facilitating comparative analysis and model evaluation. By addressing the lack of dedicated datasets, TIP-I2V supports research in user preferences and model safety, particularly in mitigating misinformation issues.'}, 'zh': {'title': 'TIP-I2V：图像到视频生成的新数据集', 'desc': '视频生成模型正在改变内容创作，图像到视频模型因其更好的可控性和视觉一致性而受到关注。尽管这些模型很受欢迎，但目前缺乏专门用于研究用户提供的文本和图像提示的数据集。本文介绍了TIP-I2V，这是第一个大规模的数据集，包含超过170万个独特的用户提供的文本和图像提示，专门用于图像到视频生成。该数据集的推出将推动图像到视频研究的进展，帮助研究人员分析用户偏好并评估模型性能。'}}}, {'id': 'https://huggingface.co/papers/2411.04952', 'title': 'M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding', 'url': 'https://huggingface.co/papers/2411.04952', 'abstract': 'Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images.', 'score': 20, 'issue_id': 477, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '56e0d2f2775dbda9', 'data': {'categories': ['#benchmark', '#rag', '#dataset', '#multimodal', '#games', '#long_context'], 'emoji': '📄', 'ru': {'title': 'M3DocRAG: Мультимодальные ответы на вопросы по документам нового поколения', 'desc': 'Статья представляет M3DocRAG - новую мультимодальную систему для ответов на вопросы по документам. Она использует мультимодальный ретривер и мультимодальную языковую модель для эффективной обработки одного или нескольких документов с сохранением визуальной информации. M3DocRAG способна работать с различными контекстами документов, типами вопросов и модальностями доказательств. Авторы также представляют новый бенчмарк M3DocVQA для оценки систем открытого домена на более чем 3000 PDF-документах.'}, 'en': {'title': 'M3DocRAG: Revolutionizing Document Visual Question Answering', 'desc': 'The paper presents M3DocRAG, a new framework for Document Visual Question Answering (DocVQA) that addresses limitations of existing methods. Unlike traditional approaches that focus on single-page documents and often overlook visual elements, M3DocRAG can process multiple pages and various document types while retaining important visual information. It utilizes a multi-modal retriever and a multi-modal language model (MLM) to efficiently find relevant documents and answer questions, accommodating both text and visual evidence. The authors also introduce M3DocVQA, a benchmark for evaluating open-domain DocVQA, demonstrating that their framework outperforms existing models in several benchmarks.'}, 'zh': {'title': 'M3DocRAG：跨页文档问答的新突破', 'desc': '本文介绍了一种新的多模态检索增强生成框架M3DocRAG，旨在解决文档视觉问答（DocVQA）中的挑战。现有方法主要处理单页文档，无法有效应对跨页或多文档的信息检索。M3DocRAG能够灵活处理不同的文档上下文和问题跳跃，同时保留重要的视觉信息，如图表和图像。通过在超过3000个PDF文档上进行评估，M3DocRAG在多个基准测试中表现优异，超越了许多强基线。'}}}, {'id': 'https://huggingface.co/papers/2411.04923', 'title': 'VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos', 'url': 'https://huggingface.co/papers/2411.04923', 'abstract': 'Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed for fine-grained pixel-level grounding in videos based on user-provided textual inputs. Our design seamlessly connects three key components: a Large Language Model, a dual vision encoder that emphasizes both spatial and temporal details, and a spatio-temporal decoder for accurate mask generation. This connection is facilitated via tunable V-L and L-V adapters that enable close Vision-Language (VL) alignment. The architecture is trained to synchronize both spatial and temporal elements of video content with textual instructions. To enable fine-grained grounding, we curate a multimodal dataset featuring detailed visually-grounded conversations using a semiautomatic annotation pipeline, resulting in a diverse set of 38k video-QA triplets along with 83k objects and 671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation. Experimental results show that our model consistently outperforms existing approaches across all three tasks.', 'score': 17, 'issue_id': 474, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '3db2b6994e9c5047', 'data': {'categories': ['#dataset', '#multimodal', '#architecture', '#games', '#alignment', '#cv'], 'emoji': '🎥', 'ru': {'title': 'Точная локализация в видео с помощью мультимодального ИИ', 'desc': 'VideoGLaMM - это новая модель крупномасштабного мультимодального обучения, разработанная для точной пиксельной локализации в видео на основе текстовых запросов пользователя. Модель объединяет языковую модель, двойной энкодер зрения и пространственно-временной декодер для генерации масок. Для обучения модели был создан специальный набор данных с детальными визуально-обоснованными диалогами. Эксперименты показали превосходство VideoGLaMM над существующими подходами в задачах генерации обоснованных диалогов, визуальной локализации и сегментации видео по запросу.'}, 'en': {'title': 'Achieving Pixel-Level Precision in Video-Text Alignment', 'desc': 'This paper presents VideoGLaMM, a Large Multimodal Model (LMM) that enhances the alignment between videos and text at a fine-grained level. It addresses the challenges of pixel-level grounding by integrating a Large Language Model with a dual vision encoder that captures both spatial and temporal dynamics of video content. The model employs tunable adapters for effective Vision-Language alignment and is trained on a comprehensive multimodal dataset with 38k video-QA triplets. Experimental results demonstrate that VideoGLaMM outperforms existing models in tasks such as Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation.'}, 'zh': {'title': '视频与文本的精细对齐新突破', 'desc': '本论文介绍了一种名为VideoGLaMM的多模态模型，旨在实现视频与文本之间的精细像素级对齐。该模型结合了大型语言模型、双重视觉编码器和时空解码器，能够处理视频中的复杂空间和时间动态。通过可调的视觉-语言适配器，VideoGLaMM实现了视觉与语言的紧密对齐。实验结果表明，该模型在生成对话、视觉对齐和视频分割等任务上均优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2411.04496', 'title': 'Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model', 'url': 'https://huggingface.co/papers/2411.04496', 'abstract': 'To increase social bonding with interlocutors, humans naturally acquire the ability to respond appropriately in a given situation by considering which conversational skill is most suitable for the response - a process we call skill-of-mind. For large language model (LLM)-based conversational agents, planning appropriate conversational skills, as humans do, is challenging due to the complexity of social dialogue, especially in interactive scenarios. To address this, we propose a skill-of-mind-annotated conversation dataset, named Multifaceted Skill-of-Mind, which includes multi-turn and multifaceted conversational skills across various interactive scenarios (e.g., long-term, counseling, task-oriented), grounded in diverse social contexts (e.g., demographics, persona, rules of thumb). This dataset consists of roughly 100K conversations. Using this dataset, we introduce a new family of skill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B parameters. With extensive experiments, these models successfully demonstrate the skill-of-mind process and exhibit strong generalizability in inferring multifaceted skills across a variety of domains. Moreover, we show that Thanos significantly enhances the quality of responses generated by LLM-based conversational agents and promotes prosocial behavior in human evaluations.', 'score': 17, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'bf7486353434568f', 'data': {'categories': ['#dataset', '#agents', '#multimodal'], 'emoji': '🗣️', 'ru': {'title': 'Языковые модели учатся искусству общения', 'desc': 'Исследователи представили новый набор данных под названием Multifaceted Skill-of-Mind, содержащий аннотации навыков ведения диалога в различных интерактивных сценариях. На основе этого набора данных была разработана серия языковых моделей Thanos, способных выбирать подходящие навыки общения в зависимости от контекста. Эксперименты показали, что модели Thanos успешно демонстрируют процесс выбора навыков и обладают хорошей обобщающей способностью в различных областях. Кроме того, использование Thanos значительно улучшило качество ответов диалоговых агентов на основе больших языковых моделей и способствовало более просоциальному поведению по оценкам людей.'}, 'en': {'title': 'Enhancing Conversational Skills in AI with Thanos', 'desc': 'This paper introduces a new dataset called Multifaceted Skill-of-Mind, which helps large language models (LLMs) learn how to respond appropriately in social conversations. The dataset contains around 100,000 conversations that cover various interactive scenarios and social contexts, allowing LLMs to understand different conversational skills. The authors also present a new family of LLMs named Thanos, which are designed to incorporate these skills into their responses. Through experiments, Thanos models show improved response quality and promote positive social interactions in conversations.'}, 'zh': {'title': '提升对话质量的技能思维模型', 'desc': '本文提出了一种名为多面技能思维的对话数据集，旨在帮助大型语言模型（LLM）更好地理解和应用社交对话中的适当回应技能。该数据集包含约10万条对话，涵盖了多轮和多方面的对话技能，适用于不同的互动场景，如长期对话、咨询和任务导向。我们还介绍了一种新的LLM家族，名为Thanos，具有1B、3B和8B参数规模，能够有效地展示技能思维过程，并在多种领域中推断多面技能。实验结果表明，Thanos显著提高了LLM对话代理生成的回应质量，并在人工评估中促进了亲社会行为。'}}}, {'id': 'https://huggingface.co/papers/2411.05000', 'title': 'Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?', 'url': 'https://huggingface.co/papers/2411.05000', 'abstract': 'As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data.', 'score': 16, 'issue_id': 474, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '72ef4dc00d41e203', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#long_context'], 'emoji': '🧵', 'ru': {'title': 'Языковые модели в лабиринте длинного контекста: нити информации и границы понимания', 'desc': 'Исследование посвящено анализу способностей современных языковых моделей (LLM) эффективно использовать увеличенный контекст. Авторы провели серию экспериментов по извлечению информации, оценивая 17 ведущих LLM на способность следовать нескольким информационным потокам в контексте. Результаты показали, что многие модели способны эффективно обрабатывать несколько потоков одновременно, но для некоторых моделей эффективная длина контекста оказалась значительно меньше заявленной. Исследование также подчеркивает важность учета различий в токенизации при сравнении моделей.'}, 'en': {'title': 'Unlocking the Potential of Long-Context LLMs', 'desc': 'This paper investigates how well large language models (LLMs) can utilize their extended context capabilities for complex information retrieval and reasoning tasks. The authors conduct experiments with 17 leading LLMs to assess their ability to track multiple threads of information within their context windows. They discover that while many models can maintain performance across various threads, the effective context limit is often shorter than the maximum supported length, leading to decreased accuracy with larger contexts. Additionally, the study emphasizes the need for caution when comparing token counts from different tokenizers, as they can represent different amounts of text.'}, 'zh': {'title': '长上下文模型的潜力与挑战', 'desc': '随着大型语言模型（LLMs）上下文限制的增加，应用范围和下游功能也在扩大。许多现实任务的决策依赖于分散在不同文档中的细节，这些文档通常包含大量无关信息。长上下文LLMs在复杂信息检索和推理方面表现出色，但我们对它们如何有效利用上下文的理解仍然滞后。我们的实验表明，尽管许多模型在跟踪信息线程方面表现良好，但有效的上下文限制往往比支持的上下文长度要短，且随着上下文窗口的增大，准确性会下降。'}}}, {'id': 'https://huggingface.co/papers/2411.05001', 'title': 'Analyzing The Language of Visual Tokens', 'url': 'https://huggingface.co/papers/2411.05001', 'abstract': 'With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models.', 'score': 14, 'issue_id': 482, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '75768d92bd8ce17a', 'data': {'categories': ['#multimodal', '#cv', '#interpretability', '#alignment'], 'emoji': '🖼️', 'ru': {'title': 'Визуальные языки: между Ципфом и хаосом', 'desc': 'Статья исследует статистические свойства дискретных визуальных языков, используемых в современных моделях компьютерного зрения. Авторы обнаружили, что визуальные языки следуют распределению Ципфа, но имеют более высокую энтропию и меньшую сжимаемость по сравнению с естественными языками. Визуальные токены в основном представляют части объектов, а не целые объекты или сцены. Исследование также показало, что визуальным языкам не хватает связной грамматической структуры и иерархической организации, характерной для естественных языков.'}, 'en': {'title': 'Unlocking the Secrets of Visual Languages in AI', 'desc': 'This paper explores the statistical properties of discrete visual languages used in transformer-based models for vision and language tasks. It reveals that these visual languages exhibit Zipfian distributions, but their token innovation leads to higher entropy and lower compression, primarily representing object parts. The study finds that visual languages lack cohesive grammatical structures, resulting in higher perplexity and less hierarchical organization compared to natural languages. Ultimately, the research highlights the importance of understanding these properties to improve the design of computer vision models.'}, 'zh': {'title': '揭示视觉语言的统计特性', 'desc': '本文探讨了基于变换器的视觉和语言任务模型（如LLaVA和Chameleon）中图像的离散标记表示。研究发现，视觉语言虽然遵循Zipf分布，但更高的标记创新会导致更大的熵和更低的压缩率，标记主要代表物体部分，显示出中间粒度。与自然语言相比，视觉语言缺乏连贯的语法结构，导致更高的困惑度和较弱的层次组织。通过这些实验，我们揭示了离散视觉语言的统计特性如何影响计算机视觉模型的设计。'}}}, {'id': 'https://huggingface.co/papers/2411.05007', 'title': 'SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models', 'url': 'https://huggingface.co/papers/2411.05007', 'abstract': 'Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, na\\"{\\i}vely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-Sigma, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5times, achieving 3.0times speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs. Our quantization library and inference engine are open-sourced.', 'score': 14, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '636ee9cbe15eefb6', 'data': {'categories': ['#inference', '#optimization', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных моделей с помощью 4-битного квантования', 'desc': 'Статья представляет новый метод квантования под названием SVDQuant для ускорения диффузионных моделей. Метод использует низкоранговую ветвь для поглощения выбросов в весах и активациях, позволяя эффективно квантовать их до 4 бит. Авторы также разработали движок вывода Nunchaku, который оптимизирует выполнение квантованных моделей. Эксперименты показали значительное снижение использования памяти и ускорение работы крупных моделей генерации изображений.'}, 'en': {'title': 'Accelerating Diffusion Models with SVDQuant: Efficient 4-Bit Quantization', 'desc': 'This paper presents SVDQuant, a novel 4-bit quantization method designed to enhance the efficiency of diffusion models for image generation. As these models increase in size, they face challenges related to memory usage and latency, which SVDQuant aims to address by effectively managing outliers in weights and activations. The method utilizes Singular Value Decomposition (SVD) to absorb outliers into a low-rank branch, improving the quantization process without compromising image quality. Additionally, the co-designed inference engine, Nunchaku, optimizes memory access, resulting in significant reductions in memory usage and increased processing speed for large models.'}, 'zh': {'title': '加速扩散模型的4位量化新方法', 'desc': '扩散模型在生成高质量图像方面非常有效，但随着模型规模的增大，它们需要更多的内存并且延迟更高，这给部署带来了挑战。本文提出了一种新的4位量化方法SVDQuant，通过量化权重和激活值来加速扩散模型。与传统的平滑方法不同，SVDQuant通过低秩分支吸收异常值，从而减轻量化过程中的困难。实验结果表明，SVDQuant在保持图像质量的同时，显著减少了内存使用和提高了速度，适用于更多互动应用。'}}}, {'id': 'https://huggingface.co/papers/2411.04999', 'title': 'DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation', 'url': 'https://huggingface.co/papers/2411.04999', 'abstract': "Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems. Our code as well as our experiment and deployment videos are open sourced and can be found on our project website: https://dynamem.github.io/", 'score': 14, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '47171ef52d95552a', 'data': {'categories': ['#robotics', '#3d', '#multimodal', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Динамическая память для роботов в изменяющемся мире', 'desc': 'DynaMem - новый подход к мобильной манипуляции с открытым словарем в динамических средах. Система использует динамическую пространственно-семантическую память для представления окружения робота, построенную на основе 3D структуры данных облаков точек. DynaMem применяет мультимодальные языковые модели и модели компьютерного зрения для локализации объектов по запросам на естественном языке. Эксперименты показали двукратное улучшение успешности захвата и перемещения нестационарных объектов по сравнению с современными статическими системами.'}, 'en': {'title': 'Empowering Robots with Dynamic Memory for Open-World Manipulation', 'desc': "This paper introduces DynaMem, a novel approach for open-vocabulary mobile manipulation that allows robots to adapt to dynamic environments. Unlike traditional systems that rely on static environments, DynaMem utilizes a dynamic spatio-semantic memory to keep track of changes in the robot's surroundings. It employs a 3D data structure to manage point clouds and leverages multimodal large language models (LLMs) for object localization. The results show that DynaMem significantly improves the robot's ability to interact with non-stationary objects, achieving a 70% success rate in pick-and-drop tasks, which is more than double the performance of existing static systems."}, 'zh': {'title': '动态记忆，智能操作！', 'desc': '本研究提出了一种新的动态空间语义记忆方法DynaMem，用于开放词汇的移动操作。与传统静态环境系统不同，DynaMem能够在不断变化的环境中进行物体定位和操作。该方法利用三维数据结构维护动态记忆，并通过多模态大语言模型进行查询。实验结果表明，DynaMem在非静态物体的抓取和放置任务中成功率达70%，显著优于现有静态系统。'}}}, {'id': 'https://huggingface.co/papers/2411.04335', 'title': 'GazeGen: Gaze-Driven User Interaction for Visual Content Generation', 'url': 'https://huggingface.co/papers/2411.04335', 'abstract': "We present GazeGen, a user interaction system that generates visual content (images and videos) for locations indicated by the user's eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and generative AI, GazeGen performs gaze-controlled image adding/deleting, repositioning, and surface material changes of image objects, and converts static images into videos. Central to GazeGen is the DFT Gaze (Distilled and Fine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters, performing accurate real-time gaze predictions tailored to individual users' eyes on small edge devices. GazeGen is the first system to combine visual content generation with real-time gaze estimation, made possible exclusively by DFT Gaze. This real-time gaze estimation enables various visual content generation tasks, all controlled by the user's gaze. The input for DFT Gaze is the user's eye images, while the inputs for visual content generation are the user's view and the predicted gaze point from DFT Gaze. To achieve efficient gaze predictions, we derive the small model from a large model (10x larger) via novel knowledge distillation and personal adaptation techniques. We integrate knowledge distillation with a masked autoencoder, developing a compact yet powerful gaze estimation model. This model is further fine-tuned with Adapters, enabling highly accurate and personalized gaze predictions with minimal user input. DFT Gaze ensures low-latency and precise gaze tracking, supporting a wide range of gaze-driven tasks. We validate the performance of DFT Gaze on AEA and OpenEDS2020 benchmarks, demonstrating low angular gaze error and low latency on the edge device (Raspberry Pi 4). Furthermore, we describe applications of GazeGen, illustrating its versatility and effectiveness in various usage scenarios.", 'score': 13, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'caa85d9f2385fc2a', 'data': {'categories': ['#agents', '#cv', '#video', '#edge_computing', '#training'], 'emoji': '👁️', 'ru': {'title': 'Взглядом управляй: революция в генерации визуального контента', 'desc': 'Исследователи представляют систему GazeGen, которая генерирует визуальный контент на основе направления взгляда пользователя. В основе системы лежит сверхлегкая модель DFT Gaze, выполняющая точное предсказание направления взгляда в реальном времени на небольших устройствах. GazeGen позволяет интуитивно манипулировать визуальным контентом, используя передовые методы обнаружения объектов и генеративного ИИ. Система была протестирована на эталонных наборах данных, продемонстрировав низкую угловую ошибку определения взгляда и низкую задержку на периферийных устройствах.'}, 'en': {'title': 'GazeGen: Transforming Eye Gaze into Visual Content Control', 'desc': 'GazeGen is an innovative user interaction system that generates visual content based on where a user is looking. It utilizes advanced object detection and generative AI techniques to allow users to manipulate images and videos by simply gazing at specific areas. The core of GazeGen is the DFT Gaze agent, a lightweight model that accurately predicts gaze in real-time, making it suitable for use on small devices like the Raspberry Pi 4. By combining gaze estimation with visual content generation, GazeGen enables intuitive and personalized interactions with digital media.'}, 'zh': {'title': '眼动控制的视觉内容生成系统', 'desc': 'GazeGen是一个用户交互系统，可以根据用户的眼动生成视觉内容（图像和视频）。它利用先进的物体检测和生成AI技术，实现了基于视线的图像添加、删除、重新定位和表面材质变化。系统的核心是DFT Gaze代理，这是一个轻量级模型，能够在小型边缘设备上进行实时的眼动预测。GazeGen是首个将视觉内容生成与实时眼动估计相结合的系统，展示了其在多种应用场景中的灵活性和有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.05005', 'title': 'Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models', 'url': 'https://huggingface.co/papers/2411.05005', 'abstract': 'Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or as mere feature extractors. In contrast to these isolated and thus sub-optimal efforts, we introduce a unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through a unique exploitation of the diffusion-denoising process. Within this framework, we further enhance discriminative visual perception via multi-modal generation, by utilizing the denoising network to create multi-modal data that mirror the distribution of the original training set. Importantly, Diff-2-in-1 optimizes the utilization of the created diverse and faithful data by leveraging a novel self-improving learning mechanism. Comprehensive experimental evaluations validate the effectiveness of our framework, showcasing consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation characterized by both realism and usefulness.', 'score': 12, 'issue_id': 477, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '249deab8440380fc', 'data': {'categories': ['#training', '#diffusion', '#data', '#multimodal', '#optimization', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Diff-2-in-1: Объединение генерации и восприятия в диффузионных моделях', 'desc': 'Статья представляет новый универсальный фреймворк на основе диффузионных моделей под названием Diff-2-in-1. Этот фреймворк способен одновременно выполнять мультимодальную генерацию данных и плотное визуальное восприятие, используя процесс диффузионного шумоподавления. Diff-2-in-1 улучшает дискриминативное визуальное восприятие путем генерации мультимодальных данных, отражающих распределение исходного обучающего набора. Фреймворк также использует новый механизм самосовершенствующегося обучения для оптимизации использования созданных разнообразных и достоверных данных.'}, 'en': {'title': 'Unifying Data Generation and Visual Perception with Diff-2-in-1', 'desc': 'This paper presents a new framework called Diff-2-in-1 that integrates diffusion models for both data generation and visual perception tasks. Unlike previous approaches that used diffusion models in isolation, this framework leverages the diffusion-denoising process to enhance the performance of visual perception. It generates multi-modal data that closely resembles the original training data, improving the discriminative capabilities of the model. The framework also includes a self-improving learning mechanism to optimize the use of the generated data, leading to better performance across various tasks.'}, 'zh': {'title': 'Diff-2-in-1：多模态生成与视觉感知的统一框架', 'desc': '本文提出了一种新的框架，称为Diff-2-in-1，旨在同时处理多模态数据生成和密集视觉感知。与以往将扩散模型视为独立组件的做法不同，该框架利用扩散去噪过程的独特特性，增强了视觉感知的判别能力。通过生成与原始训练集分布相似的多模态数据，Diff-2-in-1优化了生成数据的使用，采用了一种新颖的自我改进学习机制。实验结果表明，该框架在多种判别模型上均表现出一致的性能提升，并生成了高质量的多模态数据，具有真实感和实用性。'}}}, {'id': 'https://huggingface.co/papers/2411.04752', 'title': 'RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval', 'url': 'https://huggingface.co/papers/2411.04752', 'abstract': 'Code-mixing, the integration of lexical and grammatical elements from multiple languages within a single sentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman script, especially among migrant communities who form online groups to share relevant local information. This paper focuses on the challenges of extracting relevant information from code-mixed conversations, specifically within Roman transliterated Bengali mixed with English. This study presents a novel approach to address these challenges by developing a mechanism to automatically identify the most relevant answers from code-mixed conversations. We have experimented with a dataset comprising of queries and documents from Facebook, and Query Relevance files (QRels) to aid in this task. Our results demonstrate the effectiveness of our approach in extracting pertinent information from complex, code-mixed digital conversations, contributing to the broader field of natural language processing in multilingual and informal text environments. We use GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant documents to frame a mathematical model which helps to detect relevant documents corresponding to a query.', 'score': 12, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'e2dbb14c8f2ca6ef', 'data': {'categories': ['#multilingual', '#dataset', '#data'], 'emoji': '🗣️', 'ru': {'title': 'Извлечение информации из многоязычных разговоров: новый подход к обработке смешанных текстов', 'desc': 'Статья посвящена проблеме извлечения релевантной информации из смешанных языковых разговоров в социальных сетях Индии. Авторы разработали новый подход для автоматического определения наиболее релевантных ответов в разговорах на смеси бенгальского и английского языков, записанных латиницей. В исследовании использовались данные из Facebook и файлы Query Relevance, а также применялась языковая модель GPT-3.5 Turbo. Результаты демонстрируют эффективность предложенного метода в обработке сложных многоязычных текстов в неформальной цифровой среде.'}, 'en': {'title': 'Enhancing Information Retrieval in Code-Mixed Conversations', 'desc': 'This paper addresses the challenge of extracting relevant information from code-mixed conversations, particularly in Roman transliterated Bengali and English. It presents a novel mechanism that utilizes GPT-3.5 Turbo and a mathematical model to identify pertinent answers from social media interactions. The study experiments with a dataset from Facebook, focusing on queries and documents to enhance information retrieval. The results indicate that the proposed approach effectively navigates the complexities of multilingual and informal text, contributing to advancements in natural language processing.'}, 'zh': {'title': '从代码混合对话中提取信息的新方法', 'desc': '本文研究了在多语言环境中，如何从代码混合的对话中提取相关信息。特别是在印度，社交媒体用户常用罗马字母书写的孟加拉语与英语混合交流。我们提出了一种新方法，通过自动识别代码混合对话中的相关答案来解决这一挑战。实验结果表明，该方法在提取复杂数字对话中的相关信息方面是有效的，推动了自然语言处理领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.04989', 'title': 'SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2411.04989', 'abstract': 'Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds. Recent techniques address this issue by fine-tuning a pre-trained model to follow conditioning signals, such as bounding boxes or point trajectories. Yet, this fine-tuning procedure can be computationally expensive, and it requires datasets with annotated object motion, which can be difficult to procure. In this work, we introduce SG-I2V, a framework for controllable image-to-video generation that is self-guidedx2013offering zero-shot control by relying solely on the knowledge present in a pre-trained image-to-video diffusion model without the need for fine-tuning or external knowledge. Our zero-shot method outperforms unsupervised baselines while being competitive with supervised models in terms of visual quality and motion fidelity.', 'score': 12, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'a707043470b8dffd', 'data': {'categories': ['#video', '#diffusion', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Управляемая генерация видео без дополнительного обучения', 'desc': 'SG-I2V - это новый фреймворк для управляемой генерации видео из изображений. Он использует предобученную диффузионную модель без необходимости дополнительного обучения или внешних данных. Метод превосходит неконтролируемые базовые модели и конкурирует с контролируемыми по качеству изображения и точности движения. SG-I2V позволяет легко настраивать конкретные элементы генерируемых видео, такие как движение объектов или камеры.'}, 'en': {'title': 'Zero-Shot Control in Image-to-Video Generation', 'desc': 'This paper presents SG-I2V, a novel framework for generating videos from images with controllable features. Unlike traditional methods that require fine-tuning on annotated datasets, SG-I2V operates in a zero-shot manner, leveraging a pre-trained image-to-video diffusion model. This approach allows for easier manipulation of elements like object motion and camera movement without the computational costs associated with fine-tuning. The results show that SG-I2V achieves high visual quality and motion fidelity, outperforming unsupervised methods and competing with supervised ones.'}, 'zh': {'title': 'SG-I2V：高效的图像到视频生成方法', 'desc': '本文介绍了一种名为SG-I2V的框架，用于可控的图像到视频生成。该方法利用预训练的图像到视频扩散模型，提供零-shot控制，避免了繁琐的微调过程。与无监督基线相比，我们的方法在视觉质量和运动保真度上表现优越，并且与监督模型相竞争。此研究为视频生成提供了一种更高效的解决方案，减少了对标注数据集的依赖。'}}}, {'id': 'https://huggingface.co/papers/2411.04075', 'title': 'M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models', 'url': 'https://huggingface.co/papers/2411.04075', 'abstract': 'Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address this gap, we introduce M3SciQA, a multi-modal, multi-document scientific question answering benchmark designed for a more comprehensive evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, where each cluster represents a primary paper along with all its cited documents, mirroring the workflow of comprehending a single paper by requiring multi-modal and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of 18 foundation models. Our results indicate that current foundation models still significantly underperform compared to human experts in multi-modal information retrieval and in reasoning across multiple scientific documents. Additionally, we explore the implications of these findings for the future advancement of applying foundation models in multi-modal scientific literature analysis.', 'score': 10, 'issue_id': 480, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '654e787a9f0ab7ff', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#science'], 'emoji': '🧠', 'ru': {'title': 'M3SciQA: Новый рубеж в оценке фундаментальных моделей для научного анализа', 'desc': 'M3SciQA - это новый бенчмарк для оценки фундаментальных моделей в области научных вопросов и ответов. Он включает в себя мультимодальные и мультидокументные задачи, охватывающие 70 кластеров статей по обработке естественного языка. Результаты тестирования 18 моделей показали, что их производительность значительно уступает экспертам-людям в задачах мультимодального поиска информации и рассуждений по нескольким научным документам. Этот бенчмарк открывает новые перспективы для развития применения фундаментальных моделей в анализе научной литературы.'}, 'en': {'title': 'M3SciQA: Bridging the Gap in Multi-Modal Scientific Understanding', 'desc': 'This paper introduces M3SciQA, a new benchmark for evaluating foundation models in the context of scientific question answering. Unlike existing benchmarks that focus solely on single-document, text-only tasks, M3SciQA incorporates multi-modal and multi-document elements to better reflect real research workflows. The benchmark includes 1,452 expert-annotated questions related to clusters of natural language processing papers, requiring models to interpret both textual and non-textual data. Evaluation results show that current foundation models still lag behind human experts in retrieving multi-modal information and reasoning across multiple documents, highlighting the need for further advancements in this area.'}, 'zh': {'title': 'M3SciQA：多模态科学问答的新基准', 'desc': '现有的基础模型评估基准主要集中在单文档、文本任务上，无法全面捕捉研究工作流程的复杂性。为了解决这个问题，我们提出了M3SciQA，这是一个多模态、多文档的科学问答基准，旨在更全面地评估基础模型。M3SciQA包含1452个专家注释的问题，涵盖70个自然语言处理论文集群，模拟理解单篇论文的工作流程。我们的评估结果表明，当前的基础模型在多模态信息检索和跨多个科学文档推理方面仍显著低于人类专家的表现。'}}}, {'id': 'https://huggingface.co/papers/2411.07184', 'title': 'SAMPart3D: Segment Any Part in 3D Objects', 'url': 'https://huggingface.co/papers/2411.07184', 'abstract': '3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.', 'score': 16, 'issue_id': 541, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': 'b4e58a99e4a7e86c', 'data': {'categories': ['#games', '#3d', '#benchmark', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'SAMPart3D: гибкая сегментация 3D-объектов без предварительного обучения', 'desc': 'Статья представляет SAMPart3D - масштабируемый фреймворк для сегментации частей 3D-объектов без предварительного обучения. Авторы используют безтекстовые модели компьютерного зрения для извлечения признаков из 3D-данных, что позволяет обучаться на больших наборах неразмеченных 3D-объектов. Метод способен сегментировать объекты на части с разной степенью детализации, а затем присваивать семантические метки с помощью мультимодальных языковых моделей. SAMPart3D превосходит существующие методы и может применяться для редактирования и интерактивной сегментации 3D-объектов.'}, 'en': {'title': 'Revolutionizing 3D Part Segmentation with SAMPart3D', 'desc': 'This paper presents SAMPart3D, a novel framework for zero-shot 3D part segmentation that does not depend on predefined text prompts. It utilizes text-agnostic vision foundation models to extract 3D features, enabling it to scale effectively to large unlabeled datasets. The framework also incorporates scale-conditioned part-aware features, allowing for segmentation at various levels of detail. SAMPart3D outperforms existing methods and introduces a new benchmark to enhance the diversity and complexity of 3D part segmentation tasks.'}, 'zh': {'title': 'SAMPart3D：无文本提示的3D部件分割新框架', 'desc': '3D部件分割是3D感知中的一项重要且具有挑战性的任务，广泛应用于机器人技术、3D生成和3D编辑等领域。本文提出了SAMPart3D框架，它能够在不依赖预定义文本提示的情况下，对任意3D对象进行多粒度的语义部件分割。该框架利用无文本依赖的视觉基础模型，从大规模未标记的3D数据集中提取丰富的3D特征，并通过条件化的部件感知特征实现灵活的分割。实验结果表明，SAMPart3D在处理复杂对象时显著优于现有的零样本3D部件分割方法，并能支持多种应用，如部件级编辑和交互式分割。'}}}, {'id': 'https://huggingface.co/papers/2411.07133', 'title': 'Stronger Models are NOT Stronger Teachers for Instruction Tuning', 'url': 'https://huggingface.co/papers/2411.07133', 'abstract': "Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.", 'score': 11, 'issue_id': 546, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': 'be2fc1cdad8aa9f3', 'data': {'categories': ['#dataset', '#optimization', '#synthetic', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Парадокс больших моделей: не всегда лучшие учителя', 'desc': 'В статье рассматривается проблема настройки больших языковых моделей (LLMs) с помощью синтетических наборов инструкций. Авторы выявили парадокс, заключающийся в том, что более крупные модели не всегда являются лучшими учителями для более мелких моделей. Для решения этой проблемы они разработали новый метрик, называемый Compatibility-Adjusted Reward (CAR), который учитывает совместимость между учителем и базовой моделью. Эксперименты показали, что CAR превосходит почти все существующие подходы.'}, 'en': {'title': "Rethinking Instruction Tuning: Size Isn't Everything!", 'desc': "This paper investigates the effectiveness of instruction tuning in large language models (LLMs) and challenges the assumption that larger models are better teachers for smaller models. The authors introduce the concept of the Larger Models' Paradox, showing that bigger models do not always enhance the instruction-following capabilities of smaller models. They highlight the limitations of existing metrics in evaluating response generators, which fail to consider the compatibility between the teacher and the base model. To address this, they propose a new metric called Compatibility-Adjusted Reward (CAR), which shows improved performance in assessing the effectiveness of response generators across various models."}, 'zh': {'title': '大模型不一定是好教师！', 'desc': '本论文探讨了指令调优在大型语言模型（LLMs）中的应用，强调了指令数据集对模型性能的重要性。我们发现，较大或较强的模型并不一定是较小模型的更好教师，这一现象被称为“大模型悖论”。此外，现有的评估指标无法准确预测响应生成器的有效性，因为它们忽略了教师模型与被调优基础模型之间的兼容性。为此，我们提出了一种新的评估指标——兼容性调整奖励（CAR），并通过实验验证了其优越性。'}}}, {'id': 'https://huggingface.co/papers/2411.07975', 'title': 'JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2411.07975', 'abstract': 'We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.', 'score': 11, 'issue_id': 544, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': '294dc65a01cd1218', 'data': {'categories': ['#benchmark', '#alignment', '#architecture', '#diffusion', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'Единая модель для понимания и генерации изображений', 'desc': 'JanusFlow - это новая архитектура, объединяющая понимание и генерацию изображений в одной модели. Она интегрирует авторегрессионные языковые модели с методом rectified flow для генеративного моделирования. Ключевое преимущество - возможность обучать rectified flow в рамках больших языковых моделей без сложных модификаций архитектуры. Для улучшения производительности используется разделение энкодеров понимания и генерации, а также выравнивание их представлений при обучении.'}, 'en': {'title': 'JanusFlow: Unifying Image Understanding and Generation Efficiently', 'desc': 'JanusFlow is a novel framework that combines image understanding and generation into one cohesive model. It utilizes a simple architecture that merges autoregressive language models with rectified flow, enhancing generative modeling capabilities. The study reveals that rectified flow can be effectively trained within the large language model context without needing complex changes to the architecture. By decoupling the encoders for understanding and generation and aligning their representations during training, JanusFlow demonstrates superior performance compared to specialized models and existing unified approaches.'}, 'zh': {'title': 'JanusFlow：图像理解与生成的统一模型', 'desc': 'JanusFlow是一个强大的框架，将图像理解和生成统一在一个模型中。它采用了简约的架构，将自回归语言模型与修正流结合，这是生成建模中的一种先进方法。研究发现，修正流可以在大型语言模型框架内轻松训练，无需复杂的架构修改。通过解耦理解和生成编码器以及在统一训练中对齐它们的表示，JanusFlow在标准基准测试中表现出色，超越了现有的统一方法。'}}}, {'id': 'https://huggingface.co/papers/2411.08034', 'title': 'Scaling Properties of Diffusion Models for Perceptual Tasks', 'url': 'https://huggingface.co/papers/2411.08034', 'abstract': 'In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and segmentation under image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perception tasks. Through a careful analysis of these scaling behaviors, we present various techniques to efficiently train diffusion models for visual perception tasks. Our models achieve improved or comparable performance to state-of-the-art methods using significantly less data and compute. To use our code and models, see https://scaling-diffusion-perception.github.io .', 'score': 5, 'issue_id': 557, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': '4be3b9af89108627', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#cv'], 'emoji': '🔬', 'ru': {'title': 'Диффузионные модели: новый взгляд на задачи визуального восприятия', 'desc': 'В этой статье исследуются возможности итеративных вычислений с диффузионными моделями для задач визуального восприятия. Авторы объединяют такие задачи как оценка глубины, оптический поток и сегментация в рамках преобразования изображения в изображение. Они демонстрируют, как диффузионные модели выигрывают от увеличения вычислительных ресурсов на этапах обучения и вывода. Предложенные методы позволяют эффективно обучать диффузионные модели для задач визуального восприятия, достигая улучшенных результатов при меньшем объеме данных и вычислений.'}, 'en': {'title': 'Scaling Diffusion Models for Enhanced Visual Perception', 'desc': 'This paper explores how diffusion models can be effectively used for both generating images and performing visual perception tasks like depth estimation and segmentation. It presents a unified approach that treats these tasks as image-to-image translation problems, highlighting the advantages of scaling in training and testing. The authors analyze the scaling behaviors of diffusion models and propose techniques to enhance their efficiency in visual perception tasks. As a result, their models demonstrate competitive performance compared to leading methods while requiring less data and computational resources.'}, 'zh': {'title': '扩散模型：视觉感知的新力量', 'desc': '本文提出了扩散模型在生成和视觉感知任务中的强大能力。我们将深度估计、光流和分割等任务统一为图像到图像的转换，并展示了扩散模型在这些感知任务中如何通过扩展训练和测试计算来获益。通过对这些扩展行为的仔细分析，我们提出了多种高效训练扩散模型的技术。我们的模型在使用显著更少的数据和计算的情况下，达到了与最先进方法相当或更好的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.07461', 'title': 'BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions', 'url': 'https://huggingface.co/papers/2411.07461', 'abstract': 'We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that bridges the gap between descriptive synthetic captions and factual web-scale alt-text. KALE augments synthetic dense image captions with web-scale alt-text to generate factually grounded image captions. Our two-stage approach leverages large vision-language models and language models to create knowledge-augmented captions, which are then used to train a specialized VLM for scaling up the dataset. We train vision-language models on KALE and demonstrate improvements on vision-language tasks. Our experiments show the utility of KALE for training more capable and knowledgeable multimodal models. We release the KALE dataset at https://huggingface.co/datasets/Salesforce/blip3-kale', 'score': 5, 'issue_id': 552, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': '08fb959148999629', 'data': {'categories': ['#dataset', '#multimodal', '#synthetic', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'KALE: Обогащение мультимодальных моделей фактическими знаниями', 'desc': 'В статье представлен новый набор данных BLIP3-KALE, состоящий из 218 миллионов пар изображений и текстов. Этот датасет объединяет синтетические описательные подписи с фактическими альтернативными текстами из веб-источников. Авторы используют двухэтапный подход с применением крупных мультимодальных и языковых моделей для создания подписей, обогащенных знаниями. Эксперименты показывают, что обучение на KALE улучшает результаты в задачах компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Bridging Descriptive and Factual Captions with KALE', 'desc': 'The paper presents BLIP3-KALE, a new dataset containing 218 million image-text pairs that enhances the quality of image captions by combining synthetic captions with factual web-based alt-text. This dataset is created using a two-stage method that employs large vision-language models and language models to produce captions that are both descriptive and factually accurate. By training specialized vision-language models on the KALE dataset, the authors demonstrate significant improvements in various vision-language tasks. The findings highlight the effectiveness of KALE in developing more advanced multimodal models that can better understand and generate content across different modalities.'}, 'zh': {'title': '知识增强的图像标题生成', 'desc': '我们介绍了BLIP3-KALE，这是一个包含2.18亿对图像-文本的数据集，旨在弥合描述性合成标题与事实性网络规模替代文本之间的差距。KALE通过将合成的密集图像标题与网络规模的替代文本相结合，生成基于事实的图像标题。我们采用两阶段的方法，利用大型视觉-语言模型和语言模型创建知识增强的标题，并用这些标题训练专门的视觉语言模型，以扩大数据集规模。实验结果表明，KALE在训练更强大和更具知识性的多模态模型方面具有重要价值。'}}}, {'id': 'https://huggingface.co/papers/2411.08017', 'title': 'Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings', 'url': 'https://huggingface.co/papers/2411.08017', 'abstract': "Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively. To address this, we introduce a novel approach called Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based, compact latent encodings. Specifically, we compress a 256^3 signed distance field into a 12^3 times 4 latent grid, achieving an impressive 2427x compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing the inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at 256^3 resolution. Moreover, WaLa offers rapid inference, producing shapes within two to four seconds depending on the condition, despite the model's scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. We open-source our code and, to the best of our knowledge, release the largest pretrained 3D generative models across different modalities.", 'score': 4, 'issue_id': 547, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': '0af1f4c0dc38cc5b', 'data': {'categories': ['#inference', '#open_source', '#3d', '#dataset', '#diffusion', '#architecture'], 'emoji': '🌊', 'ru': {'title': 'Эффективная генерация 3D-моделей с помощью вейвлет-сжатия', 'desc': 'Статья представляет новый подход к генерации 3D-моделей под названием Wavelet Latent Diffusion (WaLa). Метод использует вейвлет-кодирование для создания компактных латентных представлений 3D-форм, достигая степени сжатия 2427x. Это позволяет эффективно обучать крупномасштабные генеративные нейронные сети без увеличения времени вывода. WaLa демонстрирует улучшенное качество генерации, разнообразие и вычислительную эффективность по сравнению с существующими методами.'}, 'en': {'title': 'Efficient 3D Shape Generation with Wavelet Latent Diffusion', 'desc': 'This paper presents a new method called Wavelet Latent Diffusion (WaLa) for generating high-quality 3D shapes efficiently. It addresses the limitations of existing 3D generative models by using wavelet-based latent encodings, which compress 3D shapes significantly while preserving detail. The method achieves a remarkable 2427x compression ratio, allowing for the training of large-scale generative networks without increasing inference time. WaLa demonstrates state-of-the-art performance in generating diverse and high-quality 3D shapes, and the authors have made their code and pretrained models available to the public.'}, 'zh': {'title': '高效压缩，快速生成3D形状的创新方法', 'desc': '本论文提出了一种新的方法，称为Wavelet Latent Diffusion（WaLa），旨在提高大规模3D生成模型的效率。通过将3D形状编码为基于小波的紧凑潜在编码，WaLa实现了高达2427倍的压缩比，同时保持了细节的完整性。该方法使得训练大型生成网络变得更加高效，并且在推理时不会显著增加时间。我们的模型在多个数据集上表现出色，生成高质量的3D形状，并且开源了代码，推动了3D生成模型的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.05197', 'title': 'Hardware and Software Platform Inference', 'url': 'https://huggingface.co/papers/2411.05197', 'abstract': "It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. In this paper we introduce \\textbf{hardware and software platform inference (HSPI)} -- a method for identifying the underlying  architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. Our method leverages the inherent differences of various  architectures and compilers to distinguish between different  types and software stacks. By analyzing the numerical patterns in the model's outputs, we propose a classification framework capable of accurately identifying the  used for model inference as well as the underlying software configuration. Our findings demonstrate the feasibility of inferring  type from black-box models. We evaluate HSPI against models served on different real hardware and find that in a white-box setting we can distinguish between different s with between 83.9% and 100% accuracy. Even in a black-box setting we are able to achieve results that are up to three times higher than random guess accuracy.", 'score': 3, 'issue_id': 550, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '7685c8e74f6dbc6b', 'data': {'categories': ['#leakage', '#security', '#architecture', '#inference'], 'emoji': '🕵️', 'ru': {'title': 'Разоблачение обмана: как определить реальное оборудование для языковой модели', 'desc': 'Статья представляет метод HSPI (идентификация аппаратной и программной платформы), позволяющий определить архитектуру и программный стек модели машинного обучения только на основе ее поведения при вводе-выводе. Метод использует присущие различным архитектурам и компиляторам отличия для распознавания типов оборудования и программных конфигураций. Анализируя числовые паттерны в выводе модели, предложена классификационная система, способная точно идентифицировать оборудование, используемое для вывода модели. Результаты показывают возможность определения типа оборудования для черного ящика модели с точностью до 100% в белом ящике и до трех раз выше случайного угадывания в черном ящике.'}, 'en': {'title': 'Verify Your Model: Uncovering the Truth Behind LLM Inference', 'desc': "This paper addresses the challenge of verifying the authenticity of large language model (LLM) services purchased by businesses. It introduces a novel method called hardware and software platform inference (HSPI), which analyzes the input-output behavior of machine learning models to identify their underlying hardware and software configurations. By examining the numerical patterns in the outputs, HSPI can distinguish between different architectures and software stacks, even in a black-box scenario. The results show that HSPI can achieve high accuracy in identifying the model's hardware and software, significantly improving upon random guessing."}, 'zh': {'title': '验证大型语言模型的真实性', 'desc': '本文介绍了一种名为硬件和软件平台推理（HSPI）的方法，用于识别机器学习模型的底层架构和软件堆栈。该方法通过分析模型的输入输出行为，利用不同架构和编译器的固有差异来区分不同类型的硬件和软件配置。研究表明，在白盒环境下，我们可以以83.9%到100%的准确率区分不同的硬件，而在黑盒环境下，准确率也能达到随机猜测的三倍以上。此方法为验证大型语言模型的真实性提供了一种有效的手段。'}}}, {'id': 'https://huggingface.co/papers/2411.06307', 'title': 'Acoustic Volume Rendering for Neural Impulse Response Fields', 'url': 'https://huggingface.co/papers/2411.06307', 'abstract': "Realistic audio synthesis that captures accurate acoustic phenomena is essential for creating immersive experiences in virtual and augmented reality. Synthesizing the sound received at any position relies on the estimation of impulse response (IR), which characterizes how sound propagates in one scene along different paths before arriving at the listener's position. In this paper, we present Acoustic Volume Rendering (AVR), a novel approach that adapts volume rendering techniques to model acoustic impulse responses. While volume rendering has been successful in modeling radiance fields for images and neural scene representations, IRs present unique challenges as time-series signals. To address these challenges, we introduce frequency-domain volume rendering and use spherical integration to fit the IR measurements. Our method constructs an impulse response field that inherently encodes wave propagation principles and achieves state-of-the-art performance in synthesizing impulse responses for novel poses. Experiments show that AVR surpasses current leading methods by a substantial margin. Additionally, we develop an acoustic simulation platform, AcoustiX, which provides more accurate and realistic IR simulations than existing simulators. Code for AVR and AcoustiX are available at https://zitonglan.github.io/avr.", 'score': 2, 'issue_id': 554, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 ноября', 'en': 'November 9', 'zh': '11月9日'}, 'hash': 'f6935a265562d416', 'data': {'categories': ['#audio'], 'emoji': '🔊', 'ru': {'title': 'Реалистичный синтез звука с помощью объемного рендеринга', 'desc': 'Статья представляет новый метод синтеза акустических импульсных характеристик под названием Acoustic Volume Rendering (AVR). AVR адаптирует технику рендеринга объема для моделирования распространения звука в пространстве. Метод использует рендеринг в частотной области и сферическую интеграцию для точного воспроизведения измеренных импульсных характеристик. Эксперименты показывают, что AVR значительно превосходит существующие методы в синтезе импульсных характеристик для новых позиций.'}, 'en': {'title': 'Revolutionizing Sound: Acoustic Volume Rendering for Immersive Audio Experiences', 'desc': 'This paper introduces Acoustic Volume Rendering (AVR), a new method for synthesizing realistic audio in virtual and augmented reality. AVR uses volume rendering techniques to model acoustic impulse responses (IRs), which describe how sound travels in a scene. The authors tackle the unique challenges of IRs as time-series signals by employing frequency-domain volume rendering and spherical integration. Their approach not only improves the accuracy of sound synthesis but also outperforms existing methods, demonstrating significant advancements in acoustic simulation with their platform, AcoustiX.'}, 'zh': {'title': '声学体积渲染：提升虚拟现实中的音频体验', 'desc': '本文提出了一种新的声学体积渲染（AVR）方法，用于合成真实的声学脉冲响应（IR），以增强虚拟和增强现实中的沉浸体验。AVR通过适应体积渲染技术，解决了声波传播的独特挑战，特别是将时间序列信号建模为脉冲响应场。我们引入了频域体积渲染和球面积分技术，以更准确地拟合IR测量。实验结果表明，AVR在合成新姿态的脉冲响应方面超越了现有的领先方法，并且我们还开发了一个声学模拟平台AcoustiX，提供更准确的IR模拟。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (16)', '#agents (14)', '#agi (3)', '#alignment (17)', '#architecture (57)', '#audio (2)', '#benchmark (54)', '#cv (38)', '#data (21)', '#dataset (54)', '#diffusion (26)', '#ethics (4)', '#games (17)', '#graphs (11)', '#hallucinations (5)', '#healthcare (7)', '#inference (16)', '#interpretability (14)', '#leakage (2)', '#long_context (12)', '#low_resource (8)', '#machine_translation (2)', '#math (9)', '#multilingual (12)', '#multimodal (31)', '#open_source (46)', '#optimization (60)', '#plp (8)', '#rag (5)', '#reasoning (23)', '#rl (9)', '#rlhf (7)', '#robotics (3)', '#science (13)', '#security (3)', '#small_models (9)', '#story_generation (1)', '#survey (6)', '#synthetic (18)', '#training (66)', '#transfer_learning (9)', '#video (19)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-11-15 02:18',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-15 02:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-15 02:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    