
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 225 papers. November 2024.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">ĞĞ¾ÑĞ±Ñ€ÑŒ 2024</span> | <span id="title-articles-count">225 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/m/2024-10.html">â¬…ï¸ <span id="prev-date">10.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2024-12.html">â¡ï¸ <span id="next-date">12.2024</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">ğŸ“ˆ <span id='top-day-label'>Ğ”ĞµĞ½ÑŒ</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'ĞĞ¾ÑĞ±Ñ€ÑŒ 2024', 'en': 'November 2024', 'zh': '11æœˆ2024å¹´'};
        let feedDateNext = {'ru': '12.2024', 'en': '12/2024', 'zh': '12æœˆ2024å¹´'};
        let feedDatePrev = {'ru': '10.2024', 'en': '10/2024', 'zh': '10æœˆ2024å¹´'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.02959', 'title': 'HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems', 'url': 'https://huggingface.co/papers/2411.02959', 'abstract': 'Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems.', 'score': 59, 'issue_id': 437, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '6fb8684374e5fdcb', 'authors': ['Jiejun Tan', 'Zhicheng Dou', 'Wen Wang', 'Mang Wang', 'Weipeng Chen', 'Ji-Rong Wen'], 'affiliations': ['Gaoling School of Artificial Intelligence Renmin University of China', 'Baichuan Intelligent Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02959.jpg', 'data': {'categories': ['#rag', '#hallucinations', '#inference', '#data', '#retrieval_augmented_generation'], 'emoji': 'ğŸŒ', 'ru': {'title': 'HtmlRAG: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ RAG-ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²ĞµĞ±-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ (RAG), Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ HtmlRAG. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ RAG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚, HtmlRAG ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ HTML-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸, ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€ĞµĞ·ĞºĞ¸ HTML Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ HTML Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… RAG.'}, 'en': {'title': 'Harnessing HTML for Enhanced Knowledge Retrieval in RAG Systems', 'desc': 'This paper introduces HtmlRAG, a novel approach to Retrieval-Augmented Generation (RAG) that utilizes HTML instead of plain text for knowledge retrieval. By leveraging the structural and semantic information present in HTML, HtmlRAG aims to enhance the performance of large language models (LLMs) and reduce the hallucination problem. The authors address the challenges posed by HTML, such as excess tokens and noise, by implementing cleaning, compression, and pruning techniques to streamline the input. Experimental results demonstrate that HtmlRAG outperforms traditional plain-text-based RAG systems across multiple question-answering datasets.'}, 'zh': {'title': 'ç”¨HTMLæå‡æ£€ç´¢å¢å¼ºç”Ÿæˆçš„èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•ï¼Œç§°ä¸ºHtmlRAGï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„çŸ¥è¯†èƒ½åŠ›å¹¶å‡å°‘å¹»è§‰é—®é¢˜ã€‚HtmlRAGä½¿ç”¨HTMLæ ¼å¼è€Œéçº¯æ–‡æœ¬æ¥å¢å¼ºç”Ÿæˆè¿‡ç¨‹ï¼Œä»è€Œä¿ç•™æ›´å¤šçš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹HTMLä¸­å¤šä½™å†…å®¹å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†HTMLæ¸…ç†ã€å‹ç¼©å’Œä¿®å‰ªç­–ç•¥ï¼Œä»¥å‡å°‘è¾“å…¥çš„å†—ä½™ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHtmlRAGåœ¨å…­ä¸ªé—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„çº¯æ–‡æœ¬RAGç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00871', 'title': 'LLaMo: Large Language Model-based Molecular Graph Assistant', 'url': 'https://huggingface.co/papers/2411.00871', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable generalization and instruction-following capabilities with instruction tuning. The advancements in LLMs and instruction tuning have led to the development of Large Vision-Language Models (LVLMs). However, the competency of the LLMs and instruction tuning have been less explored in the molecular domain. Thus, we propose LLaMo: Large Language Model-based Molecular graph assistant, which is an end-to-end trained large molecular graph-language model. To bridge the discrepancy between the language and graph modalities, we present the multi-level graph projector that transforms graph representations into graph tokens by abstracting the output representations of each GNN layer and motif representations with the cross-attention mechanism. We also introduce machine-generated molecular graph instruction data to instruction-tune the large molecular graph-language model for general-purpose molecule and language understanding. Our extensive experiments demonstrate that LLaMo shows the best performance on diverse tasks, such as molecular description generation, property prediction, and IUPAC name prediction. The code of LLaMo is available at https://github.com/mlvlab/LLaMo.', 'score': 20, 'issue_id': 439, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'd1284691dab4e739', 'authors': ['Jinyoung Park', 'Minseong Bae', 'Dohwan Ko', 'Hyunwoo J. Kim'], 'affiliations': ['Department of Computer Science and Engineering, Korea University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00871.jpg', 'data': {'categories': ['#cv', '#graphs', '#multimodal', '#training', '#transfer_learning', '#open_source', '#architecture'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'LLaMo: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ LLaMo - Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. LLaMo Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ…, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¼ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ¼, Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ ÑĞ·Ñ‹ĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLaMo Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ», Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ IUPAC.'}, 'en': {'title': 'Bridging Language and Molecules with LLaMo', 'desc': 'This paper introduces LLaMo, a Large Language Model-based Molecular graph assistant designed to enhance understanding in the molecular domain. It utilizes a multi-level graph projector to convert molecular graph representations into tokens, facilitating better interaction between language and graph data. The model is instruction-tuned using machine-generated molecular graph instruction data, enabling it to perform various tasks like molecular description generation and property prediction. Experimental results show that LLaMo outperforms existing models in these tasks, highlighting its effectiveness in bridging language and molecular graph understanding.'}, 'zh': {'title': 'LLaMoï¼šè¿æ¥è¯­è¨€ä¸åˆ†å­çš„æ¡¥æ¢', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æŒ‡ä»¤è°ƒä¼˜æ–¹é¢å±•ç°äº†å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†LLaMoï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ†å­å›¾åŠ©æ‰‹ï¼Œæ—¨åœ¨å¡«è¡¥è¯­è¨€å’Œå›¾å½¢æ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šå±‚å›¾æŠ•å½±å™¨ï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶å°†å›¾è¡¨ç¤ºè½¬æ¢ä¸ºå›¾æ ‡è®°ï¼Œå¹¶ä½¿ç”¨æœºå™¨ç”Ÿæˆçš„åˆ†å­å›¾æŒ‡ä»¤æ•°æ®å¯¹æ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLaMoåœ¨åˆ†å­æè¿°ç”Ÿæˆã€å±æ€§é¢„æµ‹å’ŒIUPACåç§°é¢„æµ‹ç­‰å¤šé¡¹ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.23054', 'title': 'Controlling Language and Diffusion Models by Transporting Activations', 'url': 'https://huggingface.co/papers/2410.23054', 'abstract': 'The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output. In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.', 'score': 16, 'issue_id': 444, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '5238611006cc8b68', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#diffusion', '#hallucinations', '#cv', '#optimization', '#multimodal', '#interpretability', '#ethics', '#training', '#security', '#architecture', '#alignment'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'AcT: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AcT (Activation Transport) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ°. AcT Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ AcT Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»ĞµĞ¼ Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Steering Generative Models with Activation Transport', 'desc': 'This paper presents Activation Transport (AcT), a new framework designed to control the behavior of large generative models by steering their activations. Using principles from optimal transport theory, AcT allows for precise manipulation of model outputs without significantly increasing computational costs. The framework is applicable across different modalities, including large language models (LLMs) and text-to-image diffusion models (T2Is). Experimental results demonstrate that AcT can reduce toxicity in LLMs, induce specific concepts, and enhance truthfulness, while also providing style control and concept negation in T2Is.'}, 'zh': {'title': 'æ¿€æ´»ä¼ è¾“ï¼šæ§åˆ¶ç”Ÿæˆæ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ¿€æ´»ä¼ è¾“ï¼ˆAcTï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æœ€ä¼˜ä¼ è¾“ç†è®ºæ¥å¼•å¯¼æ¨¡å‹çš„æ¿€æ´»ï¼Œä»è€Œæ§åˆ¶ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºã€‚è¯¥æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ¨¡æ€ä¸­åº”ç”¨ï¼Œä¸”å¯¹æ¨¡å‹çš„è®¡ç®—å¼€é”€å½±å“æå°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAcTåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æœ‰å®³å†…å®¹ã€å¼•å…¥ä»»æ„æ¦‚å¿µå¹¶æé«˜ç”Ÿæˆå†…å®¹çš„çœŸå®æ€§ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆT2Isï¼‰ä¸­ï¼ŒAcTåˆ™å®ç°äº†å¯¹é£æ ¼çš„ç²¾ç»†æ§åˆ¶å’Œæ¦‚å¿µçš„å¦å®šã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2411.02359', 'title': 'DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution', 'url': 'https://huggingface.co/papers/2411.02359', 'abstract': 'MLLMs have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we propose a Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (i.e., power consumption), as well as peak computational consumption (i.e., latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.', 'score': 12, 'issue_id': 437, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '08c45469caff5fa0', 'authors': ['Yang Yue', 'Yulin Wang', 'Bingyi Kang', 'Yizeng Han', 'Shenzhi Wang', 'Shiji Song', 'Jiashi Feng', 'Gao Huang'], 'affiliations': ['Department of Automation, BNRist, Tsinghua University', 'ByteDance'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02359.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#inference', '#agi', '#optimization', '#robotics', '#open_source', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², Ñ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Dynamic Early-Exit Framework Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, ÑĞ·Ñ‹ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ (DeeR-VLA). Ğ­Ñ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. DeeR Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ CALVIN ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Efficient Robotic Intelligence with Dynamic Early-Exit MLLMs', 'desc': 'This paper introduces a new framework called DeeR for improving the efficiency of robotic vision-language-action models (MLLMs). DeeR uses a Dynamic Early-Exit approach that allows the model to adjust its size based on the specific task, reducing unnecessary computations. By implementing a multi-exit architecture, the model can stop processing once it has enough information, which helps save power and memory. The results show that DeeR can significantly lower computational costs and memory usage while still performing well on tasks, making it suitable for real-world robotic applications.'}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´ï¼Œæ™ºèƒ½æœºå™¨äººæ›´é«˜æ•ˆï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŠ¨æ€æ—©æœŸé€€å‡ºæ¡†æ¶ï¼ˆDeeR-VLAï¼‰ï¼Œæ—¨åœ¨è§£å†³æœºå™¨äººè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å®é™…åº”ç”¨ä¸­çš„è®¡ç®—å’Œå†…å­˜é™åˆ¶é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¤šå‡ºå£æ¶æ„ï¼Œèƒ½å¤Ÿæ ¹æ®å…·ä½“æƒ…å†µè‡ªåŠ¨è°ƒæ•´æ¿€æ´»çš„æ¨¡å‹å¤§å°ï¼Œä»è€Œé¿å…å†—ä½™è®¡ç®—ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†æ–°ç®—æ³•ï¼Œè®¾å®šæ—©æœŸç»ˆæ­¢æ ‡å‡†ï¼Œä»¥æ»¡è¶³é¢„å®šä¹‰çš„è®¡ç®—éœ€æ±‚ï¼Œå¦‚åŠŸè€—å’Œå»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeeRåœ¨CALVINæœºå™¨äººæ“ä½œåŸºå‡†ä¸Šæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬å’ŒGPUå†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02393', 'title': 'Adaptive Length Image Tokenization via Recurrent Allocation', 'url': 'https://huggingface.co/papers/2411.02393', 'abstract': 'Current vision systems typically assign fixed-length representations to images, regardless of the information content. This contrasts with human intelligence - and even large language models - which allocate varying representational capacities based on entropy, context and familiarity. Inspired by this, we propose an approach to learn variable-length token representations for 2D images. Our encoder-decoder architecture recursively processes 2D image tokens, distilling them into 1D latent tokens over multiple iterations of recurrent rollouts. Each iteration refines the 2D tokens, updates the existing 1D latent tokens, and adaptively increases representational capacity by adding new tokens. This enables compression of images into a variable number of tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction loss and FID metrics, demonstrating that token count aligns with image entropy, familiarity and downstream task requirements. Recurrent token processing with increasing representational capacity in each iteration shows signs of token specialization, revealing potential for object / part discovery.', 'score': 11, 'issue_id': 448, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '1c3553b38b491652', 'authors': ['Shivam Duggal', 'Phillip Isola', 'Antonio Torralba', 'William T. Freeman'], 'affiliations': ['MIT CSAIL'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02393.jpg', 'data': {'categories': ['#optimization', '#architecture', '#interpretability', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€-Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 2D-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¸Ñ… Ğ² 1D-Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ğ· ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑÑ‚ÑÑ 2D-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ÑÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ 1D-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ĞµĞ¼ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ downstream-Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Adaptive Tokenization for Enhanced Image Representation', 'desc': "This paper introduces a novel method for creating variable-length token representations of images, which contrasts with traditional fixed-length approaches. The proposed encoder-decoder architecture processes 2D image tokens recursively, refining them into 1D latent tokens through multiple iterations. Each iteration not only updates the existing tokens but also allows for the addition of new tokens, enabling a flexible representation that adapts to the complexity of the image. The effectiveness of this method is validated through reconstruction loss and FID metrics, showing that the number of tokens correlates with the image's information content and can enhance downstream tasks."}, 'zh': {'title': 'å¯å˜é•¿åº¦æ ‡è®°è¡¨ç¤ºï¼šæå‡å›¾åƒç†è§£èƒ½åŠ›', 'desc': 'å½“å‰çš„è§†è§‰ç³»ç»Ÿé€šå¸¸ä¸ºå›¾åƒåˆ†é…å›ºå®šé•¿åº¦çš„è¡¨ç¤ºï¼Œè¿™ä¸äººç±»æ™ºèƒ½å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„åŠ¨æ€è¡¨ç¤ºèƒ½åŠ›å½¢æˆå¯¹æ¯”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡ç¼–ç å™¨-è§£ç å™¨æ¶æ„å­¦ä¹ äºŒç»´å›¾åƒçš„å¯å˜é•¿åº¦æ ‡è®°è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡é€’å½’å¤„ç†äºŒç»´å›¾åƒæ ‡è®°ï¼Œå°†å…¶æç‚¼ä¸ºä¸€ç»´æ½œåœ¨æ ‡è®°ï¼Œå¹¶åœ¨å¤šä¸ªè¿­ä»£ä¸­é€æ­¥æ›´æ–°å’Œå¢åŠ è¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ ‡è®°æ•°é‡ä¸å›¾åƒçš„ç†µã€ç†Ÿæ‚‰åº¦å’Œä¸‹æ¸¸ä»»åŠ¡éœ€æ±‚ç›¸ä¸€è‡´ï¼Œæ˜¾ç¤ºå‡ºæ ‡è®°çš„ä¸“ä¸šåŒ–æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.01493', 'title': 'Sample-Efficient Alignment for LLMs', 'url': 'https://huggingface.co/papers/2411.01493', 'abstract': "We study methods for efficiently aligning large language models (LLMs) with human preferences given budgeted online feedback. We first formulate the LLM alignment problem in the frame of contextual dueling bandits. This formulation, subsuming recent paradigms such as online RLHF and online DPO, inherently quests for sample-efficient algorithms that incorporate online active exploration. Leveraging insights from bandit theory, we introduce a unified algorithm based on Thompson sampling and highlight its applications in two distinct LLM alignment scenarios. The practical agent that efficiently implements this algorithm, named SEA (Sample-Efficient Alignment), is empirically validated through extensive experiments across three model scales (1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The results demonstrate that SEA achieves highly sample-efficient alignment with oracle's preferences, outperforming recent active exploration methods for LLMs. Additionally, we release the implementation of SEA together with an efficient codebase designed for online alignment of LLMs, aiming to accelerate future research in this field.", 'score': 10, 'issue_id': 440, 'pub_date': '2024-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': 'c5bb9727a6ba6119', 'authors': ['Zichen Liu', 'Changyu Chen', 'Chao Du', 'Wee Sun Lee', 'Min Lin'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'Singapore Management University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01493.jpg', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#optimization', '#training', '#open_source', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ´ÑƒÑĞ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ½Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ñ… Ğ´ÑƒÑĞ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ°Ğ½Ğ´Ğ¸Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¢Ğ¾Ğ¼Ğ¿ÑĞ¾Ğ½Ğ°. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ SEA (Sample-Efficient Alignment), Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğ¹ ÑÑ‚Ğ¾Ñ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ñ€Ğ°ĞºÑƒĞ»Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°ÑÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ SEA Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ LLM.'}, 'en': {'title': 'Efficiently Aligning LLMs with Human Preferences Using SEA', 'desc': 'This paper explores how to align large language models (LLMs) with human preferences using limited online feedback. It frames the alignment challenge as a contextual dueling bandits problem, which seeks efficient algorithms that can learn from active exploration. The authors propose a new algorithm called SEA (Sample-Efficient Alignment) based on Thompson sampling, which is tested across various model sizes and preference learning methods. The results show that SEA is highly effective in aligning LLMs with human preferences while using fewer samples compared to existing methods.'}, 'zh': {'title': 'é«˜æ•ˆå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åœ¨æœ‰é™çš„åœ¨çº¿åé¦ˆä¸‹é«˜æ•ˆåœ°å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸äººç±»åå¥½å¯¹é½ã€‚æˆ‘ä»¬å°†LLMå¯¹é½é—®é¢˜æ¡†å®šä¸ºä¸Šä¸‹æ–‡å¯¹æŠ—èµŒåšè€…é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ±¤æ™®æ£®é‡‡æ ·çš„ç»Ÿä¸€ç®—æ³•ï¼Œå¹¶åœ¨ä¸¤ä¸ªä¸åŒçš„LLMå¯¹é½åœºæ™¯ä¸­è¿›è¡Œäº†åº”ç”¨ã€‚é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼Œåä¸ºSEAï¼ˆæ ·æœ¬é«˜æ•ˆå¯¹é½ï¼‰çš„å®ç”¨ä»£ç†åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹å’Œåå¥½å­¦ä¹ ç®—æ³•ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¯¹é½æ–¹é¢çš„é«˜æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†SEAçš„å®ç°å’Œé«˜æ•ˆä»£ç åº“ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.01602', 'title': 'DreamPolish: Domain Score Distillation With Progressive Geometry Generation', 'url': 'https://huggingface.co/papers/2411.01602', 'abstract': 'We introduce DreamPolish, a text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on a view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add a surface polishing stage with only a few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find a suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce a novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such a domain. We draw inspiration from the classifier-free guidance (CFG) in textconditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods.', 'score': 9, 'issue_id': 439, 'pub_date': '2024-11-03', 'pub_date_card': {'ru': '3 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 3', 'zh': '11æœˆ3æ—¥'}, 'hash': '403fd08e60540a0a', 'authors': ['Yean Cheng', 'Ziqi Cai', 'Ming Ding', 'Wendi Zheng', 'Shiyu Huang', 'Yuxiao Dong', 'Jie Tang', 'Boxin Shi'], 'affiliations': ['Zhipu AI', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01602.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#3d', '#architecture'], 'emoji': 'ğŸ¨', 'ru': {'title': 'DreamPolish: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸', 'desc': 'DreamPolish - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ ÑƒÑ‚Ğ¾Ğ½Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğµ (DSD), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-image. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DreamPolish ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ñ Ğ¾Ñ‚Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'DreamPolish: Elevating 3D Generation with Refined Geometry and Textures', 'desc': 'DreamPolish is a text-to-3D generation model that focuses on creating high-quality 3D objects with refined geometry and textures. It improves the geometry construction by using multiple neural representations and an additional normal estimator to reduce artifacts caused by limited guidance. The model also introduces a surface polishing stage that requires minimal training to enhance the geometric details further. For texture generation, it employs a novel domain score distillation (DSD) method, inspired by classifier-free guidance, to achieve photorealistic and consistent textures in the generated 3D assets.'}, 'zh': {'title': 'DreamPolishï¼šç”Ÿæˆé«˜è´¨é‡3Dèµ„äº§çš„åˆ›æ–°æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDreamPolishçš„æ–‡æœ¬åˆ°3Dç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆç²¾ç»†çš„å‡ ä½•å½¢çŠ¶å’Œé«˜è´¨é‡çš„çº¹ç†ã€‚åœ¨å‡ ä½•æ„å»ºé˜¶æ®µï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šç§ç¥ç»è¡¨ç¤ºæ¥å¢å¼ºåˆæˆè¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œå¹¶å¼•å…¥é¢å¤–çš„æ³•çº¿ä¼°è®¡å™¨æ¥æ”¹å–„å‡ ä½•ç»†èŠ‚ã€‚çº¹ç†ç”Ÿæˆé˜¶æ®µé‡‡ç”¨äº†ä¸€ç§æ–°çš„è¯„åˆ†è’¸é¦ç›®æ ‡ï¼Œç§°ä¸ºé¢†åŸŸè¯„åˆ†è’¸é¦ï¼ˆDSDï¼‰ï¼Œä»¥å¼•å¯¼ç¥ç»è¡¨ç¤ºæœå‘åŒ…å«çœŸå®æ„Ÿå’Œä¸€è‡´æ€§æ¸²æŸ“çš„é€‚å½“é¢†åŸŸã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆè¡¨é¢å…‰æ»‘ä¸”å…·æœ‰çœŸå®æ„Ÿçº¹ç†çš„3Dèµ„äº§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.03312', 'title': 'Inference Optimal VLMs Need Only One Visual Token but Larger Models', 'url': 'https://huggingface.co/papers/2411.03312', 'abstract': 'Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5-10times), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.', 'score': 6, 'issue_id': 446, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '60954fb0c9d4b5fb', 'authors': ['Kevin Y. Li', 'Sachin Goyal', 'Joao D. Semedo', 'J. Zico Kolter'], 'affiliations': ['Carnegie Mellon University', 'Bosch Center for Artificial Intelligence'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03312.jpg', 'data': {'categories': ['#reasoning', '#cv', '#inference', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞœĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ VLM', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (VLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ»Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ VLM Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑÑ‚Ğ¸Ñ… Ğ´Ğ²ÑƒÑ… Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼ĞµÑ‰Ğ°ĞµÑ‚ÑÑ Ğ² Ğ±ÑĞ´Ğ¶ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ VLM Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Maximizing VLM Efficiency with Token Compression', 'desc': 'This paper explores the efficiency of Vision Language Models (VLMs) in visual understanding tasks, focusing on the trade-off between the number of visual tokens and the size of the language model (LLM). The authors establish scaling laws to determine how performance varies with these two factors, revealing that optimal performance is achieved with minimal visual tokens, often reducing to just one token. They highlight that traditional token reduction methods may not be sufficient for achieving the best inference performance, suggesting that higher compression ratios are necessary. The paper also proposes initial strategies for adapting VLMs to work effectively under these high token compression conditions.'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†ï¼šæœ€å¤§åŒ–LLMä¸æœ€å°åŒ–è§†è§‰æ ‡è®°çš„å¹³è¡¡', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç”±äºå¤„ç†å¤§é‡è¾“å…¥æ ‡è®°ï¼ˆä¸»è¦æ¥è‡ªå›¾åƒï¼‰æ‰€éœ€çš„è®¡ç®—é‡å¤§ï¼Œæ¨ç†å»¶è¿Ÿè¾ƒé«˜ã€‚ä¸ºäº†é™ä½æ¨ç†æˆæœ¬ï¼Œå¯ä»¥é€‰æ‹©ç¼©å°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–å‡å°‘è¾“å…¥å›¾åƒæ ‡è®°çš„æ•°é‡ï¼Œåè€…æ˜¯è®¸å¤šè¿‘æœŸç ”ç©¶çš„é‡ç‚¹ã€‚æˆ‘ä»¬é€šè¿‡å»ºç«‹ç¼©æ”¾æ³•åˆ™æ¥æè¿°è§†è§‰æ ‡è®°æ•°é‡ä¸LLMå‚æ•°ä¹‹é—´çš„æœ€ä½³æƒè¡¡ï¼Œå‘ç°å¯¹äºè§†è§‰æ¨ç†ä»»åŠ¡ï¼Œæœ€ä½³æ¨ç†è¡Œä¸ºæ˜¯åœ¨æ¨ç†é¢„ç®—å†…ä½¿ç”¨æœ€å¤§çš„LLMï¼ŒåŒæ—¶å°†è§†è§‰æ ‡è®°æ•°é‡å‡å°‘åˆ°æœ€å°ï¼Œé€šå¸¸åªéœ€ä¸€ä¸ªæ ‡è®°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè®¡ç®—æœ€ä¼˜çš„æ¨ç†æ¨¡å¼éœ€è¦åœ¨æ›´é«˜çš„æ ‡è®°å‹ç¼©æ¯”ä¸‹è¿›è¡Œæ“ä½œã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.03047', 'title': 'GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details', 'url': 'https://huggingface.co/papers/2411.03047', 'abstract': 'Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches. Project page: https://garverselod.github.io/', 'score': 6, 'issue_id': 444, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': 'eadbb8f8b93d4818', 'authors': ['Zhongjin Luo', 'Haolin Liu', 'Chenghong Li', 'Wanghao Du', 'Zirong Jin', 'Wanhu Sun', 'Yinyu Nie', 'Weikai Chen', 'Xiaoguang Han'], 'affiliations': ['SSE, CUHKSZ, China', 'FNii, CUHKSZ, China', 'Huawei Noahs Ark Lab, UK', 'DCC Algorithm Research Center, Tencent Games, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03047.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#benchmark', '#inference', '#graphs', '#data', '#training', '#dataset', '#open_source', '#3d'], 'emoji': 'ğŸ‘š', 'ru': {'title': 'GarVerseLOD: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'GarVerseLOD - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D-Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 6000 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ñ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ…ÑƒĞ´Ğ¾Ğ¶Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ñ ÑƒÑ€Ğ¾Ğ²Ğ½ÑĞ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ‚ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ Ğ´Ğ¾ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾-Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ.'}, 'en': {'title': 'Revolutionizing 3D Garment Reconstruction with GarVerseLOD', 'desc': "This paper introduces GarVerseLOD, a new dataset and framework aimed at improving 3D garment reconstruction from single images. The authors highlight the challenges faced by existing methods in generalizing to unseen images with complex clothing and poses. By collecting 6,000 high-quality cloth models and organizing them into a hierarchical dataset with varying levels of detail, they enhance the model's ability to learn and generalize. Additionally, they employ a novel labeling approach using conditional diffusion models to create realistic paired images, resulting in superior garment reconstruction quality compared to previous methods."}, 'zh': {'title': 'é«˜ä¿çœŸ3Dæœè£…é‡å»ºçš„æ–°çªç ´', 'desc': 'ç¥ç»éšå¼å‡½æ•°åœ¨ä»å¤šå¼ æˆ–å•å¼ å›¾åƒä¸­æ•°å­—åŒ–ç©¿è¡£äººç±»æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„å¸ƒæ–™å˜å½¢å’Œèº«ä½“å§¿åŠ¿çš„æœªè§å›¾åƒæ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GarVerseLODï¼Œä¸€ä¸ªæ–°çš„æ•°æ®é›†å’Œæ¡†æ¶ï¼Œæ—¨åœ¨ä»å•å¼ æ— çº¦æŸå›¾åƒä¸­å®ç°é«˜ä¿çœŸåº¦çš„3Dæœè£…é‡å»ºã€‚é€šè¿‡æ”¶é›†6000ä¸ªé«˜è´¨é‡çš„å¸ƒæ–™æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨åˆ†å±‚ç»†èŠ‚çš„æ–¹å¼ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ¨ç†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02657', 'title': 'Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge', 'url': 'https://huggingface.co/papers/2411.02657', 'abstract': "Rare diseases present unique challenges in healthcare, often suffering from delayed diagnosis and fragmented information landscapes. The scarcity of reliable knowledge in these conditions poses a distinct challenge for Large Language Models (LLMs) in supporting clinical management and delivering precise patient information underscoring the need for focused training on these 'zebra' cases. We present Zebra-Llama, a specialized context-aware language model with high precision Retrieval Augmented Generation (RAG) capability, focusing on Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000 individuals, exemplifies the complexities of rare diseases with its diverse symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing a novel context-aware fine-tuning methodology trained on questions derived from medical literature, patient experiences, and clinical resources, along with expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities in handling EDS-related queries. On a test set of real-world questions collected from EDS patients and clinicians, medical experts evaluated the responses generated by both models, revealing Zebra-Llama's substantial improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama not only provides more accessible and reliable EDS information but also establishes a framework for developing specialized AI solutions for other rare conditions. This work represents a crucial step towards democratizing expert-level knowledge in rare disease management, potentially transforming how healthcare providers and patients navigate the complex landscape of rare diseases.", 'score': 5, 'issue_id': 439, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '26c0b7bc39488945', 'authors': ['Karthik Soman', 'Andrew Langdon', 'Catalina Villouta', 'Chinmay Agrawal', 'Lashaw Salta', 'Braian Peetoom', 'Gianmarco Bellucci', 'Orion J Buske'], 'affiliations': ['Dept. of Neurology, University of California San Francisco, California, USA', 'Independent Researcher', 'Plix AI, California, USA', 'Dept. of Neurosciences, Mental Health and Sensory Organs, Sapienza University of Rome, Italy', 'PhenoTips, Toronto, Canada'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02657.jpg', 'data': {'categories': ['#science', '#rag', '#healthcare', '#training', '#open_source'], 'emoji': 'ğŸ¦“', 'ru': {'title': 'Zebra-Llama: Ğ˜Ğ˜-ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ñ€ĞµĞ´ĞºĞ¸Ğ¼ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Zebra-Llama - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ğ´Ñ€Ğ¾Ğ¼ Ğ­Ğ»ĞµÑ€ÑĞ°-Ğ”Ğ°Ğ½Ğ»Ğ¾ÑĞ° (EDS) ĞºĞ°Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG) Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ EDS. Zebra-Llama Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğµ, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğµ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ²Ñ€Ğ°Ñ‡ĞµĞ¹. Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹, Ğ´ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Zebra-Llama: Transforming Rare Disease Management with AI', 'desc': 'This paper introduces Zebra-Llama, a specialized language model designed to improve the management of rare diseases, specifically Ehlers-Danlos Syndrome (EDS). It addresses the challenges posed by limited information and delayed diagnoses in rare conditions by utilizing a context-aware fine-tuning approach. The model employs Retrieval Augmented Generation (RAG) to enhance the precision of responses to EDS-related queries, outperforming the base model in various evaluation metrics. By making Zebra-Llama an open-source resource, the authors aim to democratize access to expert knowledge in rare disease management, paving the way for similar advancements in other rare conditions.'}, 'zh': {'title': 'Zebra-Llamaï¼šç½•è§ç–¾ç—…ç®¡ç†çš„æ–°çªç ´', 'desc': 'ç½•è§ç–¾ç—…åœ¨åŒ»ç–—ä¿å¥ä¸­é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¸¸å¸¸å¯¼è‡´è¯Šæ–­å»¶è¿Ÿå’Œä¿¡æ¯ç¢ç‰‡åŒ–ã€‚é’ˆå¯¹è¿™äº›ç½•è§ç—…ä¾‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºZebra-Llamaçš„ä¸“é—¨è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡é«˜ç²¾åº¦çš„æ£€ç´¢å¢å¼ºç”Ÿæˆèƒ½åŠ›ï¼Œé‡ç‚¹å…³æ³¨Ehlers-Danlosç»¼åˆç—‡ï¼ˆEDSï¼‰ã€‚é€šè¿‡ä¸€ç§æ–°é¢–çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¾®è°ƒæ–¹æ³•ï¼ŒZebra-Llamaåœ¨å¤„ç†ä¸EDSç›¸å…³çš„é—®é¢˜æ—¶è¡¨ç°å‡ºå‰æ‰€æœªæœ‰çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†å›ç­”çš„å…¨é¢æ€§ã€å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦ã€‚è¯¥æ¨¡å‹ä½œä¸ºå¼€æºèµ„æºå‘å¸ƒï¼Œä¸ä»…æä¾›äº†æ›´æ˜“è·å–å’Œå¯é çš„EDSä¿¡æ¯ï¼Œè¿˜ä¸ºå…¶ä»–ç½•è§ç–¾ç—…å¼€å‘ä¸“é—¨çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆå¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02844', 'title': 'Correlation of Object Detection Performance with Visual Saliency and Depth Estimation', 'url': 'https://huggingface.co/papers/2411.02844', 'abstract': "As object detection techniques continue to evolve, understanding their relationships with complementary visual tasks becomes crucial for optimising model architectures and computational resources. This paper investigates the correlations between object detection accuracy and two fundamental visual tasks: depth prediction and visual saliency prediction. Through comprehensive experiments using state-of-the-art models (DeepGaze IIE, Depth Anything, DPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that visual saliency shows consistently stronger correlations with object detection accuracy (mArho up to 0.459 on Pascal VOC) compared to depth prediction (mArho up to 0.283). Our analysis reveals significant variations in these correlations across object categories, with larger objects showing correlation values up to three times higher than smaller objects. These findings suggest incorporating visual saliency features into object detection architectures could be more beneficial than depth information, particularly for specific object categories. The observed category-specific variations also provide insights for targeted feature engineering and dataset design improvements, potentially leading to more efficient and accurate object detection systems.", 'score': 3, 'issue_id': 444, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': 'd17ec72bcba1cd05', 'authors': ['Matthias Bartolo', 'Dylan Seychell'], 'affiliations': ['Dept. of Artificial Intelligence University of Malta'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02844.jpg', 'data': {'categories': ['#cv', '#graphs', '#optimization', '#dataset', '#architecture'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… COCO Ğ¸ Pascal VOC Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸ÑÑ… Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ñ‡ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ¾ Ñ‚Ñ€ĞµÑ… Ñ€Ğ°Ğ· Ğ²Ñ‹ÑˆĞµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼, Ñ‡ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ.'}, 'en': {'title': 'Enhancing Object Detection with Visual Saliency Insights', 'desc': 'This paper explores how object detection accuracy relates to two other visual tasks: depth prediction and visual saliency prediction. The authors conducted experiments with advanced models on popular datasets and found that visual saliency has a stronger correlation with object detection accuracy than depth prediction. They noted that larger objects tend to show much higher correlation values compared to smaller ones. The results suggest that integrating visual saliency features into object detection models could enhance performance, especially for certain object categories.'}, 'zh': {'title': 'è§†è§‰æ˜¾è‘—æ€§åŠ©åŠ›ç‰©ä½“æ£€æµ‹ç²¾åº¦æå‡', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†ç‰©ä½“æ£€æµ‹ç²¾åº¦ä¸æ·±åº¦é¢„æµ‹å’Œè§†è§‰æ˜¾è‘—æ€§é¢„æµ‹è¿™ä¸¤ç§åŸºæœ¬è§†è§‰ä»»åŠ¡ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡åœ¨COCOå’ŒPascal VOCæ•°æ®é›†ä¸Šä½¿ç”¨å…ˆè¿›æ¨¡å‹è¿›è¡Œå…¨é¢å®éªŒï¼Œå‘ç°è§†è§‰æ˜¾è‘—æ€§ä¸ç‰©ä½“æ£€æµ‹ç²¾åº¦çš„ç›¸å…³æ€§æ˜æ˜¾é«˜äºæ·±åº¦é¢„æµ‹ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œä¸åŒç‰©ä½“ç±»åˆ«ä¹‹é—´çš„ç›¸å…³æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¤§å‹ç‰©ä½“çš„ç›¸å…³æ€§å€¼å¯é«˜è¾¾å°å‹ç‰©ä½“çš„ä¸‰å€ã€‚ç»“æœè¡¨æ˜ï¼Œå°†è§†è§‰æ˜¾è‘—æ€§ç‰¹å¾èå…¥ç‰©ä½“æ£€æµ‹æ¶æ„å¯èƒ½æ¯”æ·±åº¦ä¿¡æ¯æ›´æœ‰åˆ©ï¼Œå°¤å…¶æ˜¯åœ¨ç‰¹å®šç‰©ä½“ç±»åˆ«ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.11844', 'title': 'Generative World Explorer', 'url': 'https://huggingface.co/papers/2411.11844', 'abstract': 'Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state.In contrast, humans can imagine unseen parts of the world through a mental exploration and revise their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the Generative World Explorer (Genex), an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train Genex, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) Genex can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.', 'score': 34, 'issue_id': 655, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': 'ad359ab18e626959', 'authors': ['Taiming Lu', 'Tianmin Shu', 'Alan Yuille', 'Daniel Khashabi', 'Jieneng Chen'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2411.11844.jpg', 'data': {'categories': ['#agents', '#3d', '#synthetic', '#games', '#dataset', '#long_context'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑ‹ÑĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² embodied AI. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D Ğ¼Ğ¸Ñ€ Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ²Ğ¾Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ½Ñ‘Ğ¼ Ğ±ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Generative World Explorer (Genex), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Genex Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Imagine to Explore: Enhancing Decision-Making with Mental Simulations', 'desc': 'This paper addresses the challenge of planning in environments where an agent has incomplete information. Unlike traditional methods that rely on physical exploration, the authors propose a framework called Generative World Explorer (Genex) that enables agents to mentally simulate and explore their surroundings. By generating imagined observations, Genex allows agents to update their beliefs about the world without needing to physically navigate it. The results show that this approach leads to improved decision-making in complex environments, demonstrating the potential of mental exploration in embodied AI.'}, 'zh': {'title': 'å¿ƒç†æ¢ç´¢ï¼Œæ™ºèƒ½å†³ç­–çš„æ–°æ–¹å¼', 'desc': 'åœ¨å…·èº«äººå·¥æ™ºèƒ½ä¸­ï¼Œéƒ¨åˆ†è§‚å¯Ÿçš„è§„åˆ’æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç ”ç©¶é€šè¿‡è®©æ™ºèƒ½ä½“ç‰©ç†æ¢ç´¢ç¯å¢ƒæ¥æ›´æ–°å¯¹ä¸–ç•ŒçŠ¶æ€çš„ä¿¡å¿µï¼Œè€Œæˆ‘ä»¬æå‡ºçš„ç”Ÿæˆä¸–ç•Œæ¢ç´¢å™¨ï¼ˆGenexï¼‰åˆ™å…è®¸æ™ºèƒ½ä½“é€šè¿‡å¿ƒç†æ¢ç´¢æ¥æƒ³è±¡æœªè§çš„ä¸–ç•Œéƒ¨åˆ†ã€‚Genexèƒ½å¤Ÿåœ¨å¤§å‹3Dä¸–ç•Œä¸­ç”Ÿæˆæƒ³è±¡çš„è§‚å¯Ÿï¼Œä»è€Œæ›´æ–°ä¿¡å¿µï¼Œå¸®åŠ©æ™ºèƒ½ä½“åœ¨å½“å‰æ­¥éª¤åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåˆæˆåŸå¸‚åœºæ™¯æ•°æ®é›†Genex-DBï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†Genexåœ¨é•¿æ—¶é—´æ¢ç´¢ä¸­çš„é«˜è´¨é‡è§‚å¯Ÿç”Ÿæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10640', 'title': 'BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices', 'url': 'https://huggingface.co/papers/2411.10640', 'abstract': 'The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with leq 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B).', 'score': 29, 'issue_id': 652, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': '0366549d4347dfd2', 'authors': ['Xudong Lu', 'Yinghao Chen', 'Cheng Chen', 'Hui Tan', 'Boheng Chen', 'Yina Xie', 'Rui Hu', 'Guanxin Tan', 'Renshou Wu', 'Yan Hu', 'Yi Zeng', 'Lei Wu', 'Liuyang Bian', 'Zhaoxiong Wang', 'Long Liu', 'Yanzhou Yang', 'Han Xiao', 'Aojun Zhou', 'Yafei Wen', 'Xiaoxin Chen', 'Shuai Ren', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.10640.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#small_models', '#multimodal', '#inference', '#optimization'], 'emoji': 'ğŸ“±', 'ru': {'title': 'BlueLM-V-3B: ĞœĞ¾Ñ‰ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ²Ğ°ÑˆĞµĞ¼ ĞºĞ°Ñ€Ğ¼Ğ°Ğ½Ğµ', 'desc': 'BlueLM-V-3B - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ¾Ğ², Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (3,1 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²), Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (24,4 Ñ‚Ğ¾ĞºĞµĞ½Ğ°/Ñ) Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. BlueLM-V-3B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ˜Ğ˜-Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½ÑƒÑ Ğ¶Ğ¸Ğ·Ğ½ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Efficient MLLMs for Mobile: BlueLM-V-3B Unleashed!', 'desc': 'This paper introduces BlueLM-V-3B, a multimodal large language model (MLLM) designed for efficient deployment on mobile devices. It addresses the challenges of limited memory and computational power by optimizing model inference through a redesigned dynamic resolution scheme and hardware-aware system optimizations. BlueLM-V-3B features a compact architecture with 2.7 billion parameters for the language model and 400 million for the vision encoder, ensuring fast processing speeds. The model achieves impressive performance metrics, outperforming larger models on benchmarks while maintaining a generation speed of 24.4 tokens per second.'}, 'zh': {'title': 'é«˜æ•ˆéƒ¨ç½²å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBlueLM-V-3Bçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨é«˜æ•ˆåœ°åœ¨ç§»åŠ¨å¹³å°ä¸Šéƒ¨ç½²ã€‚è¯¥æ¨¡å‹å…·æœ‰2.7äº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹å’Œ4äº¿å‚æ•°çš„è§†è§‰ç¼–ç å™¨ï¼Œèƒ½å¤Ÿåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®ç°å¿«é€Ÿç”Ÿæˆã€‚é€šè¿‡é‡æ–°è®¾è®¡åŠ¨æ€åˆ†è¾¨ç‡æ–¹æ¡ˆå’Œè¿›è¡Œç¡¬ä»¶ä¼˜åŒ–ï¼ŒBlueLM-V-3Båœ¨MediaTek Dimensity 9300å¤„ç†å™¨ä¸Šè¾¾åˆ°äº†æ¯ç§’24.4ä¸ªæ ‡è®°çš„ç”Ÿæˆé€Ÿåº¦ã€‚è¯¥æ¨¡å‹åœ¨OpenCompassåŸºå‡†æµ‹è¯•ä¸­è·å¾—äº†66.1çš„æœ€é«˜å¹³å‡åˆ†ï¼Œè¶…è¶Šäº†è®¸å¤šå‚æ•°æ›´å¤§çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.11504', 'title': 'Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering', 'url': 'https://huggingface.co/papers/2411.11504', 'abstract': 'The evolution of machine learning has increasingly prioritized the development of powerful models and more scalable supervision signals. However, the emergence of foundation models presents significant challenges in providing effective supervision signals necessary for further enhancing their capabilities. Consequently, there is an urgent need to explore novel supervision signals and technical approaches. In this paper, we propose verifier engineering, a novel post-training paradigm specifically designed for the era of foundation models. The core of verifier engineering involves leveraging a suite of automated verifiers to perform verification tasks and deliver meaningful feedback to foundation models. We systematically categorize the verifier engineering process into three essential stages: search, verify, and feedback, and provide a comprehensive review of state-of-the-art research developments within each stage. We believe that verifier engineering constitutes a fundamental pathway toward achieving Artificial General Intelligence.', 'score': 12, 'issue_id': 654, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': 'a48ecb5e8a1da0ae', 'authors': ['Xinyan Guan', 'Yanjiang Liu', 'Xinyu Lu', 'Boxi Cao', 'Ben He', 'Xianpei Han', 'Le Sun', 'Jie Lou', 'Bowen Yu', 'Yaojie Lu', 'Hongyu Lin'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2411.11504.jpg', 'data': {'categories': ['#rlhf', '#survey', '#training', '#agi'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ’ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ 'Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸' - Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½ Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¿Ğ¾Ğ¸ÑĞº, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ½Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°."}, 'en': {'title': 'Unlocking Foundation Models with Verifier Engineering', 'desc': 'This paper discusses the challenges of providing effective supervision signals for foundation models in machine learning. It introduces a new approach called verifier engineering, which uses automated verifiers to enhance the capabilities of these models. The process is divided into three stages: search, verify, and feedback, each aimed at improving model performance. The authors argue that this method is crucial for advancing towards Artificial General Intelligence.'}, 'zh': {'title': 'éªŒè¯å™¨å·¥ç¨‹ï¼šè¿ˆå‘äººå·¥é€šç”¨æ™ºèƒ½çš„æ–°è·¯å¾„', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨åŸºç¡€æ¨¡å‹æ—¶ä»£ï¼Œå¦‚ä½•æä¾›æœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ä»¥æå‡æ¨¡å‹èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åè®­ç»ƒèŒƒå¼â€”â€”éªŒè¯å™¨å·¥ç¨‹ï¼Œæ—¨åœ¨åˆ©ç”¨è‡ªåŠ¨åŒ–éªŒè¯å™¨è¿›è¡ŒéªŒè¯ä»»åŠ¡ï¼Œå¹¶ä¸ºåŸºç¡€æ¨¡å‹æä¾›æœ‰æ„ä¹‰çš„åé¦ˆã€‚éªŒè¯å™¨å·¥ç¨‹çš„è¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªå…³é”®é˜¶æ®µï¼šæœç´¢ã€éªŒè¯å’Œåé¦ˆï¼Œå¹¶å¯¹æ¯ä¸ªé˜¶æ®µçš„æœ€æ–°ç ”ç©¶è¿›å±•è¿›è¡Œäº†ç³»ç»Ÿæ€§å›é¡¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒéªŒè¯å™¨å·¥ç¨‹æ˜¯å®ç°äººå·¥é€šç”¨æ™ºèƒ½çš„é‡è¦é€”å¾„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.09944', 'title': 'SlimLM: An Efficient Small Language Model for On-Device Document Assistance', 'url': 'https://huggingface.co/papers/2411.09944', 'abstract': 'While small language models (SLMs) show promises for mobile deployment, their real-world performance and applications on smartphones remains underexplored. We present SlimLM, a series of SLMs optimized for document assistance tasks on mobile devices. Through extensive experiments on a Samsung Galaxy S24, we identify the optimal trade-offs between model size (ranging from 125M to 7B parameters), context length, and inference time for efficient on-device processing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on DocAssist, our constructed dataset for summarization, question answering and suggestion tasks. Our smallest model demonstrates efficient performance on S24, while larger variants offer enhanced capabilities within mobile constraints. We evaluate SlimLM against existing SLMs, showing comparable or superior performance and offering a benchmark for future research in on-device language models. We also provide an Android application, offering practical insights into SLM deployment. Our findings provide valuable insights and illuminate the capabilities of running advanced language models on high-end smartphones, potentially reducing server costs and enhancing privacy through on-device processing.', 'score': 10, 'issue_id': 655, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': 'ec53cf3813914219', 'authors': ['Thang M. Pham', 'Phat T. Nguyen', 'Seunghyun Yoon', 'Viet Dac Lai', 'Franck Dernoncourt', 'Trung Bui'], 'affiliations': ['Adobe Research', 'Auburn University', 'Georgia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2411.09944.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#small_models', '#open_source', '#dataset', '#benchmark'], 'emoji': 'ğŸ“±', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ²Ğ°ÑˆĞµĞ¼ ĞºĞ°Ñ€Ğ¼Ğ°Ğ½Ğµ', 'desc': 'SlimLM - ÑÑ‚Ğ¾ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ¾Ñ‚ 125 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ¾ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Samsung Galaxy S24. SlimLM Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ SlimPajama-627B Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DocAssist Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ°Ñ€Ñ‚Ñ„Ğ¾Ğ½Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑĞµÑ€Ğ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğµ.'}, 'en': {'title': 'SlimLM: Efficient Language Models for Mobile Document Assistance', 'desc': 'This paper introduces SlimLM, a series of small language models designed specifically for mobile devices, focusing on document assistance tasks. The models are optimized for performance on smartphones, balancing size, context length, and inference time to ensure efficient on-device processing. SlimLM is pre-trained on a large dataset and fine-tuned for tasks like summarization and question answering, demonstrating effective performance even with the smallest model. The research highlights the potential of deploying advanced language models on smartphones, which can lower server costs and improve user privacy.'}, 'zh': {'title': 'SlimLMï¼šç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜æ•ˆè¯­è¨€æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSlimLMçš„å°å‹è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä¸“ä¸ºç§»åŠ¨è®¾å¤‡ä¸Šçš„æ–‡æ¡£è¾…åŠ©ä»»åŠ¡ä¼˜åŒ–ã€‚æˆ‘ä»¬åœ¨ä¸‰æ˜ŸGalaxy S24ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œæ‰¾åˆ°äº†æ¨¡å‹å¤§å°ã€ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ¨ç†æ—¶é—´ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ï¼Œä»¥å®ç°é«˜æ•ˆçš„æœ¬åœ°å¤„ç†ã€‚SlimLMåœ¨SlimPajama-627Bä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨æˆ‘ä»¬æ„å»ºçš„DocAssistæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ”¯æŒæ‘˜è¦ã€é—®ç­”å’Œå»ºè®®ç­‰ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒSlimLMåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé™ä½æœåŠ¡å™¨æˆæœ¬å¹¶å¢å¼ºéšç§ä¿æŠ¤ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10836', 'title': 'AnimateAnything: Consistent and Controllable Animation for Video Generation', 'url': 'https://huggingface.co/papers/2411.10836', 'abstract': "We present a unified controllable video generation approach AnimateAnything that facilitates precise and consistent video manipulation across various conditions, including camera trajectories, text prompts, and user motion annotations. Specifically, we carefully design a multi-scale control feature fusion network to construct a common motion representation for different conditions. It explicitly converts all control information into frame-by-frame optical flows. Then we incorporate the optical flows as motion priors to guide final video generation. In addition, to reduce the flickering issues caused by large-scale motion, we propose a frequency-based stabilization module. It can enhance temporal coherence by ensuring the video's frequency domain consistency. Experiments demonstrate that our method outperforms the state-of-the-art approaches. For more details and videos, please refer to the webpage: https://yu-shaonian.github.io/Animate_Anything/.", 'score': 10, 'issue_id': 655, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': 'b979f7de4cf79a50', 'authors': ['Guojun Lei', 'Chi Wang', 'Hong Li', 'Rong Zhang', 'Yikai Wang', 'Weiwei Xu'], 'affiliations': ['Beihang University', 'State Key Lab of CAD&CG, Zhejiang University', 'Tsinghua University', 'Zhejiang Gongshang University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10836.jpg', 'data': {'categories': ['#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ¹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'AnimateAnything - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ²ÑÑ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¸ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ°Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ€Ñ†Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'AnimateAnything: Mastering Video Generation with Precision Control', 'desc': 'The paper introduces AnimateAnything, a method for generating videos that allows for detailed control over various aspects like camera movement and user inputs. It uses a multi-scale control feature fusion network to create a unified motion representation that can adapt to different conditions. By converting control information into optical flows, the method guides the video generation process effectively. Additionally, a frequency-based stabilization module is implemented to minimize flickering and improve the smoothness of the final video output.'}, 'zh': {'title': 'ç»Ÿä¸€å¯æ§è§†é¢‘ç”Ÿæˆï¼Œç²¾å‡†æ“æ§æ¯ä¸€å¸§', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¯æ§è§†é¢‘ç”Ÿæˆæ–¹æ³•AnimateAnythingï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ¡ä»¶ä¸‹å®ç°ç²¾ç¡®å’Œä¸€è‡´çš„è§†é¢‘æ“æ§ï¼ŒåŒ…æ‹¬ç›¸æœºè½¨è¿¹ã€æ–‡æœ¬æç¤ºå’Œç”¨æˆ·è¿åŠ¨æ³¨é‡Šã€‚è¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ä¸ªå¤šå°ºåº¦æ§åˆ¶ç‰¹å¾èåˆç½‘ç»œï¼Œä»¥æ„å»ºä¸åŒæ¡ä»¶ä¸‹çš„å…±åŒè¿åŠ¨è¡¨ç¤ºã€‚å®ƒå°†æ‰€æœ‰æ§åˆ¶ä¿¡æ¯æ˜¾å¼è½¬æ¢ä¸ºé€å¸§çš„å…‰æµï¼Œå¹¶å°†å…‰æµä½œä¸ºè¿åŠ¨å…ˆéªŒæ¥æŒ‡å¯¼æœ€ç»ˆçš„è§†é¢‘ç”Ÿæˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºé¢‘ç‡çš„ç¨³å®šæ¨¡å—ï¼Œä»¥å‡å°‘å¤§è§„æ¨¡è¿åŠ¨å¼•èµ·çš„é—ªçƒé—®é¢˜ï¼Œä»è€Œå¢å¼ºè§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07641', 'title': 'Top-$nÏƒ$: Not All Logits Are You Need', 'url': 'https://huggingface.co/papers/2411.07641', 'abstract': 'Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.', 'score': 10, 'issue_id': 651, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': 'd3439bf0c336ac57', 'authors': ['Chenxia Tang', 'Jianchun Liu', 'Hongli Xu', 'Liusheng Huang'], 'affiliations': ['School of Computer Science and Technology, University of Science and Technology of China', 'Suzhou Institute for Advanced Research, University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2411.07641.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#reasoning'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Top-nsigma: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ top-nsigma. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ ÑĞ¾Ñ„Ñ‚Ğ¼Ğ°ĞºÑĞ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Top-nsigma Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¶Ğ°Ğ´Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs', 'desc': 'This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures.'}, 'zh': {'title': 'çªç ´ä¼ ç»Ÿï¼Œæå‡æ¨ç†æ€§èƒ½çš„top-nsigmaæ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡‡æ ·æ–¹æ³•top-nsigmaï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨é¢„è½¯æœ€å¤§å€¼çš„logitsä¸Šæ“ä½œï¼Œé€šè¿‡ç»Ÿè®¡é˜ˆå€¼æ¥è¿›è¡Œæœ‰æ•ˆçš„ä»¤ç‰Œè¿‡æ»¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œlogitså¯ä»¥è‡ªç„¶åœ°åˆ†ä¸ºé«˜æ–¯åˆ†å¸ƒçš„å™ªå£°åŒºåŸŸå’Œä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œä»è€Œé¿å…äº†å¤æ‚çš„æ¦‚ç‡æ“ä½œã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œtop-nsigmaåœ¨å¤šä¸ªæ¨ç†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ç°æœ‰çš„é‡‡æ ·æ–¹æ³•å’Œè´ªå©ªè§£ç ï¼Œä¸”åœ¨é«˜æ¸©åº¦ä¸‹ä»èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10510', 'title': 'SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.10510', 'abstract': 'Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models.', 'score': 8, 'issue_id': 654, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': '991f548fec1ec8c9', 'authors': ['Joseph Liu', 'Joshua Geddes', 'Ziyu Guo', 'Haomiao Jiang', 'Mahesh Kumar Nandwana'], 'affiliations': ['Queens University', 'Roblox'], 'pdf_title_img': 'assets/pdf/title_img/2411.10510.jpg', 'data': {'categories': ['#inference', '#multimodal', '#video', '#optimization', '#audio', '#diffusion'], 'emoji': 'âš¡', 'ru': {'title': 'SmoothCache: Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ ÑƒĞ¼Ğ½Ñ‹Ğ¼ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ DiT', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SmoothCache - Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Diffusion Transformers (DiT). SmoothCache Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»Ğ¾ĞµĞ² Ğ½Ğ° ÑĞ¾ÑĞµĞ´Ğ½Ğ¸Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾ĞµĞ² Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SmoothCache Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ 8% Ğ´Ğ¾ 71% Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ´Ğ°Ğ¶Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Accelerating Diffusion Transformers with SmoothCache', 'desc': 'This paper presents SmoothCache, a technique designed to speed up the inference process of Diffusion Transformers (DiT), which are used for generating images, videos, and audio. The traditional inference method is slow because it requires many evaluations of complex attention and feed-forward layers. SmoothCache improves efficiency by caching and reusing similar outputs from adjacent diffusion timesteps, reducing the need for repeated calculations. Experiments show that this method can accelerate inference by 8% to 71% while maintaining or enhancing the quality of the generated content.'}, 'zh': {'title': 'SmoothCacheï¼šåŠ é€Ÿæ‰©æ•£å˜æ¢å™¨çš„æ¨ç†è¿‡ç¨‹', 'desc': 'æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒã€è§†é¢‘å’Œè¯­éŸ³åˆæˆç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ¨ç†è¿‡ç¨‹è®¡ç®—å¼€é”€è¾ƒå¤§ï¼Œå› ä¸ºéœ€è¦é‡å¤è¯„ä¼°èµ„æºå¯†é›†å‹çš„æ³¨æ„åŠ›å’Œå‰é¦ˆæ¨¡å—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SmoothCacheï¼Œè¿™æ˜¯ä¸€ç§ä¸æ¨¡å‹æ— å…³çš„æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œåˆ©ç”¨ç›¸é‚»æ‰©æ•£æ—¶é—´æ­¥ä¹‹é—´å±‚è¾“å‡ºçš„é«˜åº¦ç›¸ä¼¼æ€§ã€‚é€šè¿‡åˆ†æå°å‹æ ¡å‡†é›†ä¸­çš„å±‚çº§è¡¨ç¤ºè¯¯å·®ï¼ŒSmoothCacheè‡ªé€‚åº”åœ°ç¼“å­˜å’Œé‡ç”¨å…³é”®ç‰¹å¾ï¼Œä»è€Œåœ¨ä¿æŒæˆ–æé«˜ç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†8%åˆ°71%çš„é€Ÿåº¦æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.11767', 'title': 'Drowning in Documents: Consequences of Scaling Reranker Inference', 'url': 'https://huggingface.co/papers/2411.11767', 'abstract': 'Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. We challenge this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Our experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. We hope that our findings will spur future research to improve reranking.', 'score': 8, 'issue_id': 652, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '3fa06087787bc8d4', 'authors': ['Mathew Jacob', 'Erik Lindgren', 'Matei Zaharia', 'Michael Carbin', 'Omar Khattab', 'Andrew Drozdov'], 'affiliations': ['Databricks', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2411.11767.jpg', 'data': {'categories': ['#benchmark', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² (rerankers) Ğ² Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ…ÑƒĞ´ÑˆĞ°ĞµÑ‚ÑÑ. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ±ĞµĞ· Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ ÑĞ¾Ğ¼Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² Ğ½Ğ°Ğ´ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Rethinking Rerankers: Diminishing Returns in Document Scoring', 'desc': 'This paper investigates the effectiveness of rerankers, specifically cross-encoders, in the context of information retrieval (IR). Traditionally, rerankers are believed to enhance the quality of document scoring after an initial retrieval phase. However, the authors find that as more documents are scored, the performance of these rerankers diminishes and can even lead to poorer results. The study highlights that rerankers may assign high scores to irrelevant documents, suggesting a need for improved methods in reranking processes.'}, 'zh': {'title': 'é‡æ’åºå™¨çš„æœ‰æ•ˆæ€§éœ€é‡æ–°å®¡è§†', 'desc': 'æœ¬æ–‡æ¢è®¨äº†é‡æ’åºå™¨ï¼ˆé€šå¸¸æ˜¯äº¤å‰ç¼–ç å™¨ï¼‰åœ¨ä¿¡æ¯æ£€ç´¢ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡æµ‹é‡é‡æ’åºå™¨åœ¨å®Œæ•´æ£€ç´¢ä¸­çš„è¡¨ç°ï¼ŒæŒ‘æˆ˜äº†å®ƒä»¬åœ¨åˆæ­¥æ£€ç´¢åé‡æ–°è¯„åˆ†çš„å‡è®¾ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„é‡æ’åºå™¨åœ¨è¯„åˆ†è¶Šæ¥è¶Šå¤šçš„æ–‡æ¡£æ—¶ï¼Œæ•ˆæœé€æ¸å‡å¼±ï¼Œç”šè‡³åœ¨æŸä¸ªé™åº¦åè´¨é‡ä¸‹é™ã€‚æˆ‘ä»¬çš„å‘ç°å¸Œæœ›èƒ½æ¿€åŠ±æœªæ¥çš„ç ”ç©¶ï¼Œä»¥æ”¹è¿›é‡æ’åºæŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.11171', 'title': 'LLÃ¤Mmlein: Compact and Competitive German-Only Language Models from Scratch', 'url': 'https://huggingface.co/papers/2411.11171', 'abstract': 'We create two German-only decoder models, LL\\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models\' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models\' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.', 'score': 7, 'issue_id': 656, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '3eea3fe6bac7ab51', 'authors': ['Jan Pfister', 'Julia Wunderle', 'Andreas Hotho'], 'affiliations': ['Data Science Chair Center for Artificial Intelligence and Data Science (CAIDAS) Julius-Maximilians-UniversitÃ¤t WÃ¼rzburg (JMU)'], 'pdf_title_img': 'assets/pdf/title_img/2411.11171.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#low_resource', '#open_source', '#training', '#multilingual'], 'emoji': 'ğŸ‡©ğŸ‡ª', 'ru': {'title': 'ĞĞµĞ¼ĞµÑ†ĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¾Ñ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ñƒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ½Ğ° Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ: LL"aMmlein 120M Ğ¸ 1B. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ²ĞºĞ»ÑÑ‡Ğ°Ğ» Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SuperGLEBer, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Empowering German NLP with LL"aMmlein Models', 'desc': 'This paper presents two German-only decoder models, LL"aMmlein 120M and 1B, developed from scratch for the German NLP community. The training process included data preprocessing, a custom tokenizer, and evaluation against benchmarks like SuperGLEBer. The models demonstrated competitive performance, matching or exceeding state-of-the-art models of similar sizes. Insights from the training revealed that while model quality improves with size, some tasks show early performance plateaus, guiding future resource allocation in model development.'}, 'zh': {'title': 'å¾·è¯­è§£ç å™¨æ¨¡å‹çš„åˆ›æ–°ä¸å…±äº«', 'desc': 'æˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªä»…æ”¯æŒå¾·è¯­çš„è§£ç å™¨æ¨¡å‹ï¼ŒLL"aMmlein 120Må’Œ1Bï¼Œå¹¶å°†å…¶è®­ç»ƒæ•°æ®å…¬å¼€ï¼Œä¾›å¾·è¯­è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ç¤¾åŒºä½¿ç”¨ã€‚æ¨¡å‹è®­ç»ƒåŒ…æ‹¬å¤šä¸ªå…³é”®æ­¥éª¤ï¼Œå¦‚æ•°æ®é¢„å¤„ç†ã€è‡ªå®šä¹‰å¾·è¯­åˆ†è¯å™¨çš„åˆ›å»ºã€å®é™…è®­ç»ƒä»¥åŠåœ¨å„ç§åŸºå‡†ä¸Šçš„æœ€ç»ˆæ¨¡å‹è¯„ä¼°ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¿å­˜å¹¶åˆ†æäº†å¤šä¸ªæ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨SuperGLEBeråŸºå‡†ç›‘æµ‹æ¨¡å‹çš„å­¦ä¹ åŠ¨æ€ã€‚ä¸SuperGLEBeråŸºå‡†ä¸Šçš„æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼Œä¸¤ä¸ªLL"aMmleinæ¨¡å‹è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå§‹ç»ˆä¸ç›¸ä¼¼å‚æ•°å¤§å°çš„æ¨¡å‹ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10669', 'title': 'Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts', 'url': 'https://huggingface.co/papers/2411.10669', 'abstract': 'As the research of Multimodal Large Language Models (MLLMs) becomes popular, an advancing MLLM model is typically required to handle various textual and visual tasks (e.g., VQA, Detection, OCR, and ChartQA) simultaneously for real-world applications. However, due to the significant differences in representation and distribution among data from various tasks, simply mixing data of all tasks together leads to the well-known``multi-task conflict" issue, resulting in performance degradation across various tasks. To address this issue, we propose Awaker2.5-VL, a Mixture of Experts~(MoE) architecture suitable for MLLM, which acquires the multi-task capabilities through multiple sparsely activated experts. To speed up the training and inference of Awaker2.5-VL, each expert in our model is devised as a low-rank adaptation (LoRA) structure. Extensive experiments on multiple latest benchmarks demonstrate the effectiveness of Awaker2.5-VL. The code and model weight are released in our Project Page: https://github.com/MetabrainAGI/Awaker.', 'score': 7, 'issue_id': 654, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': 'f1319420ae85759e', 'authors': ['Jinqiang Long', 'Yanqi Dai', 'Guoxing Yang', 'Hongpeng Lin', 'Nanyi Fei', 'Yizhao Gao', 'Zhiwu Lu'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'Metabrain AGI Lab, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2411.10669.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#benchmark', '#optimization', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Awaker2.5-VL: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ MLLM Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Awaker2.5-VL - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (LoRA) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Awaker2.5-VL Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Awaker2.5-VL: Mastering Multimodal Tasks with Expert Precision', 'desc': "This paper introduces Awaker2.5-VL, a new model designed to improve the performance of Multimodal Large Language Models (MLLMs) on various tasks like visual question answering and detection. The authors address the 'multi-task conflict' problem, which occurs when different tasks interfere with each other due to their diverse data representations. Awaker2.5-VL uses a Mixture of Experts (MoE) architecture, where only a subset of experts is activated for each task, allowing for better specialization and efficiency. Additionally, the model incorporates low-rank adaptation (LoRA) to enhance training speed and inference performance, showing promising results in extensive experiments."}, 'zh': {'title': 'å¤šæ¨¡æ€ä»»åŠ¡çš„ä¸“å®¶æ··åˆè§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAwaker2.5-VLçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨åŒæ—¶å¤„ç†æ–‡æœ¬å’Œè§†è§‰ä»»åŠ¡ï¼Œå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€æ£€æµ‹ã€å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œå›¾è¡¨é—®ç­”ï¼ˆChartQAï¼‰ã€‚ä¸ºäº†å…‹æœå¤šä»»åŠ¡å†²çªé—®é¢˜ï¼ŒAwaker2.5-VLé‡‡ç”¨äº†ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ¶æ„ï¼Œé€šè¿‡å¤šä¸ªç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ¥å®ç°å¤šä»»åŠ¡èƒ½åŠ›ã€‚æ¯ä¸ªä¸“å®¶è¢«è®¾è®¡ä¸ºä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ç»“æ„ï¼Œä»¥åŠ é€Ÿæ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒAwaker2.5-VLåœ¨å¤šä¸ªæœ€æ–°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.09213', 'title': 'Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering', 'url': 'https://huggingface.co/papers/2411.09213', 'abstract': "Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs' ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models' limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs' reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.", 'score': 6, 'issue_id': 655, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'e99e85d88963aa4c', 'authors': ['Nghia Trung Ngo', 'Chien Van Nguyen', 'Franck Dernoncourt', 'Thien Huu Nguyen'], 'affiliations': ['Adobe Research, USA', 'Department of Computer Science, University of Oregon, OR, USA'], 'pdf_title_img': 'assets/pdf/title_img/2411.09213.jpg', 'data': {'categories': ['#rag', '#survey', '#healthcare', '#open_source', '#benchmark', '#reasoning'], 'emoji': 'ğŸ©º', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… RAG-ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… retrieval-augmented generation (RAG). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MedRGB, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‹ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒÑÑ Ñ ÑˆÑƒĞ¼Ğ¾Ğ¼ Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Enhancing Medical QA with Robust Retrieval-Augmented Generation', 'desc': 'This paper introduces a new evaluation framework called Medical Retrieval-Augmented Generation Benchmark (MedRGB) to improve the performance of large language models (LLMs) in medical question-answering tasks. It highlights the need for accurate and trustworthy systems in the sensitive medical domain, addressing gaps in existing benchmarks that do not consider practical scenarios. The authors conduct extensive evaluations of both commercial and open-source LLMs, revealing their limitations in dealing with noise and misinformation in retrieved documents. The study also analyzes the reasoning processes of these models, providing insights for future improvements in retrieval-augmented generation systems for medical applications.'}, 'zh': {'title': 'æå‡åŒ»ç–—é—®ç­”ç³»ç»Ÿçš„å¯é æ€§', 'desc': 'æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚ç„¶è€Œï¼ŒåŒ»ç–—é¢†åŸŸçš„æ•æ„Ÿæ€§è¦æ±‚ç³»ç»Ÿå¿…é¡»å®Œå…¨å‡†ç¡®å’Œå¯ä¿¡ã€‚ç°æœ‰çš„RAGåŸºå‡†ä¸»è¦å…³æ³¨æ ‡å‡†çš„æ£€ç´¢-å›ç­”è®¾ç½®ï¼Œå¿½è§†äº†è®¸å¤šå®é™…åœºæ™¯ï¼Œè¿™äº›åœºæ™¯å¯¹å¯é åŒ»ç–—ç³»ç»Ÿçš„å…³é”®æ–¹é¢è¿›è¡Œäº†è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†åŒ»ç–—æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†ï¼ˆMedRGBï¼‰ï¼Œä¸ºå››ä¸ªåŒ»ç–—é—®ç­”æ•°æ®é›†æä¾›äº†å„ç§è¡¥å……å…ƒç´ ï¼Œä»¥æµ‹è¯•LLMsåœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„å¤„ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.11024', 'title': 'VeGaS: Video Gaussian Splatting', 'url': 'https://huggingface.co/papers/2411.11024', 'abstract': 'Implicit Neural Representations (INRs) employ neural networks to approximate discrete data as continuous functions. In the context of video data, such models can be utilized to transform the coordinates of pixel locations along with frame occurrence times (or indices) into RGB color values. Although INRs facilitate effective compression, they are unsuitable for editing purposes. One potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such as the Video Gaussian Representation (VGR), which is capable of encoding video as a multitude of 3D Gaussians and is applicable for numerous video processing operations, including editing. Nevertheless, in this case, the capacity for modification is constrained to a limited set of basic transformations. To address this issue, we introduce the Video Gaussian Splatting (VeGaS) model, which enables realistic modifications of video data. To construct VeGaS, we propose a novel family of Folded-Gaussian distributions designed to capture nonlinear dynamics in a video stream and model consecutive frames by 2D Gaussians obtained as respective conditional distributions. Our experiments demonstrate that VeGaS outperforms state-of-the-art solutions in frame reconstruction tasks and allows realistic modifications of video data. The code is available at: https://github.com/gmum/VeGaS.', 'score': 5, 'issue_id': 661, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '0c6ed02c1597f4ff', 'authors': ['Weronika Smolak-DyÅ¼ewska', 'Dawid Malarz', 'Kornel Howil', 'Jan Kaczmarczyk', 'Marcin Mazur', 'PrzemysÅ‚aw Spurek'], 'affiliations': ['Jagiellonian University Faculty of Mathematics and Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2411.11024.jpg', 'data': {'categories': ['#video', '#optimization', '#3d'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VeGaS: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Video Gaussian Splatting (VeGaS) Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. VeGaS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ÑĞ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². VeGaS Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'VeGaS: Revolutionizing Video Editing with Gaussian Splatting', 'desc': 'Implicit Neural Representations (INRs) use neural networks to represent discrete data as continuous functions, particularly in video processing. They convert pixel coordinates and frame indices into RGB values, enabling effective data compression but limiting editing capabilities. The Video Gaussian Splatting (VeGaS) model enhances this by utilizing a new family of Folded-Gaussian distributions, allowing for realistic video modifications while capturing nonlinear dynamics. Our experiments show that VeGaS surpasses existing methods in frame reconstruction and supports a wider range of editing operations.'}, 'zh': {'title': 'è§†é¢‘æ•°æ®çš„çœŸå®ä¿®æ”¹æ–°æ–¹æ³•', 'desc': 'éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRsï¼‰ä½¿ç”¨ç¥ç»ç½‘ç»œå°†ç¦»æ•£æ•°æ®è¿‘ä¼¼ä¸ºè¿ç»­å‡½æ•°ã€‚åœ¨è§†é¢‘æ•°æ®ä¸­ï¼Œè¿™ç§æ¨¡å‹å¯ä»¥å°†åƒç´ ä½ç½®çš„åæ ‡å’Œå¸§å‡ºç°æ—¶é—´è½¬æ¢ä¸ºRGBé¢œè‰²å€¼ã€‚è™½ç„¶INRsåœ¨å‹ç¼©æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¸é€‚åˆç¼–è¾‘ã€‚æˆ‘ä»¬æå‡ºçš„è§†é¢‘é«˜æ–¯ç‚¹äº‘ï¼ˆVeGaSï¼‰æ¨¡å‹ï¼Œåˆ©ç”¨æ–°å‹çš„æŠ˜å é«˜æ–¯åˆ†å¸ƒï¼Œèƒ½å¤Ÿå®ç°è§†é¢‘æ•°æ®çš„çœŸå®ä¿®æ”¹ï¼Œå¹¶åœ¨å¸§é‡å»ºä»»åŠ¡ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.09661', 'title': 'Adaptive Decoding via Latent Preference Optimization', 'url': 'https://huggingface.co/papers/2411.09661', 'abstract': 'During language model decoding, it is known that using higher temperature sampling gives more creative responses, while lower temperatures are more factually accurate. However, such models are commonly applied to general instruction following, which involves both creative and fact seeking tasks, using a single fixed temperature across all examples and tokens. In this work, we introduce Adaptive Decoding, a layer added to the model to select the sampling temperature dynamically at inference time, at either the token or example level, in order to optimize performance. To learn its parameters we introduce Latent Preference Optimization (LPO) a general approach to train discrete latent variables such as choices of temperature. Our method outperforms all fixed decoding temperatures across a range of tasks that require different temperatures, including UltraFeedback, Creative Story Writing, and GSM8K.', 'score': 5, 'issue_id': 659, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '4bb5fff16280c4bc', 'authors': ['Shehzaad Dhuliawala', 'Ilia Kulikov', 'Ping Yu', 'Asli Celikyilmaz', 'Jason Weston', 'Sainbayar Sukhbaatar', 'Jack Lanchantin'], 'affiliations': ['FAIR at Meta', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2411.09661.jpg', 'data': {'categories': ['#training', '#optimization', '#story_generation', '#inference'], 'emoji': 'ğŸŒ¡ï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñƒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (LPO) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… UltraFeedback, Creative Story Writing Ğ¸ GSM8K.'}, 'en': {'title': 'Dynamic Temperature for Optimal Language Model Responses', 'desc': "This paper presents a new method called Adaptive Decoding, which allows language models to adjust their sampling temperature dynamically during inference. By varying the temperature, the model can balance creativity and factual accuracy based on the specific task at hand. The authors introduce Latent Preference Optimization (LPO) to effectively train the model's temperature selection process. Their approach shows improved performance over traditional fixed temperature methods across various tasks, demonstrating its versatility and effectiveness."}, 'zh': {'title': 'è‡ªé€‚åº”è§£ç ï¼šåŠ¨æ€é€‰æ‹©æ¸©åº¦ä¼˜åŒ–æ¨¡å‹è¡¨ç°', 'desc': 'åœ¨è¯­è¨€æ¨¡å‹è§£ç è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨è¾ƒé«˜çš„æ¸©åº¦é‡‡æ ·å¯ä»¥äº§ç”Ÿæ›´å…·åˆ›æ„çš„å“åº”ï¼Œè€Œè¾ƒä½çš„æ¸©åº¦åˆ™æ›´å‡†ç¡®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”è§£ç æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶åŠ¨æ€é€‰æ‹©é‡‡æ ·æ¸©åº¦ï¼Œä¼˜åŒ–æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å¼•å…¥äº†æ½œåœ¨åå¥½ä¼˜åŒ–ï¼ˆLPOï¼‰æ¥è®­ç»ƒæ¸©åº¦é€‰æ‹©çš„ç¦»æ•£æ½œå˜é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éœ€è¦ä¸åŒæ¸©åº¦çš„å¤šç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜äºå›ºå®šè§£ç æ¸©åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.11045', 'title': 'StableV2V: Stablizing Shape Consistency in Video-to-Video Editing', 'url': 'https://huggingface.co/papers/2411.11045', 'abstract': 'Recent advancements of generative AI have significantly promoted content creation and editing, where prevailing studies further extend this exciting progress to video editing. In doing so, these studies mainly transfer the inherent motion patterns from the source videos to the edited ones, where results with inferior consistency to user prompts are often observed, due to the lack of particular alignments between the delivered motions and edited contents. To address this limitation, we present a shape-consistent video editing method, namely StableV2V, in this paper. Our method decomposes the entire editing pipeline into several sequential procedures, where it edits the first video frame, then establishes an alignment between the delivered motions and user prompts, and eventually propagates the edited contents to all other frames based on such alignment. Furthermore, we curate a testing benchmark, namely DAVIS-Edit, for a comprehensive evaluation of video editing, considering various types of prompts and difficulties. Experimental results and analyses illustrate the outperforming performance, visual consistency, and inference efficiency of our method compared to existing state-of-the-art studies.', 'score': 5, 'issue_id': 658, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': 'd4f742e7121f2322', 'authors': ['Chang Liu', 'Rui Li', 'Kaidong Zhang', 'Yunwei Lan', 'Dong Liu'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2411.11045.jpg', 'data': {'categories': ['#optimization', '#games', '#video', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'StableV2V: Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ StableV2V, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¿Ñ€Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DAVIS-Edit Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ StableV2V Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'StableV2V: Aligning Motion with User Prompts for Consistent Video Editing', 'desc': 'This paper introduces StableV2V, a novel method for video editing that enhances the consistency between user prompts and the resulting video content. The approach involves breaking down the editing process into sequential steps, starting with the first frame and aligning the motion patterns with user instructions. By propagating the edited content across all frames based on this alignment, StableV2V achieves better visual coherence and performance. Additionally, the authors present a new benchmark, DAVIS-Edit, to evaluate the effectiveness of video editing methods under various conditions.'}, 'zh': {'title': 'ç¨³å®šä¸€è‡´çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºStableV2Vçš„å½¢çŠ¶ä¸€è‡´æ€§è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç¼–è¾‘çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•å°†æ•´ä¸ªç¼–è¾‘æµç¨‹åˆ†è§£ä¸ºå¤šä¸ªé¡ºåºæ­¥éª¤ï¼Œé¦–å…ˆç¼–è¾‘ç¬¬ä¸€å¸§è§†é¢‘ï¼Œç„¶ååœ¨ç”¨æˆ·æç¤ºå’Œä¼ é€’çš„è¿åŠ¨ä¹‹é—´å»ºç«‹å¯¹é½ï¼Œæœ€åæ ¹æ®è¿™ç§å¯¹é½å°†ç¼–è¾‘å†…å®¹ä¼ æ’­åˆ°å…¶ä»–å¸§ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ªæµ‹è¯•åŸºå‡†DAVIS-Editï¼Œä»¥å…¨é¢è¯„ä¼°è§†é¢‘ç¼–è¾‘çš„æ•ˆæœï¼Œè€ƒè™‘äº†ä¸åŒç±»å‹çš„æç¤ºå’Œéš¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›ç ”ç©¶ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ã€è§†è§‰ä¸€è‡´æ€§å’Œæ¨ç†æ•ˆç‡ä¸Šå‡è¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10499', 'title': 'FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on', 'url': 'https://huggingface.co/papers/2411.10499', 'abstract': 'Although image-based virtual try-on has made considerable progress, emerging approaches still encounter challenges in producing high-fidelity and robust fitting images across diverse scenarios. These methods often struggle with issues such as texture-aware maintenance and size-aware fitting, which hinder their overall effectiveness. To address these limitations, we propose a novel garment perception enhancement technique, termed FitDiT, designed for high-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more parameters and attention to high-resolution features. First, to further improve texture-aware maintenance, we introduce a garment texture extractor that incorporates garment priors evolution to fine-tune garment feature, facilitating to better capture rich details such as stripes, patterns, and text. Additionally, we introduce frequency-domain learning by customizing a frequency distance loss to enhance high-frequency garment details. To tackle the size-aware fitting issue, we employ a dilated-relaxed mask strategy that adapts to the correct length of garments, preventing the generation of garments that fill the entire mask area during cross-category try-on. Equipped with the above design, FitDiT surpasses all baselines in both qualitative and quantitative evaluations. It excels in producing well-fitting garments with photorealistic and intricate details, while also achieving competitive inference times of 4.57 seconds for a single 1024x768 image after DiT structure slimming, outperforming existing methods.', 'score': 4, 'issue_id': 654, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': 'b142b4be26ef6147', 'authors': ['Boyuan Jiang', 'Xiaobin Hu', 'Donghao Luo', 'Qingdong He', 'Chengming Xu', 'Jinlong Peng', 'Jiangning Zhang', 'Chengjie Wang', 'Yunsheng Wu', 'Yanwei Fu'], 'affiliations': ['Fudan University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2411.10499.jpg', 'data': {'categories': ['#cv', '#optimization', '#3d', '#diffusion'], 'emoji': 'ğŸ‘š', 'ru': {'title': 'FitDiT: Ğ’Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ FitDiT, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. FitDiT ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ° Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ğ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'FitDiT: Revolutionizing Virtual Try-On with High-Fidelity Garment Perception', 'desc': 'This paper presents FitDiT, a new technique for improving virtual try-on systems using Diffusion Transformers. It addresses challenges in producing realistic and well-fitting images by enhancing garment perception through a specialized texture extractor and frequency-domain learning. The method also incorporates a dilated-relaxed mask strategy to ensure garments fit correctly without oversizing. FitDiT demonstrates superior performance in generating high-fidelity images with intricate details while maintaining efficient processing times.'}, 'zh': {'title': 'FitDiTï¼šé«˜ä¿çœŸè™šæ‹Ÿè¯•ç©¿çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœè£…æ„ŸçŸ¥å¢å¼ºæŠ€æœ¯ï¼Œç§°ä¸ºFitDiTï¼Œæ—¨åœ¨æé«˜è™šæ‹Ÿè¯•ç©¿çš„é«˜ä¿çœŸåº¦ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰åˆ†é…æ›´å¤šå‚æ•°å’Œæ³¨æ„åŠ›äºé«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼Œä»¥è§£å†³çº¹ç†æ„ŸçŸ¥ç»´æŠ¤å’Œå°ºå¯¸æ„ŸçŸ¥é€‚é…çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†æœè£…çº¹ç†æå–å™¨å’Œé¢‘åŸŸå­¦ä¹ ï¼Œå¢å¼ºäº†æœè£…ç»†èŠ‚çš„æ•æ‰èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨æ‰©å¼ æ”¾æ¾æ©ç ç­–ç•¥æ¥é€‚åº”æœè£…çš„æ­£ç¡®é•¿åº¦ã€‚FitDiTåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­å‡è¶…è¶Šäº†æ‰€æœ‰åŸºçº¿ï¼Œèƒ½å¤Ÿç”Ÿæˆå…·æœ‰çœŸå®æ„Ÿå’Œå¤æ‚ç»†èŠ‚çš„åˆèº«æœè£…ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10168', 'title': "Evaluating the role of `Constitutions' for learning from AI feedback", 'url': 'https://huggingface.co/papers/2411.10168', 'abstract': "The growing capabilities of large language models (LLMs) have led to their use as substitutes for human feedback for training and assessing other LLMs. These methods often rely on `constitutions', written guidelines which a critic model uses to provide feedback and improve generations. We investigate how the choice of constitution affects feedback quality by using four different constitutions to improve patient-centered communication in medical interviews. In pairwise comparisons conducted by 215 human raters, we found that detailed constitutions led to better results regarding emotive qualities. However, none of the constitutions outperformed the baseline in learning more practically-oriented skills related to information gathering and provision. Our findings indicate that while detailed constitutions should be prioritised, there are possible limitations to the effectiveness of AI feedback as a reward signal in certain areas.", 'score': 3, 'issue_id': 661, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': '491ed277e2d6a217', 'authors': ['Saskia Redgate', 'Andrew M. Bean', 'Adam Mahdi'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2411.10168.jpg', 'data': {'categories': ['#rlhf', '#interpretability', '#alignment', '#healthcare'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¸ LLM: Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… LLM Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° 'ĞºĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¸' (Ğ¿Ğ¸ÑÑŒĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹) Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ€Ğ°Ñ‡Ğ¾Ğ¼ Ğ¸ Ğ¿Ğ°Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ†Ğ¸Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ñ… ÑĞ±Ğ¾Ñ€Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¾Ñ‚ Ğ˜Ğ˜ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…."}, 'en': {'title': 'Enhancing AI Feedback with Detailed Guidelines', 'desc': "This paper explores how large language models (LLMs) can be trained and evaluated using human-like feedback, specifically through the use of 'constitutions'â€”guidelines that help a critic model assess and improve LLM outputs. The study tests four different constitutions to enhance patient-centered communication during medical interviews. Results from 215 human raters show that more detailed constitutions improve the emotive qualities of the communication, but they do not significantly enhance practical skills like information gathering. The findings suggest that while detailed guidelines are beneficial, there are limitations to using AI feedback as a reward signal for certain competencies."}, 'zh': {'title': 'è¯¦ç»†æŒ‡å¯¼åŸåˆ™æå‡æƒ…æ„Ÿè´¨é‡', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®­ç»ƒå’Œè¯„ä¼°å…¶ä»–LLMsæ—¶ï¼Œå¦‚ä½•ä½œä¸ºäººç±»åé¦ˆçš„æ›¿ä»£å“ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†å››ç§ä¸åŒçš„æŒ‡å¯¼åŸåˆ™ï¼ˆconstitutionï¼‰ï¼Œä»¥æ”¹å–„åŒ»ç–—è®¿è°ˆä¸­çš„ä»¥æ‚£è€…ä¸ºä¸­å¿ƒçš„æ²Ÿé€šã€‚é€šè¿‡215åäººç±»è¯„å®¡è€…çš„å¯¹æ¯”å®éªŒï¼Œæˆ‘ä»¬å‘ç°è¯¦ç»†çš„æŒ‡å¯¼åŸåˆ™åœ¨æƒ…æ„Ÿè´¨é‡æ–¹é¢çš„åé¦ˆæ•ˆæœæ›´å¥½ã€‚ç„¶è€Œï¼Œåœ¨ä¿¡æ¯æ”¶é›†å’Œæä¾›ç­‰å®é™…æŠ€èƒ½çš„å­¦ä¹ ä¸Šï¼Œæ²¡æœ‰ä»»ä½•æŒ‡å¯¼åŸåˆ™è¶…è¶Šäº†åŸºçº¿è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.03562', 'title': 'Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level', 'url': 'https://huggingface.co/papers/2411.03562', 'abstract': "We introduce Agent K v1.0, an end-to-end autonomous data science agent designed to automate, optimise, and generalise across diverse data science tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle by learning from experience. It leverages a highly flexible structured reasoning framework to enable it to dynamically process memory in a nested structure, effectively learning from accumulated experience stored to handle complex reasoning tasks. It optimises long- and short-term memory by selectively storing and retrieving key information, guiding future decisions based on environmental rewards. This iterative approach allows it to refine decisions without fine-tuning or backpropagation, achieving continuous improvement through experiential learning. We evaluate our agent's apabilities using Kaggle competitions as a case study. Following a fully automated protocol, Agent K v1.0 systematically addresses complex and multimodal data science tasks, employing Bayesian optimisation for hyperparameter tuning and feature engineering. Our new evaluation framework rigorously assesses Agent K v1.0's end-to-end capabilities to generate and send submissions starting from a Kaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\\% success rate across tasks, spanning tabular, computer vision, NLP, and multimodal domains. When benchmarking against 5,856 human Kaggle competitors by calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\\%, demonstrating an overall skill level comparable to Expert-level users. Notably, its Elo-MMR score falls between the first and third quartiles of scores achieved by human Grandmasters. Furthermore, our results indicate that Agent K v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a record of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's progression system.", 'score': 56, 'issue_id': 457, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '1db584382b826315', 'authors': ['Antoine Grosnit', 'Alexandre Maraval', 'James Doran', 'Giuseppe Paolo', 'Albert Thomas', 'Refinath Shahul Hameed Nabeezath Beevi', 'Jonas Gonzalez', 'Khyati Khandelwal', 'Ignacio Iacobacci', 'Abdelhakim Benechehab', 'Hamza Cherkaoui', 'Youssef Attia El-Hili', 'Kun Shao', 'Jianye Hao', 'Jun Yao', 'Balazs Kegl', 'Haitham Bou-Ammar', 'Jun Wang'], 'affiliations': ['Huawei Noahs Ark', 'AI Centre, Department of Computer Science, UCL', 'Technical University of Darmstadt'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03562.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#agi', '#optimization', '#multimodal', '#training', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Agent K v1.0: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Agent K v1.0 - Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒĞºĞ¸ Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. Agent K v1.0 Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ€Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¼Ñƒ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞÑ†ĞµĞ½ĞºĞ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ°ÑÑŒ Ğ½Ğ° ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Kaggle, Ğ³Ğ´Ğµ Ğ¾Ğ½ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ» 92.5% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Agent K v1.0: Your Autonomous Data Science Expert!', 'desc': "Agent K v1.0 is an autonomous data science agent that automates the entire data science life cycle, learning from its experiences to improve over time. It uses a structured reasoning framework to manage memory and handle complex tasks without the need for traditional fine-tuning methods. By employing Bayesian optimization for hyperparameter tuning and feature engineering, it effectively addresses a variety of data science challenges. The agent's performance has been validated through Kaggle competitions, where it achieved a high success rate and ranked competitively against human experts, demonstrating its advanced capabilities in multiple domains."}, 'zh': {'title': 'Agent K v1.0ï¼šè‡ªåŠ¨åŒ–æ•°æ®ç§‘å­¦çš„æœªæ¥', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Agent K v1.0ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è‡ªä¸»æ•°æ®ç§‘å­¦ä»£ç†ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–ã€ä¼˜åŒ–å’Œæ³›åŒ–å„ç§æ•°æ®ç§‘å­¦ä»»åŠ¡ã€‚Agent K v1.0 å®Œå…¨è‡ªåŠ¨åŒ–ï¼Œèƒ½å¤Ÿç®¡ç†æ•´ä¸ªæ•°æ®ç§‘å­¦ç”Ÿå‘½å‘¨æœŸï¼Œå¹¶é€šè¿‡ç»éªŒå­¦ä¹ æ¥æå‡èƒ½åŠ›ã€‚å®ƒåˆ©ç”¨çµæ´»çš„ç»“æ„åŒ–æ¨ç†æ¡†æ¶ï¼ŒåŠ¨æ€å¤„ç†åµŒå¥—ç»“æ„ä¸­çš„è®°å¿†ï¼Œæœ‰æ•ˆåœ°ä»ç§¯ç´¯çš„ç»éªŒä¸­å­¦ä¹ ï¼Œä»¥åº”å¯¹å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡é€‰æ‹©æ€§å­˜å‚¨å’Œæ£€ç´¢å…³é”®ä¿¡æ¯ï¼ŒAgent K v1.0 ä¼˜åŒ–äº†çŸ­æœŸå’Œé•¿æœŸè®°å¿†ï¼ŒåŸºäºç¯å¢ƒå¥–åŠ±æŒ‡å¯¼æœªæ¥å†³ç­–ï¼Œå±•ç°å‡ºä¸äººç±»ä¸“å®¶ç›¸å½“çš„æŠ€èƒ½æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.03823', 'title': 'Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination', 'url': 'https://huggingface.co/papers/2411.03823', 'abstract': 'The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced.', 'score': 43, 'issue_id': 457, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '3f0a02ee67213e17', 'authors': ['Dingjie Song', 'Sicheng Lai', 'Shunian Chen', 'Lichao Sun', 'Benyou Wang'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen', 'Lehigh University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03823.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#multimodal', '#training', '#dataset', '#security'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ§Ğ¸ÑÑ‚Ğ¾Ñ‚Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - Ğ·Ğ°Ğ»Ğ¾Ğ³ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MM-Detect Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ² MLLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MM-Detect Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğº Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ ÑÑ‚ĞµĞ¿ĞµĞ½ÑĞ¼ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹ÑĞ²Ğ»ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·-Ğ·Ğ° ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Detecting Contamination in Multimodal Language Models', 'desc': 'This paper addresses the challenge of data contamination in multimodal large language models (MLLMs), which can affect their performance evaluation. The authors propose a new framework called MM-Detect, specifically designed to identify contamination in MLLMs across different modalities and training phases. Their experiments show that MM-Detect can effectively detect varying levels of contamination and reveal how training set leakage impacts performance. Additionally, the study investigates contamination sources during both the pre-training and fine-tuning phases of MLLMs, providing valuable insights into potential contamination points.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ•°æ®æ±¡æŸ“æ£€æµ‹çš„æ–°çªç ´', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ•°æ®æ±¡æŸ“æ£€æµ‹çš„æ–°æ¡†æ¶MM-Detectã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„æ•°æ®æ±¡æŸ“é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMM-Detectå¯¹ä¸åŒç¨‹åº¦çš„æ•°æ®æ±¡æŸ“éå¸¸æ•æ„Ÿï¼Œå¹¶èƒ½æ˜¾è‘—æå‡æ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†æ•°æ®æ±¡æŸ“å¯èƒ½æºè‡ªLLMsçš„é¢„è®­ç»ƒé˜¶æ®µå’ŒMLLMsçš„å¾®è°ƒé˜¶æ®µï¼Œä¸ºç†è§£æ±¡æŸ“å¼•å…¥çš„æ—¶æœºæä¾›äº†æ–°çš„è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.03884', 'title': 'Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models', 'url': 'https://huggingface.co/papers/2411.03884', 'abstract': 'Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.', 'score': 20, 'issue_id': 459, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '6ed1524392784244', 'authors': ['Zhijian Zhuo', 'Ya Wang', 'Yutao Zeng', 'Xiaoqing Li', 'Xun Zhou', 'Jinwen Ma'], 'affiliations': ['School of Mathematical Sciences, Peking University', 'Seed-Foundation-Model, ByteDance', 'Capital University of Economics and Business'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03884.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'PolyCom: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ… Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ÑƒÑ PolyCom (Ğ¿Ğ¾Ğ»Ğ¸Ğ½Ğ¾Ğ¼Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PolyCom Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ PolyCom Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ¼ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unlocking Transformer Potential with Polynomial Activations', 'desc': 'This paper introduces a new type of activation function called Polynomial Composition Activations (PolyCom) for transformer models. The authors argue that PolyCom enhances the nonlinearity of transformers, which is crucial for improving their representational capacity. Through mathematical analysis, they show that networks using PolyCom can approximate complex functions more efficiently than those using traditional activation functions. Empirical tests on large language models reveal that PolyCom leads to better performance in terms of accuracy and convergence, demonstrating its potential as a superior alternative in machine learning applications.'}, 'zh': {'title': 'å¤šé¡¹å¼ç»„åˆæ¿€æ´»å‡½æ•°ï¼šæå‡å˜æ¢å™¨æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šé¡¹å¼ç»„åˆæ¿€æ´»å‡½æ•°ï¼ˆPolyComï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–å˜æ¢å™¨çš„åŠ¨æ€æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡æ•°å­¦åˆ†æè¯æ˜äº†PolyComåœ¨è¡¨è¾¾èƒ½åŠ›å’Œæ•ˆç‡æ–¹é¢ä¼˜äºå…¶ä»–æ¿€æ´»å‡½æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨PolyComçš„ç½‘ç»œåœ¨é€¼è¿‘å…‰æ»‘å‡½æ•°æ—¶æ‰€éœ€çš„å‚æ•°æ›´å°‘ï¼Œä¸”åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒé…ç½®ä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ”¶æ•›é€Ÿåº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒPolyComèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰æ•°æ®ä¸­çš„é«˜é˜¶äº¤äº’ï¼Œä»è€Œæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04109', 'title': 'Self-Consistency Preference Optimization', 'url': 'https://huggingface.co/papers/2411.04109', 'abstract': 'Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. In this work, we extend the self-consistency concept to help train models. We thus introduce self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. We show ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku.', 'score': 13, 'issue_id': 459, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '213f2796c0bc72ae', 'authors': ['Archiki Prasad', 'Weizhe Yuan', 'Richard Yuanzhe Pang', 'Jing Xu', 'Maryam Fazel-Zarandi', 'Mohit Bansal', 'Sainbayar Sukhbaatar', 'Jason Weston', 'Jane Yu'], 'affiliations': ['Meta FAIR', 'UNC Chapel Hill', 'New York University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04109.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#rl', '#optimization', '#training', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (ScPO) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ÑƒÑ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ScPO Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ°Ğ´ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº GSM8K Ğ¸ MATH, ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼.'}, 'en': {'title': 'Empowering Models with Self-Consistency for Better Reasoning', 'desc': "This paper introduces a new method called self-consistency preference optimization (ScPO) to enhance the training of machine learning models without human annotations. ScPO focuses on improving the consistency of answers by iteratively training the model to prefer consistent responses over inconsistent ones. The authors demonstrate that ScPO significantly outperforms traditional reward model training on complex reasoning tasks, such as GSM8K and MATH, and even approaches the performance of supervised training. Additionally, when combined with standard supervised learning, ScPO further boosts the model's performance, achieving superior results on various benchmarks."}, 'zh': {'title': 'è‡ªæˆ‘ä¸€è‡´æ€§ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼', 'desc': 'è‡ªæˆ‘å¯¹é½æ˜¯æŒ‡æ¨¡å‹åœ¨æ²¡æœ‰äººå·¥æ ‡æ³¨çš„æƒ…å†µä¸‹è‡ªæˆ‘æ”¹è¿›çš„è¿‡ç¨‹ï¼Œè¿‘å¹´æ¥è¿™ä¸€ç ”ç©¶é¢†åŸŸè¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å¸¸å¸¸æ— æ³•æœ‰æ•ˆæå‡æ€§èƒ½ï¼Œå› ä¸ºå¾ˆéš¾åˆ†é…æ­£ç¡®çš„å¥–åŠ±ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”è‡ªæˆ‘ä¸€è‡´æ€§åå¥½ä¼˜åŒ–ï¼ˆScPOï¼‰ï¼Œå®ƒé€šè¿‡è¿­ä»£è®­ç»ƒä¸€è‡´çš„ç­”æ¡ˆï¼Œä½¿å…¶ä¼˜äºä¸ä¸€è‡´çš„ç­”æ¡ˆï¼Œä»è€Œè§£å†³æ— ç›‘ç£æ–°é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒScPOåœ¨æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œä¸”ä¸æ ‡å‡†ç›‘ç£å­¦ä¹ ç»“åˆåæ•ˆæœæ›´ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.03590', 'title': 'From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond', 'url': 'https://huggingface.co/papers/2411.03590', 'abstract': "Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI's o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1's performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.", 'score': 9, 'issue_id': 460, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': 'c43efbb9c2b1c150', 'authors': ['Harsha Nori', 'Naoto Usuyama', 'Nicholas King', 'Scott Mayer McKinney', 'Xavier Fernandes', 'Sheng Zhang', 'Eric Horvitz'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03590.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#inference', '#optimization', '#healthcare', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼ reasoning: o1-preview Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ o1-preview Ğ¾Ñ‚ OpenAI Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ GPT-4 Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Medprompt. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ o1-preview Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ´Ğ°Ğ¶Ğµ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ°, Ğ½Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… (few-shot learning) ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‚ ĞµÑ‘ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ o1-preview Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº GPT-4 Ñ Medprompt Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ Ñ†ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ¹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ…, Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº ĞºĞ°Ğº o1-preview Ğ±Ğ»Ğ¸Ğ·ĞºĞ° Ğº Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Unlocking Medical Mastery: The Power of Run-Time Reasoning in LLMs', 'desc': "This paper explores the effectiveness of run-time steering strategies, particularly focusing on the Medprompt technique, in enhancing the performance of large language models (LLMs) in specialized fields like medicine. It introduces OpenAI's o1-preview model, which employs a new approach to reasoning before generating responses, showing superior performance compared to the GPT-4 series even without additional prompting. The study evaluates various medical benchmarks and finds that traditional few-shot prompting may not be beneficial for reasoning-native models like o1-preview, while ensembling remains a viable but resource-intensive option. The analysis reveals a trade-off between cost and accuracy, highlighting the need for new benchmarks to further challenge these advanced models."}, 'zh': {'title': 'è¿è¡Œæ—¶æ¨ç†ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è¿è¡Œæ—¶å¼•å¯¼ç­–ç•¥ï¼ˆå¦‚Medpromptï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦ç­‰ä¸“ä¸šé¢†åŸŸè¡¨ç°ä¸­çš„é‡è¦æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œo1-previewæ¨¡å‹åœ¨ç”Ÿæˆæœ€ç»ˆå“åº”ä¹‹å‰è¿›è¡Œè¿è¡Œæ—¶æ¨ç†ï¼Œèƒ½å¤Ÿåœ¨å¤šç§åŒ»å­¦æŒ‘æˆ˜åŸºå‡†ä¸Šè¶…è¶ŠGPT-4ç³»åˆ—ã€‚å°½ç®¡o1-previewåœ¨æ²¡æœ‰æç¤ºæŠ€æœ¯çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç»å…¸çš„æç¤ºå·¥ç¨‹ç­–ç•¥åœ¨æ–°æ¨ç†æ¨¡å‹ä¸­å¯èƒ½ä¸å†æœ‰æ•ˆã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºo1-previewåœ¨ç°æœ‰åŒ»å­¦åŸºå‡†ä¸Šå·²æ¥è¿‘é¥±å’Œï¼Œå¼ºè°ƒäº†å¼€å‘æ–°åŸºå‡†çš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.23218', 'title': 'OS-ATLAS: A Foundation Action Model for Generalist GUI Agents', 'url': 'https://huggingface.co/papers/2410.23218', 'abstract': 'Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.', 'score': 46, 'issue_id': 410, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': 'd7a3f0fd08f934d5', 'authors': ['Zhiyong Wu', 'Zhenyu Wu', 'Fangzhi Xu', 'Yian Wang', 'Qiushi Sun', 'Chengyou Jia', 'Kanzhi Cheng', 'Zichen Ding', 'Liheng Chen', 'Paul Pu Liang', 'Yu Qiao'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The University of Hong Kong', 'MIT'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23218.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#cv', '#graphs', '#multiplatform', '#data', '#training', '#dataset', '#open_source', '#agents'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'OS-Atlas: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ GUI', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ OS-Atlas - Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (GUI). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ GUI Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ (OOD). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ GUI Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ… Ğ¸ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ĞºÑ€Ğ¾ÑÑ-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 13 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² GUI. OS-Atlas Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ, Ğ½Ğ°ÑÑ‚Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ²ĞµĞ±-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹.'}, 'en': {'title': 'Empowering Open-Source GUI Agents with OS-Atlas', 'desc': "This paper introduces OS-Atlas, an open-source foundational model designed for GUI grounding and Out-Of-Distribution (OOD) tasks. It addresses the performance gap between commercial Vision-Language Models (VLMs) and open-source alternatives by providing a comprehensive toolkit for synthesizing GUI grounding data across various platforms. The authors present the largest open-source cross-platform GUI grounding dataset, featuring over 13 million GUI elements, which enhances the model's ability to understand and generalize from GUI screenshots. Extensive evaluations show that OS-Atlas outperforms previous models, offering insights for further advancements in open-source VLM capabilities."}, 'zh': {'title': 'å¼€æºGUIæ¨¡å‹OS-Atlasï¼šæå‡ç•Œé¢ç†è§£èƒ½åŠ›çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†OS-Atlasï¼Œä¸€ä¸ªå¼€æºçš„GUIåŠ¨ä½œæ¨¡å‹ï¼Œä¸“æ³¨äºGUIå®šä½å’Œè¶…å‡ºåˆ†å¸ƒï¼ˆOODï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå·¥å…·åŒ…ï¼Œå¯ä»¥åœ¨å¤šä¸ªå¹³å°ä¸ŠåˆæˆGUIå®šä½æ•°æ®ï¼ŒåŒ…æ‹¬Windowsã€Linuxã€MacOSã€Androidå’Œç½‘é¡µã€‚OS-Atlasåˆ©ç”¨è¶…è¿‡1300ä¸‡ä¸ªGUIå…ƒç´ çš„æ•°æ®é›†ï¼Œç»“åˆåˆ›æ–°çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å¯¹GUIæˆªå›¾çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œå¹¿æ³›è¯„ä¼°ï¼ŒOS-Atlasåœ¨ç§»åŠ¨ã€æ¡Œé¢å’Œç½‘é¡µå¹³å°ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00027', 'title': 'Personalization of Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2411.00027', 'abstract': 'Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.', 'score': 31, 'issue_id': 409, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': 'a190b2e727d2d0ad', 'authors': ['Zhehao Zhang', 'Ryan A. Rossi', 'Branislav Kveton', 'Yijia Shao', 'Diyi Yang', 'Hamed Zamani', 'Franck Dernoncourt', 'Joe Barrow', 'Tong Yu', 'Sungchul Kim', 'Ruiyi Zhang', 'Jiuxiang Gu', 'Tyler Derr', 'Hongjie Chen', 'Junda Wu', 'Xiang Chen', 'Zichao Wang', 'Subrata Mitra', 'Nedim Lipka', 'Nesreen Ahmed', 'Yu Wang'], 'affiliations': ['Dartmouth College', 'Adobe Research', 'Stanford University', 'University of Massachusetts Amherst', 'Pattern Data', 'Vanderbilt University', 'Dolby Research', 'University of California San Diego', 'Cisco Research', 'University of Oregon'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00027.jpg', 'data': {'categories': ['#benchmark', '#training', '#dataset', '#survey', '#alignment'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… LLM, Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ÑÑ Ğ½ĞµÑ€ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Bridging Personalization Gaps in Large Language Models', 'desc': 'This paper addresses the growing need for personalization in Large Language Models (LLMs) by creating a comprehensive framework that connects personalized text generation with applications like recommendation systems. It introduces a taxonomy that categorizes various aspects of personalized LLMs, including techniques, datasets, and evaluation methods. The authors formalize the concept of personalization in LLMs, discussing its different dimensions and the challenges faced in this area. By synthesizing existing research and identifying open problems, the paper serves as a guide for researchers and practitioners interested in the personalization of LLMs.'}, 'zh': {'title': 'ç»Ÿä¸€ä¸ªæ€§åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„ç ”ç©¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ªæ€§åŒ–çš„é‡è¦æ€§åŠå…¶åº”ç”¨ã€‚æˆ‘ä»¬é¦–æ¬¡å°†ä¸ªæ€§åŒ–æ–‡æœ¬ç”Ÿæˆä¸ä¸ªæ€§åŒ–ç›¸å…³çš„ä¸‹æ¸¸åº”ç”¨ï¼ˆå¦‚æ¨èç³»ç»Ÿï¼‰ç»“åˆèµ·æ¥ï¼Œæå‡ºäº†ä¸ªæ€§åŒ–LLMsçš„åˆ†ç±»æ³•ã€‚æ–‡ç« å¯¹ä¸ªæ€§åŒ–LLMsçš„åŸºç¡€è¿›è¡Œäº†å½¢å¼åŒ–å®šä¹‰ï¼Œå¹¶è®¨è®ºäº†ä¸ªæ€§åŒ–çš„ä¸åŒæ–¹é¢ã€ä½¿ç”¨åœºæ™¯å’Œéœ€æ±‚ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†ç°æœ‰æ–‡çŒ®ï¼Œå¹¶æŒ‡å‡ºäº†ä¸ªæ€§åŒ–LLMsé¢ä¸´çš„æŒ‘æˆ˜å’Œæœªè§£å†³çš„é—®é¢˜ï¼Œä»¥å¸®åŠ©ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æ›´å¥½åœ°ç†è§£è¿™ä¸€é¢†åŸŸã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00322', 'title': 'Constant Acceleration Flow', 'url': 'https://huggingface.co/papers/2411.00322', 'abstract': 'Rectified flow and reflow procedures have significantly advanced fast generation by progressively straightening ordinary differential equation (ODE) flows. They operate under the assumption that image and noise pairs, known as couplings, can be approximated by straight trajectories with constant velocity. However, we observe that modeling with constant velocity and using reflow procedures have limitations in accurately learning straight trajectories between pairs, resulting in suboptimal performance in few-step generation. To address these limitations, we introduce Constant Acceleration Flow (CAF), a novel framework based on a simple constant acceleration equation. CAF introduces acceleration as an additional learnable variable, allowing for more expressive and accurate estimation of the ODE flow. Moreover, we propose two techniques to further improve estimation accuracy: initial velocity conditioning for the acceleration model and a reflow process for the initial velocity. Our comprehensive studies on toy datasets, CIFAR-10, and ImageNet 64x64 demonstrate that CAF outperforms state-of-the-art baselines for one-step generation. We also show that CAF dramatically improves few-step coupling preservation and inversion over Rectified flow. Code is available at https://github.com/mlvlab/CAF{https://github.com/mlvlab/CAF}.', 'score': 22, 'issue_id': 412, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': 'edca1b3005d37bab', 'authors': ['Dogyun Park', 'Sojin Lee', 'Sihyeon Kim', 'Taehoon Lee', 'Youngjoon Hong', 'Hyunwoo J. Kim'], 'affiliations': ['Korea University', 'KAIST'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00322.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'CAF: Ğ£ÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Constant Acceleration Flow (CAF). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², CAF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑˆÑƒĞ¼Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğº Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼ÑƒÑ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµÑ„Ğ»Ğ¾Ñƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ CAF Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ²ÑĞ·ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Accelerating Image Generation with Constant Acceleration Flow', 'desc': 'This paper presents a new method called Constant Acceleration Flow (CAF) to improve the generation of images using ordinary differential equations (ODEs). Traditional methods assume that the flow between images and noise can be modeled as straight lines moving at a constant speed, which can lead to inaccuracies. CAF enhances this by introducing acceleration as a learnable variable, allowing for more flexible and precise modeling of the flow. The authors demonstrate that CAF outperforms existing methods in generating images with fewer steps while maintaining better quality and accuracy.'}, 'zh': {'title': 'å¸¸åŠ é€Ÿåº¦æµï¼šæå‡å›¾åƒç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºå¸¸åŠ é€Ÿåº¦æµï¼ˆCAFï¼‰ï¼Œç”¨äºæ”¹è¿›å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚CAFé€šè¿‡å¼•å…¥åŠ é€Ÿåº¦ä½œä¸ºå¯å­¦ä¹ å˜é‡ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä¼°è®¡å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æµã€‚ä¸ä¼ ç»Ÿçš„ç›´çº¿è½¨è¿¹å‡è®¾ä¸åŒï¼ŒCAFå…è®¸åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è€ƒè™‘åŠ é€Ÿåº¦ï¼Œä»è€Œæé«˜ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCAFåœ¨å°‘æ­¥ç”Ÿæˆå’Œè€¦åˆä¿æŒæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.23266', 'title': 'TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models', 'url': 'https://huggingface.co/papers/2410.23266', 'abstract': "Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, how well do the models truly perform visual temporal reasoning? Our study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. To systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame Information Disparity. Following these principles, we introduce TOMATO, Temporal Reasoning Multimodal Evaluation, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. TOMATO comprises 1,484 carefully curated, human-annotated questions spanning six tasks (i.e., action count, direction, rotation, shape & trend, velocity & frequency, and visual cues), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. Our comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model. Moreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence. We believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending human world dynamics through the video modality.", 'score': 19, 'issue_id': 416, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '2743c77af808246f', 'authors': ['Ziyao Shangguan', 'Chuhan Li', 'Yuxuan Ding', 'Yanan Zheng', 'Yilun Zhao', 'Tesca Fitzgerald', 'Arman Cohan'], 'affiliations': ['Yale University', 'Allen Institute for AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23266.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#graphs', '#video', '#multimodal', '#survey'], 'emoji': 'ğŸ…', 'ru': {'title': 'TOMATO: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TOMATO Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ĞœĞ¤Ğœ) Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1484 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¿Ğ¾ ÑˆĞµÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ñ… Ğº 1417 Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ĞœĞ¤Ğœ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ². TOMATO Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ°Ñ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ ĞœĞ¤Ğœ Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ˜Ğ˜, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'TOMATO: A New Benchmark for Evaluating Temporal Reasoning in Video Understanding', 'desc': 'This paper investigates the true capabilities of Multimodal Foundation Models (MFMs) in visual temporal reasoning for video understanding. The authors argue that existing benchmarks may overstate the performance of these models, as many tasks can be solved using only a few frames or even out-of-order frames. To address this, they introduce TOMATO, a new benchmark designed to rigorously evaluate MFMs based on three principles: Multi-Frame Gain, Frame Order Sensitivity, and Frame Information Disparity. Their findings reveal a significant performance gap between human understanding and model capabilities, highlighting the need for improved temporal reasoning in future AI systems.'}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„æ—¶é—´æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆMFMï¼‰åœ¨è§†é¢‘ç†è§£ä¸­çš„è§†è§‰æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å¯èƒ½é«˜ä¼°äº†è¿™äº›æ¨¡å‹çš„è¡¨ç°ï¼Œå› ä¸ºè®¸å¤šé—®é¢˜å¯ä»¥é€šè¿‡å°‘é‡æˆ–æ— åºçš„å¸§æ¥è§£å†³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ä¸ªåŸåˆ™å’Œç›¸åº”çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶å¼•å…¥äº†TOMATOåŸºå‡†ï¼Œä»¥ç³»ç»Ÿè¯„ä¼°MFMåœ¨è§†é¢‘ç†è§£ä¸­çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹ä¸äººç±»è¡¨ç°ä¹‹é—´å­˜åœ¨57.3%çš„å·®è·ï¼Œæ­ç¤ºäº†MFMåœ¨å¤„ç†è¿ç»­å¸§æ—¶çš„åŸºæœ¬å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00776', 'title': 'Randomized Autoregressive Visual Generation', 'url': 'https://huggingface.co/papers/2411.00776', 'abstract': "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer", 'score': 17, 'issue_id': 408, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '0cc2c0f19f735f79', 'authors': ['Qihang Yu', 'Ju He', 'Xueqing Deng', 'Xiaohui Shen', 'Liang-Chieh Chen'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00776.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#optimization', '#open_source', '#architecture'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ğ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ°Ñ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Randomized AutoRegressive modeling (RAR) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. RAR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ImageNet-256 Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¼ FID 1.48. RAR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Image Generation with Randomized AutoRegressive Modeling', 'desc': 'This paper introduces Randomized AutoRegressive modeling (RAR), a novel approach for generating images that enhances performance while remaining compatible with existing language modeling techniques. RAR employs a unique training method where the input sequence is randomly shuffled during the autoregressive training process, allowing the model to learn from various factorization orders. This strategy helps the model to better understand and utilize bidirectional contexts, leading to improved image generation capabilities. The results show that RAR achieves a remarkable FID score of 1.48 on the ImageNet-256 benchmark, outperforming previous state-of-the-art methods in both autoregressive and diffusion-based image generation.'}, 'zh': {'title': 'éšæœºè‡ªå›å½’å»ºæ¨¡ï¼šå›¾åƒç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§éšæœºè‡ªå›å½’å»ºæ¨¡ï¼ˆRARï¼‰æ–¹æ³•ç”¨äºè§†è§‰ç”Ÿæˆï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›æ€§èƒ½ï¼ŒåŒæ—¶ä¸è¯­è¨€å»ºæ¨¡æ¡†æ¶å®Œå…¨å…¼å®¹ã€‚RARæ–¹æ³•ç®€å•ï¼šåœ¨æ ‡å‡†çš„è‡ªå›å½’è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¾“å…¥åºåˆ—é€šå¸¸æŒ‰å…‰æ …å½¢å¼æ’åˆ—ï¼Œä½†ä»¥æ¦‚ç‡réšæœºæ‰“ä¹±ä¸ºä¸åŒçš„å› å­åŒ–é¡ºåºï¼Œrä»1å¼€å§‹ï¼Œéšç€è®­ç»ƒçº¿æ€§è¡°å‡åˆ°0ã€‚è¿™ç§é€€ç«è®­ç»ƒç­–ç•¥ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æœ€å¤§åŒ–æ‰€æœ‰å› å­åŒ–é¡ºåºçš„æœŸæœ›ä¼¼ç„¶ï¼Œä»è€Œæœ‰æ•ˆæé«˜æ¨¡å‹å»ºæ¨¡åŒå‘ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚RARä¿æŒäº†è‡ªå›å½’å»ºæ¨¡æ¡†æ¶çš„å®Œæ•´æ€§ï¼Œç¡®ä¿ä¸è¯­è¨€å»ºæ¨¡çš„å®Œå…¨å…¼å®¹ï¼ŒåŒæ—¶åœ¨å›¾åƒç”Ÿæˆä¸­æ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00660', 'title': 'Physics in Next-token Prediction', 'url': 'https://huggingface.co/papers/2411.00660', 'abstract': "We discovered the underlying physics in Next-token Prediction (NTP). We identified the law of information conservation within NTP and proposed the First Law of Information Capacity (IC-1), demonstrating that the essence of intelligence emergence in auto-regressive models is fundamentally a process of information transfer. We also introduced Landauer's Principle into NTP, formulating the Second Law of Information Capacity (IC-2), which establishes the relationship between auto-regressive model training and energy consumption. Additionally, we presented several corollaries, which hold practical significance for production practices. Finally, we validated the compatibility and complementarity of our findings with existing theories.", 'score': 14, 'issue_id': 416, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '9114a8de1a432c2d', 'authors': ['Hongjun An', 'Yiliang Song', 'Xuelong Li'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00660.jpg', 'data': {'categories': ['#science', '#optimization', '#math', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° (NTP). ĞĞ½Ğ¸ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ ÑÑƒÑ‚Ğ¸ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ²ĞµĞ´Ñ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ›Ğ°Ğ½Ğ´Ğ°ÑƒÑÑ€Ğ° Ğ² NTP, ÑƒÑ‡ĞµĞ½Ñ‹Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ĞµĞ¼ĞºĞ¾ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unveiling the Physics of Next-Token Prediction', 'desc': "This paper explores the physics behind Next-token Prediction (NTP) in machine learning. It introduces the First Law of Information Capacity (IC-1), which highlights how intelligence in auto-regressive models arises from information transfer. The authors also apply Landauer's Principle to NTP, leading to the Second Law of Information Capacity (IC-2), linking model training to energy consumption. The findings are supported by practical corollaries and align with existing theories in the field."}, 'zh': {'title': 'æ­ç¤ºè‡ªå›å½’æ¨¡å‹ä¸­çš„ä¿¡æ¯ä¸èƒ½é‡å…³ç³»', 'desc': 'æˆ‘ä»¬å‘ç°äº†ä¸‹ä¸€æ­¥é¢„æµ‹ï¼ˆNTPï¼‰ä¸­çš„åŸºæœ¬ç‰©ç†åŸç†ã€‚æˆ‘ä»¬è¯†åˆ«äº†NTPä¸­çš„ä¿¡æ¯å®ˆæ’å®šå¾‹ï¼Œå¹¶æå‡ºäº†ä¿¡æ¯å®¹é‡ç¬¬ä¸€å®šå¾‹ï¼ˆIC-1ï¼‰ï¼Œè¯æ˜äº†è‡ªå›å½’æ¨¡å‹ä¸­æ™ºèƒ½å‡ºç°çš„æœ¬è´¨æ˜¯ä¿¡æ¯ä¼ é€’çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜å°†æœ—é“åŸç†å¼•å…¥NTPï¼Œåˆ¶å®šäº†ä¿¡æ¯å®¹é‡ç¬¬äºŒå®šå¾‹ï¼ˆIC-2ï¼‰ï¼Œå»ºç«‹äº†è‡ªå›å½’æ¨¡å‹è®­ç»ƒä¸èƒ½é‡æ¶ˆè€—ä¹‹é—´çš„å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å‡ ä¸ªå…·æœ‰å®é™…æ„ä¹‰çš„æ¨è®ºï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ä¸ç°æœ‰ç†è®ºçš„å…¼å®¹æ€§å’Œäº’è¡¥æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.24159', 'title': 'GPT or BERT: why not both?', 'url': 'https://huggingface.co/papers/2410.24159', 'abstract': 'We present a simple way to merge masked language modeling with causal language modeling. This hybrid training objective results in a model that combines the strengths of both modeling paradigms within a single transformer stack: GPT-BERT can be transparently used like any standard causal or masked language model. We test the pretraining process that enables this flexible behavior on the BabyLM Challenge 2024. The results show that the hybrid pretraining outperforms masked-only or causal-only models. We openly release the models, training corpora and code.', 'score': 13, 'issue_id': 415, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'f46bbe7c538f7a87', 'authors': ['Lucas Georges Gabriel Charpentier', 'David Samuel'], 'affiliations': ['University of Oslo, Language Technology Group'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24159.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: Ğ»ÑƒÑ‡ÑˆĞµĞµ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT-BERT, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… BabyLM Challenge 2024 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğµ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¸ ĞºĞ¾Ğ´.'}, 'en': {'title': 'Merging Masked and Causal Language Models for Enhanced Performance', 'desc': 'This paper introduces a novel approach that merges masked language modeling (MLM) with causal language modeling (CLM) into a single transformer architecture. The resulting model, named GPT-BERT, leverages the advantages of both paradigms, allowing it to function effectively as either a standard masked or causal language model. The authors evaluate this hybrid training method on the BabyLM Challenge 2024, demonstrating that it outperforms models trained exclusively with either MLM or CLM. Additionally, they provide open access to the models, training data, and code to facilitate further research.'}, 'zh': {'title': 'æ··åˆé¢„è®­ç»ƒï¼Œæ¨¡å‹æ€§èƒ½åŒèµ¢ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†æ©ç è¯­è¨€å»ºæ¨¡ä¸å› æœè¯­è¨€å»ºæ¨¡ç›¸ç»“åˆçš„ç®€å•æ–¹æ³•ã€‚è¿™ç§æ··åˆè®­ç»ƒç›®æ ‡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å•ä¸ªå˜æ¢å™¨æ¶æ„ä¸­ç»“åˆä¸¤ç§å»ºæ¨¡èŒƒå¼çš„ä¼˜ç‚¹ã€‚æˆ‘ä»¬åœ¨2024å¹´BabyLMæŒ‘æˆ˜èµ›ä¸­æµ‹è¯•äº†è¿™ç§çµæ´»è¡Œä¸ºçš„é¢„è®­ç»ƒè¿‡ç¨‹ã€‚ç»“æœè¡¨æ˜ï¼Œæ··åˆé¢„è®­ç»ƒçš„æ€§èƒ½ä¼˜äºä»…ä½¿ç”¨æ©ç æˆ–ä»…ä½¿ç”¨å› æœçš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.22370', 'title': 'Survey of User Interface Design and Interaction Techniques in Generative AI Applications', 'url': 'https://huggingface.co/papers/2410.22370', 'abstract': 'The applications of generative AI have become extremely impressive, and the interplay between users and AI is even more so. Current human-AI interaction literature has taken a broad look at how humans interact with generative AI, but it lacks specificity regarding the user interface designs and patterns used to create these applications. Therefore, we present a survey that comprehensively presents taxonomies of how a human interacts with AI and the user interaction patterns designed to meet the needs of a variety of relevant use cases. We focus primarily on user-guided interactions, surveying interactions that are initiated by the user and do not include any implicit signals given by the user. With this survey, we aim to create a compendium of different user-interaction patterns that can be used as a reference for designers and developers alike. In doing so, we also strive to lower the entry barrier for those attempting to learn more about the design of generative AI applications.', 'score': 11, 'issue_id': 409, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '9701ceb4e85eeeba', 'authors': ['Reuben Luera', 'Ryan A. Rossi', 'Alexa Siu', 'Franck Dernoncourt', 'Tong Yu', 'Sungchul Kim', 'Ruiyi Zhang', 'Xiang Chen', 'Hanieh Salehy', 'Jian Zhao', 'Samyadeep Basu', 'Puneet Mathur', 'Nedim Lipka'], 'affiliations': ['University of California San Diego', 'Adobe Research', 'University of Waterloo', 'University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22370.jpg', 'data': {'categories': ['#agents', '#survey', '#architecture', '#interpretability'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑƒÑ‚ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ Ğ¿Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ˜Ğ˜ Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ…, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼, Ğ±ĞµĞ· ÑƒÑ‡ĞµÑ‚Ğ° Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ². Ğ¦ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ - ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ĞµÑ€Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ´Ğ»Ñ Ñ‚ĞµÑ…, ĞºÑ‚Ğ¾ Ñ…Ğ¾Ñ‡ĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜.'}, 'en': {'title': 'Enhancing User Interaction with Generative AI', 'desc': 'This paper surveys the ways users interact with generative AI, focusing specifically on user-guided interactions. It identifies and categorizes various user interface designs and interaction patterns that cater to different use cases. By providing a comprehensive taxonomy, the authors aim to serve as a reference for designers and developers in creating more effective generative AI applications. The goal is to make it easier for newcomers to understand and engage with the design aspects of these technologies.'}, 'zh': {'title': 'æå‡äººæœºäº¤äº’ï¼Œè®¾è®¡æ›´æ™ºèƒ½çš„ç”Ÿæˆå¼AIåº”ç”¨', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸ç”¨æˆ·ä¹‹é—´çš„äº’åŠ¨ï¼Œå¼ºè°ƒäº†ç”¨æˆ·ç•Œé¢è®¾è®¡çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä»½å…¨é¢çš„è°ƒæŸ¥ï¼Œåˆ†ç±»äº†äººç±»ä¸AIçš„äº’åŠ¨æ–¹å¼ï¼Œç‰¹åˆ«å…³æ³¨ç”¨æˆ·ä¸»å¯¼çš„äº¤äº’æ¨¡å¼ã€‚é€šè¿‡è¿™é¡¹è°ƒæŸ¥ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸ºè®¾è®¡å¸ˆå’Œå¼€å‘è€…æä¾›ä¸åŒçš„ç”¨æˆ·äº¤äº’æ¨¡å¼å‚è€ƒï¼Œé™ä½å­¦ä¹ ç”Ÿæˆå¼AIåº”ç”¨è®¾è®¡çš„é—¨æ§›ã€‚æœ€ç»ˆç›®æ ‡æ˜¯æå‡äººæœºäº¤äº’çš„æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.23775', 'title': 'In-Context LoRA for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2410.23775', 'abstract': 'Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20sim 100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA', 'score': 10, 'issue_id': 407, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '748dab03a37a21a4', 'authors': ['Lianghua Huang', 'Wei Wang', 'Zhi-Fan Wu', 'Yupeng Shi', 'Huanzhang Dou', 'Chen Liang', 'Yutong Feng', 'Yu Liu', 'Jingren Zhou'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23775.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° DiT Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DiT Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑƒĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ pipeline, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ LoRA-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ IC-LoRA, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼.'}, 'en': {'title': 'Unlocking In-Context Generation with IC-LoRA', 'desc': 'This paper investigates the use of diffusion transformers (DiTs) for generating images without being tied to specific tasks. The authors propose that DiTs can generate images effectively with minimal adjustments, leveraging their inherent in-context generation capabilities. They introduce a new method called In-Context LoRA (IC-LoRA), which simplifies the process by concatenating images and using joint captioning, along with small dataset tuning. This approach enhances the quality of generated images while maintaining a flexible architecture that can adapt to various tasks without extensive retraining.'}, 'zh': {'title': 'æ¿€æ´»ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œæå‡å›¾åƒç”Ÿæˆè´¨é‡', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰åœ¨æ— ä»»åŠ¡ç‰¹å®šçš„å›¾åƒç”Ÿæˆä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„DiTsæœ¬èº«å…·å¤‡ä¸Šä¸‹æ–‡ç”Ÿæˆèƒ½åŠ›ï¼Œåªéœ€å°‘é‡è°ƒæ•´å³å¯æ¿€æ´»ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç°æœ‰çš„æ–‡æœ¬åˆ°å›¾åƒDiTsèƒ½å¤Ÿåœ¨ä¸è°ƒæ•´çš„æƒ…å†µä¸‹æœ‰æ•ˆè¿›è¡Œä¸Šä¸‹æ–‡ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æµç¨‹ï¼Œåˆ©ç”¨DiTsçš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œç”Ÿæˆé«˜ä¿çœŸåº¦çš„å›¾åƒé›†ï¼Œä¸”ä¸éœ€è¦å¯¹åŸå§‹æ¨¡å‹è¿›è¡Œä¿®æ”¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00771', 'title': 'CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes', 'url': 'https://huggingface.co/papers/2411.00771', 'abstract': 'Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10times compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs. The project page is available at https://dekuliutesla.github.io/CityGaussianV2/.', 'score': 9, 'issue_id': 415, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '00c1f9c65cf89cfe', 'authors': ['Yang Liu', 'Chuanchen Luo', 'Zhongkai Mao', 'Junran Peng', 'Zhaoxiang Zhang'], 'affiliations': ['NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences', 'Centre for Artificial Intelligence and Robotic, HKISI', 'Shandong University', 'University of Science and Technology Beijing'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00771.jpg', 'data': {'categories': ['#benchmark', '#graphs', '#optimization', '#training', '#3d'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'CityGaussianV2: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Gaussian Splatting', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CityGaussianV2 - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² 3D Gaussian Splatting. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ½ĞµĞ´Ñ€ÑÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑƒĞ¿Ğ»Ğ¾Ñ‚Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ñ‹Ñ… Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸. Ğ”Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ ÑƒĞ´Ğ»Ğ¸Ğ½ĞµĞ½Ğ¸Ñ, ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ¾ÑÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³Ğ°ÑƒÑÑĞ¸Ğ°Ğ½Ğ¾Ğ². ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ CityGaussian Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ 10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° 25% Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 50%.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with CityGaussianV2', 'desc': 'This paper introduces CityGaussianV2, a new method for reconstructing large-scale 3D scenes using Gaussian splatting techniques. It addresses challenges in geometric accuracy and efficiency that arise from the unstructured nature of traditional 3D Gaussian Splatting. The authors implement a novel densification and depth regression technique to improve image clarity and speed up the training process. Additionally, they optimize the training pipeline to reduce memory usage and training time significantly while maintaining high visual quality and accuracy.'}, 'zh': {'title': 'é«˜æ•ˆç²¾å‡†çš„å¤§è§„æ¨¡åœºæ™¯é‡å»ºæ–°æ–¹æ³•', 'desc': '3Dé«˜æ–¯ç‚¹äº‘ï¼ˆ3DGSï¼‰åœ¨è¾å°„åœºé‡å»ºä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤æ‚åœºæ™¯ä¸­å‡†ç¡®è¡¨ç¤ºè¡¨é¢ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†CityGaussianV2ï¼Œä¸€ç§é’ˆå¯¹å¤§è§„æ¨¡åœºæ™¯é‡å»ºçš„æ–°æ–¹æ³•ï¼Œè§£å†³äº†å‡ ä½•ç²¾åº¦å’Œæ•ˆç‡çš„é—®é¢˜ã€‚æˆ‘ä»¬é‡‡ç”¨åˆ†è§£æ¢¯åº¦çš„å¯†é›†åŒ–å’Œæ·±åº¦å›å½’æŠ€æœ¯ï¼Œæ¶ˆé™¤æ¨¡ç³Šä¼ªå½±å¹¶åŠ é€Ÿæ”¶æ•›ã€‚é€šè¿‡å¼•å…¥å»¶ä¼¸æ»¤æ³¢å™¨ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å‡å°‘äº†é«˜æ–¯æ•°é‡çš„çˆ†ç‚¸ï¼ŒåŒæ—¶ä¼˜åŒ–äº†CityGaussianç®¡é“ï¼Œå®ç°äº†è®­ç»ƒæ—¶é—´å’Œå†…å­˜ä½¿ç”¨çš„æ˜¾è‘—èŠ‚çœã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00680', 'title': 'Zipfian Whitening', 'url': 'https://huggingface.co/papers/2411.00680', 'abstract': "The word embedding space in neural models is skewed, and correcting this can improve task performance. We point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are uniform; in reality, word frequencies follow a highly non-uniform distribution, known as Zipf's law. Surprisingly, simply performing PCA whitening weighted by the empirical word frequency that follows Zipf's law significantly improves task performance, surpassing established baselines. From a theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures. By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective, and in terms of the loss functions for imbalanced classification. Additionally, our theory corroborates that popular natural language processing methods, such as skip-gram negative sampling, WhiteningBERT, and headless language models, work well just because their word embeddings encode the empirical word frequency into the underlying probabilistic model.", 'score': 9, 'issue_id': 413, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '26e33c177a131240', 'authors': ['Sho Yokoi', 'Han Bao', 'Hiroto Kurita', 'Hidetoshi Shimodaira'], 'affiliations': ['Tohoku University', 'RIKEN', 'Kyoto University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00680.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#math', '#data', '#architecture'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ° Ğ¦Ğ¸Ğ¿Ñ„Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ² Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ ÑÑ‚Ğ¾Ğ¹ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ·Ğ°ĞºĞ¾Ğ½Ğ° Ğ¦Ğ¸Ğ¿Ñ„Ğ° Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ². ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ PCA Ğ¾Ñ‚Ğ±ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğµ ÑĞ»Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Correcting Skewness in Word Embeddings for Better Performance', 'desc': "This paper discusses how the word embedding space in neural models can be improved by addressing its skewness. It highlights that many existing methods assume uniform word frequencies, while in reality, word frequencies follow Zipf's law, which is highly non-uniform. The authors propose using PCA whitening that is weighted by empirical word frequencies, leading to significant improvements in task performance. Their theoretical framework categorizes word representations based on exponential families, emphasizing the importance of low-frequency words and providing insights into popular NLP methods."}, 'zh': {'title': 'æ ¡æ­£è¯åµŒå…¥ç©ºé—´ï¼Œæå‡ä»»åŠ¡æ€§èƒ½ï¼', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ç¥ç»æ¨¡å‹ä¸­çš„è¯åµŒå…¥ç©ºé—´åæ–œé—®é¢˜ï¼Œå¹¶æå‡ºäº†é€šè¿‡æ ¡æ­£è¿™ä¸€åæ–œæ¥æé«˜ä»»åŠ¡æ€§èƒ½çš„æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è®¸å¤šæ¨¡å‹å‡è®¾è¯é¢‘æ˜¯å‡åŒ€åˆ†å¸ƒçš„ï¼Œä½†å®é™…ä¸Šï¼Œè¯é¢‘éµå¾ªä¸€ç§ç§°ä¸ºé½å¤«å®šå¾‹çš„é«˜åº¦éå‡åŒ€åˆ†å¸ƒã€‚é€šè¿‡å¯¹è¯é¢‘è¿›è¡ŒåŠ æƒçš„ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç™½åŒ–ï¼Œæ˜¾è‘—æå‡äº†ä»»åŠ¡æ€§èƒ½ï¼Œè¶…è¶Šäº†å·²æœ‰çš„åŸºå‡†ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•å¯ä»¥æ¸…æ™°åœ°åˆ†ç±»ï¼Œå¼ºè°ƒäº†ä½é¢‘è¯çš„é‡è¦æ€§ï¼Œå¹¶è§£é‡Šäº†æµè¡Œçš„è‡ªç„¶è¯­è¨€å¤„ç†æ–¹æ³•ä¸ºä½•æœ‰æ•ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00412', 'title': 'Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation', 'url': 'https://huggingface.co/papers/2411.00412', 'abstract': "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but often produce hallucinations for complex ones. While integrating LLMs with tools can increase reliability, this approach typically results in over-reliance on tools, diminishing the model's ability to solve simple problems through basic reasoning. In contrast, human experts first assess problem complexity using domain knowledge before choosing an appropriate solution approach. Inspired by this human problem-solving process, we propose a novel two-component fine-tuning method. In the first component World Knowledge Distillation (WKD), LLMs learn directly from solutions generated using tool's information to internalize domain knowledge. In the second component Tool Usage Adaptation (TUA), we partition problems into easy and hard categories based on the model's direct answering accuracy. While maintaining the same alignment target for easy problems as in WKD, we train the model to intelligently switch to tool usage for more challenging problems. We validate our method on six scientific benchmark datasets, spanning mathematics, climate science and epidemiology. On average, our models demonstrate a 28.18% improvement in answer accuracy and a 13.89% increase in tool usage precision across all datasets, surpassing state-of-the-art models including GPT-4o and Claude-3.5.", 'score': 9, 'issue_id': 407, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '27e4deefc7d09df0', 'authors': ['Bohan Lyu', 'Yadi Cao', 'Duncan Watson-Parris', 'Leon Bergen', 'Taylor Berg-Kirkpatrick', 'Rose Yu'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00412.jpg', 'data': {'categories': ['#science', '#reasoning', '#rl', '#hallucinations', '#benchmark', '#multilingual', '#math', '#training', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ: ĞºĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ˜Ğ˜ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… - Ğ¿Ñ€Ğ¸Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing LLMs: Smart Tool Use for Complex Problems', 'desc': "This paper addresses the limitations of Large Language Models (LLMs) in solving complex scientific problems, which often lead to inaccuracies or 'hallucinations'. The authors propose a two-component fine-tuning method that mimics human problem-solving strategies by first assessing problem complexity. The first component, World Knowledge Distillation (WKD), allows LLMs to learn from solutions that utilize external tools, while the second component, Tool Usage Adaptation (TUA), helps the model categorize problems as easy or hard and decide when to use tools. The proposed method shows significant improvements in accuracy and tool usage precision across various scientific datasets, outperforming existing models."}, 'zh': {'title': 'æ™ºèƒ½åˆ‡æ¢ï¼Œæå‡æ¨¡å‹è§£å†³é—®é¢˜çš„èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£å†³ç®€å•ç§‘å­¦é—®é¢˜æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚é—®é¢˜ä¸Šå¸¸å¸¸å‡ºç°å¹»è§‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŒç»„ä»¶å¾®è°ƒæ–¹æ³•ï¼Œæ¨¡ä»¿äººç±»ä¸“å®¶çš„è§£å†³é—®é¢˜è¿‡ç¨‹ã€‚ç¬¬ä¸€ä¸ªç»„ä»¶æ˜¯ä¸–ç•ŒçŸ¥è¯†è’¸é¦ï¼ˆWKDï¼‰ï¼Œä½¿LLMsä»å·¥å…·ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆä¸­å­¦ä¹ é¢†åŸŸçŸ¥è¯†ã€‚ç¬¬äºŒä¸ªç»„ä»¶æ˜¯å·¥å…·ä½¿ç”¨é€‚åº”ï¼ˆTUAï¼‰ï¼Œæ ¹æ®æ¨¡å‹çš„ç›´æ¥å›ç­”å‡†ç¡®æ€§å°†é—®é¢˜åˆ†ä¸ºç®€å•å’Œå›°éš¾ä¸¤ç±»ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸Šçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00225', 'title': 'Fashion-VDM: Video Diffusion Model for Virtual Try-On', 'url': 'https://huggingface.co/papers/2411.00225', 'abstract': "We present Fashion-VDM, a video diffusion model (VDM) for generating virtual try-on videos. Given an input garment image and person video, our method aims to generate a high-quality try-on video of the person wearing the given garment, while preserving the person's identity and motion. Image-based virtual try-on has shown impressive results; however, existing video virtual try-on (VVT) methods are still lacking garment details and temporal consistency. To address these issues, we propose a diffusion-based architecture for video virtual try-on, split classifier-free guidance for increased control over the conditioning inputs, and a progressive temporal training strategy for single-pass 64-frame, 512px video generation. We also demonstrate the effectiveness of joint image-video training for video try-on, especially when video data is limited. Our qualitative and quantitative experiments show that our approach sets the new state-of-the-art for video virtual try-on. For additional results, visit our project page: https://johannakarras.github.io/Fashion-VDM.", 'score': 7, 'issue_id': 418, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'a412815c3df113c6', 'authors': ['Johanna Karras', 'Yingwei Li', 'Nan Liu', 'Luyang Zhu', 'Innfarn Yoo', 'Andreas Lugmayr', 'Chris Lee', 'Ira Kemelmacher-Shlizerman'], 'affiliations': ['Google Research, University of Washington, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00225.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#video', '#optimization', '#training', '#games', '#architecture'], 'emoji': 'ğŸ‘š', 'ru': {'title': 'Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Fashion-VDM - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¾Ğ´ĞµÑ‚Ğ¾Ğ³Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ±ĞµcĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 64-ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 512 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Virtual Try-Ons with Fashion-VDM!', 'desc': 'Fashion-VDM is a novel video diffusion model designed to create realistic virtual try-on videos by combining garment images with person videos. The model focuses on maintaining the identity and motion of the person while accurately displaying the garment. It addresses challenges in existing video virtual try-on methods, such as lack of detail and temporal consistency, by utilizing a diffusion-based architecture and a progressive training strategy. Our experiments demonstrate that Fashion-VDM achieves state-of-the-art results in video virtual try-on, even with limited video data.'}, 'zh': {'title': 'Fashion-VDMï¼šè§†é¢‘è™šæ‹Ÿè¯•ç©¿çš„æ–°çªç ´', 'desc': 'æˆ‘ä»¬æå‡ºäº†Fashion-VDMï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç”Ÿæˆè™šæ‹Ÿè¯•ç©¿è§†é¢‘çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ–¹æ³•å¯ä»¥æ ¹æ®è¾“å…¥çš„æœè£…å›¾åƒå’Œäººç‰©è§†é¢‘ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è¯•ç©¿è§†é¢‘ï¼ŒåŒæ—¶ä¿æŒäººç‰©çš„èº«ä»½å’ŒåŠ¨ä½œã€‚ä¸ç°æœ‰çš„è§†é¢‘è™šæ‹Ÿè¯•ç©¿æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœè£…ç»†èŠ‚å’Œæ—¶é—´ä¸€è‡´æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFashion-VDMåœ¨è§†é¢‘è™šæ‹Ÿè¯•ç©¿é¢†åŸŸè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00762', 'title': 'Face Anonymization Made Simple', 'url': 'https://huggingface.co/papers/2411.00762', 'abstract': 'Current face anonymization techniques often depend on identity loss calculated by face recognition models, which can be inaccurate and unreliable. Additionally, many methods require supplementary data such as facial landmarks and masks to guide the synthesis process. In contrast, our approach uses diffusion models with only a reconstruction loss, eliminating the need for facial landmarks or masks while still producing images with intricate, fine-grained details. We validated our results on two public benchmarks through both quantitative and qualitative evaluations. Our model achieves state-of-the-art performance in three key areas: identity anonymization, facial attribute preservation, and image quality. Beyond its primary function of anonymization, our model can also perform face swapping tasks by incorporating an additional facial image as input, demonstrating its versatility and potential for diverse applications. Our code and models are available at https://github.com/hanweikung/face_anon_simple .', 'score': 7, 'issue_id': 415, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': '0948ed1ff23fd58b', 'authors': ['Han-Wei Kung', 'Tuomas Varanka', 'Sanjay Saha', 'Terence Sim', 'Nicu Sebe'], 'affiliations': ['University of Trento', 'University of Oulu', 'National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00762.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#open_source', '#architecture'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ† Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ»Ğ¸Ñ†ĞµĞ²Ñ‹Ñ… Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ¾Ğ² Ğ»Ğ¸Ñ†Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¾Ğ½Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½Ğµ Ğ»Ğ¸Ñ†, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑĞ²Ğ¾Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Revolutionizing Face Anonymization with Diffusion Models', 'desc': "This paper presents a novel face anonymization technique that utilizes diffusion models, focusing solely on reconstruction loss without the need for additional data like facial landmarks or masks. The method achieves high-quality image generation while effectively anonymizing identities and preserving facial attributes. The authors demonstrate the model's performance through rigorous evaluations on public benchmarks, showcasing its state-of-the-art results in identity anonymization and image quality. Additionally, the model's versatility is highlighted by its capability to perform face swapping tasks, making it suitable for various applications."}, 'zh': {'title': 'åˆ›æ–°çš„é¢éƒ¨åŒ¿ååŒ–ä¸äº¤æ¢æŠ€æœ¯', 'desc': 'å½“å‰çš„é¢éƒ¨åŒ¿ååŒ–æŠ€æœ¯é€šå¸¸ä¾èµ–äºé¢éƒ¨è¯†åˆ«æ¨¡å‹è®¡ç®—çš„èº«ä»½æŸå¤±ï¼Œè¿™å¯èƒ½ä¸å¤Ÿå‡†ç¡®å’Œå¯é ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨æ‰©æ•£æ¨¡å‹ï¼Œä»…ä¾èµ–é‡å»ºæŸå¤±ï¼Œçœå»äº†é¢éƒ¨ç‰¹å¾ç‚¹æˆ–é¢å…·çš„éœ€æ±‚ï¼ŒåŒæ—¶ä»èƒ½ç”Ÿæˆç»†è‡´çš„å›¾åƒã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨èº«ä»½åŒ¿ååŒ–ã€é¢éƒ¨å±æ€§ä¿ç•™å’Œå›¾åƒè´¨é‡ä¸‰ä¸ªå…³é”®é¢†åŸŸè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚é™¤äº†åŒ¿ååŒ–åŠŸèƒ½å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜å¯ä»¥é€šè¿‡æ·»åŠ é¢å¤–çš„é¢éƒ¨å›¾åƒä½œä¸ºè¾“å…¥æ¥æ‰§è¡Œé¢éƒ¨äº¤æ¢ä»»åŠ¡ï¼Œå±•ç¤ºäº†å…¶å¤šæ ·æ€§å’Œæ½œåœ¨åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00233', 'title': 'SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models', 'url': 'https://huggingface.co/papers/2411.00233', 'abstract': 'The state of health (SOH) of a Li-ion battery is a critical parameter that determines the remaining capacity and the remaining lifetime of the battery. In this paper, we propose SambaMixer a novel structured state space model (SSM) for predicting the state of health of Li-ion batteries. The proposed SSM is based on the MambaMixer architecture, which is designed to handle multi-variate time signals. We evaluate our model on the NASA battery discharge dataset and show that our model outperforms the state-of-the-art on this dataset. We further introduce a novel anchor-based resampling method which ensures time signals are of the expected length while also serving as augmentation technique. Finally, we condition prediction on the sample time and the cycle time difference using positional encodings to improve the performance of our model and to learn recuperation effects. Our results proof that our model is able to predict the SOH of Li-ion batteries with high accuracy and robustness.', 'score': 7, 'issue_id': 410, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '6361ca66f5ca137f', 'authors': ['JosÃ© Ignacio Olalde-Verano', 'Sascha Kirch', 'Clara PÃ©rez-Molina', 'Sergio Martin'], 'affiliations': ['UNED - Universidad Nacional de Educacion Distancia, Madrid, Spain'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00233.jpg', 'data': {'categories': ['#science', '#optimization', '#healthcare', '#training', '#dataset', '#architecture'], 'emoji': 'ğŸ”‹', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ¾ĞºĞ° ÑĞ»ÑƒĞ¶Ğ±Ñ‹ Ğ°ĞºĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SambaMixer - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Ğ»Ğ¸Ñ‚Ğ¸Ğ¹-Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ°ĞºĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ MambaMixer Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞºĞ¾Ñ€ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¸ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ñ‹ Ñ†Ğ¸ĞºĞ»Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°ĞºĞºÑƒĞ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ².'}, 'en': {'title': 'SambaMixer: Revolutionizing Li-ion Battery Health Prediction', 'desc': 'This paper introduces SambaMixer, a new structured state space model (SSM) designed to predict the state of health (SOH) of Li-ion batteries. The model utilizes the MambaMixer architecture to effectively process multi-variate time signals, enhancing prediction accuracy. It is evaluated against the NASA battery discharge dataset, demonstrating superior performance compared to existing methods. Additionally, the paper presents an innovative anchor-based resampling technique and employs positional encodings to improve predictions by accounting for time-related factors.'}, 'zh': {'title': 'é«˜æ•ˆé¢„æµ‹é”‚ç¦»å­ç”µæ± å¥åº·çŠ¶æ€çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç»“æ„åŒ–çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰ï¼Œç”¨äºé¢„æµ‹é”‚ç¦»å­ç”µæ± çš„å¥åº·çŠ¶æ€ï¼ˆSOHï¼‰ã€‚è¯¥æ¨¡å‹åŸºäºMambaMixeræ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†å¤šå˜é‡æ—¶é—´ä¿¡å·ã€‚æˆ‘ä»¬åœ¨NASAç”µæ± æ”¾ç”µæ•°æ®é›†ä¸Šè¯„ä¼°äº†è¯¥æ¨¡å‹ï¼Œç»“æœæ˜¾ç¤ºå…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŸºäºé”šç‚¹çš„é‡é‡‡æ ·æ–¹æ³•ï¼Œä»¥ç¡®ä¿æ—¶é—´ä¿¡å·çš„é¢„æœŸé•¿åº¦ï¼Œå¹¶ä½œä¸ºæ•°æ®å¢å¼ºæŠ€æœ¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.22901', 'title': 'HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models', 'url': 'https://huggingface.co/papers/2410.22901', 'abstract': 'We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (https://songkey.github.io/hellomeme).', 'score': 7, 'issue_id': 408, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '801963cbdcf75d7b', 'authors': ['Shengkai Zhang', 'Nianhong Jiao', 'Tian Li', 'Chaojie Yang', 'Chenhui Xue', 'Boya Niu', 'Jun Gao'], 'affiliations': ['HelloGroup Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22901.jpg', 'data': {'categories': ['#cv', '#video', '#training', '#open_source', '#games', '#architecture'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¼Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ² Ğ² Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ±Ñ‹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ¼Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ SD1.5, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ñ†ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing Text-to-Image Models with Adapter Integration', 'desc': 'This paper presents a novel method for integrating adapters into text-to-image foundation models, allowing them to perform complex tasks while maintaining their ability to generalize. The method focuses on optimizing the attention mechanism associated with 2D feature maps, which significantly boosts the performance of the adapters. The effectiveness of this approach was demonstrated through the task of meme video generation, yielding impressive results. The authors aim to contribute to the open-source community by sharing their code and providing insights for post-training tasks in large text-to-image models.'}, 'zh': {'title': 'é€‚é…å™¨æ’å…¥ï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå°†é€‚é…å™¨æ’å…¥æ–‡æœ¬åˆ°å›¾åƒçš„åŸºç¡€æ¨¡å‹ä¸­ï¼Œä»è€Œåœ¨æ‰§è¡Œå¤æ‚çš„ä¸‹æ¸¸ä»»åŠ¡æ—¶ä¿æŒåŸºç¡€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¼˜åŒ–ä¸äºŒç»´ç‰¹å¾å›¾ç›¸å…³çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œå¢å¼ºé€‚é…å™¨çš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ç”Ÿæˆè¡¨æƒ…åŒ…è§†é¢‘çš„ä»»åŠ¡ä¸ŠéªŒè¯äº†è¯¥æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½ä¸ºå¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åè®­ç»ƒä»»åŠ¡æä¾›ä¸€äº›è§è§£ï¼Œå¹¶ä¸ºå¼€æºç¤¾åŒºå¸¦æ¥ä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00369', 'title': 'GRS-QA -- Graph Reasoning-Structured Question Answering Dataset', 'url': 'https://huggingface.co/papers/2411.00369', 'abstract': 'Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.', 'score': 6, 'issue_id': 409, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': 'b3e4773e065d1bc1', 'authors': ['Anish Pahilajani', 'Devasha Trivedi', 'Jincen Shuai', 'Khin S. Yone', 'Samyak Rajesh Jain', 'Namyong Park', 'Ryan A. Rossi', 'Nesreen K. Ahmed', 'Franck Dernoncourt', 'Yu Wang'], 'affiliations': ['University of California Santa Cruz', 'Adobe Research', 'Cisco Outshift', 'University of Oregon'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00369.jpg', 'data': {'categories': ['#rag', '#reasoning', '#cv', '#graphs', '#dataset', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“Ñ€Ğ°Ñ„Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GRS-QA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. GRS-QA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ĞºĞ°Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹, Ñ‚Ğ°Ğº Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ ÑƒĞ·Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹, Ğ° Ñ€ĞµĞ±Ñ€Ğ° Ğ¾Ğ±Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ°ÑÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²ÑĞ·Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking LLMs: Understanding Reasoning Structures in Multi-Hop QA', 'desc': 'This paper discusses the performance of Large Language Models (LLMs) in multi-hop question-answering (M-QA) tasks, focusing on their reasoning abilities. The authors identify a gap in existing datasets that do not provide detailed reasoning structures, which are crucial for evaluating LLM performance. To fill this gap, they introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which features reasoning graphs that clearly outline the logical pathways for answering questions. Their analysis shows that LLMs exhibit varying performance based on the complexity of the reasoning structures involved, highlighting the importance of understanding both textual and semantic elements in M-QA.'}, 'zh': {'title': 'æ­ç¤ºæ¨ç†ç»“æ„å¯¹LLMè¡¨ç°çš„å½±å“', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šè·³é—®ç­”ï¼ˆM-QAï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸»è¦å¾—ç›Šäºå…¶å…ˆè¿›çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMåœ¨å¤šè·³é—®ç­”ä¸­çš„è¡¨ç°å—å›ºæœ‰æ¨ç†ç»“æ„çš„å½±å“å°šä¸æ˜ç¡®ï¼Œä¸»è¦æ˜¯å› ä¸ºç¼ºä¹æä¾›ç»†ç²’åº¦æ¨ç†ç»“æ„çš„é—®ç­”æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å›¾æ¨ç†ç»“æ„é—®ç­”æ•°æ®é›†ï¼ˆGRS-QAï¼‰ï¼Œè¯¥æ•°æ®é›†ä¸ºé—®ç­”å¯¹æä¾›äº†è¯­ä¹‰ä¸Šä¸‹æ–‡å’Œæ¨ç†ç»“æ„ã€‚ä¸ç°æœ‰çš„M-QAæ•°æ®é›†ä¸åŒï¼ŒGRS-QAé€šè¿‡æ„å»ºæ¨ç†å›¾æ¥æ˜ç¡®æ•æ‰å¤æ‚çš„æ¨ç†è·¯å¾„ï¼ŒèŠ‚ç‚¹è¡¨ç¤ºæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼Œè¾¹è¡¨ç¤ºé€»è¾‘æµï¼Œä»è€Œå®ç°å¯¹LLMæ¨ç†èƒ½åŠ›çš„ç»†ç²’åº¦è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.21157', 'title': 'M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation', 'url': 'https://huggingface.co/papers/2410.21157', 'abstract': 'Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.', 'score': 6, 'issue_id': 408, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'd6a0779456870cae', 'authors': ['Jiaheng Liu', 'Ken Deng', 'Congnan Liu', 'Jian Yang', 'Shukai Liu', 'He Zhu', 'Peng Zhao', 'Linzheng Chai', 'Yanan Wu', 'Ke Jin', 'Ge Zhang', 'Zekun Wang', 'Guoan Zhang', 'Bangyu Xiang', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Alibaba Group', 'University of Waterloo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21157.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#plp', '#data', '#dataset', '#transfer_learning', '#open_source'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº M2RC-EVAL Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 18 ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… M2RC-INSTRUCT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… LLM Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Multilingual Code Completion with M2RC-EVAL and M2RC-INSTRUCT', 'desc': 'This paper introduces a new benchmark called M2RC-EVAL for repository-level code completion that supports 18 programming languages, addressing the limitations of existing benchmarks that only cover a few languages. It provides fine-grained annotations based on abstract syntax trees, allowing for a more detailed evaluation of code completion scenarios. Additionally, the authors present the M2RC-INSTRUCT dataset to enhance the performance of code Large Language Models (LLMs) in multilingual contexts. Experimental results show that both M2RC-EVAL and M2RC-INSTRUCT significantly improve the capabilities of existing code LLMs.'}, 'zh': {'title': 'å¤šè¯­è¨€ä»£ç è¡¥å…¨çš„æ–°åŸºå‡†æµ‹è¯•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šè¯­è¨€ä»£ç è¡¥å…¨åŸºå‡†æµ‹è¯•ï¼Œç§°ä¸ºM2RC-EVALï¼Œæ¶µç›–äº†18ç§ç¼–ç¨‹è¯­è¨€ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸åªå…³æ³¨å°‘æ•°å‡ ç§è¯­è¨€ï¼Œæ— æ³•å…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸åŒè¯­è¨€ä¸­çš„ä»£ç æ™ºèƒ½èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒM2RC-EVALæä¾›äº†ç»†ç²’åº¦çš„æ³¨é‡Šï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£æ¨¡å‹åœ¨ä¸åŒè¡¥å…¨åœºæ™¯ä¸‹çš„è¡¨ç°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ä»£ç è¡¥å…¨èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªå¤šè¯­è¨€æŒ‡ä»¤æ•°æ®é›†M2RC-INSTRUCTã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00030', 'title': 'WikiNER-fr-gold: A Gold-Standard NER Corpus', 'url': 'https://huggingface.co/papers/2411.00030', 'abstract': 'We address in this article the the quality of the WikiNER corpus, a multilingual Named Entity Recognition corpus, and provide a consolidated version of it. The annotation of WikiNER was produced in a semi-supervised manner i.e. no manual verification has been carried out a posteriori. Such corpus is called silver-standard. In this paper we propose WikiNER-fr-gold which is a revised version of the French proportion of WikiNER. Our corpus consists of randomly sampled 20% of the original French sub-corpus (26,818 sentences with 700k tokens). We start by summarizing the entity types included in each category in order to define an annotation guideline, and then we proceed to revise the corpus. Finally we present an analysis of errors and inconsistency observed in the WikiNER-fr corpus, and we discuss potential future work directions.', 'score': 4, 'issue_id': 411, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '5f3e739256c5e800', 'authors': ['Danrun Cao', 'Nicolas BÃ©chet', 'Pierre-FranÃ§ois Marteau'], 'affiliations': ['Univ. Bretagne Sud, CNRS, IRISA', 'OctopusMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00030.jpg', 'data': {'categories': ['#synthetic', '#multilingual', '#data', '#dataset', '#low_resource'], 'emoji': 'ğŸ·ï¸', 'ru': {'title': 'Ğ—Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ³Ğ¾ NER: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ WikiNER', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° WikiNER Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ WikiNER-fr-gold - Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ÑƒÑ Ğ¸Ğ· 20% Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ»Ğ¸ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼ WikiNER-fr. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ½Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ NER Ğ½Ğ° Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.'}, 'en': {'title': 'Enhancing Named Entity Recognition with WikiNER-fr-gold', 'desc': 'This paper focuses on improving the quality of the WikiNER corpus, which is used for Named Entity Recognition (NER) in multiple languages. The original WikiNER corpus was created using a semi-supervised approach, resulting in a silver-standard dataset without manual verification. The authors introduce WikiNER-fr-gold, a refined version of the French subset of WikiNER, based on a carefully sampled 20% of the original data. They outline the types of entities included, establish annotation guidelines, and analyze errors in the original corpus to suggest future improvements.'}, 'zh': {'title': 'æå‡WikiNERè¯­æ–™åº“è´¨é‡çš„æ¢ç´¢', 'desc': 'æœ¬æ–‡è®¨è®ºäº†WikiNERè¯­æ–™åº“çš„è´¨é‡ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè¯­è¨€å‘½åå®ä½“è¯†åˆ«è¯­æ–™åº“ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ•´åˆç‰ˆæœ¬ã€‚WikiNERçš„æ ‡æ³¨æ˜¯ä»¥åŠç›‘ç£çš„æ–¹å¼ç”Ÿæˆçš„ï¼Œå³æ²¡æœ‰è¿›è¡ŒåæœŸçš„äººå·¥éªŒè¯ï¼Œå› æ­¤è¯¥è¯­æ–™åº“è¢«ç§°ä¸ºé“¶æ ‡å‡†ã€‚æˆ‘ä»¬æå‡ºäº†WikiNER-fr-goldï¼Œè¿™æ˜¯WikiNERæ³•è¯­éƒ¨åˆ†çš„ä¿®è®¢ç‰ˆæœ¬ï¼ŒåŒ…å«äº†åŸæ³•è¯­å­è¯­æ–™åº“çš„20%éšæœºæŠ½æ ·ï¼ˆ26,818ä¸ªå¥å­ï¼Œ700kä¸ªæ ‡è®°ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†WikiNER-frè¯­æ–™åº“ä¸­è§‚å¯Ÿåˆ°çš„é”™è¯¯å’Œä¸ä¸€è‡´ï¼Œå¹¶è®¨è®ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10442', 'title': 'Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization', 'url': 'https://huggingface.co/papers/2411.10442', 'abstract': 'Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance. To address this, we introduce a preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset. and (2) on the model side, we explore integrating PO with MLLMs, developing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance. Our approach demonstrates improved performance across multiple benchmarks, particularly in multimodal reasoning tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10x larger InternVL2-76B. We hope this study could inspire further advancements in MLLMs. Code, data, and model shall be publicly released.', 'score': 49, 'issue_id': 722, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': '3cc3675b352b1634', 'authors': ['Weiyun Wang', 'Zhe Chen', 'Wenhai Wang', 'Yue Cao', 'Yangzhou Liu', 'Zhangwei Gao', 'Jinguo Zhu', 'Xizhou Zhu', 'Lewei Lu', 'Yu Qiao', 'Jifeng Dai'], 'affiliations': ['Fudan University', 'Nanjing University', 'OpenGVLab, Shanghai AI Laboratory', 'SenseTime Research', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10442.jpg', 'data': {'categories': ['#training', '#benchmark', '#reasoning', '#open_source', '#dataset', '#multimodal', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (PO). ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMPR Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (MPO). Ğ˜Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ InternVL2-8B-MPO Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ‚Ğ¸Ğ¿Ğ° Chain-of-Thought. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ MLLM Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Boosting Multimodal Reasoning with Preference Optimization', 'desc': 'This paper addresses the limitations of existing multimodal large language models (MLLMs) in reasoning tasks due to distribution shifts. It introduces a preference optimization (PO) process to enhance the Chain-of-Thought (CoT) performance of these models. The authors create a high-quality dataset called MMPR through an automated preference data construction pipeline and develop a method called Mixed Preference Optimization (MPO) to integrate PO with MLLMs. Their model, InternVL2-8B-MPO, shows significant improvements in multimodal reasoning tasks, achieving higher accuracy than previous models and demonstrating the potential for further advancements in this field.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åå¥½ä¼˜åŒ–ï¼ˆPOï¼‰è¿‡ç¨‹ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚é€šè¿‡æ„å»ºé«˜è´¨é‡çš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨ç†åå¥½æ•°æ®é›†MMPRï¼Œå¹¶å°†POä¸MLLMsç»“åˆï¼Œæˆ‘ä»¬å¼€å‘äº†æ··åˆåå¥½ä¼˜åŒ–ï¼ˆMPOï¼‰æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14402', 'title': 'Multimodal Autoregressive Pre-training of Large Vision Encoders', 'url': 'https://huggingface.co/papers/2411.14402', 'abstract': 'We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Our encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal image understanding across diverse settings.', 'score': 34, 'issue_id': 723, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '95826a974f0f9bb2', 'authors': ['Enrico Fini', 'Mustafa Shukor', 'Xiujun Li', 'Philipp Dufter', 'Michal Klein', 'David Haldimann', 'Sai Aitharaju', 'Victor Guilherme Turrisi da Costa', 'Louis BÃ©thune', 'Zhe Gan', 'Alexander T Toshev', 'Marcin Eichner', 'Moin Nabi', 'Yinfei Yang', 'Joshua M. Susskind', 'Alaaeldin El-Nouby'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2411.14402.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'AIMV2: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ AIMV2. ĞĞ½ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚. AIMV2 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'AIMV2: Unifying Vision and Text for Superior Multimodal Understanding', 'desc': 'This paper presents AIMV2, a new method for pre-training vision encoders that can handle both images and text. It builds on recent techniques in autoregressive pre-training, allowing the model to generate image patches and text tokens effectively. AIMV2 is designed to be scalable and performs exceptionally well on various tasks, including localization and classification. The results show that AIMV2 outperforms existing models like CLIP in multimodal image understanding, achieving high accuracy on benchmarks like ImageNet-1k.'}, 'zh': {'title': 'AIMV2ï¼šå¤šæ¨¡æ€è§†è§‰ç¼–ç çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¤§è§„æ¨¡è§†è§‰ç¼–ç å™¨é¢„è®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºæœ€è¿‘åœ¨è‡ªå›å½’è§†è§‰æ¨¡å‹é¢„è®­ç»ƒæ–¹é¢çš„è¿›å±•ï¼Œæ‰©å±•åˆ°å¤šæ¨¡æ€è®¾ç½®ï¼Œå³å›¾åƒå’Œæ–‡æœ¬ã€‚æˆ‘ä»¬ä»‹ç»äº†AIMV2ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é€šç”¨çš„è§†è§‰ç¼–ç å™¨ï¼Œå…·æœ‰ç®€å•çš„é¢„è®­ç»ƒè¿‡ç¨‹ã€å¯æ‰©å±•æ€§å’Œåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å“è¶Šè¡¨ç°ã€‚æˆ‘ä»¬çš„ç¼–ç å™¨åœ¨å¤šæ¨¡æ€è¯„ä¼°å’Œè§†è§‰åŸºå‡†æµ‹è¯•ï¼ˆå¦‚å®šä½ã€åŸºç¡€å’Œåˆ†ç±»ï¼‰ä¸­è¡¨ç°å‡ºè‰²ï¼ŒAIMV2-3Bç¼–ç å™¨åœ¨ImageNet-1kä¸Šå®ç°äº†89.5%çš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.13676', 'title': 'Hymba: A Hybrid-head Architecture for Small Language Models', 'url': 'https://huggingface.co/papers/2411.13676', 'abstract': 'We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the "forced-to-attend" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput.', 'score': 32, 'issue_id': 721, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '24009a4acf67d4c7', 'authors': ['Xin Dong', 'Yonggan Fu', 'Shizhe Diao', 'Wonmin Byeon', 'Zijia Chen', 'Ameya Sunil Mahabaleshwarkar', 'Shih-Yang Liu', 'Matthijs Van Keirsbilck', 'Min-Hung Chen', 'Yoshi Suhara', 'Yingyan Lin', 'Jan Kautz', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.13676.jpg', 'data': {'categories': ['#small_models', '#training', '#architecture', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Hymba: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Hymba - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ĞµĞ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ (SSM) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ°-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ñ…Ñ€Ğ°Ğ½ÑÑ‰Ğ¸Ğµ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ½Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµĞ¶ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğµ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ Ğ¾ĞºĞ½Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Hymba-1.5B-Base Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ÑĞµ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ 2 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ°Ğ¶Ğµ Llama-3.2-3B Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ĞºÑÑˆĞ° Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Hymba: Efficient Language Models with Hybrid Architecture', 'desc': 'Hymba is a new family of small language models that combines transformer attention with state space models (SSMs) to improve efficiency. The model uses attention heads for detailed recall and SSM heads for summarizing context effectively. It also features learnable meta tokens that help retain important information without overloading the attention mechanism. Our experiments show that Hymba outperforms existing small language models, achieving better accuracy while using significantly less cache and providing faster processing speeds.'}, 'zh': {'title': 'Hymbaï¼šé«˜æ•ˆçš„å°å‹è¯­è¨€æ¨¡å‹æ–°é€‰æ‹©', 'desc': 'æˆ‘ä»¬æå‡ºäº†Hymbaï¼Œè¿™æ˜¯ä¸€ç§å°å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ··åˆå¤´å¹¶è¡Œæ¶æ„ï¼Œå°†å˜æ¢å™¨æ³¨æ„æœºåˆ¶ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ç»“åˆï¼Œä»¥æé«˜æ•ˆç‡ã€‚æ³¨æ„å¤´æä¾›é«˜åˆ†è¾¨ç‡çš„å›å¿†ï¼Œè€ŒSSMå¤´åˆ™å®ç°é«˜æ•ˆçš„ä¸Šä¸‹æ–‡æ€»ç»“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯å­¦ä¹ çš„å…ƒæ ‡è®°ï¼Œè¿™äº›æ ‡è®°è¢«æ·»åŠ åˆ°æç¤ºå‰ï¼Œå­˜å‚¨å…³é”®ä¿¡æ¯ï¼Œå‡è½»äº†ä¸æ³¨æ„æœºåˆ¶ç›¸å…³çš„â€œå¼ºåˆ¶å…³æ³¨â€è´Ÿæ‹…ã€‚æˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡è·¨å±‚é”®å€¼ï¼ˆKVï¼‰å…±äº«å’Œéƒ¨åˆ†æ»‘åŠ¨çª—å£æ³¨æ„æœºåˆ¶è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œç»“æœæ˜¯ç¼“å­˜å¤§å°ç´§å‡‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14405', 'title': 'Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions', 'url': 'https://huggingface.co/papers/2411.14405', 'abstract': 'Currently OpenAI o1 has sparked a surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding -- which are well-suited for reinforcement learning (RL) -- but also places greater emphasis on open-ended resolutions. We aim to address the question: "Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?" Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies -- optimized for complex real-world problem-solving tasks.', 'score': 32, 'issue_id': 720, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'ef4a95abeea69237', 'authors': ['Yu Zhao', 'Huifeng Yin', 'Bo Zeng', 'Hao Wang', 'Tianqi Shi', 'Chenyang Lyu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce'], 'pdf_title_img': 'assets/pdf/title_img/2411.14405.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°: Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Marco-o1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ OpenAI o1 Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ°Ñ†ĞµĞ»ĞµĞ½Ğ° Ğ½Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ†Ğ¾Ğ¼, Ğ³Ğ´Ğµ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ñ‡ĞµÑ‚ĞºĞ¸Ğµ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Marco-o1 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞœĞ¾Ğ½Ñ‚Ğµ-ĞšĞ°Ñ€Ğ»Ğ¾. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ†ĞµĞ»ÑŒ - ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'Unlocking Generalization in Large Reasoning Models', 'desc': "This paper explores the capabilities of the Marco-o1 model in handling large reasoning tasks across various domains. It emphasizes the model's ability to generalize beyond traditional areas like mathematics and coding, where answers are clear-cut. The research investigates how Marco-o1 can tackle open-ended problems where standard solutions and quantifiable rewards are not readily available. Key techniques employed include Chain-of-Thought fine-tuning and Monte Carlo Tree Search, which enhance the model's reasoning and problem-solving abilities in complex scenarios."}, 'zh': {'title': 'æ¨åŠ¨æ¨ç†æ¨¡å‹çš„å¹¿æ³›åº”ç”¨', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯OpenAIçš„o1æ¨¡å‹ã€‚Marco-o1ä¸ä»…å…³æ³¨æ•°å­¦ã€ç‰©ç†å’Œç¼–ç¨‹ç­‰æœ‰æ ‡å‡†ç­”æ¡ˆçš„å­¦ç§‘ï¼Œè¿˜å¼ºè°ƒå¼€æ”¾å¼é—®é¢˜çš„è§£å†³èƒ½åŠ›ã€‚ç ”ç©¶çš„æ ¸å¿ƒé—®é¢˜æ˜¯o1æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¨å¹¿åˆ°ç¼ºä¹æ˜ç¡®æ ‡å‡†å’Œéš¾ä»¥é‡åŒ–å¥–åŠ±çš„æ›´å¹¿æ³›é¢†åŸŸã€‚Marco-o1ç»“åˆäº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰å¾®è°ƒã€è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ã€åæ€æœºåˆ¶å’Œåˆ›æ–°æ¨ç†ç­–ç•¥ï¼Œä»¥ä¼˜åŒ–å¤æ‚çš„ç°å®é—®é¢˜è§£å†³ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14199', 'title': 'OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs', 'url': 'https://huggingface.co/papers/2411.14199', 'abstract': "Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? We introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, we develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's 32%. We open-source all of our code, models, datastore, data and a public demo.", 'score': 21, 'issue_id': 720, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'f429efe07ec308f2', 'authors': ['Akari Asai', 'Jacqueline He', 'Rulin Shao', 'Weijia Shi', 'Amanpreet Singh', 'Joseph Chee Chang', 'Kyle Lo', 'Luca Soldaini', 'Sergey Feldman', "Mike D'arcy", 'David Wadden', 'Matt Latzke', 'Minyang Tian', 'Pan Ji', 'Shengyan Liu', 'Hao Tong', 'Bohao Wu', 'Yanyu Xiong', 'Luke Zettlemoyer', 'Graham Neubig', 'Dan Weld', 'Doug Downey', 'Wen-tau Yih', 'Pang Wei Koh', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for AI', 'Carnegie Mellon University', 'Meta', 'Stanford University', 'University of Illinois, Urbana-Champaign', 'University of North Carolina, Chapel Hill', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.14199.jpg', 'data': {'categories': ['#science', '#rag', '#open_source', '#multimodal', '#benchmark', '#hallucinations'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'OpenScholar: Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OpenScholar - ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 45 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ScholarQABench - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. OpenScholar-8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ GPT-4 Ğ¸ PaperQA2 Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ğ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ OpenScholar-8B Ğ¸ OpenScholar-GPT4o ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼ Ğ² 51% Ğ¸ 70% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾.'}, 'en': {'title': 'Empowering Scientific Research with OpenScholar: Accurate, Citation-Backed Insights', 'desc': 'This paper presents OpenScholar, a retrieval-augmented language model designed to assist researchers in synthesizing scientific literature. OpenScholar effectively identifies relevant passages from a vast collection of 45 million open-access papers and generates citation-backed responses to scientific queries. The model is evaluated using ScholarQABench, a benchmark that includes expert-written queries and answers across multiple domains, demonstrating superior performance in correctness compared to other models like GPT-4o. Additionally, OpenScholar shows a significant reduction in citation hallucination, achieving accuracy comparable to human experts, and enhances the performance of existing models through its innovative architecture.'}, 'zh': {'title': 'OpenScholarï¼šæå‡ç§‘å­¦æ–‡çŒ®æ£€ç´¢çš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOpenScholarçš„ä¸“ç”¨æ£€ç´¢å¢å¼ºè¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å¸®åŠ©ç§‘å­¦å®¶ä»4500ä¸‡ç¯‡å¼€æ”¾è·å–è®ºæ–‡ä¸­æå–ç›¸å…³ä¿¡æ¯å¹¶ç”ŸæˆåŸºäºå¼•ç”¨çš„å›ç­”ã€‚æˆ‘ä»¬å¼€å‘äº†ScholarQABenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šé¢†åŸŸæ–‡çŒ®æœç´¢åŸºå‡†ï¼ŒåŒ…å«2967ä¸ªä¸“å®¶ç¼–å†™çš„æŸ¥è¯¢å’Œ208ä¸ªé•¿ç­”æ¡ˆï¼Œæ¶µç›–è®¡ç®—æœºç§‘å­¦ã€ç‰©ç†å­¦ã€ç¥ç»ç§‘å­¦å’Œç”Ÿç‰©åŒ»å­¦ç­‰é¢†åŸŸã€‚OpenScholaråœ¨å‡†ç¡®æ€§ä¸Šè¶…è¶Šäº†GPT-4oå’ŒPaperQA2ï¼Œå°½ç®¡å…¶æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä¸”åœ¨å¼•ç”¨å‡†ç¡®æ€§æ–¹é¢ä¸äººç±»ä¸“å®¶ç›¸å½“ã€‚æˆ‘ä»¬è¿˜å¼€æºäº†æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®ï¼Œæä¾›äº†å…¬å…±æ¼”ç¤ºï¼Œä¿ƒè¿›äº†ç§‘å­¦æ–‡çŒ®çš„æ£€ç´¢å’Œç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14251', 'title': 'Natural Language Reinforcement Learning', 'url': 'https://huggingface.co/papers/2411.14251', 'abstract': 'Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases. Our code will be released at https://github.com/waterhorse1/Natural-language-RL.', 'score': 21, 'issue_id': 719, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '351fc2a705b34aff', 'authors': ['Xidong Feng', 'Ziyu Wan', 'Haotian Fu', 'Bo Liu', 'Mengyue Yang', 'Girish A. Koushik', 'Zhiyuan Hu', 'Ying Wen', 'Jun Wang'], 'affiliations': ['Brown University', 'National University of Singapore', 'Shanghai Jiao Tong University', 'University College London', 'University of Bristol', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2411.14251.jpg', 'data': {'categories': ['#interpretability', '#rl', '#rlhf', '#open_source', '#games'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ğ»Ğ¾ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ - Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ (NLRL). NLRL Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ¿ĞµÑ€ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ RL Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, NLRL Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¸Ğ³Ñ€Ğ°Ñ… Maze, Breakthrough Ğ¸ ĞºÑ€ĞµÑÑ‚Ğ¸ĞºĞ¸-Ğ½Ğ¾Ğ»Ğ¸ĞºĞ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Revolutionizing Decision-Making with Language: Natural Language Reinforcement Learning', 'desc': "This paper introduces Natural Language Reinforcement Learning (NLRL), which adapts traditional Reinforcement Learning (RL) methods to work with natural language representations. By extending the Markov Decision Process (MDP) framework, NLRL redefines key RL concepts such as task objectives, policies, and value functions in the context of language. The approach leverages advancements in large language models (LLMs) to enhance policy and value updates through prompting or gradient-based training. Experimental results on games like Maze, Breakthrough, and Tic-Tac-Toe showcase NLRL's effectiveness and interpretability across various applications."}, 'zh': {'title': 'è‡ªç„¶è¯­è¨€å¼ºåŒ–å­¦ä¹ ï¼šå†³ç­–çš„æ–°è§†è§’', 'desc': 'å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šè¿‡é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ¥æ•°å­¦åŒ–å†³ç­–åˆ¶å®šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯èƒ½æ€§ï¼Œå³è‡ªç„¶è¯­è¨€å¼ºåŒ–å­¦ä¹ ï¼ˆNLRLï¼‰ï¼Œé€šè¿‡å°†ä¼ ç»Ÿçš„MDPæ‰©å±•åˆ°åŸºäºè‡ªç„¶è¯­è¨€çš„è¡¨ç¤ºç©ºé—´ã€‚NLRLåˆ›æ–°æ€§åœ°é‡æ–°å®šä¹‰äº†å¼ºåŒ–å­¦ä¹ çš„åŸåˆ™ï¼ŒåŒ…æ‹¬ä»»åŠ¡ç›®æ ‡ã€ç­–ç•¥ã€ä»·å€¼å‡½æ•°ã€è´å°”æ›¼æ–¹ç¨‹å’Œç­–ç•¥è¿­ä»£ï¼Œä½¿å…¶é€‚åº”è¯­è¨€çš„å¯¹åº”å…³ç³»ã€‚é€šè¿‡åœ¨è¿·å®«ã€çªç ´å’Œäº•å­—æ£‹ç­‰æ¸¸æˆä¸­çš„å®éªŒï¼ŒéªŒè¯äº†NLRLæ¡†æ¶åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œå¯è§£é‡Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.12364', 'title': 'Ultra-Sparse Memory Network', 'url': 'https://huggingface.co/papers/2411.12364', 'abstract': 'It is widely acknowledged that the performance of Transformer models is exponentially related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms traditional models. In our experiments, we train networks with up to 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget.', 'score': 16, 'issue_id': 721, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '090bf8a39ee13838', 'authors': ['Zihao Huang', 'Qiyang Min', 'Hongzhi Huang', 'Defa Zhu', 'Yutao Zeng', 'Ran Guo', 'Xun Zhou'], 'affiliations': ['Seed-Foundation-Model Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2411.12364.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'UltraMem: ÑĞ²ĞµÑ€Ñ…Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ UltraMem, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ĞµÑ‘ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞµÑ‚ÑĞ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¼Ğ¸ Ğ´Ğ¾ 20 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² ÑÑ‡ĞµĞµĞº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ state-of-the-art ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğµ.'}, 'en': {'title': 'UltraMem: Speeding Up Transformers with Sparse Memory!', 'desc': 'This paper presents UltraMem, a novel architecture designed to enhance the efficiency of Transformer models by integrating a large-scale, ultra-sparse memory layer. By decoupling the number of parameters from computational complexity, UltraMem addresses the high memory access costs that hinder inference speed in existing models. The authors demonstrate that their approach not only reduces inference latency but also maintains competitive model performance, even with networks containing up to 20 million memory slots. Additionally, the study explores the scaling laws of UltraMem, showing that it outperforms traditional models while adhering to computational constraints.'}, 'zh': {'title': 'UltraMemï¼šæå‡æ¨ç†é€Ÿåº¦çš„æ–°æ¶æ„', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºUltraMemçš„æ–°æ¶æ„ï¼Œæ—¨åœ¨è§£å†³Transformeræ¨¡å‹åœ¨æ¨ç†æ—¶çš„é«˜å†…å­˜è®¿é—®æˆæœ¬é—®é¢˜ã€‚é€šè¿‡å¼•å…¥å¤§è§„æ¨¡çš„è¶…ç¨€ç–å†…å­˜å±‚ï¼ŒUltraMemèƒ½å¤Ÿåœ¨ä¿æŒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†è¿™ç§æ–°æ¶æ„çš„æ‰©å±•è§„å¾‹ï¼Œç»“æœè¡¨æ˜å…¶å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ï¼Œå¹¶ä¸”åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»™å®šçš„è®¡ç®—é¢„ç®—å†…å®ç°äº†æœ€å…ˆè¿›çš„æ¨ç†é€Ÿåº¦å’Œæ¨¡å‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14432', 'title': 'Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2411.14432', 'abstract': "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.", 'score': 15, 'issue_id': 720, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '0af1bb82d8021d3b', 'authors': ['Yuhao Dong', 'Zuyan Liu', 'Hai-Long Sun', 'Jingkang Yang', 'Winston Hu', 'Yongming Rao', 'Ziwei Liu'], 'affiliations': ['Nanjing University', 'S-Lab, NTU', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14432.jpg', 'data': {'categories': ['#reasoning', '#long_context', '#multimodal', '#data', '#dataset', '#benchmark', '#agents', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Insight-V - Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰ÑƒÑ Ğ¸Ğ· Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°-Ñ€ĞµĞ·ÑĞ¼Ğµ, Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ MLLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Multi-Modal Reasoning with Insight-V', 'desc': 'This paper introduces Insight-V, a novel approach to enhance the reasoning capabilities of multi-modal large language models (MLLMs) by generating high-quality long-chain reasoning data. The authors propose a two-step pipeline that creates structured reasoning paths without human intervention and employs a multi-granularity assessment method to ensure the quality of the generated data. They also develop a multi-agent system that includes a reasoning agent for long-chain reasoning and a summary agent to evaluate and condense the reasoning outputs. The results show that Insight-V significantly improves performance on complex multi-modal tasks, particularly those requiring visual reasoning, while maintaining effectiveness on perception-focused tasks.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºInsight-Vçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤æ­¥ç”Ÿæˆç®¡é“ï¼Œä»¥æ— äººå·¥å¹²é¢„çš„æ–¹å¼åˆ›å»ºé•¿ä¸”ç»“æ„åŒ–çš„æ¨ç†æ•°æ®ï¼Œå¹¶é‡‡ç”¨å¤šç²’åº¦è¯„ä¼°æ–¹æ³•ç¡®ä¿æ•°æ®è´¨é‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç›´æ¥ç”¨å¤æ‚æ¨ç†æ•°æ®ç›‘ç£MLLMså¹¶ä¸èƒ½è¾¾åˆ°ç†æƒ³æ•ˆæœï¼Œå› æ­¤æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤šä»£ç†ç³»ç»Ÿï¼ŒåŒ…æ‹¬ä¸“æ³¨äºé•¿é“¾æ¨ç†çš„æ¨ç†ä»£ç†å’Œè´Ÿè´£è¯„ä¼°å’Œæ€»ç»“æ¨ç†ç»“æœçš„æ€»ç»“ä»£ç†ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒInsight-Våœ¨è§†è§‰æ¨ç†ç­‰å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14430', 'title': 'Stable Flow: Vital Layers for Training-Free Image Editing', 'url': 'https://huggingface.co/papers/2411.14430', 'abstract': 'Diffusion models have revolutionized the field of content synthesis and editing. Recent models have replaced the traditional UNet architecture with the Diffusion Transformer (DiT), and employed flow-matching for improved training and sampling. However, they exhibit limited generation diversity. In this work, we leverage this limitation to perform consistent image edits via selective injection of attention features. The main challenge is that, unlike the UNet-based models, DiT lacks a coarse-to-fine synthesis structure, making it unclear in which layers to perform the injection. Therefore, we propose an automatic method to identify "vital layers" within DiT, crucial for image formation, and demonstrate how these layers facilitate a range of controlled stable edits, from non-rigid modifications to object addition, using the same mechanism. Next, to enable real-image editing, we introduce an improved image inversion method for flow models. Finally, we evaluate our approach through qualitative and quantitative comparisons, along with a user study, and demonstrate its effectiveness across multiple applications. The project page is available at https://omriavrahami.com/stable-flow', 'score': 9, 'issue_id': 723, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '4d5707c1fdd2e4f9', 'authors': ['Omri Avrahami', 'Or Patashnik', 'Ohad Fried', 'Egor Nemchinov', 'Kfir Aberman', 'Dani Lischinski', 'Daniel Cohen-Or'], 'affiliations': ['Reichman University', 'Snap Research', 'Tel Aviv University', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2411.14430.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² DiT', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° (DiT) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Enhancing Image Editing with Vital Layer Injection in Diffusion Transformers', 'desc': "This paper discusses advancements in diffusion models for content synthesis and editing, specifically focusing on the Diffusion Transformer (DiT) architecture. The authors address the challenge of limited generation diversity in DiT by proposing a method to selectively inject attention features into vital layers of the model. This approach allows for consistent and controlled image edits, such as non-rigid modifications and object additions, despite DiT's lack of a traditional coarse-to-fine synthesis structure. Additionally, the paper introduces an improved image inversion method for flow models and validates the effectiveness of their technique through various evaluations and user studies."}, 'zh': {'title': 'åˆ©ç”¨æ‰©æ•£å˜æ¢å™¨å®ç°ç¨³å®šçš„å›¾åƒç¼–è¾‘', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å†…å®¹åˆæˆå’Œç¼–è¾‘é¢†åŸŸå–å¾—äº†é©å‘½æ€§è¿›å±•ã€‚æœ€è¿‘çš„æ¨¡å‹ç”¨æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ›¿ä»£äº†ä¼ ç»Ÿçš„UNetæ¶æ„ï¼Œå¹¶é‡‡ç”¨æµåŒ¹é…æŠ€æœ¯ä»¥æ”¹å–„è®­ç»ƒå’Œé‡‡æ ·ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„ç”Ÿæˆå¤šæ ·æ€§æœ‰é™ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸€é™åˆ¶ï¼Œé€šè¿‡é€‰æ‹©æ€§æ³¨å…¥æ³¨æ„åŠ›ç‰¹å¾æ¥å®ç°ä¸€è‡´çš„å›¾åƒç¼–è¾‘ï¼Œå¹¶æå‡ºäº†ä¸€ç§è‡ªåŠ¨è¯†åˆ«DiTä¸­å…³é”®å±‚çš„æ–¹æ³•ï¼Œä»¥ä¾¿è¿›è¡Œç¨³å®šçš„å›¾åƒä¿®æ”¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14257', 'title': 'Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models', 'url': 'https://huggingface.co/papers/2411.14257', 'abstract': "Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This suggests that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token.", 'score': 8, 'issue_id': 732, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '765c1d51aaa40d67', 'authors': ['Javier Ferrando', 'Oscar Obeso', 'Senthooran Rajamanoharan', 'Neel Nanda'], 'affiliations': ['ETH ZÃ¼rich', 'UPC'], 'pdf_title_img': 'assets/pdf/title_img/2411.14257.jpg', 'data': {'categories': ['#rlhf', '#architecture', '#interpretability', '#training', '#hallucinations'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜: ĞºĞ»ÑÑ‡ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹, Ğ¾Ğ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚, Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ĞµÑ‚ Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­Ñ‚Ğ¸ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞµĞµ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¸Ğ»Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ¿Ğ¾ÑĞ»Ğµ Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ñ‡Ğ°Ñ‚-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Understanding Hallucinations: Entity Recognition in Language Models', 'desc': "This paper investigates the issue of hallucinations in large language models, which occur when these models generate incorrect or fabricated information. The authors utilize sparse autoencoders to analyze how these models recognize entities and determine their knowledge about them. They find that the model's internal representations can indicate whether it knows about an entity, influencing its responses to questions. The study reveals that these representations can affect the model's behavior, such as leading it to refuse to answer questions about known entities or to invent details about unknown ones."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘çŸ¥è¯†ä¸å¹»è§‰æœºåˆ¶', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¹»è§‰ç°è±¡æ™®éå­˜åœ¨ï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶å°šä¸æ¸…æ¥šï¼Œè¿™é™åˆ¶äº†æˆ‘ä»¬è§£å†³è¿™ä¸€é—®é¢˜çš„èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ä½œä¸ºå¯è§£é‡Šæ€§å·¥å…·ï¼Œæˆ‘ä»¬å‘ç°å®ä½“è¯†åˆ«æ˜¯è¿™äº›æœºåˆ¶çš„å…³é”®éƒ¨åˆ†ï¼Œæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å‡ºè‡ªå·±èƒ½å¦å›å¿†èµ·æŸä¸ªå®ä½“çš„äº‹å®ã€‚ç¨€ç–è‡ªç¼–ç å™¨æ­ç¤ºäº†è¡¨ç¤ºç©ºé—´ä¸­çš„æœ‰æ„ä¹‰æ–¹å‘ï¼Œè¿™äº›æ–¹å‘å¯ä»¥æ£€æµ‹æ¨¡å‹æ˜¯å¦è®¤è¯†æŸä¸ªå®ä½“ï¼Œä¾‹å¦‚è¯†åˆ«å‡ºå®ƒå¯¹æŸä¸ªè¿åŠ¨å‘˜æˆ–ç”µå½±å¹¶ä¸äº†è§£ã€‚è¿™è¡¨æ˜æ¨¡å‹å…·æœ‰è‡ªæˆ‘çŸ¥è¯†ï¼šå…³äºè‡ªèº«èƒ½åŠ›çš„å†…éƒ¨è¡¨ç¤ºï¼Œè¿™äº›æ–¹å‘åœ¨æ¨¡å‹çš„æ‹’ç»å›ç­”å·²çŸ¥å®ä½“é—®é¢˜æˆ–å¯¹æœªçŸ¥å®ä½“è¿›è¡Œå¹»è§‰æ—¶å…·æœ‰å› æœç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.13807', 'title': 'MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control', 'url': 'https://huggingface.co/papers/2411.13807', 'abstract': 'The rapid advancement of diffusion models has greatly improved video synthesis, especially in controllable video generation, which is essential for applications like autonomous driving. However, existing methods are limited by scalability and how control conditions are integrated, failing to meet the needs for high-resolution and long videos for autonomous driving applications. In this paper, we introduce MagicDriveDiT, a novel approach based on the DiT architecture, and tackle these challenges. Our method enhances scalability through flow matching and employs a progressive training strategy to manage complex scenarios. By incorporating spatial-temporal conditional encoding, MagicDriveDiT achieves precise control over spatial-temporal latents. Comprehensive experiments show its superior performance in generating realistic street scene videos with higher resolution and more frames. MagicDriveDiT significantly improves video generation quality and spatial-temporal controls, expanding its potential applications across various tasks in autonomous driving.', 'score': 6, 'issue_id': 728, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '95ef9fbe239921f8', 'authors': ['Ruiyuan Gao', 'Kai Chen', 'Bo Xiao', 'Lanqing Hong', 'Zhenguo Li', 'Qiang Xu'], 'affiliations': ['CUHK', 'HKUST', 'Huawei Cloud', 'Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.13807.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#training', '#video'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MagicDriveDiT - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ DiT. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ flow matching Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ². MagicDriveDiT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ĞºĞ°Ğ´Ñ€Ğ¾Ğ².'}, 'en': {'title': 'MagicDriveDiT: Revolutionizing Video Generation for Autonomous Driving', 'desc': 'This paper presents MagicDriveDiT, a new method for generating high-quality videos using diffusion models, specifically designed for applications in autonomous driving. It addresses the limitations of existing techniques by enhancing scalability and integrating control conditions more effectively. The approach utilizes flow matching and a progressive training strategy to handle complex video scenarios, ensuring better performance in generating long, high-resolution videos. By incorporating spatial-temporal conditional encoding, MagicDriveDiT allows for precise control over the generated video content, making it suitable for various autonomous driving tasks.'}, 'zh': {'title': 'MagicDriveDiTï¼šæå‡è§†é¢‘ç”Ÿæˆè´¨é‡ä¸æ§åˆ¶èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMagicDriveDiTçš„æ–°æ–¹æ³•ï¼ŒåŸºäºDiTæ¶æ„ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘åˆæˆä¸­çš„å¯æ‰©å±•æ€§å’Œæ§åˆ¶æ¡ä»¶é›†æˆé—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æµåŒ¹é…æŠ€æœ¯å¢å¼ºäº†å¯æ‰©å±•æ€§ï¼Œå¹¶é‡‡ç”¨æ¸è¿›å¼è®­ç»ƒç­–ç•¥æ¥å¤„ç†å¤æ‚åœºæ™¯ã€‚é€šè¿‡å¼•å…¥æ—¶ç©ºæ¡ä»¶ç¼–ç ï¼ŒMagicDriveDiTèƒ½å¤Ÿç²¾ç¡®æ§åˆ¶æ—¶ç©ºæ½œå˜é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆé«˜åˆ†è¾¨ç‡å’Œæ›´å¤šå¸§çš„çœŸå®è¡—æ™¯è§†é¢‘æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆè´¨é‡å’Œæ—¶ç©ºæ§åˆ¶èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14343', 'title': 'UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages', 'url': 'https://huggingface.co/papers/2411.14343', 'abstract': 'Large language models (LLMs) under-perform on low-resource languages due to limited training data. We present a method to efficiently collect text data for low-resource languages from the entire Common Crawl corpus. Our approach, UnifiedCrawl, filters and extracts common crawl using minimal compute resources, yielding mono-lingual datasets much larger than previously available sources. We demonstrate that leveraging this data to fine-tuning multilingual LLMs via efficient adapter methods (QLoRA) significantly boosts performance on the low-resource language, while minimizing VRAM usage. Our experiments show large improvements in language modeling perplexity and an increase in few-shot prompting scores. Our work and released source code provide an affordable approach to improve LLMs for low-resource languages using consumer hardware. Our source code is available here at https://github.com/bethelmelesse/unifiedcrawl.', 'score': 4, 'issue_id': 728, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'e3be7df0af13931a', 'authors': ['Bethel Melesse Tessema', 'Akhil Kedia', 'Tae-Sun Chung'], 'affiliations': ['Ajou University Suwon, South Korea', 'Independent Researcher Seoul, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2411.14343.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#open_source', '#training', '#multilingual', '#data'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ UnifiedCrawl', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ UnifiedCrawl Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ±Ğ¾Ñ€Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ¸Ğ· ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Common Crawl. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ (QLoRA) Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ¿Ğ»ĞµĞºÑĞ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¿Ñ€Ğ¸ few-shot Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Boosting Low-Resource Languages with UnifiedCrawl', 'desc': 'This paper addresses the challenge of large language models (LLMs) performing poorly on low-resource languages due to a lack of training data. The authors introduce a method called UnifiedCrawl, which efficiently collects and filters text data from the Common Crawl corpus, creating larger mono-lingual datasets. They demonstrate that fine-tuning multilingual LLMs with this data using efficient adapter methods like QLoRA leads to significant improvements in language modeling and few-shot prompting scores. The approach is designed to be accessible, allowing enhancements to LLMs for low-resource languages using standard consumer hardware.'}, 'zh': {'title': 'æå‡ä½èµ„æºè¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºUnifiedCrawlçš„æ–¹æ³•ï¼Œç”¨äºä»Common Crawlæ•°æ®é›†ä¸­é«˜æ•ˆæ”¶é›†ä½èµ„æºè¯­è¨€çš„æ–‡æœ¬æ•°æ®ã€‚è¯¥æ–¹æ³•é€šè¿‡æœ€å°åŒ–è®¡ç®—èµ„æºçš„ä½¿ç”¨ï¼Œè¿‡æ»¤å’Œæå–æ•°æ®ï¼Œä»è€Œç”Ÿæˆæ¯”ä»¥å¾€æ›´å¤§çš„å•è¯­æ•°æ®é›†ã€‚æˆ‘ä»¬å±•ç¤ºäº†åˆ©ç”¨è¿™äº›æ•°æ®é€šè¿‡é«˜æ•ˆçš„é€‚é…å™¨æ–¹æ³•ï¼ˆQLoRAï¼‰å¯¹å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æ˜¾è‘—æé«˜ä½èµ„æºè¯­è¨€çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘æ˜¾å­˜ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­è¨€å»ºæ¨¡çš„å›°æƒ‘åº¦æœ‰äº†æ˜¾è‘—æ”¹å–„ï¼Œå°‘é‡ç¤ºä¾‹æç¤ºçš„å¾—åˆ†ä¹Ÿæœ‰æ‰€æé«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.13082', 'title': 'Patience Is The Key to Large Language Model Reasoning', 'url': 'https://huggingface.co/papers/2411.13082', 'abstract': 'Recent advancements in the field of large language models, particularly through the Chain of Thought (CoT) approach, have demonstrated significant improvements in solving complex problems. However, existing models either tend to sacrifice detailed reasoning for brevity due to user preferences, or require extensive and expensive training data to learn complicated reasoning ability, limiting their potential in solving complex tasks. To bridge this gap, following the concept of scaling test-time, we propose a simple method by encouraging models to adopt a more patient reasoning style without the need of introducing new knowledge or skills. To employ a preference optimization approach, we generate detailed reasoning processes as positive examples and simple answers as negative examples, thereby training the model to favor thoroughness in its responses. Our results demonstrate a performance increase of up to 6.7% on GSM8k with training just on a lightweight dataset.', 'score': 4, 'issue_id': 726, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'b18aa77451c249b7', 'authors': ['Yijiong Yu'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.13082.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢ĞµÑ€Ğ¿ĞµĞ»Ğ¸Ğ²Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ²Ñ‹Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ ĞºĞ°Ğº Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 6.7% Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GSM8k Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ.'}, 'en': {'title': 'Enhancing Reasoning in Language Models with Simple Training Techniques', 'desc': 'This paper discusses improvements in large language models using the Chain of Thought (CoT) method, which enhances their ability to solve complex problems. It identifies a challenge where models often prioritize brevity over detailed reasoning, or require large amounts of training data to develop reasoning skills. The authors propose a new method that encourages models to adopt a more thorough reasoning style without needing additional knowledge. By using preference optimization, they train models with detailed reasoning as positive examples and simple answers as negative examples, achieving a notable performance increase on the GSM8k dataset with minimal training data.'}, 'zh': {'title': 'æå‡æ¨ç†èƒ½åŠ›ï¼Œç®€åŒ–è®­ç»ƒè¿‡ç¨‹', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•çš„åº”ç”¨ã€‚ç°æœ‰æ¨¡å‹å¾€å¾€ä¸ºäº†ç®€æ´è€Œç‰ºç‰²è¯¦ç»†æ¨ç†ï¼Œæˆ–è€…éœ€è¦å¤§é‡æ˜‚è´µçš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ å¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œé¼“åŠ±æ¨¡å‹é‡‡ç”¨æ›´è€å¿ƒçš„æ¨ç†é£æ ¼ï¼Œè€Œæ— éœ€å¼•å…¥æ–°çŸ¥è¯†æˆ–æŠ€èƒ½ã€‚é€šè¿‡ç”Ÿæˆè¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ä½œä¸ºæ­£ä¾‹å’Œç®€å•ç­”æ¡ˆä½œä¸ºè´Ÿä¾‹ï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹æ›´å€¾å‘äºå…¨é¢çš„å›ç­”ï¼Œç»“æœæ˜¾ç¤ºåœ¨GSM8kä¸Šæ€§èƒ½æé«˜äº†6.7%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14347', 'title': 'DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding', 'url': 'https://huggingface.co/papers/2411.14347', 'abstract': "In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model's core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model's open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, both improving the previous SOTA performance by 5.8 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects.", 'score': 3, 'issue_id': 739, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '5805ca9f7ff81a78', 'authors': ['Tianhe Ren', 'Yihao Chen', 'Qing Jiang', 'Zhaoyang Zeng', 'Yuda Xiong', 'Wenlong Liu', 'Zhengyu Ma', 'Junyi Shen', 'Yuan Gao', 'Xiaoke Jiang', 'Xingyu Chen', 'Zhuheng Song', 'Yuhong Zhang', 'Hongjie Huang', 'Han Gao', 'Shilong Liu', 'Hao Zhang', 'Feng Li', 'Kent Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)'], 'pdf_title_img': 'assets/pdf/title_img/2411.14347.jpg', 'data': {'categories': ['#optimization', '#long_context', '#cv', '#benchmark', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'DINO-X: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº', 'desc': 'DINO-X - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ IDEA Research, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ½Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½ÑÑˆĞ½Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ñ‚Ğ¸Ğ¿Ğ° ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº-Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ. DINO-X Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Grounding-100M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ĞµĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ 100 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ·ĞµĞ¼Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ DINO-X, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ…Ğ²Ğ¾ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'DINO-X: Revolutionizing Open-World Object Detection', 'desc': 'DINO-X is a cutting-edge object-centric vision model that excels in open-world object detection. It utilizes a Transformer-based encoder-decoder architecture to create detailed object representations, allowing for flexible input through text, visual, or customized prompts. The model is trained on a massive dataset of over 100 million samples, enhancing its ability to detect a wide range of objects without needing specific prompts. Experimental results show that DINO-X outperforms previous models, especially in recognizing rare objects, achieving significant improvements in detection accuracy across various benchmarks.'}, 'zh': {'title': 'DINO-Xï¼šå¼€æ”¾ä¸–ç•Œç‰©ä½“æ£€æµ‹çš„æ–°æ ‡æ†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†DINO-Xï¼Œè¿™æ˜¯ç”±IDEAç ”ç©¶å›¢é˜Ÿå¼€å‘çš„ç»Ÿä¸€å¯¹è±¡ä¸­å¿ƒè§†è§‰æ¨¡å‹ï¼Œå…·æœ‰å½“å‰æœ€ä½³çš„å¼€æ”¾ä¸–ç•Œç‰©ä½“æ£€æµ‹æ€§èƒ½ã€‚DINO-Xé‡‡ç”¨ä¸Grounding DINO 1.5ç›¸åŒçš„åŸºäºTransformerçš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œæ—¨åœ¨å®ç°å¼€æ”¾ä¸–ç•Œç‰©ä½“ç†è§£çš„å¯¹è±¡çº§è¡¨ç¤ºã€‚ä¸ºäº†ç®€åŒ–é•¿å°¾ç‰©ä½“æ£€æµ‹ï¼ŒDINO-Xæ‰©å±•äº†è¾“å…¥é€‰é¡¹ï¼Œæ”¯æŒæ–‡æœ¬æç¤ºã€è§†è§‰æç¤ºå’Œè‡ªå®šä¹‰æç¤ºï¼Œç”šè‡³å¯ä»¥å®ç°æ— æç¤ºçš„å¼€æ”¾ä¸–ç•Œæ£€æµ‹ã€‚é€šè¿‡åœ¨è¶…è¿‡1äº¿ä¸ªé«˜è´¨é‡æ ·æœ¬çš„Grounding-100Mæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒDINO-Xèƒ½å¤ŸåŒæ—¶æ”¯æŒå¤šç§ç‰©ä½“æ„ŸçŸ¥å’Œç†è§£ä»»åŠ¡ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14384', 'title': 'Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation', 'url': 'https://huggingface.co/papers/2411.14384', 'abstract': 'Existing feed-forward image-to-3D methods mainly rely on 2D multi-view diffusion models that cannot guarantee 3D consistency. These methods easily collapse when changing the prompt view direction and mainly handle object-centric prompt images. In this paper, we propose a novel single-stage 3D diffusion model, DiffusionGS, for object and scene generation from a single view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to enforce view consistency and allow the model to generate robustly given prompt views of any directions, beyond object-centric inputs. Plus, to improve the capability and generalization ability of DiffusionGS, we scale up 3D training data by developing a scene-object mixed training strategy. Experiments show that our method enjoys better generation quality (2.20 dB higher in PSNR and 23.25 lower in FID) and over 5x faster speed (~6s on an A100 GPU) than SOTA methods. The user study and text-to-3D applications also reveals the practical values of our method. Our Project page at https://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and interactive generation results.', 'score': 2, 'issue_id': 733, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': '1f0661cca4948898', 'authors': ['Yuanhao Cai', 'He Zhang', 'Kai Zhang', 'Yixun Liang', 'Mengwei Ren', 'Fujun Luan', 'Qing Liu', 'Soo Ye Kim', 'Jianming Zhang', 'Zhifei Zhang', 'Yuqian Zhou', 'Zhe Lin', 'Alan Yuille'], 'affiliations': ['Adobe Research', 'Hong Kong University of Science and Technology', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14384.jpg', 'data': {'categories': ['#3d', '#diffusion'], 'emoji': 'ğŸŒŸ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ 2D Ğº Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ğ¼', 'desc': 'DiffusionGS - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑÑ†ĞµĞ½ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ½Ğ° Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ 3D Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑÑ†ĞµĞ½, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞµĞµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DiffusionGS Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D Generation with DiffusionGS', 'desc': 'This paper introduces DiffusionGS, a new single-stage 3D diffusion model designed to generate 3D representations from a single 2D view. Unlike existing methods that struggle with 3D consistency and are limited to object-centric images, DiffusionGS produces 3D Gaussian point clouds that maintain view consistency across various prompt directions. The model enhances its performance by utilizing a scene-object mixed training strategy, which increases the diversity of the training data. Experimental results demonstrate that DiffusionGS achieves superior generation quality and speed compared to state-of-the-art methods, making it a valuable tool for text-to-3D applications.'}, 'zh': {'title': 'DiffusionGSï¼šä»å•è§†è§’ç”Ÿæˆä¸€è‡´çš„3Dåœºæ™¯', 'desc': 'ç°æœ‰çš„å‰é¦ˆå›¾åƒåˆ°3Dæ–¹æ³•ä¸»è¦ä¾èµ–äº2Då¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ— æ³•ä¿è¯3Dä¸€è‡´æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å•é˜¶æ®µ3Dæ‰©æ•£æ¨¡å‹DiffusionGSï¼Œå¯ä»¥ä»å•ä¸€è§†è§’ç”Ÿæˆç‰©ä½“å’Œåœºæ™¯ã€‚DiffusionGSåœ¨æ¯ä¸ªæ—¶é—´æ­¥ç›´æ¥è¾“å‡º3Dé«˜æ–¯ç‚¹äº‘ï¼Œä»¥ç¡®ä¿è§†è§’ä¸€è‡´æ€§ï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®ä»»ä½•æ–¹å‘çš„æç¤ºè§†å›¾è¿›è¡Œç¨³å¥ç”Ÿæˆã€‚é€šè¿‡å¼€å‘åœºæ™¯-ç‰©ä½“æ··åˆè®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬æ‰©å¤§äº†3Dè®­ç»ƒæ•°æ®ï¼Œæé«˜äº†DiffusionGSçš„èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.15124', 'title': 'TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training', 'url': 'https://huggingface.co/papers/2411.15124', 'abstract': 'Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\\"ULU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\\"ULU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\\"ULU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.   In addition to the T\\"ULU 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the T\\"ULU 3 approach to more domains.', 'score': 35, 'issue_id': 755, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '44809ea81d71ef97', 'authors': ['Nathan Lambert', 'Jacob Morrison', 'Valentina Pyatkin', 'Shengyi Huang', 'Hamish Ivison', 'Faeze Brahman', 'Lester James V. Miranda', 'Alisa Liu', 'Nouha Dziri', 'Shane Lyu', 'Yuling Gu', 'Saumya Malik', 'Victoria Graf', 'Jena D. Hwang', 'Jiangjiang Yang', 'Ronan Le Bras', 'Oyvind Tafjord', 'Chris Wilhelm', 'Luca Soldaini', 'Noah A. Smith', 'Yizhong Wang', 'Pradeep Dasigi', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.15124.jpg', 'data': {'categories': ['#dataset', '#training', '#data', '#open_source', '#optimization', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ T"ULU 3 - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SFT, DPO Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ RLVR. T"ULU 3 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Llama 3.1 Ğ¸ GPT-4o-mini. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Language Model Potential with T"ULU 3', 'desc': 'This paper presents T"ULU 3, a set of fully-open post-trained language models that enhance performance and capabilities beyond existing models. It emphasizes the importance of transparency in training data and methodologies, providing a comprehensive guide for implementing modern post-training techniques. The models utilize advanced training algorithms, including supervised finetuning and a novel reinforcement learning method, achieving superior results compared to both open and closed counterparts. Additionally, the paper offers extensive resources for replication and adaptation, including datasets, training code, and evaluation tools.'}, 'zh': {'title': 'T"ULU 3ï¼šå¼€æ”¾çš„åè®­ç»ƒæ¨¡å‹æ–°çºªå…ƒ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†T"ULU 3ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„æœ€æ–°åè®­ç»ƒæ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºå’ŒæŠ€èƒ½ã€‚æˆ‘ä»¬æä¾›äº†æ¨¡å‹çš„è®­ç»ƒæ•°æ®ã€ä»£ç å’Œè®­ç»ƒé…æ–¹ï¼Œå¡«è¡¥äº†å¼€æ”¾æŠ€æœ¯ä¸ä¸“æœ‰æŠ€æœ¯ä¹‹é—´çš„é€æ˜åº¦å·®è·ã€‚T"ULU 3åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬Llama 3.1å’ŒGPT-4o-miniç­‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å¤šä»»åŠ¡è¯„ä¼°æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„æŠ¥å‘Šï¼Œä»¥ä¾¿äºåœ¨æ›´å¤šé¢†åŸŸä¸­å¤ç°å’Œé€‚åº”T"ULU 3çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14793', 'title': 'Style-Friendly SNR Sampler for Style-Driven Generation', 'url': 'https://huggingface.co/papers/2411.14793', 'abstract': 'Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new "style templates", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation.', 'score': 27, 'issue_id': 752, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '03859b57f29683ab', 'authors': ['Jooyoung Choi', 'Chaehun Shin', 'Yeongtak Oh', 'Heeseung Kim', 'Sungroh Yoon'], 'affiliations': ['AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University', 'Data Science and AI Laboratory, ECE, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14793.jpg', 'data': {'categories': ['#synthetic', '#3d', '#multimodal', '#cv', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¼ ÑÑ‚Ğ¸Ğ»Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Style-friendly SNR sampler, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¼ĞµÑ‰Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»-ÑˆÑƒĞ¼ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑˆÑƒĞ¼Ğ° Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ¸Ğ»Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°ĞºĞ²Ğ°Ñ€ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ¸, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹, 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹ Ğ¸ Ğ¼ĞµĞ¼Ñ‹ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼.'}, 'en': {'title': 'Unlocking Unique Artistic Styles with Style-friendly SNR Sampler', 'desc': 'This paper addresses the challenge of adapting large-scale diffusion models to generate personalized artistic styles. The authors introduce the Style-friendly SNR sampler, which modifies the signal-to-noise ratio (SNR) during fine-tuning to emphasize higher noise levels where stylistic features are more prominent. By doing so, the model improves its ability to capture unique styles, resulting in images that align better with the desired artistic expression. The proposed method expands the creative possibilities for generating diverse styles, including watercolor paintings and cartoons, thus enhancing personalized content creation.'}, 'zh': {'title': 'æå‡ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼ç”Ÿæˆçš„ä¿¡å™ªæ¯”æ–¹æ³•', 'desc': 'æœ€è¿‘çš„å¤§è§„æ¨¡æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼Œä½†åœ¨å­¦ä¹ æ–°çš„ä¸ªæ€§åŒ–è‰ºæœ¯é£æ ¼æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™é™åˆ¶äº†ç‹¬ç‰¹é£æ ¼æ¨¡æ¿çš„åˆ›å»ºã€‚å¾®è°ƒå‚è€ƒå›¾åƒæ˜¯æœ€æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œä½†é€šå¸¸ç›²ç›®ä½¿ç”¨é¢„è®­ç»ƒæ—¶çš„ç›®æ ‡å’Œå™ªå£°æ°´å¹³åˆ†å¸ƒï¼Œå¯¼è‡´é£æ ¼å¯¹é½ä¸ç†æƒ³ã€‚æˆ‘ä»¬æå‡ºäº†é£æ ¼å‹å¥½çš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰é‡‡æ ·å™¨ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç§¯æå°†ä¿¡å™ªæ¯”åˆ†å¸ƒå‘æ›´é«˜çš„å™ªå£°æ°´å¹³è½¬ç§»ï¼Œä»¥ä¸“æ³¨äºé£æ ¼ç‰¹å¾å‡ºç°çš„å™ªå£°æ°´å¹³ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰ç‹¬ç‰¹é£æ ¼ï¼Œå¹¶ç”Ÿæˆå…·æœ‰æ›´é«˜é£æ ¼å¯¹é½çš„å›¾åƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.15098', 'title': 'OminiControl: Minimal and Universal Control for Diffusion Transformer', 'url': 'https://huggingface.co/papers/2411.15098', 'abstract': 'In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only ~0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation.', 'score': 19, 'issue_id': 754, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '9cd668db99ed0902', 'authors': ['Zhenxiong Tan', 'Songhua Liu', 'Xingyi Yang', 'Qiaochu Xue', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.15098.jpg', 'data': {'categories': ['#open_source', '#synthetic', '#data', '#diffusion', '#dataset', '#architecture', '#multimodal', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'OminiControl: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'OminiControl - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Diffusion Transformer (DiT). ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ DiT ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. OminiControl Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 0,1% Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ UNet Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ DiT ĞºĞ°Ğº Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Efficient Image Conditioning with OminiControl', 'desc': 'OminiControl is a new framework that enhances pre-trained Diffusion Transformer (DiT) models by integrating image conditions efficiently. It uses a parameter reuse mechanism, allowing the DiT to process image conditions with minimal additional parameters, specifically around 0.1%. This framework can handle various image conditioning tasks, such as generating images based on specific subjects or aligning them with spatial features like edges and depth. OminiControl has shown superior performance compared to traditional UNet-based models and other DiT adaptations, and it comes with a large dataset, Subjects200K, to support further research.'}, 'zh': {'title': 'OminiControlï¼šé«˜æ•ˆæ•´åˆå›¾åƒæ¡ä»¶çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†OminiControlï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜åº¦çµæ´»ä¸”å‚æ•°é«˜æ•ˆçš„æ¡†æ¶ï¼Œèƒ½å¤Ÿå°†å›¾åƒæ¡ä»¶é›†æˆåˆ°é¢„è®­ç»ƒçš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰æ¨¡å‹ä¸­ã€‚OminiControlåˆ©ç”¨å‚æ•°é‡ç”¨æœºåˆ¶ï¼Œä½¿DiTèƒ½å¤Ÿä½¿ç”¨è‡ªèº«ä½œä¸ºå¼ºå¤§çš„åŸºç¡€ï¼Œç¼–ç å›¾åƒæ¡ä»¶ï¼Œå¹¶é€šè¿‡çµæ´»çš„å¤šæ¨¡æ€æ³¨æ„åŠ›å¤„ç†å™¨è¿›è¡Œå¤„ç†ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒOminiControlä»…éœ€çº¦0.1%çš„é¢å¤–å‚æ•°ï¼Œå°±èƒ½æœ‰æ•ˆåœ°æ•´åˆæ³¨å…¥çš„å›¾åƒæ¡ä»¶ï¼Œå¹¶ä»¥ç»Ÿä¸€çš„æ–¹å¼å¤„ç†å¤šç§å›¾åƒæ¡ä»¶ä»»åŠ¡ã€‚é€šè¿‡åœ¨DiTè‡ªèº«ç”Ÿæˆçš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼ŒOminiControlåœ¨ä¸»é¢˜é©±åŠ¨ç”Ÿæˆå’Œç©ºé—´å¯¹é½æ¡ä»¶ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ä¼˜äºç°æœ‰çš„UNetå’ŒDiTé€‚åº”æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.12946', 'title': 'A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection', 'url': 'https://huggingface.co/papers/2411.12946', 'abstract': 'Large Language Models are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety.', 'score': 15, 'issue_id': 756, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'de5ca9118a5cab35', 'authors': ['Gabriel Chua', 'Shing Yee Chan', 'Shaun Khoo'], 'affiliations': ['Government Technology Agency Singapore', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.12946.jpg', 'data': {'categories': ['#data', '#dataset', '#hallucinations', '#synthetic', '#open_source', '#benchmark'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ñ‚ Ğ½ĞµÑ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ·Ğ»Ğ¾ÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ LLM.'}, 'en': {'title': 'Building Better Guardrails for Large Language Models', 'desc': 'This paper addresses the issue of off-topic misuse in Large Language Models (LLMs) by proposing a new methodology for developing guardrails without relying on real-world data. The authors create a synthetic dataset by using LLMs to generate diverse prompts based on a well-defined problem space, which helps in training more effective off-topic guardrails. Their approach not only reduces false positives but also enhances adaptability to various misuse scenarios, such as harmful prompts and jailbreak attempts. Furthermore, the authors contribute to the community by open-sourcing their synthetic dataset and guardrail models, promoting further research in LLM safety.'}, 'zh': {'title': 'æ„å»ºçµæ´»çš„é˜²æŠ¤æªæ–½ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨æ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨ä¸­å¯èƒ½å‡ºç°çš„åç¦»ä¸»é¢˜çš„è¯¯ç”¨é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„ã€æ— æ•°æ®çš„é˜²æŠ¤æªæ–½å¼€å‘æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•çš„é«˜è¯¯æŠ¥ç‡å’Œé€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ã€‚é€šè¿‡å¯¹é—®é¢˜ç©ºé—´çš„å®šæ€§å®šä¹‰ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„æç¤ºï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œç”¨äºåŸºå‡†æµ‹è¯•å’Œè®­ç»ƒé˜²æŠ¤æªæ–½ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€æºäº†åˆæˆæ•°æ®é›†å’Œé˜²æŠ¤æ¨¡å‹ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ç ”ç©¶å’Œå¼€å‘æä¾›äº†é‡è¦èµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.13543', 'title': 'BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games', 'url': 'https://huggingface.co/papers/2411.13543', 'abstract': 'Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community.', 'score': 11, 'issue_id': 762, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'c4fe6e6278490fc9', 'authors': ['Davide Paglieri', 'BartÅ‚omiej CupiaÅ‚', 'Samuel Coward', 'Ulyana Piterbarg', 'Maciej Wolczyk', 'Akbir Khan', 'Eduardo Pignatelli', 'Åukasz KuciÅ„ski', 'Lerrel Pinto', 'Rob Fergus', 'Jakob Nicolaus Foerster', 'Jack Parker-Holder', 'Tim RocktÃ¤schel'], 'affiliations': ['AI Centre, University College London', 'Anthropic', 'IDEAS NCBR', 'New York University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2411.13543.jpg', 'data': {'categories': ['#agents', '#rl', '#benchmark', '#reasoning', '#open_source', '#games'], 'emoji': 'ğŸ®', 'ru': {'title': 'BALROG: Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ğ³Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'BALROG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ€ÑĞ´ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ´Ğ¾ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº NetHack Learning Environment. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'BALROG: Benchmarking Agentic Capabilities of LLMs and VLMs', 'desc': 'This paper introduces BALROG, a new benchmark aimed at evaluating the agentic capabilities of Large Language Models (LLMs) and Vision Language Models (VLMs) in complex environments. The benchmark includes a variety of challenging games that test advanced skills like spatial reasoning and long-term planning, which are essential for real-world tasks. The authors implement fine-grained metrics to assess model performance across different difficulty levels, revealing that current models excel in simpler tasks but struggle with more complex ones, particularly in vision-based decision-making. BALROG is released as an open resource to support further research in enhancing the capabilities of LLMs and VLMs.'}, 'zh': {'title': 'è¯„ä¼°æ™ºèƒ½ä»£ç†èƒ½åŠ›çš„æ–°åŸºå‡†ï¼šBALROG', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ç°å®ä»»åŠ¡éœ€è¦å¤„ç†å¤æ‚çš„äº¤äº’ã€é«˜çº§ç©ºé—´æ¨ç†ã€é•¿æœŸè§„åˆ’å’ŒæŒç»­æ¢ç´¢æ–°ç­–ç•¥ï¼Œè€Œæˆ‘ä»¬åœ¨è¿™äº›é¢†åŸŸç¼ºä¹æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†BALROGï¼Œä¸€ä¸ªæ–°é¢–çš„åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡å¤šæ ·åŒ–çš„æŒ‘æˆ˜æ€§æ¸¸æˆè¯„ä¼°LLMså’ŒVLMsçš„ä»£ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡å½“å‰æ¨¡å‹åœ¨ç®€å•æ¸¸æˆä¸­å–å¾—äº†ä¸€å®šæˆåŠŸï¼Œä½†åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­è¡¨ç°æ˜¾è‘—ä¸è¶³ï¼Œå°¤å…¶æ˜¯åœ¨åŸºäºè§†è§‰çš„å†³ç­–æ–¹é¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14982', 'title': 'Large Multi-modal Models Can Interpret Features in Large Multi-modal Models', 'url': 'https://huggingface.co/papers/2411.14982', 'abstract': "Recent advances in Large Multimodal Models (LMMs) lead to significant breakthroughs in both academia and industry. One question that arises is how we, as humans, can understand their internal neural representations. This paper takes an initial step towards addressing this question by presenting a versatile framework to identify and interpret the semantics within LMMs. Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the representations into human understandable features. 2) We then present an automatic interpretation framework to interpreted the open-semantic features learned in SAE by the LMMs themselves. We employ this framework to analyze the LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these features can effectively steer the model's behavior. Our results contribute to a deeper understanding of why LMMs excel in specific tasks, including EQ tests, and illuminate the nature of their mistakes along with potential strategies for their rectification. These findings offer new insights into the internal mechanisms of LMMs and suggest parallels with the cognitive processes of the human brain.", 'score': 11, 'issue_id': 761, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '7d4fae86425adb6c', 'authors': ['Kaichen Zhang', 'Yifei Shen', 'Bo Li', 'Ziwei Liu'], 'affiliations': ['LMMs-Lab Team, S-Lab, NTU, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.14982.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LMM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LLaVA-NeXT-8B Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLaVA-OV-72B, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, ĞºĞ°Ğº Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LMM Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸ Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ°.'}, 'en': {'title': 'Unlocking the Secrets of Large Multimodal Models', 'desc': "This paper explores how we can understand the internal workings of Large Multimodal Models (LMMs) by using a Sparse Autoencoder (SAE) to break down their representations into features that humans can comprehend. It introduces a framework for automatically interpreting these features, which are learned by the LMMs themselves. The study specifically analyzes the LLaVA-NeXT-8B model in relation to the LLaVA-OV-72B model, showing that the identified features can influence the model's performance on various tasks. The findings enhance our understanding of LMMs' strengths and weaknesses, drawing parallels to human cognitive processes."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å†…éƒ¨æœºåˆ¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„å†…éƒ¨ç¥ç»è¡¨ç¤ºå¦‚ä½•è¢«ç†è§£ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰å°†è¡¨ç¤ºè§£è€¦ä¸ºäººç±»å¯ç†è§£çš„ç‰¹å¾ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªåŠ¨è§£é‡Šæ¡†æ¶ï¼Œç”¨äºè§£é‡ŠLMMsè‡ªèº«å­¦ä¹ çš„å¼€æ”¾è¯­ä¹‰ç‰¹å¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœåŠ æ·±äº†å¯¹LMMsåœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚åŸå› çš„ç†è§£ï¼Œå¹¶æ­ç¤ºäº†å®ƒä»¬é”™è¯¯çš„æ€§è´¨åŠå¯èƒ½çš„çº æ­£ç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14794', 'title': 'VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection', 'url': 'https://huggingface.co/papers/2411.14794', 'abstract': 'The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities. Our code and dataset will be released at: https://github.com/hshjerry/VideoEspresso', 'score': 8, 'issue_id': 757, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': 'f03437948b77773f', 'authors': ['Songhao Han', 'Wei Huang', 'Hairong Shi', 'Le Zhuo', 'Xiu Su', 'Shifeng Zhang', 'Xu Zhou', 'Xiaojuan Qi', 'Yue Liao', 'Si Liu'], 'affiliations': ['Beihang University', 'CUHK', 'Central South University', 'Sangfor Technologies Inc.', 'Shanghai AI Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2411.14794.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#cv', '#games', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'VideoEspresso: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VideoEspresso Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Hybrid LVLMs Collaboration, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing VideoQA with VideoEspresso: A New Era of Multimodal Reasoning', 'desc': 'This paper presents VideoEspresso, a new dataset designed to enhance video question-answering (VideoQA) by providing high-quality pairs that maintain spatial and temporal coherence. It addresses the limitations of existing datasets, which often rely on expensive manual annotations or ineffective automatic methods. The authors introduce a semantic-aware construction pipeline that reduces redundancy and generates QA pairs using GPT-4o, along with video Chain-of-Thought (CoT) annotations to improve reasoning. Additionally, they propose a Hybrid LVLMs Collaboration framework that optimally selects frames and performs reasoning, achieving superior performance on various tasks compared to existing models.'}, 'zh': {'title': 'VideoEspressoï¼šæå‡è§†é¢‘æ¨ç†çš„æ–°æ•°æ®é›†ä¸æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘é—®ç­”æ•°æ®é›†VideoEspressoï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†åœ¨è§†é¢‘æ¨ç†ä»»åŠ¡ä¸­çš„ä¸è¶³ã€‚è¯¥æ•°æ®é›†ä¿ç•™äº†é‡è¦çš„ç©ºé—´ç»†èŠ‚å’Œæ—¶é—´è¿è´¯æ€§ï¼Œå¹¶æä¾›äº†ä¸­é—´æ¨ç†æ­¥éª¤çš„å¤šæ¨¡æ€æ³¨é‡Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åä½œæ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”é€‰æ‹©æ ¸å¿ƒå¸§å¹¶è¿›è¡Œè¿é”æ€ç»´æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªä»»åŠ¡ä¸Šä¼˜äºç°æœ‰åŸºçº¿ï¼Œå±•ç¤ºäº†æ›´å¼ºçš„è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14762', 'title': 'Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction', 'url': 'https://huggingface.co/papers/2411.14762', 'abstract': 'Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled (x,y,t) coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128times128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.', 'score': 8, 'issue_id': 756, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '9490a40884583735', 'authors': ['Huiwon Jang', 'Sihyun Yu', 'Jinwoo Shin', 'Pieter Abbeel', 'Younggyo Seo'], 'affiliations': ['KAIST', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2411.14762.jpg', 'data': {'categories': ['#long_context', '#3d', '#diffusion', '#video', '#training'], 'emoji': 'ğŸ¥', 'ru': {'title': 'CoordTok: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'CoordTok - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ². ĞĞ½ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ‚Ñ‡Ğ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ğ¼ (x,y,t), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. CoordTok Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Efficient Video Tokenization with CoordTok', 'desc': 'This paper presents CoordTok, a novel video tokenizer designed to efficiently encode long video clips by leveraging coordinate-based representations. Unlike traditional tokenizers that reconstruct all frames simultaneously, CoordTok uses a factorized triplane representation to map (x,y,t) coordinates to video patches, significantly reducing the number of tokens needed. The approach allows for training large models on long videos without incurring high computational costs. Experimental results demonstrate that CoordTok can encode a 128-frame video into just 1280 tokens, outperforming existing methods that require thousands of tokens for similar quality.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘æ ‡è®°åŒ–ï¼Œé™ä½è®­ç»ƒæˆæœ¬ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºCoordTokçš„è§†é¢‘æ ‡è®°å™¨ï¼Œæ—¨åœ¨é«˜æ•ˆå¤„ç†é•¿è§†é¢‘çš„æ ‡è®°åŒ–é—®é¢˜ã€‚CoordToké€šè¿‡å­¦ä¹ åæ ‡è¡¨ç¤ºä¸è¾“å…¥è§†é¢‘è¡¥ä¸ä¹‹é—´çš„æ˜ å°„ï¼Œåˆ©ç”¨äº†è§†é¢‘çš„æ—¶é—´ä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼ŒCoordTokæ˜¾è‘—å‡å°‘äº†ç¼–ç é•¿è§†é¢‘æ‰€éœ€çš„æ ‡è®°æ•°é‡ï¼Œä»è€Œé™ä½äº†è®­ç»ƒæˆæœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒCoordTokèƒ½å¤Ÿåœ¨ä¿æŒé‡å»ºè´¨é‡çš„åŒæ—¶ï¼Œå°†128å¸§è§†é¢‘çš„æ ‡è®°æ•°é‡å‡å°‘åˆ°1280ä¸ªã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14521', 'title': 'MyTimeMachine: Personalized Facial Age Transformation', 'url': 'https://huggingface.co/papers/2411.14521', 'abstract': "Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20sim40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches.", 'score': 7, 'issue_id': 755, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'd104e8a00be886bb', 'authors': ['Luchao Qi', 'Jiaye Wu', 'Bang Gong', 'Annie N. Wang', 'David W. Jacobs', 'Roni Sengupta'], 'affiliations': ['University of Maryland, College Park', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.14521.jpg', 'data': {'categories': ['#architecture', '#training', '#multimodal', '#cv', '#video'], 'emoji': 'ğŸ‘´', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ğµ Ğ»Ğ¸Ñ† Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ñ† Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ MyTimeMachine (MyTM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ†Ğ¸ĞµĞ¹ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ ĞĞ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ğ°Ñ Ğ¡ĞµÑ‚ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ StyleGAN2. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Personalized Aging: Your Face, Your Time', 'desc': "This paper presents MyTimeMachine (MyTM), a novel approach to facial aging that combines global aging knowledge with personalized photo collections. It addresses the challenge of accurately predicting an individual's appearance at a target age by utilizing as few as 50 personal images. The method employs an Adapter Network that integrates personalized and global aging features, generating realistic re-aged images using StyleGAN2. Additionally, the authors introduce three specialized loss functions to enhance the personalization of the aging process, resulting in high-quality, identity-preserving, and temporally consistent aging effects."}, 'zh': {'title': 'ä¸ªæ€§åŒ–è€åŒ–ï¼ŒçœŸå®å†ç°', 'desc': 'é¢éƒ¨è€åŒ–æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œå—åˆ°æ€§åˆ«ã€ç§æ—ã€ç”Ÿæ´»æ–¹å¼ç­‰å¤šç§å› ç´ çš„å½±å“ï¼Œå› æ­¤å¾ˆéš¾å­¦ä¹ åˆ°ä¸€ä¸ªé€šç”¨çš„è€åŒ–æ¨¡å‹æ¥å‡†ç¡®é¢„æµ‹ä¸ªä½“çš„è€åŒ–æƒ…å†µã€‚ç°æœ‰æŠ€æœ¯è™½ç„¶èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è€åŒ–æ•ˆæœï¼Œä½†é‡å¡‘çš„å›¾åƒå¾€å¾€ä¸ç›®æ ‡å¹´é¾„çš„å¤–è²Œä¸ç¬¦ï¼Œå› æ­¤éœ€è¦ä¸ªæ€§åŒ–å¤„ç†ã€‚æˆ‘ä»¬æå‡ºäº†MyTimeMachineï¼ˆMyTMï¼‰ï¼Œå®ƒç»“åˆäº†å…¨çƒè€åŒ–æ¨¡å‹å’Œä¸ªäººç…§ç‰‡é›†ï¼ˆä»…éœ€50å¼ å›¾åƒï¼‰æ¥å­¦ä¹ ä¸ªæ€§åŒ–çš„å¹´é¾„è½¬æ¢ã€‚æˆ‘ä»¬çš„åˆ›æ–°åœ¨äºå¼•å…¥äº†é€‚é…å™¨ç½‘ç»œå’Œä¸‰ç§æŸå¤±å‡½æ•°ï¼Œä½¿å¾—ç”Ÿæˆçš„è€åŒ–å›¾åƒèƒ½å¤Ÿä¿æŒèº«ä»½ä¸€è‡´æ€§ï¼Œå¹¶åœ¨è§†é¢‘ä¸­å®ç°é«˜è´¨é‡çš„è€åŒ–æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14208', 'title': 'Novel View Extrapolation with Video Diffusion Priors', 'url': 'https://huggingface.co/papers/2411.14208', 'abstract': 'The field of novel view synthesis has made significant strides thanks to the development of radiance field methods. However, most radiance field techniques are far better at novel view interpolation than novel view extrapolation where the synthesis novel views are far beyond the observed training views. We design ViewExtrapolator, a novel view synthesis approach that leverages the generative priors of Stable Video Diffusion (SVD) for realistic novel view extrapolation. By redesigning the SVD denoising process, ViewExtrapolator refines the artifact-prone views rendered by radiance fields, greatly enhancing the clarity and realism of the synthesized novel views. ViewExtrapolator is a generic novel view extrapolator that can work with different types of 3D rendering such as views rendered from point clouds when only a single view or monocular video is available. Additionally, ViewExtrapolator requires no fine-tuning of SVD, making it both data-efficient and computation-efficient. Extensive experiments demonstrate the superiority of ViewExtrapolator in novel view extrapolation. Project page: https://kunhao-liu.github.io/ViewExtrapolator/.', 'score': 6, 'issue_id': 755, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'dddb7a1ecc9850f3', 'authors': ['Kunhao Liu', 'Ling Shao', 'Shijian Lu'], 'affiliations': ['Nanyang Technological University', 'UCAS-Terminus AI Lab, UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2411.14208.jpg', 'data': {'categories': ['#diffusion', '#3d', '#video'], 'emoji': 'ğŸ”­', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ViewExtrapolator - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ²Ğ¸Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Stable Video Diffusion (SVD) Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ´Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‡ĞµÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´Ğ¾Ğ². ViewExtrapolator Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ 3D-Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ SVD. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ViewExtrapolator Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Novel View Extrapolation with ViewExtrapolator', 'desc': 'This paper introduces ViewExtrapolator, a new method for synthesizing novel views that go beyond the original training views. It utilizes the generative capabilities of Stable Video Diffusion (SVD) to improve the quality of these extrapolated views. By modifying the SVD denoising process, the method reduces artifacts and enhances the realism of the generated images. ViewExtrapolator is versatile, working with various 3D rendering types and requiring no fine-tuning, making it efficient in both data and computation.'}, 'zh': {'title': 'æå‡æ–°è§†å›¾å¤–æ¨çš„æ¸…æ™°åº¦ä¸çœŸå®æ„Ÿ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†å›¾åˆæˆæ–¹æ³•ï¼Œç§°ä¸ºViewExtrapolatorï¼Œæ—¨åœ¨æ”¹å–„æ–°è§†å›¾å¤–æ¨çš„æ•ˆæœã€‚ä¼ ç»Ÿçš„è¾å°„åœºæŠ€æœ¯åœ¨æ–°è§†å›¾æ’å€¼æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ–°è§†å›¾å¤–æ¨æ—¶æ•ˆæœè¾ƒå·®ã€‚ViewExtrapolatoråˆ©ç”¨ç¨³å®šè§†é¢‘æ‰©æ•£ï¼ˆSVDï¼‰çš„ç”Ÿæˆå…ˆéªŒï¼Œé€šè¿‡é‡æ–°è®¾è®¡å»å™ªè¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†åˆæˆæ–°è§†å›¾çš„æ¸…æ™°åº¦å’ŒçœŸå®æ„Ÿã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä¸åŒç±»å‹çš„3Dæ¸²æŸ“ï¼Œå¹¶ä¸”æ— éœ€å¯¹SVDè¿›è¡Œå¾®è°ƒï¼Œå…·æœ‰æ•°æ®å’Œè®¡ç®—æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.15131', 'title': 'WildLMa: Long Horizon Loco-Manipulation in the Wild', 'url': 'https://huggingface.co/papers/2411.15131', 'abstract': "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world environments, which requires the robot to (1) have skills that generalize across object configurations; (2) be capable of long-horizon task execution in diverse environments; and (3) perform complex manipulation beyond pick-and-place. Quadruped robots with manipulators hold promise for extending the workspace and enabling robust locomotion, but existing results do not investigate such a capability. This paper proposes WildLMa with three components to address these issues: (1) adaptation of learned low-level controller for VR-enabled whole-body teleoperation and traversability; (2) WildLMa-Skill -- a library of generalizable visuomotor skills acquired via imitation learning or heuristics and (3) WildLMa-Planner -- an interface of learned skills that allow LLM planners to coordinate skills for long-horizon tasks. We demonstrate the importance of high-quality training data by achieving higher grasping success rate over existing RL baselines using only tens of demonstrations. WildLMa exploits CLIP for language-conditioned imitation learning that empirically generalizes to objects unseen in training demonstrations. Besides extensive quantitative evaluation, we qualitatively demonstrate practical robot applications, such as cleaning up trash in university hallways or outdoor terrains, operating articulated objects, and rearranging items on a bookshelf.", 'score': 5, 'issue_id': 755, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': 'c38df92e9599db09', 'authors': ['Ri-Zhao Qiu', 'Yuchen Song', 'Xuanbin Peng', 'Sai Aneesh Suryadevara', 'Ge Yang', 'Minghuan Liu', 'Mazeyu Ji', 'Chengzhe Jia', 'Ruihan Yang', 'Xueyan Zou', 'Xiaolong Wang'], 'affiliations': ['MIT', 'NVIDIA', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2411.15131.jpg', 'data': {'categories': ['#rl', '#robotics', '#training', '#agi', '#transfer_learning', '#long_context'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'WildLMa: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚-Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ WildLMa Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€ Ğ´Ğ»Ñ Ñ‚ĞµĞ»ĞµĞ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºÑƒ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ¾Ğ¼Ğ¾Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. WildLMa Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ CLIP Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑĞ²Ğ¾Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº ÑƒĞ±Ğ¾Ñ€ĞºĞ° Ğ¼ÑƒÑĞ¾Ñ€Ğ° Ğ¸ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Empowering Robots for Real-World Manipulation with WildLMa', 'desc': 'This paper presents WildLMa, a framework designed for mobile manipulation robots to operate effectively in varied real-world settings. It includes a low-level controller for teleoperation, a library of generalizable visuomotor skills learned through imitation, and a planner that coordinates these skills for complex tasks. The approach emphasizes the significance of high-quality training data, achieving better performance in grasping tasks compared to existing reinforcement learning methods. Additionally, WildLMa utilizes CLIP for language-conditioned imitation learning, enabling the robot to adapt to new objects not seen during training.'}, 'zh': {'title': 'WildLMaï¼šæå‡æœºå™¨äººåœ¨çœŸå®ç¯å¢ƒä¸­çš„æ“æ§èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºWildLMaçš„ç§»åŠ¨æ“æ§æœºå™¨äººç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³åœ¨å¤šæ ·åŒ–çœŸå®ç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚è¯¥ç³»ç»ŸåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šé€‚åº”æ€§ä½çº§æ§åˆ¶å™¨ã€é€šç”¨è§†è§‰è¿åŠ¨æŠ€èƒ½åº“å’Œé•¿æ—¶é—´ä»»åŠ¡è§„åˆ’æ¥å£ã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ å’Œé«˜è´¨é‡è®­ç»ƒæ•°æ®ï¼ŒWildLMaåœ¨æŠ“å–æˆåŠŸç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„å¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº†æœºå™¨äººåœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œå¦‚æ¸…ç†æ ¡å›­èµ°å»Šåƒåœ¾å’Œæ“ä½œå¤æ‚ç‰©ä½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.13127', 'title': 'Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images', 'url': 'https://huggingface.co/papers/2411.13127', 'abstract': 'Cloud segmentation is a critical challenge in remote sensing image interpretation, as its accuracy directly impacts the effectiveness of subsequent data processing and analysis. Recently, vision foundation models (VFM) have demonstrated powerful generalization capabilities across various visual tasks. In this paper, we present a parameter-efficient adaptive approach, termed Cloud-Adapter, designed to enhance the accuracy and robustness of cloud segmentation. Our method leverages a VFM pretrained on general domain data, which remains frozen, eliminating the need for additional training. Cloud-Adapter incorporates a lightweight spatial perception module that initially utilizes a convolutional neural network (ConvNet) to extract dense spatial representations. These multi-scale features are then aggregated and serve as contextual inputs to an adapting module, which modulates the frozen transformer layers within the VFM. Experimental results demonstrate that the Cloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the frozen backbone, achieves substantial performance gains. Cloud-Adapter consistently attains state-of-the-art (SOTA) performance across a wide variety of cloud segmentation datasets from multiple satellite sources, sensor series, data processing levels, land cover scenarios, and annotation granularities. We have released the source code and pretrained models at https://github.com/XavierJiezou/Cloud-Adapter to support further research.', 'score': 3, 'issue_id': 765, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': '06d32240370757a2', 'authors': ['Xuechao Zou', 'Shun Zhang', 'Kai Li', 'Shiying Wang', 'Junliang Xing', 'Lei Jin', 'Congyan Lang', 'Pin Tao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.13127.jpg', 'data': {'categories': ['#cv', '#dataset', '#open_source', '#architecture', '#training'], 'emoji': 'â˜ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cloud-Adapter Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ğ½Ğ° ÑĞ¿ÑƒÑ‚Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ…. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ (VFM) Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Cloud-Adapter Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° VFM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚ÑŒÑ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 0,6% Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Cloud Segmentation with Efficient Adaptation', 'desc': 'This paper addresses the challenge of cloud segmentation in remote sensing images, which is crucial for accurate data analysis. It introduces a novel method called Cloud-Adapter, which enhances segmentation performance by using a vision foundation model (VFM) that is pretrained and kept frozen. The approach employs a lightweight convolutional neural network to extract spatial features, which are then used to adapt the transformer layers of the VFM without additional training. Experimental results show that Cloud-Adapter achieves state-of-the-art performance while using only a small fraction of the trainable parameters, making it efficient and effective for various cloud segmentation tasks.'}, 'zh': {'title': 'äº‘åˆ†å‰²çš„æ–°æ–¹æ³•ï¼šCloud-Adapter', 'desc': 'äº‘åˆ†å‰²åœ¨é¥æ„Ÿå›¾åƒè§£è¯»ä¸­æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ï¼Œå…¶å‡†ç¡®æ€§ç›´æ¥å½±å“åç»­æ•°æ®å¤„ç†å’Œåˆ†æçš„æ•ˆæœã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCloud-Adapterçš„å‚æ•°é«˜æ•ˆè‡ªé€‚åº”æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜äº‘åˆ†å‰²çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åœ¨é€šç”¨é¢†åŸŸæ•°æ®ä¸Šé¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ï¼Œå¹¶ä¿æŒå…¶ä¸å˜ï¼Œé¿å…äº†é¢å¤–çš„è®­ç»ƒéœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCloud-Adapteråœ¨å¤šä¸ªå«æ˜Ÿæ¥æºçš„äº‘åˆ†å‰²æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…ä½¿ç”¨äº†å†»ç»“ä¸»å¹²ç½‘ç»œ0.6%çš„å¯è®­ç»ƒå‚æ•°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.15115', 'title': 'VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement', 'url': 'https://huggingface.co/papers/2411.15115', 'abstract': 'Recent text-to-video (T2V) diffusion models have demonstrated impressive generation capabilities across various domains. However, these models often generate videos that have misalignments with text prompts, especially when the prompts describe complex scenes with multiple objects and attributes. To address this, we introduce VideoRepair, a novel model-agnostic, training-free video refinement framework that automatically identifies fine-grained text-video misalignments and generates explicit spatial and textual feedback, enabling a T2V diffusion model to perform targeted, localized refinements. VideoRepair consists of four stages: In (1) video evaluation, we detect misalignments by generating fine-grained evaluation questions and answering those questions with MLLM. In (2) refinement planning, we identify accurately generated objects and then create localized prompts to refine other areas in the video. Next, in (3) region decomposition, we segment the correctly generated area using a combined grounding module. We regenerate the video by adjusting the misaligned regions while preserving the correct regions in (4) localized refinement. On two popular video generation benchmarks (EvalCrafter and T2V-CompBench), VideoRepair substantially outperforms recent baselines across various text-video alignment metrics. We provide a comprehensive analysis of VideoRepair components and qualitative examples.', 'score': 3, 'issue_id': 763, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '9d767f888609fdd4', 'authors': ['Daeun Lee', 'Jaehong Yoon', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.15115.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#alignment', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'VideoRepair - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ text-to-video Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². VideoRepair Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹, Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Enhancing Text-Video Alignment with VideoRepair', 'desc': 'The paper presents VideoRepair, a new framework designed to improve the alignment between text prompts and generated videos in text-to-video (T2V) diffusion models. It identifies and corrects misalignments by generating specific evaluation questions and using a multi-layer language model (MLLM) to assess the video. The framework operates in four stages: evaluating the video for misalignments, planning refinements, segmenting correctly generated areas, and finally refining the misaligned regions while keeping the accurate parts intact. VideoRepair shows significant improvements over existing methods on standard benchmarks, demonstrating its effectiveness in enhancing text-video coherence.'}, 'zh': {'title': 'VideoRepairï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„ç²¾ç¡®åº¦', 'desc': 'æœ€è¿‘çš„æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸå±•ç¤ºäº†å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆè§†é¢‘æ—¶ï¼Œå¸¸å¸¸ä¸æ–‡æœ¬æç¤ºå­˜åœ¨ä¸ä¸€è‡´ï¼Œå°¤å…¶æ˜¯åœ¨æè¿°å¤æ‚åœºæ™¯æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VideoRepairï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ— æ¨¡å‹ã€æ— è®­ç»ƒçš„è§†é¢‘ä¿®å¤æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«ç»†ç²’åº¦çš„æ–‡æœ¬è§†é¢‘ä¸å¯¹é½ï¼Œå¹¶ç”Ÿæˆæ˜ç¡®çš„ç©ºé—´å’Œæ–‡æœ¬åé¦ˆï¼Œä»è€Œä½¿T2Væ‰©æ•£æ¨¡å‹èƒ½å¤Ÿè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å±€éƒ¨ä¿®æ­£ã€‚VideoRepairé€šè¿‡è§†é¢‘è¯„ä¼°ã€ä¿®å¤è§„åˆ’ã€åŒºåŸŸåˆ†è§£å’Œå±€éƒ¨ä¿®å¤å››ä¸ªé˜¶æ®µæ¥å®ç°è§†é¢‘çš„ä¼˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.15033', 'title': 'One to rule them all: natural language to bind communication, perception and action', 'url': 'https://huggingface.co/papers/2411.15033', 'abstract': 'In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have a wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans. This paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback. The Planner Module is the core of the system where LLMs embedded in a modified ReAct framework are employed to interpret and carry out user commands. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances a robot adaptability, task execution, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robot ability to perform tasks. Using a dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue.', 'score': 2, 'issue_id': 758, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': 'f76bbfe3f8854ad7', 'authors': ['Simone Colombani', 'Dimitri Ognibene', 'Giuseppe Boccignone'], 'affiliations': ['Oversonic Robotics, Carate Brianza, Italy', 'University of Milan, Italy', 'University of Milano-Bicocca, Milan, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2411.15033.jpg', 'data': {'categories': ['#games', '#architecture', '#agents', '#robotics', '#optimization', '#agi', '#interpretability'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñ‹: Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ĞºĞ¾Ğ¼Ğ¼ÑƒĞ½Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¸ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğµ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑ Ğ¿Ğ»Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞœĞ¾Ğ´ÑƒĞ»ÑŒ ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°, Ğ³Ğ´Ğµ LLM, Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ Ğ² Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ReAct, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Empowering Robots with Natural Language Understanding and Dynamic Adaptability', 'desc': "This paper discusses a new architecture for robots that helps them understand and follow human instructions in real-time. It combines Large Language Models (LLMs) with a planning system that allows robots to interpret natural language commands and adapt to changing environments. The core of this system is a Planner Module that uses LLMs to translate user requests into actions while considering the current state of the environment. By incorporating feedback loops and a dynamic semantic map, the architecture improves the robot's ability to work alongside humans and adjust its plans as needed."}, 'zh': {'title': 'æ™ºèƒ½æœºå™¨äººï¼šè‡ªç„¶è¯­è¨€ä¸åŠ¨æ€ç¯å¢ƒçš„å®Œç¾ç»“åˆ', 'desc': 'è¿‘å¹´æ¥ï¼Œäººæœºäº¤äº’é¢†åŸŸçš„ç ”ç©¶é›†ä¸­åœ¨å¼€å‘èƒ½å¤Ÿç†è§£å¤æ‚äººç±»æŒ‡ä»¤å¹¶åœ¨åŠ¨æ€å¤šæ ·ç¯å¢ƒä¸­æ‰§è¡Œä»»åŠ¡çš„æœºå™¨äººã€‚è¿™äº›ç³»ç»Ÿåœ¨ä¸ªäººåŠ©ç†å’Œå·¥ä¸šæœºå™¨äººç­‰å¤šä¸ªåº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œå¼ºè°ƒæœºå™¨äººä¸äººç±»çµæ´»ã€è‡ªç„¶å’Œå®‰å…¨çš„äº’åŠ¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æœºå™¨äººè¡ŒåŠ¨è§„åˆ’æ¶æ„ï¼Œç»“åˆäº†é€šä¿¡ã€æ„ŸçŸ¥å’Œè§„åˆ’ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°†è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„å‘½ä»¤è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„æœºå™¨äººåŠ¨ä½œã€‚é€šè¿‡å®æ—¶åé¦ˆåŠ¨æ€æ›´æ–°è®¡åˆ’ï¼Œè¯¥ç³»ç»Ÿæé«˜äº†æœºå™¨äººåœ¨å…±äº«å’ŒåŠ¨æ€ç¯å¢ƒä¸­é€‚åº”æ€§ã€ä»»åŠ¡æ‰§è¡Œå’Œä¸äººç±»ç”¨æˆ·çš„æ— ç¼åä½œèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.15138', 'title': 'Material Anything: Generating Materials for Any 3D Object via Diffusion', 'url': 'https://huggingface.co/papers/2411.15138', 'abstract': 'We present Material Anything, a fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as a dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing a progressive material generation strategy guided by these confidence masks, along with a UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across a wide range of object categories and lighting conditions.', 'score': 14, 'issue_id': 776, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 22', 'zh': '11æœˆ22æ—¥'}, 'hash': '34b8f6718115f1e3', 'authors': ['Xin Huang', 'Tengfei Wang', 'Ziwei Liu', 'Qing Wang'], 'affiliations': ['Northwestern Polytechnical University', 'S-Lab, Nanyang Technological University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.15138.jpg', 'data': {'categories': ['#3d', '#architecture', '#optimization', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Material Anything - Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ½ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ñ€Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ÑÑ Ğ¼Ğ°ÑĞºĞ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ĞµĞ»ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ… Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Automating Realistic Material Generation for 3D Objects', 'desc': 'Material Anything is a novel framework that automates the generation of realistic materials for 3D objects using a unified diffusion approach. It simplifies the material creation process by eliminating the need for complex workflows and optimizations tailored to specific cases. The framework utilizes a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss, to ensure high-quality and stable material outputs. By incorporating confidence masks, it dynamically adapts to different object types and lighting scenarios, resulting in consistent and UV-ready materials across various conditions.'}, 'zh': {'title': 'å…¨è‡ªåŠ¨ææ–™ç”Ÿæˆï¼Œé€‚åº”å¤šç§å…‰ç…§æ¡ä»¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMaterial Anythingçš„å…¨è‡ªåŠ¨ç»Ÿä¸€æ‰©æ•£æ¡†æ¶ï¼Œæ—¨åœ¨ä¸º3Dç‰©ä½“ç”ŸæˆåŸºäºç‰©ç†çš„ææ–™ã€‚ä¸ç°æœ‰æ–¹æ³•ä¾èµ–å¤æ‚æµç¨‹æˆ–ç‰¹å®šä¼˜åŒ–ä¸åŒï¼ŒMaterial Anythingæä¾›äº†ä¸€ç§ç¨³å¥çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œé€‚åº”ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„ç‰©ä½“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶é€šè¿‡ä¸‰å¤´æ¶æ„å’Œæ¸²æŸ“æŸå¤±æ¥æé«˜ç¨³å®šæ€§å’Œææ–™è´¨é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç½®ä¿¡æ©ç ä½œä¸ºæ‰©æ•£æ¨¡å‹ä¸­çš„åŠ¨æ€åˆ‡æ¢å™¨ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†æœ‰çº¹ç†å’Œæ— çº¹ç†çš„ç‰©ä½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.15466', 'title': 'Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator', 'url': 'https://huggingface.co/papers/2411.15466', 'abstract': 'Subject-driven text-to-image generation aims to produce images of a new subject within a desired context by accurately capturing both the visual characteristics of the subject and the semantic content of a text prompt. Traditional methods rely on time- and resource-intensive fine-tuning for subject alignment, while recent zero-shot approaches leverage on-the-fly image prompting, often sacrificing subject alignment. In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Diptych Prompting arranges an incomplete diptych with the reference image in the left panel, and performs text-conditioned inpainting on the right panel. We further prevent unwanted content leakage by removing the background in the reference image and improve fine-grained details in the generated subject by enhancing attention weights between the panels during inpainting. Experimental results confirm that our approach significantly outperforms zero-shot image prompting methods, resulting in images that are visually preferred by users. Additionally, our method supports not only subject-driven generation but also stylized image generation and subject-driven image editing, demonstrating versatility across diverse image generation applications. Project page: https://diptychprompting.github.io/', 'score': 12, 'issue_id': 777, 'pub_date': '2024-11-23', 'pub_date_card': {'ru': '23 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 23', 'zh': '11æœˆ23æ—¥'}, 'hash': '288600e8c54930f4', 'authors': ['Chaehun Shin', 'Jooyoung Choi', 'Heeseung Kim', 'Sungroh Yoon'], 'affiliations': ['AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University', 'Data Science and AI Laboratory, ECE, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2411.15466.jpg', 'data': {'categories': ['#synthetic', '#cv', '#multimodal', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Diptych Prompting: Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Diptych Prompting. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ¿Ñ‚Ğ¸Ñ…Ğ° Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… text-to-image Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ĞºĞ°Ğº Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰Ğ°Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² Ğ»ĞµĞ²Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ¿Ñ‚Ğ¸Ñ…Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ°Ğ²ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°. Diptych Prompting Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ zero-shot Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Diptych Prompting: Zero-Shot Image Generation with Subject Precision', 'desc': 'This paper presents Diptych Prompting, a new method for generating images from text prompts while maintaining accurate subject alignment. Unlike traditional methods that require extensive fine-tuning, this zero-shot approach treats the task as inpainting, using a diptych format with a reference image. By focusing on the left panel for the reference and performing text-conditioned inpainting on the right, the method enhances detail and prevents unwanted content from leaking into the generated image. The results show that Diptych Prompting not only improves visual quality but also allows for versatile applications in stylized image generation and editing.'}, 'zh': {'title': 'Diptych Promptingï¼šç²¾å‡†çš„ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é›¶-shotæ–¹æ³•ï¼Œç§°ä¸ºDiptych Promptingï¼Œæ—¨åœ¨å®ç°ä¸»é¢˜é©±åŠ¨çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡å°†ç”Ÿæˆä»»åŠ¡é‡æ–°è§£é‡Šä¸ºå›¾åƒä¿®è¡¥ï¼Œç¡®ä¿äº†ä¸»é¢˜çš„ç²¾ç¡®å¯¹é½ã€‚Diptych Promptingåˆ©ç”¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„åŒè”ç”Ÿæˆç‰¹æ€§ï¼Œå·¦ä¾§é¢æ¿å±•ç¤ºå‚è€ƒå›¾åƒï¼Œå³ä¾§é¢æ¿è¿›è¡Œæ–‡æœ¬æ¡ä»¶çš„ä¿®è¡¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰æ•ˆæœä¸Šä¼˜äºä¼ ç»Ÿçš„é›¶-shotå›¾åƒæç¤ºæ–¹æ³•ï¼Œä¸”æ”¯æŒå¤šç§å›¾åƒç”Ÿæˆåº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14522', 'title': 'GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI', 'url': 'https://huggingface.co/papers/2411.14522', 'abstract': "Despite significant advancements in general artificial intelligence, such as GPT-4, their effectiveness in the medical domain (general medical AI, GMAI) remains constrained due to the absence of specialized medical knowledge. To address this challenge, we present GMAI-VL-5.5M, a comprehensive multimodal medical dataset created by converting hundreds of specialized medical datasets into meticulously constructed image-text pairs. This dataset features comprehensive task coverage, diverse modalities, and high-quality image-text data. Building upon this multimodal dataset, we propose GMAI-VL, a general medical vision-language model with a progressively three-stage training strategy. This approach significantly enhances the model's ability by integrating visual and textual information, thereby improving its ability to process multimodal data and support accurate diagnosis and clinical decision-making. Experimental evaluations demonstrate that GMAI-VL achieves state-of-the-art results across a wide range of multimodal medical tasks, such as visual question answering and medical image diagnosis. Our contributions include the development of the GMAI-VL-5.5M dataset, the introduction of the GMAI-VL model, and the establishment of new benchmarks in multiple medical domains. Code and dataset will be released at https://github.com/uni-medical/GMAI-VL.", 'score': 5, 'issue_id': 778, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'eb0e262f1661d5c8', 'authors': ['Tianbin Li', 'Yanzhou Su', 'Wei Li', 'Bin Fu', 'Zhe Chen', 'Ziyan Huang', 'Guoan Wang', 'Chenglong Ma', 'Ying Chen', 'Ming Hu', 'Yanjun Li', 'Pengcheng Chen', 'Xiaowei Hu', 'Zhongying Deng', 'Yuanfeng Ji', 'Jin Ye', 'Yu Qiao', 'Junjun He'], 'affiliations': ['East China Normal University', 'Fudan University', 'Monash University', 'Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences', 'Stanford University', 'University of Cambridge', 'University of Washington', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14522.jpg', 'data': {'categories': ['#agi', '#benchmark', '#multimodal', '#optimization', '#science', '#healthcare', '#dataset'], 'emoji': 'ğŸ¥', 'ru': {'title': 'GMAI-VL: ĞœĞ¾Ñ‰Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ GMAI-VL-5.5M - Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ‚ĞµĞ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¿Ğ°Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GMAI-VL - Ğ¾Ğ±Ñ‰Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ¾ Ñ‚Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸. GMAI-VL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ²ĞºĞ»Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸.'}, 'en': {'title': 'Empowering Medical AI with Multimodal Learning', 'desc': "This paper introduces GMAI-VL-5.5M, a new multimodal medical dataset designed to enhance general medical AI (GMAI) by combining various specialized medical datasets into image-text pairs. The dataset supports a wide range of medical tasks and includes high-quality data that improves the model's understanding of both visual and textual information. The authors propose a three-stage training strategy for the GMAI-VL model, which significantly boosts its performance in tasks like visual question answering and medical image diagnosis. Experimental results show that GMAI-VL sets new benchmarks in the medical domain, demonstrating its effectiveness in aiding clinical decision-making."}, 'zh': {'title': 'åŒ»å­¦é¢†åŸŸçš„å¤šæ¨¡æ€æ™ºèƒ½çªç ´', 'desc': 'å°½ç®¡é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆå¦‚GPT-4ï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨åŒ»å­¦é¢†åŸŸçš„æœ‰æ•ˆæ€§ä»ç„¶å—åˆ°é™åˆ¶ï¼Œå› ä¸ºç¼ºä¹ä¸“ä¸šçš„åŒ»å­¦çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GMAI-VL-5.5Mï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å°†æ•°ç™¾ä¸ªä¸“ä¸šåŒ»å­¦æ•°æ®é›†è½¬æ¢ä¸ºç²¾å¿ƒæ„å»ºçš„å›¾åƒ-æ–‡æœ¬å¯¹è€Œåˆ›å»ºçš„ç»¼åˆå¤šæ¨¡æ€åŒ»å­¦æ•°æ®é›†ã€‚åŸºäºè¿™ä¸ªå¤šæ¨¡æ€æ•°æ®é›†ï¼Œæˆ‘ä»¬æå‡ºäº†GMAI-VLï¼Œä¸€ä¸ªå…·æœ‰é€æ­¥ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥çš„é€šç”¨åŒ»å­¦è§†è§‰-è¯­è¨€æ¨¡å‹ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒGMAI-VLåœ¨è§†è§‰é—®ç­”å’ŒåŒ»å­¦å›¾åƒè¯Šæ–­ç­‰å¤šç§å¤šæ¨¡æ€åŒ»å­¦ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.16681', 'title': 'Factorized Visual Tokenization and Generation', 'url': 'https://huggingface.co/papers/2411.16681', 'abstract': 'Visual tokenizers are fundamental to image generation. They convert visual data into discrete tokens, enabling transformer-based models to excel at image generation. Despite their success, VQ-based tokenizers like VQGAN face significant limitations due to constrained vocabulary sizes. Simply expanding the codebook often leads to training instability and diminishing performance gains, making scalability a critical challenge. In this work, we introduce Factorized Quantization (FQ), a novel approach that revitalizes VQ-based tokenizers by decomposing a large codebook into multiple independent sub-codebooks. This factorization reduces the lookup complexity of large codebooks, enabling more efficient and scalable visual tokenization. To ensure each sub-codebook captures distinct and complementary information, we propose a disentanglement regularization that explicitly reduces redundancy, promoting diversity across the sub-codebooks. Furthermore, we integrate representation learning into the training process, leveraging pretrained vision models like CLIP and DINO to infuse semantic richness into the learned representations. This design ensures our tokenizer captures diverse semantic levels, leading to more expressive and disentangled representations. Experiments show that the proposed FQGAN model substantially improves the reconstruction quality of visual tokenizers, achieving state-of-the-art performance. We further demonstrate that this tokenizer can be effectively adapted into auto-regressive image generation. https://showlab.github.io/FQGAN', 'score': 4, 'issue_id': 780, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '966d673404fb7a77', 'authors': ['Zechen Bai', 'Jianxiong Gao', 'Ziteng Gao', 'Pichao Wang', 'Zheng Zhang', 'Tong He', 'Mike Zheng Shou'], 'affiliations': ['Amazon', 'Fudan University', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.16681.jpg', 'data': {'categories': ['#optimization', '#architecture', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¤Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ - Ğ¤Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (FQ). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ĞºĞ¾Ğ´Ğ±ÑƒĞº Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ´-ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ğ´-ĞºĞ¾Ğ´Ğ±ÑƒĞºĞ°Ğ¼Ğ¸. Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº CLIP Ğ¸ DINO, Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Revitalizing Image Generation with Factorized Quantization', 'desc': 'This paper presents a new method called Factorized Quantization (FQ) to improve visual tokenizers used in image generation. Traditional VQ-based tokenizers struggle with limited vocabulary sizes, which can hinder performance and scalability. FQ addresses this by breaking down a large codebook into smaller, independent sub-codebooks, reducing complexity and enhancing efficiency. Additionally, the method incorporates disentanglement regularization and representation learning to ensure diverse and rich semantic representations, leading to better image generation results.'}, 'zh': {'title': 'å› å­åŒ–é‡åŒ–ï¼šæå‡è§†è§‰æ ‡è®°å™¨çš„æ•ˆç‡ä¸è¡¨ç°', 'desc': 'è§†è§‰æ ‡è®°å™¨æ˜¯å›¾åƒç”Ÿæˆçš„åŸºç¡€ï¼Œå®ƒå°†è§†è§‰æ•°æ®è½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°ï¼Œä½¿åŸºäºå˜æ¢å™¨çš„æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚å°½ç®¡VQGANç­‰åŸºäºVQçš„æ ‡è®°å™¨å–å¾—äº†ä¸€å®šæˆåŠŸï¼Œä½†ç”±äºè¯æ±‡é‡æœ‰é™ï¼Œå®ƒä»¬é¢ä¸´ç€æ˜¾è‘—çš„å±€é™æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”å› å­åŒ–é‡åŒ–ï¼ˆFQï¼‰ï¼Œé€šè¿‡å°†å¤§å‹ä»£ç æœ¬åˆ†è§£ä¸ºå¤šä¸ªç‹¬ç«‹çš„å­ä»£ç æœ¬ï¼Œæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä»è€Œæé«˜äº†è§†è§‰æ ‡è®°åŒ–çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFQGANæ¨¡å‹æ˜¾è‘—æé«˜äº†è§†è§‰æ ‡è®°å™¨çš„é‡å»ºè´¨é‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.16594', 'title': 'From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2411.16594', 'abstract': 'Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the "LLM-as-a-judge" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and https://llm-as-a-judge.github.io.', 'score': 4, 'issue_id': 778, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '56883eb77dcb5fa3', 'authors': ['Dawei Li', 'Bohan Jiang', 'Liangjie Huang', 'Alimohammad Beigi', 'Chengshuai Zhao', 'Zhen Tan', 'Amrita Bhattacharjee', 'Yuxuan Jiang', 'Canyu Chen', 'Tianhao Wu', 'Kai Shu', 'Lu Cheng', 'Huan Liu'], 'affiliations': ['Arizona State University', 'Emory University', 'Illinois Institute of Technology', 'University of California, Berkeley', 'University of Illinois Chicago', 'University of Maryland, Baltimore County'], 'pdf_title_img': 'assets/pdf/title_img/2411.16594.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#survey'], 'emoji': 'âš–ï¸', 'ru': {'title': 'LLM ĞºĞ°Ğº ÑÑƒĞ´ÑŒÑ: Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² AI Ğ¸ NLP', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑÑƒĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ÑƒÑ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° 'LLM-as-a-judge', Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ Ñ‡Ñ‚Ğ¾, ĞºĞ°Ğº Ğ¸ Ğ³Ğ´Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ. Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."}, 'en': {'title': 'Harnessing LLMs for Enhanced AI Evaluation', 'desc': 'This paper discusses the challenges of assessment and evaluation in artificial intelligence and natural language processing, particularly focusing on the limitations of traditional methods. It introduces the innovative concept of using Large Language Models (LLMs) as judges to score, rank, or select outputs in various tasks. The authors provide a detailed framework that categorizes LLM-based judgment into three key areas: what to judge, how to judge, and where to judge. Additionally, the paper compiles benchmarks for evaluating these models and identifies future research directions to enhance the effectiveness of LLMs in assessment tasks.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼šè¯„åˆ¤çš„æ–°åŠ›é‡', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°å’Œåˆ¤æ–­ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†â€œLLMä½œä¸ºè¯„åˆ¤è€…â€çš„æ–°èŒƒå¼ã€‚ä¼ ç»Ÿçš„è¯„ä¼°æ–¹æ³•å¾€å¾€æ— æ³•æœ‰æ•ˆåˆ¤æ–­ç»†å¾®çš„å±æ€§ï¼Œè€ŒLLMèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸­è¿›è¡Œæ‰“åˆ†ã€æ’åå’Œé€‰æ‹©ã€‚æˆ‘ä»¬ä»è¾“å…¥å’Œè¾“å‡ºçš„è§’åº¦è¯¦ç»†å®šä¹‰äº†è¯„åˆ¤çš„æ¦‚å¿µï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„åˆ†ç±»æ³•ï¼Œæ¢è®¨äº†è¯„åˆ¤çš„å†…å®¹ã€æ–¹å¼å’Œåœºæ‰€ã€‚æœ€åï¼Œæˆ‘ä»¬ç¼–åˆ¶äº†è¯„ä¼°LLMä½œä¸ºè¯„åˆ¤è€…çš„åŸºå‡†ï¼Œå¹¶å¼ºè°ƒäº†å…³é”®æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.16657', 'title': 'DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation', 'url': 'https://huggingface.co/papers/2411.16657', 'abstract': "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.", 'score': 4, 'issue_id': 777, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '02cf3312e8d1f6ca', 'authors': ['Zun Wang', 'Jialu Li', 'Han Lin', 'Jaehong Yoon', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.16657.jpg', 'data': {'categories': ['#multimodal', '#3d', '#video', '#story_generation'], 'emoji': 'ğŸ¬', 'ru': {'title': 'DreamRunner: ĞÑ‚ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'DreamRunner - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ğµ. DreamRunner Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ñ….'}, 'en': {'title': 'DreamRunner: Crafting Seamless Storytelling Videos from Text', 'desc': 'This paper introduces DreamRunner, a new method for generating storytelling videos from text scripts. It addresses challenges in creating complex motions and maintaining object consistency across scenes by using a large language model for scene planning and a retrieval-augmented approach for motion customization. The method incorporates a novel spatial-temporal region-based attention module to ensure precise object-motion binding and semantic control in video frames. DreamRunner outperforms existing models in character consistency and smooth transitions, showcasing its effectiveness in generating multi-object interactions and adhering to compositional text prompts.'}, 'zh': {'title': 'DreamRunnerï¼šåˆ›æ–°çš„æ•…äº‹è§†é¢‘ç”Ÿæˆæ–¹æ³•', 'desc': 'æ•…äº‹è§†é¢‘ç”Ÿæˆï¼ˆSVGï¼‰æ˜¯ä¸€é¡¹æ–°å…´ä»»åŠ¡ï¼Œæ—¨åœ¨æ ¹æ®è¾“å…¥æ–‡æœ¬è„šæœ¬åˆ›å»ºé•¿ç¯‡ã€å¤šåŠ¨ä½œã€å¤šåœºæ™¯çš„è§†é¢‘ã€‚è¯¥æ–¹æ³•é¢ä¸´ç€å¤šä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¯¹è±¡éœ€è¦å±•ç°å¤æ‚çš„ç»†å¾®åŠ¨ä½œï¼Œä»¥åŠå¤šä¸ªå¯¹è±¡åœ¨ä¸åŒåœºæ™¯ä¸­çš„ä¸€è‡´æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DreamRunnerï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æ•…äº‹åˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œåœºæ™¯è§„åˆ’å’Œå¯¹è±¡å¸ƒå±€ã€‚DreamRunnerè¿˜å¼•å…¥äº†ç©ºé—´-æ—¶é—´åŒºåŸŸåŸºç¡€çš„3Dæ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿå®ç°ç»†ç²’åº¦çš„å¯¹è±¡åŠ¨ä½œç»‘å®šå’Œé€å¸§è¯­ä¹‰æ§åˆ¶ï¼Œå±•ç°å‡ºåœ¨è§’è‰²ä¸€è‡´æ€§å’Œæ–‡æœ¬å¯¹é½æ–¹é¢çš„å…ˆè¿›æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.16205', 'title': 'MH-MoE:Multi-Head Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2411.16205', 'abstract': 'Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by using the multi-head mechanism to collectively attend to information from various representation spaces within different experts. In this paper, we present a novel implementation of MH-MoE that maintains both FLOPs and parameter parity with sparse Mixture of Experts models. Experimental results on language models show that the new implementation yields quality improvements over both vanilla MoE and fine-grained MoE models. Additionally, our experiments demonstrate that MH-MoE is compatible with 1-bit Large Language Models (LLMs) such as BitNet.', 'score': 3, 'issue_id': 778, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'b684b6d745cb66ff', 'authors': ['Shaohan Huang', 'Xun Wu', 'Shuming Ma', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2411.16205.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ñ ÑĞ¼ĞµÑÑŒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MH-MoE), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ¸Ñ‚ĞµÑ‚ Ğ¿Ğ¾ FLOP Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ MoE Ğ¸ Ğ¼ĞµĞ»ĞºĞ¾Ğ·ĞµÑ€Ğ½Ğ¸ÑÑ‚Ñ‹Ğ¼Ğ¸ MoE Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. MH-MoE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ MH-MoE Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº BitNet.'}, 'en': {'title': 'Unlocking Performance with Multi-Head Mixture-of-Experts', 'desc': 'The Multi-Head Mixture-of-Experts (MH-MoE) model enhances performance by allowing multiple heads to focus on different aspects of data from various experts. This paper introduces a new way to implement MH-MoE that keeps the same computational cost and number of parameters as traditional sparse Mixture of Experts models. Experiments conducted on language models reveal that this new approach provides better results compared to standard MoE and fine-grained MoE models. Furthermore, the findings indicate that MH-MoE can effectively work with 1-bit Large Language Models like BitNet.'}, 'zh': {'title': 'å¤šå¤´æ··åˆä¸“å®¶ï¼šæå‡æ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'å¤šå¤´æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMH-MoEï¼‰é€šè¿‡å¤šå¤´æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶å…³æ³¨æ¥è‡ªä¸åŒä¸“å®¶çš„å¤šç§è¡¨ç¤ºç©ºé—´çš„ä¿¡æ¯ï¼Œä»è€Œå±•ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„MH-MoEå®ç°ï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—é‡ï¼ˆFLOPsï¼‰å’Œå‚æ•°æ•°é‡ä¸Šä¸ç¨€ç–æ··åˆä¸“å®¶æ¨¡å‹ä¿æŒä¸€è‡´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–°å®ç°ç›¸è¾ƒäºä¼ ç»Ÿçš„MoEå’Œç»†ç²’åº¦MoEæ¨¡å‹åœ¨è¯­è¨€æ¨¡å‹ä¸Šæœ‰æ˜¾è‘—çš„è´¨é‡æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¿˜è¡¨æ˜MH-MoEä¸1ä½å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BitNetï¼‰å…¼å®¹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14486', 'title': 'The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz', 'url': 'https://huggingface.co/papers/2411.14486', 'abstract': "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to acknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand challenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including both open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect responses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in fields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem difficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more challenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more prone to generate speculative answers when problems appear more tractable. The study also revealed significant variations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and NP-hard problems while performing relatively better on philosophical and psychological challenges. These results contribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the importance of uncertainty recognition as a critical component of future machine intelligence evaluation. This impossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing empirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting new directions for improving model training architectures and evaluation approaches.", 'score': 3, 'issue_id': 777, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'c30a94b30cace49a', 'authors': ['David Noever', 'Forrest McKee'], 'affiliations': ['PeopleTec, Inc., Huntsville, AL'], 'pdf_title_img': 'assets/pdf/title_img/2411.14486.jpg', 'data': {'categories': ['#architecture', '#training', '#benchmark', '#interpretability', '#agi', '#dataset'], 'emoji': 'ğŸ¤”', 'ru': {'title': 'ĞŸÑ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ·Ğ½Ğ°Ğ½Ğ¸Ñ: ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ°ÑĞ¿ĞµĞºÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 675 Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµÑ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ñ…. Ğ”Ğ²ĞµĞ½Ğ°Ğ´Ñ†Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¸Ñ… ÑĞºĞ»Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ·Ğ½Ğ°Ğ½Ğ¸Ğµ, Ğ° Ğ½Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 62-68% Ğ² Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼.'}, 'en': {'title': 'Evaluating Uncertainty: A New Benchmark for Language Models', 'desc': "This research presents a new framework to evaluate how well large language models (LLMs) recognize their own uncertainty when faced with unsolvable problems. By testing twelve advanced LLMs on a set of graduate-level questions that have no answers, the study found that the best models could admit ignorance 62-68% of the time. Interestingly, the models were more likely to acknowledge uncertainty on harder problems, with GPT-4 showing a 35.8% acknowledgment rate on difficult questions. The findings emphasize the need for better training and evaluation methods to enhance LLMs' ability to recognize their knowledge limits, which is crucial for advancing artificial general intelligence (AGI)."}, 'zh': {'title': 'æ‰¿è®¤ä¸ç¡®å®šæ€§ï¼šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–°è§†è§’', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨675ä¸ªæ ¹æœ¬æ— æ³•è§£å†³çš„é—®é¢˜ä¸Šæ‰¿è®¤ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç»„ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„ç ”ç©¶ç”Ÿçº§åˆ«çš„é‡å¤§æŒ‘æˆ˜é—®é¢˜æ•°æ®é›†ï¼Œè¯„ä¼°äº†åŒ…æ‹¬å¼€æºå’Œé—­æºæ¨¡å‹åœ¨å†…çš„åäºŒä¸ªæœ€å…ˆè¿›çš„LLMsï¼Œè§‚å¯Ÿå®ƒä»¬æ‰¿è®¤æ— çŸ¥çš„å€¾å‘ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹åœ¨æ‰¿è®¤é—®é¢˜è§£å†³æ–¹æ¡ˆæœªçŸ¥çš„å‡†ç¡®ç‡èŒƒå›´ä¸º62%åˆ°68%ï¼Œå¹¶ä¸”åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ä¸Šï¼ˆå¦‚ç”Ÿç‰©å­¦ã€å“²å­¦å’Œæ•°å­¦ï¼‰è¡¨ç°å‡ºæ›´é«˜çš„ä¸ç¡®å®šæ€§æ‰¿è®¤ç‡ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä¸åŒé—®é¢˜ç±»åˆ«ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œæ¨¡å‹åœ¨æ‰¿è®¤å‘æ˜å’ŒNPéš¾é¢˜çš„ä¸ç¡®å®šæ€§æ—¶è¡¨ç°è¾ƒå·®ï¼Œè€Œåœ¨å“²å­¦å’Œå¿ƒç†å­¦æŒ‘æˆ˜ä¸­è¡¨ç°ç›¸å¯¹è¾ƒå¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.16443', 'title': 'SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis', 'url': 'https://huggingface.co/papers/2411.16443', 'abstract': "Text-based generation and editing of 3D scenes hold significant potential for streamlining content creation through intuitive user interactions. While recent advances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time rendering, existing methods are often specialized and task-focused, lacking a unified framework for both generation and editing. In this paper, we introduce SplatFlow, a comprehensive framework that addresses this gap by enabling direct 3DGS generation and editing. SplatFlow comprises two main components: a multi-view rectified flow (RF) model and a Gaussian Splatting Decoder (GSDecoder). The multi-view RF model operates in latent space, generating multi-view images, depths, and camera poses simultaneously, conditioned on text prompts, thus addressing challenges like diverse scene scales and complex camera trajectories in real-world settings. Then, the GSDecoder efficiently translates these latent outputs into 3DGS representations through a feed-forward 3DGS method. Leveraging training-free inversion and inpainting techniques, SplatFlow enables seamless 3DGS editing and supports a broad range of 3D tasks-including object editing, novel view synthesis, and camera pose estimation-within a unified framework without requiring additional complex pipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K datasets, demonstrating its versatility and effectiveness in various 3D generation, editing, and inpainting-based tasks.", 'score': 2, 'issue_id': 779, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '8447f5d110a89105', 'authors': ['Hyojun Go', 'Byeongjun Park', 'Jiho Jang', 'Jin-Young Kim', 'Soonwoo Kwon', 'Changick Kim'], 'affiliations': ['KAIST', 'Twelve Labs'], 'pdf_title_img': 'assets/pdf/title_img/2411.16443.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½', 'desc': 'SplatFlow - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-ÑÑ†ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ½Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ¸ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ ÑĞ¿Ğ»Ğ°Ñ‚Ñ‚Ğ¸Ğ½Ğ³Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ² 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. SplatFlow Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'SplatFlow: Unifying 3D Scene Generation and Editing', 'desc': 'This paper presents SplatFlow, a unified framework for generating and editing 3D scenes using 3D Gaussian Splatting (3DGS). It features a multi-view rectified flow model that generates images, depths, and camera poses from text prompts, addressing challenges in scene diversity and camera movement. The framework also includes a Gaussian Splatting Decoder that converts latent outputs into 3DGS representations efficiently. SplatFlow supports various 3D tasks like object editing and novel view synthesis, demonstrating its effectiveness on multiple datasets without the need for complex pipelines.'}, 'zh': {'title': 'SplatFlowï¼šç»Ÿä¸€çš„3Dç”Ÿæˆä¸ç¼–è¾‘æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSplatFlowçš„ç»¼åˆæ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–3Dåœºæ™¯çš„ç”Ÿæˆå’Œç¼–è¾‘ã€‚SplatFlowç»“åˆäº†å¤šè§†è§’æ•´æµæµæ¨¡å‹å’Œé«˜æ–¯ç‚¹äº‘è§£ç å™¨ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºåŒæ—¶ç”Ÿæˆå¤šè§†è§’å›¾åƒã€æ·±åº¦å’Œç›¸æœºå§¿æ€ã€‚è¯¥æ¡†æ¶é€šè¿‡æ— è®­ç»ƒåæ¼”å’Œä¿®è¡¥æŠ€æœ¯ï¼Œå®ç°äº†æ— ç¼çš„3Dé«˜æ–¯ç‚¹äº‘ç¼–è¾‘ï¼Œæ”¯æŒå¤šç§3Dä»»åŠ¡ï¼Œå¦‚ç‰©ä½“ç¼–è¾‘å’Œæ–°è§†è§’åˆæˆã€‚æˆ‘ä»¬åœ¨MVImgNetå’ŒDL3DV-7Kæ•°æ®é›†ä¸ŠéªŒè¯äº†SplatFlowçš„èƒ½åŠ›ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§3Dç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.16035', 'title': 'Predicting Emergent Capabilities by Finetuning', 'url': 'https://huggingface.co/papers/2411.16035', 'abstract': 'A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models. In this work, we first pose the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? We then discover a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, we can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., "emergence laws"). We validate this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, we find that, in some cases, we can accurately predict whether models trained with up to 4x more compute have emerged. Finally, we present a case study of two realistic uses for emergence prediction.', 'score': 2, 'issue_id': 779, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': 'a3e818d78a8bea58', 'authors': ['Charlie Snell', 'Eric Wallace', 'Dan Klein', 'Sergey Levine'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2411.16035.jpg', 'data': {'categories': ['#optimization', '#training', '#agi', '#benchmark', '#open_source'], 'emoji': 'ğŸ”®', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ˜Ğ˜: Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñƒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ´Ğ²Ğ¸Ğ½ÑƒÑ‚ÑŒ Ñ‚Ğ¾Ñ‡ĞºÑƒ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¼ĞµĞ½ĞµĞµ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¼Ğ¾Ğ³Ğ»Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ¼ĞµÑ€Ğ´Ğ¶ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Predicting Emergence: Unlocking Future LLM Capabilities', 'desc': 'This paper addresses the challenge of predicting emergent capabilities in large language models (LLMs) as they scale. While the pretraining loss of LLMs is predictable based on compute resources, their downstream performance can show unexpected jumps in capability. The authors introduce the concept of emergence prediction, which involves fine-tuning LLMs on specific tasks to determine when these emergent capabilities will appear. They validate their approach using standard NLP benchmarks and demonstrate that it is possible to predict the emergence of capabilities in future models based on the performance of smaller models.'}, 'zh': {'title': 'é¢„æµ‹è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„çªç ´', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ‰©å±•æ—¶å‡ºç°çš„èƒ½åŠ›é¢„æµ‹é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡å¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æå‰é¢„æµ‹æœªæ¥æ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¾®è°ƒå¯ä»¥å°†èƒ½åŠ›å‡ºç°çš„ä¸´ç•Œç‚¹å‘èƒ½åŠ›è¾ƒä½çš„æ¨¡å‹ç§»åŠ¨ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†ä¸ŠéªŒè¯äº†è¿™ä¸€æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™ä¸€é¢„æµ‹èƒ½åŠ›è¿›è¡Œå®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.16341', 'title': 'From CISC to RISC: language-model guided assembly transpilation', 'url': 'https://huggingface.co/papers/2411.16341', 'abstract': "The transition from x86 to ARM architecture is becoming increasingly common across various domains, primarily driven by ARM's energy efficiency and improved performance across traditional sectors. However, this ISA shift poses significant challenges, mainly due to the extensive legacy ecosystem of x86 software and lack of portability across proprietary ecosystems and software stacks. This paper introduces CRT, a lightweight LLM-based transpiler that automatically converts x86 assembly to ARM assembly. Our approach bridges the fundamental architectural gap between x86's CISC-based and ARM's RISC-based computing paradigms while preserving program semantics and optimizing performance. We evaluate CRT on diverse real-world applications, achieving 79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite, and an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2 hardware (ARMv8), our transpiled code achieves 1.73times speedup compared to Apple's Rosetta 2 virtualization engine, while delivering 2.41times memory efficiency and 1.47times better energy consumption. Through testing and analysis, we show that CRT successfully navigates the CISC/RISC divide and generates correctly executable RISC code despite machine ``language'' barriers. We release our code, models, training datasets, and benchmarks at: https://ahmedheakl.github.io/asm2asm/.", 'score': 2, 'issue_id': 778, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '1d28eaae89074f91', 'authors': ['Ahmed Heakl', 'Chaimaa Abi', 'Rania Hossam', 'Abdulrahman Mahmoud'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE'], 'pdf_title_img': 'assets/pdf/title_img/2411.16341.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#benchmark'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° CISC/RISC: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞ»ÑÑ†Ğ¸Ñ x86 Ğ² ARM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CRT - Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»ÑÑ‚Ğ¾Ñ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°ÑÑĞµĞ¼Ğ±Ğ»ĞµÑ€Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ x86 Ğ² Ğ°ÑÑĞµĞ¼Ğ±Ğ»ĞµÑ€ ARM. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°Ğ¼Ğ¸ CISC (x86) Ğ¸ RISC (ARM), ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ CRT Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ 79.25% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ñ x86 Ğ½Ğ° ARMv5. Ğ’ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ½Ğ° Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Apple M2 (ARMv8) Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ² 1.73 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞºĞ¾Ğ¼ Apple Rosetta 2.'}, 'en': {'title': 'Bridging the CISC/RISC Divide with CRT Transpiler', 'desc': "This paper presents CRT, a lightweight LLM-based transpiler designed to convert x86 assembly code into ARM assembly code. The transition from x86 to ARM architecture is challenging due to the differences in their instruction set architectures (ISAs), specifically x86's Complex Instruction Set Computing (CISC) and ARM's Reduced Instruction Set Computing (RISC). CRT effectively bridges this gap while maintaining the original program's functionality and optimizing performance. The evaluation shows that CRT achieves high translation accuracy and outperforms existing solutions in terms of speed, memory efficiency, and energy consumption on ARM hardware."}, 'zh': {'title': 'è½»æ¾è·¨è¶Šæ¶æ„é¸¿æ²Ÿï¼Œæå‡æ€§èƒ½ä¸æ•ˆç‡', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCRTçš„è½»é‡çº§LLMåŸºç¡€çš„è½¬è¯‘å™¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†x86æ±‡ç¼–ä»£ç è½¬æ¢ä¸ºARMæ±‡ç¼–ä»£ç ã€‚è¯¥æ–¹æ³•è§£å†³äº†x86çš„å¤æ‚æŒ‡ä»¤é›†ï¼ˆCISCï¼‰ä¸ARMçš„ç²¾ç®€æŒ‡ä»¤é›†ï¼ˆRISCï¼‰ä¹‹é—´çš„æ¶æ„å·®å¼‚ï¼ŒåŒæ—¶ä¿æŒç¨‹åºè¯­ä¹‰å¹¶ä¼˜åŒ–æ€§èƒ½ã€‚é€šè¿‡åœ¨çœŸå®åº”ç”¨ä¸Šçš„è¯„ä¼°ï¼ŒCRTåœ¨x86åˆ°ARMv5çš„è½¬æ¢å‡†ç¡®ç‡è¾¾åˆ°79.25%ï¼Œåœ¨x86åˆ°RISC-Vçš„è½¬æ¢å‡†ç¡®ç‡è¾¾åˆ°88.68%ã€‚åœ¨Apple M2ç¡¬ä»¶ä¸Šçš„å®é™…éƒ¨ç½²ä¸­ï¼Œè½¬è¯‘åçš„ä»£ç ç›¸æ¯”äºAppleçš„Rosetta 2è™šæ‹ŸåŒ–å¼•æ“å®ç°äº†1.73å€çš„é€Ÿåº¦æå‡å’Œ2.41å€çš„å†…å­˜æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.16489', 'title': 'O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?', 'url': 'https://huggingface.co/papers/2411.16489', 'abstract': "This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount.", 'score': 1, 'issue_id': 780, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '9d7613aad6cae404', 'authors': ['Zhen Huang', 'Haoyang Zou', 'Xuefeng Li', 'Yixiu Liu', 'Yuxiang Zheng', 'Ethan Chern', 'Shijie Xia', 'Yiwei Qin', 'Weizhe Yuan', 'Pengfei Liu'], 'affiliations': ['Generative AI Research Lab (GAIR)', 'NYU', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2411.16489.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#data', '#math', '#benchmark', '#transfer_learning', '#hallucinations', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹: ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ O1 Ğ¾Ñ‚ OpenAI, ÑƒĞ´ĞµĞ»ÑÑ Ğ¾ÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ, Ğ½Ğ¾ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½ĞµÑ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, ĞºĞ°Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ· API O1 Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ´ĞµÑÑÑ‚ĞºĞ°Ñ… Ñ‚Ñ‹ÑÑÑ‡ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· O1 Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ O1-preview Ğ½Ğ° Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ ÑĞºĞ·Ğ°Ğ¼ĞµĞ½Ğµ AIME Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· O1, Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': 'Unlocking AI Potential: The Power of Transparent Distillation', 'desc': "This paper critically analyzes the methods used to replicate the capabilities of OpenAI's O1 model, emphasizing the often hidden use of knowledge distillation techniques. The authors demonstrate that fine-tuning a base model with data distilled from O1's API can lead to better performance on complex mathematical reasoning tasks compared to the original O1 model. Their experiments reveal that models trained on O1-distilled data not only excel in math but also generalize well to other tasks, showing reduced biases and improved safety. The study advocates for transparency in AI research and highlights the importance of foundational understanding in developing advanced AI systems."}, 'zh': {'title': 'çŸ¥è¯†è’¸é¦ï¼šæå‡AIæ¨¡å‹æ€§èƒ½çš„å…³é”®', 'desc': 'æœ¬æ–‡å¯¹å½“å‰å¤åˆ¶OpenAI O1æ¨¡å‹èƒ½åŠ›çš„æ–¹æ³•è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œç‰¹åˆ«å…³æ³¨çŸ¥è¯†è’¸é¦æŠ€æœ¯çš„å¹¿æ³›ä½¿ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç®€å•çš„ä»O1 APIè¿›è¡Œè’¸é¦ï¼Œå¹¶ç»“åˆç›‘ç£å¾®è°ƒï¼Œå¯ä»¥åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šå®ç°ä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„åŸºç¡€æ¨¡å‹åœ¨ç¾å›½é‚€è¯·æ•°å­¦è€ƒè¯•ï¼ˆAIMEï¼‰ä¸­è¡¨ç°ä¼˜äºO1é¢„è§ˆï¼Œä¸”æŠ€æœ¯å¤æ‚æ€§è¾ƒä½ã€‚æ­¤å¤–ï¼Œå°½ç®¡æ¨¡å‹ä»…åœ¨æ•°å­¦é—®é¢˜è§£å†³æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨å¼€æ”¾å¼é—®ç­”ä»»åŠ¡ä¸­ä¹Ÿå±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.15671', 'title': 'Best of Both Worlds: Advantages of Hybrid Graph Sequence Models', 'url': 'https://huggingface.co/papers/2411.15671', 'abstract': 'Modern sequence models (e.g., Transformers, linear RNNs, etc.) emerged as dominant backbones of recent deep learning frameworks, mainly due to their efficiency, representational power, and/or ability to capture long-range dependencies. Adopting these sequence models for graph-structured data has recently gained popularity as the alternative to Message Passing Neural Networks (MPNNs). There is, however, a lack of a common foundation about what constitutes a good graph sequence model, and a mathematical description of the benefits and deficiencies in adopting different sequence models for learning on graphs. To this end, we first present Graph Sequence Model (GSM), a unifying framework for adopting sequence models for graphs, consisting of three main steps: (1) Tokenization, which translates the graph into a set of sequences; (2) Local Encoding, which encodes local neighborhoods around each node; and (3) Global Encoding, which employs a scalable sequence model to capture long-range dependencies within the sequences. This framework allows us to understand, evaluate, and compare the power of different sequence model backbones in graph tasks. Our theoretical evaluations of the representation power of Transformers and modern recurrent models through the lens of global and local graph tasks show that there are both negative and positive sides for both types of models. Building on this observation, we present GSM++, a fast hybrid model that uses the Hierarchical Affinity Clustering (HAC) algorithm to tokenize the graph into hierarchical sequences, and then employs a hybrid architecture of Transformer to encode these sequences. Our theoretical and experimental results support the design of GSM++, showing that GSM++ outperforms baselines in most benchmark evaluations.', 'score': 1, 'issue_id': 779, 'pub_date': '2024-11-23', 'pub_date_card': {'ru': '23 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 23', 'zh': '11æœˆ23æ—¥'}, 'hash': '540358181ae0274e', 'authors': ['Ali Behrouz', 'Ali Parviz', 'Mahdi Karami', 'Clayton Sanford', 'Bryan Perozzi', 'Vahab Mirrokni'], 'affiliations': ['Google Research', 'New Jersey Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2411.15671.jpg', 'data': {'categories': ['#optimization', '#math', '#architecture', '#benchmark', '#graphs'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Graph Sequence Model (GSM) Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. GSM ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾ĞºÑ€ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞºÑƒÑ€Ñ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GSM++, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Unifying Sequence Models for Enhanced Graph Learning', 'desc': 'This paper introduces the Graph Sequence Model (GSM), a framework that integrates sequence models with graph-structured data. It consists of three steps: tokenization of graphs into sequences, local encoding of node neighborhoods, and global encoding to capture long-range dependencies. The authors evaluate the strengths and weaknesses of different sequence models, particularly Transformers and recurrent models, in graph tasks. They also propose GSM++, a hybrid model that enhances performance by using hierarchical tokenization and a Transformer architecture, demonstrating superior results in benchmark tests.'}, 'zh': {'title': 'å›¾åºåˆ—æ¨¡å‹ï¼šç»“åˆåºåˆ—ä¸å›¾çš„åŠ›é‡', 'desc': 'ç°ä»£åºåˆ—æ¨¡å‹ï¼ˆå¦‚å˜æ¢å™¨å’Œçº¿æ€§é€’å½’ç¥ç»ç½‘ç»œï¼‰åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œå› å…¶é«˜æ•ˆæ€§ã€è¡¨ç¤ºèƒ½åŠ›å’Œæ•æ‰é•¿è·ç¦»ä¾èµ–çš„èƒ½åŠ›ã€‚å°†è¿™äº›åºåˆ—æ¨¡å‹åº”ç”¨äºå›¾ç»“æ„æ•°æ®çš„ç ”ç©¶é€æ¸å—åˆ°å…³æ³¨ï¼Œä½œä¸ºæ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬æ–‡æå‡ºäº†å›¾åºåˆ—æ¨¡å‹ï¼ˆGSMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼ŒåŒ…å«å›¾çš„æ ‡è®°åŒ–ã€å±€éƒ¨ç¼–ç å’Œå…¨å±€ç¼–ç ä¸‰ä¸ªä¸»è¦æ­¥éª¤ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†GSM++ï¼Œä¸€ç§å¿«é€Ÿæ··åˆæ¨¡å‹ï¼Œåˆ©ç”¨å±‚æ¬¡äº²å’Œèšç±»ç®—æ³•å¯¹å›¾è¿›è¡Œåˆ†å±‚åºåˆ—æ ‡è®°ï¼Œå¹¶é‡‡ç”¨å˜æ¢å™¨çš„æ··åˆæ¶æ„è¿›è¡Œç¼–ç ï¼Œå®éªŒç»“æœè¡¨æ˜GSM++åœ¨å¤§å¤šæ•°åŸºå‡†è¯„ä¼°ä¸­ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.14525', 'title': 'SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation', 'url': 'https://huggingface.co/papers/2411.14525', 'abstract': 'Computed Tomography (CT) is one of the most popular modalities for medical imaging. By far, CT images have contributed to the largest publicly available datasets for volumetric medical segmentation tasks, covering full-body anatomical structures. Large amounts of full-body CT images provide the opportunity to pre-train powerful models, e.g., STU-Net pre-trained in a supervised fashion, to segment numerous anatomical structures. However, it remains unclear in which conditions these pre-trained models can be transferred to various downstream medical segmentation tasks, particularly segmenting the other modalities and diverse targets. To address this problem, a large-scale benchmark for comprehensive evaluation is crucial for finding these conditions. Thus, we collected 87 public datasets varying in modality, target, and sample size to evaluate the transfer ability of full-body CT pre-trained models. We then employed a representative model, STU-Net with multiple model scales, to conduct transfer learning across modalities and targets. Our experimental results show that (1) there may be a bottleneck effect concerning the dataset size in fine-tuning, with more improvement on both small- and large-scale datasets than medium-size ones. (2) Models pre-trained on full-body CT demonstrate effective modality transfer, adapting well to other modalities such as MRI. (3) Pre-training on the full-body CT not only supports strong performance in structure detection but also shows efficacy in lesion detection, showcasing adaptability across target tasks. We hope that this large-scale open evaluation of transfer learning can direct future research in volumetric medical image segmentation.', 'score': 1, 'issue_id': 778, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 21', 'zh': '11æœˆ21æ—¥'}, 'hash': 'eb68ad946d69ba56', 'authors': ['Jin Ye', 'Ying Chen', 'Yanjun Li', 'Haoyu Wang', 'Zhongying Deng', 'Ziyan Huang', 'Yanzhou Su', 'Chenglong Ma', 'Yuanfeng Ji', 'Junjun He'], 'affiliations': ['East China Normal University', 'Shanghai AI Laboratory', 'Stanford University', 'University of Cambridge', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14525.jpg', 'data': {'categories': ['#benchmark', '#healthcare', '#open_source', '#training', '#transfer_learning', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ĞšĞ¢-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ĞšĞ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 87 Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğº Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, ĞœĞ Ğ¢) Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚ 'Ğ±ÑƒÑ‚Ñ‹Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ¾Ñ€Ğ»Ñ‹ÑˆĞºĞ°' Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹."}, 'en': {'title': 'Unlocking Transfer Learning in Medical Imaging with CT Datasets', 'desc': 'This paper investigates the transferability of models pre-trained on full-body CT images for various medical segmentation tasks. It highlights the effectiveness of the STU-Net model in adapting to different imaging modalities, such as MRI, and diverse anatomical targets. The study reveals that dataset size impacts fine-tuning performance, with both small and large datasets yielding better results than medium-sized ones. Overall, the findings emphasize the potential of using large-scale CT datasets to enhance model performance in medical image segmentation tasks.'}, 'zh': {'title': 'å…¨èº«CTé¢„è®­ç»ƒæ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ç ”ç©¶', 'desc': 'è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯åŒ»å­¦æˆåƒä¸­æœ€å¸¸ç”¨çš„æŠ€æœ¯ä¹‹ä¸€ã€‚æœ¬æ–‡æ¢è®¨äº†åœ¨ä¸åŒæ¡ä»¶ä¸‹ï¼ŒåŸºäºå…¨èº«CTé¢„è®­ç»ƒæ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶ä»–æˆåƒæ¨¡æ€å’Œå¤šæ ·åŒ–ç›®æ ‡çš„åˆ†å‰²ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬æ”¶é›†äº†87ä¸ªå…¬å…±æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼Œæ•°æ®é›†å¤§å°å¯¹å¾®è°ƒæœ‰ç“¶é¢ˆæ•ˆåº”ï¼Œä¸”å…¨èº«CTé¢„è®­ç»ƒæ¨¡å‹åœ¨è¿ç§»åˆ°å…¶ä»–æ¨¡æ€ï¼ˆå¦‚MRIï¼‰æ—¶è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¸Œæœ›ä¸ºæœªæ¥çš„ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ç ”ç©¶æä¾›æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.16085', 'title': 'Cautious Optimizers: Improving Training with One Line of Code', 'url': 'https://huggingface.co/papers/2411.16085', 'abstract': "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to 1.47times. Code is available at https://github.com/kyleliang919/C-Optim", 'score': 1, 'issue_id': 777, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 25', 'zh': '11æœˆ25æ—¥'}, 'hash': '48a2e1454fdde298', 'authors': ['Kaizhao Liang', 'Lizhang Chen', 'Bo Liu', 'Qiang Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.16085.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ°, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Cautious Optimizer. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ´Ğµ PyTorch, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ“Ğ°Ğ¼Ğ¸Ğ»ÑŒÑ‚Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ Adam Ğ¸ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ›ÑĞ¿ÑƒĞ½Ğ¾Ğ²Ğ°. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ†ĞµĞ»Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Llama Ğ¸ MAE Ğ´Ğ¾ 1.47 Ñ€Ğ°Ğ·.'}, 'en': {'title': 'Cautious Optimizer: Speeding Up Transformer Pretraining!', 'desc': 'This paper introduces a new optimizer called the Cautious Optimizer, which is a simple modification of existing momentum-based optimizers like AdamW and Lion. The modification preserves the Hamiltonian function of Adam, ensuring that the convergence properties remain intact according to Lyapunov stability analysis. The authors demonstrate that this new optimizer can significantly speed up the pretraining of models like Llama and MAE, achieving improvements of up to 1.47 times. Additionally, the research opens the door to a new family of optimizers, expanding the options available for machine learning practitioners.'}, 'zh': {'title': 'æå‡å˜æ¢å™¨é¢„è®­ç»ƒé€Ÿåº¦çš„æ–°ä¼˜åŒ–å™¨', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–å™¨ï¼Œç§°ä¸ºCautious Optimizerï¼Œæ—¨åœ¨æé«˜å˜æ¢å™¨é¢„è®­ç»ƒçš„é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚é€šè¿‡å¯¹ç°æœ‰çš„åŠ¨é‡ä¼˜åŒ–å™¨è¿›è¡Œç®€å•çš„ä¿®æ”¹ï¼Œæˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿™ç§ä¿®æ”¹ä¿æŒäº†Adamçš„å“ˆå¯†é¡¿å‡½æ•°ï¼Œå¹¶ä¸”åœ¨Lyapunovåˆ†æä¸‹ä¸ç ´åæ”¶æ•›æ€§ä¿è¯ã€‚æˆ‘ä»¬è¿˜æ­ç¤ºäº†ä¸€ç³»åˆ—æ–°çš„ä¼˜åŒ–å™¨ï¼Œå¹¶é€‰æ‹©äº†å…¶ä¸­æœ€ç®€å•çš„è¿›è¡Œå®è¯å®éªŒï¼Œç»“æœæ˜¾ç¤ºåœ¨Llamaå’ŒMAEé¢„è®­ç»ƒä¸­é€Ÿåº¦æå‡å¯è¾¾1.47å€ã€‚ç›¸å…³ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.12372', 'title': 'RedPajama: an Open Dataset for Training Large Language Models', 'url': 'https://huggingface.co/papers/2411.12372', 'abstract': "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.", 'score': 20, 'issue_id': 682, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '9393337102332466', 'authors': ['Maurice Weber', 'Daniel Fu', 'Quentin Anthony', 'Yonatan Oren', 'Shane Adams', 'Anton Alexandrov', 'Xiaozhong Lyu', 'Huu Nguyen', 'Xiaozhe Yao', 'Virginia Adams', 'Ben Athiwaratkun', 'Rahul Chalamala', 'Kezhen Chen', 'Max Ryabinin', 'Tri Dao', 'Percy Liang', 'Christopher RÃ©', 'Irina Rish', 'Ce Zhang'], 'affiliations': ['Caltech', 'ETH Zurich', 'EleutherAI', 'Mila, MontrÃ©al, Canada', 'Ohio State University', 'Ontocord.ai', 'Princeton University', 'Stanford University', 'Together AI', 'University of Chicago', 'UniversitÃ© de MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2411.12372.jpg', 'data': {'categories': ['#science', '#data', '#open_source', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'RedPajama: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğ¼ Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… RedPajama Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°: Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². RedPajama-V1 Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LLaMA, Ğ° RedPajama-V2 ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ĞµĞ±-Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Advancing Open-Source Language Models with RedPajama Datasets', 'desc': 'This paper discusses the importance of transparency and quality in the datasets used for training large language models. It identifies three main challenges: the need for clear data curation processes, access to high-quality data, and the availability of metadata for better dataset analysis. To tackle these issues, the authors introduce the RedPajama datasets, which include a comprehensive reproduction of the LLaMA training dataset and a large web-only dataset with quality signals. The findings highlight how these datasets can improve the development of open-source language models by providing high-quality data and insights into effective data curation practices.'}, 'zh': {'title': 'æ¨åŠ¨å¼€æ”¾è¯­è¨€æ¨¡å‹çš„é€æ˜ä¸é«˜æ•ˆ', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°æ®é›†æ„å»ºå’Œè¿‡æ»¤æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†é€æ˜åº¦ã€æ•°æ®è´¨é‡å’Œå…ƒæ•°æ®å¯ç”¨æ€§çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å‘å¸ƒäº†RedPajama-V1å’ŒRedPajama-V2æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œæä¾›é«˜è´¨é‡çš„å¼€æ”¾æ•°æ®ã€‚RedPajamaæ•°æ®é›†åŒ…å«è¶…è¿‡100ä¸‡äº¿ä¸ªæ ‡è®°ï¼Œæ¶µç›–å¤šä¸ªé¢†åŸŸï¼Œå¹¶æä¾›è´¨é‡ä¿¡å·ä»¥å¸®åŠ©æ•°æ®è¿‡æ»¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œåˆ©ç”¨ç½‘ç»œæ•°æ®çš„è´¨é‡ä¿¡å·å¯ä»¥æœ‰æ•ˆåœ°æ„å»ºé«˜è´¨é‡çš„æ•°æ®å­é›†ï¼Œæ¨åŠ¨é€æ˜ä¸”é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.11925', 'title': 'Continuous Speculative Decoding for Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2411.11925', 'abstract': 'Continuous-valued Autoregressive (AR) image generation models have demonstrated notable superiority over their discrete-token counterparts, showcasing considerable reconstruction quality and higher generation fidelity. However, the computational demands of the autoregressive framework result in significant inference overhead. While speculative decoding has proven effective in accelerating Large Language Models (LLMs), their adaptation to continuous-valued visual autoregressive models remains unexplored. This work generalizes the speculative decoding algorithm from discrete tokens to continuous space. By analyzing the intrinsic properties of output distribution, we establish a tailored acceptance criterion for the diffusion distributions prevalent in such models. To overcome the inconsistency that occurred in speculative decoding output distributions, we introduce denoising trajectory alignment and token pre-filling methods. Additionally, we identify the hard-to-sample distribution in the rejection phase. To mitigate this issue, we propose a meticulous acceptance-rejection sampling method with a proper upper bound, thereby circumventing complex integration. Experimental results show that our continuous speculative decoding achieves a remarkable 2.33times speed-up on off-the-shelf models while maintaining the output distribution. Codes will be available at https://github.com/MarkXCloud/CSpD', 'score': 12, 'issue_id': 674, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '17049106ecc06192', 'authors': ['Zili Wang', 'Robert Zhang', 'Kun Ding', 'Qi Yang', 'Fei Li', 'Shiming Xiang'], 'affiliations': ['China Tower Corporation Limited', 'Institute of Automation, Chinese Academy of Sciences, China', 'University of Chinese Academy of Sciences, China'], 'pdf_title_img': 'assets/pdf/title_img/2411.11925.jpg', 'data': {'categories': ['#optimization', '#inference', '#diffusion', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¿ĞµĞºÑƒĞ»ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ€Ğ°Ğ½ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞ²ÑˆĞ¸Ğ¹ÑÑ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğº Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ñƒ. ĞĞ½Ğ¸ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ 2.33-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Speeding Up Image Generation with Continuous Speculative Decoding', 'desc': 'This paper presents a new approach to improve the speed of continuous-valued autoregressive image generation models. It adapts speculative decoding, a technique previously used in large language models, to work with continuous data. The authors introduce methods to align denoising trajectories and pre-fill tokens to enhance the output quality during the decoding process. Their experiments demonstrate that this new method can significantly speed up the generation process by over two times while preserving the quality of the generated images.'}, 'zh': {'title': 'åŠ é€Ÿè¿ç»­å€¼è‡ªå›å½’å›¾åƒç”Ÿæˆçš„æ¨æµ‹è§£ç ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹è¿ç»­å€¼è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨æµ‹è§£ç ç®—æ³•ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆé€Ÿåº¦ã€‚é€šè¿‡åˆ†æè¾“å‡ºåˆ†å¸ƒçš„å†…åœ¨ç‰¹æ€§ï¼Œå»ºç«‹äº†é€‚åˆæ‰©æ•£åˆ†å¸ƒçš„æ¥å—æ ‡å‡†ã€‚ä¸ºäº†è§£å†³æ¨æµ‹è§£ç è¾“å‡ºåˆ†å¸ƒçš„ä¸ä¸€è‡´æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†å»å™ªè½¨è¿¹å¯¹é½å’Œä»¤ç‰Œé¢„å¡«å……æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒè¾“å‡ºåˆ†å¸ƒçš„åŒæ—¶ï¼Œå®ç°äº†2.33å€çš„é€Ÿåº¦æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.12044', 'title': 'ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements', 'url': 'https://huggingface.co/papers/2411.12044', 'abstract': "Recent advances in foundational Vision Language Models (VLMs) have reshaped the evaluation paradigm in computer vision tasks. These foundational models, especially CLIP, have accelerated research in open-vocabulary computer vision tasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the initial results are promising, the dense prediction capabilities of VLMs still require further improvement. In this study, we enhance the semantic segmentation performance of CLIP by introducing new modules and modifications: 1) architectural changes in the last layer of ViT and the incorporation of attention maps from the middle layers with the last layer, 2) Image Engineering: applying data augmentations to enrich input image representations, and 3) using Large Language Models (LLMs) to generate definitions and synonyms for each class name to leverage CLIP's open-vocabulary capabilities. Our training-free method, ITACLIP, outperforms current state-of-the-art approaches on segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and Pascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP.", 'score': 10, 'issue_id': 684, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': 'd123699ae0dacdaa', 'authors': ['M. Arda AydÄ±n', 'Efe Mert Ã‡Ä±rpar', 'Elvin Abdinli', 'Gozde Unal', 'Yusuf H. Sahin'], 'affiliations': ['Bilkent University', 'Istanbul Technical University', 'RWTH Aachen University', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2411.12044.jpg', 'data': {'categories': ['#open_source', '#optimization', '#training', '#architecture', '#benchmark', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ITACLIP: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¼ ÑĞ»Ğ¾Ğµ ViT Ğ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· ÑÑ€ĞµĞ´Ğ½Ğ¸Ñ… ÑĞ»Ğ¾ĞµĞ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ¾Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑĞ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ CLIP Ğ¿Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼.'}, 'en': {'title': 'Enhancing CLIP for Open-Vocabulary Semantic Segmentation', 'desc': "This paper discusses improvements to Vision Language Models (VLMs), particularly CLIP, for better performance in Open-Vocabulary Semantic Segmentation (OVSS). The authors propose enhancements through architectural modifications, including changes to the Vision Transformer (ViT) layers and the integration of attention maps. They also introduce data augmentation techniques to improve image representation and utilize Large Language Models (LLMs) to generate class definitions and synonyms, enhancing CLIP's open-vocabulary capabilities. The proposed method, ITACLIP, shows superior performance on various segmentation benchmarks compared to existing methods."}, 'zh': {'title': 'æå‡CLIPçš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½', 'desc': 'æœ¬ç ”ç©¶é’ˆå¯¹åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨è¿›è¡Œäº†æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯CLIPæ¨¡å‹åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥æ–°çš„æ¨¡å—å’Œä¿®æ”¹ï¼Œæå‡äº†CLIPçš„è¯­ä¹‰åˆ†å‰²æ€§èƒ½ï¼ŒåŒ…æ‹¬å¯¹ViTæœ€åä¸€å±‚çš„æ¶æ„è°ƒæ•´å’Œä¸­é—´å±‚æ³¨æ„åŠ›å›¾çš„ç»“åˆã€‚æˆ‘ä»¬è¿˜é€šè¿‡å›¾åƒå·¥ç¨‹æŠ€æœ¯å¢å¼ºè¾“å…¥å›¾åƒçš„è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆæ¯ä¸ªç±»åˆ«åç§°çš„å®šä¹‰å’ŒåŒä¹‰è¯ï¼Œä»¥å……åˆ†åˆ©ç”¨CLIPçš„å¼€æ”¾è¯æ±‡èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ— è®­ç»ƒæ–¹æ³•ITACLIPåœ¨COCO-Stuffã€COCO-Objectã€Pascal Contextå’ŒPascal VOCç­‰åˆ†å‰²åŸºå‡†ä¸Šè¶…è¶Šäº†å½“å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.12734', 'title': 'Soft Robotic Dynamic In-Hand Pen Spinning', 'url': 'https://huggingface.co/papers/2411.12734', 'abstract': "Dynamic in-hand manipulation remains a challenging task for soft robotic systems that have demonstrated advantages in safe compliant interactions but struggle with high-speed dynamic tasks. In this work, we present SWIFT, a system for learning dynamic tasks using a soft and compliant robotic hand. Unlike previous works that rely on simulation, quasi-static actions and precise object models, the proposed system learns to spin a pen through trial-and-error using only real-world data without requiring explicit prior knowledge of the pen's physical attributes. With self-labeled trials sampled from the real world, the system discovers the set of pen grasping and spinning primitive parameters that enables a soft hand to spin a pen robustly and reliably. After 130 sampled actions per object, SWIFT achieves 100% success rate across three pens with different weights and weight distributions, demonstrating the system's generalizability and robustness to changes in object properties. The results highlight the potential for soft robotic end-effectors to perform dynamic tasks including rapid in-hand manipulation. We also demonstrate that SWIFT generalizes to spinning items with different shapes and weights such as a brush and a screwdriver which we spin with 10/10 and 5/10 success rates respectively. Videos, data, and code are available at https://soft-spin.github.io.", 'score': 8, 'issue_id': 676, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'f75a2283a0a8a06f', 'authors': ['Yunchao Yao', 'Uksang Yoo', 'Jean Oh', 'Christopher G. Atkeson', 'Jeffrey Ichnowski'], 'affiliations': ['Robotics Institute at Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2411.12734.jpg', 'data': {'categories': ['#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœÑĞ³ĞºĞ°Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ€ÑƒĞºĞ° Ğ¾ÑĞ²Ğ°Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ SWIFT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ÑĞ³ĞºĞ¾Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€ÑƒĞºĞ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ñ€ÑƒÑ‡ĞºÑƒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ± Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°. ĞŸĞ¾ÑĞ»Ğµ 130 Ğ¿Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ SWIFT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 100% ÑƒÑĞ¿ĞµÑ…Ğ° Ğ¿Ñ€Ğ¸ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ñ‚Ñ€ĞµÑ… Ñ€ÑƒÑ‡ĞµĞº Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ²ĞµÑĞ¾Ğ¼ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑÑ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ°Ñ… Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ Ğ²ĞµÑĞ°.'}, 'en': {'title': 'SWIFT: Mastering Dynamic Manipulation with Soft Robotics', 'desc': "This paper introduces SWIFT, a novel system designed for dynamic in-hand manipulation using a soft robotic hand. Unlike traditional methods that depend on simulations or precise object models, SWIFT learns to perform tasks like spinning a pen through real-world trial-and-error. The system effectively identifies optimal grasping and spinning parameters without needing prior knowledge of the object's characteristics. SWIFT demonstrates impressive generalizability, achieving a 100% success rate in spinning various pens and also successfully manipulating other objects like brushes and screwdrivers."}, 'zh': {'title': 'è½¯æœºå™¨äººåŠ¨æ€æ“ä½œçš„æ–°çªç ´', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºSWIFTçš„ç³»ç»Ÿï¼Œç”¨äºå­¦ä¹ åŠ¨æ€ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨è½¯æœºå™¨äººæ‰‹ä¸­è¿›è¡Œå¿«é€Ÿçš„ç‰©ä½“æ“ä½œã€‚ä¸ä»¥å¾€ä¾èµ–äºæ¨¡æ‹Ÿå’Œç²¾ç¡®ç‰©ä½“æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒSWIFTé€šè¿‡çœŸå®ä¸–ç•Œçš„æ•°æ®è¿›è¡Œè¯•é”™å­¦ä¹ ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰ç‰©ä½“ç‰©ç†å±æ€§å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸåœ°æ—‹è½¬ç¬”ã€‚ç»è¿‡130æ¬¡é‡‡æ ·æ“ä½œï¼ŒSWIFTåœ¨ä¸‰ç§ä¸åŒé‡é‡å’Œåˆ†å¸ƒçš„ç¬”ä¸Šå®ç°äº†100%çš„æˆåŠŸç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨ç‰©ä½“å±æ€§å˜åŒ–ä¸‹çš„é€šç”¨æ€§å’Œé²æ£’æ€§ã€‚è¯¥ç³»ç»Ÿè¿˜èƒ½å¤Ÿæ¨å¹¿åˆ°å…¶ä»–å½¢çŠ¶å’Œé‡é‡çš„ç‰©ä½“ï¼Œå¦‚åˆ·å­å’Œèºä¸åˆ€ï¼Œåˆ†åˆ«å®ç°äº†10/10å’Œ5/10çš„æˆåŠŸç‡ï¼Œæ˜¾ç¤ºäº†è½¯æœºå™¨äººåœ¨åŠ¨æ€ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10818', 'title': 'FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations', 'url': 'https://huggingface.co/papers/2411.10818', 'abstract': 'Sketch animations offer a powerful medium for visual storytelling, from simple flip-book doodles to professional studio productions. While traditional animation requires teams of skilled artists to draw key frames and in-between frames, existing automation attempts still demand significant artistic effort through precise motion paths or keyframe specification. We present FlipSketch, a system that brings back the magic of flip-book animation -- just draw your idea and describe how you want it to move! Our approach harnesses motion priors from text-to-video diffusion models, adapting them to generate sketch animations through three key innovations: (i) fine-tuning for sketch-style frame generation, (ii) a reference frame mechanism that preserves visual integrity of input sketch through noise refinement, and (iii) a dual-attention composition that enables fluid motion without losing visual consistency. Unlike constrained vector animations, our raster frames support dynamic sketch transformations, capturing the expressive freedom of traditional animation. The result is an intuitive system that makes sketch animation as simple as doodling and describing, while maintaining the artistic essence of hand-drawn animation.', 'score': 6, 'issue_id': 687, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': '2aee0f98e4694b74', 'authors': ['Hmrishav Bandyopadhyay', 'Yi-Zhe Song'], 'affiliations': ['SketchX, CVSSP, University of Surrey, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2411.10818.jpg', 'data': {'categories': ['#story_generation', '#cv', '#video', '#multimodal', '#diffusion'], 'emoji': 'âœï¸', 'ru': {'title': 'ĞĞ¶Ğ¸Ğ²Ğ»ÑĞµĞ¼ ÑĞºĞµÑ‚Ñ‡Ğ¸ ÑĞ¸Ğ»Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ Ğ˜Ğ˜', 'desc': 'FlipSketch - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºĞµÑ‚Ñ‡ĞµĞ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¸ÑÑƒĞ½ĞºĞ° Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ÑĞºĞµÑ‚Ñ‡Ğ°Ñ…, Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ°Ğº Ğ¶Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾, ĞºĞ°Ğº Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ´ĞµÑ.'}, 'en': {'title': 'Doodle Your Dreams: Effortless Sketch Animation with FlipSketch!', 'desc': 'FlipSketch is a novel system that simplifies the process of creating sketch animations by allowing users to draw their ideas and describe desired movements. It leverages motion priors from text-to-video diffusion models, which are fine-tuned for generating sketch-style frames. The system incorporates a reference frame mechanism to ensure the visual integrity of the sketches and employs dual-attention composition for smooth motion while preserving consistency. This approach enables dynamic transformations in raster frames, making sketch animation accessible and intuitive, akin to traditional hand-drawn techniques.'}, 'zh': {'title': 'è®©è‰å›¾åŠ¨ç”»åˆ›ä½œå˜å¾—ç®€å•å¦‚æ¶‚é¸¦', 'desc': 'FlipSketch æ˜¯ä¸€ä¸ªæ–°ç³»ç»Ÿï¼Œæ—¨åœ¨ç®€åŒ–è‰å›¾åŠ¨ç”»çš„åˆ¶ä½œè¿‡ç¨‹ã€‚ç”¨æˆ·åªéœ€ç»˜åˆ¶è‰å›¾å¹¶æè¿°åŠ¨ç”»çš„è¿åŠ¨æ–¹å¼ï¼Œæ— éœ€å¤æ‚çš„å…³é”®å¸§è®¾ç½®ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿åŠ¨å…ˆéªŒï¼Œé€šè¿‡ä¸‰é¡¹åˆ›æ–°æ¥ç”Ÿæˆè‰å›¾åŠ¨ç”»ã€‚æœ€ç»ˆï¼ŒFlipSketch ä½¿å¾—è‰å›¾åŠ¨ç”»çš„åˆ›ä½œå˜å¾—åƒæ¶‚é¸¦ä¸€æ ·ç®€å•ï¼ŒåŒæ—¶ä¿ç•™äº†æ‰‹ç»˜åŠ¨ç”»çš„è‰ºæœ¯æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.12275', 'title': 'Building Trust: Foundations of Security, Safety and Transparency in AI', 'url': 'https://huggingface.co/papers/2411.12275', 'abstract': 'This paper explores the rapidly evolving ecosystem of publicly available AI models, and their potential implications on the security and safety landscape. As AI models become increasingly prevalent, understanding their potential risks and vulnerabilities is crucial. We review the current security and safety scenarios while highlighting challenges such as tracking issues, remediation, and the apparent absence of AI model lifecycle and ownership processes. Comprehensive strategies to enhance security and safety for both model developers and end-users are proposed. This paper aims to provide some of the foundational pieces for more standardized security, safety, and transparency in the development and operation of AI models and the larger open ecosystems and communities forming around them.', 'score': 6, 'issue_id': 682, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '628941b4647bf155', 'authors': ['Huzaifa Sidhpurwala', 'Garth Mollett', 'Emily Fox', 'Mark Bestavros', 'Huamin Chen'], 'affiliations': ['Red Hat'], 'pdf_title_img': 'assets/pdf/title_img/2411.12275.jpg', 'data': {'categories': ['#open_source', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜: Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ, ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ¦ĞµĞ»ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ - Ğ·Ğ°Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜.'}, 'en': {'title': 'Securing the Future of Open AI Models', 'desc': 'This paper examines the growing availability of AI models and their impact on security and safety. It emphasizes the importance of identifying risks and vulnerabilities associated with these models as they become more common. The authors discuss challenges like tracking model usage, addressing security issues, and the lack of clear ownership and lifecycle management for AI models. They propose strategies to improve security and safety for developers and users, aiming to establish standards for transparency in AI model development and operation.'}, 'zh': {'title': 'æå‡äººå·¥æ™ºèƒ½æ¨¡å‹çš„å®‰å…¨ä¸é€æ˜æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å…¬å¼€å¯ç”¨çš„äººå·¥æ™ºèƒ½æ¨¡å‹å¿«é€Ÿå‘å±•çš„ç”Ÿæ€ç³»ç»ŸåŠå…¶å¯¹å®‰å…¨å’Œå®‰å…¨æ€§å½±å“çš„æ½œåœ¨å«ä¹‰ã€‚éšç€äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ™®åŠï¼Œç†è§£å…¶æ½œåœ¨é£é™©å’Œè„†å¼±æ€§å˜å¾—è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å›é¡¾äº†å½“å‰çš„å®‰å…¨å’Œå®‰å…¨åœºæ™¯ï¼Œå¹¶å¼ºè°ƒäº†è·Ÿè¸ªé—®é¢˜ã€ä¿®å¤æªæ–½ä»¥åŠäººå·¥æ™ºèƒ½æ¨¡å‹ç”Ÿå‘½å‘¨æœŸå’Œæ‰€æœ‰æƒæµç¨‹ç¼ºå¤±ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†å¢å¼ºæ¨¡å‹å¼€å‘è€…å’Œæœ€ç»ˆç”¨æˆ·å®‰å…¨ä¸å®‰å…¨æ€§çš„ç»¼åˆç­–ç•¥ï¼Œæ—¨åœ¨ä¸ºäººå·¥æ™ºèƒ½æ¨¡å‹åŠå…¶å‘¨å›´å¼€æ”¾ç”Ÿæ€ç³»ç»Ÿå’Œç¤¾åŒºçš„å‘å±•æä¾›æ›´æ ‡å‡†åŒ–çš„å®‰å…¨æ€§ã€é€æ˜æ€§å’Œå®‰å…¨æ€§çš„åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10161', 'title': 'SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning', 'url': 'https://huggingface.co/papers/2411.10161', 'abstract': "Existing Image Quality Assessment (IQA) methods achieve remarkable success in analyzing quality for overall image, but few works explore quality analysis for Regions of Interest (ROIs). The quality analysis of ROIs can provide fine-grained guidance for image quality improvement and is crucial for scenarios focusing on region-level quality. This paper proposes a novel network, SEAGULL, which can SEe and Assess ROIs quality with GUidance from a Large vision-Language model. SEAGULL incorporates a vision-language model (VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and a meticulously designed Mask-based Feature Extractor (MFE) to extract global and local tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs. Moreover, this paper constructs two ROI-based IQA datasets, SEAGULL-100w and SEAGULL-3k, for training and evaluating ROI-based IQA. SEAGULL-100w comprises about 100w synthetic distortion images with 33 million ROIs for pre-training to improve the model's ability of regional quality perception, and SEAGULL-3k contains about 3k authentic distortion ROIs to enhance the model's ability to perceive real world distortions. After pre-training on SEAGULL-100w and fine-tuning on SEAGULL-3k, SEAGULL shows remarkable performance on fine-grained ROI quality assessment. Code and datasets are publicly available at the https://github.com/chencn2020/Seagull.", 'score': 4, 'issue_id': 684, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': 'f2475fbc98477ad9', 'authors': ['Zewen Chen', 'Juan Wang', 'Wen Wang', 'Sunhan Xu', 'Hang Xiong', 'Yun Zeng', 'Jian Guo', 'Shuxun Wang', 'Chunfeng Yuan', 'Bing Li', 'Weiming Hu'], 'affiliations': ['Beijing Jiaotong University', 'Beijing Union University', 'China University of Petroleum', 'PeopleAI Inc.', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'School of Information Science and Technology, ShanghaiTech University', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.10161.jpg', 'data': {'categories': ['#dataset', '#open_source', '#cv', '#synthetic'], 'emoji': 'ğŸ¦…', 'ru': {'title': 'SEAGULL: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SEAGULL - Ğ½Ğ¾Ğ²ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ° (ROI). SEAGULL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ°, Ğ¼Ğ°ÑĞºĞ¸ Ğ¸Ğ· Segment Anything Model Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ROI. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: SEAGULL-100w Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ SEAGULL-3k Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ SEAGULL Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ´Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ROI.'}, 'en': {'title': 'SEAGULL: Fine-Grained Quality Assessment for Image Regions', 'desc': 'This paper introduces SEAGULL, a novel network designed for assessing the quality of Regions of Interest (ROIs) in images, which is often overlooked by traditional Image Quality Assessment (IQA) methods. By leveraging a vision-language model and masks from the Segment Anything Model, SEAGULL effectively extracts both global and local features to provide detailed quality evaluations for specified ROIs. The authors also present two new datasets, SEAGULL-100w and SEAGULL-3k, which are used to train and evaluate the model, enhancing its ability to perceive both synthetic and real-world distortions. The results demonstrate that SEAGULL significantly improves fine-grained ROI quality assessment, making it a valuable tool for image quality enhancement.'}, 'zh': {'title': 'ç²¾ç»†åŒ–åŒºåŸŸè´¨é‡è¯„ä¼°çš„æ–°æ–¹æ³•', 'desc': 'ç°æœ‰çš„å›¾åƒè´¨é‡è¯„ä¼°æ–¹æ³•åœ¨æ•´ä½“å›¾åƒè´¨é‡åˆ†æä¸Šå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å¯¹æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„è´¨é‡åˆ†æç ”ç©¶è¾ƒå°‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç½‘ç»œSEAGULLï¼Œèƒ½å¤Ÿé€šè¿‡å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„æŒ‡å¯¼æ¥è¯„ä¼°ROIçš„è´¨é‡ã€‚SEAGULLç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€ç”±Segment Anything Modelï¼ˆSAMï¼‰ç”Ÿæˆçš„æ©ç æ¥æŒ‡å®šROIï¼Œä»¥åŠç²¾å¿ƒè®¾è®¡çš„åŸºäºæ©ç çš„ç‰¹å¾æå–å™¨ï¼ˆMFEï¼‰ï¼Œå®ç°äº†å¯¹ROIçš„ç²¾ç»†è´¨é‡è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ„å»ºäº†ä¸¤ä¸ªåŸºäºROIçš„è´¨é‡è¯„ä¼°æ•°æ®é›†SEAGULL-100wå’ŒSEAGULL-3kï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°ROIçš„è´¨é‡è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.12240', 'title': 'Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages', 'url': 'https://huggingface.co/papers/2411.12240', 'abstract': "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer's superior handling of Indic languages, GPT-4o's advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.", 'score': 2, 'issue_id': 675, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': 'aee934b73b340b71', 'authors': ['S. Tamang', 'D. J. Bora'], 'affiliations': ['Department of IT The Assam Kaziranga University Jorhat, India'], 'pdf_title_img': 'assets/pdf/title_img/2411.12240.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#data', '#dataset'], 'emoji': 'ğŸ‡®ğŸ‡³', 'ru': {'title': 'SUTRA: Ğ›Ğ¸Ğ´ĞµÑ€ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² 12 Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ´Ğ»Ñ 22 Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ² Ğ˜Ğ½Ğ´Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (NSL) Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ SUTRA Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ 14 ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ½Ğ´Ğ¸Ğ¹ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¸.'}, 'en': {'title': 'Optimizing Tokenization for Multilingual Mastery', 'desc': 'This paper evaluates the effectiveness of tokenizers used in Large Language Models (LLMs) for all 22 official languages of India, emphasizing the importance of tokenization in multilingual contexts. The study introduces the Normalized Sequence Length (NSL) as a metric to assess the efficiency of different tokenizers. Results indicate that the SUTRA tokenizer significantly outperforms other models, particularly in handling Indic languages. The findings highlight the need for specialized tokenization strategies to improve the performance of LLMs in diverse linguistic settings.'}, 'zh': {'title': 'ä¼˜åŒ–å¤šè¯­è¨€æ¨¡å‹çš„åˆ†è¯ç­–ç•¥', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºå˜æ¢å™¨æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åˆ†è¯æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å°åº¦å®˜æ–¹è¯­è¨€ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å¯¹12ç§LLMsä½¿ç”¨çš„åˆ†è¯å™¨è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œé‡ç‚¹æ¯”è¾ƒäº†å®ƒä»¬çš„åˆ†è¯æ•ˆç‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒSUTRAåˆ†è¯å™¨åœ¨14ç§è¯­è¨€ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å°åº¦è¯­è¨€æ–¹é¢ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†ä¸ºå¤šè¯­è¨€å’Œä»¥å°åº¦è¯­è¨€ä¸ºä¸­å¿ƒçš„æ¨¡å‹å¼€å‘é’ˆå¯¹æ€§åˆ†è¯ç­–ç•¥çš„é‡è¦æ€§ï¼Œä»¥æé«˜è¯­è¨€è¦†ç›–ç‡å’Œæ¨¡å‹æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.24024', 'title': 'AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents', 'url': 'https://huggingface.co/papers/2410.24024', 'abstract': 'Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.', 'score': 48, 'issue_id': 422, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '4ba16ad433c7511f', 'authors': ['Yifan Xu', 'Xiao Liu', 'Xueqiao Sun', 'Siyi Cheng', 'Hao Yu', 'Hanyu Lai', 'Shudan Zhang', 'Dan Zhang', 'Jie Tang', 'Yuxiao Dong'], 'affiliations': ['Tsinghua University', 'Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24024.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#training', '#dataset', '#open_source', '#games', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AndroidLab: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Android-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AndroidLab - ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Android. Ğ­Ñ‚Ğ° ÑÑ€ĞµĞ´Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ¾Ğ¼. AndroidLab Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM), Ñ‚Ğ°Ğº Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LMM) Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AndroidLab Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Android Instruction Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ ÑˆĞµÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ² Ğ¸Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Empowering Android Agents with AndroidLab: A New Benchmark Framework', 'desc': 'This paper introduces AndroidLab, a comprehensive framework designed for training and evaluating Android agents. It provides a structured environment that includes various modalities and a defined action space, allowing for systematic testing of both open-source and closed-source models. The framework supports large language models (LLMs) and multimodal models (LMMs), facilitating their development and benchmarking across 138 tasks in nine different applications. By utilizing AndroidLab, the authors significantly improved the success rates of LLMs and LMMs, demonstrating its effectiveness in enhancing the performance of Android agents.'}, 'zh': {'title': 'AndroidLabï¼šæå‡å®‰å“ä»£ç†çš„è®­ç»ƒä¸è¯„ä¼°', 'desc': 'è‡ªä¸»ä»£ç†åœ¨ä¸ç°å®ä¸–ç•Œäº’åŠ¨ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå°¤å…¶æ˜¯å®‰å“ä»£ç†ã€‚ç°æœ‰çš„å®‰å“ä»£ç†è®­ç»ƒå’Œè¯„ä¼°ç ”ç©¶ç¼ºä¹ç³»ç»Ÿæ€§ï¼Œæ— æ³•å…¨é¢æ¯”è¾ƒå¼€æºå’Œé—­æºæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†AndroidLabï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»ŸåŒ–çš„å®‰å“ä»£ç†æ¡†æ¶ï¼Œæ”¯æŒå¤šç§æ“ä½œç¯å¢ƒå’ŒåŠ¨ä½œç©ºé—´ã€‚é€šè¿‡AndroidLabï¼Œæˆ‘ä»¬å¼€å‘äº†å®‰å“æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶æ˜¾è‘—æé«˜äº†å¤§è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹çš„æˆåŠŸç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02355', 'title': '"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization', 'url': 'https://huggingface.co/papers/2411.02355', 'abstract': 'Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the "best" format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous "continuous batching" deployment of mid- and large-size models on high-end GPUs. Our results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements.', 'score': 44, 'issue_id': 431, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '8cc3629c4f7e76a1', 'authors': ['Eldar Kurtic', 'Alexandre Marques', 'Shubhra Pandit', 'Mark Kurtz', 'Dan Alistarh'], 'affiliations': ['Neural Magic', 'Institute of Science and Technology Austria'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02355.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#training', '#open_source'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LLM: Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ² ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (FP8, INT8, INT4) Ğ½Ğ° ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama-3.1, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ FP8 Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±ĞµĞ·Ğ¾ÑˆĞ¸Ğ±Ğ¾Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ° INT8 Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ´Ğ°ĞµÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 1-3%. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… GPU Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… LLM.'}, 'en': {'title': 'Optimizing Quantization for Efficient Large Language Model Deployment', 'desc': 'This paper investigates the accuracy and performance trade-offs of different quantization formats for large language models (LLMs), specifically focusing on FP8, INT8, and INT4 quantization methods. The authors conducted extensive evaluations on the Llama-3.1 model family, analyzing over 500,000 instances to determine the impact of quantization on model accuracy and text generation. Key findings indicate that FP8 quantization is lossless, while INT8 can achieve minimal accuracy loss with proper tuning, and INT4 quantization remains competitive. The study also provides practical guidelines for selecting the optimal quantization format based on deployment scenarios and GPU architectures.'}, 'zh': {'title': 'é‡åŒ–æ¨¡å‹çš„æœ€ä½³é€‰æ‹©ä¸æ€§èƒ½ä¼˜åŒ–', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡åŒ–å¯¹æ¨ç†åŠ é€Ÿçš„å½±å“ï¼Œç‰¹åˆ«å…³æ³¨ä¸åŒé‡åŒ–æ ¼å¼ï¼ˆå¦‚FP8ã€INT8ã€INT4ï¼‰åœ¨å‡†ç¡®æ€§å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬å¯¹Llama-3.1æ¨¡å‹ç³»åˆ—è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œè¯„ä¼°äº†é‡åŒ–æ¨¡å‹åœ¨å­¦æœ¯åŸºå‡†å’Œå®é™…ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼ŒFP8é‡åŒ–åœ¨æ‰€æœ‰æ¨¡å‹è§„æ¨¡ä¸Šéƒ½æ˜¯æ— æŸçš„ï¼Œè€Œç»è¿‡é€‚å½“è°ƒä¼˜çš„INT8é‡åŒ–ä»…æœ‰1-3%çš„å‡†ç¡®æ€§ä¸‹é™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€äº›é‡åŒ–æ”¹è¿›æ–¹æ³•ï¼Œå¸®åŠ©å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§æ¢å¤ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02337', 'title': 'WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning', 'url': 'https://huggingface.co/papers/2411.02337', 'abstract': "Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.", 'score': 36, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': 'a9af7d20e52c1f39', 'authors': ['Zehan Qi', 'Xiao Liu', 'Iat Long Iong', 'Hanyu Lai', 'Xueqiao Sun', 'Xinyue Yang', 'Jiadai Sun', 'Yu Yang', 'Shuntian Yao', 'Tianjie Zhang', 'Wei Xu', 'Jie Tang', 'Yuxiao Dong'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02337.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#rl', '#benchmark', '#optimization', '#training', '#open_source', '#agents', '#architecture'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'WebRL: ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ² Ğ²ĞµĞ±-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ WebRL - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. WebRL Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ WebRL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Llama-3.1 Ğ¸ GLM-4 Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²ĞµĞ±-Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ğ´Ñ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-4.'}, 'en': {'title': 'Empowering Open LLMs for Superior Web Performance with WebRL', 'desc': 'This paper presents WebRL, a novel framework that enhances open large language models (LLMs) for web-based tasks through reinforcement learning. It tackles challenges such as limited training tasks, sparse feedback, and policy drift by implementing a self-evolving curriculum that generates new tasks from failures, a robust outcome-supervised reward model, and adaptive learning strategies. The framework significantly improves the performance of open models like Llama-3.1 and GLM-4, achieving success rates that surpass proprietary models like GPT-4-Turbo. Overall, WebRL demonstrates a promising approach to making powerful web agents more accessible by leveraging open-source LLMs.'}, 'zh': {'title': 'WebRLï¼šå¼€æ”¾LLMçš„è‡ªæˆ‘è¿›åŒ–ç½‘ç»œä»£ç†è®­ç»ƒæ¡†æ¶', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç½‘ç»œä»»åŠ¡ä¸­å±•ç°äº†å‡ºè‰²çš„æ½œåŠ›ï¼Œä½†ç°æœ‰çš„LLMç½‘ç»œä»£ç†ä¾èµ–æ˜‚è´µçš„ä¸“æœ‰APIï¼Œè€Œå¼€æ”¾çš„LLMç¼ºä¹å†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†WebRLï¼Œä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„åœ¨çº¿è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨å¼€æ”¾çš„LLMè®­ç»ƒé«˜æ€§èƒ½çš„ç½‘ç»œä»£ç†ã€‚WebRLè§£å†³äº†æ„å»ºLLMç½‘ç»œä»£ç†çš„ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®­ç»ƒä»»åŠ¡ç¨€ç¼ºã€åé¦ˆä¿¡å·ç¨€ç–å’Œåœ¨çº¿å­¦ä¹ ä¸­çš„ç­–ç•¥åˆ†å¸ƒæ¼‚ç§»ã€‚é€šè¿‡WebRLï¼Œæˆ‘ä»¬å°†å¼€æ”¾çš„Llama-3.1å’ŒGLM-4æ¨¡å‹è½¬å˜ä¸ºé«˜æ•ˆçš„ç½‘ç»œä»£ç†ï¼Œæ˜¾è‘—æé«˜äº†å®ƒä»¬çš„æˆåŠŸç‡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ä¸“æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02385', 'title': 'How Far is Video Generation from World Model: A Physical Law Perspective', 'url': 'https://huggingface.co/papers/2411.02385', 'abstract': 'OpenAI\'s Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit "case-based" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora\'s broader success. See our project page at https://phyworld.github.io', 'score': 32, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '771395452deb397f', 'authors': ['Bingyi Kang', 'Yang Yue', 'Rui Lu', 'Zhijie Lin', 'Yang Zhao', 'Kaixin Wang', 'Gao Huang', 'Jiashi Feng'], 'affiliations': ['Bytedance Research', 'Tsinghua University', 'Technion'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02385.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#benchmark', '#cv', '#video', '#training', '#open_source', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ·ÑƒÑ‡Ğ°Ñ‚ÑŒ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¸Ğ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ 2D ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ğ½Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑĞºÑÑ‚Ñ€Ğ°Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ°Ğ±ÑÑ‚Ñ€Ğ°Ğ³Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°, Ğ° ÑĞºĞ¾Ñ€ĞµĞµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Video Generation: Beyond Scaling to Understand Physics', 'desc': 'This paper investigates the ability of video generation models to learn and predict physical laws from visual data without human input. The authors created a 2D simulation environment to generate videos based on classical mechanics, allowing for extensive testing of model performance. They found that while the models excelled in familiar scenarios, they struggled with new, unseen situations, indicating a reliance on specific examples rather than generalizing physical principles. The study concludes that simply increasing model size is not enough for these systems to grasp fundamental laws of physics, highlighting the need for improved generalization techniques.'}, 'zh': {'title': 'è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸ç‰©ç†æ³•åˆ™çš„æ¢ç´¢', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å­¦ä¹ ç‰©ç†æ³•åˆ™æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªäºŒç»´æ¨¡æ‹Ÿæµ‹è¯•å¹³å°ï¼Œç”¨äºç”Ÿæˆå—ç»å…¸åŠ›å­¦æ³•åˆ™æ”¯é…çš„è§†é¢‘æ•°æ®ã€‚é€šè¿‡å¯¹æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨å·²çŸ¥åˆ†å¸ƒå†…è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æœªçŸ¥åˆ†å¸ƒä¸­åˆ™å‡ºç°å¤±è´¥ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ¨¡å‹åœ¨æ¨å¹¿æ–°æ¡ˆä¾‹æ—¶ï¼Œä¼˜å…ˆè€ƒè™‘çš„å› ç´ ä¾æ¬¡ä¸ºé¢œè‰²ã€å¤§å°ã€é€Ÿåº¦å’Œå½¢çŠ¶ï¼Œè€Œä¸æ˜¯æŠ½è±¡å‡ºä¸€èˆ¬çš„ç‰©ç†è§„åˆ™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02336', 'title': 'MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D', 'url': 'https://huggingface.co/papers/2411.02336', 'abstract': 'Texturing is a crucial step in the 3D asset production workflow, which enhances the visual appeal and diversity of 3D assets. Despite recent advancements in Text-to-Texture (T2T) generation, existing methods often yield subpar results, primarily due to local discontinuities, inconsistencies across multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle these challenges, we propose a novel generation-refinement 3D texturing framework called MVPaint, which can generate high-resolution, seamless textures while emphasizing multi-view consistency. MVPaint mainly consists of three key modules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model, MVPaint first simultaneously generates multi-view images by employing an SMG model, which leads to coarse texturing results with unpainted parts due to missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete 3D texturing, we introduce the S3I method, specifically designed to effectively texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore, MVPaint employs a UVR module to improve the texture quality in the UV space, which first performs a UV-space Super-Resolution, followed by a Spatial-aware Seam-Smoothing algorithm for revising spatial texturing discontinuities caused by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the Objaverse T2T benchmark and the GSO T2T benchmark, based on selected high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset, respectively. Extensive experimental results demonstrate that MVPaint surpasses existing state-of-the-art methods. Notably, MVPaint could generate high-fidelity textures with minimal Janus issues and highly enhanced cross-view consistency.', 'score': 23, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '0f7de0fb0e5cdd86', 'authors': ['Wei Cheng', 'Juncheng Mu', 'Xianfang Zeng', 'Xin Chen', 'Anqi Pang', 'Chi Zhang', 'Zhibin Wang', 'Bin Fu', 'Gang Yu', 'Ziwei Liu', 'Liang Pan'], 'affiliations': ['Tencent PCG', 'Shanghai AI Laboratory', 'S-Lab, NTU', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02336.jpg', 'data': {'categories': ['#benchmark', '#cv', '#optimization', '#dataset', '#games', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'MVPaint: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MVPaint Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ´Ğ»Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹: ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ 3D-Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ UV-Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚ĞºĞ¸. MVPaint Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ², Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ UV-Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚ĞºĞ¸, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ.'}, 'en': {'title': 'MVPaint: Revolutionizing 3D Texturing with Multi-View Consistency', 'desc': 'This paper introduces MVPaint, a new framework for generating high-quality textures for 3D models. It addresses common issues in Text-to-Texture (T2T) generation, such as local discontinuities and inconsistencies across different views. MVPaint consists of three main components: Synchronized Multi-view Generation for initial texture creation, Spatial-aware 3D Inpainting for filling in unpainted areas, and UV Refinement for enhancing texture quality in UV space. The framework outperforms existing methods, providing seamless textures with improved visual consistency across multiple perspectives.'}, 'zh': {'title': 'MVPaintï¼šæå‡3Dçº¹ç†ç”Ÿæˆçš„ä¸€ä½“åŒ–è§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„3Dçº¹ç†ç”Ÿæˆä¸ä¼˜åŒ–æ¡†æ¶MVPaintï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–‡æœ¬åˆ°çº¹ç†ç”Ÿæˆæ–¹æ³•ä¸­çš„å±€éƒ¨ä¸è¿ç»­æ€§å’Œå¤šè§†å›¾ä¸€è‡´æ€§é—®é¢˜ã€‚MVPaintåŒ…å«ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šåŒæ­¥å¤šè§†å›¾ç”Ÿæˆï¼ˆSMGï¼‰ã€ç©ºé—´æ„ŸçŸ¥3Dä¿®è¡¥ï¼ˆS3Iï¼‰å’ŒUVä¼˜åŒ–ï¼ˆUVRï¼‰ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€æ— ç¼çš„çº¹ç†ã€‚é€šè¿‡SMGæ¨¡å—ï¼ŒMVPaintå¯ä»¥åŒæ—¶ç”Ÿæˆå¤šè§†å›¾å›¾åƒï¼Œè€ŒS3Iæ¨¡å—åˆ™ä¸“æ³¨äºå¡«è¡¥æœªè§‚å¯Ÿåˆ°çš„åŒºåŸŸã€‚æœ€åï¼ŒUVRæ¨¡å—é€šè¿‡è¶…åˆ†è¾¨ç‡å’Œç¼åˆå¹³æ»‘ç®—æ³•æ¥æå‡UVç©ºé—´ä¸­çš„çº¹ç†è´¨é‡ï¼Œå®éªŒç»“æœè¡¨æ˜MVPaintåœ¨çº¹ç†ç”Ÿæˆæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00860', 'title': 'Survey of Cultural Awareness in Language Models: Text and Beyond', 'url': 'https://huggingface.co/papers/2411.00860', 'abstract': 'Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology. In this paper, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking the definitions of culture from anthropology and psychology as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of Human-Computer Interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature.', 'score': 23, 'issue_id': 425, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 30', 'zh': '10æœˆ30æ—¥'}, 'hash': '8e4a6348db0215e3', 'authors': ['Siddhesh Pawar', 'Junyeong Park', 'Jiho Jin', 'Arnav Arora', 'Junho Myung', 'Srishti Yadav', 'Faiz Ghifari Haznitrama', 'Inhwa Song', 'Alice Oh', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen, Denmark', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'University of Copenhagen, Denmark', 'University of Copenhagen, Denmark', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'University of Copenhagen, Denmark'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00860.jpg', 'data': {'categories': ['#science', '#benchmark', '#multilingual', '#multimodal', '#ethics', '#training', '#dataset', '#survey', '#architecture', '#alignment'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞšÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ°Ñ Ğ¸Ğ½ĞºĞ»ÑĞ·Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ¿Ğ¾Ñ…Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ LLM Ğ¸ Ñ€Ğ¾Ğ»ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ¾Ğ¼ Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Enhancing Cultural Sensitivity in Language Models', 'desc': 'This paper explores the importance of cultural sensitivity in large language models (LLMs) used in applications like chatbots. It reviews existing research on how to make LLMs more inclusive by integrating insights from psychology and anthropology. The authors define cultural awareness in LLMs and discuss methods for creating diverse datasets and evaluating cultural inclusivity. Additionally, they highlight ethical considerations and suggest future research directions to enhance cultural alignment in LLMs.'}, 'zh': {'title': 'è®©å¤§å‹è¯­è¨€æ¨¡å‹æ›´å…·æ–‡åŒ–æ•æ„Ÿæ€§', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­èå…¥æ–‡åŒ–æ•æ„Ÿæ€§çš„é‡è¦æ€§ï¼Œä»¥ç¡®ä¿ç”¨æˆ·çš„åŒ…å®¹æ€§ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº†LLMsä¸­çš„æ–‡åŒ–æ„è¯†ï¼Œå¹¶åŸºäºäººç±»å­¦å’Œå¿ƒç†å­¦çš„å®šä¹‰è¿›è¡Œè®¨è®ºã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ†æäº†åˆ›å»ºè·¨æ–‡åŒ–æ•°æ®é›†çš„æ–¹æ³•ã€åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°æ–‡åŒ–åŒ…å®¹çš„ç­–ç•¥ï¼Œä»¥åŠç”¨äºè¯„ä¼°LLMsæ–‡åŒ–æ„è¯†çš„æ–¹æ³•è®ºã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†æ–‡åŒ–å¯¹é½çš„ä¼¦ç†å½±å“ã€äººæœºäº¤äº’åœ¨æ¨åŠ¨æ–‡åŒ–åŒ…å®¹ä¸­çš„ä½œç”¨ï¼Œä»¥åŠæœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02395', 'title': 'Training-free Regional Prompting for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02395', 'abstract': 'Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.', 'score': 23, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '2a0401bfd2cb136b', 'authors': ['Anthony Chen', 'Jianjin Xu', 'Wenzhao Zheng', 'Gaole Dai', 'Yida Wang', 'Renrui Zhang', 'Haofan Wang', 'Shanghang Zhang'], 'affiliations': ['Peking University', 'InstantX Team', 'Carnegie Mellon University', 'UC Berkeley', 'Li Auto Inc.', 'CUHK'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02395.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#open_source', '#cv'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ DiT', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Diffusion Transformer (DiT), Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹Ğ¹ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ FLUX.1. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ DiT Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Regional Prompting in Diffusion Transformers', 'desc': 'This paper discusses the limitations of current diffusion models in generating images from long and complex text prompts. It highlights the challenges these models face when dealing with multiple objects and their attributes in a spatial context. The authors propose a new method called regional prompting for the Diffusion Transformer (DiT) architecture, specifically for the FLUX.1 model, which enhances its ability to generate images based on detailed text descriptions without requiring additional training. This approach utilizes attention manipulation to achieve fine-grained compositional generation.'}, 'zh': {'title': 'åŒºåŸŸæç¤ºæå‡æ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆèƒ½åŠ›', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ç†è§£è¯­ä¹‰æ–¹é¢å¾—åˆ°äº†å¾ˆå¤§æå‡ã€‚å°½ç®¡å·²æœ‰è®¸å¤šåŒºåŸŸæç¤ºæ–¹æ³•è¢«æå‡ºï¼Œä½†ç°æœ‰æ¨¡å‹ä»æ— æ³•å®Œç¾å¤„ç†é•¿ä¸”å¤æ‚çš„æ–‡æœ¬æç¤ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›æ“ä½œçš„åŒºåŸŸæç¤ºæ–¹æ³•ï¼Œä¸“é—¨é’ˆå¯¹FLUX.1æ¶æ„è¿›è¡Œå®ç°ã€‚è¯¥æ–¹æ³•ä½¿å¾—æ‰©æ•£å˜æ¢å™¨èƒ½å¤Ÿåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå®ç°ç»†ç²’åº¦çš„ç»„åˆæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02265', 'title': 'Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent', 'url': 'https://huggingface.co/papers/2411.02265', 'abstract': "In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.   Codes: https://github.com/Tencent/Hunyuan-Large   Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large", 'score': 23, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '775afc5ff4d7fbdf', 'authors': ['Xingwu Sun', 'Yanfeng Chen', 'Yiqing Huang', 'Ruobing Xie', 'Jiaqi Zhu', 'Kai Zhang', 'Shuaipeng Li', 'Zhen Yang', 'Jonny Han', 'Xiaobo Shu', 'Jiahao Bu', 'Zhongzhi Chen', 'Xuemeng Huang', 'Fengzong Lian', 'Saiyong Yang', 'Jianfeng Yan', 'Yuyuan Zeng', 'Xiaoqin Ren', 'Chao Yu', 'Lulu Wu', 'Yue Mao', 'Jun Xia', 'Tao Yang', 'Suncong Zheng', 'Kan Wu', 'Dian Jiao', 'Jinbao Xue', 'Xipeng Zhang', 'Decheng Wu', 'Kai Liu', 'Dengpeng Wu', 'Guanghui Xu', 'Shaohua Chen', 'Shuang Chen', 'Xiao Feng', 'Yigeng Hong', 'Junqiang Zheng', 'Chengcheng Xu', 'Zongwei Li', 'Xiong Kuang', 'Jianglu Hu', 'Yiqi Chen', 'Yuchi Deng', 'Guiyang Li', 'Ao Liu', 'Chenchen Zhang', 'Shihui Hu', 'Zilong Zhao', 'Zifan Wu', 'Yao Ding', 'Weichao Wang', 'Han Liu', 'Roberts Wang', 'Hao Fei', 'Peijie Yu', 'Ze Zhao', 'Xun Cao', 'Hai Wang', 'Fusheng Xiang', 'Mengyuan Huang', 'Zhiyuan Xiong', 'Bin Hu', 'Xuebin Hou', 'Lei Jiang', 'Jianqiang Ma', 'Jiajia Wu', 'Yaping Deng', 'Yi Shen', 'Qian Wang', 'Weijie Liu', 'Jie Liu', 'Meng Chen', 'Liang Dong', 'Weiwen Jia', 'Hu Chen', 'Feifei Liu', 'Rui Yuan', 'Huilin Xu', 'Zhenxiang Yan', 'Tengfei Cao', 'Zhichao Hu', 'Xinhua Feng', 'Dong Du', 'Tinghao Yu', 'Yangyu Tao', 'Feng Zhang', 'Jianchen Zhu', 'Chengzhong Xu', 'Xirui Li', 'Chong Zha', 'Wen Ouyang', 'Yinben Xia', 'Xiang Li', 'Zekun He', 'Rongpeng Chen', 'Jiawei Song', 'Ruibin Chen', 'Fan Jiang', 'Chongqing Zhao', 'Bo Wang', 'Hao Gong', 'Rong Gan', 'Winston Hu', 'Zhanhui Kang', 'Yong Yang', 'Yuhong Liu', 'Di Wang', 'Jie Jiang'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02265.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#synthetic', '#benchmark', '#optimization', '#math', '#plp', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Hunyuan-Large: Ğ“Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ğ¹ ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Hunyuan-Large - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆÑƒÑ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰ÑƒÑ 389 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ°, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Hunyuan-Large Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking New Frontiers in AI with Hunyuan-Large', 'desc': 'Hunyuan-Large is a groundbreaking Transformer-based mixture of experts model with an impressive 389 billion parameters, designed to process up to 256K tokens. It demonstrates exceptional performance in various tasks such as language understanding, logical reasoning, and coding, surpassing the capabilities of LLama3.1-70B and rivaling the larger LLama3.1-405B. The model employs innovative techniques like large-scale synthetic data generation, a mixed expert routing strategy, and expert-specific learning rates to enhance its efficiency. Furthermore, the paper explores scaling laws and learning rate schedules, offering insights for future advancements in model optimization.'}, 'zh': {'title': 'Hunyuan-Largeï¼šè¶…å¤§è§„æ¨¡ä¸“å®¶æ··åˆæ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Hunyuan-Largeï¼Œè¿™æ˜¯ç›®å‰æœ€å¤§çš„å¼€æºåŸºäºTransformerçš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œæ‹¥æœ‰3890äº¿ä¸ªå‚æ•°å’Œ520äº¿ä¸ªæ¿€æ´»å‚æ•°ï¼Œèƒ½å¤Ÿå¤„ç†å¤šè¾¾256Kçš„tokenã€‚æˆ‘ä»¬å¯¹Hunyuan-Largeåœ¨è¯­è¨€ç†è§£ä¸ç”Ÿæˆã€é€»è¾‘æ¨ç†ã€æ•°å­¦é—®é¢˜è§£å†³ã€ç¼–ç ã€é•¿ä¸Šä¸‹æ–‡å’Œèšåˆä»»åŠ¡ç­‰å¤šä¸ªåŸºå‡†ä¸Šçš„ä¼˜è¶Šæ€§èƒ½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶ä¼˜äºLLama3.1-70Bï¼Œå¹¶ä¸”åœ¨ä¸æ›´å¤§æ¨¡å‹LLama3.1-405Bçš„æ¯”è¾ƒä¸­è¡¨ç°ç›¸å½“ã€‚Hunyuan-Largeçš„å…³é”®å®è·µåŒ…æ‹¬å¤§è§„æ¨¡åˆæˆæ•°æ®ã€æ··åˆä¸“å®¶è·¯ç”±ç­–ç•¥ã€é”®å€¼ç¼“å­˜å‹ç¼©æŠ€æœ¯å’Œä¸“å®¶ç‰¹å®šå­¦ä¹ ç‡ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†ä¸“å®¶æ··åˆæ¨¡å‹çš„æ‰©å±•è§„å¾‹å’Œå­¦ä¹ ç‡è°ƒåº¦ï¼Œä¸ºæœªæ¥æ¨¡å‹çš„å¼€å‘å’Œä¼˜åŒ–æä¾›äº†å®è´µçš„è§è§£å’ŒæŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02397', 'title': 'Adaptive Caching for Faster Video Generation with Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02397', 'abstract': 'Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that "not all videos are created equal": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.', 'score': 20, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': 'd7fa22d791789900', 'authors': ['Kumara Kahatapitiya', 'Haozhe Liu', 'Sen He', 'Ding Liu', 'Menglin Jia', 'Chenyang Zhang', 'Michael S. Ryoo', 'Tian Xie'], 'affiliations': ['Meta AI', 'Stony Brook University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02397.jpg', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ AdaCache Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² (DiT) Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. AdaCache Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ² ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ…ĞµĞ¼Ğ° Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ (MoReg) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ (Ğ´Ğ¾ 4.7 Ñ€Ğ°Ğ· Ğ½Ğ° Open-Sora 720p) Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Accelerating Video Generation with Adaptive Caching!', 'desc': 'This paper presents a method called Adaptive Caching (AdaCache) to improve the speed of video generation using Diffusion Transformers (DiTs). The authors argue that different videos have varying requirements for denoising steps, allowing for a more efficient computation process. By caching computations and creating a tailored caching schedule for each video, they enhance the balance between quality and speed. Additionally, they introduce a Motion Regularization (MoReg) technique to optimize resource allocation based on the motion content of the video, achieving significant speed improvements without compromising quality.'}, 'zh': {'title': 'åŠ é€Ÿè§†é¢‘ç”Ÿæˆï¼Œæå‡è´¨é‡ä¸æ•ˆç‡ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºè‡ªé€‚åº”ç¼“å­˜ï¼ˆAdaCacheï¼‰çš„æ–¹æ³•ï¼Œç”¨äºåŠ é€Ÿè§†é¢‘ç”Ÿæˆä¸­çš„æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTsï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¼“å­˜è®¡ç®—è¿‡ç¨‹ï¼Œé’ˆå¯¹æ¯ä¸ªè§†é¢‘ç”Ÿæˆåˆ¶å®šç¼“å­˜è®¡åˆ’ï¼Œä»è€Œä¼˜åŒ–è´¨é‡ä¸å»¶è¿Ÿçš„å¹³è¡¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†è¿åŠ¨æ­£åˆ™åŒ–ï¼ˆMoRegï¼‰æ–¹æ¡ˆï¼Œæ ¹æ®è§†é¢‘ä¸­çš„è¿åŠ¨å†…å®¹æ¥æ§åˆ¶è®¡ç®—åˆ†é…ã€‚æ•´ä½“è€Œè¨€ï¼Œè¿™äº›åˆ›æ–°æ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02319', 'title': 'GenXD: Generating Any 3D and 4D Scenes', 'url': 'https://huggingface.co/papers/2411.02319', 'abstract': "Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, GenXD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, GenXD employs masked latent conditions to support a variety of conditioning views. GenXD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating GenXD's effectiveness and versatility compared to previous methods in 3D and 4D generation.", 'score': 19, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '51444eddaf6bbbe7', 'authors': ['Yuyang Zhao', 'Chung-Ching Lin', 'Kevin Lin', 'Zhiwen Yan', 'Linjie Li', 'Zhengyuan Yang', 'Jianfeng Wang', 'Gim Hee Lee', 'Lijuan Wang'], 'affiliations': ['National University of Singapore', 'Microsoft Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02319.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#video', '#data', '#dataset', '#3d', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'GenXD: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ 3D Ğ¸ 4D ÑÑ†ĞµĞ½', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¸ 4D ÑÑ†ĞµĞ½ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ GenXD. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… 4D ÑÑ†ĞµĞ½ CamVid-30K, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾. GenXD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D Ğ²Ğ¸Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking 3D and 4D Generation with GenXD', 'desc': 'This paper addresses the challenges of generating 3D and 4D visual content, which are more complex than 2D generation due to limited data and model design issues. The authors introduce a new dataset called CamVid-30K, created by extracting camera poses and object movements from videos, which helps in training models effectively. They present a framework named GenXD that utilizes multiview-temporal modules to differentiate between camera and object movements, enabling the generation of realistic 3D and 4D scenes. Extensive evaluations show that GenXD outperforms existing methods, demonstrating its capability to create coherent videos and 3D representations from diverse inputs.'}, 'zh': {'title': 'çªç ´3Dä¸4Dç”Ÿæˆçš„ç“¶é¢ˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†3Då’Œ4Dè§†è§‰ç”Ÿæˆçš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹å¤§è§„æ¨¡4Dæ•°æ®çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ•°æ®æ•´ç†æµç¨‹ï¼Œä»è§†é¢‘ä¸­è·å–ç›¸æœºå§¿æ€å’Œç‰©ä½“è¿åŠ¨å¼ºåº¦ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå¤§å‹çš„4Dåœºæ™¯æ•°æ®é›†CamVid-30Kã€‚åŸºäºè¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬å¼€å‘äº†GenXDæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆä»»æ„3Dæˆ–4Dåœºæ™¯ã€‚é€šè¿‡å¤šè§†è§’æ—¶é—´æ¨¡å—ï¼ŒGenXDèƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ ç›¸æœºå’Œç‰©ä½“çš„è¿åŠ¨ï¼Œå¹¶ç”Ÿæˆä¸ç›¸æœºè½¨è¿¹ä¸€è‡´çš„è§†é¢‘å’Œå¯æå‡ä¸º3Dè¡¨ç¤ºçš„è§†å›¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02394', 'title': 'AutoVFX: Physically Realistic Video Editing from Natural Language Instructions', 'url': 'https://huggingface.co/papers/2411.02394', 'abstract': "Modern visual effects (VFX) software has made it possible for skilled artists to create imagery of virtually anything. However, the creation process remains laborious, complex, and largely inaccessible to everyday users. In this work, we present AutoVFX, a framework that automatically creates realistic and dynamic VFX videos from a single video and natural language instructions. By carefully integrating neural scene modeling, LLM-based code generation, and physical simulation, AutoVFX is able to provide physically-grounded, photorealistic editing effects that can be controlled directly using natural language instructions. We conduct extensive experiments to validate AutoVFX's efficacy across a diverse spectrum of videos and instructions. Quantitative and qualitative results suggest that AutoVFX outperforms all competing methods by a large margin in generative quality, instruction alignment, editing versatility, and physical plausibility.", 'score': 15, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '96ed53359fbc24a5', 'authors': ['Hao-Yu Hsu', 'Zhi-Hao Lin', 'Albert Zhai', 'Hongchi Xia', 'Shenlong Wang'], 'affiliations': ['University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02394.jpg', 'data': {'categories': ['#reasoning', '#cv', '#video', '#multimodal', '#games', '#architecture', '#alignment'], 'emoji': 'ğŸ¬', 'ru': {'title': 'AutoVFX: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ² ÑĞ¸Ğ»Ğ¾Ğ¹ Ğ¼Ñ‹ÑĞ»Ğ¸', 'desc': 'AutoVFX - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ†ĞµĞ½, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. AutoVFX Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Transforming VFX Creation with Natural Language and AI', 'desc': 'AutoVFX is a novel framework designed to simplify the creation of visual effects (VFX) by allowing users to generate realistic videos from a single input video and natural language commands. It combines advanced techniques such as neural scene modeling, large language model (LLM)-based code generation, and physical simulation to produce high-quality, dynamic effects. The framework is evaluated through extensive experiments, demonstrating superior performance in generative quality, alignment with user instructions, versatility in editing, and adherence to physical realism. Overall, AutoVFX makes sophisticated VFX creation accessible to a broader audience, reducing the complexity of traditional methods.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–è§†è§‰ç‰¹æ•ˆï¼Œè½»æ¾åˆ›ä½œçœŸå®å½±åƒ', 'desc': 'ç°ä»£è§†è§‰ç‰¹æ•ˆè½¯ä»¶ä½¿å¾—ç†Ÿç»ƒçš„è‰ºæœ¯å®¶èƒ½å¤Ÿåˆ›é€ å‡ ä¹ä»»ä½•å›¾åƒï¼Œä½†åˆ›ä½œè¿‡ç¨‹ä»ç„¶ç¹çä¸”å¤æ‚ï¼Œæ™®é€šç”¨æˆ·éš¾ä»¥æ¥è§¦ã€‚æœ¬æ–‡æå‡ºäº†AutoVFXï¼Œä¸€ä¸ªæ¡†æ¶å¯ä»¥æ ¹æ®å•ä¸ªè§†é¢‘å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤è‡ªåŠ¨åˆ›å»ºé€¼çœŸä¸”åŠ¨æ€çš„è§†è§‰ç‰¹æ•ˆè§†é¢‘ã€‚é€šè¿‡ç²¾å¿ƒæ•´åˆç¥ç»åœºæ™¯å»ºæ¨¡ã€åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä»£ç ç”Ÿæˆå’Œç‰©ç†ä»¿çœŸï¼ŒAutoVFXèƒ½å¤Ÿæä¾›ç‰©ç†åŸºç¡€çš„ã€ç…§ç‰‡çº§çœŸå®æ„Ÿçš„ç¼–è¾‘æ•ˆæœï¼Œå¹¶å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç›´æ¥æ§åˆ¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAutoVFXåœ¨ç”Ÿæˆè´¨é‡ã€æŒ‡ä»¤å¯¹é½ã€ç¼–è¾‘å¤šæ ·æ€§å’Œç‰©ç†åˆç†æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºæ‰€æœ‰ç«äº‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00836', 'title': 'DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models', 'url': 'https://huggingface.co/papers/2411.00836', 'abstract': "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.", 'score': 15, 'issue_id': 420, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': 'e73ae00a5621a2b9', 'authors': ['Chengke Zou', 'Xingang Guo', 'Rui Yang', 'Junyu Zhang', 'Bin Hu', 'Huan Zhang'], 'affiliations': ['University of Illinois at Urbana-Champaign', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00836.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#benchmark', '#cv', '#graphs', '#optimization', '#math', '#dataset', '#architecture'], 'emoji': 'ğŸ§®', 'ru': {'title': 'DynaMath: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DynaMath - Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Vision-Language Models (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº GPT-4V, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑˆĞ°Ğ³Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğº Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸. DynaMath Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 501 Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Python-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 14 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ…ÑƒĞ´ÑˆĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ¸Ğ¶Ğµ ÑÑ€ĞµĞ´Ğ½ĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing VLMs: Unveiling Mathematical Reasoning Limitations with DynaMath', 'desc': "This paper explores the limitations of Vision-Language Models (VLMs) in performing mathematical reasoning tasks that involve visual elements. It highlights that while humans can adapt their problem-solving strategies to slight changes in problems, current state-of-the-art VLMs like GPT-4o struggle with this adaptability. To address this issue, the authors introduce DynaMath, a dynamic benchmark that generates a wide variety of mathematical questions to rigorously test VLMs' reasoning capabilities. The findings reveal that VLMs exhibit significantly lower accuracy in worst-case scenarios compared to average cases, underscoring the importance of evaluating their robustness in mathematical reasoning."}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨è§†è§‰ä¸Šä¸‹æ–‡çš„å½±å“ä¸‹ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡äººç±»èƒ½å¤Ÿçµæ´»åº”å¯¹ç›¸ä¼¼é—®é¢˜çš„å˜åŒ–ï¼Œå½“å‰çš„æœ€å…ˆè¿›æ¨¡å‹å¦‚GPT-4oåœ¨é¢å¯¹è¿™äº›å˜åŒ–æ—¶å´è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºå‡ºå…¶æ•°å­¦æ¨ç†èƒ½åŠ›çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DynaMathï¼Œä¸€ä¸ªåŠ¨æ€è§†è§‰æ•°å­¦åŸºå‡†ï¼Œæ—¨åœ¨æ·±å…¥è¯„ä¼°VLMsçš„æ¨ç†ç¨³å¥æ€§ã€‚é€šè¿‡å¯¹501ä¸ªé«˜è´¨é‡ç§å­é—®é¢˜çš„è‡ªåŠ¨ç”Ÿæˆï¼ŒDynaMathèƒ½å¤Ÿè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè¾“å…¥æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç»“æœæ˜¾ç¤ºæ¨¡å‹åœ¨æœ€åæƒ…å†µä¸‹çš„å‡†ç¡®ç‡æ˜¾è‘—ä½äºå¹³å‡æƒ…å†µã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.01747', 'title': 'DynaSaur: Large Language Agents Beyond Predefined Actions', 'url': 'https://huggingface.co/papers/2411.01747', 'abstract': 'Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in https://github.com/adobe-research/dynasaur{https://github.com/adobe-research/dynasaur}.', 'score': 13, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '772b11b15cab80a0', 'authors': ['Dang Nguyen', 'Viet Dac Lai', 'Seunghyun Yoon', 'Ryan A. Rossi', 'Handong Zhao', 'Ruiyi Zhang', 'Puneet Mathur', 'Nedim Lipka', 'Yu Wang', 'Trung Bui', 'Franck Dernoncourt', 'Tianyi Zhou'], 'affiliations': ['University of Maryland', 'Adobe Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01747.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#agi', '#plp', '#open_source', '#agents', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ LLM: ÑˆĞ°Ğ³ Ğº Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾ĞºÑ€ÑƒĞ¶Ğ°ÑÑ‰ĞµĞ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ GAIA. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ³Ğ´Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹.'}, 'en': {'title': 'Empowering LLM Agents with Dynamic Action Generation', 'desc': 'This paper introduces a new framework for large language model (LLM) agents that allows them to dynamically create and execute actions in real-time, rather than relying on a fixed set of predefined actions. This approach enhances the planning and acting capabilities of LLM agents, making them more adaptable to complex and unpredictable environments. By generating programs in a general-purpose programming language, the agents can respond to unforeseen situations and accumulate useful actions for future tasks. The experiments conducted on the GAIA benchmark show that this framework significantly outperforms traditional methods, providing greater flexibility and effectiveness in real-world applications.'}, 'zh': {'title': 'åŠ¨æ€åˆ›å»ºä¸ç»„åˆåŠ¨ä½œçš„LLMä»£ç†æ¡†æ¶', 'desc': 'ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ç³»ç»Ÿé€šå¸¸åœ¨æ¯ä¸€æ­¥ä»å›ºå®šçš„é¢„å®šä¹‰åŠ¨ä½œé›†ä¸­é€‰æ‹©åŠ¨ä½œã€‚è¿™ç§æ–¹æ³•åœ¨å°é—­çš„ã€ç‹­çª„çš„ç¯å¢ƒä¸­æœ‰æ•ˆï¼Œä½†åœ¨ç°å®ä¸–ç•Œä¸­åº”ç”¨æ—¶é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šä¸€æ˜¯ä»å›ºå®šåŠ¨ä½œé›†ä¸­é€‰æ‹©é™åˆ¶äº†LLMä»£ç†çš„è§„åˆ’å’Œæ‰§è¡Œèƒ½åŠ›ï¼ŒäºŒæ˜¯éœ€è¦å¤§é‡äººåŠ›æ¥åˆ—ä¸¾å’Œå®ç°æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Œè¿™åœ¨å¤æ‚ç¯å¢ƒä¸­å˜å¾—ä¸åˆ‡å®é™…ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§LLMä»£ç†æ¡†æ¶ï¼Œå…è®¸åœ¨çº¿åŠ¨æ€åˆ›å»ºå’Œç»„åˆåŠ¨ä½œï¼Œä»£ç†é€šè¿‡ç”Ÿæˆå’Œæ‰§è¡Œé€šç”¨ç¼–ç¨‹è¯­è¨€ç¼–å†™çš„ç¨‹åºä¸ç¯å¢ƒäº’åŠ¨ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ï¼Œå¹¶åœ¨GAIAåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä»¥å¾€æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02327', 'title': 'PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance', 'url': 'https://huggingface.co/papers/2411.02327', 'abstract': "The past year has witnessed the significant advancement of video-based large language models. However, the challenge of developing a unified model for both short and long video understanding remains unresolved. Most existing video LLMs cannot handle hour-long videos, while methods custom for long videos tend to be ineffective for shorter videos and images. In this paper, we identify the key issue as the redundant content in videos. To address this, we propose a novel pooling strategy that simultaneously achieves token compression and instruction-aware visual feature aggregation. Our model is termed Prompt-guided Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three core components: the CLIP-based visual-prompt alignment that extracts visual information relevant to the user's instructions, the prompt-guided pooling that compresses the visual sequence to arbitrary scales using convolution-style pooling, and the clip context extension designed for lengthy prompt common in visual dialogue. Moreover, our codebase also integrates the most advanced video Direct Preference Optimization (DPO) and visual interleave training. Extensive experiments have validated the performance of our model. With superior throughput and only 1024 visual context, PPLLaVA achieves better results on image benchmarks as a video LLM, while achieving state-of-the-art performance across various video benchmarks, excelling in tasks ranging from caption generation to multiple-choice questions, and handling video lengths from seconds to hours. Codes have been available at https://github.com/farewellthree/PPLLaVA.", 'score': 11, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': 'ed9d356ccf75d780', 'authors': ['Ruyang Liu', 'Haoran Tang', 'Haibo Liu', 'Yixiao Ge', 'Ying Shan', 'Chen Li', 'Jiankun Yang'], 'affiliations': ['Peking University', 'Applied Research Center (ARC), Tencent PCG', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02327.jpg', 'data': {'categories': ['#long_context', '#rlhf', '#benchmark', '#video', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PPLLaVA Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºĞ°Ğº ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CLIP, Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ². PPLLaVA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unified Video Understanding with PPLLaVA', 'desc': 'This paper presents a new model called Prompt-guided Pooling LLaVA (PPLLaVA) that improves video understanding for both short and long videos. The authors identify redundant content in videos as a key challenge and propose a pooling strategy that compresses tokens while aggregating visual features based on user instructions. PPLLaVA includes components for visual-prompt alignment, convolution-style pooling, and context extension for lengthy prompts. The model demonstrates superior performance across various benchmarks, effectively handling video lengths from seconds to hours.'}, 'zh': {'title': 'è§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šPPLLaVAæ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼Œç§°ä¸ºPPLLaVAï¼Œæ—¨åœ¨è§£å†³çŸ­è§†é¢‘å’Œé•¿è§†é¢‘ç†è§£çš„ç»Ÿä¸€æ¨¡å‹é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°è§†é¢‘ä¸­çš„å†—ä½™å†…å®¹æ˜¯ä¸»è¦æŒ‘æˆ˜ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ–°çš„æ± åŒ–ç­–ç•¥ï¼Œå®ç°äº†ä»¤ç‰Œå‹ç¼©å’ŒæŒ‡ä»¤æ„ŸçŸ¥çš„è§†è§‰ç‰¹å¾èšåˆã€‚PPLLaVAåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œåˆ†åˆ«æ˜¯åŸºäºCLIPçš„è§†è§‰æç¤ºå¯¹é½ã€æç¤ºå¼•å¯¼æ± åŒ–å’Œå‰ªè¾‘ä¸Šä¸‹æ–‡æ‰©å±•ã€‚ç»è¿‡å¹¿æ³›å®éªŒéªŒè¯ï¼ŒPPLLaVAåœ¨å¤„ç†ä»å‡ ç§’åˆ°å‡ å°æ—¶çš„è§†é¢‘æ—¶ï¼Œè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è§†é¢‘åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02335', 'title': 'Sparsing Law: Towards Large Language Models with Greater Activation Sparsity', 'url': 'https://huggingface.co/papers/2411.02335', 'abstract': 'Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-p% sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1-sparsity ratio) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable.', 'score': 11, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '3c9152f4d267fc7b', 'authors': ['Yuqi Luo', 'Chenyang Song', 'Xu Han', 'Yingfa Chen', 'Chaojun Xiao', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02335.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² LLM: ĞºĞ»ÑÑ‡ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ PPL-p% Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑˆĞ¸Ñ€Ğ¸Ğ½Ñ‹ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹, Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ°Ğ±Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking Efficiency: The Power of Activation Sparsity in LLMs', 'desc': 'This paper investigates activation sparsity in decoder-only Transformer-based large language models (LLMs), focusing on how weakly-contributed activation outputs can be reduced for efficiency. The authors introduce a new metric called PPL-p% sparsity to quantitatively measure activation sparsity across different activation functions. Their experiments reveal that while ReLU and SiLU functions perform similarly, they exhibit opposite trends in training-time sparsity, with ReLU being more efficient. Additionally, the study finds that activation sparsity is largely unaffected by the parameter scale, suggesting that deeper architectures may enhance efficiency without requiring more parameters.'}, 'zh': {'title': 'æ¿€æ´»ç¨€ç–æ€§ï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ•ˆç‡çš„å…³é”®', 'desc': 'æ¿€æ´»ç¨€ç–æ€§æŒ‡çš„æ˜¯åœ¨æ¿€æ´»è¾“å‡ºä¸­å­˜åœ¨å¤§é‡è´¡çŒ®è¾ƒå¼±çš„å…ƒç´ ï¼Œè¿™äº›å…ƒç´ å¯ä»¥è¢«æ¶ˆé™¤ï¼Œä»è€Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®¸å¤šé‡è¦åº”ç”¨æœ‰ç›Šã€‚æœ¬æ–‡å¯¹åŸºäºè§£ç å™¨çš„Transformer LLMä¸­çš„æ¿€æ´»ç¨€ç–æ€§è¿›è¡Œäº†å…¨é¢çš„å®šé‡ç ”ç©¶ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¿€æ´»ç¨€ç–æ€§åº¦é‡æ ‡å‡†PPL-p%ç¨€ç–æ€§ï¼Œé€‚ç”¨äºä»»ä½•æ¿€æ´»å‡½æ•°ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒçš„æ¿€æ´»å‡½æ•°åœ¨æ€§èƒ½ä¸Šç›¸ä¼¼ï¼Œä½†åœ¨è®­ç»ƒæ—¶é—´çš„ç¨€ç–æ€§è¶‹åŠ¿ä¸Šå´ç›¸åï¼ŒReLUæ¿€æ´»å‡½æ•°åœ¨åˆ©ç”¨æ›´å¤šè®­ç»ƒæ•°æ®æ–¹é¢æ›´ä¸ºé«˜æ•ˆã€‚æœ€åï¼Œç ”ç©¶è¡¨æ˜ï¼Œæ¿€æ´»ç¨€ç–æ€§çš„æé™å€¼ä¸å‚æ•°è§„æ¨¡çš„å˜åŒ–å…³ç³»ä¸å¤§ï¼Œè¿™ä¸ºæé«˜LLMsçš„æ•ˆç‡å’Œå¯è§£é‡Šæ€§æä¾›äº†é‡è¦çš„å¯ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.01798', 'title': 'SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF', 'url': 'https://huggingface.co/papers/2411.01798', 'abstract': "In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.", 'score': 8, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': 'e5a60efe079c6136', 'authors': ['Atoosa Chegini', 'Hamid Kazemi', 'Iman Mirzadeh', 'Dong Yin', 'Maxwell Horton', 'Moin Nabi', 'Mehrdad Farajtabar', 'Keivan Alizadeh'], 'affiliations': ['University of Maryland', 'Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01798.jpg', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#benchmark', '#optimization', '#training', '#open_source', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SALSA: Ğ“Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ SALSA Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° (RLHF). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° PPO, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ, SALSA ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ¸Ğ±ĞºÑƒÑ Ğ¾Ğ¿Ğ¾Ñ€Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿ÑƒÑ‚ĞµĞ¼ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ²ÑƒÑ… Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºÑƒÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ½Ğµ Ğ¶ĞµÑ€Ñ‚Ğ²ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SALSA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ PPO Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'SALSA: Enhancing LLM Alignment through Flexible Exploration', 'desc': "This paper introduces SALSA, a new method for improving the alignment of Large Language Models (LLMs) with human preferences using Reinforcement Learning from Human Feedback (RLHF). Traditional RLHF methods use a fixed reference policy, which can restrict the model's ability to explore and find better solutions due to the penalty imposed by Kullback-Leibler (KL) divergence. SALSA addresses this issue by averaging weights from two independently fine-tuned models, creating a more flexible reference that allows for greater exploration of the reward landscape. The results show that SALSA enhances model performance, robustness, and generalization across various benchmarks compared to traditional methods like Proximal Policy Optimization (PPO)."}, 'zh': {'title': 'SALSAï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹å¯¹é½ä¸æ¢ç´¢çš„åˆ›æ–°æ–¹æ³•', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€å‘ä¸­ï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å¯¹äºä½¿æ¨¡å‹ä¸äººç±»ä»·å€¼è§‚å’Œåå¥½ä¿æŒä¸€è‡´è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„RLHFä¾èµ–äºå½“å‰ç­–ç•¥ä¸å†»ç»“åˆå§‹ç­–ç•¥ä¹‹é—´çš„Kullback-Leiblerï¼ˆKLï¼‰æ•£åº¦ä½œä¸ºå‚è€ƒï¼Œè¿™åœ¨ç­–ç•¥ä¼˜åŒ–ç®—æ³•ä¸­ä½œä¸ºæƒ©ç½šé¡¹ä½¿ç”¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•SALSAï¼ˆåŸºäºæ¨¡å‹é›†åˆçš„å¯¹é½å­¦ä¹ ï¼‰ï¼Œé€šè¿‡å¯¹ä¸¤ä¸ªç‹¬ç«‹çš„ç›‘ç£å¾®è°ƒæ¨¡å‹è¿›è¡Œæƒé‡ç©ºé—´å¹³å‡ï¼Œåˆ›å»ºä¸€ä¸ªæ›´çµæ´»çš„å‚è€ƒæ¨¡å‹ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚SALSAé€šè¿‡æ›´å¥½çš„æ¢ç´¢èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„é²æ£’æ€§å’Œæ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00785', 'title': 'IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI', 'url': 'https://huggingface.co/papers/2411.00785', 'abstract': 'We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can "migrate" the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.', 'score': 8, 'issue_id': 434, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 17', 'zh': '10æœˆ17æ—¥'}, 'hash': 'a7e2824d0e474c0b', 'authors': ['Xiaoyu Chen', 'Junliang Guo', 'Tianyu He', 'Chuheng Zhang', 'Pushi Zhang', 'Derek Cathera Yang', 'Li Zhao', 'Jiang Bian'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00785.jpg', 'data': {'categories': ['#science', '#cv', '#graphs', '#video', '#multimodal', '#training', '#dataset', '#robotics', '#transfer_learning', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ IGOR - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². IGOR ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… ĞºĞ°Ğº Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸. IGOR Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ´Ğ°Ğ¶Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Unified Action Space for Human-Robot Interaction', 'desc': 'The paper presents Image-GOal Representations (IGOR), a framework designed to create a unified action space that is semantically consistent for both humans and robots. By compressing the visual changes between an initial image and its goal state into latent actions, IGOR facilitates knowledge transfer across diverse datasets of human and robot activities. This approach allows for the generation of latent action labels from large-scale video data, enabling the training of foundational policy and world models for various tasks. Ultimately, IGOR enhances the ability to transfer movement knowledge between humans and robots, aligning actions with natural language for improved robot control.'}, 'zh': {'title': 'ç»Ÿä¸€åŠ¨ä½œç©ºé—´ï¼Œè¿æ¥äººç±»ä¸æœºå™¨äºº', 'desc': 'æœ¬æ–‡ä»‹ç»äº†å›¾åƒç›®æ ‡è¡¨ç¤ºï¼ˆIGORï¼‰ï¼Œæ—¨åœ¨å­¦ä¹ ä¸€ä¸ªç»Ÿä¸€ä¸”è¯­ä¹‰ä¸€è‡´çš„åŠ¨ä½œç©ºé—´ï¼Œé€‚ç”¨äºäººç±»å’Œå„ç§æœºå™¨äººã€‚é€šè¿‡è¿™ä¸ªç»Ÿä¸€çš„æ½œåœ¨åŠ¨ä½œç©ºé—´ï¼ŒIGORèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æœºå™¨äººå’Œäººç±»æ´»åŠ¨æ•°æ®ä¹‹é—´è¿›è¡ŒçŸ¥è¯†è½¬ç§»ã€‚æˆ‘ä»¬é€šè¿‡å‹ç¼©åˆå§‹å›¾åƒä¸ç›®æ ‡çŠ¶æ€ä¹‹é—´çš„è§†è§‰å˜åŒ–æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä»è€Œç”Ÿæˆæ½œåœ¨åŠ¨ä½œæ ‡ç­¾ã€‚IGORä¸ºæœºå™¨äººæ§åˆ¶å’Œäººæœºäº¤äº’å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00918', 'title': 'LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models', 'url': 'https://huggingface.co/papers/2411.00918', 'abstract': 'Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.', 'score': 8, 'issue_id': 420, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': 'a406640433a3de34', 'authors': ['Nam V. Nguyen', 'Thong T. Doan', 'Luong Tran', 'Van Nguyen', 'Quang Pham'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00918.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LibMoE: Ğ”ĞµĞ¼Ğ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Mixture of Experts Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LibMoE - ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Mixture of Experts (MoE) Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). LibMoE Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ñ…: Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²ÑĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½ÑÑ Ğ¾Ñ†ĞµĞ½ĞºĞ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ LibMoE, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² MoE Ğ½Ğ° Ñ‚Ñ€ĞµÑ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¸ 11 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸, Ğ²ÑĞµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ MoE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ ÑƒÑÑ€ĞµĞ´Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ñƒ ÑĞ¿ĞµĞºÑ‚Ñ€Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'LibMoE: Streamlining Mixture of Experts for Large Language Models', 'desc': 'This paper introduces LibMoE, a framework designed to facilitate research on Mixture of Experts (MoE) algorithms for large language models (LLMs). It emphasizes a modular design, efficient training processes, and comprehensive evaluation methods to make MoE more accessible to researchers. The authors benchmark five leading MoE algorithms across three LLMs and 11 datasets, revealing that their performance is generally similar across various tasks. LibMoE aims to standardize the training and evaluation of MoE, helping researchers advance the development of future LLMs.'}, 'zh': {'title': 'LibMoEï¼šè®©æ··åˆä¸“å®¶ç®—æ³•æ›´æ˜“äºç ”ç©¶å’Œåº”ç”¨', 'desc': 'æ··åˆä¸“å®¶ï¼ˆMoEï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é«˜æ•ˆå’Œæœ‰æ•ˆå‘å±•ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚ç”±äºèµ„æºéœ€æ±‚å·¨å¤§ï¼Œè®¸å¤šç ”ç©¶è€…éš¾ä»¥ç ”ç©¶å¤§è§„æ¨¡çš„MoEç®—æ³•ã€‚æœ¬æ–‡å¼€å‘äº†LibMoEï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢ä¸”æ¨¡å—åŒ–çš„æ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–MoEç®—æ³•çš„ç ”ç©¶ã€è®­ç»ƒå’Œè¯„ä¼°ã€‚é€šè¿‡æ¨¡å—åŒ–è®¾è®¡ã€é«˜æ•ˆè®­ç»ƒå’Œå…¨é¢è¯„ä¼°ï¼ŒLibMoEä½¿å¾—MoEåœ¨LLMsä¸­çš„åº”ç”¨å¯¹æ›´å¤šç ”ç©¶è€…å˜å¾—å¯åŠã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00743', 'title': 'Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models', 'url': 'https://huggingface.co/papers/2411.00743', 'abstract': 'Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and L_0 sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5\\% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.', 'score': 6, 'issue_id': 421, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': 'af234e3c99f935ec', 'authors': ['Aashiq Muhamed', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University', 'Machine Learning Department, Carnegie Mellon University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00743.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#ethics', '#data', '#training', '#dataset', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹ Ğ² Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SSAE). SSAE Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ€ĞµĞ´ĞºĞ¸Ğµ, Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Tilted Empirical Risk Minimization Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ SSAE Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ… Ğ¸ Ğ² Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Bias in Bios.'}, 'en': {'title': 'Illuminating Hidden Concepts in Foundation Models with SSAEs', 'desc': 'This paper discusses the challenges of understanding and mitigating risks in foundation models (FMs) through effective interpretability methods. It introduces Specialized Sparse Autoencoders (SSAEs), which are designed to better identify rare but important concepts in data by focusing on specific subdomains. The authors provide a training approach that utilizes dense retrieval for data selection and Tilted Empirical Risk Minimization to enhance concept recall. The results show that SSAEs outperform traditional Sparse Autoencoders in capturing these critical features, as demonstrated in a case study that improved classification accuracy by addressing bias in gender information.'}, 'zh': {'title': 'ä¸“æ³¨å­é¢†åŸŸçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼šæ­ç¤ºåŸºç¡€æ¨¡å‹çš„æ½œåœ¨ç‰¹å¾', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰æ½œåœ¨é£é™©çš„ç†è§£ä¸ç¼“è§£ï¼Œå¼ºè°ƒäº†æœ‰æ•ˆçš„å¯è§£é‡Šæ€§æ–¹æ³•çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSSAEsï¼‰ï¼Œæ—¨åœ¨æ­ç¤ºæ•°æ®ä¸­ç¨€æœ‰ä½†é‡è¦çš„æ¦‚å¿µï¼Œç‰¹åˆ«å…³æ³¨ç‰¹å®šå­é¢†åŸŸã€‚é€šè¿‡å¯†é›†æ£€ç´¢å’Œå€¾æ–œç»éªŒé£é™©æœ€å°åŒ–ç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬å±•ç¤ºäº†SSAEsåœ¨æ•æ‰å­é¢†åŸŸå°¾éƒ¨æ¦‚å¿µæ–¹é¢çš„ä¼˜åŠ¿ã€‚æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒSSAEsåœ¨å»é™¤è™šå‡æ€§åˆ«ä¿¡æ¯æ—¶ï¼Œåˆ†ç±»å‡†ç¡®ç‡æé«˜äº†12.5%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00359', 'title': 'Constrained Diffusion Implicit Models', 'url': 'https://huggingface.co/papers/2411.00359', 'abstract': 'This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose constrained diffusion implicit models (CDIM) that modify the diffusion updates to enforce a constraint upon the final output. For noiseless inverse problems, CDIM exactly satisfies the constraints; in the noisy case, we generalize CDIM to satisfy an exact constraint on the residual distribution of the noise. Experiments across a variety of tasks and metrics show strong performance of CDIM, with analogous inference acceleration to unconstrained DDIM: 10 to 50 times faster than previous conditional diffusion methods. We demonstrate the versatility of our approach on many problems including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reconstruction.', 'score': 5, 'issue_id': 434, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': 'e85bdeb4e14858fd', 'authors': ['Vivek Jayaram', 'Ira Kemelmacher-Shlizerman', 'Steven M. Seitz', 'John Thickstun'], 'affiliations': ['University of Washington', 'Cornell University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00359.jpg', 'data': {'categories': ['#diffusion', '#cv', '#inference', '#optimization', '#3d', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ constrained diffusion implicit models (CDIM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. CDIM Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ±ĞµĞ· ÑˆÑƒĞ¼Ğ°, Ğ° Ğ´Ğ»Ñ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ CDIM Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² 10-50 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Accelerating Inverse Problem Solutions with Constrained Diffusion Models', 'desc': "This paper introduces a new algorithm called Constrained Diffusion Implicit Models (CDIM) for tackling noisy linear inverse problems. CDIM builds on the existing Denoising Diffusion Implicit Models (DDIM) by incorporating constraints into the diffusion process, ensuring that the final output adheres to specific requirements. In scenarios without noise, CDIM perfectly meets these constraints, while in noisy situations, it adapts to maintain an exact constraint on the noise's residual distribution. The results demonstrate that CDIM not only performs well across various tasks like super-resolution and denoising but also accelerates inference significantly, achieving speeds 10 to 50 times faster than traditional methods."}, 'zh': {'title': 'é«˜æ•ˆè§£å†³å¸¦å™ªå£°çº¿æ€§é€†é—®é¢˜çš„çº¦æŸæ‰©æ•£æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ç®—æ³•ï¼Œç”¨äºè§£å†³å¸¦å™ªå£°çš„çº¿æ€§é€†é—®é¢˜ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬æ‰©å±•äº†å»å™ªæ‰©æ•£éšå¼æ¨¡å‹ï¼ˆDDIMï¼‰çš„èŒƒå¼ï¼Œæå‡ºäº†çº¦æŸæ‰©æ•£éšå¼æ¨¡å‹ï¼ˆCDIMï¼‰ï¼Œé€šè¿‡ä¿®æ”¹æ‰©æ•£æ›´æ–°æ¥å¼ºåˆ¶æœ€ç»ˆè¾“å‡ºæ»¡è¶³çº¦æŸæ¡ä»¶ã€‚åœ¨æ— å™ªå£°çš„é€†é—®é¢˜ä¸­ï¼ŒCDIMèƒ½å¤Ÿç²¾ç¡®æ»¡è¶³çº¦æŸï¼›è€Œåœ¨æœ‰å™ªå£°çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†CDIMæ¨å¹¿åˆ°æ»¡è¶³å™ªå£°æ®‹å·®åˆ†å¸ƒçš„ç²¾ç¡®çº¦æŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCDIMåœ¨å¤šç§ä»»åŠ¡å’ŒæŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå…¶æ¨ç†é€Ÿåº¦æ¯”ä¹‹å‰çš„æ¡ä»¶æ‰©æ•£æ–¹æ³•å¿«10åˆ°50å€ï¼Œå±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•åœ¨è¶…åˆ†è¾¨ç‡ã€å»å™ªã€ä¿®å¤ã€å»æ¨¡ç³Šå’Œ3Dç‚¹äº‘é‡å»ºç­‰é—®é¢˜ä¸Šçš„å¤šæ ·æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.00492', 'title': 'Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models', 'url': 'https://huggingface.co/papers/2411.00492', 'abstract': 'We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.', 'score': 5, 'issue_id': 433, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 1', 'zh': '11æœˆ1æ—¥'}, 'hash': 'd4857ddaa86c9914', 'authors': ['Do Xuan Long', 'Duong Ngoc Yen', 'Anh Tuan Luu', 'Kenji Kawaguchi', 'Min-Yen Kan', 'Nancy F. Chen'], 'affiliations': ['National University of Singapore', 'Institute for Infocomm Research (I2R), A*STAR', 'Nanyang Technological University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00492.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#interpretability', '#architecture', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·ÑƒĞ¼ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ˜Ğ˜', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Multi-expert Prompting, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ÑÑ Ğ² Ğ²Ğ¸Ğ´Ğµ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞµĞ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞµ Nominal Group Technique. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Multi-expert Prompting Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Harnessing Collective Expertise for Superior Language Model Responses', 'desc': 'Multi-expert Prompting is an advanced technique that enhances the performance of large language models (LLMs) by simulating the input of multiple experts. It aggregates responses from these simulated experts and selects the best one, ensuring that the output is more accurate and informative. This method is structured around seven subtasks inspired by the Nominal Group Technique, which aids in effective decision-making. Evaluations show that this approach significantly improves the truthfulness and usefulness of LLM responses while minimizing negative outputs like toxicity.'}, 'zh': {'title': 'å¤šä¸“å®¶æç¤ºï¼šæå‡è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å…¨æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¢å¼ºæ–¹æ³•ï¼Œç§°ä¸ºå¤šä¸“å®¶æç¤ºï¼ˆMulti-expert Promptingï¼‰ï¼Œæ—¨åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆæ•ˆæœã€‚è¯¥æ–¹æ³•é€šè¿‡æ¨¡æ‹Ÿå¤šä¸ªä¸“å®¶æ¥æŒ‡å¯¼LLMæ‰§è¡Œè¾“å…¥æŒ‡ä»¤ï¼Œèšåˆå„ä¸ªä¸“å®¶çš„å“åº”ï¼Œå¹¶é€‰æ‹©æœ€ä½³çš„å•ä¸ªå’Œèšåˆå“åº”ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸ƒä¸ªå­ä»»åŠ¡ï¼ŒåŸºäºæˆç†Ÿçš„å†³ç­–æ¡†æ¶â€”â€”åä¹‰å°ç»„æŠ€æœ¯ï¼ˆNominal Group Techniqueï¼‰ï¼Œä»¥å®ç°è¿™ä¸€è¿‡ç¨‹ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå¤šä¸“å®¶æç¤ºåœ¨æé«˜å“åº”çš„çœŸå®æ€§ã€äº‹å®æ€§ã€ä¿¡æ¯é‡å’Œå®ç”¨æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¸“å®¶æç¤ºï¼ˆExpertPromptingï¼‰åŠå…¶ä»–åŸºçº¿ï¼ŒåŒæ—¶å‡å°‘äº†æœ‰å®³æ€§å’Œæ”»å‡»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.01106', 'title': 'LoRA-Contextualizing Adaptation of Large Multimodal Models for Long Document Understanding', 'url': 'https://huggingface.co/papers/2411.01106', 'abstract': 'Large multimodal models (LMMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page, visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to LMMs leads to inefficiencies, especially with lengthy documents. In this work, we present a novel framework named LoRA-Contextualizing Adaptation of Large multimodal models (LoCAL), which broadens the capabilities of any LMM to support long-document understanding. We demonstrate that LMMs can effectively serve as multimodal retrievers, fetching relevant pages to answer user questions based on these pages. LoCAL is implemented with two specific LMM adapters: one for evidence page retrieval and another for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of LoCAL.', 'score': 4, 'issue_id': 433, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 2', 'zh': '11æœˆ2æ—¥'}, 'hash': '9ac41ff2238a6f8d', 'authors': ['Jian Chen', 'Ruiyi Zhang', 'Yufan Zhou', 'Tong Yu', 'Franck Dernoncourt', 'Jiuxiang Gu', 'Ryan A. Rossi', 'Changyou Chen', 'Tong Sun'], 'affiliations': ['University at Buffalo', 'Adobe Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01106.jpg', 'data': {'categories': ['#science', '#rag', '#benchmark', '#multimodal', '#architecture', '#long_context'], 'emoji': 'ğŸ“„', 'ru': {'title': 'LoCAL: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LMM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LoCAL Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LMM). LoCAL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° LMM: Ğ¾Ğ´Ğ¸Ğ½ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ LoCAL Ğ½Ğ° Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Long-Document Understanding with LoCAL', 'desc': 'This paper introduces LoCAL, a new framework designed to enhance large multimodal models (LMMs) for understanding long and complex documents. Traditional methods struggle with efficiency when processing multiple pages, but LoCAL allows LMMs to act as multimodal retrievers, selecting relevant pages to answer questions. The framework includes two specialized adapters: one for retrieving evidence pages and another for answering questions based on those pages. Empirical results indicate that LoCAL achieves state-of-the-art performance on public benchmarks, showcasing its effectiveness in long-document comprehension.'}, 'zh': {'title': 'æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„é•¿æ–‡æ¡£ç†è§£èƒ½åŠ›', 'desc': 'å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ç†è§£æ–‡æœ¬ä¸°å¯Œçš„å›¾åƒæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚çš„å¤šé¡µè§†è§‰æ–‡æ¡£æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æ–‡æ¡£è§£ææ–¹æ³•åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­å­˜åœ¨æ€§èƒ½å’Œæ•ˆç‡çš„é™åˆ¶ï¼Œè€Œç›´æ¥å°†æ‰€æœ‰é¡µé¢å‘ˆç°ç»™LMMsåˆ™å¯¼è‡´æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è¾ƒé•¿æ–‡æ¡£æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œç§°ä¸ºLoRA-ä¸Šä¸‹æ–‡é€‚åº”çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLoCALï¼‰ï¼Œå®ƒæ‰©å±•äº†ä»»ä½•LMMæ”¯æŒé•¿æ–‡æ¡£ç†è§£çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLoCALåœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.01192', 'title': 'Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks', 'url': 'https://huggingface.co/papers/2411.01192', 'abstract': 'We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research.', 'score': 3, 'issue_id': 427, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 2', 'zh': '11æœˆ2æ—¥'}, 'hash': 'df042a726644eac5', 'authors': ['Gagan Bhatia', 'El Moatez Billah Nagoudi', 'Abdellah El Mekki', 'Fakhraddin Alwajih', 'Muhammad Abdul-Mageed'], 'affiliations': ['MBZUAI', 'The University of British Columbia', 'Invertible AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01192.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#multilingual', '#dataset', '#open_source', '#low_resource', '#architecture'], 'emoji': 'ğŸ¦¢', 'ru': {'title': 'Swan: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Swan Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°: Swan-Small Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ARBERTv2 Ğ¸ Swan-Large Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ArMistral. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ArabicMTEB, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 8 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 94 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Swan-Large Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Multilingual-E5-large Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ñƒ Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚Ğ½ÑƒÑ Ğ¸ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Swan: Advancing Arabic Language Embeddings for Diverse Applications', 'desc': 'This paper presents Swan, a new family of embedding models specifically designed for the Arabic language, which includes two versions: Swan-Small and Swan-Large. Swan-Small is based on ARBERTv2, while Swan-Large utilizes the ArMistral model, a large pretrained Arabic language model. The authors introduce ArabicMTEB, a benchmark suite that evaluates the performance of these models across various Arabic text tasks, ensuring they are effective in different dialects and cultural contexts. The results show that Swan-Large outperforms existing models in most Arabic tasks, highlighting its efficiency and effectiveness in Arabic natural language processing.'}, 'zh': {'title': 'Swanï¼šé˜¿æ‹‰ä¼¯è¯­åµŒå…¥æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Swanï¼Œä¸€ä¸ªä»¥é˜¿æ‹‰ä¼¯è¯­ä¸ºä¸­å¿ƒçš„åµŒå…¥æ¨¡å‹å®¶æ—ï¼Œé€‚ç”¨äºå°è§„æ¨¡å’Œå¤§è§„æ¨¡çš„åº”ç”¨åœºæ™¯ã€‚SwanåŒ…æ‹¬ä¸¤ä¸ªå˜ä½“ï¼šåŸºäºARBERTv2çš„Swan-Smallå’ŒåŸºäºé¢„è®­ç»ƒé˜¿æ‹‰ä¼¯å¤§å‹è¯­è¨€æ¨¡å‹ArMistralçš„Swan-Largeã€‚æˆ‘ä»¬æå‡ºäº†ArabicMTEBï¼Œä¸€ä¸ªå…¨é¢çš„åŸºå‡†å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°é˜¿æ‹‰ä¼¯æ–‡æœ¬åµŒå…¥åœ¨è·¨è¯­è¨€ã€å¤šæ–¹è¨€ã€å¤šé¢†åŸŸå’Œå¤šæ–‡åŒ–æ–¹é¢çš„è¡¨ç°ï¼Œæ¶µç›–å…«ä¸ªä¸åŒä»»åŠ¡å’Œ94ä¸ªæ•°æ®é›†ã€‚Swan-Largeåœ¨å¤§å¤šæ•°é˜¿æ‹‰ä¼¯ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†Multilingual-E5-largeï¼Œè€ŒSwan-Smallä¹Ÿå§‹ç»ˆä¼˜äºMultilingual-E5 baseã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.22366', 'title': 'Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2410.22366', 'abstract': "Sparse autoencoders (SAEs) have become a core ingredient in the reverse engineering of large-language models (LLMs). For LLMs, they have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigated the possibility of using SAEs to learn interpretable features for a few-step text-to-image diffusion models, such as SDXL Turbo. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net. We find that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find one block that deals mainly with image composition, one that is mainly responsible for adding local details, and one for color, illumination, and style. Therefore, our work is an important first step towards better understanding the internals of generative text-to-image models like SDXL Turbo and showcases the potential of features learned by SAEs for the visual domain.   Code is available at https://github.com/surkovv/sdxl-unbox", 'score': 73, 'issue_id': 366, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': 'bb45ca9d1b89342a', 'authors': ['Viacheslav Surkov', 'Chris Wendler', 'Mikhail Terekhov', 'Justin Deschenaux', 'Robert West', 'Caglar Gulcehre'], 'affiliations': ['School of Computer and Communication Sciences EPFL Lausanne, Switzerland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22366.jpg', 'data': {'categories': ['#diffusion', '#cv', '#interpretability', '#open_source', '#3d', '#architecture'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ ÑĞµĞºÑ€ĞµÑ‚Ñ‹ SDXL Turbo: Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ (SAE) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ SDXL Turbo. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ SAE ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ÑÑ… Ğ¸ Ñ†Ğ²ĞµÑ‚Ğµ/Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¼Ñƒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Interpretability in Text-to-Image Models with Sparse Autoencoders', 'desc': "This paper explores the use of Sparse Autoencoders (SAEs) to analyze and interpret the inner workings of text-to-image diffusion models, specifically SDXL Turbo. By training SAEs on the updates from transformer blocks in the model's denoising U-net, the authors demonstrate that these autoencoders can extract interpretable features that influence the image generation process. The study reveals that different transformer blocks specialize in various aspects of image creation, such as composition, local details, and color. This research marks a significant advancement in understanding generative models and highlights the utility of SAEs in the visual domain."}, 'zh': {'title': 'ç¨€ç–è‡ªç¼–ç å™¨åŠ©åŠ›æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç†è§£', 'desc': 'ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEsï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é€†å‘å·¥ç¨‹ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†å°†SAEsåº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å¯èƒ½æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹SDXL Turboçš„å‡ æ­¥æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°ï¼ŒSAEså­¦ä¹ åˆ°çš„ç‰¹å¾å…·æœ‰å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”å¯¹ç”Ÿæˆè¿‡ç¨‹æœ‰å› æœå½±å“ï¼Œæ­ç¤ºäº†æ¨¡å‹å†…éƒ¨å—ä¹‹é—´çš„ä¸“ä¸šåŒ–ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ›´å¥½åœ°ç†è§£ç”Ÿæˆæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„å†…éƒ¨æœºåˆ¶è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.23743', 'title': 'What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective', 'url': 'https://huggingface.co/papers/2410.23743', 'abstract': 'What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. Our code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer_Gradient.', 'score': 58, 'issue_id': 364, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '6af756426d4b0064', 'authors': ['Ming Li', 'Yanhong Li', 'Tianyi Zhou'], 'affiliations': ['University of Chicago', 'University of Maryland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23743.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training', '#open_source', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ ÑĞ»Ğ¾ĞµĞ², ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM Ğ½Ğ° Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ÑÑ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ CoT Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ LLM, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Unlocking Stability: The Power of Slow Thinking in LLM Training', 'desc': 'This paper explores how different training patterns in large language models (LLMs) affect their learning stability and efficiency. It focuses on the impact of fast versus slow thinking on layer-wise gradients during training, particularly when using reasoning techniques like chain-of-thoughts (CoT). The findings reveal that fast thinking generates larger gradients and more variability across layers, while slow thinking promotes stability and better distinguishes correct reasoning paths. Additionally, the study highlights that pre-trained LLMs are more resilient to the instability caused by fast thinking compared to instruction-tuned models.'}, 'zh': {'title': 'å¿«é€Ÿä¸æ…¢é€Ÿæ€ç»´å¯¹å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒçš„å½±å“', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åè®­ç»ƒé˜¶æ®µçš„ä¸åŒå±‚æ¬¡çš„è®­ç»ƒæ¨¡å¼ï¼Œç‰¹åˆ«å…³æ³¨å¿«é€Ÿæ€ç»´ä¸æ…¢é€Ÿæ€ç»´å¯¹æ¢¯åº¦çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼Œå¿«é€Ÿæ€ç»´åœ¨æ²¡æœ‰é“¾å¼æ€ç»´ï¼ˆCoTï¼‰çš„æƒ…å†µä¸‹ï¼Œä¼šå¯¼è‡´æ›´å¤§çš„æ¢¯åº¦å’Œå±‚é—´æ¢¯åº¦å·®å¼‚ï¼Œè€Œæ…¢é€Ÿæ€ç»´åˆ™å¸¦æ¥äº†æ›´å¥½çš„å­¦ä¹ ç¨³å®šæ€§ã€‚é¢„è®­ç»ƒçš„LLMså¯¹å¿«é€Ÿæ€ç»´çš„ä¸ç¨³å®šæ€§å½±å“è¾ƒå°ï¼Œè€ŒæŒ‡ä»¤è°ƒä¼˜çš„LLMsåˆ™æ›´ä¸ºæ•æ„Ÿã€‚æ­¤å¤–ï¼Œæ…¢é€Ÿæ€ç»´çš„æ¢¯åº¦æ¨¡å¼èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†æ­£ç¡®ä¸æ— å…³çš„æ¨ç†è·¯å¾„ï¼Œå¢å¼ºäº†å¯¹LLMè®­ç»ƒçš„åŸºæœ¬ç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.22476', 'title': 'A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents', 'url': 'https://huggingface.co/papers/2410.22476', 'abstract': 'In task-oriented dialogue systems, intent detection is crucial for interpreting user queries and providing appropriate responses. Existing research primarily addresses simple queries with a single intent, lacking effective systems for handling complex queries with multiple intents and extracting different intent spans. Additionally, there is a notable absence of multilingual, multi-intent datasets. This study addresses three critical tasks: extracting multiple intent spans from queries, detecting multiple intents, and developing a multi-lingual multi-label intent dataset. We introduce a novel multi-label multi-class intent detection dataset (MLMCID-dataset) curated from existing benchmark datasets. We also propose a pointer network-based architecture (MLMCID) to extract intent spans and detect multiple intents with coarse and fine-grained labels in the form of sextuplets. Comprehensive analysis demonstrates the superiority of our pointer network-based system over baseline approaches in terms of accuracy and F1-score across various datasets.', 'score': 24, 'issue_id': 370, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '22f1775d93baf7e1', 'authors': ['Ankan Mullick', 'Sombit Bose', 'Abhilash Nandy', 'Gajula Sai Chaitanya', 'Pawan Goyal'], 'affiliations': ['Computer Science and Engineering Department, IIT Kharagpur, India', 'Qualcomm, India'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22476.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#multilingual', '#graphs', '#optimization', '#dataset', '#transfer_learning', '#games', '#low_resource', '#machine_translation', '#architecture'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ F1-Ğ¼ĞµÑ€Ğµ.'}, 'en': {'title': 'Enhancing Intent Detection for Complex Queries in Multilingual Dialogue Systems', 'desc': 'This paper focuses on improving task-oriented dialogue systems by enhancing intent detection, especially for complex queries that involve multiple intents. The authors identify a gap in existing research, which often only addresses simple queries, and they highlight the need for multilingual datasets that can handle multiple intents. To tackle this, they introduce the MLMCID-dataset, a new multi-label multi-class intent detection dataset, and a pointer network-based architecture designed to extract intent spans and detect multiple intents effectively. Their experiments show that this new approach outperforms traditional methods, achieving better accuracy and F1-scores across different datasets.'}, 'zh': {'title': 'å¤šæ„å›¾æ£€æµ‹çš„æ–°çªç ´', 'desc': 'åœ¨ä»»åŠ¡å¯¼å‘å¯¹è¯ç³»ç»Ÿä¸­ï¼Œæ„å›¾æ£€æµ‹å¯¹äºç†è§£ç”¨æˆ·æŸ¥è¯¢å’Œæä¾›åˆé€‚çš„å“åº”è‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºå¤„ç†å•ä¸€æ„å›¾çš„ç®€å•æŸ¥è¯¢ï¼Œç¼ºä¹æœ‰æ•ˆçš„ç³»ç»Ÿæ¥å¤„ç†å¤æ‚çš„å¤šæ„å›¾æŸ¥è¯¢å’Œæå–ä¸åŒçš„æ„å›¾èŒƒå›´ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¤šè¯­è¨€å’Œå¤šæ„å›¾çš„æ•°æ®é›†ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤šæ ‡ç­¾å¤šç±»æ„å›¾æ£€æµ‹æ•°æ®é›†ï¼ˆMLMCID-datasetï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæŒ‡é’ˆç½‘ç»œçš„æ¶æ„ï¼ˆMLMCIDï¼‰ï¼Œèƒ½å¤Ÿæå–æ„å›¾èŒƒå›´å¹¶æ£€æµ‹å¤šç§æ„å›¾ï¼Œåˆ†æç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿåœ¨å‡†ç¡®æ€§å’ŒF1åˆ†æ•°ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.24198', 'title': 'SelfCodeAlign: Self-Alignment for Code Generation', 'url': 'https://huggingface.co/papers/2410.24198', 'abstract': "Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component's effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance.", 'score': 20, 'issue_id': 372, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '76a150925181ec52', 'authors': ['Yuxiang Wei', 'Federico Cassano', 'Jiawei Liu', 'Yifeng Ding', 'Naman Jain', 'Zachary Mueller', 'Harm de Vries', 'Leandro von Werra', 'Arjun Guha', 'Lingming Zhang'], 'affiliations': ['Cursor AI', 'Hugging Face', 'Northeastern University', 'Roblox', 'ServiceNow Research', 'University of California, Berkeley', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24198.jpg', 'data': {'categories': ['#small_models', '#synthetic', '#benchmark', '#plp', '#data', '#training', '#dataset', '#open_source', '#alignment'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'SelfCodeAlign: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ° Ğ±ĞµĞ· Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'SelfCodeAlign - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ°Ğ¼Ğ¾Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞĞ½ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SelfCodeAlign Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğº Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'SelfCodeAlign: Revolutionizing Code LLMs with Minimal Human Input', 'desc': "This paper introduces SelfCodeAlign, a novel method for improving large language models (LLMs) specifically for coding tasks without needing extensive human input. The approach involves generating diverse coding tasks from high-quality code snippets and validating the model's responses in a controlled environment. By fine-tuning the model on a dataset of 74,000 instruction-response pairs, SelfCodeAlign significantly enhances the model's ability to follow instructions, achieving superior performance compared to larger models. The results demonstrate that this method is effective across various model sizes and establishes a new standard for self-aligned code LLMs."}, 'zh': {'title': 'è‡ªæˆ‘å¯¹é½ï¼Œæå‡ä»£ç æ¨¡å‹æ€§èƒ½ï¼', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSelfCodeAlignçš„è‡ªæˆ‘å¯¹é½ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹éµå¾ªäººç±»æŒ‡ä»¤çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•æ— éœ€å¤§é‡äººå·¥æ ‡æ³¨æˆ–è’¸é¦ï¼Œåˆ©ç”¨ç›¸åŒçš„åŸºç¡€æ¨¡å‹åœ¨æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸­è¿›è¡Œæ¨ç†ã€‚SelfCodeAligné€šè¿‡ä»é«˜è´¨é‡çš„ç§å­ä»£ç ç‰‡æ®µä¸­æå–å¤šæ ·åŒ–çš„ç¼–ç æ¦‚å¿µï¼Œç”Ÿæˆæ–°çš„ä»»åŠ¡ï¼Œå¹¶åœ¨æ²™ç®±ç¯å¢ƒä¸­éªŒè¯æ¯ä¸ªä»»åŠ¡çš„å¤šä¸ªå“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨SelfCodeAlignç”Ÿæˆçš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒåï¼Œæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.23918', 'title': 'BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments', 'url': 'https://huggingface.co/papers/2410.23918', 'abstract': 'Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from capability to availability, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce BitStack, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code is available at https://github.com/xinghaow99/BitStack.', 'score': 17, 'issue_id': 371, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '7699f83913665aca', 'authors': ['Xinghao Wang', 'Pengyu Wang', 'Bo Wang', 'Dong Zhang', 'Yunhua Zhou', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23918.jpg', 'data': {'categories': ['#inference', '#optimization', '#training', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'BitStack: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ BitStack. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¸ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. BitStack Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ², ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ·Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 1 Ğ±Ğ¸Ñ‚ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BitStack ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ°Ñ… ÑĞ¶Ğ°Ñ‚Ğ¸Ñ.'}, 'en': {'title': 'Dynamic Memory Management for Large Language Models with BitStack', 'desc': 'This paper presents BitStack, a new method for compressing large language models (LLMs) without the need for prior training. It addresses the challenge of deploying LLMs on devices with limited memory by allowing dynamic adjustments to model size based on available memory. BitStack uses weight decomposition to create small, efficient blocks of model parameters, which can be easily transmitted and loaded as needed. The results show that BitStack not only provides fine control over memory usage but also performs as well or better than traditional quantization methods, especially under high compression scenarios.'}, 'zh': {'title': 'BitStackï¼šé«˜æ•ˆçš„æƒé‡å‹ç¼©æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¸å¤šåº”ç”¨ä¸­å–å¾—äº†é©å‘½æ€§è¿›å±•ï¼Œä½†åœ¨æœ¬åœ°è®¾å¤‡ä¸Šçš„éƒ¨ç½²ä»é¢ä¸´å†…å­˜é™åˆ¶çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBitStackçš„æ–°æ–¹æ³•ï¼Œå®ƒæ˜¯ä¸€ç§æ— è®­ç»ƒçš„æƒé‡å‹ç¼©æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å†…å­˜ä½¿ç”¨å’Œæ¨¡å‹æ€§èƒ½ä¹‹é—´å®ç°çµæ´»çš„æƒè¡¡ã€‚é€šè¿‡æƒé‡åˆ†è§£ï¼ŒBitStackå¯ä»¥åŠ¨æ€è°ƒæ•´æ¨¡å‹å¤§å°ï¼Œå‡å°‘è¿è¡Œå†…å­˜ä¸å­˜å‚¨è®¾å¤‡ä¹‹é—´çš„ä¼ è¾“ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æä¾›äº†ç»†ç²’åº¦çš„å¤§å°æ§åˆ¶ï¼ŒBitStackåœ¨æç«¯å‹ç¼©æ¯”ä¸‹çš„æ€§èƒ½ä»ç„¶ä¸å¼ºé‡åŒ–åŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.20650', 'title': 'NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks', 'url': 'https://huggingface.co/papers/2410.20650', 'abstract': 'The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available.', 'score': 16, 'issue_id': 380, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'}, 'hash': '2878f8bac8da005d', 'authors': ['Yongchang Hao', 'Yanshuai Cao', 'Lili Mou'], 'affiliations': ['Borealis AI', 'Dept. Computing Science & Alberta Machine Intelligence Institute (Amii), University of Alberta', 'Canada CIFAR AI Chair'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20650.jpg', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#open_source'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'NeuZip: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'NeuZip - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ‡Ğ¸ÑĞµĞ» Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, NeuZip ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama-3 8B Ñ 31 Ğ“Ğ‘ Ğ´Ğ¾ Ğ¼ĞµĞ½ĞµĞµ Ñ‡ĞµĞ¼ 16 Ğ“Ğ‘. ĞŸÑ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ²Ğ´Ğ²Ğ¾Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ±ĞµĞ·ÑƒĞ¿Ñ€ĞµÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'NeuZip: Compressing Neural Networks Without Compromise', 'desc': 'This paper presents NeuZip, a novel weight compression technique designed for neural networks. NeuZip leverages the entropy of floating-point numbers to compress model weights, allowing for more efficient use of on-device memory during both training and inference. The method significantly reduces the memory requirements of large models, such as the Llama-3 8B, without compromising their performance. By implementing NeuZip, users can achieve a memory footprint reduction of over 50% while maintaining the integrity of the training dynamics and inference results.'}, 'zh': {'title': 'NeuZipï¼šé«˜æ•ˆå†…å­˜å‹ç¼©ï¼Œæ€§èƒ½ä¸æ‰“æŠ˜ï¼', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æƒé‡å‹ç¼©æ–¹æ¡ˆï¼Œç§°ä¸ºNeuZipï¼Œæ—¨åœ¨æé«˜ç¥ç»ç½‘ç»œçš„å†…å­˜æ•ˆç‡ã€‚é€šè¿‡åˆ©ç”¨æµ®ç‚¹æ•°çš„ç†µï¼ŒNeuZipèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—å‡å°‘æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†Llama-3 8Bæ¨¡å‹çš„è®­ç»ƒå†…å­˜éœ€æ±‚ä»31GBé™ä½åˆ°16GBä»¥ä¸‹ï¼ŒåŒæ—¶ä¿æŒè®­ç»ƒåŠ¨æ€ä¸å˜ã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å°†å†…å­˜ä½¿ç”¨å‡å°‘ä¸€åŠä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒæ¥è¿‘æ— æŸçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.23933', 'title': 'Language Models can Self-Lengthen to Generate Long Texts', 'url': 'https://huggingface.co/papers/2410.23933', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to process long contexts, yet a notable gap remains in generating long, aligned outputs. This limitation stems from a training gap where pre-training lacks effective instructions for long-text generation, and post-training data primarily consists of short query-response pairs. Current approaches, such as instruction backtranslation and behavior imitation, face challenges including data quality, copyright issues, and constraints on proprietary model usage. In this paper, we introduce an innovative iterative training framework called Self-Lengthen that leverages only the intrinsic knowledge and skills of LLMs without the need for auxiliary data or proprietary models. The framework consists of two roles: the Generator and the Extender. The Generator produces the initial response, which is then split and expanded by the Extender. This process results in a new, longer response, which is used to train both the Generator and the Extender iteratively. Through this process, the models are progressively trained to handle increasingly longer responses. Experiments on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long-text generation, when applied to top open-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at https://github.com/QwenLM/Self-Lengthen.', 'score': 15, 'issue_id': 363, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '2ba3bbe4b8a9836d', 'authors': ['Shanghaoran Quan', 'Tianyi Tang', 'Bowen Yu', 'An Yang', 'Dayiheng Liu', 'Bofei Gao', 'Jianhong Tu', 'Yichang Zhang', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Qwen Team, Alibaba Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23933.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#optimization', '#training', '#open_source', '#architecture', '#long_context'], 'emoji': 'ğŸ“', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ´Ğ»Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Self-Lengthen. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ñ€Ğ¾Ğ»ÑĞ¼Ğ¸: Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ Ğ Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Self-Lengthen Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ğº Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ LLM.'}, 'en': {'title': 'Empowering LLMs to Generate Longer, Coherent Texts with Self-Lengthen', 'desc': "This paper addresses the challenge of generating long, coherent outputs from Large Language Models (LLMs), which often struggle due to a lack of effective training for long-text generation. The authors propose a novel iterative training framework called Self-Lengthen, which utilizes the models' existing capabilities without relying on external data or proprietary models. The framework involves two components: a Generator that creates an initial response and an Extender that expands this response into a longer format. Through iterative training, the models improve their ability to generate longer, aligned outputs, demonstrating superior performance compared to existing methods in experiments and human evaluations."}, 'zh': {'title': 'è‡ªæˆ‘å»¶é•¿ï¼šæå‡é•¿æ–‡æœ¬ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥æ˜¾è‘—æå‡äº†å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆé•¿ä¸”ä¸€è‡´çš„è¾“å‡ºæ–¹é¢ä»å­˜åœ¨ä¸è¶³ã€‚è¿™ä¸€é™åˆ¶æºäºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¼ºé™·ï¼Œé¢„è®­ç»ƒç¼ºä¹æœ‰æ•ˆçš„é•¿æ–‡æœ¬ç”ŸæˆæŒ‡ä»¤ï¼Œè€ŒåæœŸè®­ç»ƒçš„æ•°æ®ä¸»è¦æ˜¯çŸ­é—®ç­”å¯¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„è¿­ä»£è®­ç»ƒæ¡†æ¶Self-Lengthenï¼Œåˆ©ç”¨LLMsçš„å†…åœ¨çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œæ— éœ€è¾…åŠ©æ•°æ®æˆ–ä¸“æœ‰æ¨¡å‹ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ç”Ÿæˆå™¨å’Œæ‰©å±•å™¨ä¸¤ä¸ªè§’è‰²ï¼Œé€šè¿‡ç”Ÿæˆåˆå§‹å“åº”å¹¶è¿›è¡Œæ‰©å±•ï¼Œé€æ­¥è®­ç»ƒæ¨¡å‹ä»¥å¤„ç†æ›´é•¿çš„å“åº”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.24175', 'title': 'Constraint Back-translation Improves Complex Instruction Following of Large Language Models', 'url': 'https://huggingface.co/papers/2410.24175', 'abstract': "Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.", 'score': 15, 'issue_id': 363, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '6550f79d46b1945c', 'authors': ['Yunjia Qi', 'Hao Peng', 'Xiaozhi Wang', 'Bin Xu', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Department of Computer Science and Technology, BNRist, Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24175.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#optimization', '#data', '#training', '#dataset', '#open_source'], 'emoji': 'ğŸ¦€', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹', 'desc': "Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ 'Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… CRAB, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñƒ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ, Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾Ğ¹ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ†ĞµĞ»ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Enhancing LLMs with Constraint Back-Translation for Complex Instructions', 'desc': 'This paper addresses the challenges that large language models (LLMs) face when following complex instructions. It critiques traditional instruction-tuning methods that rely on generating complex instruction-response pairs, which often leads to poor performance due to the limitations of LLMs. The authors introduce a new technique called constraint back-translation, which enhances existing high-quality instruction-response pairs by adding implicit complex constraints. Their experiments show that using this method to create a dataset, named CRAB, significantly improves the ability of various LLMs to follow complex instructions, demonstrating its effectiveness as a training strategy.'}, 'zh': {'title': 'æå‡å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤æ—¶è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨æ ¼å¼å’Œé•¿åº¦ç­‰çº¦æŸæ–¹é¢ã€‚ä»¥å¾€çš„ç ”ç©¶é€šè¿‡å¯¹å¤æ‚æŒ‡ä»¤-å“åº”å¯¹è¿›è¡Œåè®­ç»ƒæ¥æ”¹è¿›æ¨¡å‹ï¼Œä½†æ•ˆæœæœ‰é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ•°æ®ç”ŸæˆæŠ€æœ¯ï¼Œç§°ä¸ºçº¦æŸåå‘ç¿»è¯‘ï¼Œåˆ©ç”¨ç°æœ‰æ•°æ®é›†ä¸­çš„é«˜è´¨é‡æŒ‡ä»¤-å“åº”å¯¹ï¼Œæ·»åŠ å·²æ»¡è¶³çš„å¤æ‚çº¦æŸï¼Œä»è€Œæé«˜æ•°æ®è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ–°ç”Ÿæˆçš„æ•°æ®é›†CRABä¸Šè¿›è¡Œåè®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—æå‡å¤šä¸ªåŸºç¡€LLMåœ¨å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.24213', 'title': 'Learning Video Representations without Natural Videos', 'url': 'https://huggingface.co/papers/2410.24213', 'abstract': 'In this paper, we show that useful video representations can be learned from synthetic videos and natural images, without incorporating natural videos in the training. We propose a progression of video datasets synthesized by simple generative processes, that model a growing set of natural video properties (e.g. motion, acceleration, and shape transformations). The downstream performance of video models pre-trained on these generated datasets gradually increases with the dataset progression. A VideoMAE model pre-trained on our synthetic videos closes 97.2% of the performance gap on UCF101 action classification between training from scratch and self-supervised pre-training from natural videos, and outperforms the pre-trained model on HMDB51. Introducing crops of static images to the pre-training stage results in similar performance to UCF101 pre-training and outperforms the UCF101 pre-trained model on 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the low-level properties of the datasets, we identify correlations between frame diversity, frame similarity to natural data, and downstream performance. Our approach provides a more controllable and transparent alternative to video data curation processes for pre-training.', 'score': 14, 'issue_id': 362, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'a01960d8f855aede', 'authors': ['Xueyang Yu', 'Xinlei Chen', 'Yossi Gandelsman'], 'affiliations': ['Meta AI', 'ShanghaiTech University', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24213.jpg', 'data': {'categories': ['#synthetic', '#cv', '#video', '#training', '#dataset', '#transfer_learning'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…, Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ VideoMAE, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğµ Ğº Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼ ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ğ¸Ñ… ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚ÑŒÑ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Synthetic Videos: A New Path to Effective Video Representation Learning', 'desc': 'This paper demonstrates that effective video representations can be learned using synthetic videos and natural images, without needing natural videos for training. The authors introduce a series of progressively complex synthetic video datasets that capture essential characteristics of natural videos, such as motion and shape changes. They show that a VideoMAE model pre-trained on these synthetic datasets significantly narrows the performance gap compared to models trained on natural videos. Additionally, incorporating static image crops during pre-training enhances performance on various action classification tasks, suggesting a strong link between dataset properties and model effectiveness.'}, 'zh': {'title': 'åˆæˆè§†é¢‘åŠ©åŠ›è§†é¢‘è¡¨ç¤ºå­¦ä¹ çš„çªç ´', 'desc': 'æœ¬æ–‡å±•ç¤ºäº†å¦‚ä½•ä»åˆæˆè§†é¢‘å’Œè‡ªç„¶å›¾åƒä¸­å­¦ä¹ æœ‰ç”¨çš„è§†é¢‘è¡¨ç¤ºï¼Œè€Œæ— éœ€åœ¨è®­ç»ƒä¸­ä½¿ç”¨è‡ªç„¶è§†é¢‘ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—é€šè¿‡ç®€å•ç”Ÿæˆè¿‡ç¨‹åˆæˆçš„è§†é¢‘æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†æ¨¡æ‹Ÿäº†è¶Šæ¥è¶Šå¤šçš„è‡ªç„¶è§†é¢‘ç‰¹æ€§ï¼Œå¦‚è¿åŠ¨ã€åŠ é€Ÿåº¦å’Œå½¢çŠ¶å˜æ¢ã€‚é€šè¿‡åœ¨è¿™äº›åˆæˆæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„VideoMAEæ¨¡å‹ï¼Œåœ¨UCF101åŠ¨ä½œåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæ€§èƒ½å·®è·ç¼©å°äº†97.2%ï¼Œå¹¶åœ¨HMDB51ä¸Šè¡¨ç°ä¼˜äºé¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ•°æ®é›†çš„å¸§å¤šæ ·æ€§å’Œä¸è‡ªç„¶æ•°æ®çš„ç›¸ä¼¼æ€§ä¸ä¸‹æ¸¸æ€§èƒ½ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§ï¼Œä¸ºè§†é¢‘æ•°æ®çš„é¢„è®­ç»ƒæä¾›äº†æ›´å¯æ§å’Œé€æ˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.22394', 'title': "AAAR-1.0: Assessing AI's Potential to Assist Research", 'url': 'https://huggingface.co/papers/2410.22394', 'abstract': 'Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new versions.', 'score': 13, 'issue_id': 379, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': 'cbb8d9bd8efa2823', 'authors': ['Renze Lou', 'Hanzi Xu', 'Sijia Wang', 'Jiangshu Du', 'Ryo Kamoi', 'Xiaoxin Lu', 'Jian Xie', 'Yuxuan Sun', 'Yusen Zhang', 'Jihyun Janice Ahn', 'Hongchao Fang', 'Zhuoyang Zou', 'Wenchao Ma', 'Xi Li', 'Kai Zhang', 'Congying Xia', 'Lifu Huang', 'Wenpeng Yin'], 'affiliations': ['Fudan University', 'Netflix', 'Ohio State University', 'Pennsylvania State University', 'Salesforce Research', 'University of Alabama at Birmingham', 'University of California, Davis', 'University of Illinois Chicago', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22394.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#plp', '#dataset', '#open_source', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'AAAR-1.0: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AAAR-1.0 Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ: Ğ²Ñ‹Ğ²Ğ¾Ğ´ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ°Ğ±Ñ‹Ñ… Ğ¼ĞµÑÑ‚ Ğ² ÑÑ‚Ğ°Ñ‚ÑŒÑÑ… Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµÑ†ĞµĞ½Ğ·Ğ¸Ğ¹. AAAR-1.0 Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒÑ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞÑ†ĞµĞ½ĞºĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… LLM Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Empowering Researchers with AI: Introducing AAAR-1.0', 'desc': 'This paper presents AAAR-1.0, a new benchmark dataset aimed at evaluating the performance of large language models (LLMs) in research-related tasks. The dataset focuses on four key tasks: assessing equation correctness, designing experiments, identifying weaknesses in papers, and critiquing reviews. Unlike previous benchmarks, AAAR-1.0 is specifically tailored for research activities that require specialized knowledge. The study highlights both the capabilities and limitations of current LLMs in handling complex research tasks, with plans for future iterations of the benchmark.'}, 'zh': {'title': 'AAAR-1.0ï¼šæå‡ç ”ç©¶æ•ˆç‡çš„è¯­è¨€æ¨¡å‹åŸºå‡†', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†AAAR-1.0ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç ”ç©¶ä»»åŠ¡ä¸­çš„è¡¨ç°çš„åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†ä¸“æ³¨äºä¸‰ä¸ªå…³é”®ä»»åŠ¡ï¼šæ–¹ç¨‹æ¨ç†ã€å®éªŒè®¾è®¡å’Œè®ºæ–‡å¼±ç‚¹è¯†åˆ«ï¼Œæ—¨åœ¨å¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°åˆ©ç”¨LLMsã€‚ä¸ä»¥å¾€çš„åŸºå‡†ä¸åŒï¼ŒAAAR-1.0å¼ºè°ƒç ”ç©¶å¯¼å‘ï¼Œè¦æ±‚æ·±åšçš„é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚é€šè¿‡å¯¹å¼€æºå’Œä¸“æœ‰LLMsçš„è¯„ä¼°ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬åœ¨å¤æ‚ç ”ç©¶ä»»åŠ¡ä¸­çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.24032', 'title': 'Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks', 'url': 'https://huggingface.co/papers/2410.24032', 'abstract': "The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface. CARE's interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In a within-subject user study with 22 participants, CARE was consistently preferred over a baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CARE's potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration.", 'score': 8, 'issue_id': 368, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'b4fbb07af8c8665c', 'authors': ['Yingzhe Peng', 'Xiaoting Qin', 'Zhiyang Zhang', 'Jue Zhang', 'Qingwei Lin', 'Xu Yang', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Microsoft, China', 'Microsoft, USA', 'Southeast University, China', 'State Key Laboratory for Novel Software Technology, Nanjing University, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24032.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#interpretability', '#agents', '#architecture', '#alignment'], 'emoji': 'ğŸ¤', 'ru': {'title': 'CARE: ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ CARE (Collaborative Assistant for Personalized Exploration), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼. CARE ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ¿Ğ°Ğ½ĞµĞ»ĞµĞ¹ Ñ‡Ğ°Ñ‚Ğ°, Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ²Ğ½Ñ‹Ñ… Ğ¸ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ, Ğ´ĞµĞ¹ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ CARE Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ĞµĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ‡Ğ°Ñ‚-Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ÑĞ½Ğ¸Ğ¶Ğ°Ñ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ Ğ¸ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Transforming LLMs into Proactive Personal Assistants', 'desc': 'This paper presents the Collaborative Assistant for Personalized Exploration (CARE), a system that improves how users interact with large language models (LLMs) during exploratory tasks. CARE uses a multi-agent framework that works together to understand both the explicit and implicit needs of users, allowing for more personalized assistance. The system features a structured user interface with different panels that help users refine their queries and generate solutions dynamically. A user study showed that participants preferred CARE over traditional LLM chatbots, noting its effectiveness in reducing cognitive load and providing tailored support.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–æ¢ç´¢çš„åä½œåŠ©æ‰‹CARE', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCAREçš„ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¢ç´¢æ€§ä»»åŠ¡ä¸­çš„ä¸ªæ€§åŒ–æ”¯æŒã€‚CAREç»“åˆäº†å¤šä»£ç†LLMæ¡†æ¶å’Œç»“æ„åŒ–ç”¨æˆ·ç•Œé¢ï¼Œå¸®åŠ©ç”¨æˆ·åœ¨æ¨¡ç³ŠæŸ¥è¯¢æ—¶æ›´å¥½åœ°è·å–ä¿¡æ¯ã€‚ç³»ç»Ÿçš„ç•Œé¢åŒ…æ‹¬èŠå¤©é¢æ¿ã€è§£å†³æ–¹æ¡ˆé¢æ¿å’Œéœ€æ±‚é¢æ¿ï¼Œæ”¯æŒç”¨æˆ·è¿­ä»£ä¼˜åŒ–æŸ¥è¯¢å’ŒåŠ¨æ€ç”Ÿæˆè§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶è¡¨æ˜ï¼ŒCAREåœ¨ç”¨æˆ·ä½“éªŒä¸Šä¼˜äºä¼ ç»Ÿçš„LLMèŠå¤©æœºå™¨äººï¼Œèƒ½å¤Ÿå‡è½»è®¤çŸ¥è´Ÿæ‹…ï¼Œæ¿€å‘åˆ›é€ åŠ›ï¼Œå¹¶æä¾›æ›´å…·é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.24211', 'title': 'DELTA: Dense Efficient Long-range 3D Tracking for any video', 'url': 'https://huggingface.co/papers/2410.24211', 'abstract': 'Tracking dense 3D motion from monocular videos remains challenging, particularly when aiming for pixel-level precision over long sequences. We introduce \\Approach, a novel method that efficiently tracks every pixel in 3D space, enabling accurate motion estimation across entire videos. Our approach leverages a joint global-local attention mechanism for reduced-resolution tracking, followed by a transformer-based upsampler to achieve high-resolution predictions. Unlike existing methods, which are limited by computational inefficiency or sparse tracking, \\Approach delivers dense 3D tracking at scale, running over 8x faster than previous methods while achieving state-of-the-art accuracy. Furthermore, we explore the impact of depth representation on tracking performance and identify log-depth as the optimal choice. Extensive experiments demonstrate the superiority of \\Approach on multiple benchmarks, achieving new state-of-the-art results in both 2D and 3D dense tracking tasks. Our method provides a robust solution for applications requiring fine-grained, long-term motion tracking in 3D space.', 'score': 8, 'issue_id': 368, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': 'e9fe68be8b772de2', 'authors': ['Tuan Duc Ngo', 'Peiye Zhuang', 'Chuang Gan', 'Evangelos Kalogerakis', 'Sergey Tulyakov', 'Hsin-Ying Lee', 'Chaoyang Wang'], 'affiliations': ['Snap Inc.', 'TU Crete', 'UMass Amherst'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24211.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#cv', '#video', '#optimization', '#3d', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¼ 3D-Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğµ: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¸ĞºÑĞµĞ»Ñ Ğ² 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 8 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² 2D Ğ¸ 3D.'}, 'en': {'title': 'Efficient Dense 3D Motion Tracking with State-of-the-Art Precision', 'desc': 'This paper presents a new method called \\Approach for tracking dense 3D motion from monocular videos with high precision. It utilizes a joint global-local attention mechanism to perform efficient tracking at reduced resolution, followed by a transformer-based upsampler to enhance the predictions to high resolution. The method significantly improves computational efficiency, running over 8 times faster than previous techniques while achieving state-of-the-art accuracy in both 2D and 3D tracking tasks. Additionally, the study identifies log-depth as the best depth representation for enhancing tracking performance.'}, 'zh': {'title': 'é«˜æ•ˆç¨ å¯†ä¸‰ç»´è¿åŠ¨è¿½è¸ªçš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•\textit{Approach}ï¼Œç”¨äºä»å•ç›®è§†é¢‘ä¸­é«˜æ•ˆè¿½è¸ªæ¯ä¸ªåƒç´ çš„ä¸‰ç»´è¿åŠ¨ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…ˆè¿›è¡Œä½åˆ†è¾¨ç‡è¿½è¸ªï¼Œç„¶åä½¿ç”¨åŸºäºå˜æ¢å™¨çš„ä¸Šé‡‡æ ·å™¨å®ç°é«˜åˆ†è¾¨ç‡é¢„æµ‹ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œ\textit{Approach}åœ¨è®¡ç®—æ•ˆç‡å’Œç¨ å¯†è¿½è¸ªæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€Ÿåº¦æ¯”ä¹‹å‰çš„æ–¹æ³•å¿«8å€ï¼ŒåŒæ—¶ä¿æŒäº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†\textit{Approach}åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„ä¼˜è¶Šæ€§ï¼Œæä¾›äº†åœ¨ä¸‰ç»´ç©ºé—´ä¸­è¿›è¡Œç²¾ç»†ã€é•¿æœŸè¿åŠ¨è¿½è¸ªçš„å¼ºå¤§è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.21969', 'title': 'BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays', 'url': 'https://huggingface.co/papers/2410.21969', 'abstract': 'Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.', 'score': 8, 'issue_id': 362, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': 'f3fe2798fa53bad0', 'authors': ['Yang Zhou', 'Tan Li Hui Faith', 'Yanyu Xu', 'Sicong Leng', 'Xinxing Xu', 'Yong Liu', 'Rick Siow Mong Goh'], 'affiliations': ['C-FAIR, Shandong University, China', 'Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore', 'Microsoft Research Asia Singapore', 'Nanyang Technological University, Singapore', 'National University of Singapore, Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21969.jpg', 'data': {'categories': ['#benchmark', '#cv', '#graphs', '#healthcare', '#transfer_learning', '#dataset', '#open_source', '#survey'], 'emoji': 'ã€°ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BenchX - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² (MedVLP). BenchX Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ½Ñ‚Ğ³ĞµĞ½Ğ¾Ğ²ÑĞºĞ¸Ñ… ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ³Ñ€ÑƒĞ´Ğ½Ğ¾Ğ¹ ĞºĞ»ĞµÑ‚ĞºĞ¸, ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ BenchX Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ´ĞµĞ²ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² MedVLP, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ğ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Standardizing Medical Vision-Language Pretraining Evaluation with BenchX', 'desc': 'The paper introduces MedVLP, a method for learning visual representations from medical images and reports, which can be applied to various medical tasks. It highlights the challenges in evaluating MedVLP methods due to inconsistencies in datasets and implementations. To address this, the authors propose BenchX, a benchmark framework that standardizes evaluation across multiple MedVLP methods using public chest X-ray datasets. BenchX includes comprehensive datasets, standardized preprocessing, and unified finetuning protocols, allowing for systematic comparisons and improved performance assessments of MedVLP techniques.'}, 'zh': {'title': 'ç»Ÿä¸€åŸºå‡†ï¼Œæå‡åŒ»ç–—è§†è§‰è¯­è¨€é¢„è®­ç»ƒçš„æ¯”è¾ƒä¸è¯„ä¼°', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºBenchXçš„ç»Ÿä¸€åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°åŒ»ç–—è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆMedVLPï¼‰æ–¹æ³•çš„æ€§èƒ½ã€‚BenchXåŒ…å«ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼šæ¶µç›–ä¹ä¸ªæ•°æ®é›†å’Œå››ä¸ªåŒ»ç–—ä»»åŠ¡çš„ç»¼åˆæ•°æ®é›†ã€æ ‡å‡†åŒ–çš„æ•°æ®é¢„å¤„ç†å’Œè®­ç»ƒæµ‹è¯•åˆ’åˆ†çš„åŸºå‡†å¥—ä»¶ï¼Œä»¥åŠé€‚åº”ä¸åŒMedVLPæ–¹æ³•çš„ä¸€è‡´å¾®è°ƒåè®®ã€‚é€šè¿‡ä½¿ç”¨BenchXï¼Œæˆ‘ä»¬ä¸ºä¹ç§æœ€å…ˆè¿›çš„MedVLPæ–¹æ³•å»ºç«‹äº†åŸºçº¿ï¼Œå¹¶å‘ç°ä¸€äº›æ—©æœŸçš„MedVLPæ–¹æ³•çš„æ€§èƒ½å¯ä»¥æå‡ï¼Œè¶…è¿‡æ›´è¿‘æœŸçš„æ–¹æ³•ã€‚è¿™ä¸€å‘ç°ä¿ƒä½¿æˆ‘ä»¬é‡æ–°å®¡è§†MedVLPé¢†åŸŸçš„ç ”ç©¶è¿›å±•å’Œç»“è®ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.21666', 'title': 'Minimum Entropy Coupling with Bottleneck', 'url': 'https://huggingface.co/papers/2410.21666', 'abstract': 'This paper investigates a novel lossy compression framework operating under logarithmic loss, designed to handle situations where the reconstruction distribution diverges from the source distribution. This framework is especially relevant for applications that require joint compression and retrieval, and in scenarios involving distributional shifts due to processing. We show that the proposed formulation extends the classical minimum entropy coupling framework by integrating a bottleneck, allowing for a controlled degree of stochasticity in the coupling. We explore the decomposition of the Minimum Entropy Coupling with Bottleneck (MEC-B) into two distinct optimization problems: Entropy-Bounded Information Maximization (EBIM) for the encoder, and Minimum Entropy Coupling (MEC) for the decoder. Through extensive analysis, we provide a greedy algorithm for EBIM with guaranteed performance, and characterize the optimal solution near functional mappings, yielding significant theoretical insights into the structural complexity of this problem. Furthermore, we illustrate the practical application of MEC-B through experiments in Markov Coding Games (MCGs) under rate limits. These games simulate a communication scenario within a Markov Decision Process, where an agent must transmit a compressed message from a sender to a receiver through its actions. Our experiments highlight the trade-offs between MDP rewards and receiver accuracy across various compression rates, showcasing the efficacy of our method compared to conventional compression baseline.', 'score': 4, 'issue_id': 379, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 29', 'zh': '10æœˆ29æ—¥'}, 'hash': '9e1b09f8eff55094', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rl', '#benchmark', '#optimization', '#math', '#data', '#training', '#games'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ÑƒÑ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞµ 'Ğ±ÑƒÑ‚Ñ‹Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ³Ğ¾Ñ€Ğ»Ñ‹ÑˆĞºĞ¾' Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ´Ğ²Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ (EBIM) Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¹Ğ½Ğ¾Ğµ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ (MEC) Ğ´Ğ»Ñ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°. ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ñ… Ğ¸Ğ³Ñ€ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (MCG) Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."}, 'en': {'title': 'Revolutionizing Compression with Controlled Stochasticity', 'desc': 'This paper presents a new lossy compression framework that uses logarithmic loss to effectively manage cases where the output distribution differs from the original input distribution. It is particularly useful for tasks that involve both compression and retrieval, especially when there are changes in the data distribution. The authors enhance the traditional minimum entropy coupling method by adding a bottleneck, which allows for a controlled level of randomness in the data coupling process. They break down the new framework, called Minimum Entropy Coupling with Bottleneck (MEC-B), into two optimization tasks: one for maximizing information during encoding and another for minimizing entropy during decoding, providing a greedy algorithm with proven performance.'}, 'zh': {'title': 'æ–°å‹æœ‰æŸå‹ç¼©æ¡†æ¶ï¼šæ§åˆ¶éšæœºæ€§çš„æœ€å°ç†µè€¦åˆ', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§æ–°é¢–çš„æœ‰æŸå‹ç¼©æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å¯¹æ•°æŸå¤±ä¸‹è¿è¡Œï¼Œæ—¨åœ¨å¤„ç†é‡å»ºåˆ†å¸ƒä¸æºåˆ†å¸ƒä¸ä¸€è‡´çš„æƒ…å†µã€‚è¯¥æ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºéœ€è¦è”åˆå‹ç¼©å’Œæ£€ç´¢çš„åº”ç”¨ï¼Œä»¥åŠç”±äºå¤„ç†å¯¼è‡´çš„åˆ†å¸ƒå˜åŒ–åœºæ™¯ã€‚æˆ‘ä»¬å±•ç¤ºäº†æ‰€æå‡ºçš„å…¬å¼å¦‚ä½•é€šè¿‡é›†æˆç“¶é¢ˆæ‰©å±•ç»å…¸çš„æœ€å°ç†µè€¦åˆæ¡†æ¶ï¼Œä»è€Œåœ¨è€¦åˆä¸­å…è®¸æ§åˆ¶çš„éšæœºæ€§ç¨‹åº¦ã€‚é€šè¿‡å¯¹æœ€å°ç†µè€¦åˆä¸ç“¶é¢ˆï¼ˆMEC-Bï¼‰çš„åˆ†è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ç¼–ç å™¨çš„ç†µçº¦æŸä¿¡æ¯æœ€å¤§åŒ–ï¼ˆEBIMï¼‰å’Œè§£ç å™¨çš„æœ€å°ç†µè€¦åˆï¼ˆMECï¼‰ä¸¤ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æä¾›äº†ä¿è¯æ€§èƒ½çš„è´ªå©ªç®—æ³•ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.24218', 'title': 'Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use', 'url': 'https://huggingface.co/papers/2410.24218', 'abstract': "In real-world scenarios, it is desirable for embodied agents to have the ability to leverage human language to gain explicit or implicit knowledge for learning tasks. Despite recent progress, most previous approaches adopt simple low-level instructions as language inputs, which may not reflect natural human communication. It's not clear how to incorporate rich language use to facilitate task learning. To address this question, this paper studies different types of language inputs in facilitating reinforcement learning (RL) embodied agents. More specifically, we examine how different levels of language informativeness (i.e., feedback on past behaviors and future guidance) and diversity (i.e., variation of language expressions) impact agent learning and inference. Our empirical results based on four RL benchmarks demonstrate that agents trained with diverse and informative language feedback can achieve enhanced generalization and fast adaptation to new tasks. These findings highlight the pivotal role of language use in teaching embodied agents new tasks in an open world. Project website: https://github.com/sled-group/Teachable_RL", 'score': 4, 'issue_id': 375, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '3f784be6816fb87a', 'authors': ['Jiajun Xi', 'Yinong He', 'Jianing Yang', 'Yinpei Dai', 'Joyce Chai'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24218.jpg', 'data': {'categories': ['#reasoning', '#rl', '#rlhf', '#benchmark', '#transfer_learning', '#games', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¯Ğ·Ñ‹Ğº ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ° Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼.'}, 'en': {'title': 'Empowering Agents with Rich Language for Smarter Learning', 'desc': 'This paper explores how embodied agents can use human language to improve their learning in real-world tasks. It focuses on the impact of different types of language inputs, specifically looking at how informative and diverse language can enhance reinforcement learning (RL) processes. The study shows that agents receiving varied and detailed language feedback perform better in adapting to new tasks and generalizing their knowledge. These findings emphasize the importance of natural language communication in training intelligent agents to operate effectively in dynamic environments.'}, 'zh': {'title': 'è¯­è¨€åŠ©åŠ›æ™ºèƒ½ä½“å­¦ä¹ æ–°ä»»åŠ¡', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨äººç±»è¯­è¨€æ¥å¸®åŠ©å…·èº«æ™ºèƒ½ä½“å­¦ä¹ ä»»åŠ¡ã€‚æˆ‘ä»¬æ¢è®¨äº†ä¸åŒç±»å‹çš„è¯­è¨€è¾“å…¥å¯¹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯è¯­è¨€ä¿¡æ¯é‡å’Œå¤šæ ·æ€§å¦‚ä½•å½±å“å­¦ä¹ æ•ˆæœã€‚å®éªŒè¯æ˜ï¼Œä½¿ç”¨å¤šæ ·ä¸”ä¿¡æ¯ä¸°å¯Œçš„è¯­è¨€åé¦ˆè®­ç»ƒçš„æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ–°ä»»åŠ¡å¹¶æé«˜æ³›åŒ–èƒ½åŠ›ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†è¯­è¨€åœ¨æ•™å¯¼å…·èº«æ™ºèƒ½ä½“æ–°ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2410.23825', 'title': 'GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages', 'url': 'https://huggingface.co/papers/2410.23825', 'abstract': 'The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it - including the pipeline, language identification model, and filters - available to the research community. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1, Pipeline v. 3.0 https://github.com/cisnlp/GlotCC.', 'score': 3, 'issue_id': 375, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 31', 'zh': '10æœˆ31æ—¥'}, 'hash': '7d1bf1bef5a430e0', 'authors': ['Amir Hossein Kargaran', 'FranÃ§ois Yvon', 'Hinrich SchÃ¼tze'], 'affiliations': ['LMU Munich & Munich Center for Machine Learning, Munich, Germany', 'Sorbonne UniversitÃ© & CNRS, ISIR, Paris, France'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23825.jpg', 'data': {'categories': ['#multilingual', '#data', '#dataset', '#open_source', '#low_resource'], 'emoji': 'ğŸŒ', 'ru': {'title': 'GlotCC: ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GlotCC - Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 2 Ğ¢Ğ‘, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 1000 ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ¸Ğ½Ğ¾Ñ€Ğ¸Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğµ. ĞšĞ¾Ñ€Ğ¿ÑƒÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ CommonCrawl Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. GlotCC Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½ Ğ¾Ñ‚ ÑˆÑƒĞ¼Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞ³Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ¾Ñ€Ğ¿ÑƒÑÑƒ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ ĞµĞ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Empowering Minority Languages with GlotCC: A Clean, Large-Scale Corpus', 'desc': 'This paper introduces GlotCC, a large and clean text corpus designed for minority languages, addressing the gap in available data for these languages in the context of pretrained language models. The corpus is derived from CommonCrawl and spans over 2TB, covering more than 1000 languages, making it a valuable resource for researchers. The authors emphasize the importance of having a reproducible and open-source pipeline for generating such corpora, which includes a language identification model and noise filters. By providing GlotCC and its generation system to the research community, the authors aim to enhance the development of language models for underrepresented languages.'}, 'zh': {'title': 'GlotCCï¼šå¤šè¯­è¨€ç ”ç©¶çš„æ–°èµ„æº', 'desc': 'éšç€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œå¯¹å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“çš„éœ€æ±‚ä¸æ–­å¢åŠ ã€‚ç°æœ‰çš„è¯­æ–™åº“ä¸»è¦é›†ä¸­åœ¨å¤§å‹è¯­è¨€ç¤¾åŒºï¼Œç¼ºä¹è¦†ç›–å¹¿æ³›å°‘æ•°è¯­è¨€çš„èµ„æºã€‚æˆ‘ä»¬æå‡ºäº†GlotCCï¼Œè¿™æ˜¯ä¸€ä¸ªå¹²å‡€çš„ã€æ–‡æ¡£çº§çš„2TBé€šç”¨è¯­æ–™åº“ï¼Œæ¥æºäºCommonCrawlï¼Œæ¶µç›–äº†1000å¤šç§è¯­è¨€ã€‚æˆ‘ä»¬å°†GlotCCåŠå…¶ç”Ÿæˆç³»ç»Ÿï¼ˆåŒ…æ‹¬ç®¡é“ã€è¯­è¨€è¯†åˆ«æ¨¡å‹å’Œè¿‡æ»¤å™¨ï¼‰æä¾›ç»™ç ”ç©¶ç¤¾åŒºï¼Œä»¥ä¿ƒè¿›å¤šè¯­è¨€ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10958', 'title': 'SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration', 'url': 'https://huggingface.co/papers/2411.10958', 'abstract': 'Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. SageAttention utilizes 8-bit matrix multiplication, 16-bit matrix multiplication with 16-bit accumulator, and precision-enhancing methods, implementing an accurate and 2x speedup kernel compared to FlashAttention2. To further enhance the efficiency of attention computation while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrixes (Q, K) to INT4 in a warp-level granularity and quantize matrixes (widetilde P, V) to FP8. Second, we propose a method to smooth Q and V, enhancing the accuracy of attention with INT4 QK and FP8 PV. Third, we analyze the quantization accuracy across timesteps and layers, then propose an adaptive quantization method to ensure the end-to-end metrics over various models. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention.', 'score': 33, 'issue_id': 697, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 17', 'zh': '11æœˆ17æ—¥'}, 'hash': '3500eddd4100341a', 'authors': ['Jintao Zhang', 'Haofeng Huang', 'Pengle Zhang', 'Jia Wei', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10958.jpg', 'data': {'categories': ['#video', '#inference', '#training', '#optimization', '#cv'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½ĞµĞµ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'SageAttention2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Q Ğ¸ K Ğ´Ğ¾ INT4, Ğ° Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† P Ğ¸ V Ğ´Ğ¾ FP8, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ³Ğ»Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Q Ğ¸ V Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. SageAttention2 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ FlashAttention2 Ğ¸ xformers Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 3 Ğ¸ 5 Ñ€Ğ°Ğ· ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° RTX4090.'}, 'en': {'title': 'Accelerating Attention with Precision: SageAttention2', 'desc': 'This paper introduces SageAttention, a novel approach to improve the efficiency of attention mechanisms in machine learning models by utilizing quantization techniques. It employs 8-bit and 16-bit matrix multiplications, achieving a 2x speedup over existing methods like FlashAttention2. The follow-up, SageAttention2, further enhances performance by implementing 4-bit matrix multiplication while maintaining accuracy through advanced precision-enhancing methods. The results demonstrate significant improvements in operations per second (OPS) and minimal loss in performance across various model types, including those used for language processing and image generation.'}, 'zh': {'title': 'æå‡æ³¨æ„åŠ›è®¡ç®—æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶SageAttention2ï¼Œæ—¨åœ¨æé«˜è®¡ç®—æ•ˆç‡å¹¶ä¿æŒç²¾åº¦ã€‚é€šè¿‡ä½¿ç”¨4ä½çŸ©é˜µä¹˜æ³•å’Œå…¶ä»–ç²¾åº¦å¢å¼ºæŠ€æœ¯ï¼ŒSageAttention2åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è‡ªé€‚åº”é‡åŒ–æ–¹æ³•ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„ç«¯åˆ°ç«¯æŒ‡æ ‡ä¿æŒç¨³å®šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSageAttention2åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è¯­è¨€å¤„ç†å’Œå›¾åƒç”Ÿæˆæ–¹é¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.13503', 'title': 'VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models', 'url': 'https://huggingface.co/papers/2411.13503', 'abstract': 'Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects "video generation quality" into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has several appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). The evaluation metrics with fine-grained levels reveal individual models\' strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks\' alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models\' ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models. 4) Versatile Benchmarking: VBench++ supports evaluating text-to-video and image-to-video. We introduce a high-quality Image Suite with an adaptive aspect ratio to enable fair evaluations across different image-to-video generation settings. Beyond assessing technical quality, VBench++ evaluates the trustworthiness of video generative models, providing a more holistic view of model performance. 5) Full Open-Sourcing: We fully open-source VBench++ and continually add new video generation models to our leaderboard to drive forward the field of video generation.', 'score': 23, 'issue_id': 700, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'b7c2e10d1b01293e', 'authors': ['Ziqi Huang', 'Fan Zhang', 'Xiaojie Xu', 'Yinan He', 'Jiashuo Yu', 'Ziyue Dong', 'Qianli Ma', 'Nattapol Chanpaisit', 'Chenyang Si', 'Yuming Jiang', 'Yaohui Wang', 'Xinyuan Chen', 'Ying-Cong Chen', 'Limin Wang', 'Dahua Lin', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Hong Kong University of Science and Technology', 'Nanjing University', 'Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2411.13503.jpg', 'data': {'categories': ['#open_source', '#alignment', '#benchmark', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VBench: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'VBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° 16 Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹, ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. VBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ insights Ğ¾ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ ĞºĞ°Ğº Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'VBench: A Comprehensive Benchmark for Evaluating Video Generation Models', 'desc': 'This paper introduces VBench, a new evaluation benchmark for video generation models that addresses the shortcomings of existing metrics. VBench breaks down video generation quality into 16 specific dimensions, such as motion smoothness and subject identity inconsistency, allowing for a detailed analysis of model performance. It includes human preference annotations to ensure that the evaluation aligns with human perceptions, providing valuable insights into the strengths and weaknesses of different models. Additionally, VBench++ supports various types of video generation, including text-to-video and image-to-video, and is fully open-sourced to encourage ongoing improvements in the field.'}, 'zh': {'title': 'VBenchï¼šè§†é¢‘ç”Ÿæˆè¯„ä¼°çš„æ–°æ ‡å‡†', 'desc': 'è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¯„ä¼°è¿™äº›æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VBenchï¼Œä¸€ä¸ªå…¨é¢çš„è¯„ä¼°åŸºå‡†ï¼Œèƒ½å¤Ÿå°†â€œè§†é¢‘ç”Ÿæˆè´¨é‡â€åˆ†è§£ä¸ºå…·ä½“çš„ã€åˆ†å±‚çš„å’Œè§£è€¦çš„ç»´åº¦ã€‚VBenchåŒ…å«16ä¸ªè§†é¢‘ç”Ÿæˆç»´åº¦ï¼Œå¹¶æä¾›ä¸äººç±»æ„ŸçŸ¥ä¸€è‡´çš„è¯„ä¼°æ–¹æ³•ï¼Œå¸®åŠ©è¯†åˆ«æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚é€šè¿‡å…¨é¢çš„è¯„ä¼°ï¼ŒVBenchä¸ä»…è¯„ä¼°æŠ€æœ¯è´¨é‡ï¼Œè¿˜å…³æ³¨è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¯ä¿¡åº¦ï¼Œæ¨åŠ¨è§†é¢‘ç”Ÿæˆé¢†åŸŸçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.13281', 'title': 'VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation', 'url': 'https://huggingface.co/papers/2411.13281', 'abstract': "Large multimodal models (LMMs) with advanced video analysis capabilities have recently garnered significant attention. However, most evaluations rely on traditional methods like multiple-choice questions in benchmarks such as VideoMME and LongVideoBench, which are prone to lack the depth needed to capture the complex demands of real-world users. To address this limitation-and due to the prohibitive cost and slow pace of human annotation for video tasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS Chatbot Arena's framework, designed to automatically assess LMMs' video analysis abilities. VideoAutoArena utilizes user simulation to generate open-ended, adaptive questions that rigorously assess model performance in video understanding. The benchmark features an automated, scalable evaluation framework, incorporating a modified ELO Rating System for fair and continuous comparisons across multiple LMMs. To validate our automated judging system, we construct a 'gold standard' using a carefully curated subset of human annotations, demonstrating that our arena strongly aligns with human judgment while maintaining scalability. Additionally, we introduce a fault-driven evolution strategy, progressively increasing question complexity to push models toward handling more challenging video analysis scenarios. Experimental results demonstrate that VideoAutoArena effectively differentiates among state-of-the-art LMMs, providing insights into model strengths and areas for improvement. To further streamline our evaluation, we introduce VideoAutoBench as an auxiliary benchmark, where human annotators label winners in a subset of VideoAutoArena battles. We use GPT-4o as a judge to compare responses against these human-validated answers. Together, VideoAutoArena and VideoAutoBench offer a cost-effective, and scalable framework for evaluating LMMs in user-centric video analysis.", 'score': 15, 'issue_id': 702, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'e8e997ff5dc00260', 'authors': ['Ziyang Luo', 'Haoning Wu', 'Dongxu Li', 'Jing Ma', 'Mohan Kankanhalli', 'Junnan Li'], 'affiliations': ['Hong Kong Baptist University', 'National University of Singapore', 'Rhymes AI'], 'pdf_title_img': 'assets/pdf/title_img/2411.13281.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#optimization', '#benchmark', '#video', '#games'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ€ĞµĞ½Ğ° Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ VideoAutoArena - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², VideoAutoArena Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° ELO Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ VideoAutoBench - Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ GPT-4 Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸, Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ»ÑĞ´ÑŒĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Video Analysis Evaluation with VideoAutoArena', 'desc': 'This paper presents VideoAutoArena, a new benchmark for evaluating large multimodal models (LMMs) in video analysis. Unlike traditional evaluation methods that use multiple-choice questions, VideoAutoArena employs user simulation to create open-ended, adaptive questions that better reflect real-world user needs. The framework includes an automated judging system based on a modified ELO Rating System, allowing for continuous and fair comparisons among various LMMs. Additionally, the paper introduces VideoAutoBench, which incorporates human annotations to validate the automated assessments, ensuring that the evaluation process is both scalable and aligned with human judgment.'}, 'zh': {'title': 'VideoAutoArenaï¼šè§†é¢‘åˆ†æçš„æ–°è¯„ä¼°æ ‡å‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è§†é¢‘åˆ†æåŸºå‡†æµ‹è¯•å·¥å…·ï¼Œåä¸ºVideoAutoArenaï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„å¤šé¡¹é€‰æ‹©é¢˜è¯„ä¼°æ–¹æ³•ä¸åŒï¼ŒVideoAutoArenaé€šè¿‡ç”¨æˆ·æ¨¡æ‹Ÿç”Ÿæˆå¼€æ”¾å¼å’Œè‡ªé€‚åº”çš„é—®é¢˜ï¼Œä»¥æ›´æ·±å…¥åœ°æµ‹è¯•æ¨¡å‹åœ¨è§†é¢‘ç†è§£æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†é‡‡ç”¨è‡ªåŠ¨åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶ç»“åˆä¿®æ”¹åçš„ELOè¯„åˆ†ç³»ç»Ÿï¼Œå®ç°å¯¹å¤šä¸ªLMMsçš„å…¬å¹³æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideoAutoArenaèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ä¸åŒçš„LMMsï¼Œå¹¶æä¾›æœ‰å…³æ¨¡å‹ä¼˜ç¼ºç‚¹çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.11922', 'title': 'SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory', 'url': 'https://huggingface.co/papers/2411.11922', 'abstract': 'The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of memories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incorporating temporal motion cues with the proposed motion-aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, showcasing its ability to generalize without fine-tuning. In evaluations, SAMURAI achieves significant improvements in success rate and precision over existing trackers, with a 7.1% AUC gain on LaSOT_{ext} and a 3.5% AO gain on GOT-10k. Moreover, it achieves competitive results compared to fully supervised methods on LaSOT, underscoring its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments. Code and results are available at https://github.com/yangchris11/samurai.', 'score': 12, 'issue_id': 697, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 18', 'zh': '11æœˆ18æ—¥'}, 'hash': '8b3456a33d980c50', 'authors': ['Cheng-Yen Yang', 'Hsiang-Wei Huang', 'Wenhao Chai', 'Zhongyu Jiang', 'Jenq-Neng Hwang'], 'affiliations': ['University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.11922.jpg', 'data': {'categories': ['#transfer_learning', '#video', '#benchmark', '#optimization', '#cv'], 'emoji': 'ğŸ¥·', 'ru': {'title': 'SAMURAI: Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'SAMURAI - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ SAM 2, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ°ÑĞ¾Ğº. SAMURAI Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ zero-shot Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑÑ… ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞºĞµÑ€Ğ°Ğ¼Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'SAMURAI: Real-Time Tracking with Motion-Aware Memory', 'desc': 'The Segment Anything Model 2 (SAM 2) excels in object segmentation but struggles with visual object tracking in crowded and dynamic scenes. This paper presents SAMURAI, an improved version of SAM 2 that integrates temporal motion cues and a motion-aware memory selection mechanism to enhance tracking accuracy. SAMURAI effectively predicts object motion and refines mask selection, allowing it to operate in real-time without the need for retraining. The model shows significant performance improvements over existing trackers, achieving higher success rates and precision on various benchmark datasets, making it suitable for real-world applications.'}, 'zh': {'title': 'SAMURAIï¼šå®æ—¶è§†è§‰ç›®æ ‡è·Ÿè¸ªçš„æ–°çªç ´', 'desc': 'SAMURAIæ˜¯å¯¹Segment Anything Model 2ï¼ˆSAM 2ï¼‰çš„å¢å¼ºç‰ˆæœ¬ï¼Œä¸“é—¨ç”¨äºè§†è§‰ç›®æ ‡è·Ÿè¸ªã€‚å®ƒé€šè¿‡å¼•å…¥æ—¶é—´è¿åŠ¨çº¿ç´¢å’Œè¿åŠ¨æ„ŸçŸ¥è®°å¿†é€‰æ‹©æœºåˆ¶ï¼Œè§£å†³äº†åœ¨æ‹¥æŒ¤åœºæ™¯ä¸­å¿«é€Ÿç§»åŠ¨æˆ–è‡ªé®æŒ¡ç‰©ä½“çš„è·Ÿè¸ªæŒ‘æˆ˜ã€‚ä¸åŸå§‹æ¨¡å‹çš„å›ºå®šçª—å£è®°å¿†æ–¹æ³•ä¸åŒï¼ŒSAMURAIèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹ç‰©ä½“è¿åŠ¨å¹¶ä¼˜åŒ–æ©è†œé€‰æ‹©ï¼Œä»è€Œå®ç°å®æ—¶ã€å‡†ç¡®çš„è·Ÿè¸ªã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒSAMURAIåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒæˆåŠŸç‡å’Œç²¾åº¦æ˜¾è‘—æé«˜ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­çš„åº”ç”¨æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06559', 'title': 'Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents', 'url': 'https://huggingface.co/papers/2411.06559', 'abstract': 'Language agents have demonstrated promising capabilities in automating web-based tasks, though their current reactive approaches still underperform largely compared to humans. While incorporating advanced planning algorithms, particularly tree search methods, could enhance these agents\' performance, implementing tree search directly on live websites poses significant safety risks and practical constraints due to irreversible actions such as confirming a purchase. In this paper, we introduce a novel paradigm that augments language agents with model-based planning, pioneering the innovative use of large language models (LLMs) as world models in complex web environments. Our method, WebDreamer, builds on the key insight that LLMs inherently encode comprehensive knowledge about website structures and functionalities. Specifically, WebDreamer uses LLMs to simulate outcomes for each candidate action (e.g., "what would happen if I click this button?") using natural language descriptions, and then evaluates these imagined outcomes to determine the optimal action at each step. Empirical results on two representative web agent benchmarks with online interaction -- VisualWebArena and Mind2Web-live -- demonstrate that WebDreamer achieves substantial improvements over reactive baselines. By establishing the viability of LLMs as world models in web environments, this work lays the groundwork for a paradigm shift in automated web interaction. More broadly, our findings open exciting new avenues for future research into 1) optimizing LLMs specifically for world modeling in complex, dynamic environments, and 2) model-based speculative planning for language agents.', 'score': 9, 'issue_id': 698, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '6ce3ac73a0235409', 'authors': ['Yu Gu', 'Boyuan Zheng', 'Boyu Gou', 'Kai Zhang', 'Cheng Chang', 'Sanjari Srivastava', 'Yanan Xie', 'Peng Qi', 'Huan Sun', 'Yu Su'], 'affiliations': ['Orby AI', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2411.06559.jpg', 'data': {'categories': ['#architecture', '#agents', '#optimization', '#benchmark'], 'emoji': 'ğŸŒ', 'ru': {'title': 'WebDreamer: LLM ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ WebDreamer Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²ĞµĞ±-Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ¸Ñ€Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ°Ñ…. WebDreamer Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ»Ğ¸Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Web Agents with Model-Based Planning', 'desc': "This paper presents a new approach called WebDreamer that enhances language agents' ability to perform web-based tasks by integrating model-based planning with large language models (LLMs). Unlike traditional reactive methods, WebDreamer allows agents to simulate potential outcomes of actions in a web environment, helping them to make better decisions. By leveraging the inherent knowledge of LLMs about website structures, the agents can evaluate different actions and choose the most effective one. The results show that WebDreamer significantly outperforms existing reactive methods in two benchmark tests, indicating a promising direction for future research in automated web interaction."}, 'zh': {'title': 'ç”¨æ¨¡å‹é©±åŠ¨è§„åˆ’æå‡è¯­è¨€ä»£ç†çš„èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„èŒƒå¼ï¼Œé€šè¿‡æ¨¡å‹é©±åŠ¨çš„è§„åˆ’å¢å¼ºè¯­è¨€ä»£ç†çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„ç½‘ç»œç¯å¢ƒä¸­ã€‚æˆ‘ä»¬æå‡ºçš„WebDreameræ–¹æ³•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºä¸–ç•Œæ¨¡å‹ï¼Œæ¨¡æ‹Ÿæ¯ä¸ªå€™é€‰åŠ¨ä½œçš„ç»“æœï¼Œå¹¶è¯„ä¼°è¿™äº›ç»“æœä»¥ç¡®å®šæœ€ä½³è¡ŒåŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWebDreameråœ¨ä¸¤ä¸ªä»£è¡¨æ€§çš„ç½‘ç»œä»£ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ååº”å¼æ–¹æ³•ã€‚æ­¤ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–ç½‘ç»œäº¤äº’çš„èŒƒå¼è½¬å˜å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.13476', 'title': 'When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training', 'url': 'https://huggingface.co/papers/2411.13476', 'abstract': "Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16's limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\% compared to standard full attention mechanisms, while preserving the original LLM's capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.", 'score': 6, 'issue_id': 704, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'e463af90b96a6aa1', 'authors': ['Haonan Wang', 'Qian Liu', 'Chao Du', 'Tongyao Zhu', 'Cunxiao Du', 'Kenji Kawaguchi', 'Tianyu Pang'], 'affiliations': ['National University of Singapore', 'SSea AI Lab, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.13476.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#long_context'], 'emoji': 'âš“', 'ru': {'title': 'AnchorAttention: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ AnchorAttention Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Rotary Positional Embedding (RoPE) Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ¼ BFloat16, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. AnchorAttention Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ ĞºĞ°Ğº Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ÑĞºĞ¾Ñ€ÑŒ Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AnchorAttention Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'AnchorAttention: Enhancing Long-Context Performance in LLMs', 'desc': "This paper introduces AnchorAttention, a new attention mechanism designed to improve the performance of large language models (LLMs) when processing long sequences. The authors identify that using Rotary Positional Embedding (RoPE) with BFloat16 format leads to numerical issues, particularly affecting relative positional encoding in long-context scenarios. AnchorAttention addresses these issues by treating the first token as a shared anchor, which enhances computational efficiency and maintains semantic coherence. Experimental results show that this method significantly boosts long-context capabilities and reduces training time by over 50% without compromising the model's overall performance."}, 'zh': {'title': 'AnchorAttentionï¼šæå‡é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºAnchorAttentionï¼Œæ—¨åœ¨è§£å†³ä½¿ç”¨BFloat16æ ¼å¼æ—¶çš„æ•°å€¼é—®é¢˜ã€‚ä¼ ç»Ÿçš„æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡è®­ç»ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨BFloat16æ ¼å¼ä¸‹ä¼šå‡ºç°ç²¾åº¦ä¸è¶³çš„é—®é¢˜ï¼Œå½±å“ç›¸å¯¹ä½ç½®ç¼–ç çš„æ•ˆæœã€‚AnchorAttentioné€šè¿‡å°†ç¬¬ä¸€ä¸ªtokenè§†ä¸ºå…±äº«é”šç‚¹ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œæé«˜äº†é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œå¹¶åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒAnchorAttentionåœ¨é•¿ä¸Šä¸‹æ–‡æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†çš„å…¨æ³¨æ„åŠ›æœºåˆ¶ï¼ŒåŒæ—¶ä¿æŒäº†åŸæœ‰å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.12811', 'title': 'Stylecodes: Encoding Stylistic Information For Image Generation', 'url': 'https://huggingface.co/papers/2411.12811', 'abstract': 'Diffusion models excel in image generation, but controlling them remains a challenge. We focus on the problem of style-conditioned image generation. Although example images work, they are cumbersome: srefs (style-reference codes) from MidJourney solve this issue by expressing a specific image style in a short numeric code. These have seen widespread adoption throughout social media due to both their ease of sharing and the fact they allow using an image for style control, without having to post the source images themselves. However, users are not able to generate srefs from their own images, nor is the underlying training procedure public. We propose StyleCodes: an open-source and open-research style encoder architecture and training procedure to express image style as a 20-symbol base64 code. Our experiments show that our encoding results in minimal loss in quality compared to traditional image-to-style techniques.', 'score': 5, 'issue_id': 706, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '55d7513a0c99c789', 'authors': ['Ciara Rowles'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.12811.jpg', 'data': {'categories': ['#cv', '#architecture', '#open_source', '#diffusion', '#dataset'], 'emoji': 'ğŸ¨', 'ru': {'title': 'StyleCodes: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ ÑÑ‚Ğ¸Ğ»ĞµĞ¼ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ StyleCodes - ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ ÑÑ‚Ğ¸Ğ»Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ´ Ğ¸Ğ· 20 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾ Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒÑÑ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¸ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ StyleCodes ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğ¸ ÑÑ‚Ğ¸Ğ»Ñ Ğ¾Ñ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unlocking Style Control in Image Generation with StyleCodes', 'desc': 'This paper addresses the challenge of controlling diffusion models for style-conditioned image generation. It introduces StyleCodes, an open-source architecture that encodes image styles into a compact 20-symbol base64 code, making it easier for users to generate styles from their own images. The proposed method allows for efficient style sharing without the need to post original images, enhancing usability in social media contexts. Experimental results indicate that StyleCodes maintain high image quality, comparable to traditional methods of style encoding.'}, 'zh': {'title': 'é£æ ¼ç¼–ç ï¼Œè½»æ¾ç”Ÿæˆå›¾åƒé£æ ¼ï¼', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æ§åˆ¶è¿™äº›æ¨¡å‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬å…³æ³¨é£æ ¼æ¡ä»¶çš„å›¾åƒç”Ÿæˆé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºStyleCodesçš„å¼€æ”¾æºä»£ç é£æ ¼ç¼–ç å™¨æ¶æ„ã€‚StyleCodesèƒ½å¤Ÿå°†å›¾åƒé£æ ¼è¡¨è¾¾ä¸ºä¸€ä¸ª20ä¸ªç¬¦å·çš„base64ä»£ç ï¼Œç”¨æˆ·å¯ä»¥æ–¹ä¾¿åœ°ç”Ÿæˆè‡ªå·±çš„é£æ ¼å‚è€ƒä»£ç ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„å›¾åƒåˆ°é£æ ¼æŠ€æœ¯ç›¸æ¯”ï¼Œè¿™ç§ç¼–ç æ–¹æ³•åœ¨è´¨é‡ä¸Šå‡ ä¹æ²¡æœ‰æŸå¤±ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.12925', 'title': 'Loss-to-Loss Prediction: Scaling Laws for All Datasets', 'url': 'https://huggingface.co/papers/2411.12925', 'abstract': 'While scaling laws provide a reliable methodology for predicting train loss across compute scales for a single data distribution, less is known about how these predictions should change as we change the distribution. In this paper, we derive a strategy for predicting one loss from another and apply it to predict across different pre-training datasets and from pre-training data to downstream task data. Our predictions extrapolate well even at 20x the largest FLOP budget used to fit the curves. More precisely, we find that there are simple shifted power law relationships between (1) the train losses of two models trained on two separate datasets when the models are paired by training compute (train-to-train), (2) the train loss and the test loss on any downstream distribution for a single model (train-to-test), and (3) the test losses of two models trained on two separate train datasets (test-to-test). The results hold up for pre-training datasets that differ substantially (some are entirely code and others have no code at all) and across a variety of downstream tasks. Finally, we find that in some settings these shifted power law relationships can yield more accurate predictions than extrapolating single-dataset scaling laws.', 'score': 2, 'issue_id': 716, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 19', 'zh': '11æœˆ19æ—¥'}, 'hash': '2c87616b49396672', 'authors': ['David Brandfonbrener', 'Nikhil Anand', 'Nikhil Vyas', 'Eran Malach', 'Sham Kakade'], 'affiliations': ['Kempner Institute, Harvard University', 'SEAS, Harvard University'], 'pdf_title_img': 'assets/pdf/title_img/2411.12925.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#data', '#training', '#dataset'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ñ‹ Ğ´Ğ»Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Predicting Loss Across Data Distributions with Power Laws', 'desc': 'This paper explores how to predict training loss when changing data distributions in machine learning models. It introduces a method to derive relationships between losses from different datasets, allowing for effective predictions across pre-training and downstream tasks. The authors discover that simple shifted power law relationships exist between train losses of models trained on different datasets, as well as between train and test losses. Their findings suggest that these relationships can provide more accurate predictions than traditional scaling laws, even when datasets vary significantly.'}, 'zh': {'title': 'è·¨æ•°æ®åˆ†å¸ƒçš„æŸå¤±é¢„æµ‹æ–°ç­–ç•¥', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åœ¨ä¸åŒæ•°æ®åˆ†å¸ƒä¸‹é¢„æµ‹è®­ç»ƒæŸå¤±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç­–ç•¥ï¼Œå¯ä»¥ä»ä¸€ä¸ªæŸå¤±é¢„æµ‹å¦ä¸€ä¸ªæŸå¤±ï¼Œé€‚ç”¨äºä¸åŒçš„é¢„è®­ç»ƒæ•°æ®é›†å’Œä¸‹æ¸¸ä»»åŠ¡æ•°æ®ã€‚ç ”ç©¶å‘ç°ï¼Œè®­ç»ƒæŸå¤±ä¹‹é—´ã€è®­ç»ƒæŸå¤±ä¸æµ‹è¯•æŸå¤±ä¹‹é—´ã€ä»¥åŠæµ‹è¯•æŸå¤±ä¹‹é—´å­˜åœ¨ç®€å•çš„åç§»å¹‚å¾‹å…³ç³»ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™äº›åç§»å¹‚å¾‹å…³ç³»çš„é¢„æµ‹ç²¾åº¦ä¼˜äºå•ä¸€æ•°æ®é›†çš„æ‰©å±•æ³•åˆ™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.13025', 'title': 'ORID: Organ-Regional Information Driven Framework for Radiology Report Generation', 'url': 'https://huggingface.co/papers/2411.13025', 'abstract': 'The objective of Radiology Report Generation (RRG) is to automatically generate coherent textual analyses of diseases based on radiological images, thereby alleviating the workload of radiologists. Current AI-based methods for RRG primarily focus on modifications to the encoder-decoder model architecture. To advance these approaches, this paper introduces an Organ-Regional Information Driven (ORID) framework which can effectively integrate multi-modal information and reduce the influence of noise from unrelated organs. Specifically, based on the LLaVA-Med, we first construct an RRG-related instruction dataset to improve organ-regional diagnosis description ability and get the LLaVA-Med-RRG. After that, we propose an organ-based cross-modal fusion module to effectively combine the information from the organ-regional diagnosis description and radiology image. To further reduce the influence of noise from unrelated organs on the radiology report generation, we introduce an organ importance coefficient analysis module, which leverages Graph Neural Network (GNN) to examine the interconnections of the cross-modal information of each organ region. Extensive experiments an1d comparisons with state-of-the-art methods across various evaluation metrics demonstrate the superior performance of our proposed method.', 'score': 2, 'issue_id': 702, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 20', 'zh': '11æœˆ20æ—¥'}, 'hash': 'e62215e28a0bdfdb', 'authors': ['Tiancheng Gu', 'Kaicheng Yang', 'Xiang An', 'Ziyong Feng', 'Dongnan Liu', 'Weidong Cai'], 'affiliations': ['DeepGlint', 'University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2411.13025.jpg', 'data': {'categories': ['#multimodal', '#graphs', '#dataset', '#games', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ² Ñ Ñ„Ğ¾ĞºÑƒÑĞ¾Ğ¼ Ğ½Ğ° Ğ¾Ñ€Ğ³Ğ°Ğ½Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ ORID (Organ-Regional Information Driven). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ LLaVA-Med, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ². ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ± Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Radiology Reports with Organ-Regional Insights', 'desc': 'This paper presents a novel framework called Organ-Regional Information Driven (ORID) for improving Radiology Report Generation (RRG) by effectively integrating multi-modal data from radiological images and textual descriptions. The approach enhances the encoder-decoder model by introducing an organ-based cross-modal fusion module, which combines relevant information while minimizing noise from unrelated organs. Additionally, it employs a Graph Neural Network (GNN) to analyze the importance of different organ regions, ensuring that only the most relevant data influences the report generation. Experimental results show that the ORID framework outperforms existing state-of-the-art methods in generating coherent and accurate radiology reports.'}, 'zh': {'title': 'æå‡æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„æ™ºèƒ½åŒ–æ°´å¹³', 'desc': 'æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼ˆRRGï¼‰çš„ç›®æ ‡æ˜¯æ ¹æ®æ”¾å°„å›¾åƒè‡ªåŠ¨ç”Ÿæˆè¿è´¯çš„ç–¾ç—…åˆ†ææ–‡æœ¬ï¼Œä»è€Œå‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œè´Ÿæ‹…ã€‚ç›®å‰åŸºäºäººå·¥æ™ºèƒ½çš„RRGæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å¯¹ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ¶æ„çš„ä¿®æ”¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æœ‰æœºåŒºåŸŸä¿¡æ¯é©±åŠ¨ï¼ˆORIDï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•´åˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶å‡å°‘æ¥è‡ªæ— å…³å™¨å®˜çš„å™ªå£°å½±å“ã€‚é€šè¿‡æ„å»ºä¸RRGç›¸å…³çš„æŒ‡ä»¤æ•°æ®é›†å’Œå¼•å…¥åŸºäºå™¨å®˜çš„è·¨æ¨¡æ€èåˆæ¨¡å—ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10867', 'title': 'ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models', 'url': 'https://huggingface.co/papers/2411.10867', 'abstract': 'Latest developments in Large Multimodal Models (LMMs) have broadened their capabilities to include video understanding. Specifically, Text-to-video (T2V) models have made significant progress in quality, comprehension, and duration, excelling at creating videos from simple textual prompts. Yet, they still frequently produce hallucinated content that clearly signals the video is AI-generated. We introduce ViBe: a large-scale Text-to-Video Benchmark of hallucinated videos from T2V models. We identify five major types of hallucination: Vanishing Subject, Numeric Variability, Temporal Dysmorphia, Omission Error, and Physical Incongruity. Using 10 open-source T2V models, we developed the first large-scale dataset of hallucinated videos, comprising 3,782 videos annotated by humans into these five categories. ViBe offers a unique resource for evaluating the reliability of T2V models and provides a foundation for improving hallucination detection and mitigation in video generation. We establish classification as a baseline and present various ensemble classifier configurations, with the TimeSFormer + CNN combination yielding the best performance, achieving 0.345 accuracy and 0.342 F1 score. This benchmark aims to drive the development of robust T2V models that produce videos more accurately aligned with input prompts.', 'score': 1, 'issue_id': 715, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 16', 'zh': '11æœˆ16æ—¥'}, 'hash': 'be2a7eb4c0e51788', 'authors': ['Vipula Rawte', 'Sarthak Jain', 'Aarush Sinha', 'Garv Kaushik', 'Aman Bansal', 'Prathiksha Rumale Vishwanath', 'Samyak Rajesh Jain', 'Aishwarya Naresh Reganti', 'Vinija Jain', 'Aman Chadha', 'Amit P. Sheth', 'Amitava Das'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'Amazon Web Services, USA', 'Guru Gobind Singh Indraprastha University, India', 'Indian Institute of Technology (BHU), India', 'Meta, USA', 'University of California, Santa Cruz, USA', 'University of Massachusetts Amherst, USA', 'Vellore Institute of Technology, India'], 'pdf_title_img': 'assets/pdf/title_img/2411.10867.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#video', '#hallucinations', '#open_source', '#benchmark'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ViBe: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ´Ğ»Ñ Ğ±Ğ¾Ñ€ÑŒĞ±Ñ‹ Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ ViBe - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (T2V). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸Ğ»Ğ¸ Ğ¿ÑÑ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 3782 Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ T2V Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹. Ğ’ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ³Ğ´Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ TimeSFormer + CNN Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚.'}, 'en': {'title': 'ViBe: Benchmarking Hallucinations in Text-to-Video Models', 'desc': 'This paper presents ViBe, a benchmark designed to evaluate hallucinations in videos generated by Text-to-Video (T2V) models. It identifies five types of hallucinations that can occur in AI-generated videos, such as Vanishing Subject and Temporal Dysmorphia. The authors created a large dataset of 3,782 annotated videos from various T2V models to facilitate this evaluation. By establishing a baseline for classification and testing different ensemble classifiers, they aim to enhance the reliability of T2V models and reduce the occurrence of hallucinated content.'}, 'zh': {'title': 'æå‡æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„å¯é æ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ViBeï¼Œä¸€ä¸ªé’ˆå¯¹æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹ç”Ÿæˆçš„å¹»è§‰è§†é¢‘çš„å¤§è§„æ¨¡åŸºå‡†ã€‚æˆ‘ä»¬è¯†åˆ«äº†äº”ç§ä¸»è¦çš„å¹»è§‰ç±»å‹ï¼ŒåŒ…æ‹¬æ¶ˆå¤±ä¸»ä½“ã€æ•°å€¼å˜å¼‚ã€æ—¶é—´ç•¸å½¢ã€é—æ¼é”™è¯¯å’Œç‰©ç†ä¸ä¸€è‡´ã€‚é€šè¿‡ä½¿ç”¨10ä¸ªå¼€æºT2Væ¨¡å‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ç¬¬ä¸€ä¸ªåŒ…å«3782ä¸ªå¹»è§‰è§†é¢‘çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†äººå·¥æ ‡æ³¨ã€‚ViBeä¸ºè¯„ä¼°T2Væ¨¡å‹çš„å¯é æ€§æä¾›äº†ç‹¬ç‰¹çš„èµ„æºï¼Œå¹¶ä¸ºæ”¹å–„å¹»è§‰æ£€æµ‹å’Œç¼“è§£å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08147', 'title': 'Large Language Models Can Self-Improve in Long-context Reasoning', 'url': 'https://huggingface.co/papers/2411.08147', 'abstract': 'Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose SEALONG, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of SEALONG, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, SEALONG achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs.', 'score': 47, 'issue_id': 567, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': 'ea4232d9ddd5ef31', 'authors': ['Siheng Li', 'Cheng Yang', 'Zesen Cheng', 'Lemao Liu', 'Mo Yu', 'Yujiu Yang', 'Wai Lam'], 'affiliations': ['The Chinese University of Hong Kong', 'Peking University', 'Tsinghua University', 'Tencent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08147.jpg', 'data': {'categories': ['#training', '#synthetic', '#long_context', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLMs Ğ´Ğ»Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLMs) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ SEALONG, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¸ÑĞºĞ° Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SEALONG Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Llama-3.1-8B-Instruct.'}, 'en': {'title': 'Empowering LLMs to Self-Improve Long-Context Reasoning', 'desc': 'This paper addresses the challenges that large language models (LLMs) face in reasoning over long contexts. The authors propose a new method called SEALONG, which allows LLMs to enhance their long-context reasoning capabilities without relying on human-annotated data. Instead, the approach involves generating multiple outputs for each question, scoring them using Minimum Bayes Risk, and then fine-tuning the model based on these scores. Experimental results show that SEALONG significantly improves performance on Llama-3.1-8B-Instruct, outperforming previous methods that depend on expert-generated data.'}, 'zh': {'title': 'è®©å¤§å‹è¯­è¨€æ¨¡å‹è‡ªæˆ‘æå‡é•¿æ–‡æœ¬æ¨ç†èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é•¿æ–‡æœ¬æ¨ç†ä¸Šä»ç„¶å­˜åœ¨å›°éš¾ã€‚ç°æœ‰çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºäººå·¥ä¸“å®¶æˆ–å…ˆè¿›æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰æä¾›çš„åˆæˆæ•°æ®è¿›è¡Œå¾®è°ƒï¼Œè¿™é™åˆ¶äº†è¿›ä¸€æ­¥çš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSEALONGçš„æ–¹æ³•ï¼Œæ—¨åœ¨è®©LLMsåœ¨é•¿æ–‡æœ¬æ¨ç†ä¸­è‡ªæˆ‘æ”¹è¿›ã€‚é€šè¿‡å¯¹æ¯ä¸ªé—®é¢˜é‡‡æ ·å¤šä¸ªè¾“å‡ºï¼Œå¹¶ä½¿ç”¨æœ€å°è´å¶æ–¯é£é™©è¿›è¡Œè¯„åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºè¿™äº›è¾“å‡ºè¿›è¡Œç›‘ç£å¾®è°ƒæˆ–åå¥½ä¼˜åŒ–ï¼Œä»è€Œæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08380', 'title': 'EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation', 'url': 'https://huggingface.co/papers/2411.08380', 'abstract': 'Video generation has emerged as a promising tool for world simulation, leveraging visual data to replicate real-world environments. Within this context, egocentric video generation, which centers on the human perspective, holds significant potential for enhancing applications in virtual reality, augmented reality, and gaming. However, the generation of egocentric videos presents substantial challenges due to the dynamic nature of egocentric viewpoints, the intricate diversity of actions, and the complex variety of scenes encountered. Existing datasets are inadequate for addressing these challenges effectively. To bridge this gap, we present EgoVid-5M, the first high-quality dataset specifically curated for egocentric video generation. EgoVid-5M encompasses 5 million egocentric video clips and is enriched with detailed action annotations, including fine-grained kinematic control and high-level textual descriptions. To ensure the integrity and usability of the dataset, we implement a sophisticated data cleaning pipeline designed to maintain frame consistency, action coherence, and motion smoothness under egocentric conditions. Furthermore, we introduce EgoDreamer, which is capable of generating egocentric videos driven simultaneously by action descriptions and kinematic control signals. The EgoVid-5M dataset, associated action annotations, and all data cleansing metadata will be released for the advancement of research in egocentric video generation.', 'score': 21, 'issue_id': 565, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '7c82b1fc5a785fe0', 'authors': ['Xiaofeng Wang', 'Kang Zhao', 'Feng Liu', 'Jiayu Wang', 'Guosheng Zhao', 'Xiaoyi Bao', 'Zheng Zhu', 'Yingya Zhang', 'Xingang Wang'], 'affiliations': ['CASIA', 'Tsinghua University', 'Alibaba', 'UCAS'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08380.jpg', 'data': {'categories': ['#data', '#synthetic', '#games', '#video', '#dataset'], 'emoji': 'ğŸ‘€', 'ru': {'title': 'EgoVid-5M: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ EgoVid-5M - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ²Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EgoDreamer, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'EgoVid-5M: Revolutionizing Egocentric Video Generation', 'desc': 'This paper introduces EgoVid-5M, a new dataset designed for generating egocentric videos, which are videos captured from a first-person perspective. The dataset contains 5 million video clips with detailed annotations that describe the actions and movements occurring in each clip. The authors also present EgoDreamer, a model that can create egocentric videos based on both action descriptions and kinematic controls. This work aims to address the challenges of existing datasets and improve the quality and usability of egocentric video generation for applications in virtual and augmented reality.'}, 'zh': {'title': 'EgoVid-5Mï¼šè‡ªæˆ‘è§†è§’è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'è§†é¢‘ç”Ÿæˆæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„ä¸–ç•Œæ¨¡æ‹Ÿå·¥å…·ï¼Œå¯ä»¥åˆ©ç”¨è§†è§‰æ•°æ®å¤åˆ¶ç°å®ç¯å¢ƒã€‚ä»¥è‡ªæˆ‘è§†è§’ä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆåœ¨è™šæ‹Ÿç°å®ã€å¢å¼ºç°å®å’Œæ¸¸æˆåº”ç”¨ä¸­å…·æœ‰é‡è¦æ½œåŠ›ã€‚ç„¶è€Œï¼Œè‡ªæˆ‘è§†è§’è§†é¢‘çš„ç”Ÿæˆé¢ä¸´ç€åŠ¨æ€è§†è§’ã€å¤æ‚åŠ¨ä½œå’Œå¤šæ ·åœºæ™¯çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EgoVid-5Mï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºè‡ªæˆ‘è§†è§’è§†é¢‘ç”Ÿæˆè€Œåˆ›å»ºçš„é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«500ä¸‡ä¸ªè§†é¢‘ç‰‡æ®µå’Œè¯¦ç»†çš„åŠ¨ä½œæ³¨é‡Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07618', 'title': 'Direct Preference Optimization Using Sparse Feature-Level Constraints', 'url': 'https://huggingface.co/papers/2411.07618', 'abstract': 'The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.', 'score': 13, 'issue_id': 565, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': 'b7ab0d7ebab29360', 'authors': ['Qingyu Yin', 'Chak Tou Leong', 'Hongbo Zhang', 'Minjun Zhu', 'Hanqi Yan', 'Qiang Zhang', 'Yulan He', 'Wenjie Li', 'Jun Wang', 'Yue Zhang', 'Linyi Yang'], 'affiliations': ['Westlake University', 'Zhejiang University', 'The Hong Kong Polytechnic University', 'Kings College London', 'University College London'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07618.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ - Feature-level constrained Preference Optimization (FPO). FPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 5.08% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. FPO Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ LLM.'}, 'en': {'title': 'Efficient Alignment of Language Models with Human Preferences', 'desc': 'This paper addresses the challenge of aligning large language models (LLMs) with human preferences. It introduces a new method called Feature-level constrained Preference Optimization (FPO), which aims to improve the alignment process while maintaining stability and efficiency. FPO utilizes pre-trained Sparse Autoencoders (SAEs) and applies feature-level constraints to enhance the alignment without incurring high computational costs. Experimental results show that FPO outperforms existing methods, achieving a significant improvement in performance while being more resource-efficient.'}, 'zh': {'title': 'é«˜æ•ˆç¨³å®šçš„è¯­è¨€æ¨¡å‹å¯¹é½æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºç‰¹å¾çº§çº¦æŸåå¥½ä¼˜åŒ–ï¼ˆFPOï¼‰ï¼Œæ—¨åœ¨ç®€åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½çš„å¯¹é½è¿‡ç¨‹ã€‚FPOåˆ©ç”¨é¢„è®­ç»ƒçš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰å¹¶å¼•å…¥ç‰¹å¾çº§çº¦æŸï¼Œä»è€Œå®ç°é«˜æ•ˆä¸”ç¨³å®šçš„å¯¹é½ã€‚é€šè¿‡æ¿€æ´»ç¨€ç–ç‰¹å¾å’Œä½¿ç”¨ç‰¹å¾çº§ç¦»çº¿å‚è€ƒï¼ŒFPOåœ¨è®¡ç®—æˆæœ¬ä¸Šæ˜¾è‘—é™ä½ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFPOåœ¨åŸºå‡†æ•°æ®é›†ä¸Šç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œèµ¢ç‡æé«˜äº†5.08%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08868', 'title': 'CamemBERT 2.0: A Smarter French Language Model Aged to Perfection', 'url': 'https://huggingface.co/papers/2411.08868', 'abstract': 'French language models, such as CamemBERT, have been widely adopted across industries for natural language processing (NLP) tasks, with models like CamemBERT seeing over 4 million downloads per month. However, these models face challenges due to temporal concept drift, where outdated training data leads to a decline in performance, especially when encountering new topics and terminology. This issue emphasizes the need for updated models that reflect current linguistic trends. In this paper, we introduce two new versions of the CamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these challenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use of the Replaced Token Detection (RTD) objective for better contextual understanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked Language Modeling (MLM) objective. Both models are trained on a significantly larger and more recent dataset with longer context length and an updated tokenizer that enhances tokenization performance for French. We evaluate the performance of these models on both general-domain NLP tasks and domain-specific applications, such as medical field tasks, demonstrating their versatility and effectiveness across a range of use cases. Our results show that these updated models vastly outperform their predecessors, making them valuable tools for modern NLP systems. All our new models, as well as intermediate checkpoints, are made openly available on Huggingface.', 'score': 11, 'issue_id': 569, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '6be618c24defa5fb', 'authors': ['Wissam Antoun', 'Francis Kulumba', 'Rian Touchent', 'Ã‰ric de la Clergerie', 'BenoÃ®t Sagot', 'DjamÃ© Seddah'], 'affiliations': ['Inria, Paris, France'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08868.jpg', 'data': {'categories': ['#training', '#multilingual', '#healthcare', '#long_context', '#architecture', '#dataset', '#low_resource', '#open_source'], 'emoji': 'ğŸ‡«ğŸ‡·', 'ru': {'title': 'ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ NLP', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CamemBERT - CamemBERTav2 Ğ¸ CamemBERTv2. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ñ‹ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ñ€ĞµĞ¹Ñ„Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹, Ğ¸Ğ·-Ğ·Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ÑƒÑÑ‚Ğ°Ñ€ĞµĞ²ÑˆĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. CamemBERTav2 Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ DeBERTaV3 Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Replaced Token Detection, Ğ° CamemBERTv2 Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ° Ğ½Ğ° RoBERTa Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ Masked Language Modeling. ĞĞ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¼ Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ´Ğ»Ñ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Revamping CamemBERT for Modern NLP Challenges', 'desc': 'This paper presents two updated versions of the French language model CamemBERT, named CamemBERTav2 and CamemBERTv2, to tackle the issue of temporal concept drift in natural language processing. CamemBERTav2 utilizes the DeBERTaV3 architecture with a Replaced Token Detection objective, while CamemBERTv2 is based on RoBERTa and employs Masked Language Modeling. Both models are trained on a larger, more recent dataset, improving their contextual understanding and tokenization performance for French. The evaluation shows that these models significantly outperform previous versions in various NLP tasks, including general and domain-specific applications.'}, 'zh': {'title': 'æ›´æ–°æ¨¡å‹ï¼Œæå‡æ³•è¯­NLPæ€§èƒ½', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸¤ç§æ–°çš„æ³•è¯­è¯­è¨€æ¨¡å‹CamemBERTav2å’ŒCamemBERTv2ï¼Œæ—¨åœ¨è§£å†³ç”±äºè¿‡æ—¶è®­ç»ƒæ•°æ®å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚è¿™äº›æ¨¡å‹åŸºäºDeBERTaV3å’ŒRoBERTaæ¶æ„ï¼Œåˆ†åˆ«é‡‡ç”¨äº†æ›¿æ¢æ ‡è®°æ£€æµ‹ï¼ˆRTDï¼‰å’Œæ©è”½è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡ï¼Œä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚å®ƒä»¬åœ¨æ›´å¤§ä¸”æ›´æ–°çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ”¹è¿›çš„åˆ†è¯å™¨ï¼Œæå‡äº†æ³•è¯­çš„åˆ†è¯æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ›´æ–°çš„æ¨¡å‹åœ¨é€šç”¨å’Œç‰¹å®šé¢†åŸŸçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæˆä¸ºç°ä»£NLPç³»ç»Ÿçš„é‡è¦å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08307', 'title': 'PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation', 'url': 'https://huggingface.co/papers/2411.08307', 'abstract': 'Music generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains a significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io.', 'score': 6, 'issue_id': 575, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '1f1b5e7081062b6f', 'authors': ['Yungang Yi', 'Weihua Li', 'Matthew Kuo', 'Quan Bai'], 'affiliations': ['Auckland University of Technology, Auckland, New Zealand', 'University of Tasmania, Tasmania, Australia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08307.jpg', 'data': {'categories': ['#audio', '#architecture', '#story_generation'], 'emoji': 'ğŸ¼', 'ru': {'title': 'PerceiverS: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PerceiverS - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸. PerceiverS ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ĞºĞ°Ğº Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ·Ñ‹ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€, Ñ‚Ğ°Ğº Ğ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ½ÑĞ°Ğ½ÑĞ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ Maestro Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ¹ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸.'}, 'en': {'title': 'Enhancing Symbolic Music Generation with PerceiverS', 'desc': 'This paper introduces PerceiverS, a new architecture aimed at improving symbolic music generation. It utilizes Effective Segmentation and Multi-Scale attention mechanisms to learn both long-term structures and short-term expressive details in music. By integrating cross-attention and self-attention, the model effectively captures the overall musical structure while maintaining nuanced performance elements. Evaluations on datasets like Maestro show that PerceiverS generates more coherent and diverse music, achieving better structural consistency and expressive variation.'}, 'zh': {'title': 'PerceiverSï¼šç”Ÿæˆå¯Œæœ‰è¡¨ç°åŠ›çš„ç¬¦å·éŸ³ä¹æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„éŸ³ä¹ç”Ÿæˆæ¶æ„ï¼Œç§°ä¸ºPerceiverSï¼ˆåˆ†æ®µä¸å°ºåº¦ï¼‰ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆé•¿ç»“æ„å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„ç¬¦å·éŸ³ä¹çš„æŒ‘æˆ˜ã€‚è¯¥æ¨¡å‹ç»“åˆäº†æœ‰æ•ˆçš„åˆ†æ®µå’Œå¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶å­¦ä¹ é•¿æœŸç»“æ„ä¾èµ–å’ŒçŸ­æœŸè¡¨ç°ç»†èŠ‚ã€‚é€šè¿‡åœ¨å¤šå°ºåº¦è®¾ç½®ä¸­ç»“åˆäº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›ï¼ŒPerceiverSèƒ½å¤Ÿæ•æ‰é•¿è·ç¦»çš„éŸ³ä¹ç»“æ„ï¼ŒåŒæ—¶ä¿ç•™æ¼”å¥çš„ç»†å¾®å·®åˆ«ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆè¿è´¯ä¸”å¤šæ ·åŒ–çš„éŸ³ä¹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ç»“æ„ä¸€è‡´æ€§å’Œè¡¨ç°å˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08790', 'title': 'Can sparse autoencoders be used to decompose and interpret steering vectors?', 'url': 'https://huggingface.co/papers/2411.08790', 'abstract': 'Steering vectors are a promising approach to control the behaviour of large language models. However, their underlying mechanisms remain poorly understood. While sparse autoencoders (SAEs) may offer a potential method to interpret steering vectors, recent findings show that SAE-reconstructed vectors often lack the steering properties of the original vectors. This paper investigates why directly applying SAEs to steering vectors yields misleading decompositions, identifying two reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in feature directions, which SAEs are not designed to accommodate. These limitations hinder the direct use of SAEs for interpreting steering vectors.', 'score': 6, 'issue_id': 570, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '68e55764db274419', 'authors': ['Harry Mayne', 'Yushi Yang', 'Adam Mahdi'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08790.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#training'], 'emoji': 'ğŸ§­', 'ru': {'title': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ² Ñ€Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² (SAE) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ: Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ SAE Ğ¸ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¹. Ğ­Ñ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼Ñƒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ SAE Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Understanding Steering Vectors: Limitations of Sparse Autoencoders', 'desc': 'This paper explores the use of steering vectors in controlling large language models and highlights the challenges in understanding their mechanisms. It specifically examines the limitations of sparse autoencoders (SAEs) when applied to these steering vectors. The authors identify that steering vectors do not fit the input distribution that SAEs are designed for, and they can have significant negative projections that SAEs cannot handle. These findings suggest that using SAEs for interpreting steering vectors may lead to inaccurate results, indicating a need for alternative methods.'}, 'zh': {'title': 'æ­ç¤ºå¼•å¯¼å‘é‡çš„å¥¥ç§˜', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¼•å¯¼å‘é‡åœ¨æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹è¡Œä¸ºä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œä½†å…¶æœºåˆ¶å°šä¸æ¸…æ¥šã€‚ç ”ç©¶å‘ç°ï¼Œç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰åœ¨è§£é‡Šå¼•å¯¼å‘é‡æ—¶å­˜åœ¨é—®é¢˜ï¼Œé‡æ„çš„å‘é‡å¾€å¾€ç¼ºä¹åŸå§‹å‘é‡çš„å¼•å¯¼ç‰¹æ€§ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œç›´æ¥åº”ç”¨SAEäºå¼•å¯¼å‘é‡ä¼šå¯¼è‡´è¯¯å¯¼æ€§çš„åˆ†è§£ï¼ŒåŸå› åŒ…æ‹¬å¼•å¯¼å‘é‡è¶…å‡ºäº†SAEè®¾è®¡çš„è¾“å…¥åˆ†å¸ƒï¼Œä»¥åŠå¼•å¯¼å‘é‡åœ¨ç‰¹å¾æ–¹å‘ä¸Šå¯èƒ½å…·æœ‰æœ‰æ„ä¹‰çš„è´ŸæŠ•å½±ã€‚ç”±äºè¿™äº›é™åˆ¶ï¼ŒSAEåœ¨è§£é‡Šå¼•å¯¼å‘é‡æ—¶çš„ç›´æ¥åº”ç”¨å—åˆ°é˜»ç¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08328', 'title': 'Motion Control for Enhanced Complex Action Video Generation', 'url': 'https://huggingface.co/papers/2411.08328', 'abstract': "Existing text-to-video (T2V) models often struggle with generating videos with sufficiently pronounced or complex actions. A key limitation lies in the text prompt's inability to precisely convey intricate motion details. To address this, we propose a novel framework, MVideo, designed to produce long-duration videos with precise, fluid actions. MVideo overcomes the limitations of text prompts by incorporating mask sequences as an additional motion condition input, providing a clearer, more accurate representation of intended actions. Leveraging foundational vision models such as GroundingDINO and SAM2, MVideo automatically generates mask sequences, enhancing both efficiency and robustness. Our results demonstrate that, after training, MVideo effectively aligns text prompts with motion conditions to produce videos that simultaneously meet both criteria. This dual control mechanism allows for more dynamic video generation by enabling alterations to either the text prompt or motion condition independently, or both in tandem. Furthermore, MVideo supports motion condition editing and composition, facilitating the generation of videos with more complex actions. MVideo thus advances T2V motion generation, setting a strong benchmark for improved action depiction in current video diffusion models. Our project page is available at https://mvideo-v1.github.io/.", 'score': 2, 'issue_id': 575, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'ff99307fadb40120', 'authors': ['Qiang Zhou', 'Shaofeng Zhang', 'Nianzu Yang', 'Ye Qian', 'Hao Li'], 'affiliations': ['INF Tech', 'Shanghai Jiao Tong University', 'Fudan University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08328.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#video', '#benchmark', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞ¾Ğº', 'desc': 'MVideo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹. MVideo Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'MVideo: Revolutionizing Text-to-Video with Precise Motion Control', 'desc': 'The paper introduces MVideo, a new framework for text-to-video (T2V) generation that enhances the depiction of complex actions in videos. It addresses the limitations of traditional text prompts by using mask sequences as an additional input, which provides clearer motion details. MVideo utilizes advanced vision models to automatically create these mask sequences, improving the efficiency and accuracy of video generation. The framework allows for independent or combined adjustments to text prompts and motion conditions, enabling the creation of more dynamic and intricate videos.'}, 'zh': {'title': 'MVideoï¼šæå‡æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„åŠ¨æ€æ€§ä¸ç²¾ç¡®æ€§', 'desc': 'ç°æœ‰çš„æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ¨¡å‹åœ¨ç”Ÿæˆå¤æ‚åŠ¨ä½œçš„è§†é¢‘æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä¸»è¦é—®é¢˜åœ¨äºæ–‡æœ¬æç¤ºæ— æ³•å‡†ç¡®ä¼ è¾¾å¤æ‚çš„è¿åŠ¨ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶MVideoï¼Œæ—¨åœ¨ç”Ÿæˆå…·æœ‰ç²¾ç¡®æµç•…åŠ¨ä½œçš„é•¿æ—¶è§†é¢‘ã€‚MVideoé€šè¿‡å¼•å…¥æ©ç åºåˆ—ä½œä¸ºé¢å¤–çš„è¿åŠ¨æ¡ä»¶è¾“å…¥ï¼Œå…‹æœäº†æ–‡æœ¬æç¤ºçš„å±€é™æ€§ï¼Œä»è€Œå®ç°æ›´åŠ¨æ€çš„è§†é¢‘ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.09595', 'title': 'LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models', 'url': 'https://huggingface.co/papers/2411.09595', 'abstract': 'This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived from textual sources like 3D tutorials, and (2) enabling conversational 3D generation and mesh understanding. A primary challenge is effectively tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly. To address this, we introduce LLaMA-Mesh, a novel approach that represents the vertex coordinates and face definitions of 3D meshes as plain text, allowing direct integration with LLMs without expanding the vocabulary. We construct a supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate 3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs as required, and (3) understand and interpret 3D meshes. Our work is the first to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge for 3D mesh generation in a text-based format, effectively unifying the 3D and text modalities. LLaMA-Mesh achieves mesh generation quality on par with models trained from scratch while maintaining strong text generation performance.', 'score': 55, 'issue_id': 588, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': 'ec4879add3585cee', 'authors': ['Zhengyi Wang', 'Jonathan Lorraine', 'Yikai Wang', 'Hang Su', 'Jun Zhu', 'Sanja Fidler', 'Xiaohui Zeng'], 'affiliations': ['Tsinghua University', 'NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.09595.jpg', 'data': {'categories': ['#optimization', '#games', '#3d', '#multimodal', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¼Ğ¸Ñ€Ñ‹', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ LLaMA-Mesh - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑ‚ÑŒ 3D-ÑĞµÑ‚ĞºĞ¸ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¸ 3D-Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ 3D-ÑĞµÑ‚ĞºĞ¸. LLaMA-Mesh Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Unifying Text and 3D: LLaMA-Mesh Transforms Language into Spatial Creations', 'desc': 'This paper presents LLaMA-Mesh, a novel method that enables large language models (LLMs) to generate 3D meshes from text inputs. By tokenizing 3D mesh data into a text format, the model can leverage existing spatial knowledge from its text training. The approach allows for conversational interactions where users can request 3D mesh generation and receive outputs in both text and mesh formats. LLaMA-Mesh demonstrates that LLMs can effectively learn complex spatial relationships, achieving high-quality mesh generation comparable to models specifically trained for this task.'}, 'zh': {'title': 'ç»Ÿä¸€æ–‡æœ¬ä¸3Dç”Ÿæˆçš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ç»Ÿä¸€æ¨¡å‹ä¸­ç”Ÿæˆ3Dç½‘æ ¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•LLaMA-Meshï¼Œå°†3Dç½‘æ ¼çš„é¡¶ç‚¹åæ ‡å’Œé¢å®šä¹‰è¡¨ç¤ºä¸ºæ™®é€šæ–‡æœ¬ï¼Œä»è€Œå®ç°ä¸LLMsçš„ç›´æ¥é›†æˆã€‚é€šè¿‡æ„å»ºä¸€ä¸ªç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œä½¿é¢„è®­ç»ƒçš„LLMsèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆ3Dç½‘æ ¼ï¼Œå¹¶ç†è§£å’Œè§£é‡Š3Dç½‘æ ¼ã€‚æˆ‘ä»¬çš„å·¥ä½œé¦–æ¬¡è¯æ˜äº†LLMså¯ä»¥é€šè¿‡å¾®è°ƒè·å¾—å¤æ‚çš„ç©ºé—´çŸ¥è¯†ï¼Œä»è€Œåœ¨æ–‡æœ¬åŸºç¡€ä¸Šç”Ÿæˆ3Dç½‘æ ¼ï¼ŒæˆåŠŸå®ç°äº†3Då’Œæ–‡æœ¬æ¨¡æ€çš„ç»Ÿä¸€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.09703', 'title': 'MagicQuill: An Intelligent Interactive Image Editing System', 'url': 'https://huggingface.co/papers/2411.09703', 'abstract': 'Image editing involves a variety of complex tasks and requires efficient and precise manipulation techniques. In this paper, we present MagicQuill, an integrated image editing system that enables swift actualization of creative ideas. Our system features a streamlined yet functionally robust interface, allowing for the articulation of editing operations (e.g., inserting elements, erasing objects, altering color) with minimal input. These interactions are monitored by a multimodal large language model (MLLM) to anticipate editing intentions in real time, bypassing the need for explicit prompt entry. Finally, we apply a powerful diffusion prior, enhanced by a carefully learned two-branch plug-in module, to process editing requests with precise control. Experimental results demonstrate the effectiveness of MagicQuill in achieving high-quality image edits. Please visit https://magic-quill.github.io to try out our system.', 'score': 40, 'issue_id': 586, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 14', 'zh': '11æœˆ14æ—¥'}, 'hash': '530cdb8733e44b9e', 'authors': ['Zichen Liu', 'Yue Yu', 'Hao Ouyang', 'Qiuyu Wang', 'Ka Leong Cheng', 'Wen Wang', 'Zhiheng Liu', 'Qifeng Chen', 'Yujun Shen'], 'affiliations': ['HKUST', 'Ant Group', 'ZJU', 'HKU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.09703.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#cv'], 'emoji': 'ğŸ–Œï¸', 'ru': {'title': 'MagicQuill: Ğ˜Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'MagicQuill - ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (MLLM) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ğ¼ĞµĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ²Ğ¾Ğ´Ğ¾Ğ¼. Ğ”Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€, ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ MagicQuill Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Effortless Image Editing with MagicQuill', 'desc': 'MagicQuill is an advanced image editing system designed to facilitate quick and precise editing tasks. It utilizes a multimodal large language model (MLLM) to understand user intentions in real time, allowing for seamless interactions without the need for explicit commands. The system incorporates a diffusion prior and a two-branch plug-in module to ensure high-quality and controlled image modifications. Experimental results indicate that MagicQuill significantly enhances the efficiency and effectiveness of image editing processes.'}, 'zh': {'title': 'MagicQuillï¼šé«˜æ•ˆç²¾å‡†çš„å›¾åƒç¼–è¾‘ç³»ç»Ÿ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMagicQuillçš„é›†æˆå›¾åƒç¼–è¾‘ç³»ç»Ÿï¼Œæ—¨åœ¨é«˜æ•ˆã€ç²¾ç¡®åœ°å®ç°åˆ›æ„æ„æ€ã€‚è¯¥ç³»ç»Ÿå…·æœ‰ç®€æ´è€Œå¼ºå¤§çš„ç•Œé¢ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æœ€å°‘çš„è¾“å…¥è¿›è¡Œç¼–è¾‘æ“ä½œï¼Œå¦‚æ’å…¥å…ƒç´ ã€æ“¦é™¤å¯¹è±¡å’Œæ”¹å˜é¢œè‰²ã€‚ç³»ç»Ÿé€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å®æ—¶ç›‘æµ‹ç”¨æˆ·çš„ç¼–è¾‘æ„å›¾ï¼Œçœå»äº†æ˜¾å¼è¾“å…¥æç¤ºçš„éœ€æ±‚ã€‚æœ€åï¼Œç»“åˆå¼ºå¤§çš„æ‰©æ•£å…ˆéªŒå’Œç²¾å¿ƒè®¾è®¡çš„åŒåˆ†æ”¯æ’ä»¶æ¨¡å—ï¼ŒMagicQuillèƒ½å¤Ÿä»¥ç²¾ç¡®çš„æ§åˆ¶å¤„ç†ç¼–è¾‘è¯·æ±‚ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨é«˜è´¨é‡å›¾åƒç¼–è¾‘æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.09009', 'title': 'Cut Your Losses in Large-Vocabulary Language Models', 'url': 'https://huggingface.co/papers/2411.09009', 'abstract': 'As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e., below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.', 'score': 27, 'issue_id': 588, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': 'a3ceb77f367c961a', 'authors': ['Erik Wijmans', 'Brody Huval', 'Alexander Hertzberg', 'Vladlen Koltun', 'Philipp KrÃ¤henbÃ¼hl'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.09009.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cut Cross-Entropy (CCE) Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. CCE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ kernel Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸Ğ¸ log-sum-exp Ğ² Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CCE drastically ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Revolutionizing Memory Efficiency in Language Model Training', 'desc': 'This paper introduces Cut Cross-Entropy (CCE), a novel method designed to reduce the memory usage of large language models (LLMs) during training. Traditional cross-entropy loss computation requires storing a large logit matrix, which can consume excessive memory, especially as model vocabularies grow. CCE addresses this by calculating the loss without creating the full logit matrix, instead focusing only on the correct token and dynamically evaluating the necessary computations. The implementation of CCE significantly decreases memory requirements, allowing for efficient training without compromising performance or speed.'}, 'zh': {'title': 'Cut Cross-Entropyï¼šæ˜¾è‘—é™ä½å†…å­˜å ç”¨çš„åˆ›æ–°æ–¹æ³•', 'desc': 'éšç€è¯­è¨€æ¨¡å‹çš„è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œå®ƒä»¬çš„è¯æ±‡é‡ä¹Ÿåœ¨å¢åŠ ã€‚è¿™å¯¼è‡´åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œäº¤å‰ç†µçš„å†…å­˜å ç”¨åœ¨ä¸€ä¸ªå•ç‹¬çš„å±‚ä¸­æ˜¾è‘—å¢åŠ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCut Cross-Entropyï¼ˆCCEï¼‰çš„æ–¹æ³•ï¼Œå®ƒåœ¨ä¸å°†æ‰€æœ‰æ ‡è®°çš„logitså­˜å‚¨åˆ°å…¨å±€å†…å­˜ä¸­çš„æƒ…å†µä¸‹è®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼ŒCCEæ˜¾è‘—å‡å°‘äº†å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†è®­ç»ƒé€Ÿåº¦å’Œæ”¶æ•›æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06469', 'title': 'ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?', 'url': 'https://huggingface.co/papers/2411.06469', 'abstract': "Large Language Models (LLMs) hold great promise to revolutionize current clinical systems for their superior capacities on medical text processing tasks and medical licensing exams. Meanwhile, traditional ML models such as SVM and XGBoost have still been mainly adopted in clinical prediction tasks. An emerging question is Can LLMs beat traditional ML models in clinical prediction? Thus, we build a new benchmark ClinicalBench to comprehensively study the clinical predictive modeling capacities of both general-purpose and medical LLMs, and compare them with traditional ML models. ClinicalBench embraces three common clinical prediction tasks, two databases, 14 general-purpose LLMs, 8 medical LLMs, and 11 traditional ML models. Through extensive empirical investigation, we discover that both general-purpose and medical LLMs, even with different model scales, diverse prompting or fine-tuning strategies, still cannot beat traditional ML models in clinical prediction yet, shedding light on their potential deficiency in clinical reasoning and decision-making. We call for caution when practitioners adopt LLMs in clinical applications. ClinicalBench can be utilized to bridge the gap between LLMs' development for healthcare and real-world clinical practice.", 'score': 13, 'issue_id': 586, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '933f68c6c7b9c7f5', 'authors': ['Canyu Chen', 'Jian Yu', 'Shan Chen', 'Che Liu', 'Zhongwei Wan', 'Danielle Bitterman', 'Fei Wang', 'Kai Shu'], 'affiliations': ['Illinois Institute of Technology', 'Emory University', 'Mass General Brigham & Boston Childrens Hospital, Harvard Medical School', 'Weill Cornell Medicine, Cornell University', 'Imperial College London', 'Ohio State University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06469.jpg', 'data': {'categories': ['#science', '#benchmark', '#healthcare', '#reasoning'], 'emoji': 'ğŸ¥', 'ru': {'title': 'LLM vs Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ML: ĞºÑ‚Ğ¾ Ğ¿Ğ¾Ğ±ĞµĞ´Ğ¸Ñ‚ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸?', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ClinicalBench Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ LLM Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ LLM, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ¿Ğ¾ĞºĞ° Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğº Ğ¾ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğ¸ LLM Ğ² ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ClinicalBench Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ LLM Ğ´Ğ»Ñ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ¾Ğ¹.'}, 'en': {'title': 'LLMs vs. Traditional Models: A Cautionary Tale in Clinical Prediction', 'desc': 'This paper investigates the effectiveness of Large Language Models (LLMs) in clinical prediction tasks compared to traditional machine learning models like SVM and XGBoost. The authors created a benchmark called ClinicalBench, which includes various clinical prediction tasks and a wide range of models for comparison. Their findings reveal that, despite the advanced capabilities of LLMs in processing medical text, they still fall short of outperforming traditional models in clinical prediction scenarios. The study emphasizes the need for caution in using LLMs for clinical applications, highlighting their limitations in clinical reasoning and decision-making.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠé¢„æµ‹ä¸­çš„æŒ‘æˆ˜', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»å­¦æ–‡æœ¬å¤„ç†å’ŒåŒ»å­¦æ‰§ç…§è€ƒè¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰é©å‘½æ€§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹å¦‚æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰å’ŒXGBoostä»ç„¶æ˜¯ä¸´åºŠé¢„æµ‹ä»»åŠ¡çš„ä¸»è¦é€‰æ‹©ã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ClinicalBenchï¼Œå…¨é¢ç ”ç©¶é€šç”¨å’ŒåŒ»å­¦LLMsåœ¨ä¸´åºŠé¢„æµ‹å»ºæ¨¡ä¸­çš„èƒ½åŠ›ï¼Œå¹¶ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡LLMsåœ¨ä¸åŒè§„æ¨¡å’Œç­–ç•¥ä¸‹è¿›è¡Œå®éªŒï¼Œä½†åœ¨ä¸´åºŠé¢„æµ‹ä¸­ä»æœªèƒ½è¶…è¶Šä¼ ç»Ÿæ¨¡å‹ï¼Œè¿™è¡¨æ˜å®ƒä»¬åœ¨ä¸´åºŠæ¨ç†å’Œå†³ç­–æ–¹é¢å¯èƒ½å­˜åœ¨ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08768', 'title': 'Sharingan: Extract User Action Sequence from Desktop Recordings', 'url': 'https://huggingface.co/papers/2411.08768', 'abstract': 'Video recordings of user activities, particularly desktop recordings, offer a rich source of data for understanding user behaviors and automating processes. However, despite advancements in Vision-Language Models (VLMs) and their increasing use in video analysis, extracting user actions from desktop recordings remains an underexplored area. This paper addresses this gap by proposing two novel VLM-based methods for user action extraction: the Direct Frame-Based Approach (DF), which inputs sampled frames directly into VLMs, and the Differential Frame-Based Approach (DiffF), which incorporates explicit frame differences detected via computer vision techniques. We evaluate these methods using a basic self-curated dataset and an advanced benchmark adapted from prior work. Our results show that the DF approach achieves an accuracy of 70% to 80% in identifying user actions, with the extracted action sequences being re-playable though Robotic Process Automation. We find that while VLMs show potential, incorporating explicit UI changes can degrade performance, making the DF approach more reliable. This work represents the first application of VLMs for extracting user action sequences from desktop recordings, contributing new methods, benchmarks, and insights for future research.', 'score': 6, 'issue_id': 618, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '1179d90820efd445', 'authors': ['Yanting Chen', 'Yi Ren', 'Xiaoting Qin', 'Jue Zhang', 'Kehong Yuan', 'Lu Han', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Tsinghua University', 'Microsoft'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08768.jpg', 'data': {'categories': ['#agents', '#training', '#optimization', '#video', '#cv', '#benchmark', '#games'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Vision-Language Models', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Vision-Language Models Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸Ğ· Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ°. ĞŸÑ€ÑĞ¼Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² (DF) Ğ¿Ğ¾Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ² VLM, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ (DiffF) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ²Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ DF-Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ 70-80% Ğ² Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ VLM Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ¸Ğ· Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ°.'}, 'en': {'title': 'Unlocking User Actions from Desktop Videos with VLMs', 'desc': 'This paper explores the use of Vision-Language Models (VLMs) to extract user actions from desktop video recordings, an area that has not been thoroughly investigated. It introduces two innovative methods: the Direct Frame-Based Approach (DF), which uses sampled video frames directly, and the Differential Frame-Based Approach (DiffF), which analyzes differences between frames. The study evaluates these methods on a self-curated dataset and a benchmark, revealing that the DF method achieves a 70% to 80% accuracy in identifying user actions. The findings suggest that while VLMs are promising, the inclusion of explicit user interface changes can negatively impact performance, highlighting the DF approach as a more dependable option.'}, 'zh': {'title': 'ä»æ¡Œé¢å½•åˆ¶ä¸­æå–ç”¨æˆ·è¡Œä¸ºçš„æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ä»æ¡Œé¢å½•åˆ¶è§†é¢‘ä¸­æå–ç”¨æˆ·è¡Œä¸ºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸¤ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ˜¯ç›´æ¥å¸§è¾“å…¥æ–¹æ³•ï¼ˆDFï¼‰ï¼Œå®ƒå°†é‡‡æ ·çš„å¸§ç›´æ¥è¾“å…¥VLMè¿›è¡Œåˆ†æï¼›ç¬¬äºŒç§æ˜¯å·®å¼‚å¸§è¾“å…¥æ–¹æ³•ï¼ˆDiffFï¼‰ï¼Œå®ƒåˆ©ç”¨è®¡ç®—æœºè§†è§‰æŠ€æœ¯æ£€æµ‹çš„å¸§å·®å¼‚æ¥å¢å¼ºåˆ†æã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDFæ–¹æ³•åœ¨è¯†åˆ«ç”¨æˆ·è¡Œä¸ºæ–¹é¢çš„å‡†ç¡®ç‡è¾¾åˆ°70%åˆ°80%ï¼Œå¹¶ä¸”æå–çš„è¡Œä¸ºåºåˆ—å¯ä»¥é€šè¿‡æœºå™¨äººæµç¨‹è‡ªåŠ¨åŒ–ï¼ˆRPAï¼‰é‡æ”¾ã€‚å°½ç®¡VLMæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†æ˜¾å¼ç”¨æˆ·ç•Œé¢å˜åŒ–çš„å¼•å…¥å¯èƒ½ä¼šé™ä½æ€§èƒ½ï¼Œå› æ­¤DFæ–¹æ³•è¢«è®¤ä¸ºæ›´ä¸ºå¯é ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08954', 'title': 'Inconsistencies In Consistency Models: Better ODE Solving Does Not Imply Better Samples', 'url': 'https://huggingface.co/papers/2411.08954', 'abstract': 'Although diffusion models can generate remarkably high-quality samples, they are intrinsically bottlenecked by their expensive iterative sampling procedure. Consistency models (CMs) have recently emerged as a promising diffusion model distillation method, reducing the cost of sampling by generating high-fidelity samples in just a few iterations. Consistency model distillation aims to solve the probability flow ordinary differential equation (ODE) defined by an existing diffusion model. CMs are not directly trained to minimize error against an ODE solver, rather they use a more computationally tractable objective. As a way to study how effectively CMs solve the probability flow ODE, and the effect that any induced error has on the quality of generated samples, we introduce Direct CMs, which directly minimize this error. Intriguingly, we find that Direct CMs reduce the ODE solving error compared to CMs but also result in significantly worse sample quality, calling into question why exactly CMs work well in the first place. Full code is available at: https://github.com/layer6ai-labs/direct-cms.', 'score': 5, 'issue_id': 618, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 13', 'zh': '11æœˆ13æ—¥'}, 'hash': '9cff9dea78bd5669', 'authors': ['NoÃ«l Vouitsis', 'Rasa Hosseinzadeh', 'Brendan Leigh Ross', 'Valentin Villecroze', 'Satya Krishna Gorti', 'Jesse C. Cresswell', 'Gabriel Loaiza-Ganem'], 'affiliations': ['Layer 6 AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08954.jpg', 'data': {'categories': ['#diffusion', '#training', '#optimization', '#cv'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Direct Consistency Models (Direct CMs), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ‹ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ (ODE) Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ‚Ğ¾, Ñ‡Ñ‚Ğ¾ Direct CMs ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ODE Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Consistency Models (CMs), Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´ÑÑ‚ Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ…ÑƒĞ´ÑˆĞµĞ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ ÑÑĞ¼Ğ¿Ğ»Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ Ğ¿Ğ¾Ğ´ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ CMs Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Streamlining Diffusion: The Paradox of Consistency Models', 'desc': 'This paper discusses the limitations of diffusion models in generating high-quality samples due to their costly iterative sampling process. It introduces consistency models (CMs) as a method to distill diffusion models, allowing for high-fidelity sample generation in fewer iterations. The authors explore the relationship between CMs and the probability flow ordinary differential equation (ODE) that governs diffusion models, proposing Direct CMs that aim to minimize the error in solving this ODE. Surprisingly, while Direct CMs reduce the ODE solving error, they produce lower quality samples, raising questions about the underlying reasons for the effectiveness of CMs.'}, 'zh': {'title': 'æå‡æ‰©æ•£æ¨¡å‹æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ï¼Œä½†å…¶è¿­ä»£é‡‡æ ·è¿‡ç¨‹éå¸¸è€—æ—¶ã€‚æœ€è¿‘å‡ºç°çš„è¿ç»­æ€§æ¨¡å‹ï¼ˆCMsï¼‰é€šè¿‡åœ¨å°‘é‡è¿­ä»£ä¸­ç”Ÿæˆé«˜ä¿çœŸæ ·æœ¬ï¼Œæˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ‰©æ•£æ¨¡å‹è’¸é¦æ–¹æ³•ã€‚è¿ç»­æ€§æ¨¡å‹è’¸é¦æ—¨åœ¨è§£å†³ç”±ç°æœ‰æ‰©æ•£æ¨¡å‹å®šä¹‰çš„æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ï¼Œä½†å¹¶ä¸æ˜¯ç›´æ¥é€šè¿‡æœ€å°åŒ–ODEæ±‚è§£å™¨çš„è¯¯å·®æ¥è®­ç»ƒã€‚æˆ‘ä»¬å¼•å…¥äº†ç›´æ¥è¿ç»­æ€§æ¨¡å‹ï¼ˆDirect CMsï¼‰ï¼Œç›´æ¥æœ€å°åŒ–è¯¥è¯¯å·®ï¼Œå‘ç°è™½ç„¶Direct CMså‡å°‘äº†ODEæ±‚è§£è¯¯å·®ï¼Œä½†ç”Ÿæˆæ ·æœ¬çš„è´¨é‡æ˜¾è‘—ä¸‹é™ï¼Œè¿™å¼•å‘äº†å¯¹CMsæœ‰æ•ˆæ€§çš„è´¨ç–‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06490', 'title': 'Hermes: A Large Language Model Framework on the Journey to Autonomous Networks', 'url': 'https://huggingface.co/papers/2411.06490', 'abstract': 'The drive toward automating cellular network operations has grown with the increasing complexity of these systems. Despite advancements, full autonomy currently remains out of reach due to reliance on human intervention for modeling network behaviors and defining policies to meet target requirements. Network Digital Twins (NDTs) have shown promise in enhancing network intelligence, but the successful implementation of this technology is constrained by use case-specific architectures, limiting its role in advancing network autonomy. A more capable network intelligence, or "telecommunications brain", is needed to enable seamless, autonomous management of cellular network. Large Language Models (LLMs) have emerged as potential enablers for this vision but face challenges in network modeling, especially in reasoning and handling diverse data types. To address these gaps, we introduce Hermes, a chain of LLM agents that uses "blueprints" for constructing NDT instances through structured and explainable logical steps. Hermes allows automatic, reliable, and accurate network modeling of diverse use cases and configurations, thus marking progress toward fully autonomous network operations.', 'score': 5, 'issue_id': 618, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '9ee1aeb9095ebee4', 'authors': ['Fadhel Ayed', 'Ali Maatouk', 'Nicola Piovesan', 'Antonio De Domenico', 'Merouane Debbah', 'Zhi-Quan Luo'], 'affiliations': ['Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France', 'Khalifa University of Science and Technology, Abu Dhabi, UAE', 'The Chinese University of Hong Kong, Shenzhen, China', 'Yale University, New Haven, Connecticut, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06490.jpg', 'data': {'categories': ['#reasoning', '#agents', '#agi', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': "Hermes: Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ 'Ğ¼Ğ¾Ğ·Ğ³' Ğ´Ğ»Ñ ÑĞµÑ‚ĞµĞ¹ ÑĞ²ÑĞ·Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ˜Ğ˜", 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Hermes - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¾Ğ² ÑĞµÑ‚ĞµĞ¹ ÑĞ²ÑĞ·Ğ¸. Hermes Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 'Ñ‡ĞµÑ€Ñ‚ĞµĞ¶Ğ¸' Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑˆĞ°Ğ³Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞµÑ‚ÑĞ¼Ğ¸ ÑĞ²ÑĞ·Ğ¸."}, 'en': {'title': 'Towards Fully Autonomous Cellular Networks with Hermes', 'desc': 'This paper discusses the challenges of automating cellular network operations due to their complexity and the need for human intervention in modeling and policy definition. It highlights the potential of Network Digital Twins (NDTs) to improve network intelligence but notes that their effectiveness is limited by specific architectures. The authors propose Hermes, a system of Large Language Model (LLM) agents that create NDT instances using structured logical steps, enhancing network modeling capabilities. By enabling automatic and accurate modeling for various use cases, Hermes aims to advance the goal of fully autonomous cellular networks.'}, 'zh': {'title': 'è¿ˆå‘å®Œå…¨è‡ªä¸»çš„èœ‚çªç½‘ç»œç®¡ç†', 'desc': 'éšç€èœ‚çªç½‘ç»œæ“ä½œçš„å¤æ‚æ€§å¢åŠ ï¼Œè‡ªåŠ¨åŒ–çš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚å°½ç®¡å·²æœ‰è¿›å±•ï¼Œä½†ç”±äºä¾èµ–äººå·¥å¹²é¢„æ¥å»ºæ¨¡ç½‘ç»œè¡Œä¸ºå’Œå®šä¹‰æ”¿ç­–ï¼Œå®Œå…¨è‡ªä¸»ä»ç„¶éš¾ä»¥å®ç°ã€‚ç½‘ç»œæ•°å­—åŒèƒèƒï¼ˆNDTï¼‰åœ¨æå‡ç½‘ç»œæ™ºèƒ½æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶æˆåŠŸå®æ–½å—åˆ°ç‰¹å®šç”¨ä¾‹æ¶æ„çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Hermesï¼Œä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„ç³»ç»Ÿï¼Œé€šè¿‡ç»“æ„åŒ–å’Œå¯è§£é‡Šçš„é€»è¾‘æ­¥éª¤æ„å»ºNDTå®ä¾‹ï¼Œä»è€Œå®ç°å¤šæ ·åŒ–ç”¨ä¾‹å’Œé…ç½®çš„è‡ªåŠ¨ã€å¯é å’Œå‡†ç¡®çš„ç½‘ç»œå»ºæ¨¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07232', 'title': 'Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models', 'url': 'https://huggingface.co/papers/2411.07232', 'abstract': 'Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models\' attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed "Additing Affordance Benchmark" for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics.', 'score': 56, 'issue_id': 523, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '5e344b551de578a9', 'authors': ['Yoad Tewel', 'Rinon Gal', 'Dvir Samuel', 'Yuval Atzmon', 'Lior Wolf', 'Gal Chechik'], 'affiliations': ['NVIDIA', 'Tel-Aviv University', 'Bar-Ilan University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07232.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#cv', '#benchmark'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Add-it: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Add-it Ğ´Ğ»Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· ÑÑ†ĞµĞ½Ñ‹, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Add-it Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ĞµĞ½ Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 80% ÑĞ»ÑƒÑ‡Ğ°ĞµĞ² Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Seamless Object Insertion with Add-it: No Fine-Tuning Needed!', 'desc': "This paper presents Add-it, a novel approach for adding objects to images based on text instructions, addressing the challenge of maintaining the original scene's integrity while ensuring the new object is placed naturally. The method leverages diffusion models' attention mechanisms, integrating information from the scene image, text prompt, and generated image to achieve seamless object insertion. By employing a weighted extended-attention mechanism, Add-it preserves structural consistency and fine details, resulting in more plausible object placements. Remarkably, Add-it does not require task-specific fine-tuning and outperforms existing supervised methods on various benchmarks, including a new evaluation standard for object placement plausibility."}, 'zh': {'title': 'æ— ç¼å›¾åƒç¼–è¾‘çš„æ–°çªç ´', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAdd-itçš„æ–¹æ³•ï¼Œç”¨äºæ ¹æ®æ–‡æœ¬æŒ‡ä»¤å°†ç‰©ä½“æ·»åŠ åˆ°å›¾åƒä¸­ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç»“åˆåœºæ™¯å›¾åƒã€æ–‡æœ¬æç¤ºå’Œç”Ÿæˆå›¾åƒçš„ä¿¡æ¯ï¼Œä»¥å®ç°è‡ªç„¶çš„ç‰©ä½“æ”¾ç½®ã€‚Add-itåœ¨ä¸è¿›è¡Œç‰¹å®šä»»åŠ¡å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†å›¾åƒæ’å…¥åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›ç»“æœï¼Œå¹¶åœ¨80%ä»¥ä¸Šçš„æƒ…å†µä¸‹è¢«äººç±»è¯„ä¼°è€…æ‰€åå¥½ã€‚è¯¥æ–¹æ³•ä¿æŒäº†ç»“æ„ä¸€è‡´æ€§å’Œç»†èŠ‚ï¼ŒåŒæ—¶ç¡®ä¿äº†ç‰©ä½“çš„è‡ªç„¶ä½ç½®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07199', 'title': 'OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision', 'url': 'https://huggingface.co/papers/2411.07199', 'abstract': 'Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \\omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \\omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \\omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/', 'score': 42, 'issue_id': 522, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '89d7bedc1b5241ac', 'authors': ['Cong Wei', 'Zheyang Xiong', 'Weiming Ren', 'Xinrun Du', 'Ge Zhang', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'University of Wisconsin-Madison', 'Vector Institute', 'M-A-P'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07199.jpg', 'data': {'categories': ['#dataset', '#data', '#optimization', '#cv', '#architecture', '#open_source', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'OmniEdit: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ»Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ÑˆÑƒĞ¼Ğ½Ñ‹Ğµ Ğ¸ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ OmniEdit, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ÑĞµĞ¼ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ»ÑĞ±Ñ‹Ğ¼ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½. OmniEdit Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ ÑĞµĞ¼Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ EditNet Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'OmniEdit: The All-in-One Image Editing Solution', 'desc': "This paper introduces \textit{omniedit}, a versatile image editing model designed to tackle multiple editing tasks with varying aspect ratios. The authors address key challenges in existing methods, such as biased synthesis, noisy datasets, and fixed resolutions, which limit practical applications. By leveraging supervision from multiple specialist models and employing advanced importance sampling techniques, \textit{omniedit} enhances data quality and editing performance. The proposed EditNet architecture further improves the model's success rate, making it a powerful tool for real-world image editing scenarios."}, 'zh': {'title': 'å…¨èƒ½å›¾åƒç¼–è¾‘ï¼Œæ‰“ç ´ç°å®åº”ç”¨çš„é™åˆ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸º\textit{omniedit}çš„å…¨èƒ½å›¾åƒç¼–è¾‘å™¨ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åè§åˆæˆè¿‡ç¨‹å¯¼è‡´çš„ç¼–è¾‘èƒ½åŠ›æœ‰é™ã€è®­ç»ƒæ•°æ®é›†ä¸­çš„å™ªå£°å’Œä¼ªå½±é—®é¢˜ï¼Œä»¥åŠæ•°æ®é›†çš„ä½åˆ†è¾¨ç‡å’Œå›ºå®šå®½é«˜æ¯”é™åˆ¶ã€‚é€šè¿‡åˆ©ç”¨ä¸ƒä¸ªä¸åŒä¸“ä¸šæ¨¡å‹çš„ç›‘ç£ï¼Œ\textit{omniedit}èƒ½å¤Ÿå¤„ç†ä¸ƒç§ä¸åŒçš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶ä¸”æ”¯æŒä»»æ„å®½é«˜æ¯”ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œ\textit{omniedit}åœ¨è‡ªåŠ¨è¯„ä¼°å’Œäººå·¥è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06176', 'title': 'M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework', 'url': 'https://huggingface.co/papers/2411.06176', 'abstract': 'The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io.', 'score': 42, 'issue_id': 521, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': '950719af940fd8d0', 'authors': ['Yew Ken Chia', 'Liying Cheng', 'Hou Pong Chan', 'Chaoqun Liu', 'Maojia Song', 'Sharifah Mahani Aljunied', 'Soujanya Poria', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group, Singapore', 'Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06176.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#dataset', '#multimodal', '#open_source', '#training'], 'emoji': 'ğŸ“„', 'ru': {'title': 'M-LongDoc: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ M-LongDoc - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ…. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 851 Ğ¾Ğ±Ñ€Ğ°Ğ·ĞµÑ† ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 4.6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Enhancing Multimodal Document Understanding with M-LongDoc', 'desc': 'This paper presents M-LongDoc, a benchmark designed to evaluate large multimodal models on lengthy documents that include text, figures, and tables. The authors introduce a retrieval-aware tuning method that enhances the efficiency and effectiveness of multimodal document reading, particularly for open-ended question-answering tasks. Unlike previous benchmarks, M-LongDoc features more recent and extensive documents, requiring models to generate comprehensive answers rather than just extractive responses. Experimental results indicate that the proposed tuning approach improves the accuracy of model responses by 4.6% compared to existing baseline models.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ–‡æ¡£ç†è§£çš„æ•ˆç‡ä¸æ•ˆæœ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºM-LongDocçš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«851ä¸ªæ ·æœ¬ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨æ–‡æ¡£ç†è§£å’Œé—®ç­”ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚ç”±äºæ–‡æ¡£é€šå¸¸åŒ…å«æ–‡æœ¬ã€å›¾å½¢å’Œè¡¨æ ¼ç­‰å¤šç§å†…å®¹ï¼Œäººå·¥é˜…è¯»è€—æ—¶è¾ƒé•¿ï¼Œå› æ­¤éœ€è¦å¼€å‘æœ‰æ•ˆçš„è‡ªåŠ¨åŒ–æ–¹æ³•æ¥è¾…åŠ©äººç±»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ£€ç´¢æ„ŸçŸ¥çš„è°ƒä¼˜æ–¹æ³•ï¼Œä»¥æé«˜å¤šæ¨¡æ€æ–‡æ¡£é˜…è¯»çš„æ•ˆç‡å’Œæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡å‹å“åº”çš„æ­£ç¡®æ€§ä¸Šç›¸è¾ƒäºåŸºçº¿å¼€æºæ¨¡å‹æœ‰4.6%çš„ç›¸å¯¹æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07140', 'title': 'Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models', 'url': 'https://huggingface.co/papers/2411.07140', 'abstract': 'New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.', 'score': 33, 'issue_id': 522, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'ffca97b13123516b', 'authors': ['Yancheng He', 'Shilong Li', 'Jiaheng Liu', 'Yingshui Tan', 'Weixun Wang', 'Hui Huang', 'Xingyuan Bu', 'Hangyu Guo', 'Chengwei Hu', 'Boren Zheng', 'Zhuoran Lin', 'Xuepeng Liu', 'Dekai Sun', 'Shirong Lin', 'Zhicheng Zheng', 'Xiaoyong Zhu', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Taobao & Tmall Group of Alibaba'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07140.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#low_resource', '#multilingual'], 'emoji': 'ğŸ‡¨ğŸ‡³', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Chinese SimpleQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 6 Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ‚ĞµĞ¼ Ğ¸ 99 Ğ¿Ğ¾Ğ´Ñ‚ĞµĞ¼, Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Chinese SimpleQA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ API OpenAI Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²Ğ¾Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼.'}, 'en': {'title': 'Empowering Chinese LLMs with SimpleQA Factuality Benchmark', 'desc': 'This paper introduces Chinese SimpleQA, a new benchmark designed to evaluate the factuality of Large Language Models (LLMs) specifically for the Chinese language. It features a diverse set of questions across six major topics, ensuring high-quality and static reference answers for consistency in evaluation. The benchmark emphasizes short questions and answers, making the grading process straightforward and efficient, particularly using the OpenAI API. The authors aim for Chinese SimpleQA to help developers assess and improve the factuality capabilities of their models in the Chinese context.'}, 'zh': {'title': 'ä¸­æ–‡SimpleQAï¼šæå‡è¯­è¨€æ¨¡å‹äº‹å®èƒ½åŠ›çš„åŸºå‡†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸­æ–‡SimpleQAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹å›ç­”çŸ­é—®é¢˜çš„äº‹å®èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†ä¸“æ³¨äºä¸­æ–‡ï¼Œæ¶µç›–å…­ä¸ªä¸»è¦ä¸»é¢˜å’Œ99ä¸ªå¤šæ ·åŒ–çš„å­ä¸»é¢˜ã€‚æˆ‘ä»¬é€šè¿‡ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶è¿‡ç¨‹ï¼Œç¡®ä¿é—®é¢˜å’Œç­”æ¡ˆçš„é«˜è´¨é‡ï¼Œå¹¶ä¸”å‚è€ƒç­”æ¡ˆæ˜¯é™æ€çš„ï¼Œä¸ä¼šéšæ—¶é—´å˜åŒ–ã€‚å¸Œæœ›ä¸­æ–‡SimpleQAèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£å…¶æ¨¡å‹çš„ä¸­æ–‡äº‹å®èƒ½åŠ›ï¼Œä¿ƒè¿›åŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07126', 'title': 'Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models', 'url': 'https://huggingface.co/papers/2411.07126', 'abstract': 'We introduce Edify Image, a family of diffusion models capable of generating photorealistic image content with pixel-perfect accuracy. Edify Image utilizes cascaded pixel-space diffusion models trained using a novel Laplacian diffusion process, in which image signals at different frequency bands are attenuated at varying rates. Edify Image supports a wide range of applications, including text-to-image synthesis, 4K upsampling, ControlNets, 360 HDR panorama generation, and finetuning for image customization.', 'score': 27, 'issue_id': 521, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'a7486a925b416669', 'authors': ['NVIDIA', ':', 'Yuval Atzmon', 'Maciej Bala', 'Yogesh Balaji', 'Tiffany Cai', 'Yin Cui', 'Jiaojiao Fan', 'Yunhao Ge', 'Siddharth Gururani', 'Jacob Huffman', 'Ronald Isaac', 'Pooya Jannaty', 'Tero Karras', 'Grace Lam', 'J. P. Lewis', 'Aaron Licata', 'Yen-Chen Lin', 'Ming-Yu Liu', 'Qianli Ma', 'Arun Mallya', 'Ashlee Martino-Tarr', 'Doug Mendez', 'Seungjun Nah', 'Chris Pruett', 'Fitsum Reda', 'Jiaming Song', 'Ting-Chun Wang', 'Fangyin Wei', 'Xiaohui Zeng', 'Yu Zeng', 'Qinsheng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07126.jpg', 'data': {'categories': ['#3d', '#diffusion', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¤Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Edify Image - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ»Ğ°Ğ¿Ğ»Ğ°ÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ°Ñ… Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‚ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ. Edify Image Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ ÑĞ¿ĞµĞºÑ‚Ñ€ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ğ°Ğ¿ÑĞºĞµĞ¹Ğ»Ğ¸Ğ½Ğ³ Ğ´Ğ¾ 4K, ControlNets Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼ HDR 360Â°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ´Ğ»Ñ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Edify Image: Revolutionizing Photorealistic Image Generation with Precision', 'desc': 'Edify Image is a new set of diffusion models designed to create highly realistic images with precise detail. It employs a unique Laplacian diffusion process that adjusts the diffusion rates for different frequency bands of image signals. This allows for versatile applications such as generating images from text, enhancing image resolution to 4K, and creating panoramic images. Additionally, it offers customization options through finetuning, making it adaptable for various image generation tasks.'}, 'zh': {'title': 'Edify Imageï¼šç”ŸæˆçœŸå®æ„Ÿå›¾åƒçš„æ–°çªç ´', 'desc': 'Edify Imageæ˜¯ä¸€ç§æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆåƒç´ çº§ç²¾ç¡®çš„çœŸå®æ„Ÿå›¾åƒå†…å®¹ã€‚å®ƒé‡‡ç”¨çº§è”åƒç´ ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ–°é¢–çš„æ‹‰æ™®æ‹‰æ–¯æ‰©æ•£è¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿä»¥ä¸åŒçš„é€Ÿç‡è¡°å‡ä¸åŒé¢‘ç‡å¸¦çš„å›¾åƒä¿¡å·ã€‚è¯¥æ¨¡å‹æ”¯æŒå¤šç§åº”ç”¨ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€4Kè¶…åˆ†è¾¨ç‡ã€ControlNetsã€360 HDRå…¨æ™¯ç”Ÿæˆä»¥åŠå›¾åƒå®šåˆ¶çš„å¾®è°ƒã€‚Edify Imageåœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°äº†å¼ºå¤§çš„çµæ´»æ€§å’Œé«˜è´¨é‡çš„è¾“å‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05830', 'title': 'GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models', 'url': 'https://huggingface.co/papers/2411.05830', 'abstract': "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\GitChameleon{}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.  is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, GPT-4o achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries,  serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.", 'score': 20, 'issue_id': 528, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': '86b9530c6bdf431e', 'authors': ['Nizar Islah', 'Justine Gehring', 'Diganta Misra', 'Eilif Muller', 'Irina Rish', 'Terry Yue Zhuo', 'Massimo Caccia'], 'affiliations': ['Mila - Quebec AI Institute', 'MPI-IS Tubingen, ELLIS Tubingen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05830.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#open_source', '#plp', '#dataset'], 'emoji': 'ğŸ¦', 'ru': {'title': 'GitChameleon: Ğ¢ĞµÑÑ‚ Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²ĞµÑ€ÑĞ¸ÑĞ¼ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ GitChameleon - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ´, ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞº. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 116 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Python Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ½Ğ¸Ñ‚-Ñ‚ĞµÑÑ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹ - GPT-4 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 39.9% ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· 10 Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº. GitChameleon Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Adapting Code Generation to Evolving Libraries with GitChameleon', 'desc': 'This paper addresses the challenges faced by code generation models due to the rapid evolution of software libraries. It introduces \textit{GitChameleon}, a new dataset with 116 Python code completion problems that are specifically tied to different library versions and include executable unit tests. The dataset allows for a more rigorous evaluation of large language models (LLMs) in generating code that is both syntactically correct and functionally accurate. The findings indicate that current state-of-the-art models, like GPT-4o, struggle with this task, underscoring the need for improved adaptability in code generation systems.'}, 'zh': {'title': 'åº”å¯¹è½¯ä»¶åº“ç‰ˆæœ¬æ›´æ–°çš„ä»£ç ç”ŸæˆæŒ‘æˆ˜', 'desc': 'éšç€è½¯ä»¶åº“çš„å¿«é€Ÿå‘å±•ï¼Œä»£ç ç”Ÿæˆæ¨¡å‹é¢ä¸´ç€é€‚åº”é¢‘ç¹ç‰ˆæœ¬æ›´æ–°çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶è¿˜éœ€ä¿æŒä¸æ—§ç‰ˆæœ¬çš„å…¼å®¹æ€§ã€‚ç°æœ‰çš„ä»£ç è¡¥å…¨åŸºå‡†æµ‹è¯•å¾€å¾€å¿½è§†äº†è¿™ä¸€åŠ¨æ€ç‰¹æ€§ï¼Œè€Œè€ƒè™‘åˆ°è¿™ä¸€ç‚¹çš„æµ‹è¯•åˆä¾èµ–äºé™æ€ä»£ç é¢„æµ‹ä»»åŠ¡ï¼Œç¼ºä¹åŸºäºæ‰§è¡Œçš„è¯„ä¼°ï¼Œé™åˆ¶äº†æ¨¡å‹çš„å®é™…å¯ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textit{GitChameleon}ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ‰‹åŠ¨æ•´ç†æ•°æ®é›†ï¼ŒåŒ…å«116ä¸ªPythonä»£ç è¡¥å…¨é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½åŸºäºç‰¹å®šçš„åº“ç‰ˆæœ¬ï¼Œå¹¶é™„æœ‰å¯æ‰§è¡Œçš„å•å…ƒæµ‹è¯•ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç‰ˆæœ¬ç‰¹å®šä»£ç æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†è¿™ä¸€é—®é¢˜çš„å¤æ‚æ€§å’Œç°æœ‰æ¨¡å‹çš„å±€é™æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07231', 'title': 'Watermark Anything with Localized Messages', 'url': 'https://huggingface.co/papers/2411.07231', 'abstract': 'Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. We introduce a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). The WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked. The models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks. Experiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. Moreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions - no larger than 10% of the image surface - even for small 256times 256 images.', 'score': 18, 'issue_id': 524, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '0f19ada656cd9116', 'authors': ['Tom Sander', 'Pierre Fernandez', 'Alain Durmus', 'Teddy Furon', 'Matthijs Douze'], 'affiliations': ['Meta FAIR', 'Ã‰cole polytechnique', 'Inria Rennes'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07231.jpg', 'data': {'categories': ['#training', '#cv', '#dataset'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'WAM: Ğ›Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ´ÑĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°ĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Watermark Anything Model (WAM). WAM ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ Ğ¸Ñ…, Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ±Ñ‹Ğ»Ğ¾ Ğ¾Ñ‚Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¾ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ WAM ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ½ĞµĞ·Ğ°Ğ¼ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ²ÑÑ‚Ğ°Ğ²Ğ¾Ğº Ğ¸ ÑĞºĞ»ĞµĞµĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Watermark Anything: Revolutionizing Localized Image Watermarking', 'desc': 'The paper presents a novel deep-learning model called the Watermark Anything Model (WAM) designed for localized image watermarking. Unlike traditional methods, WAM effectively handles small watermarked areas, making it suitable for images with diverse sources or edits. The model consists of an embedder that subtly alters the image and an extractor that identifies watermarked regions to recover hidden messages. Experimental results demonstrate that WAM achieves high imperceptibility and robustness against common image manipulations, outperforming existing techniques, especially in challenging scenarios involving splicing and inpainting.'}, 'zh': {'title': 'å±€éƒ¨æ°´å°çš„æ–°çªç ´ï¼šæ°´å°ä»»ä½•æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç§°ä¸ºæ°´å°ä»»ä½•æ¨¡å‹ï¼ˆWAMï¼‰ï¼Œç”¨äºå±€éƒ¨å›¾åƒæ°´å°ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸æ˜¾è‘—æ”¹å˜å›¾åƒçš„æƒ…å†µä¸‹ï¼ŒåµŒå…¥æ°´å°å¹¶æå–éšè—ä¿¡æ¯ã€‚WAMé€šè¿‡è”åˆè®­ç»ƒå’ŒåæœŸè®­ç»ƒï¼Œç¡®ä¿æ°´å°çš„éšè”½æ€§å’Œå¤šé‡æ°´å°çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒWAMåœ¨éšè”½æ€§å’Œé²æ£’æ€§æ–¹é¢ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“ï¼Œå°¤å…¶åœ¨å¤„ç†æ‹¼æ¥å’Œä¿®å¤æ—¶è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06208', 'title': 'IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization', 'url': 'https://huggingface.co/papers/2411.06208', 'abstract': 'In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.', 'score': 18, 'issue_id': 521, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'de83b5a8e14da36e', 'authors': ['Xinghua Zhang', 'Haiyang Yu', 'Cheng Fu', 'Fei Huang', 'Yongbin Li'], 'affiliations': ['Alibaba Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06208.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#optimization', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'TRACE Ğ¸ IOPO: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TRACE - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ IOPO (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ²Ğ¾Ğ´Ğ°-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. TRACE Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 120 Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 1000 Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Enhancing LLMs with TRACE and IOPO for Complex Instructions', 'desc': 'This paper addresses the challenge of large language models (LLMs) in following complex instructions, which is becoming increasingly important as their applications grow. It introduces TRACE, a benchmark designed to enhance and evaluate the ability of LLMs to handle complex instructions, featuring a substantial dataset of 120K training examples and 1K evaluation cases. The authors propose a novel alignment method called IOPO (Input-Output Preference Optimization), which focuses on both input and output preferences to improve LLM responses. Experimental results demonstrate that IOPO significantly enhances performance on both in-domain and out-of-domain datasets, outperforming existing methods like SFT and DPO.'}, 'zh': {'title': 'æå‡å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¢†åŸŸï¼Œæ¨¡å‹å‡†ç¡®éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‡ä»¤å¤æ‚æ€§è¿…é€Ÿå¢åŠ çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†TRACEï¼Œä¸€ä¸ªç”¨äºæé«˜å’Œè¯„ä¼°å¤æ‚æŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„åŸºå‡†ï¼ŒåŒ…å«12ä¸‡æ¡è®­ç»ƒæ•°æ®å’Œ1000æ¡è¯„ä¼°æ•°æ®ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†IOPOï¼ˆè¾“å…¥-è¾“å‡ºåå¥½ä¼˜åŒ–ï¼‰å¯¹é½æ–¹æ³•ï¼Œè€ƒè™‘äº†è¾“å…¥å’Œè¾“å‡ºåå¥½å¯¹ï¼Œå¸®åŠ©LLMså¿«é€Ÿå¯¹é½å“åº”åå¥½å¹¶æ·±å…¥æ¢ç´¢æŒ‡ä»¤åå¥½ã€‚é€šè¿‡åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒéªŒè¯äº†IOPOçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨é¢†åŸŸå†…æ•°æ®ä¸Šåˆ†åˆ«æé«˜äº†8.15%å’Œ2.18%ï¼Œåœ¨é¢†åŸŸå¤–æ•°æ®ä¸Šæé«˜äº†6.29%å’Œ3.13%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05902', 'title': 'Autoregressive Models in Vision: A Survey', 'url': 'https://huggingface.co/papers/2411.05902', 'abstract': 'Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the strategy of representation. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multi-faceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multi-modal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.', 'score': 12, 'issue_id': 532, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '49d4d6e55fb35ca2', 'authors': ['Jing Xiong', 'Gongye Liu', 'Lun Huang', 'Chengyue Wu', 'Taiqiang Wu', 'Yao Mu', 'Yuan Yao', 'Hui Shen', 'Zhongwei Wan', 'Jinfa Huang', 'Chaofan Tao', 'Shen Yan', 'Huaxiu Yao', 'Lingpeng Kong', 'Hongxia Yang', 'Mi Zhang', 'Guillermo Sapiro', 'Jiebo Luo', 'Ping Luo', 'Ngai Wong'], 'affiliations': ['The University of Hong Kong', 'University of Rochester', 'The University of North Carolina at Chapel Hill', 'The Hong Kong Polytechnic University', 'Tsinghua University', 'The Ohio State University', 'Apple', 'Princeton University', 'Duke University', 'Bytedance'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05902.jpg', 'data': {'categories': ['#healthcare', '#survey', '#video', '#3d', '#open_source', '#multimodal', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸: Ğ¾Ñ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ´Ğ¾ 3D', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ². Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ¸ 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ğ˜Ğ˜. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ĞµÑ‚ÑÑ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Exploring Autoregressive Models: Bridging NLP and Computer Vision', 'desc': 'This paper surveys the application of autoregressive models in computer vision, highlighting their success in generating high-quality visual content. It categorizes these models into three main types: pixel-based, token-based, and scale-based, reflecting the hierarchical nature of visual data. The authors also discuss the relationship between autoregressive models and other generative models, as well as their applications in various fields such as embodied AI and 3D medical AI. Additionally, the paper addresses current challenges and suggests future research directions, providing a comprehensive resource for researchers in the area.'}, 'zh': {'title': 'è‡ªå›å½’æ¨¡å‹ï¼šè§†è§‰ç”Ÿæˆçš„æ–°å‰æ²¿', 'desc': 'è‡ªå›å½’å»ºæ¨¡åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œæœ€è¿‘åœ¨è®¡ç®—æœºè§†è§‰ä¸­ä¹Ÿæˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒå’Œè§†é¢‘ã€‚æœ¬æ–‡ç»¼è¿°äº†è‡ªå›å½’æ¨¡å‹åœ¨è§†è§‰ä¸­çš„åº”ç”¨ï¼Œæ¢è®¨äº†åƒç´ çº§ã€æ ‡è®°çº§å’Œå°ºåº¦çº§ç­‰ä¸åŒçš„è¡¨ç¤ºç­–ç•¥ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è‡ªå›å½’æ¨¡å‹ä¸å…¶ä»–ç”Ÿæˆæ¨¡å‹çš„å…³ç³»ï¼Œå¹¶æå‡ºäº†åœ¨æ–°å…´é¢†åŸŸä¸­çš„åº”ç”¨å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05990', 'title': 'Game-theoretic LLM: Agent Workflow for Negotiation Games', 'url': 'https://huggingface.co/papers/2411.05990', 'abstract': "This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.   To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.   Our research contributes to a deeper understanding of LLMs' decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.", 'score': 6, 'issue_id': 522, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': 'aae23469f2886f4c', 'authors': ['Wenyue Hua', 'Ollie Liu', 'Lingyao Li', 'Alfonso Amayuelas', 'Julie Chen', 'Lucas Jiang', 'Mingyu Jin', 'Lizhou Fan', 'Fei Sun', 'William Wang', 'Xintong Wang', 'Yongfeng Zhang'], 'affiliations': ['Rutgers University, New Brunswick', 'University of Southern California', 'University of South Florida', 'University of California, Santa Barbara', 'Independent Researcher', 'Harvard University', 'Institute of Computing Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05990.jpg', 'data': {'categories': ['#games', '#agents', '#rl', '#math', '#reasoning'], 'emoji': 'ğŸ²', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¸Ğ³Ñ€. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ LLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€Ğ°Ñ… Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑÑ‚ÑÑ Ğ¾Ñ‚ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑÑ‚ÑŒ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ¸Ñ ĞÑÑˆĞ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ LLM Ğ² Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ².'}, 'en': {'title': 'Enhancing LLM Rationality in Strategic Decision-Making', 'desc': "This paper examines how large language models (LLMs) make decisions in strategic situations using game theory. It finds that LLMs often do not follow rational strategies, especially in complex games with larger payoff matrices. To improve their decision-making, the authors propose game-theoretic workflows that help LLMs better compute Nash Equilibria and make rational choices under uncertainty. The results show that these workflows significantly enhance the models' ability to identify optimal strategies and perform better in negotiation scenarios."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„åšå¼ˆç†æ€§', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æˆ˜ç•¥å†³ç­–ä¸­çš„ç†æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åšå¼ˆè®ºæ¡†æ¶ä¸‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§å…ˆè¿›çš„LLMsåœ¨å®Œå…¨ä¿¡æ¯å’Œä¸å®Œå…¨ä¿¡æ¯åšå¼ˆä¸­çš„è¡¨ç°ï¼Œå‘ç°éšç€åšå¼ˆå¤æ‚æ€§çš„å¢åŠ ï¼ŒLLMså¸¸å¸¸åç¦»ç†æ€§ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¤šç§åšå¼ˆè®ºå·¥ä½œæµç¨‹ï¼Œä»¥æŒ‡å¯¼LLMsçš„æ¨ç†å’Œå†³ç­–è¿‡ç¨‹ï¼Œä»è€Œæé«˜å…¶è®¡ç®—çº³ä»€å‡è¡¡å’Œåœ¨ä¸ç¡®å®šæ¡ä»¶ä¸‹åšå‡ºç†æ€§é€‰æ‹©çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨è¿™äº›å·¥ä½œæµç¨‹æ˜¾è‘—æé«˜äº†LLMsåœ¨åšå¼ˆä»»åŠ¡ä¸­çš„ç†æ€§å’Œç¨³å¥æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06424', 'title': 'Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction', 'url': 'https://huggingface.co/papers/2411.06424', 'abstract': 'Safety fine-tuning algorithms are commonly used to fine-tune language models to reduce harmful outputs, but the exact internal mechanisms of how those models achieve this remain unclear. In studying direct preference optimisation (DPO) for toxicity reduction, current explanations claim that DPO works by dampening the most toxic MLP neurons to learn an offset to avert toxic regions in the residual stream. However, by ablating the most toxic neurons and applying activation patching, we find this explanation incomplete. By projecting neuron activation changes onto a toxicity probe, we find that only 31.8\\% of toxicity reduction comes from dampened toxic neurons. Instead, DPO reduces toxicity by accumulating effects across multiple neuron groups, both reducing writing in the toxic direction and promoting anti-toxicity in the residual stream. Moreover, DPO gives noisy adjustments to neuron activations, with many neurons actually increasing toxicity. This indicates that DPO is a balancing process between opposing neuron effects to achieve toxicity reduction.', 'score': 5, 'issue_id': 524, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '045698e0e102aa83', 'authors': ['Yushi Yang', 'Filip Sondej', 'Harry Mayne', 'Adam Mahdi'], 'affiliations': ['University of Oxford', 'Jagiellonian University', 'University of Oxford'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06424.jpg', 'data': {'categories': ['#ethics', '#training', '#hallucinations', '#rlhf', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'DPO: Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (DPO) Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ³Ğ°ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ DPO Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 31.8% ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ². DPO ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ°ĞºĞ°Ğ¿Ğ»Ğ¸Ğ²Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², ĞºĞ°Ğº ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°Ñ Ğ°Ğ½Ñ‚Ğ¸-Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ğµ Ğ² Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ.'}, 'en': {'title': 'DPO: Balancing Neuron Effects for Safer Language Models', 'desc': 'This paper investigates how direct preference optimization (DPO) helps reduce harmful outputs in language models. It challenges the common belief that DPO primarily works by dampening the most toxic neurons. Instead, the study reveals that only a small portion of toxicity reduction is due to this dampening effect, with most of the reduction coming from a complex interplay of multiple neuron groups. The findings suggest that DPO functions as a balancing act, where both toxic and anti-toxic neuron activations are adjusted to achieve an overall decrease in toxicity.'}, 'zh': {'title': 'DPOï¼šåœ¨å¯¹ç«‹ç¥ç»å…ƒæ•ˆåº”ä¸­å®ç°æ¯’æ€§å‡å°‘çš„å¹³è¡¡è¿‡ç¨‹', 'desc': 'å®‰å…¨å¾®è°ƒç®—æ³•å¸¸ç”¨äºè°ƒæ•´è¯­è¨€æ¨¡å‹ï¼Œä»¥å‡å°‘æœ‰å®³è¾“å‡ºï¼Œä½†å…¶å†…éƒ¨æœºåˆ¶ä»ä¸æ¸…æ¥šã€‚æœ¬æ–‡ç ”ç©¶äº†ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰åœ¨å‡å°‘æ¯’æ€§æ–¹é¢çš„ä½œç”¨ï¼Œå‘ç°ç°æœ‰è§£é‡Šä¸å¤Ÿå…¨é¢ã€‚é€šè¿‡å¯¹æœ€æ¯’ç¥ç»å…ƒçš„æ¶ˆèå’Œæ¿€æ´»ä¿®è¡¥ï¼Œæˆ‘ä»¬å‘ç°åªæœ‰31.8%çš„æ¯’æ€§å‡å°‘æ¥è‡ªäºæŠ‘åˆ¶æ¯’æ€§ç¥ç»å…ƒã€‚DPOé€šè¿‡å¤šä¸ªç¥ç»å…ƒç»„çš„ç´¯ç§¯æ•ˆåº”æ¥å‡å°‘æ¯’æ€§ï¼ŒåŒæ—¶åœ¨æ®‹å·®æµä¸­ä¿ƒè¿›åæ¯’æ€§ï¼Œè¡¨æ˜DPOæ˜¯ä¸€ä¸ªåœ¨å¯¹ç«‹ç¥ç»å…ƒæ•ˆåº”ä¹‹é—´å¹³è¡¡çš„è¿‡ç¨‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05966', 'title': 'Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation', 'url': 'https://huggingface.co/papers/2411.05966', 'abstract': 'Large language models (LLMs) have demonstrated significant success in natural language processing (NLP) tasks and have shown promising results in other domains such as protein sequence generation. However, there remain salient differences between LLMs used for NLP, which effectively handle multiple tasks and are available in small sizes, and protein language models that are often specialized for specific tasks and only exist in larger sizes. In this work, we introduce two small protein language models, based on Llama-3-8B and Phi-3-mini, that are capable of both uncontrollable and controllable protein generation. For the uncontrollable generation task, our best model achieves an average pLDDT score of 69.75, demonstrating robust performance in generating viable protein structures. For the controllable generation task, in which the model generates proteins according to properties specified in the prompt, we achieve a remarkable average TM-Score of 0.84, indicating high structural similarity to target proteins. We chose 10 properties, including six classes of enzymes, to extend the capabilities of prior protein language models. Our approach utilizes the Low-Rank Adaptor (LoRA) technique, reducing trainable parameters to just 4% of the original model size, lowering computational requirements. By using a subset of the UniRef50 dataset and small models, we reduced the overall training time by 70% without compromising performance. Notably, Phi-3-mini reduced trainable parameters by 60%, decreasing training cost by 30% compared to Llama 3. Consequently, Phi-3 achieved a comparable TM-Score of 0.81, demonstrating that smaller models can match the performance of larger ones, like Llama 3. We also demonstrate the deployment of our models on the energy efficient ET-SoC-1 chip, significantly improving the TPS/W by a factor of 3.', 'score': 4, 'issue_id': 534, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '7cf32991005092d0', 'authors': ['Aayush Shah', 'Shankar Jayaratnam'], 'affiliations': ['Esperanto Technologies'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05966.jpg', 'data': {'categories': ['#low_resource', '#optimization', '#science', '#dataset', '#small_models', '#training', '#inference'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ²Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ±ĞµĞ»ĞºĞ°Ğ¼Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Llama-3-8B Ğ¸ Phi-3-mini. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ĞºĞ°Ğº Ğº Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ pLDDT Ğ¸ TM-Score ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Low-Rank Adaptor (LoRA) Ğ´Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ.'}, 'en': {'title': 'Small Models, Big Impact: Efficient Protein Generation', 'desc': 'This paper presents two small protein language models, Llama-3-8B and Phi-3-mini, which excel in both uncontrollable and controllable protein generation tasks. The models achieve impressive performance metrics, with an average pLDDT score of 69.75 for generating viable protein structures and a TM-Score of 0.84 for generating proteins based on specified properties. By employing the Low-Rank Adaptor (LoRA) technique, the models significantly reduce the number of trainable parameters, leading to a 70% decrease in training time and a 30% reduction in training costs. The results indicate that smaller models can achieve comparable performance to larger models while being more efficient, especially when deployed on energy-efficient hardware.'}, 'zh': {'title': 'å°å‹æ¨¡å‹ä¹Ÿèƒ½ç”Ÿæˆé«˜è´¨é‡è›‹ç™½è´¨', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†ä¸¤ç§å°å‹è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºLlama-3-8Bå’ŒPhi-3-miniï¼Œèƒ½å¤Ÿè¿›è¡Œä¸å¯æ§å’Œå¯æ§çš„è›‹ç™½è´¨ç”Ÿæˆã€‚æˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨ä¸å¯æ§ç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ°äº†69.75çš„å¹³å‡pLDDTåˆ†æ•°ï¼Œæ˜¾ç¤ºå‡ºç”Ÿæˆå¯è¡Œè›‹ç™½è´¨ç»“æ„çš„å¼ºå¤§æ€§èƒ½ã€‚åœ¨å¯æ§ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ¨¡å‹æ ¹æ®æç¤ºç”Ÿæˆç‰¹å®šå±æ€§çš„è›‹ç™½è´¨ï¼Œå¹³å‡TM-Scoreè¾¾åˆ°äº†0.84ï¼Œè¡¨æ˜ä¸ç›®æ ‡è›‹ç™½è´¨çš„ç»“æ„ç›¸ä¼¼æ€§å¾ˆé«˜ã€‚é€šè¿‡ä½¿ç”¨ä½ç§©é€‚é…å™¨ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†å¯è®­ç»ƒå‚æ•°å‡å°‘åˆ°åŸæ¨¡å‹çš„4%ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05945', 'title': 'NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts', 'url': 'https://huggingface.co/papers/2411.05945', 'abstract': "Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an ``expert'' of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset's tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.", 'score': 4, 'issue_id': 528, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '1306ade09b2fc5c5', 'authors': ['Yen-Ting Lin', 'Chao-Han Huck Yang', 'Zhehuai Chen', 'Piotr Zelasko', 'Xuesong Yang', 'Zih-Ching Chen', 'Krishna C Puvvada', 'Szu-Wei Fu', 'Ke Hu', 'Jun Wei Chiu', 'Jagadeesh Balam', 'Boris Ginsburg', 'Yu-Chiang Frank Wang'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05945.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#machine_translation', '#transfer_learning', '#multimodal', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: Ğ¾Ğ´Ğ¸Ğ½ MoE Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture-of-Experts (MoE) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ñ‚Ñ‹ Ğ² MoE Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ NeKo Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Harnessing Mixture-of-Experts for Superior Post-Recognition Error Correction', 'desc': "This paper introduces a novel approach to post-recognition error correction using a Mixture-of-Experts (MoE) model. The authors propose a Multi-Task Correction MoE that effectively learns from diverse datasets, such as speech-to-text and language-to-text, by routing tokens to specialized experts. This method reduces the need for separate correction models, leading to fewer parameters while maintaining high performance. Experimental results demonstrate significant improvements in word error rate (WER) and BLEU scores, showcasing the model's effectiveness across various tasks, including zero-shot evaluations against leading models."}, 'zh': {'title': 'æ··åˆä¸“å®¶æ¨¡å‹ï¼šæå‡å¤šä»»åŠ¡å­¦ä¹ çš„åˆ©å™¨', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æœ‰æ•ˆåœ°è®­ç»ƒä¸€ä¸ªé€šç”¨çš„åè¯†åˆ«é”™è¯¯ä¿®æ­£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šç§é¢†åŸŸæ•°æ®é›†æ—¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMixture-of-Expertsï¼‰ï¼Œé€šè¿‡è®©ä¸“å®¶å­¦ä¹ ç‰¹å®šæ•°æ®é›†çš„ç‰¹å¾ï¼Œä»è€Œåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­æ•´åˆè¿™äº›çŸ¥è¯†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨è¯­éŸ³è½¬æ–‡æœ¬ã€è¯­è¨€è½¬æ–‡æœ¬å’Œè§†è§‰è½¬æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—é™ä½äº†é”™è¯¯ç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ï¼Œå±•ç¤ºäº†æ··åˆä¸“å®¶æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07180', 'title': 'Counterfactual Generation from Language Models', 'url': 'https://huggingface.co/papers/2411.07180', 'abstract': "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.", 'score': 4, 'issue_id': 521, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': '6b57fa07bdf242ce', 'authors': ['Shauli Ravfogel', 'Anej Svete', 'VÃ©steinn SnÃ¦bjarnarson', 'Ryan Cotterell'], 'affiliations': ['ETH Zurich', 'University of Copenhagen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07180.jpg', 'data': {'categories': ['#math', '#interpretability', '#reasoning', '#training', '#data'], 'emoji': 'ğŸ”€', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²ÑĞ·Ğ¸', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ°Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€ÑĞº Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ-Ğ¼Ğ°ĞºÑĞ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ². Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ“ÑƒĞ¼Ğ±ĞµĞ»Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº.'}, 'en': {'title': 'Harnessing Counterfactuals for Better Control of Language Models', 'desc': 'This paper focuses on understanding how to control language models by manipulating their causal generation mechanisms. It critiques existing methods like representation surgery, which alter model behavior but may not provide precise insights into their effects. The authors introduce a new framework that uses counterfactual reasoning to generate true string counterfactuals, distinguishing it from traditional interventions. Their approach employs Generalized Structural-equation Models and Gumbel-max sampling to effectively model the relationship between original strings and their counterfactuals, revealing the limitations of current intervention techniques.'}, 'zh': {'title': 'æŒæ¡è¯­è¨€æ¨¡å‹çš„å› æœç”Ÿæˆæœºåˆ¶', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨è¯­è¨€æ¨¡å‹ä¸­ç†è§£å’Œæ“æ§å› æœç”Ÿæˆæœºåˆ¶çš„é‡è¦æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦ä¾èµ–äºè¡¨ç¤ºæ‰‹æœ¯ç­‰æŠ€æœ¯æ¥å¹²é¢„æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å¼ºè°ƒåäº‹å®æ¨ç†ä¸å¹²é¢„æ˜¯ä¸åŒçš„æ¦‚å¿µã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œé€šè¿‡å°†è¯­è¨€æ¨¡å‹é‡æ„ä¸ºå¹¿ä¹‰ç»“æ„æ–¹ç¨‹æ¨¡å‹ï¼Œç”ŸæˆçœŸå®çš„å­—ç¬¦ä¸²åäº‹å®ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæœ‰æ„ä¹‰çš„åäº‹å®ï¼ŒåŒæ—¶æ­ç¤ºäº†å¸¸ç”¨å¹²é¢„æŠ€æœ¯çš„æ˜¾è‘—å‰¯ä½œç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06481', 'title': 'KMM: Key Frame Mask Mamba for Extended Motion Generation', 'url': 'https://huggingface.co/papers/2411.06481', 'abstract': "Human motion generation is a cut-edge area of research in generative computer vision, with promising applications in video creation, game development, and robotic manipulation. The recent Mamba architecture shows promising results in efficiently modeling long and complex sequences, yet two significant challenges remain: Firstly, directly applying Mamba to extended motion generation is ineffective, as the limited capacity of the implicit memory leads to memory decay. Secondly, Mamba struggles with multimodal fusion compared to Transformers, and lack alignment with textual queries, often confusing directions (left or right) or omitting parts of longer text queries. To address these challenges, our paper presents three key contributions: Firstly, we introduce KMM, a novel architecture featuring Key frame Masking Modeling, designed to enhance Mamba's focus on key actions in motion segments. This approach addresses the memory decay problem and represents a pioneering method in customizing strategic frame-level masking in SSMs. Additionally, we designed a contrastive learning paradigm for addressing the multimodal fusion problem in Mamba and improving the motion-text alignment. Finally, we conducted extensive experiments on the go-to dataset, BABEL, achieving state-of-the-art performance with a reduction of more than 57% in FID and 70% parameters compared to previous state-of-the-art methods. See project website: https://steve-zeyu-zhang.github.io/KMM", 'score': 3, 'issue_id': 524, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': '970f532cd0007e6c', 'authors': ['Zeyu Zhang', 'Hang Gao', 'Akide Liu', 'Qi Chen', 'Feng Chen', 'Yiran Wang', 'Danning Li', 'Hao Tang'], 'affiliations': ['Peking University', 'The University of Adelaide', 'The Australian National University', 'The University of Sydney', 'Monash University', 'McGill University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06481.jpg', 'data': {'categories': ['#cv', '#optimization', '#games', '#multimodal', '#long_context', '#architecture', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'KMM: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Mamba', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ KMM Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mamba. KMM Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Mamba, Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ BABEL Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Enhancing Motion Generation with Key Frame Masking', 'desc': "This paper focuses on improving human motion generation using a new architecture called KMM, which stands for Key frame Masking Modeling. The KMM architecture enhances the Mamba model's ability to handle long motion sequences by addressing memory decay and improving focus on key actions. Additionally, it introduces a contrastive learning approach to better fuse multimodal data and align motion with textual queries, overcoming limitations seen in previous models. The authors demonstrate the effectiveness of their method through extensive experiments on the BABEL dataset, achieving significant performance improvements over existing techniques."}, 'zh': {'title': 'æå‡äººç±»åŠ¨ä½œç”Ÿæˆçš„å…³é”®æŠ€æœ¯', 'desc': 'äººç±»åŠ¨ä½œç”Ÿæˆæ˜¯ç”Ÿæˆè®¡ç®—æœºè§†è§‰ä¸­çš„å‰æ²¿ç ”ç©¶é¢†åŸŸï¼Œå…·æœ‰è§†é¢‘åˆ›ä½œã€æ¸¸æˆå¼€å‘å’Œæœºå™¨äººæ“ä½œç­‰åº”ç”¨å‰æ™¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„KMMæ¶æ„ï¼Œé€šè¿‡å…³é”®å¸§æ©è”½å»ºæ¨¡æ¥å¢å¼ºMambaåœ¨åŠ¨ä½œç‰‡æ®µä¸­çš„å…³é”®åŠ¨ä½œå…³æ³¨èƒ½åŠ›ï¼Œä»è€Œè§£å†³äº†å†…å­˜è¡°å‡é—®é¢˜ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ èŒƒå¼ï¼Œä»¥æ”¹å–„Mambaçš„å¤šæ¨¡æ€èåˆå’Œè¿åŠ¨-æ–‡æœ¬å¯¹é½é—®é¢˜ã€‚é€šè¿‡åœ¨BABELæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨FIDæŒ‡æ ‡ä¸Šå‡å°‘äº†è¶…è¿‡57%ï¼Œå¹¶ä¸”å‚æ•°é‡å‡å°‘äº†70%ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06272', 'title': 'Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models', 'url': 'https://huggingface.co/papers/2411.06272', 'abstract': 'As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose "Golden Touchstone", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models\' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.', 'score': 3, 'issue_id': 520, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': '2559c023f673c9b4', 'authors': ['Xiaojun Wu', 'Junxi Liu', 'Huanyi Su', 'Zhouchi Lin', 'Yiyan Qi', 'Chengjin Xu', 'Jiajun Su', 'Jiajie Zhong', 'Fuwei Wang', 'Saizhuo Wang', 'Fengrui Hua', 'Jia Li', 'Jian Guo'], 'affiliations': ['IDEA Research', 'The Hong Kong University of Science and Technology (Guangzhou)', 'The Hong Kong University of Science and Technology', 'Nanjing University', 'South China Normal University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06272.jpg', 'data': {'categories': ['#low_resource', '#optimization', '#open_source', '#multilingual', '#benchmark'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'Ğ­Ñ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ ÑĞµĞºÑ‚Ğ¾Ñ€Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ "Golden Touchstone", Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ NLP. Ğ­Ñ‚Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ĞºĞ¾Ğ´Ñƒ Ğ¸ Ğ²ĞµÑĞ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Touchstone-GPT, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… LLM.'}, 'en': {'title': 'Golden Touchstone: Elevating Financial LLM Evaluation', 'desc': "This paper introduces 'Golden Touchstone', a new bilingual benchmark designed to evaluate the performance of large language models (LLMs) in the financial sector. It addresses the shortcomings of existing benchmarks by providing a comprehensive assessment across eight key financial NLP tasks in both Chinese and English. The benchmark is built from high-quality datasets and reflects industry needs, allowing for a thorough evaluation of models like GPT-4o, Llama3, FinGPT, and FinMA. Additionally, the paper presents Touchstone-GPT, a financial LLM that has been fine-tuned for better performance on this benchmark, while also making the resources publicly available to support further research in financial LLMs."}, 'zh': {'title': 'é‡‘èé¢†åŸŸçš„æ ‡å‡†åŒ–è¯„ä¼°å·¥å…·â€”â€”é‡‘è‰²åŸºå‡†', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œè¯„ä¼°å…¶æ€§èƒ½çš„æ ‡å‡†åŒ–æ–¹æ³•å˜å¾—å°¤ä¸ºé‡è¦ã€‚ç°æœ‰çš„é‡‘èåŸºå‡†æµ‹è¯•å­˜åœ¨è¯­è¨€å’Œä»»åŠ¡è¦†ç›–é¢æœ‰é™ã€æ•°æ®é›†è´¨é‡ä½ä»¥åŠé€‚åº”æ€§ä¸è¶³ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œé‡‘è‰²åŸºå‡†â€ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢çš„åŒè¯­é‡‘èåŸºå‡†ï¼Œæ¶µç›–äº†ä¸­è‹±æ–‡çš„å…«ä¸ªæ ¸å¿ƒé‡‘èè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚é€šè¿‡å¯¹ä¸»è¦æ¨¡å‹çš„æ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†å®ƒä»¬åœ¨å¤„ç†å¤æ‚é‡‘èä¿¡æ¯æ—¶çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¼€æºäº†Touchstone-GPTæ¨¡å‹ï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶å’Œä¼˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04997', 'title': 'LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation', 'url': 'https://huggingface.co/papers/2411.04997', 'abstract': "CLIP is one of the most important multimodal foundational models today. What powers CLIP's capabilities? The rich supervision signals provided by natural language, the carrier of human knowledge, shape a powerful cross-modal representation space. However, with the rapid advancements in large language models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs' strong textual understanding can fundamentally improve CLIP's ability to handle image captions, drastically enhancing its ability to process long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP's visual encoder. Thanks to the LLM's presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP's text encoder's context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks.", 'score': 31, 'issue_id': 513, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '5e5b851688791e8a', 'authors': ['Weiquan Huang', 'Aoqi Wu', 'Yifan Yang', 'Xufang Luo', 'Yuqing Yang', 'Liang Hu', 'Qi Dai', 'Xiyang Dai', 'Dongdong Chen', 'Chong Luo', 'Lili Qiu'], 'affiliations': ['Tongji University', 'Microsoft Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04997.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#games', '#training'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ CLIP Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'LLM2CLIP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ CLIP Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ LLM Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ CLIP. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ LLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° CLIP. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° CLIP.'}, 'en': {'title': "Unlocking CLIP's Potential with LLMs", 'desc': "This paper introduces LLM2CLIP, a new method that enhances the CLIP model by integrating large language models (LLMs) like GPT-4. By fine-tuning the LLM in the caption space using contrastive learning, the model improves its ability to understand and generate complex image captions. The LLM acts as a teacher for CLIP's visual encoder, allowing it to process longer and more intricate texts than the original CLIP could handle. The results show significant advancements in cross-modal tasks, demonstrating the effectiveness of combining LLMs with multimodal representation learning."}, 'zh': {'title': 'åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æå‡CLIPçš„å¤šæ¨¡æ€å­¦ä¹ èƒ½åŠ›', 'desc': 'CLIPæ˜¯ä¸€ä¸ªé‡è¦çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•LLM2CLIPï¼Œæ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å¢å¼ºCLIPçš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹LLMè¿›è¡Œå¾®è°ƒå¹¶ç»“åˆå¯¹æ¯”å­¦ä¹ ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæå–å…¶æ–‡æœ¬èƒ½åŠ›ï¼Œä»è€Œæ˜¾è‘—æé«˜CLIPåœ¨å¤„ç†å›¾åƒæ ‡é¢˜æ—¶çš„è¡¨ç°ã€‚LLMçš„å¼ºå¤§æ–‡æœ¬ç†è§£èƒ½åŠ›ä½¿å¾—CLIPèƒ½å¤Ÿå¤„ç†æ›´é•¿å’Œæ›´å¤æ‚çš„æ–‡æœ¬ï¼Œå…‹æœäº†ä¼ ç»ŸCLIPçš„å±€é™æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04282', 'title': 'Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding', 'url': 'https://huggingface.co/papers/2411.04282', 'abstract': 'Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO.', 'score': 25, 'issue_id': 518, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': 'e566070395107bc0', 'authors': ['Haolin Chen', 'Yihao Feng', 'Zuxin Liu', 'Weiran Yao', 'Akshara Prabhakar', 'Shelby Heinecke', 'Ricky Ho', 'Phil Mui', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04282.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ LaTRO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. LaTRO Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸Ğ· Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞµĞ³Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ LLM Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GSM8K Ğ¸ ARC-Challenge Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Unlocking Reasoning Potential in Large Language Models with LaTRO', 'desc': 'This paper presents LaTent Reasoning Optimization (LaTRO), a new framework designed to enhance the reasoning abilities of large language models (LLMs) during training. LaTRO treats reasoning as a process of sampling from a latent distribution and uses variational methods to optimize this process. The framework allows LLMs to improve their reasoning skills and assess the quality of their reasoning simultaneously, without needing external feedback. Experimental results show that LaTRO significantly boosts the performance of LLMs on reasoning tasks, indicating that pre-trained models have untapped reasoning potential that can be further developed through this method.'}, 'zh': {'title': 'è§£é”å¤§å‹è¯­è¨€æ¨¡å‹çš„æ½œåœ¨æ¨ç†èƒ½åŠ›', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºLaTentæ¨ç†ä¼˜åŒ–ï¼ˆLaTROï¼‰ï¼Œå®ƒé€šè¿‡å˜åˆ†æ–¹æ³•å°†æ¨ç†è¿‡ç¨‹è§†ä¸ºä»æ½œåœ¨åˆ†å¸ƒä¸­é‡‡æ ·ã€‚LaTROå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œè¯„ä¼°æ¨ç†è´¨é‡çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€å¤–éƒ¨åé¦ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLaTROæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨GSM8Kå’ŒARC-Challengeæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œè¯æ˜äº†é¢„è®­ç»ƒLLMsçš„æ½œåœ¨æ¨ç†èƒ½åŠ›å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹æ³•å¾—åˆ°å¢å¼ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05288', 'title': 'Balancing Pipeline Parallelism with Vocabulary Parallelism', 'url': 'https://huggingface.co/papers/2411.05288', 'abstract': 'Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .', 'score': 18, 'issue_id': 509, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': '19accdb712f507d9', 'authors': ['Man Tsung Yeung', 'Penghui Qi', 'Min Lin', 'Xinyi Wan'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05288.jpg', 'data': {'categories': ['#architecture', '#inference', '#open_source', '#optimization', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾ĞµĞ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ ÑÑ…ĞµĞ¼Ğ°Ğ¼Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ¸ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ² ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ¾Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑĞ¼Ğ¸.'}, 'en': {'title': 'Balancing Memory and Computation in Pipeline Parallelism for Language Models', 'desc': 'This paper focuses on improving the efficiency of training large language models by addressing the imbalance caused by vocabulary layers in pipeline parallelism. The authors propose a method to evenly distribute vocabulary layers across different pipeline devices, which helps to balance computation and memory usage. They introduce algorithms to minimize communication delays within these layers and integrate their approach with existing pipeline schedules. The results show significant improvements in throughput and reduced memory usage, making the training process more efficient, especially for models with large vocabularies.'}, 'zh': {'title': 'ä¼˜åŒ–è¯æ±‡å±‚ä»¥å¹³è¡¡è®¡ç®—ä¸å†…å­˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œè¯æ±‡å±‚å¯¼è‡´çš„è®¡ç®—å’Œå†…å­˜ä¸å¹³è¡¡é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå°†è¯æ±‡å±‚å‡åŒ€åˆ†é…åˆ°ç®¡é“è®¾å¤‡ä¸Šï¼Œå¹¶å°†è®¡ç®—åˆ†ç»„åˆ°ç®¡é“ä¼ é€’ä¸­ã€‚ä¸ºäº†å‡å°‘æ¿€æ´»å†…å­˜å¼€é”€ï¼Œæˆ‘ä»¬è®¾è®¡äº†å‡ ç§ç®—æ³•æ¥é™ä½è¯æ±‡å±‚å†…çš„é€šä¿¡éšœç¢ã€‚é€šè¿‡ç»“åˆè¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—å’Œå‚æ•°å†…å­˜ä¹‹é—´å®ç°äº†æœ‰æ•ˆå¹³è¡¡ï¼Œå¹¶åœ¨å¤§è¯æ±‡åœºæ™¯ä¸‹æ˜¾è‘—é™ä½äº†å³°å€¼å†…å­˜ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05738', 'title': 'StdGEN: Semantic-Decomposed 3D Character Generation from Single Images', 'url': 'https://huggingface.co/papers/2411.05738', 'abstract': 'We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io', 'score': 13, 'issue_id': 507, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': 'b23d3650ace21f86', 'authors': ['Yuze He', 'Yanning Zhou', 'Wang Zhao', 'Zhongkai Wu', 'Kaiwen Xiao', 'Wei Yang', 'Yong-Jin Liu', 'Xiao Han'], 'affiliations': ['Tencent AI Lab', 'Tsinghua University', 'Beihang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05738.jpg', 'data': {'categories': ['#3d', '#diffusion', '#games'], 'emoji': 'ğŸ­', 'ru': {'title': 'StdGEN: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'StdGEN - ÑÑ‚Ğ¾ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ· Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ StdGEN Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Semantic-aware Large Reconstruction Model (S-LRM), Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ†Ğ²ĞµÑ‚ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ StdGEN Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ°Ğ½Ğ¸Ğ¼Ğµ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D Character Generation with StdGEN', 'desc': 'StdGEN is a novel pipeline designed to create high-quality 3D characters from single images, focusing on semantic decomposition. It overcomes limitations of previous methods by generating detailed characters with distinct components like body, clothing, and hair in just three minutes. The core of StdGEN is the Semantic-aware Large Reconstruction Model (S-LRM), which uses a transformer architecture to reconstruct geometry, color, and semantics efficiently. With additional features like a multi-layer surface extraction and a diffusion model, StdGEN achieves superior performance in 3D character generation, particularly for anime, allowing for easy customization and broad application.'}, 'zh': {'title': 'StdGENï¼šé«˜æ•ˆç”Ÿæˆå¯åˆ†è§£3Dè§’è‰²çš„åˆ›æ–°ç®¡é“', 'desc': 'StdGENæ˜¯ä¸€ç§åˆ›æ–°çš„ç®¡é“ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒç”Ÿæˆè¯­ä¹‰åˆ†è§£çš„é«˜è´¨é‡3Dè§’è‰²ï¼Œå¹¿æ³›åº”ç”¨äºè™šæ‹Ÿç°å®ã€æ¸¸æˆå’Œç”µå½±åˆ¶ä½œç­‰é¢†åŸŸã€‚ä¸ä»¥å¾€æ–¹æ³•ç›¸æ¯”ï¼ŒStdGENåœ¨å¯åˆ†è§£æ€§ã€æœ‰æ•ˆæ€§å’Œæ•ˆç‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä¸‰åˆ†é’Ÿå†…ç”Ÿæˆç»†è‡´çš„3Dè§’è‰²ï¼Œä¸”å„ä¸ªè¯­ä¹‰ç»„ä»¶å¦‚èº«ä½“ã€è¡£æœå’Œå¤´å‘åˆ†ç¦»ã€‚å…¶æ ¸å¿ƒæ˜¯è¯­ä¹‰æ„ŸçŸ¥çš„å¤§å‹é‡å»ºæ¨¡å‹ï¼ˆS-LRMï¼‰ï¼Œè¯¥æ¨¡å‹åŸºäºå˜æ¢å™¨ï¼Œèƒ½å¤Ÿä»å¤šè§†å›¾å›¾åƒä¸­è”åˆé‡å»ºå‡ ä½•ã€é¢œè‰²å’Œè¯­ä¹‰ã€‚é€šè¿‡å¼•å…¥å¯å¾®åˆ†çš„å¤šå±‚è¯­ä¹‰è¡¨é¢æå–æ–¹æ¡ˆï¼ŒStdGENå®ç°äº†é«˜è´¨é‡ã€å¯åˆ†è§£çš„3Dè§’è‰²ç”Ÿæˆï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨3DåŠ¨æ¼«è§’è‰²ç”Ÿæˆæ–¹é¢çš„æ€§èƒ½è¶…è¶Šäº†ç°æœ‰åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.02462', 'title': 'Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study', 'url': 'https://huggingface.co/papers/2411.02462', 'abstract': "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.", 'score': 9, 'issue_id': 508, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 4', 'zh': '11æœˆ4æ—¥'}, 'hash': '38beaabd86eeaa88', 'authors': ['AndrÃ© Storhaug', 'Jingyue Li'], 'affiliations': ['Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02462.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#plp'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (PEFT) Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ñ‚Ğ¾Ğ½ĞºÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ PEFT, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ LoRA, (IA)^3 Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ PEFT Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼ÑƒÑ Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², Ğ° LoRA Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸.'}, 'en': {'title': 'Unlocking Cost-Effective Fine-Tuning for Unit Test Generation', 'desc': 'This paper explores the use of parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) in the context of unit test generation. It highlights the challenges of full fine-tuning, which can be costly and resource-intensive, especially as LLMs increase in size. The authors evaluate various PEFT techniques, such as LoRA and prompt tuning, to determine their effectiveness compared to full fine-tuning. The results indicate that PEFT methods can achieve performance similar to full fine-tuning, with prompt tuning being the most efficient option for resource utilization.'}, 'zh': {'title': 'å‚æ•°é«˜æ•ˆå¾®è°ƒï¼šæå‡å•å…ƒæµ‹è¯•ç”Ÿæˆçš„ç»æµæ€§ä¸æœ‰æ•ˆæ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚ä¼ ç»Ÿçš„å…¨é‡å¾®è°ƒè™½ç„¶æœ‰æ•ˆï¼Œä½†æˆæœ¬é«˜æ˜‚ï¼ŒPEFTæ–¹æ³•é€šè¿‡åªå¾®è°ƒéƒ¨åˆ†å‚æ•°æ¥é™ä½è®¡ç®—å¼€é”€ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒPEFTæ–¹æ³•åœ¨å•å…ƒæµ‹è¯•ç”Ÿæˆä¸­èƒ½å¤Ÿè¾¾åˆ°ä¸å…¨é‡å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯æç¤ºå¾®è°ƒåœ¨æˆæœ¬å’Œèµ„æºåˆ©ç”¨ä¸Šæœ€ä¸ºæœ‰æ•ˆã€‚è®ºæ–‡è¿˜æ¯”è¾ƒäº†ä¸åŒæ¨¡å‹æ¶æ„å’Œå¤§å°ä¸‹çš„å¤šç§PEFTæ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04425', 'title': 'DELIFT: Data Efficient Language model Instruction Fine Tuning', 'url': 'https://huggingface.co/papers/2411.04425', 'abstract': "Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.", 'score': 8, 'issue_id': 509, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '397d7c5c26dfad0f', 'authors': ['Ishika Agarwal', 'Krishnateja Killamsetty', 'Lucian Popa', 'Marina Danilevksy'], 'affiliations': ['University of Illinois Urbana-Champaign', 'IBM Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04425.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ DELIFT Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). DELIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ°: Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ DELIFT Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ½Ğ° 70% Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Optimize Data, Maximize Performance with DELIFT!', 'desc': 'This paper presents DELIFT, a new algorithm designed to improve the fine-tuning process of large language models (LLMs) by optimizing data selection. DELIFT focuses on three stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning, making it more efficient than traditional methods. It uses a pairwise utility metric to evaluate the informational value of data samples, ensuring that only the most beneficial data is selected. The results show that DELIFT can significantly reduce the amount of data needed for fine-tuning by up to 70%, while maintaining or even enhancing model performance.'}, 'zh': {'title': 'é«˜æ•ˆå¾®è°ƒï¼šDELIFTç®—æ³•çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDELIFTçš„æ–°ç®—æ³•ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘å†—ä½™å’Œæ— æ•ˆæ•°æ®çš„ä½¿ç”¨ã€‚DELIFTé€šè¿‡ä¼˜åŒ–æ•°æ®é€‰æ‹©ï¼Œç³»ç»Ÿæ€§åœ°æ”¹è¿›äº†ä¸‰ä¸ªå…³é”®çš„å¾®è°ƒé˜¶æ®µï¼šæŒ‡ä»¤å¾®è°ƒã€ä»»åŠ¡ç‰¹å®šå¾®è°ƒå’ŒæŒç»­å¾®è°ƒã€‚è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§æˆå¯¹æ•ˆç”¨åº¦é‡ï¼Œé‡åŒ–æ•°æ®æ ·æœ¬å¯¹æ¨¡å‹å“åº”å…¶ä»–æ ·æœ¬çš„æ”¹å–„ç¨‹åº¦ï¼Œä»è€Œæœ‰æ•ˆè¯„ä¼°ä¿¡æ¯ä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDELIFTèƒ½å¤Ÿåœ¨ä¸é™ä½æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå°†å¾®è°ƒæ•°æ®é‡å‡å°‘å¤šè¾¾70%ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04954', 'title': 'CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM', 'url': 'https://huggingface.co/papers/2411.04954', 'abstract': "This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user's inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models' vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points. The project page and more visualizations can be found at: https://cad-mllm.github.io/", 'score': 7, 'issue_id': 518, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'f3ddd073293c6b26', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#multimodal', '#optimization', '#open_source'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'CAD-MLLM: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'CAD-MLLM - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Omni-CAD, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¾ĞºĞ¾Ğ»Ğ¾ 450 Ñ‚Ñ‹ÑÑÑ‡ CAD-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CAD-MLLM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğº ÑˆÑƒĞ¼Ğ°Ğ¼ Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼.'}, 'en': {'title': 'Revolutionizing CAD Generation with Multimodal Inputs', 'desc': 'This paper presents CAD-MLLM, a novel system designed to generate Computer-Aided Design (CAD) models from various user inputs, including text, images, and point clouds. It utilizes large language models (LLMs) to effectively align different types of input data with the vectorized representations of CAD models. The authors introduce the Omni-CAD dataset, which is the first of its kind, containing around 450,000 instances of multimodal data paired with CAD construction sequences. The evaluation of the generated models includes new metrics for topology and surface quality, showing that CAD-MLLM outperforms existing methods and is resilient to data noise.'}, 'zh': {'title': 'ç»Ÿä¸€CADç”Ÿæˆç³»ç»Ÿï¼šå¤šæ¨¡æ€è¾“å…¥çš„åˆ›æ–°åº”ç”¨', 'desc': 'æœ¬æ–‡æ—¨åœ¨è®¾è®¡ä¸€ä¸ªç»Ÿä¸€çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬æè¿°ã€å›¾åƒã€ç‚¹äº‘æˆ–å…¶ç»„åˆè½»æ¾ç”ŸæˆCADæ¨¡å‹ã€‚æˆ‘ä»¬ä»‹ç»äº†CAD-MLLMï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤ŸåŸºäºå¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆå‚æ•°åŒ–CADæ¨¡å‹çš„ç³»ç»Ÿã€‚è¯¥æ¡†æ¶åˆ©ç”¨CADæ¨¡å‹çš„å‘½ä»¤åºåˆ—ï¼Œå¹¶é‡‡ç”¨å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä¸åŒæ¨¡æ€æ•°æ®å’ŒCADæ¨¡å‹çš„å‘é‡è¡¨ç¤ºè¿›è¡Œç‰¹å¾ç©ºé—´å¯¹é½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºOmni-CADçš„ç»¼åˆæ•°æ®é›†ï¼ŒåŒ…å«æ–‡æœ¬æè¿°ã€å¤šè§†å›¾å›¾åƒã€ç‚¹å’Œå‘½ä»¤åºåˆ—ï¼Œçº¦æœ‰45ä¸‡ä¸ªå®ä¾‹ï¼Œè¯„ä¼°ç»“æœæ˜¾ç¤ºCAD-MLLMåœ¨ç”Ÿæˆè´¨é‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2411.04097', 'title': 'RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models', 'url': 'https://huggingface.co/papers/2411.04097', 'abstract': 'Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RaVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RaVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.', 'score': 5, 'issue_id': 516, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '62101343e5b62784', 'authors': ['Maya Varma', 'Jean-Benoit Delbrouck', 'Zhihong Chen', 'Akshay Chaudhari', 'Curtis Langlotz'], 'affiliations': ['Stanford University', 'Hugging Face'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04097.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#cv', '#training', '#healthcare', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ RaVL Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). RaVL Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 654 VLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing VLM Robustness by Targeting Spurious Correlations at the Local Level', 'desc': 'This paper introduces RaVL, a method designed to improve the robustness of fine-tuned vision-language models (VLMs) by addressing spurious correlations between image features and text attributes. Unlike existing methods that focus on global image-level features, RaVL operates on fine-grained local image features to identify and mitigate these correlations. It employs a region-level clustering approach to pinpoint specific image features that lead to classification errors in zero-shot scenarios. The results demonstrate that RaVL significantly enhances the discovery and mitigation of spurious correlations, leading to improved classification accuracy across various VLM architectures and domains.'}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹é²æ£’æ€§çš„RaVLæ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRaVLçš„æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é²æ£’æ€§ã€‚RaVLé€šè¿‡å±€éƒ¨å›¾åƒç‰¹å¾æ¥å‘ç°å’Œå‡è½»è™šå‡ç›¸å…³æ€§ï¼Œè€Œä¸æ˜¯ä»…åœ¨å…¨å±€å›¾åƒå±‚é¢è¿›è¡Œå¹²é¢„ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŒºåŸŸçº§èšç±»æ¥è¯†åˆ«å¯¼è‡´é›¶-shotåˆ†ç±»é”™è¯¯çš„ç²¾ç¡®å›¾åƒç‰¹å¾ï¼Œå¹¶é€šè¿‡æ–°çš„åŒºåŸŸæ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥å‡è½»è¿™äº›è™šå‡ç›¸å…³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRaVLåœ¨å‘ç°å’Œå‡è½»è™šå‡ç›¸å…³æ€§æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04986', 'title': 'The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities', 'url': 'https://huggingface.co/papers/2411.04986', 'abstract': 'Modern language models can process inputs across diverse languages and modalities. We hypothesize that models acquire this capability through learning a shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. We term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through a transmodal semantic "hub" which integrates information from various modality-specific "spokes" regions. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the model\'s dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply a vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing.', 'score': 4, 'issue_id': 515, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '9a27aef11b630a04', 'authors': ['Zhaofeng Wu', 'Xinyan Velocity Yu', 'Dani Yogatama', 'Jiasen Lu', 'Yoon Kim'], 'affiliations': ['MIT', 'University of Southern California', 'Allen Institute for AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04986.jpg', 'data': {'categories': ['#multilingual', '#multimodal', '#transfer_learning', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ¾Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ ĞºĞ°Ğº ĞºĞ»ÑÑ‡ Ğº Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ…Ğ°Ğ±Ğ° Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ‰ĞµĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸Ğ¼ĞµÑÑ‚ ÑÑ…Ğ¾Ğ¶Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ÑÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ´ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Unlocking Multimodal Understanding: The Semantic Hub Hypothesis', 'desc': "This paper explores how modern language models can understand and process different languages and types of data by learning a shared representation space. The authors propose the 'semantic hub hypothesis', which suggests that these models organize information similarly to how the human brain integrates knowledge from various sources. They demonstrate that representations of semantically similar inputs, even from different languages or modalities, are closely aligned in the model's intermediate layers. Additionally, they show that changes in one type of data representation can influence outputs in other types, indicating that this shared space is actively used by the model rather than being a mere artifact of training."}, 'zh': {'title': 'å…±äº«è¡¨ç¤ºç©ºé—´ï¼šè·¨æ¨¡æ€ç†è§£çš„å…³é”®', 'desc': 'ç°ä»£è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤šç§è¯­è¨€å’Œæ¨¡æ€çš„è¾“å…¥ã€‚æˆ‘ä»¬å‡è®¾æ¨¡å‹é€šè¿‡å­¦ä¹ ä¸€ä¸ªå…±äº«çš„è¡¨ç¤ºç©ºé—´æ¥è·å¾—è¿™ä¸€èƒ½åŠ›ï¼Œè¿™ä¸ªç©ºé—´å°†è¯­ä¹‰ç›¸ä¼¼çš„è¾“å…¥æ”¾åœ¨ä¸€èµ·ï¼Œå³ä½¿å®ƒä»¬æ¥è‡ªä¸åŒçš„æ¨¡æ€æˆ–è¯­è¨€ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºè¯­ä¹‰ä¸­å¿ƒå‡è®¾ï¼Œç±»ä¼¼äºç¥ç»ç§‘å­¦ä¸­çš„ä¸­å¿ƒ-è¾å°„æ¨¡å‹ï¼Œè®¤ä¸ºäººè„‘ä¸­çš„è¯­ä¹‰çŸ¥è¯†æ˜¯é€šè¿‡ä¸€ä¸ªè·¨æ¨¡æ€çš„è¯­ä¹‰â€œä¸­å¿ƒâ€ç»„ç»‡çš„ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸åŒè¯­è¨€ä¸­è¯­ä¹‰ç­‰ä»·çš„è¾“å…¥åœ¨æ¨¡å‹çš„ä¸­é—´å±‚å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºï¼Œè¿™ç§å…±äº«è¡¨ç¤ºç©ºé—´åœ¨å¤„ç†è¾“å…¥æ—¶è¢«æ¨¡å‹ç§¯æåˆ©ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05457', 'title': 'Improving the detection of technical debt in Java source code with an enriched dataset', 'url': 'https://huggingface.co/papers/2411.05457', 'abstract': 'Technical debt (TD) is a term used to describe the additional work and costs that emerge when developers have opted for a quick and easy solution to a problem, rather than a more effective and well-designed, but time-consuming approach. Self-Admitted Technical Debts (SATDs) are a specific type of technical debts that developers intentionally document and acknowledge, typically via textual comments. While these self-admitted comments are a useful tool for identifying technical debts, most of the existing approaches focus on capturing crucial tokens associated with various categories of TD, neglecting the rich information embedded within the source code itself. Recent research has focused on detecting SATDs by analyzing comments embedded in source code, and there has been little work dealing with technical debts contained in the source code. To fill such a gap, in this study, through the analysis of comments and their associated source code from 974 Java projects hosted in the Stack corpus, we curated the first ever dataset of TD identified by code comments, coupled with its associated source code. Through an empirical evaluation, we found out that the comments of the resulting dataset help enhance the prediction performance of state-of-the-art SATD detection models. More importantly, including the classified source code significantly improves the accuracy in predicting various types of technical debt. In this respect, our work is two-fold: (i) We believe that our dataset will catalyze future work in the domain, inspiring various research issues related to the recognition of technical debt; (ii) The proposed classifiers may serve as baselines for other studies on the detection of TD by means of the curated dataset.', 'score': 2, 'issue_id': 510, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 8', 'zh': '11æœˆ8æ—¥'}, 'hash': 'bc9c84b19f317115', 'authors': ['Nam Le Hai', 'Anh M. T. Bui', 'Phuong T. Nguyen', 'Davide Di Ruscio', 'Rick Kazman'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05457.jpg', 'data': {'categories': ['#data', '#training', '#dataset'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¾Ğ»Ğ³: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ´Ğ° Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ²', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ° (Ğ¢Ğ”) Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¢Ğ”, Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ÑÑ… Ğº ĞºĞ¾Ğ´Ñƒ, Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ»Ğ³Ğ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¢Ğ”, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Bridging Comments and Code: Enhancing Technical Debt Detection', 'desc': 'This paper addresses the issue of Technical Debt (TD) in software development, particularly focusing on Self-Admitted Technical Debts (SATDs) documented by developers. It highlights the limitations of existing methods that primarily analyze comment tokens, overlooking the valuable information within the source code itself. The authors present a novel dataset created from 974 Java projects, linking SATD comments to their corresponding source code, which enhances the detection of technical debts. Their empirical evaluation demonstrates that incorporating both comments and source code significantly improves the performance of SATD detection models, paving the way for future research in this area.'}, 'zh': {'title': 'æŠ€æœ¯å€ºåŠ¡è¯†åˆ«çš„æ–°è§†è§’', 'desc': 'æŠ€æœ¯å€ºåŠ¡ï¼ˆTDï¼‰æ˜¯æŒ‡å¼€å‘è€…ä¸ºäº†å¿«é€Ÿè§£å†³é—®é¢˜è€Œé€‰æ‹©çš„ç®€å•æ–¹æ¡ˆæ‰€å¸¦æ¥çš„é¢å¤–å·¥ä½œå’Œæˆæœ¬ã€‚è‡ªæˆ‘æ‰¿è®¤çš„æŠ€æœ¯å€ºåŠ¡ï¼ˆSATDï¼‰æ˜¯å¼€å‘è€…é€šè¿‡æ–‡æœ¬æ³¨é‡Šä¸»åŠ¨è®°å½•å’Œæ‰¿è®¤çš„ä¸€ç§ç‰¹å®šç±»å‹çš„æŠ€æœ¯å€ºåŠ¡ã€‚æœ¬æ–‡é€šè¿‡åˆ†ææ¥è‡ª974ä¸ªJavaé¡¹ç›®çš„æ³¨é‡Šå’Œç›¸å…³æºä»£ç ï¼Œåˆ›å»ºäº†é¦–ä¸ªç”±ä»£ç æ³¨é‡Šè¯†åˆ«çš„æŠ€æœ¯å€ºåŠ¡æ•°æ®é›†ï¼Œå¹¶å‘ç°è¿™äº›æ³¨é‡Šèƒ½æ˜¾è‘—æé«˜ç°æœ‰SATDæ£€æµ‹æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…ä¸ºæŠ€æœ¯å€ºåŠ¡çš„è¯†åˆ«æä¾›äº†æ–°çš„æ•°æ®é›†ï¼Œè¿˜æå‡ºäº†å¯ä½œä¸ºåŸºçº¿çš„åˆ†ç±»å™¨ï¼Œæ¨åŠ¨æœªæ¥ç›¸å…³ç ”ç©¶çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04905', 'title': 'OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models', 'url': 'https://huggingface.co/papers/2411.04905', 'abstract': "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems.While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an ``open cookbook'' for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.", 'score': 102, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '799dedd6597ce7ab', 'authors': ['Siming Huang', 'Tianhao Cheng', 'J. K. Liu', 'Jiaran Hao', 'Liuyihan Song', 'Yang Xu', 'J. Yang', 'J. H. Liu', 'Chenchen Zhang', 'Linzheng Chai', 'Ruifeng Yuan', 'Zhaoxiang Zhang', 'Jie Fu', 'Qian Liu', 'Ge Zhang', 'Zili Wang', 'Yuan Qi', 'Yinghui Xu', 'Wei Chu'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04905.jpg', 'data': {'categories': ['#dataset', '#data', '#training', '#synthetic', '#agents', '#plp'], 'emoji': 'ğŸ§‘\u200dğŸ’»', 'ru': {'title': 'OpenCoder: Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ĞºĞ½Ğ¸Ğ³Ğ° Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ´Ğ°', 'desc': 'OpenCoder - ÑÑ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ°Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ½Ğ¾ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½Ğ³Ñ€ĞµĞ´Ğ¸ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼, Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­Ñ‚Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ğ° ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'OpenCoder: Unlocking Code AI with Transparency and Reproducibility', 'desc': 'This paper introduces OpenCoder, a high-performance large language model (LLM) specifically designed for code generation and reasoning tasks. It addresses the lack of open-access models that provide reproducible data processing and transparent training protocols, which are essential for scientific research. OpenCoder not only matches the performance of proprietary models but also shares its model weights, inference code, and detailed training methodologies. By emphasizing data cleaning, corpus recall, and synthetic data generation, OpenCoder aims to enhance accessibility and foster reproducible advancements in the field of code AI.'}, 'zh': {'title': 'OpenCoderï¼šå¼€æ”¾çš„é¡¶çº§ä»£ç å¤§è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†OpenCoderï¼Œä¸€ä¸ªé«˜è´¨é‡çš„ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨ä¸ºç§‘å­¦ç ”ç©¶æä¾›å¼€æ”¾çš„èµ„æºã€‚ä¸å…¶ä»–æ¨¡å‹ä¸åŒï¼ŒOpenCoderä¸ä»…æä¾›æ¨¡å‹æƒé‡å’Œæ¨ç†ä»£ç ï¼Œè¿˜åŒ…æ‹¬å¯é‡å¤çš„è®­ç»ƒæ•°æ®å’Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºæ„å»ºé¡¶çº§ä»£ç LLMçš„å…³é”®è¦ç´ ï¼ŒåŒ…æ‹¬æ•°æ®æ¸…æ´—çš„å¯å‘å¼è§„åˆ™ã€ä¸ä»£ç ç›¸å…³çš„æ–‡æœ¬è¯­æ–™åº“çš„å›å¿†ä»¥åŠé«˜è´¨é‡çš„åˆæˆæ•°æ®ã€‚é€šè¿‡è¿™ç§å¼€æ”¾æ€§ï¼Œæˆ‘ä»¬å¸Œæœ›åŠ é€Ÿä»£ç äººå·¥æ™ºèƒ½çš„ç ”ç©¶å’Œå¯é‡å¤çš„è¿›å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05003', 'title': 'ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning', 'url': 'https://huggingface.co/papers/2411.05003', 'abstract': 'Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique.', 'score': 65, 'issue_id': 464, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'f71f2e0f1addbe57', 'authors': ['David Junhao Zhang', 'Roni Paiss', 'Shiran Zada', 'Nikhil Karnad', 'David E. Jacobs', 'Yael Pritch', 'Inbar Mosseri', 'Mike Zheng Shou', 'Neal Wadhwa', 'Nataniel Ruiz'], 'affiliations': ['Google', 'National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05003.jpg', 'data': {'categories': ['#video', '#diffusion', '#hallucinations'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸĞµÑ€ĞµÑĞ½Ğ¸Ğ¼Ğ°ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ReCapture - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ ÑƒĞ³Ğ»Ğ°Ğ¼Ğ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ¸ ĞºĞ¸Ğ½ĞµĞ¼Ğ°Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾Ğµ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ ÑÑ†ĞµĞ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ ÑˆÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ĞµĞ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ½Ğ¾ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ¸ Ğ´ĞµĞ»Ğ°ĞµÑ‚ÑÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼. ReCapture Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹.'}, 'en': {'title': 'ReCapture: Transforming User Videos with New Camera Perspectives', 'desc': 'This paper introduces ReCapture, a novel method for generating new videos with different camera angles from a single user-provided video. It leverages multiview diffusion models and depth-based point cloud rendering to create an initial noisy video with a new camera trajectory. The method then refines this video using a masked video fine-tuning technique to ensure temporal consistency and clarity. Additionally, ReCapture can convincingly generate parts of the scene that were not visible in the original video, enhancing the overall viewing experience.'}, 'zh': {'title': 'ReCaptureï¼šä»ç”¨æˆ·è§†é¢‘ç”Ÿæˆæ–°è§†è§’çš„é­”æ³•', 'desc': 'æœ€è¿‘åœ¨è§†é¢‘å»ºæ¨¡æ–¹é¢å–å¾—äº†çªç ´ï¼Œä½¿å¾—ç”Ÿæˆè§†é¢‘ä¸­çš„ç›¸æœºè½¨è¿¹å¯æ§ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æ— æ³•ç›´æ¥åº”ç”¨äºç”¨æˆ·æä¾›çš„éç”Ÿæˆè§†é¢‘ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºReCaptureçš„æ–¹æ³•ï¼Œå¯ä»¥ä»å•ä¸ªç”¨æˆ·æä¾›çš„è§†é¢‘ç”Ÿæˆå…·æœ‰æ–°ç›¸æœºè½¨è¿¹çš„æ–°è§†é¢‘ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½å¤Ÿä»ä¸åŒè§’åº¦é‡æ–°ç”Ÿæˆå‚è€ƒè§†é¢‘ï¼Œè¿˜èƒ½åˆç†åœ°å¹»è§‰å‡ºå‚è€ƒè§†é¢‘ä¸­ä¸å¯è§çš„åœºæ™¯éƒ¨åˆ†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04965', 'title': 'BitNet a4.8: 4-bit Activations for 1-bit LLMs', 'url': 'https://huggingface.co/papers/2411.04965', 'abstract': 'Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.', 'score': 63, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'dcfd440f9caf6714', 'authors': ['Hongyu Wang', 'Shuming Ma', 'Furu Wei'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04965.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'BitNet a4.8: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ¸Ñ€Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BitNet a4.8 - ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ 4-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² ÑĞ»Ğ¾ÑÑ… Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ 8-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BitNet a4.8 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ğ¾Ğ¹ Ñ BitNet b1.58, Ğ¿Ñ€Ğ¸ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ½Ğ¾ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 55% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ 3-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ğ¹ KV-ĞºÑÑˆ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Efficient Inference with 4-bit Activations in LLMs', 'desc': 'This paper introduces BitNet a4.8, a new model that enhances the efficiency of 1-bit Large Language Models (LLMs) by using 4-bit activations. It combines hybrid quantization and sparsification techniques to reduce errors from outlier channels, allowing for better performance. The model uses 4-bit activations in key layers while applying 8-bit quantization to intermediate states, resulting in faster inference times. Experiments show that BitNet a4.8 matches the performance of its predecessor, BitNet b1.58, while being more efficient in terms of parameter activation and supporting a 3-bit key-value cache.'}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘å…³äº1ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç ”ç©¶ï¼Œå¦‚BitNet b1.58ï¼Œå±•ç¤ºäº†åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½æ¨ç†æˆæœ¬çš„å‰æ™¯ã€‚æœ¬æ–‡ä»‹ç»äº†BitNet a4.8ï¼Œæ”¯æŒ1ä½LLMsçš„4ä½æ¿€æ´»ã€‚BitNet a4.8é‡‡ç”¨æ··åˆé‡åŒ–å’Œç¨€ç–åŒ–ç­–ç•¥ï¼Œä»¥å‡è½»ç”±å¼‚å¸¸é€šé“å¼•å…¥çš„é‡åŒ–è¯¯å·®ã€‚å®éªŒè¡¨æ˜ï¼ŒBitNet a4.8åœ¨è®­ç»ƒæˆæœ¬ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œæ¨ç†é€Ÿåº¦æ›´å¿«ï¼Œä¸”ä»…æ¿€æ´»55%çš„å‚æ•°ï¼Œæ”¯æŒ3ä½KVç¼“å­˜ï¼Œè¿›ä¸€æ­¥æé«˜äº†å¤§è§„æ¨¡LLMçš„éƒ¨ç½²å’Œæ¨ç†æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04996', 'title': 'Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models', 'url': 'https://huggingface.co/papers/2411.04996', 'abstract': "The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. We evaluate MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8\\% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2\\% of the wall-clock time and text quality in 75.6\\% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs).", 'score': 46, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '53d29fd65eda072e', 'authors': ['Weixin Liang', 'Lili Yu', 'Liang Luo', 'Srinivasan Iyer', 'Ning Dong', 'Chunting Zhou', 'Gargi Ghosh', 'Mike Lewis', 'Wen-tau Yih', 'Luke Zettlemoyer', 'Xi Victoria Lin'], 'affiliations': ['FAIR at Meta', 'Stanford University, Department of Computer Science'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04996.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Mixture-of-Transformers (MoT) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. MoT Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑ‡ÑŒ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MoT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Efficient Multi-Modal Processing with Mixture-of-Transformers', 'desc': 'This paper presents Mixture-of-Transformers (MoT), a novel sparse multi-modal transformer architecture designed to efficiently handle text, images, and speech. By decoupling non-embedding parameters by modality, MoT allows for specialized processing while maintaining global self-attention across the entire input. The architecture significantly reduces computational costs, achieving comparable performance to dense models with fewer floating point operations (FLOPs). Evaluations show that MoT not only matches but often outperforms dense baselines in various settings, demonstrating its effectiveness in multi-modal tasks.'}, 'zh': {'title': 'æ··åˆå˜æ¢å™¨ï¼šé«˜æ•ˆçš„å¤šæ¨¡æ€å­¦ä¹ æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç¨€ç–å¤šæ¨¡æ€å˜æ¢å™¨æ¶æ„ï¼Œç§°ä¸ºæ··åˆå˜æ¢å™¨ï¼ˆMoTï¼‰ï¼Œæ—¨åœ¨é™ä½å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè®¡ç®—æˆæœ¬ã€‚MoTé€šè¿‡æ¨¡æ€è§£è€¦æ¨¡å‹çš„éåµŒå…¥å‚æ•°ï¼Œä½¿å¾—ä¸åŒæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒå’Œè¯­éŸ³ï¼‰å¯ä»¥è¿›è¡Œç‰¹å®šå¤„ç†ï¼ŒåŒæ—¶ä¿æŒå…¨å±€è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMoTåœ¨å¤šä¸ªè®¾ç½®ä¸‹è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä»¥æ›´å°‘çš„è®¡ç®—èµ„æºè¾¾åˆ°ä¸å¯†é›†åŸºçº¿ç›¸å½“çš„æ€§èƒ½ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­å‡å±•ç°äº†æ˜¾è‘—çš„æ•ˆç‡ä¼˜åŠ¿ï¼Œè¯æ˜äº†å…¶åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04928', 'title': 'DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion', 'url': 'https://huggingface.co/papers/2411.04928', 'abstract': 'In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively represented through sequences of video frames. While recent video diffusion models have shown remarkable success in producing vivid visuals, they face limitations in directly recovering 3D/4D scenes due to limited spatial and temporal controllability during generation. To overcome this, we propose ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant data. This controllable video diffusion approach enables precise manipulation of spatial structure and temporal dynamics, allowing us to reconstruct both 3D and 4D representations from sequential frames with the combination of spatial and temporal dimensions. Additionally, to bridge the gap between generated videos and real-world scenes, we introduce a trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation. Extensive experiments on various real-world and synthetic datasets demonstrate that DimensionX achieves superior results in controllable video generation, as well as in 3D and 4D scene generation, compared with previous methods.', 'score': 43, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'b0958f934bc56a95', 'authors': ['Wenqiang Sun', 'Shuo Chen', 'Fangfu Liu', 'Zilong Chen', 'Yueqi Duan', 'Jun Zhang', 'Yikai Wang'], 'affiliations': ['HKUST', 'Tsinghua University', 'ShengShu'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04928.jpg', 'data': {'categories': ['#3d', '#video', '#synthetic'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞÑ‚ 2D Ğº 4D: DimensionX Ñ€Ğ°Ğ·Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'DimensionX - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D Ğ¸ 4D ÑÑ†ĞµĞ½ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ST-Director, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾-Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ñ‹Ğµ LoRA Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ”Ğ»Ñ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ, Ğ° Ğ´Ğ»Ñ 4D - ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ°Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ DimensionX Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ 3D/4D ÑÑ†ĞµĞ½ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Transforming Single Images into Stunning 3D and 4D Worlds!', 'desc': 'DimensionX is a novel framework that generates photorealistic 3D and 4D scenes from a single image using video diffusion techniques. It addresses the limitations of existing video diffusion models by introducing ST-Director, which separates spatial and temporal factors, allowing for better control over the generated scenes. By learning dimension-aware Low-Rank Adaptations (LoRAs) from diverse data, DimensionX enhances the manipulation of both spatial structures and temporal dynamics. The framework also incorporates a trajectory-aware mechanism and an identity-preserving denoising strategy to improve the realism of generated scenes, outperforming previous methods in extensive experiments.'}, 'zh': {'title': 'DimensionXï¼šä»å•å›¾åƒç”ŸæˆçœŸå®3Då’Œ4Dåœºæ™¯çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†DimensionXï¼Œä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å•å¼ å›¾åƒå’Œè§†é¢‘æ‰©æ•£ç”Ÿæˆé€¼çœŸçš„3Då’Œ4Dåœºæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è§†é¢‘å¸§åºåˆ—æœ‰æ•ˆè¡¨ç¤º3Dåœºæ™¯çš„ç©ºé—´ç»“æ„å’Œ4Dåœºæ™¯çš„æ—¶é—´æ¼”å˜ã€‚ä¸ºäº†è§£å†³ç°æœ‰è§†é¢‘æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç©ºé—´å’Œæ—¶é—´å¯æ§æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ST-Directorï¼Œé€šè¿‡ä»ç»´åº¦å˜åŒ–çš„æ•°æ®ä¸­å­¦ä¹ ç»´åº¦æ„ŸçŸ¥çš„LoRAï¼Œè§£è€¦ç©ºé—´å’Œæ—¶é—´å› ç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDimensionXåœ¨å¯æ§è§†é¢‘ç”Ÿæˆä»¥åŠ3Då’Œ4Dåœºæ™¯ç”Ÿæˆæ–¹é¢ä¼˜äºä»¥å¾€çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04952', 'title': 'M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding', 'url': 'https://huggingface.co/papers/2411.04952', 'abstract': 'Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images.', 'score': 26, 'issue_id': 477, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '56e0d2f2775dbda9', 'authors': ['Jaemin Cho', 'Debanjan Mahata', 'Ozan Irsoy', 'Yujie He', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill', 'Bloomberg'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04952.jpg', 'data': {'categories': ['#benchmark', '#rag', '#dataset', '#multimodal', '#games', '#long_context'], 'emoji': 'ğŸ“„', 'ru': {'title': 'M3DocRAG: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ M3DocRAG - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ»Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. M3DocRAG ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº M3DocVQA Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 3000 PDF-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ….'}, 'en': {'title': 'M3DocRAG: Revolutionizing Document Visual Question Answering', 'desc': 'The paper presents M3DocRAG, a new framework for Document Visual Question Answering (DocVQA) that addresses limitations of existing methods. Unlike traditional approaches that focus on single-page documents and often overlook visual elements, M3DocRAG can process multiple pages and various document types while retaining important visual information. It utilizes a multi-modal retriever and a multi-modal language model (MLM) to efficiently find relevant documents and answer questions, accommodating both text and visual evidence. The authors also introduce M3DocVQA, a benchmark for evaluating open-domain DocVQA, demonstrating that their framework outperforms existing models in several benchmarks.'}, 'zh': {'title': 'M3DocRAGï¼šè·¨é¡µæ–‡æ¡£é—®ç­”çš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆæ¡†æ¶M3DocRAGï¼Œæ—¨åœ¨è§£å†³æ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocVQAï¼‰ä¸­çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å¤„ç†å•é¡µæ–‡æ¡£ï¼Œæ— æ³•æœ‰æ•ˆåº”å¯¹è·¨é¡µæˆ–å¤šæ–‡æ¡£çš„ä¿¡æ¯æ£€ç´¢ã€‚M3DocRAGèƒ½å¤Ÿçµæ´»å¤„ç†ä¸åŒçš„æ–‡æ¡£ä¸Šä¸‹æ–‡å’Œé—®é¢˜è·³è·ƒï¼ŒåŒæ—¶ä¿ç•™é‡è¦çš„è§†è§‰ä¿¡æ¯ï¼Œå¦‚å›¾è¡¨å’Œå›¾åƒã€‚é€šè¿‡åœ¨è¶…è¿‡3000ä¸ªPDFæ–‡æ¡£ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒM3DocRAGåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†è®¸å¤šå¼ºåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04709', 'title': 'TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2411.04709', 'abstract': 'Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io.', 'score': 24, 'issue_id': 464, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 5', 'zh': '11æœˆ5æ—¥'}, 'hash': 'fcc8e4daf79a82b9', 'authors': ['Wenhao Wang', 'Yi Yang'], 'affiliations': ['University of Technology Sydney', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04709.jpg', 'data': {'categories': ['#dataset', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'TIP-I2V: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ TIP-I2V - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 1,70 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹-Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ñ‚ Ğ¿ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. TIP-I2V Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Empowering Image-to-Video Generation with TIP-I2V Dataset', 'desc': 'This paper presents TIP-I2V, the first large-scale dataset containing over 1.70 million unique user-provided text and image prompts for image-to-video generation. The dataset aims to enhance the controllability and visual consistency of video generation models by providing a rich source of prompts. It also includes generated videos from five advanced image-to-video models, facilitating comparative analysis and model evaluation. By addressing the lack of dedicated datasets, TIP-I2V supports research in user preferences and model safety, particularly in mitigating misinformation issues.'}, 'zh': {'title': 'TIP-I2Vï¼šå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆçš„æ–°æ•°æ®é›†', 'desc': 'è§†é¢‘ç”Ÿæˆæ¨¡å‹æ­£åœ¨æ”¹å˜å†…å®¹åˆ›ä½œï¼Œå›¾åƒåˆ°è§†é¢‘æ¨¡å‹å› å…¶æ›´å¥½çš„å¯æ§æ€§å’Œè§†è§‰ä¸€è‡´æ€§è€Œå—åˆ°å…³æ³¨ã€‚å°½ç®¡è¿™äº›æ¨¡å‹å¾ˆå—æ¬¢è¿ï¼Œä½†ç›®å‰ç¼ºä¹ä¸“é—¨ç”¨äºç ”ç©¶ç”¨æˆ·æä¾›çš„æ–‡æœ¬å’Œå›¾åƒæç¤ºçš„æ•°æ®é›†ã€‚æœ¬æ–‡ä»‹ç»äº†TIP-I2Vï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡170ä¸‡ä¸ªç‹¬ç‰¹çš„ç”¨æˆ·æä¾›çš„æ–‡æœ¬å’Œå›¾åƒæç¤ºï¼Œä¸“é—¨ç”¨äºå›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€‚è¯¥æ•°æ®é›†çš„æ¨å‡ºå°†æ¨åŠ¨å›¾åƒåˆ°è§†é¢‘ç ”ç©¶çš„è¿›å±•ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜åˆ†æç”¨æˆ·åå¥½å¹¶è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05000', 'title': 'Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?', 'url': 'https://huggingface.co/papers/2411.05000', 'abstract': 'As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data.', 'score': 20, 'issue_id': 474, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '72ef4dc00d41e203', 'authors': ['Jonathan Roberts', 'Kai Han', 'Samuel Albanie'], 'affiliations': ['University of Cambridge', 'The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05000.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#long_context'], 'emoji': 'ğŸ§µ', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°: Ğ½Ğ¸Ñ‚Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞµÑ€Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ 17 Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… LLM Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ğ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ°ÑÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ·Ğ°ÑĞ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡ĞµÑ‚Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Unlocking the Potential of Long-Context LLMs', 'desc': 'This paper investigates how well large language models (LLMs) can utilize their extended context capabilities for complex information retrieval and reasoning tasks. The authors conduct experiments with 17 leading LLMs to assess their ability to track multiple threads of information within their context windows. They discover that while many models can maintain performance across various threads, the effective context limit is often shorter than the maximum supported length, leading to decreased accuracy with larger contexts. Additionally, the study emphasizes the need for caution when comparing token counts from different tokenizers, as they can represent different amounts of text.'}, 'zh': {'title': 'é•¿ä¸Šä¸‹æ–‡æ¨¡å‹çš„æ½œåŠ›ä¸æŒ‘æˆ˜', 'desc': 'éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸Šä¸‹æ–‡é™åˆ¶çš„å¢åŠ ï¼Œåº”ç”¨èŒƒå›´å’Œä¸‹æ¸¸åŠŸèƒ½ä¹Ÿåœ¨æ‰©å¤§ã€‚è®¸å¤šç°å®ä»»åŠ¡çš„å†³ç­–ä¾èµ–äºåˆ†æ•£åœ¨ä¸åŒæ–‡æ¡£ä¸­çš„ç»†èŠ‚ï¼Œè¿™äº›æ–‡æ¡£é€šå¸¸åŒ…å«å¤§é‡æ— å…³ä¿¡æ¯ã€‚é•¿ä¸Šä¸‹æ–‡LLMsåœ¨å¤æ‚ä¿¡æ¯æ£€ç´¢å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†æˆ‘ä»¬å¯¹å®ƒä»¬å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡çš„ç†è§£ä»ç„¶æ»åã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡è®¸å¤šæ¨¡å‹åœ¨è·Ÿè¸ªä¿¡æ¯çº¿ç¨‹æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡é™åˆ¶å¾€å¾€æ¯”æ”¯æŒçš„ä¸Šä¸‹æ–‡é•¿åº¦è¦çŸ­ï¼Œä¸”éšç€ä¸Šä¸‹æ–‡çª—å£çš„å¢å¤§ï¼Œå‡†ç¡®æ€§ä¼šä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04923', 'title': 'VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos', 'url': 'https://huggingface.co/papers/2411.04923', 'abstract': 'Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed for fine-grained pixel-level grounding in videos based on user-provided textual inputs. Our design seamlessly connects three key components: a Large Language Model, a dual vision encoder that emphasizes both spatial and temporal details, and a spatio-temporal decoder for accurate mask generation. This connection is facilitated via tunable V-L and L-V adapters that enable close Vision-Language (VL) alignment. The architecture is trained to synchronize both spatial and temporal elements of video content with textual instructions. To enable fine-grained grounding, we curate a multimodal dataset featuring detailed visually-grounded conversations using a semiautomatic annotation pipeline, resulting in a diverse set of 38k video-QA triplets along with 83k objects and 671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation. Experimental results show that our model consistently outperforms existing approaches across all three tasks.', 'score': 20, 'issue_id': 474, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '3db2b6994e9c5047', 'authors': ['Shehan Munasinghe', 'Hanan Gani', 'Wenqi Zhu', 'Jiale Cao', 'Eric Xing', 'Fahad Shahbaz Khan', 'Salman Khan'], 'affiliations': ['Mohamed bin Zayed University of AI', 'Tianjin University', 'Carnegie Mellon University', 'LinkÃ¶ping University', 'Australian National University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04923.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#architecture', '#games', '#alignment', '#cv'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜', 'desc': 'VideoGLaMM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ VideoGLaMM Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ², Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ.'}, 'en': {'title': 'Achieving Pixel-Level Precision in Video-Text Alignment', 'desc': 'This paper presents VideoGLaMM, a Large Multimodal Model (LMM) that enhances the alignment between videos and text at a fine-grained level. It addresses the challenges of pixel-level grounding by integrating a Large Language Model with a dual vision encoder that captures both spatial and temporal dynamics of video content. The model employs tunable adapters for effective Vision-Language alignment and is trained on a comprehensive multimodal dataset with 38k video-QA triplets. Experimental results demonstrate that VideoGLaMM outperforms existing models in tasks such as Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation.'}, 'zh': {'title': 'è§†é¢‘ä¸æ–‡æœ¬çš„ç²¾ç»†å¯¹é½æ–°çªç ´', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVideoGLaMMçš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°è§†é¢‘ä¸æ–‡æœ¬ä¹‹é—´çš„ç²¾ç»†åƒç´ çº§å¯¹é½ã€‚è¯¥æ¨¡å‹ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ã€åŒé‡è§†è§‰ç¼–ç å™¨å’Œæ—¶ç©ºè§£ç å™¨ï¼Œèƒ½å¤Ÿå¤„ç†è§†é¢‘ä¸­çš„å¤æ‚ç©ºé—´å’Œæ—¶é—´åŠ¨æ€ã€‚é€šè¿‡å¯è°ƒçš„è§†è§‰-è¯­è¨€é€‚é…å™¨ï¼ŒVideoGLaMMå®ç°äº†è§†è§‰ä¸è¯­è¨€çš„ç´§å¯†å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç”Ÿæˆå¯¹è¯ã€è§†è§‰å¯¹é½å’Œè§†é¢‘åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šå‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04496', 'title': 'Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model', 'url': 'https://huggingface.co/papers/2411.04496', 'abstract': 'To increase social bonding with interlocutors, humans naturally acquire the ability to respond appropriately in a given situation by considering which conversational skill is most suitable for the response - a process we call skill-of-mind. For large language model (LLM)-based conversational agents, planning appropriate conversational skills, as humans do, is challenging due to the complexity of social dialogue, especially in interactive scenarios. To address this, we propose a skill-of-mind-annotated conversation dataset, named Multifaceted Skill-of-Mind, which includes multi-turn and multifaceted conversational skills across various interactive scenarios (e.g., long-term, counseling, task-oriented), grounded in diverse social contexts (e.g., demographics, persona, rules of thumb). This dataset consists of roughly 100K conversations. Using this dataset, we introduce a new family of skill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B parameters. With extensive experiments, these models successfully demonstrate the skill-of-mind process and exhibit strong generalizability in inferring multifaceted skills across a variety of domains. Moreover, we show that Thanos significantly enhances the quality of responses generated by LLM-based conversational agents and promotes prosocial behavior in human evaluations.', 'score': 20, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'bf7486353434568f', 'authors': ['Young-Jun Lee', 'Dokyong Lee', 'Junyoung Youn', 'Kyeongjin Oh', 'Ho-Jin Choi'], 'affiliations': ['KAIST', 'KT Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04496.jpg', 'data': {'categories': ['#dataset', '#agents', '#multimodal'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²Ñƒ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Multifaceted Skill-of-Mind, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞµÑ€Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Thanos, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Thanos ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞµĞ¹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Thanos Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ»Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Enhancing Conversational Skills in AI with Thanos', 'desc': 'This paper introduces a new dataset called Multifaceted Skill-of-Mind, which helps large language models (LLMs) learn how to respond appropriately in social conversations. The dataset contains around 100,000 conversations that cover various interactive scenarios and social contexts, allowing LLMs to understand different conversational skills. The authors also present a new family of LLMs named Thanos, which are designed to incorporate these skills into their responses. Through experiments, Thanos models show improved response quality and promote positive social interactions in conversations.'}, 'zh': {'title': 'æå‡å¯¹è¯è´¨é‡çš„æŠ€èƒ½æ€ç»´æ¨¡å‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå¤šé¢æŠ€èƒ½æ€ç»´çš„å¯¹è¯æ•°æ®é›†ï¼Œæ—¨åœ¨å¸®åŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨ç¤¾äº¤å¯¹è¯ä¸­çš„é€‚å½“å›åº”æŠ€èƒ½ã€‚è¯¥æ•°æ®é›†åŒ…å«çº¦10ä¸‡æ¡å¯¹è¯ï¼Œæ¶µç›–äº†å¤šè½®å’Œå¤šæ–¹é¢çš„å¯¹è¯æŠ€èƒ½ï¼Œé€‚ç”¨äºä¸åŒçš„äº’åŠ¨åœºæ™¯ï¼Œå¦‚é•¿æœŸå¯¹è¯ã€å’¨è¯¢å’Œä»»åŠ¡å¯¼å‘ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§æ–°çš„LLMå®¶æ—ï¼Œåä¸ºThanosï¼Œå…·æœ‰1Bã€3Bå’Œ8Bå‚æ•°è§„æ¨¡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å±•ç¤ºæŠ€èƒ½æ€ç»´è¿‡ç¨‹ï¼Œå¹¶åœ¨å¤šç§é¢†åŸŸä¸­æ¨æ–­å¤šé¢æŠ€èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒThanosæ˜¾è‘—æé«˜äº†LLMå¯¹è¯ä»£ç†ç”Ÿæˆçš„å›åº”è´¨é‡ï¼Œå¹¶åœ¨äººå·¥è¯„ä¼°ä¸­ä¿ƒè¿›äº†äº²ç¤¾ä¼šè¡Œä¸ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05001', 'title': 'Analyzing The Language of Visual Tokens', 'url': 'https://huggingface.co/papers/2411.05001', 'abstract': 'With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models.', 'score': 19, 'issue_id': 482, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '75768d92bd8ce17a', 'authors': ['David M. Chan', 'Rodolfo Corona', 'Joonyong Park', 'Cheol Jun Cho', 'Yutong Bai', 'Trevor Darrell'], 'affiliations': ['University of California, Berkeley', 'The University of Tokyo, Tokyo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05001.jpg', 'data': {'categories': ['#multimodal', '#cv', '#interpretability', '#alignment'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸: Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¦Ğ¸Ğ¿Ñ„Ğ¾Ğ¼ Ğ¸ Ñ…Ğ°Ğ¾ÑĞ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¦Ğ¸Ğ¿Ñ„Ğ°, Ğ½Ğ¾ Ğ¸Ğ¼ĞµÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆÑƒÑ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸. Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ° Ğ½Ğµ Ñ†ĞµĞ»Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸Ğ»Ğ¸ ÑÑ†ĞµĞ½Ñ‹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ğ¼ Ğ½Ğµ Ñ…Ğ²Ğ°Ñ‚Ğ°ĞµÑ‚ ÑĞ²ÑĞ·Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': 'Unlocking the Secrets of Visual Languages in AI', 'desc': 'This paper explores the statistical properties of discrete visual languages used in transformer-based models for vision and language tasks. It reveals that these visual languages exhibit Zipfian distributions, but their token innovation leads to higher entropy and lower compression, primarily representing object parts. The study finds that visual languages lack cohesive grammatical structures, resulting in higher perplexity and less hierarchical organization compared to natural languages. Ultimately, the research highlights the importance of understanding these properties to improve the design of computer vision models.'}, 'zh': {'title': 'æ­ç¤ºè§†è§‰è¯­è¨€çš„ç»Ÿè®¡ç‰¹æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åŸºäºå˜æ¢å™¨çš„è§†è§‰å’Œè¯­è¨€ä»»åŠ¡æ¨¡å‹ï¼ˆå¦‚LLaVAå’ŒChameleonï¼‰ä¸­å›¾åƒçš„ç¦»æ•£æ ‡è®°è¡¨ç¤ºã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰è¯­è¨€è™½ç„¶éµå¾ªZipfåˆ†å¸ƒï¼Œä½†æ›´é«˜çš„æ ‡è®°åˆ›æ–°ä¼šå¯¼è‡´æ›´å¤§çš„ç†µå’Œæ›´ä½çš„å‹ç¼©ç‡ï¼Œæ ‡è®°ä¸»è¦ä»£è¡¨ç‰©ä½“éƒ¨åˆ†ï¼Œæ˜¾ç¤ºå‡ºä¸­é—´ç²’åº¦ã€‚ä¸è‡ªç„¶è¯­è¨€ç›¸æ¯”ï¼Œè§†è§‰è¯­è¨€ç¼ºä¹è¿è´¯çš„è¯­æ³•ç»“æ„ï¼Œå¯¼è‡´æ›´é«˜çš„å›°æƒ‘åº¦å’Œè¾ƒå¼±çš„å±‚æ¬¡ç»„ç»‡ã€‚é€šè¿‡è¿™äº›å®éªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†ç¦»æ•£è§†è§‰è¯­è¨€çš„ç»Ÿè®¡ç‰¹æ€§å¦‚ä½•å½±å“è®¡ç®—æœºè§†è§‰æ¨¡å‹çš„è®¾è®¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04999', 'title': 'DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation', 'url': 'https://huggingface.co/papers/2411.04999', 'abstract': "Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems. Our code as well as our experiment and deployment videos are open sourced and can be found on our project website: https://dynamem.github.io/", 'score': 16, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '47171ef52d95552a', 'authors': ['Peiqi Liu', 'Zhanqiu Guo', 'Mohit Warke', 'Soumith Chintala', 'Chris Paxton', 'Nur Muhammad Mahi Shafiullah', 'Lerrel Pinto'], 'affiliations': ['New York University', 'Hello Robot Inc.', 'Meta Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04999.jpg', 'data': {'categories': ['#robotics', '#3d', '#multimodal', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑÑ‰ĞµĞ¼ÑÑ Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'DynaMem - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 3D ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº. DynaMem Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ½ĞµÑÑ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Empowering Robots with Dynamic Memory for Open-World Manipulation', 'desc': "This paper introduces DynaMem, a novel approach for open-vocabulary mobile manipulation that allows robots to adapt to dynamic environments. Unlike traditional systems that rely on static environments, DynaMem utilizes a dynamic spatio-semantic memory to keep track of changes in the robot's surroundings. It employs a 3D data structure to manage point clouds and leverages multimodal large language models (LLMs) for object localization. The results show that DynaMem significantly improves the robot's ability to interact with non-stationary objects, achieving a 70% success rate in pick-and-drop tasks, which is more than double the performance of existing static systems."}, 'zh': {'title': 'åŠ¨æ€è®°å¿†ï¼Œæ™ºèƒ½æ“ä½œï¼', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€ç©ºé—´è¯­ä¹‰è®°å¿†æ–¹æ³•DynaMemï¼Œç”¨äºå¼€æ”¾è¯æ±‡çš„ç§»åŠ¨æ“ä½œã€‚ä¸ä¼ ç»Ÿé™æ€ç¯å¢ƒç³»ç»Ÿä¸åŒï¼ŒDynaMemèƒ½å¤Ÿåœ¨ä¸æ–­å˜åŒ–çš„ç¯å¢ƒä¸­è¿›è¡Œç‰©ä½“å®šä½å’Œæ“ä½œã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸‰ç»´æ•°æ®ç»“æ„ç»´æŠ¤åŠ¨æ€è®°å¿†ï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡ŒæŸ¥è¯¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDynaMemåœ¨éé™æ€ç‰©ä½“çš„æŠ“å–å’Œæ”¾ç½®ä»»åŠ¡ä¸­æˆåŠŸç‡è¾¾70%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰é™æ€ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05007', 'title': 'SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models', 'url': 'https://huggingface.co/papers/2411.05007', 'abstract': 'Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, na\\"{\\i}vely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-Sigma, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5times, achieving 3.0times speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs. Our quantization library and inference engine are open-sourced.', 'score': 15, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '636ee9cbe15eefb6', 'authors': ['Muyang Li', 'Yujun Lin', 'Zhekai Zhang', 'Tianle Cai', 'Xiuyu Li', 'Junxian Guo', 'Enze Xie', 'Chenlin Meng', 'Jun-Yan Zhu', 'Song Han'], 'affiliations': ['MIT', 'UC Berkeley', 'NVIDIA', 'CMU', 'Princeton', 'SJTU', 'Pika Labs'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05007.jpg', 'data': {'categories': ['#inference', '#optimization', '#diffusion'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SVDQuant Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²ÑƒÑ Ğ²ĞµÑ‚Ğ²ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ³Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ğ²ĞµÑĞ°Ñ… Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑÑ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ñ… Ğ´Ğ¾ 4 Ğ±Ğ¸Ñ‚. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ²Ğ¸Ğ¶Ğ¾Ğº Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Nunchaku, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Accelerating Diffusion Models with SVDQuant: Efficient 4-Bit Quantization', 'desc': 'This paper presents SVDQuant, a novel 4-bit quantization method designed to enhance the efficiency of diffusion models for image generation. As these models increase in size, they face challenges related to memory usage and latency, which SVDQuant aims to address by effectively managing outliers in weights and activations. The method utilizes Singular Value Decomposition (SVD) to absorb outliers into a low-rank branch, improving the quantization process without compromising image quality. Additionally, the co-designed inference engine, Nunchaku, optimizes memory access, resulting in significant reductions in memory usage and increased processing speed for large models.'}, 'zh': {'title': 'åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„4ä½é‡åŒ–æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†éšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§ï¼Œå®ƒä»¬éœ€è¦æ›´å¤šçš„å†…å­˜å¹¶ä¸”å»¶è¿Ÿæ›´é«˜ï¼Œè¿™ç»™éƒ¨ç½²å¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„4ä½é‡åŒ–æ–¹æ³•SVDQuantï¼Œé€šè¿‡é‡åŒ–æƒé‡å’Œæ¿€æ´»å€¼æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ã€‚ä¸ä¼ ç»Ÿçš„å¹³æ»‘æ–¹æ³•ä¸åŒï¼ŒSVDQuanté€šè¿‡ä½ç§©åˆ†æ”¯å¸æ”¶å¼‚å¸¸å€¼ï¼Œä»è€Œå‡è½»é‡åŒ–è¿‡ç¨‹ä¸­çš„å›°éš¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSVDQuantåœ¨ä¿æŒå›¾åƒè´¨é‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜ä½¿ç”¨å’Œæé«˜äº†é€Ÿåº¦ï¼Œé€‚ç”¨äºæ›´å¤šäº’åŠ¨åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04752', 'title': 'RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval', 'url': 'https://huggingface.co/papers/2411.04752', 'abstract': 'Code-mixing, the integration of lexical and grammatical elements from multiple languages within a single sentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman script, especially among migrant communities who form online groups to share relevant local information. This paper focuses on the challenges of extracting relevant information from code-mixed conversations, specifically within Roman transliterated Bengali mixed with English. This study presents a novel approach to address these challenges by developing a mechanism to automatically identify the most relevant answers from code-mixed conversations. We have experimented with a dataset comprising of queries and documents from Facebook, and Query Relevance files (QRels) to aid in this task. Our results demonstrate the effectiveness of our approach in extracting pertinent information from complex, code-mixed digital conversations, contributing to the broader field of natural language processing in multilingual and informal text environments. We use GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant documents to frame a mathematical model which helps to detect relevant documents corresponding to a query.', 'score': 15, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'e2dbb14c8f2ca6ef', 'authors': ['Aniket Deroy', 'Subhankar Maity'], 'affiliations': ['IIT Kharagpur, Kharagpur, India'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04752.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#data'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ğ² ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞµÑ‚ÑÑ… Ğ˜Ğ½Ğ´Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ°Ñ… Ğ½Ğ° ÑĞ¼ĞµÑĞ¸ Ğ±ĞµĞ½Ğ³Ğ°Ğ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ñ†ĞµĞ¹. Ğ’ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Facebook Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Query Relevance, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞ»Ğ°ÑÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT-3.5 Turbo. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ² Ğ½ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ.'}, 'en': {'title': 'Enhancing Information Retrieval in Code-Mixed Conversations', 'desc': 'This paper addresses the challenge of extracting relevant information from code-mixed conversations, particularly in Roman transliterated Bengali and English. It presents a novel mechanism that utilizes GPT-3.5 Turbo and a mathematical model to identify pertinent answers from social media interactions. The study experiments with a dataset from Facebook, focusing on queries and documents to enhance information retrieval. The results indicate that the proposed approach effectively navigates the complexities of multilingual and informal text, contributing to advancements in natural language processing.'}, 'zh': {'title': 'ä»ä»£ç æ··åˆå¯¹è¯ä¸­æå–ä¿¡æ¯çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸­ï¼Œå¦‚ä½•ä»ä»£ç æ··åˆçš„å¯¹è¯ä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚ç‰¹åˆ«æ˜¯åœ¨å°åº¦ï¼Œç¤¾äº¤åª’ä½“ç”¨æˆ·å¸¸ç”¨ç½—é©¬å­—æ¯ä¹¦å†™çš„å­ŸåŠ æ‹‰è¯­ä¸è‹±è¯­æ··åˆäº¤æµã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨è¯†åˆ«ä»£ç æ··åˆå¯¹è¯ä¸­çš„ç›¸å…³ç­”æ¡ˆæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æå–å¤æ‚æ•°å­—å¯¹è¯ä¸­çš„ç›¸å…³ä¿¡æ¯æ–¹é¢æ˜¯æœ‰æ•ˆçš„ï¼Œæ¨åŠ¨äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04075', 'title': 'M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models', 'url': 'https://huggingface.co/papers/2411.04075', 'abstract': 'Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address this gap, we introduce M3SciQA, a multi-modal, multi-document scientific question answering benchmark designed for a more comprehensive evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, where each cluster represents a primary paper along with all its cited documents, mirroring the workflow of comprehending a single paper by requiring multi-modal and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of 18 foundation models. Our results indicate that current foundation models still significantly underperform compared to human experts in multi-modal information retrieval and in reasoning across multiple scientific documents. Additionally, we explore the implications of these findings for the future advancement of applying foundation models in multi-modal scientific literature analysis.', 'score': 14, 'issue_id': 480, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 6', 'zh': '11æœˆ6æ—¥'}, 'hash': '654e787a9f0ab7ff', 'authors': ['Chuhan Li', 'Ziyao Shangguan', 'Yilun Zhao', 'Deyuan Li', 'Yixin Liu', 'Arman Cohan'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04075.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#science'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'M3SciQA: ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°', 'desc': 'M3SciQA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğµ 70 ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 18 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼-Ğ»ÑĞ´ÑĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼. Ğ­Ñ‚Ğ¾Ñ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹.'}, 'en': {'title': 'M3SciQA: Bridging the Gap in Multi-Modal Scientific Understanding', 'desc': 'This paper introduces M3SciQA, a new benchmark for evaluating foundation models in the context of scientific question answering. Unlike existing benchmarks that focus solely on single-document, text-only tasks, M3SciQA incorporates multi-modal and multi-document elements to better reflect real research workflows. The benchmark includes 1,452 expert-annotated questions related to clusters of natural language processing papers, requiring models to interpret both textual and non-textual data. Evaluation results show that current foundation models still lag behind human experts in retrieving multi-modal information and reasoning across multiple documents, highlighting the need for further advancements in this area.'}, 'zh': {'title': 'M3SciQAï¼šå¤šæ¨¡æ€ç§‘å­¦é—®ç­”çš„æ–°åŸºå‡†', 'desc': 'ç°æœ‰çš„åŸºç¡€æ¨¡å‹è¯„ä¼°åŸºå‡†ä¸»è¦é›†ä¸­åœ¨å•æ–‡æ¡£ã€æ–‡æœ¬ä»»åŠ¡ä¸Šï¼Œæ— æ³•å…¨é¢æ•æ‰ç ”ç©¶å·¥ä½œæµç¨‹çš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†M3SciQAï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šæ–‡æ¡£çš„ç§‘å­¦é—®ç­”åŸºå‡†ï¼Œæ—¨åœ¨æ›´å…¨é¢åœ°è¯„ä¼°åŸºç¡€æ¨¡å‹ã€‚M3SciQAåŒ…å«1452ä¸ªä¸“å®¶æ³¨é‡Šçš„é—®é¢˜ï¼Œæ¶µç›–70ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†è®ºæ–‡é›†ç¾¤ï¼Œæ¨¡æ‹Ÿç†è§£å•ç¯‡è®ºæ–‡çš„å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„åŸºç¡€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢å’Œè·¨å¤šä¸ªç§‘å­¦æ–‡æ¡£æ¨ç†æ–¹é¢ä»æ˜¾è‘—ä½äºäººç±»ä¸“å®¶çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04335', 'title': 'GazeGen: Gaze-Driven User Interaction for Visual Content Generation', 'url': 'https://huggingface.co/papers/2411.04335', 'abstract': "We present GazeGen, a user interaction system that generates visual content (images and videos) for locations indicated by the user's eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and generative AI, GazeGen performs gaze-controlled image adding/deleting, repositioning, and surface material changes of image objects, and converts static images into videos. Central to GazeGen is the DFT Gaze (Distilled and Fine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters, performing accurate real-time gaze predictions tailored to individual users' eyes on small edge devices. GazeGen is the first system to combine visual content generation with real-time gaze estimation, made possible exclusively by DFT Gaze. This real-time gaze estimation enables various visual content generation tasks, all controlled by the user's gaze. The input for DFT Gaze is the user's eye images, while the inputs for visual content generation are the user's view and the predicted gaze point from DFT Gaze. To achieve efficient gaze predictions, we derive the small model from a large model (10x larger) via novel knowledge distillation and personal adaptation techniques. We integrate knowledge distillation with a masked autoencoder, developing a compact yet powerful gaze estimation model. This model is further fine-tuned with Adapters, enabling highly accurate and personalized gaze predictions with minimal user input. DFT Gaze ensures low-latency and precise gaze tracking, supporting a wide range of gaze-driven tasks. We validate the performance of DFT Gaze on AEA and OpenEDS2020 benchmarks, demonstrating low angular gaze error and low latency on the edge device (Raspberry Pi 4). Furthermore, we describe applications of GazeGen, illustrating its versatility and effectiveness in various usage scenarios.", 'score': 14, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'caa85d9f2385fc2a', 'authors': ['He-Yen Hsieh', 'Ziyun Li', 'Sai Qian Zhang', 'Wei-Te Mark Ting', 'Kao-Den Chang', 'Barbara De Salvo', 'Chiao Liu', 'H. T. Kung'], 'affiliations': ['Reality Labs Research, Meta', 'New York University', 'Harvard University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04335.jpg', 'data': {'categories': ['#agents', '#cv', '#video', '#edge_computing', '#training'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'Ğ’Ğ·Ğ³Ğ»ÑĞ´Ğ¾Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞ¹: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ GazeGen, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ»ĞµĞ¶Ğ¸Ñ‚ ÑĞ²ĞµÑ€Ñ…Ğ»ĞµĞ³ĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DFT Gaze, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. GazeGen Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² Ğ½Ğ¸Ğ·ĞºÑƒÑ ÑƒĞ³Ğ»Ğ¾Ğ²ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ³Ğ»ÑĞ´Ğ° Ğ¸ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ğ½Ğ° Ğ¿ĞµÑ€Ğ¸Ñ„ĞµÑ€Ğ¸Ğ¹Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ….'}, 'en': {'title': 'GazeGen: Transforming Eye Gaze into Visual Content Control', 'desc': 'GazeGen is an innovative user interaction system that generates visual content based on where a user is looking. It utilizes advanced object detection and generative AI techniques to allow users to manipulate images and videos by simply gazing at specific areas. The core of GazeGen is the DFT Gaze agent, a lightweight model that accurately predicts gaze in real-time, making it suitable for use on small devices like the Raspberry Pi 4. By combining gaze estimation with visual content generation, GazeGen enables intuitive and personalized interactions with digital media.'}, 'zh': {'title': 'çœ¼åŠ¨æ§åˆ¶çš„è§†è§‰å†…å®¹ç”Ÿæˆç³»ç»Ÿ', 'desc': 'GazeGenæ˜¯ä¸€ä¸ªç”¨æˆ·äº¤äº’ç³»ç»Ÿï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·çš„çœ¼åŠ¨ç”Ÿæˆè§†è§‰å†…å®¹ï¼ˆå›¾åƒå’Œè§†é¢‘ï¼‰ã€‚å®ƒåˆ©ç”¨å…ˆè¿›çš„ç‰©ä½“æ£€æµ‹å’Œç”ŸæˆAIæŠ€æœ¯ï¼Œå®ç°äº†åŸºäºè§†çº¿çš„å›¾åƒæ·»åŠ ã€åˆ é™¤ã€é‡æ–°å®šä½å’Œè¡¨é¢æè´¨å˜åŒ–ã€‚ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯DFT Gazeä»£ç†ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å°å‹è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œå®æ—¶çš„çœ¼åŠ¨é¢„æµ‹ã€‚GazeGenæ˜¯é¦–ä¸ªå°†è§†è§‰å†…å®¹ç”Ÿæˆä¸å®æ—¶çœ¼åŠ¨ä¼°è®¡ç›¸ç»“åˆçš„ç³»ç»Ÿï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šç§åº”ç”¨åœºæ™¯ä¸­çš„çµæ´»æ€§å’Œæœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05005', 'title': 'Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models', 'url': 'https://huggingface.co/papers/2411.05005', 'abstract': 'Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or as mere feature extractors. In contrast to these isolated and thus sub-optimal efforts, we introduce a unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through a unique exploitation of the diffusion-denoising process. Within this framework, we further enhance discriminative visual perception via multi-modal generation, by utilizing the denoising network to create multi-modal data that mirror the distribution of the original training set. Importantly, Diff-2-in-1 optimizes the utilization of the created diverse and faithful data by leveraging a novel self-improving learning mechanism. Comprehensive experimental evaluations validate the effectiveness of our framework, showcasing consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation characterized by both realism and usefulness.', 'score': 13, 'issue_id': 477, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '249deab8440380fc', 'authors': ['Shuhong Zheng', 'Zhipeng Bao', 'Ruoyu Zhao', 'Martial Hebert', 'Yu-Xiong Wang'], 'affiliations': ['University of Illinois Urbana-Champaign', 'Carnegie Mellon University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05005.jpg', 'data': {'categories': ['#training', '#diffusion', '#data', '#multimodal', '#optimization', '#cv'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Diff-2-in-1: ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Diff-2-in-1. Ğ­Ñ‚Ğ¾Ñ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Diff-2-in-1 ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unifying Data Generation and Visual Perception with Diff-2-in-1', 'desc': 'This paper presents a new framework called Diff-2-in-1 that integrates diffusion models for both data generation and visual perception tasks. Unlike previous approaches that used diffusion models in isolation, this framework leverages the diffusion-denoising process to enhance the performance of visual perception. It generates multi-modal data that closely resembles the original training data, improving the discriminative capabilities of the model. The framework also includes a self-improving learning mechanism to optimize the use of the generated data, leading to better performance across various tasks.'}, 'zh': {'title': 'Diff-2-in-1ï¼šå¤šæ¨¡æ€ç”Ÿæˆä¸è§†è§‰æ„ŸçŸ¥çš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºDiff-2-in-1ï¼Œæ—¨åœ¨åŒæ—¶å¤„ç†å¤šæ¨¡æ€æ•°æ®ç”Ÿæˆå’Œå¯†é›†è§†è§‰æ„ŸçŸ¥ã€‚ä¸ä»¥å¾€å°†æ‰©æ•£æ¨¡å‹è§†ä¸ºç‹¬ç«‹ç»„ä»¶çš„åšæ³•ä¸åŒï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£å»å™ªè¿‡ç¨‹çš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œå¢å¼ºäº†è§†è§‰æ„ŸçŸ¥çš„åˆ¤åˆ«èƒ½åŠ›ã€‚é€šè¿‡ç”Ÿæˆä¸åŸå§‹è®­ç»ƒé›†åˆ†å¸ƒç›¸ä¼¼çš„å¤šæ¨¡æ€æ•°æ®ï¼ŒDiff-2-in-1ä¼˜åŒ–äº†ç”Ÿæˆæ•°æ®çš„ä½¿ç”¨ï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„è‡ªæˆ‘æ”¹è¿›å­¦ä¹ æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§åˆ¤åˆ«æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½æå‡ï¼Œå¹¶ç”Ÿæˆäº†é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®ï¼Œå…·æœ‰çœŸå®æ„Ÿå’Œå®ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.04989', 'title': 'SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2411.04989', 'abstract': 'Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds. Recent techniques address this issue by fine-tuning a pre-trained model to follow conditioning signals, such as bounding boxes or point trajectories. Yet, this fine-tuning procedure can be computationally expensive, and it requires datasets with annotated object motion, which can be difficult to procure. In this work, we introduce SG-I2V, a framework for controllable image-to-video generation that is self-guidedx2013offering zero-shot control by relying solely on the knowledge present in a pre-trained image-to-video diffusion model without the need for fine-tuning or external knowledge. Our zero-shot method outperforms unsupervised baselines while being competitive with supervised models in terms of visual quality and motion fidelity.', 'score': 13, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': 'a707043470b8dffd', 'authors': ['Koichi Namekata', 'Sherwin Bahmani', 'Ziyi Wu', 'Yash Kant', 'Igor Gilitschenski', 'David B. Lindell'], 'affiliations': ['University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04989.jpg', 'data': {'categories': ['#video', '#diffusion', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'SG-I2V - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. SG-I2V Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': 'Zero-Shot Control in Image-to-Video Generation', 'desc': 'This paper presents SG-I2V, a novel framework for generating videos from images with controllable features. Unlike traditional methods that require fine-tuning on annotated datasets, SG-I2V operates in a zero-shot manner, leveraging a pre-trained image-to-video diffusion model. This approach allows for easier manipulation of elements like object motion and camera movement without the computational costs associated with fine-tuning. The results show that SG-I2V achieves high visual quality and motion fidelity, outperforming unsupervised methods and competing with supervised ones.'}, 'zh': {'title': 'SG-I2Vï¼šé«˜æ•ˆçš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSG-I2Vçš„æ¡†æ¶ï¼Œç”¨äºå¯æ§çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæä¾›é›¶-shotæ§åˆ¶ï¼Œé¿å…äº†ç¹ççš„å¾®è°ƒè¿‡ç¨‹ã€‚ä¸æ— ç›‘ç£åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œè¿åŠ¨ä¿çœŸåº¦ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå¹¶ä¸”ä¸ç›‘ç£æ¨¡å‹ç›¸ç«äº‰ã€‚æ­¤ç ”ç©¶ä¸ºè§†é¢‘ç”Ÿæˆæä¾›äº†ä¸€ç§æ›´é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå‡å°‘äº†å¯¹æ ‡æ³¨æ•°æ®é›†çš„ä¾èµ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10440', 'title': 'LLaVA-o1: Let Vision Language Models Reason Step-by-Step', 'url': 'https://huggingface.co/papers/2411.10440', 'abstract': "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-o1 to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-o1-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-o1 not only outperforms its base model by 8.9% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct.", 'score': 60, 'issue_id': 627, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': 'a706d5cae9964c9e', 'authors': ['Guowei Xu', 'Peng Jin', 'Li Hao', 'Yibing Song', 'Lichao Sun', 'Li Yuan'], 'affiliations': ['School of Electronic and Computer Engineering, Peking University', 'Institute for Interdisciplinary Information Sciences, Tsinghua University', 'Peng Cheng Laboratory', 'AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School', 'Alibaba DAMO Academy', 'Computer Science and Engineering, Lehigh University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10440.jpg', 'data': {'categories': ['#dataset', '#inference', '#reasoning', '#multimodal', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LLaVA-o1: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸', 'desc': 'LLaVA-o1 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², LLaVA-o1 ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ LLaVA-o1-100k ÑĞ¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, LLaVA-o1 Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€ÑĞ´Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'LLaVA-o1: Revolutionizing Visual Reasoning with Structured Multistage Processing', 'desc': 'This paper presents LLaVA-o1, a new Vision-Language Model (VLM) that enhances reasoning capabilities in visual question-answering tasks. Unlike traditional methods that rely on chain-of-thought prompting, LLaVA-o1 autonomously performs multistage reasoning, which includes summarization, visual interpretation, logical reasoning, and conclusion generation. The model is trained on the LLaVA-o1-100k dataset, which contains diverse visual question-answering samples with structured reasoning annotations. With an innovative inference-time stage-level beam search method, LLaVA-o1 achieves significant improvements in precision, outperforming both its base model and larger closed-source models with only 100k training samples.'}, 'zh': {'title': 'LLaVA-o1ï¼šè‡ªä¸»å¤šé˜¶æ®µæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è§†è§‰è¯­è¨€æ¨¡å‹LLaVA-o1ï¼Œæ—¨åœ¨è¿›è¡Œè‡ªä¸»çš„å¤šé˜¶æ®µæ¨ç†ã€‚ä¸ä¼ ç»Ÿçš„é“¾å¼æ€ç»´æç¤ºä¸åŒï¼ŒLLaVA-o1èƒ½å¤Ÿç‹¬ç«‹è¿›è¡Œæ€»ç»“ã€è§†è§‰è§£è¯»ã€é€»è¾‘æ¨ç†å’Œç»“è®ºç”Ÿæˆç­‰å¤šä¸ªé˜¶æ®µã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–çš„æ–¹æ³•ï¼ŒLLaVA-o1åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†LLaVA-o1-100kæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ¨ç†æ—¶é—´é˜¶æ®µçº§æŸæœç´¢æ–¹æ³•ï¼Œä»¥å®ç°æ¨ç†æ—¶é—´çš„æœ‰æ•ˆæ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06558', 'title': 'Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement', 'url': 'https://huggingface.co/papers/2411.06558', 'abstract': 'In this paper, we present RAG, a Regional-Aware text-to-image Generation method conditioned on regional descriptions for precise layout composition. Regional prompting, or compositional generation, which enables fine-grained spatial control, has gained increasing attention for its practicality in real-world applications. However, previous methods either introduce additional trainable modules, thus only applicable to specific models, or manipulate on score maps within cross-attention layers using attention masks, resulting in limited control strength when the number of regions increases. To handle these limitations, we decouple the multi-region generation into two sub-tasks, the construction of individual region (Regional Hard Binding) that ensures the regional prompt is properly executed, and the overall detail refinement (Regional Soft Refinement) over regions that dismiss the visual boundaries and enhance adjacent interactions. Furthermore, RAG novelly makes repainting feasible, where users can modify specific unsatisfied regions in the last generation while keeping all other regions unchanged, without relying on additional inpainting models. Our approach is tuning-free and applicable to other frameworks as an enhancement to the prompt following property. Quantitative and qualitative experiments demonstrate that RAG achieves superior performance over attribute binding and object relationship than previous tuning-free methods.', 'score': 23, 'issue_id': 641, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 10', 'zh': '11æœˆ10æ—¥'}, 'hash': 'f4d24a7a6c0b27a0', 'authors': ['Zhennan Chen', 'Yajie Li', 'Haofan Wang', 'Zhibo Chen', 'Zhengkai Jiang', 'Jun Li', 'Qian Wang', 'Jian Yang', 'Ying Tai'], 'affiliations': ['China Mobile', 'HKUST', 'InstantX', 'Liblib AI', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2411.06558.jpg', 'data': {'categories': ['#rag', '#cv', '#multimodal', '#optimization', '#games'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ RAG (Regional-Aware Generation) Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹. RAG Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. RAG Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµÑ€Ğ¸ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'RAG: Precision in Image Generation through Regional Awareness', 'desc': 'This paper introduces RAG, a method for generating images from text that focuses on specific regions of the image for better layout control. It addresses limitations of previous methods by breaking down the generation process into two tasks: creating individual regions accurately and refining the overall image details. RAG allows users to modify specific areas of an image without affecting others, making it user-friendly and flexible. The method is designed to work without additional training and shows improved performance in generating images with clear attributes and relationships compared to earlier techniques.'}, 'zh': {'title': 'åŒºåŸŸæ„ŸçŸ¥ç”Ÿæˆï¼Œç²¾ç¡®å¸ƒå±€æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºRAGçš„åŒºåŸŸæ„ŸçŸ¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡åŒºåŸŸæè¿°å®ç°ç²¾ç¡®çš„å¸ƒå±€ç»„åˆã€‚è¯¥æ–¹æ³•é€šè¿‡åŒºåŸŸæç¤ºå’Œç»„åˆç”Ÿæˆï¼Œæä¾›äº†ç»†ç²’åº¦çš„ç©ºé—´æ§åˆ¶ï¼Œé€‚ç”¨äºå®é™…åº”ç”¨ã€‚ä¸ä»¥å¾€æ–¹æ³•ä¸åŒï¼ŒRAGå°†å¤šåŒºåŸŸç”Ÿæˆåˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼Œç¡®ä¿åŒºåŸŸæç¤ºçš„æœ‰æ•ˆæ‰§è¡Œå’Œæ•´ä½“ç»†èŠ‚çš„ä¼˜åŒ–ã€‚RAGè¿˜æ”¯æŒç”¨æˆ·åœ¨æœ€åç”Ÿæˆä¸­ä¿®æ”¹ç‰¹å®šåŒºåŸŸï¼Œè€Œæ— éœ€ä¾èµ–é¢å¤–çš„ä¿®å¤æ¨¡å‹ï¼Œå±•ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08033', 'title': 'GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation', 'url': 'https://huggingface.co/papers/2411.08033', 'abstract': 'While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our framework employs a Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single/multi-view image inputs. Notably, the newly proposed latent space naturally enables geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate the effectiveness of our approach on multiple datasets, outperforming existing methods in both text- and image-conditioned 3D generation.', 'score': 19, 'issue_id': 627, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '9c28ee6de37c05b2', 'authors': ['Yushi Lan', 'Shangchen Zhou', 'Zhaoyang Lyu', 'Fangzhou Hong', 'Shuai Yang', 'Bo Dai', 'Xingang Pan', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University, Singapore', 'Shanghai Artificial Intelligence Laboratory', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2411.08033.jpg', 'data': {'categories': ['#synthetic', '#3d', '#multimodal', '#diffusion'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ¾Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ°, Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ GaussianAnything, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ (VAE) Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¼Ğ¸ RGB-D-N Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ 3D-Ñ„Ğ¾Ñ€Ğ¼Ğµ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑƒÑĞ»Ğ¾Ğ²Ğ½ÑƒÑ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ/Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D Generation with GaussianAnything', 'desc': 'This paper presents a new framework for generating 3D content that overcomes limitations in current methods related to input formats and latent space design. It utilizes a Variational Autoencoder (VAE) that takes multi-view RGB-D-Normal renderings as input, creating a structured latent space that maintains essential 3D shape details. The framework also features a cascaded latent diffusion model to separate shape and texture effectively, enhancing the quality of generated 3D models. The proposed method, called GaussianAnything, allows for flexible 3D generation based on various input types, demonstrating superior performance in experiments compared to existing techniques.'}, 'zh': {'title': 'åˆ›æ–°3Dç”Ÿæˆæ¡†æ¶ï¼šè§£è€¦å½¢çŠ¶ä¸çº¹ç†', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨è¾“å…¥æ ¼å¼ã€æ½œåœ¨ç©ºé—´è®¾è®¡å’Œè¾“å‡ºè¡¨ç¤ºæ–¹é¢çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œä»¥å¤šè§†è§’çš„RGB-Dï¼ˆæ·±åº¦ï¼‰-Nï¼ˆæ³•çº¿ï¼‰æ¸²æŸ“ä½œä¸ºè¾“å…¥ï¼Œç‹¬ç‰¹çš„æ½œåœ¨ç©ºé—´è®¾è®¡èƒ½å¤Ÿä¿ç•™3Då½¢çŠ¶ä¿¡æ¯ã€‚é€šè¿‡å¼•å…¥çº§è”æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œæ”¹è¿›äº†å½¢çŠ¶ä¸çº¹ç†çš„è§£è€¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ–‡æœ¬å’Œå›¾åƒæ¡ä»¶ä¸‹çš„3Dç”Ÿæˆæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10323', 'title': 'The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use', 'url': 'https://huggingface.co/papers/2411.10323', 'abstract': "The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic, which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community. All the test cases in the paper can be tried through the project: https://github.com/showlab/computer_use_ootb.", 'score': 14, 'issue_id': 631, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': 'fc2174371c27ab64', 'authors': ['Siyuan Hu', 'Mingyu Ouyang', 'Difei Gao', 'Mike Zheng Shou'], 'affiliations': ['Shou Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.10323.jpg', 'data': {'categories': ['#dataset', '#agents'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'Claude 3.5: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Claude 3.5 Computer Use - Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ñ€ÑĞ´ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ±ĞµÑĞ¿Ñ€ĞµÑ†ĞµĞ´ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Claude 3.5 Ğ² Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‡ĞµĞ¼ ÑÑ‚Ğ¾Ğ»Ğµ. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ°Ñ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ API.'}, 'en': {'title': 'Exploring the Future of GUI Agents with Claude 3.5', 'desc': "The paper introduces Claude 3.5 Computer Use, a pioneering AI model that functions as a graphical user interface (GUI) agent in public beta. It presents a case study that evaluates the model's performance across various tasks and software applications, highlighting its ability to translate natural language commands into desktop actions. The study also provides a framework for deploying API-based GUI automation models, making it easier for developers to implement similar systems. Through detailed analyses, the paper discusses the capabilities and limitations of Claude 3.5, raising important questions for future research in the GUI agent field."}, 'zh': {'title': 'æ¢ç´¢Claude 3.5ï¼šå‰æ²¿AIçš„GUIä»£ç†èƒ½åŠ›', 'desc': 'Claude 3.5è®¡ç®—æœºä½¿ç”¨æ¨¡å‹æ˜¯é¦–ä¸ªæä¾›å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„å‰æ²¿äººå·¥æ™ºèƒ½æ¨¡å‹ã€‚æœ¬æ–‡é€šè¿‡è®¾è®¡ä¸€ç³»åˆ—ä»»åŠ¡ï¼Œæ¢ç´¢å…¶åœ¨å¤æ‚ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒClaude 3.5åœ¨è¯­è¨€åˆ°æ¡Œé¢æ“ä½œçš„ç«¯åˆ°ç«¯èƒ½åŠ›ä¸Šå…·æœ‰å‰æ‰€æœªæœ‰çš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªæ˜“äºå®ç°çš„APIåŸºç¡€çš„GUIè‡ªåŠ¨åŒ–æ¨¡å‹æ¡†æ¶ï¼Œä»¥ä¾¿äºéƒ¨ç½²å’Œæµ‹è¯•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10332', 'title': 'Number it: Temporal Grounding Videos like Flipping Manga', 'url': 'https://huggingface.co/papers/2411.10332', 'abstract': 'Video Large Language Models (Vid-LLMs) have made remarkable advancements in comprehending video content for QA dialogue. However, they struggle to extend this visual understanding to tasks requiring precise temporal localization, known as Video Temporal Grounding (VTG). To address this gap, we introduce Number-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual comprehension with temporal grounding by adding unique numerical identifiers to each video frame. Treating a video as a sequence of numbered frame images, NumPro transforms VTG into an intuitive process: flipping through manga panels in sequence. This allows Vid-LLMs to "read" event timelines, accurately linking visual content with corresponding temporal information. Our experiments demonstrate that NumPro significantly boosts VTG performance of top-tier Vid-LLMs without additional computational cost. Furthermore, fine-tuning on a NumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing previous top-performing methods by up to 6.9\\% in mIoU for moment retrieval and 8.5\\% in mAP for highlight detection. The code will be available at https://github.com/yongliang-wu/NumPro.', 'score': 9, 'issue_id': 631, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': '92d2bcabe629d81c', 'authors': ['Yongliang Wu', 'Xinting Hu', 'Yuyang Sun', 'Yizhou Zhou', 'Wenbo Zhu', 'Fengyun Rao', 'Bernt Schiele', 'Xu Yang'], 'affiliations': ['WeChat, Tencent Inc.', 'Max Planck Institute for Informatics', 'University of California, Berkeley', 'Southeast University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10332.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#video', '#games'], 'emoji': 'ğŸ¬', 'ru': {'title': 'NumPro: ĞŸĞ¾Ğ¼Ğ¾Ğ³Ğ°ĞµĞ¼ Ğ˜Ğ˜ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': "Video-LLM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Number-Prompt (NumPro), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğº ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ ĞºĞ°Ğ´Ñ€Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Video-LLM 'Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ' Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ»Ğ¸Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ NumPro Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Video-LLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚."}, 'en': {'title': 'Bridging Visual Understanding and Temporal Grounding in Videos', 'desc': 'This paper presents a new method called Number-Prompt (NumPro) to improve Video Temporal Grounding (VTG) in Video Large Language Models (Vid-LLMs). Vid-LLMs have difficulty linking visual content with specific timeframes, which is crucial for tasks like question answering about videos. NumPro addresses this by assigning unique numerical identifiers to each video frame, allowing the model to process video as a sequence of numbered images, similar to reading manga panels. The results show that NumPro enhances the performance of Vid-LLMs in VTG tasks significantly, achieving state-of-the-art results without increasing computational costs.'}, 'zh': {'title': 'æ•°å­—æç¤ºï¼šæå‡è§†é¢‘æ—¶é—´å®šä½çš„åˆ›æ–°æ–¹æ³•', 'desc': 'è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVid-LLMsï¼‰åœ¨ç†è§£è§†é¢‘å†…å®¹æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®æ—¶é—´å®šä½çš„ä»»åŠ¡ï¼ˆè§†é¢‘æ—¶é—´å®šä½ï¼ŒVTGï¼‰ä¸­ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”æ•°å­—æç¤ºï¼ˆNumProï¼‰ï¼Œé€šè¿‡ä¸ºæ¯ä¸ªè§†é¢‘å¸§æ·»åŠ ç‹¬ç‰¹çš„æ•°å­—æ ‡è¯†ç¬¦ï¼Œå¸®åŠ©Vid-LLMså°†è§†è§‰ç†è§£ä¸æ—¶é—´å®šä½ç»“åˆèµ·æ¥ã€‚NumProå°†VTGè½¬åŒ–ä¸ºä¸€ç§ç›´è§‚çš„è¿‡ç¨‹ï¼Œç±»ä¼¼äºæŒ‰é¡ºåºç¿»é˜…æ¼«ç”»é¢æ¿ï¼Œä½¿Vid-LLMsèƒ½å¤Ÿâ€œé˜…è¯»â€äº‹ä»¶æ—¶é—´çº¿ï¼Œå‡†ç¡®åœ°å°†è§†è§‰å†…å®¹ä¸ç›¸åº”çš„æ—¶é—´ä¿¡æ¯å…³è”ã€‚å®éªŒè¡¨æ˜ï¼ŒNumProæ˜¾è‘—æå‡äº†é¡¶çº§Vid-LLMsåœ¨VTGä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸”æ²¡æœ‰é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10083', 'title': 'Xmodel-1.5: An 1B-scale Multilingual LLM', 'url': 'https://huggingface.co/papers/2411.10083', 'abstract': "We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model pretrained on approximately 2 trillion tokens. The model demonstrates strong performance across several languages, with particularly notable results in Thai, Arabic, and French, alongside its effectiveness in Chinese and English. In addition, we contribute to the research community by releasing a Thai evaluation dataset, which includes hundreds of questions annotated by students from Chulalongkorn University's School of Integrated Innovation. While the results are promising, we acknowledge that there is still room for improvement. We hope this work advances ongoing efforts in multilingual AI research and promotes better cross-linguistic understanding in various natural language processing tasks. Our models and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelLM.", 'score': 7, 'issue_id': 641, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': '745c93b05bda9f01', 'authors': ['Wang Qun', 'Liu Yang', 'Lin Qingquan', 'Jiang Ling'], 'affiliations': ['XiaoduoAI'], 'pdf_title_img': 'assets/pdf/title_img/2411.10083.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#open_source', '#small_models', '#low_resource'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Xmodel-1.5: ĞœÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Xmodel-1.5 - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ·Ñ‹Ñ‡Ğ½ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ½Ğ° 2 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼, Ğ°Ñ€Ğ°Ğ±ÑĞºĞ¾Ğ¼ Ğ¸ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ¼, Ğ½Ğ°Ñ€ÑĞ´Ñƒ Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ ÑĞ¾Ñ‚Ğ½Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ ĞºĞ¾Ğ´ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ Ğ½Ğ° GitHub.'}, 'en': {'title': 'Empowering Multilingual AI with Xmodel-1.5', 'desc': 'Xmodel-1.5 is a large multilingual model with 1 billion parameters, trained on a massive dataset of 2 trillion tokens. It shows impressive performance in multiple languages, especially in Thai, Arabic, and French, while also being effective in Chinese and English. The authors provide a new evaluation dataset for Thai, created with input from students, to aid in further research. This work aims to enhance multilingual AI capabilities and foster better understanding across different languages in natural language processing tasks.'}, 'zh': {'title': 'æ¨åŠ¨å¤šè¯­è¨€AIç ”ç©¶çš„å‰æ²¿', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†Xmodel-1.5ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„åäº¿å‚æ•°å¤šè¯­è¨€å¤§æ¨¡å‹ï¼Œé¢„è®­ç»ƒäºå¤§çº¦2ä¸‡äº¿ä¸ªæ ‡è®°ä¸Šã€‚è¯¥æ¨¡å‹åœ¨å¤šç§è¯­è¨€ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨æ³°è¯­ã€é˜¿æ‹‰ä¼¯è¯­å’Œæ³•è¯­æ–¹é¢çš„ç»“æœå°¤ä¸ºæ˜¾è‘—ï¼ŒåŒæ—¶åœ¨ä¸­æ–‡å’Œè‹±æ–‡ä¸­ä¹Ÿè¡¨ç°è‰¯å¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºç ”ç©¶ç¤¾åŒºè´¡çŒ®äº†ä¸€ä¸ªæ³°è¯­è¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…å«ç”±æœ±æ‹‰éš†åŠŸå¤§å­¦ç»¼åˆåˆ›æ–°å­¦é™¢çš„å­¦ç”Ÿæ ‡æ³¨çš„æ•°ç™¾ä¸ªé—®é¢˜ã€‚å°½ç®¡ç»“æœä»¤äººé¼“èˆï¼Œä½†æˆ‘ä»¬æ‰¿è®¤ä»æœ‰æ”¹è¿›çš„ç©ºé—´ï¼Œå¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¨åŠ¨å¤šè¯­è¨€äººå·¥æ™ºèƒ½ç ”ç©¶çš„è¿›å±•ï¼Œå¹¶ä¿ƒè¿›å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„è·¨è¯­è¨€ç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.10438', 'title': 'MARS: Unleashing the Power of Variance Reduction for Training Large Models', 'url': 'https://huggingface.co/papers/2411.10438', 'abstract': 'Training deep neural networks--and more recently, large models--demands efficient and scalable optimizers. Adaptive gradient algorithms like Adam, AdamW, and their variants have been central to this task. Despite the development of numerous variance reduction algorithms in the past decade aimed at accelerating stochastic optimization in both convex and nonconvex settings, variance reduction has not found widespread success in training deep neural networks or large language models. Consequently, it has remained a less favored approach in modern AI. In this paper, to unleash the power of variance reduction for efficient training of large models, we propose a unified optimization framework, MARS (Make vAriance Reduction Shine), which reconciles preconditioned gradient methods with variance reduction via a scaled stochastic recursive momentum technique. Within our framework, we introduce three instances of MARS that leverage preconditioned gradient updates based on AdamW, Lion, and Shampoo, respectively. We also draw a connection between our algorithms and existing optimizers. Experimental results on training GPT-2 models indicate that MARS consistently outperforms AdamW by a large margin.', 'score': 1, 'issue_id': 646, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 15', 'zh': '11æœˆ15æ—¥'}, 'hash': 'c08911f01fcd1d00', 'authors': ['Huizhuo Yuan', 'Yifeng Liu', 'Shuang Wu', 'Xun Zhou', 'Quanquan Gu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.10438.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'MARS: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MARS, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ñ Ñ€ĞµĞ´ÑƒĞºÑ†Ğ¸ĞµĞ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ° MARS, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° AdamW, Lion Ğ¸ Shampoo. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° GPT-2 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MARS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ AdamW Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unleashing Variance Reduction for Efficient Model Training', 'desc': 'This paper addresses the need for efficient optimizers in training deep neural networks and large models. It highlights the limitations of existing variance reduction techniques in this context and introduces a new framework called MARS (Make vAriance Reduction Shine). MARS combines preconditioned gradient methods with variance reduction using a scaled stochastic recursive momentum approach. The authors demonstrate that MARS significantly outperforms the popular AdamW optimizer in training large models like GPT-2.'}, 'zh': {'title': 'è®©æ–¹å·®å‡å°‘æŠ€æœ¯åœ¨å¤§æ¨¡å‹è®­ç»ƒä¸­å¤§æ”¾å¼‚å½©', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä¼˜åŒ–æ¡†æ¶MARSï¼ˆè®©æ–¹å·®å‡å°‘é—ªè€€ï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚æˆ‘ä»¬å°†é¢„å¤„ç†æ¢¯åº¦æ–¹æ³•ä¸æ–¹å·®å‡å°‘æŠ€æœ¯ç»“åˆï¼Œé‡‡ç”¨ç¼©æ”¾éšæœºé€’å½’åŠ¨é‡æŠ€æœ¯ã€‚é€šè¿‡å¼•å…¥åŸºäºAdamWã€Lionå’ŒShampooçš„ä¸‰ç§MARSå®ä¾‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ–¹å·®å‡å°‘åœ¨æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„æ½œåŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMARSåœ¨è®­ç»ƒGPT-2æ¨¡å‹æ—¶æ˜¾è‘—ä¼˜äºAdamWã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07133', 'title': 'Stronger Models are NOT Stronger Teachers for Instruction Tuning', 'url': 'https://huggingface.co/papers/2411.07133', 'abstract': "Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.", 'score': 25, 'issue_id': 546, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'be2fc1cdad8aa9f3', 'authors': ['Zhangchen Xu', 'Fengqing Jiang', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran'], 'affiliations': ['University of Washington', 'Allen Institute for AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07133.jpg', 'data': {'categories': ['#dataset', '#optimization', '#synthetic', '#training', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLMs) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¾ĞºÑ, Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¼ĞµĞ»ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ğ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Compatibility-Adjusted Reward (CAR), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ CAR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ²ÑĞµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹.'}, 'en': {'title': "Rethinking Instruction Tuning: Size Isn't Everything!", 'desc': "This paper investigates the effectiveness of instruction tuning in large language models (LLMs) and challenges the assumption that larger models are better teachers for smaller models. The authors introduce the concept of the Larger Models' Paradox, showing that bigger models do not always enhance the instruction-following capabilities of smaller models. They highlight the limitations of existing metrics in evaluating response generators, which fail to consider the compatibility between the teacher and the base model. To address this, they propose a new metric called Compatibility-Adjusted Reward (CAR), which shows improved performance in assessing the effectiveness of response generators across various models."}, 'zh': {'title': 'å¤§æ¨¡å‹ä¸ä¸€å®šæ˜¯å¥½æ•™å¸ˆï¼', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†æŒ‡ä»¤è°ƒä¼˜åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†æŒ‡ä»¤æ•°æ®é›†å¯¹æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¾ƒå¤§æˆ–è¾ƒå¼ºçš„æ¨¡å‹å¹¶ä¸ä¸€å®šæ˜¯è¾ƒå°æ¨¡å‹çš„æ›´å¥½æ•™å¸ˆï¼Œè¿™ä¸€ç°è±¡è¢«ç§°ä¸ºâ€œå¤§æ¨¡å‹æ‚–è®ºâ€ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡æ— æ³•å‡†ç¡®é¢„æµ‹å“åº”ç”Ÿæˆå™¨çš„æœ‰æ•ˆæ€§ï¼Œå› ä¸ºå®ƒä»¬å¿½ç•¥äº†æ•™å¸ˆæ¨¡å‹ä¸è¢«è°ƒä¼˜åŸºç¡€æ¨¡å‹ä¹‹é—´çš„å…¼å®¹æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”å…¼å®¹æ€§è°ƒæ•´å¥–åŠ±ï¼ˆCARï¼‰ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07184', 'title': 'SAMPart3D: Segment Any Part in 3D Objects', 'url': 'https://huggingface.co/papers/2411.07184', 'abstract': '3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.', 'score': 24, 'issue_id': 541, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 11', 'zh': '11æœˆ11æ—¥'}, 'hash': 'b4e58a99e4a7e86c', 'authors': ['Yunhan Yang', 'Yukun Huang', 'Yuan-Chen Guo', 'Liangjun Lu', 'Xiaoyang Wu', 'Edmund Y. Lam', 'Yan-Pei Cao', 'Xihui Liu'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07184.jpg', 'data': {'categories': ['#games', '#3d', '#benchmark', '#optimization'], 'emoji': 'ğŸ§©', 'ru': {'title': 'SAMPart3D: Ğ³Ğ¸Ğ±ĞºĞ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SAMPart3D - Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±ĞµĞ·Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸Ğ· 3D-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ½ĞµÑ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¿Ğ¾ÑĞ¾Ğ±ĞµĞ½ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸ÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. SAMPart3D Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 3D Part Segmentation with SAMPart3D', 'desc': 'This paper presents SAMPart3D, a novel framework for zero-shot 3D part segmentation that does not depend on predefined text prompts. It utilizes text-agnostic vision foundation models to extract 3D features, enabling it to scale effectively to large unlabeled datasets. The framework also incorporates scale-conditioned part-aware features, allowing for segmentation at various levels of detail. SAMPart3D outperforms existing methods and introduces a new benchmark to enhance the diversity and complexity of 3D part segmentation tasks.'}, 'zh': {'title': 'SAMPart3Dï¼šæ— æ–‡æœ¬æç¤ºçš„3Déƒ¨ä»¶åˆ†å‰²æ–°æ¡†æ¶', 'desc': '3Déƒ¨ä»¶åˆ†å‰²æ˜¯3Dæ„ŸçŸ¥ä¸­çš„ä¸€é¡¹é‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå¹¿æ³›åº”ç”¨äºæœºå™¨äººæŠ€æœ¯ã€3Dç”Ÿæˆå’Œ3Dç¼–è¾‘ç­‰é¢†åŸŸã€‚æœ¬æ–‡æå‡ºäº†SAMPart3Dæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸ä¾èµ–é¢„å®šä¹‰æ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹ï¼Œå¯¹ä»»æ„3Då¯¹è±¡è¿›è¡Œå¤šç²’åº¦çš„è¯­ä¹‰éƒ¨ä»¶åˆ†å‰²ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ— æ–‡æœ¬ä¾èµ–çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œä»å¤§è§„æ¨¡æœªæ ‡è®°çš„3Dæ•°æ®é›†ä¸­æå–ä¸°å¯Œçš„3Dç‰¹å¾ï¼Œå¹¶é€šè¿‡æ¡ä»¶åŒ–çš„éƒ¨ä»¶æ„ŸçŸ¥ç‰¹å¾å®ç°çµæ´»çš„åˆ†å‰²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAMPart3Dåœ¨å¤„ç†å¤æ‚å¯¹è±¡æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬3Déƒ¨ä»¶åˆ†å‰²æ–¹æ³•ï¼Œå¹¶èƒ½æ”¯æŒå¤šç§åº”ç”¨ï¼Œå¦‚éƒ¨ä»¶çº§ç¼–è¾‘å’Œäº¤äº’å¼åˆ†å‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07975', 'title': 'JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2411.07975', 'abstract': 'We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.', 'score': 20, 'issue_id': 544, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '294dc65a01cd1218', 'authors': ['Yiyang Ma', 'Xingchao Liu', 'Xiaokang Chen', 'Wen Liu', 'Chengyue Wu', 'Zhiyu Wu', 'Zizheng Pan', 'Zhenda Xie', 'Haowei Zhang', 'Xingkai yu', 'Liang Zhao', 'Yisong Wang', 'Jiaying Liu', 'Chong Ruan'], 'affiliations': ['DeepSeek-AI', 'Peking University', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07975.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#architecture', '#diffusion', '#multimodal'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'JanusFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ rectified flow Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ rectified flow Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'JanusFlow: Unifying Image Understanding and Generation Efficiently', 'desc': 'JanusFlow is a novel framework that combines image understanding and generation into one cohesive model. It utilizes a simple architecture that merges autoregressive language models with rectified flow, enhancing generative modeling capabilities. The study reveals that rectified flow can be effectively trained within the large language model context without needing complex changes to the architecture. By decoupling the encoders for understanding and generation and aligning their representations during training, JanusFlow demonstrates superior performance compared to specialized models and existing unified approaches.'}, 'zh': {'title': 'JanusFlowï¼šå›¾åƒç†è§£ä¸ç”Ÿæˆçš„ç»Ÿä¸€æ¨¡å‹', 'desc': 'JanusFlowæ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œå°†å›¾åƒç†è§£å’Œç”Ÿæˆç»Ÿä¸€åœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ã€‚å®ƒé‡‡ç”¨äº†ç®€çº¦çš„æ¶æ„ï¼Œå°†è‡ªå›å½’è¯­è¨€æ¨¡å‹ä¸ä¿®æ­£æµç»“åˆï¼Œè¿™æ˜¯ç”Ÿæˆå»ºæ¨¡ä¸­çš„ä¸€ç§å…ˆè¿›æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œä¿®æ­£æµå¯ä»¥åœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ¡†æ¶å†…è½»æ¾è®­ç»ƒï¼Œæ— éœ€å¤æ‚çš„æ¶æ„ä¿®æ”¹ã€‚é€šè¿‡è§£è€¦ç†è§£å’Œç”Ÿæˆç¼–ç å™¨ä»¥åŠåœ¨ç»Ÿä¸€è®­ç»ƒä¸­å¯¹é½å®ƒä»¬çš„è¡¨ç¤ºï¼ŒJanusFlowåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„ç»Ÿä¸€æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.07461', 'title': 'BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions', 'url': 'https://huggingface.co/papers/2411.07461', 'abstract': 'We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that bridges the gap between descriptive synthetic captions and factual web-scale alt-text. KALE augments synthetic dense image captions with web-scale alt-text to generate factually grounded image captions. Our two-stage approach leverages large vision-language models and language models to create knowledge-augmented captions, which are then used to train a specialized VLM for scaling up the dataset. We train vision-language models on KALE and demonstrate improvements on vision-language tasks. Our experiments show the utility of KALE for training more capable and knowledgeable multimodal models. We release the KALE dataset at https://huggingface.co/datasets/Salesforce/blip3-kale', 'score': 18, 'issue_id': 552, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '08fb959148999629', 'authors': ['Anas Awadalla', 'Le Xue', 'Manli Shu', 'An Yan', 'Jun Wang', 'Senthil Purushwalkam', 'Sheng Shen', 'Hannah Lee', 'Oscar Lo', 'Jae Sung Park', 'Etash Guha', 'Silvio Savarese', 'Ludwig Schmidt', 'Yejin Choi', 'Caiming Xiong', 'Ran Xu'], 'affiliations': ['University of Washington', 'Salesforce Research', 'Stanford University', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07461.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#synthetic', '#open_source'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'KALE: ĞĞ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… BLIP3-KALE, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ¸Ğ· 218 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ñ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ· Ğ²ĞµĞ±-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° KALE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Bridging Descriptive and Factual Captions with KALE', 'desc': 'The paper presents BLIP3-KALE, a new dataset containing 218 million image-text pairs that enhances the quality of image captions by combining synthetic captions with factual web-based alt-text. This dataset is created using a two-stage method that employs large vision-language models and language models to produce captions that are both descriptive and factually accurate. By training specialized vision-language models on the KALE dataset, the authors demonstrate significant improvements in various vision-language tasks. The findings highlight the effectiveness of KALE in developing more advanced multimodal models that can better understand and generate content across different modalities.'}, 'zh': {'title': 'çŸ¥è¯†å¢å¼ºçš„å›¾åƒæ ‡é¢˜ç”Ÿæˆ', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†BLIP3-KALEï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«2.18äº¿å¯¹å›¾åƒ-æ–‡æœ¬çš„æ•°æ®é›†ï¼Œæ—¨åœ¨å¼¥åˆæè¿°æ€§åˆæˆæ ‡é¢˜ä¸äº‹å®æ€§ç½‘ç»œè§„æ¨¡æ›¿ä»£æ–‡æœ¬ä¹‹é—´çš„å·®è·ã€‚KALEé€šè¿‡å°†åˆæˆçš„å¯†é›†å›¾åƒæ ‡é¢˜ä¸ç½‘ç»œè§„æ¨¡çš„æ›¿ä»£æ–‡æœ¬ç›¸ç»“åˆï¼Œç”ŸæˆåŸºäºäº‹å®çš„å›¾åƒæ ‡é¢˜ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µçš„æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹å’Œè¯­è¨€æ¨¡å‹åˆ›å»ºçŸ¥è¯†å¢å¼ºçš„æ ‡é¢˜ï¼Œå¹¶ç”¨è¿™äº›æ ‡é¢˜è®­ç»ƒä¸“é—¨çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»¥æ‰©å¤§æ•°æ®é›†è§„æ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKALEåœ¨è®­ç»ƒæ›´å¼ºå¤§å’Œæ›´å…·çŸ¥è¯†æ€§çš„å¤šæ¨¡æ€æ¨¡å‹æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08034', 'title': 'Scaling Properties of Diffusion Models for Perceptual Tasks', 'url': 'https://huggingface.co/papers/2411.08034', 'abstract': 'In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and segmentation under image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perception tasks. Through a careful analysis of these scaling behaviors, we present various techniques to efficiently train diffusion models for visual perception tasks. Our models achieve improved or comparable performance to state-of-the-art methods using significantly less data and compute. To use our code and models, see https://scaling-diffusion-perception.github.io .', 'score': 12, 'issue_id': 557, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '4be3b9af89108627', 'authors': ['Rahul Ravishankar', 'Zeeshan Patel', 'Jathushan Rajasegaran', 'Jitendra Malik'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08034.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#cv'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ÑÑ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‚ Ñ‚Ğ°ĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹, Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, ĞºĞ°Ğº Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Scaling Diffusion Models for Enhanced Visual Perception', 'desc': 'This paper explores how diffusion models can be effectively used for both generating images and performing visual perception tasks like depth estimation and segmentation. It presents a unified approach that treats these tasks as image-to-image translation problems, highlighting the advantages of scaling in training and testing. The authors analyze the scaling behaviors of diffusion models and propose techniques to enhance their efficiency in visual perception tasks. As a result, their models demonstrate competitive performance compared to leading methods while requiring less data and computational resources.'}, 'zh': {'title': 'æ‰©æ•£æ¨¡å‹ï¼šè§†è§‰æ„ŸçŸ¥çš„æ–°åŠ›é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå’Œè§†è§‰æ„ŸçŸ¥ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›ã€‚æˆ‘ä»¬å°†æ·±åº¦ä¼°è®¡ã€å…‰æµå’Œåˆ†å‰²ç­‰ä»»åŠ¡ç»Ÿä¸€ä¸ºå›¾åƒåˆ°å›¾åƒçš„è½¬æ¢ï¼Œå¹¶å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨è¿™äº›æ„ŸçŸ¥ä»»åŠ¡ä¸­å¦‚ä½•é€šè¿‡æ‰©å±•è®­ç»ƒå’Œæµ‹è¯•è®¡ç®—æ¥è·ç›Šã€‚é€šè¿‡å¯¹è¿™äº›æ‰©å±•è¡Œä¸ºçš„ä»”ç»†åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†å¤šç§é«˜æ•ˆè®­ç»ƒæ‰©æ•£æ¨¡å‹çš„æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä½¿ç”¨æ˜¾è‘—æ›´å°‘çš„æ•°æ®å’Œè®¡ç®—çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.08017', 'title': 'Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings', 'url': 'https://huggingface.co/papers/2411.08017', 'abstract': "Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively. To address this, we introduce a novel approach called Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based, compact latent encodings. Specifically, we compress a 256^3 signed distance field into a 12^3 times 4 latent grid, achieving an impressive 2427x compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing the inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at 256^3 resolution. Moreover, WaLa offers rapid inference, producing shapes within two to four seconds depending on the condition, despite the model's scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. We open-source our code and, to the best of our knowledge, release the largest pretrained 3D generative models across different modalities.", 'score': 11, 'issue_id': 547, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 12', 'zh': '11æœˆ12æ—¥'}, 'hash': '0af1f4c0dc38cc5b', 'authors': ['Aditya Sanghi', 'Aliasghar Khani', 'Pradyumna Reddy', 'Arianna Rampini', 'Derek Cheung', 'Kamal Rahimi Malekshan', 'Kanika Madan', 'Hooman Shayani'], 'affiliations': ['Autodesk AI Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08017.jpg', 'data': {'categories': ['#inference', '#open_source', '#3d', '#dataset', '#diffusion', '#architecture'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-ÑĞ¶Ğ°Ñ‚Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Wavelet Latent Diffusion (WaLa). ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ 3D-Ñ„Ğ¾Ñ€Ğ¼, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ 2427x. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. WaLa Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient 3D Shape Generation with Wavelet Latent Diffusion', 'desc': 'This paper presents a new method called Wavelet Latent Diffusion (WaLa) for generating high-quality 3D shapes efficiently. It addresses the limitations of existing 3D generative models by using wavelet-based latent encodings, which compress 3D shapes significantly while preserving detail. The method achieves a remarkable 2427x compression ratio, allowing for the training of large-scale generative networks without increasing inference time. WaLa demonstrates state-of-the-art performance in generating diverse and high-quality 3D shapes, and the authors have made their code and pretrained models available to the public.'}, 'zh': {'title': 'é«˜æ•ˆå‹ç¼©ï¼Œå¿«é€Ÿç”Ÿæˆ3Då½¢çŠ¶çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºWavelet Latent Diffusionï¼ˆWaLaï¼‰ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡3Dç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡ã€‚é€šè¿‡å°†3Då½¢çŠ¶ç¼–ç ä¸ºåŸºäºå°æ³¢çš„ç´§å‡‘æ½œåœ¨ç¼–ç ï¼ŒWaLaå®ç°äº†é«˜è¾¾2427å€çš„å‹ç¼©æ¯”ï¼ŒåŒæ—¶ä¿æŒäº†ç»†èŠ‚çš„å®Œæ•´æ€§ã€‚è¯¥æ–¹æ³•ä½¿å¾—è®­ç»ƒå¤§å‹ç”Ÿæˆç½‘ç»œå˜å¾—æ›´åŠ é«˜æ•ˆï¼Œå¹¶ä¸”åœ¨æ¨ç†æ—¶ä¸ä¼šæ˜¾è‘—å¢åŠ æ—¶é—´ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç”Ÿæˆé«˜è´¨é‡çš„3Då½¢çŠ¶ï¼Œå¹¶ä¸”å¼€æºäº†ä»£ç ï¼Œæ¨åŠ¨äº†3Dç”Ÿæˆæ¨¡å‹çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.06307', 'title': 'Acoustic Volume Rendering for Neural Impulse Response Fields', 'url': 'https://huggingface.co/papers/2411.06307', 'abstract': "Realistic audio synthesis that captures accurate acoustic phenomena is essential for creating immersive experiences in virtual and augmented reality. Synthesizing the sound received at any position relies on the estimation of impulse response (IR), which characterizes how sound propagates in one scene along different paths before arriving at the listener's position. In this paper, we present Acoustic Volume Rendering (AVR), a novel approach that adapts volume rendering techniques to model acoustic impulse responses. While volume rendering has been successful in modeling radiance fields for images and neural scene representations, IRs present unique challenges as time-series signals. To address these challenges, we introduce frequency-domain volume rendering and use spherical integration to fit the IR measurements. Our method constructs an impulse response field that inherently encodes wave propagation principles and achieves state-of-the-art performance in synthesizing impulse responses for novel poses. Experiments show that AVR surpasses current leading methods by a substantial margin. Additionally, we develop an acoustic simulation platform, AcoustiX, which provides more accurate and realistic IR simulations than existing simulators. Code for AVR and AcoustiX are available at https://zitonglan.github.io/avr.", 'score': 4, 'issue_id': 554, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 9', 'zh': '11æœˆ9æ—¥'}, 'hash': 'f6935a265562d416', 'authors': ['Zitong Lan', 'Chenhao Zheng', 'Zhiwei Zheng', 'Mingmin Zhao'], 'affiliations': ['University of Pennsylvania', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06307.jpg', 'data': {'categories': ['#audio'], 'emoji': 'ğŸ”Š', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ·Ğ²ÑƒĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑŠĞµĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ°ĞºÑƒÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Acoustic Volume Rendering (AVR). AVR Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ° Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ² Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ ÑÑ„ĞµÑ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AVR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ¸Ğ¼Ğ¿ÑƒĞ»ÑŒÑĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Sound: Acoustic Volume Rendering for Immersive Audio Experiences', 'desc': 'This paper introduces Acoustic Volume Rendering (AVR), a new method for synthesizing realistic audio in virtual and augmented reality. AVR uses volume rendering techniques to model acoustic impulse responses (IRs), which describe how sound travels in a scene. The authors tackle the unique challenges of IRs as time-series signals by employing frequency-domain volume rendering and spherical integration. Their approach not only improves the accuracy of sound synthesis but also outperforms existing methods, demonstrating significant advancements in acoustic simulation with their platform, AcoustiX.'}, 'zh': {'title': 'å£°å­¦ä½“ç§¯æ¸²æŸ“ï¼šæå‡è™šæ‹Ÿç°å®ä¸­çš„éŸ³é¢‘ä½“éªŒ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å£°å­¦ä½“ç§¯æ¸²æŸ“ï¼ˆAVRï¼‰æ–¹æ³•ï¼Œç”¨äºåˆæˆçœŸå®çš„å£°å­¦è„‰å†²å“åº”ï¼ˆIRï¼‰ï¼Œä»¥å¢å¼ºè™šæ‹Ÿå’Œå¢å¼ºç°å®ä¸­çš„æ²‰æµ¸ä½“éªŒã€‚AVRé€šè¿‡é€‚åº”ä½“ç§¯æ¸²æŸ“æŠ€æœ¯ï¼Œè§£å†³äº†å£°æ³¢ä¼ æ’­çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å°†æ—¶é—´åºåˆ—ä¿¡å·å»ºæ¨¡ä¸ºè„‰å†²å“åº”åœºã€‚æˆ‘ä»¬å¼•å…¥äº†é¢‘åŸŸä½“ç§¯æ¸²æŸ“å’Œçƒé¢ç§¯åˆ†æŠ€æœ¯ï¼Œä»¥æ›´å‡†ç¡®åœ°æ‹ŸåˆIRæµ‹é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAVRåœ¨åˆæˆæ–°å§¿æ€çš„è„‰å†²å“åº”æ–¹é¢è¶…è¶Šäº†ç°æœ‰çš„é¢†å…ˆæ–¹æ³•ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå£°å­¦æ¨¡æ‹Ÿå¹³å°AcoustiXï¼Œæä¾›æ›´å‡†ç¡®çš„IRæ¨¡æ‹Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.05197', 'title': 'Hardware and Software Platform Inference', 'url': 'https://huggingface.co/papers/2411.05197', 'abstract': "It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. In this paper we introduce \\textbf{hardware and software platform inference (HSPI)} -- a method for identifying the underlying  architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. Our method leverages the inherent differences of various  architectures and compilers to distinguish between different  types and software stacks. By analyzing the numerical patterns in the model's outputs, we propose a classification framework capable of accurately identifying the  used for model inference as well as the underlying software configuration. Our findings demonstrate the feasibility of inferring  type from black-box models. We evaluate HSPI against models served on different real hardware and find that in a white-box setting we can distinguish between different s with between 83.9% and 100% accuracy. Even in a black-box setting we are able to achieve results that are up to three times higher than random guess accuracy.", 'score': 3, 'issue_id': 550, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 7', 'zh': '11æœˆ7æ—¥'}, 'hash': '7685c8e74f6dbc6b', 'authors': ['Cheng Zhang', 'Hanna Foerster', 'Robert D. Mullins', 'Yiren Zhao', 'Ilia Shumailov'], 'affiliations': ['Imperial College London', 'University of Cambridge', 'Google DeepMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05197.jpg', 'data': {'categories': ['#leakage', '#security', '#architecture', '#inference'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ¾Ğ±Ğ»Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ğ°: ĞºĞ°Ğº Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ HSPI (Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¹ ÑÑ‚ĞµĞº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµĞµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğµ-Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸ÑÑƒÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ‚Ğ¾Ñ€Ğ°Ğ¼ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ° Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ´Ğ¾ 100% Ğ² Ğ±ĞµĞ»Ğ¾Ğ¼ ÑÑ‰Ğ¸ĞºĞµ Ğ¸ Ğ´Ğ¾ Ñ‚Ñ€ĞµÑ… Ñ€Ğ°Ğ· Ğ²Ñ‹ÑˆĞµ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‡ĞµÑ€Ğ½Ğ¾Ğ¼ ÑÑ‰Ğ¸ĞºĞµ.'}, 'en': {'title': 'Verify Your Model: Uncovering the Truth Behind LLM Inference', 'desc': "This paper addresses the challenge of verifying the authenticity of large language model (LLM) services purchased by businesses. It introduces a novel method called hardware and software platform inference (HSPI), which analyzes the input-output behavior of machine learning models to identify their underlying hardware and software configurations. By examining the numerical patterns in the outputs, HSPI can distinguish between different architectures and software stacks, even in a black-box scenario. The results show that HSPI can achieve high accuracy in identifying the model's hardware and software, significantly improving upon random guessing."}, 'zh': {'title': 'éªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹çš„çœŸå®æ€§', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºç¡¬ä»¶å’Œè½¯ä»¶å¹³å°æ¨ç†ï¼ˆHSPIï¼‰çš„æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«æœºå™¨å­¦ä¹ æ¨¡å‹çš„åº•å±‚æ¶æ„å’Œè½¯ä»¶å †æ ˆã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ†ææ¨¡å‹çš„è¾“å…¥è¾“å‡ºè¡Œä¸ºï¼Œåˆ©ç”¨ä¸åŒæ¶æ„å’Œç¼–è¯‘å™¨çš„å›ºæœ‰å·®å¼‚æ¥åŒºåˆ†ä¸åŒç±»å‹çš„ç¡¬ä»¶å’Œè½¯ä»¶é…ç½®ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç™½ç›’ç¯å¢ƒä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥83.9%åˆ°100%çš„å‡†ç¡®ç‡åŒºåˆ†ä¸åŒçš„ç¡¬ä»¶ï¼Œè€Œåœ¨é»‘ç›’ç¯å¢ƒä¸‹ï¼Œå‡†ç¡®ç‡ä¹Ÿèƒ½è¾¾åˆ°éšæœºçŒœæµ‹çš„ä¸‰å€ä»¥ä¸Šã€‚æ­¤æ–¹æ³•ä¸ºéªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹çš„çœŸå®æ€§æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ‰‹æ®µã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (28)', '#agents (22)', '#agi (10)', '#alignment (20)', '#architecture (81)', '#audio (3)', '#benchmark (88)', '#cv (59)', '#data (32)', '#dataset (82)', '#diffusion (43)', '#ethics (4)', '#games (29)', '#graphs (13)', '#hallucinations (10)', '#healthcare (12)', '#inference (25)', '#interpretability (21)', '#leakage (2)', '#long_context (18)', '#low_resource (12)', '#machine_translation (2)', '#math (11)', '#multilingual (16)', '#multimodal (59)', '#open_source (71)', '#optimization (101)', '#plp (8)', '#rag (8)', '#reasoning (35)', '#rl (13)', '#rlhf (11)', '#robotics (6)', '#science (17)', '#security (4)', '#small_models (13)', '#story_generation (4)', '#survey (9)', '#synthetic (25)', '#training (104)', '#transfer_learning (14)', '#video (37)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-11-26 06:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-26 06:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-26 06:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    