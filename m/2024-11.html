
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 225 papers. November 2024.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Ноябрь 2024</span> | <span id="title-articles-count">225 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/m/2024-10.html">⬅️ <span id="prev-date">10.2024</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2024-12.html">➡️ <span id="next-date">12.2024</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Ноябрь 2024', 'en': 'November 2024', 'zh': '11月2024年'};
        let feedDateNext = {'ru': '12.2024', 'en': '12/2024', 'zh': '12月2024年'};
        let feedDatePrev = {'ru': '10.2024', 'en': '10/2024', 'zh': '10月2024年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.02959', 'title': 'HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems', 'url': 'https://huggingface.co/papers/2411.02959', 'abstract': 'Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems.', 'score': 59, 'issue_id': 437, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '6fb8684374e5fdcb', 'authors': ['Jiejun Tan', 'Zhicheng Dou', 'Wen Wang', 'Mang Wang', 'Weipeng Chen', 'Ji-Rong Wen'], 'affiliations': ['Gaoling School of Artificial Intelligence Renmin University of China', 'Baichuan Intelligent Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02959.jpg', 'data': {'categories': ['#rag', '#hallucinations', '#inference', '#data', '#retrieval_augmented_generation'], 'emoji': '🌐', 'ru': {'title': 'HtmlRAG: Улучшение RAG-систем с помощью структурированной веб-информации', 'desc': 'Статья представляет новый подход к генерации с извлечением информации (RAG), названный HtmlRAG. В отличие от традиционных систем RAG, использующих простой текст, HtmlRAG сохраняет структурную и семантическую информацию HTML-документов. Авторы предлагают методы очистки, сжатия и обрезки HTML для уменьшения шума и сокращения входных токенов. Эксперименты на шести наборах данных для вопросно-ответных систем подтверждают превосходство использования HTML в системах RAG.'}, 'en': {'title': 'Harnessing HTML for Enhanced Knowledge Retrieval in RAG Systems', 'desc': 'This paper introduces HtmlRAG, a novel approach to Retrieval-Augmented Generation (RAG) that utilizes HTML instead of plain text for knowledge retrieval. By leveraging the structural and semantic information present in HTML, HtmlRAG aims to enhance the performance of large language models (LLMs) and reduce the hallucination problem. The authors address the challenges posed by HTML, such as excess tokens and noise, by implementing cleaning, compression, and pruning techniques to streamline the input. Experimental results demonstrate that HtmlRAG outperforms traditional plain-text-based RAG systems across multiple question-answering datasets.'}, 'zh': {'title': '用HTML提升检索增强生成的能力', 'desc': '本文提出了一种新的检索增强生成（RAG）方法，称为HtmlRAG，旨在改善大语言模型（LLMs）的知识能力并减少幻觉问题。HtmlRAG使用HTML格式而非纯文本来增强生成过程，从而保留更多的结构和语义信息。为了应对HTML中多余内容带来的挑战，本文提出了HTML清理、压缩和修剪策略，以减少输入的冗余信息。实验结果表明，HtmlRAG在六个问答数据集上的表现优于传统的纯文本RAG系统。'}}}, {'id': 'https://huggingface.co/papers/2411.00871', 'title': 'LLaMo: Large Language Model-based Molecular Graph Assistant', 'url': 'https://huggingface.co/papers/2411.00871', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable generalization and instruction-following capabilities with instruction tuning. The advancements in LLMs and instruction tuning have led to the development of Large Vision-Language Models (LVLMs). However, the competency of the LLMs and instruction tuning have been less explored in the molecular domain. Thus, we propose LLaMo: Large Language Model-based Molecular graph assistant, which is an end-to-end trained large molecular graph-language model. To bridge the discrepancy between the language and graph modalities, we present the multi-level graph projector that transforms graph representations into graph tokens by abstracting the output representations of each GNN layer and motif representations with the cross-attention mechanism. We also introduce machine-generated molecular graph instruction data to instruction-tune the large molecular graph-language model for general-purpose molecule and language understanding. Our extensive experiments demonstrate that LLaMo shows the best performance on diverse tasks, such as molecular description generation, property prediction, and IUPAC name prediction. The code of LLaMo is available at https://github.com/mlvlab/LLaMo.', 'score': 20, 'issue_id': 439, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'd1284691dab4e739', 'authors': ['Jinyoung Park', 'Minseong Bae', 'Dohwan Ko', 'Hyunwoo J. Kim'], 'affiliations': ['Department of Computer Science and Engineering, Korea University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00871.jpg', 'data': {'categories': ['#cv', '#graphs', '#multimodal', '#training', '#transfer_learning', '#open_source', '#architecture'], 'emoji': '🧬', 'ru': {'title': 'LLaMo: Революция в понимании молекулярных структур с помощью ИИ', 'desc': 'Исследователи представили LLaMo - ассистента для работы с молекулярными графами, основанного на большой языковой модели. LLaMo использует многоуровневый графовый проектор для преобразования графовых представлений в токены, что позволяет объединить языковую и графовую модальности. Модель обучена на инструкциях, сгенерированных машинным способом, для понимания молекул и языка. Эксперименты показывают, что LLaMo превосходит существующие решения в задачах генерации описаний молекул, предсказания свойств и определения названий по IUPAC.'}, 'en': {'title': 'Bridging Language and Molecules with LLaMo', 'desc': 'This paper introduces LLaMo, a Large Language Model-based Molecular graph assistant designed to enhance understanding in the molecular domain. It utilizes a multi-level graph projector to convert molecular graph representations into tokens, facilitating better interaction between language and graph data. The model is instruction-tuned using machine-generated molecular graph instruction data, enabling it to perform various tasks like molecular description generation and property prediction. Experimental results show that LLaMo outperforms existing models in these tasks, highlighting its effectiveness in bridging language and molecular graph understanding.'}, 'zh': {'title': 'LLaMo：连接语言与分子的桥梁', 'desc': '大型语言模型（LLMs）在指令调优方面展现了出色的泛化能力和遵循指令的能力。本文提出了LLaMo，一个基于大型语言模型的分子图助手，旨在填补语言和图形模态之间的差距。我们引入了多层图投影器，通过跨注意力机制将图表示转换为图标记，并使用机器生成的分子图指令数据对模型进行指令调优。实验结果表明，LLaMo在分子描述生成、属性预测和IUPAC名称预测等多项任务中表现最佳。'}}}, {'id': 'https://huggingface.co/papers/2410.23054', 'title': 'Controlling Language and Diffusion Models by Transporting Activations', 'url': 'https://huggingface.co/papers/2410.23054', 'abstract': 'The increasing capabilities of large generative models and their ever more widespread deployment have raised concerns about their reliability, safety, and potential misuse. To address these issues, recent works have proposed to control model generation by steering model activations in order to effectively induce or prevent the emergence of concepts or behaviors in the generated output. In this paper we introduce Activation Transport (AcT), a general framework to steer activations guided by optimal transport theory that generalizes many previous activation-steering works. AcT is modality-agnostic and provides fine-grained control over the model behavior with negligible computational overhead, while minimally impacting model abilities. We experimentally show the effectiveness and versatility of our approach by addressing key challenges in large language models (LLMs) and text-to-image diffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate toxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is, we show how AcT enables fine-grained style control and concept negation.', 'score': 16, 'issue_id': 444, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '5238611006cc8b68', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#diffusion', '#hallucinations', '#cv', '#optimization', '#multimodal', '#interpretability', '#ethics', '#training', '#security', '#architecture', '#alignment'], 'emoji': '🎛️', 'ru': {'title': 'AcT: точное управление генеративными моделями через активации нейронов', 'desc': 'Эта статья представляет AcT (Activation Transport) - новый фреймворк для управления генеративными моделями, основанный на теории оптимального транспорта. AcT позволяет контролировать поведение модели путем управления активациями нейронов, что обобщает предыдущие подходы в этой области. Фреймворк применим к различным модальностям и обеспечивает точный контроль с минимальными вычислительными затратами. Эксперименты показали эффективность AcT для снижения токсичности, индукции концепций и повышения правдивости языковых моделей, а также для управления стилем и отрицания концепций в моделях генерации изображений.'}, 'en': {'title': 'Steering Generative Models with Activation Transport', 'desc': 'This paper presents Activation Transport (AcT), a new framework designed to control the behavior of large generative models by steering their activations. Using principles from optimal transport theory, AcT allows for precise manipulation of model outputs without significantly increasing computational costs. The framework is applicable across different modalities, including large language models (LLMs) and text-to-image diffusion models (T2Is). Experimental results demonstrate that AcT can reduce toxicity in LLMs, induce specific concepts, and enhance truthfulness, while also providing style control and concept negation in T2Is.'}, 'zh': {'title': '激活传输：控制生成模型的新方法', 'desc': '本文提出了一种名为激活传输（AcT）的新框架，旨在通过最优传输理论来引导模型的激活，从而控制生成模型的输出。该方法具有通用性，可以在不同的模态中应用，且对模型的计算开销影响极小。实验结果表明，AcT在大型语言模型（LLMs）中能够有效减少有害内容、引入任意概念并提高生成内容的真实性。在文本到图像的扩散模型（T2Is）中，AcT则实现了对风格的精细控制和概念的否定。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2411.02359', 'title': 'DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution', 'url': 'https://huggingface.co/papers/2411.02359', 'abstract': 'MLLMs have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we propose a Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (i.e., power consumption), as well as peak computational consumption (i.e., latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.', 'score': 12, 'issue_id': 437, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '08c45469caff5fa0', 'authors': ['Yang Yue', 'Yulin Wang', 'Bingyi Kang', 'Yizeng Han', 'Shenzhi Wang', 'Shiji Song', 'Jiashi Feng', 'Gao Huang'], 'affiliations': ['Department of Automation, BNRist, Tsinghua University', 'ByteDance'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02359.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#inference', '#agi', '#optimization', '#robotics', '#open_source', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Эффективные языковые модели для роботов: меньше ресурсов, та же мощность', 'desc': 'Статья представляет Dynamic Early-Exit Framework для робототехнических моделей зрения, языка и действия (DeeR-VLA). Эта система автоматически регулирует размер активированной мультимодальной языковой модели (MLLM) в зависимости от ситуации, используя архитектуру с множественными выходами. DeeR разработан для эффективной работы в условиях ограниченных вычислительных ресурсов роботов. На бенчмарке CALVIN система показала значительное снижение вычислительных затрат и использования памяти GPU без ущерба для производительности.'}, 'en': {'title': 'Efficient Robotic Intelligence with Dynamic Early-Exit MLLMs', 'desc': 'This paper introduces a new framework called DeeR for improving the efficiency of robotic vision-language-action models (MLLMs). DeeR uses a Dynamic Early-Exit approach that allows the model to adjust its size based on the specific task, reducing unnecessary computations. By implementing a multi-exit architecture, the model can stop processing once it has enough information, which helps save power and memory. The results show that DeeR can significantly lower computational costs and memory usage while still performing well on tasks, making it suitable for real-world robotic applications.'}, 'zh': {'title': '动态调整，智能机器人更高效！', 'desc': '本论文提出了一种动态早期退出框架（DeeR-VLA），旨在解决机器人视觉-语言-动作模型（MLLM）在实际应用中的计算和内存限制问题。该框架通过多出口架构，能够根据具体情况自动调整激活的模型大小，从而避免冗余计算。我们还开发了新算法，设定早期终止标准，以满足预定义的计算需求，如功耗和延迟。实验结果表明，DeeR在CALVIN机器人操作基准上显著降低了计算成本和GPU内存使用，同时保持了竞争力的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.02393', 'title': 'Adaptive Length Image Tokenization via Recurrent Allocation', 'url': 'https://huggingface.co/papers/2411.02393', 'abstract': 'Current vision systems typically assign fixed-length representations to images, regardless of the information content. This contrasts with human intelligence - and even large language models - which allocate varying representational capacities based on entropy, context and familiarity. Inspired by this, we propose an approach to learn variable-length token representations for 2D images. Our encoder-decoder architecture recursively processes 2D image tokens, distilling them into 1D latent tokens over multiple iterations of recurrent rollouts. Each iteration refines the 2D tokens, updates the existing 1D latent tokens, and adaptively increases representational capacity by adding new tokens. This enables compression of images into a variable number of tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction loss and FID metrics, demonstrating that token count aligns with image entropy, familiarity and downstream task requirements. Recurrent token processing with increasing representational capacity in each iteration shows signs of token specialization, revealing potential for object / part discovery.', 'score': 11, 'issue_id': 448, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '1c3553b38b491652', 'authors': ['Shivam Duggal', 'Phillip Isola', 'Antonio Torralba', 'William T. Freeman'], 'affiliations': ['MIT CSAIL'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02393.jpg', 'data': {'categories': ['#optimization', '#architecture', '#interpretability', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Адаптивное токенизированное представление изображений', 'desc': 'Статья предлагает новый подход к представлению изображений с помощью токенов переменной длины. Авторы разработали архитектуру энкодер-декодер, которая рекурсивно обрабатывает 2D-токены изображения, преобразуя их в 1D-латентные токены. Процесс включает несколько итераций, на каждой из которых уточняются 2D-токены, обновляются существующие 1D-токены и адаптивно увеличивается емкость представления путем добавления новых токенов. Эксперименты показали, что количество токенов соответствует энтропии изображения и требованиям downstream-задач.'}, 'en': {'title': 'Adaptive Tokenization for Enhanced Image Representation', 'desc': "This paper introduces a novel method for creating variable-length token representations of images, which contrasts with traditional fixed-length approaches. The proposed encoder-decoder architecture processes 2D image tokens recursively, refining them into 1D latent tokens through multiple iterations. Each iteration not only updates the existing tokens but also allows for the addition of new tokens, enabling a flexible representation that adapts to the complexity of the image. The effectiveness of this method is validated through reconstruction loss and FID metrics, showing that the number of tokens correlates with the image's information content and can enhance downstream tasks."}, 'zh': {'title': '可变长度标记表示：提升图像理解能力', 'desc': '当前的视觉系统通常为图像分配固定长度的表示，这与人类智能和大型语言模型的动态表示能力形成对比。我们提出了一种方法，通过编码器-解码器架构学习二维图像的可变长度标记表示。该方法通过递归处理二维图像标记，将其提炼为一维潜在标记，并在多个迭代中逐步更新和增加表示能力。我们的实验表明，标记数量与图像的熵、熟悉度和下游任务需求相一致，显示出标记的专业化潜力。'}}}, {'id': 'https://huggingface.co/papers/2411.01493', 'title': 'Sample-Efficient Alignment for LLMs', 'url': 'https://huggingface.co/papers/2411.01493', 'abstract': "We study methods for efficiently aligning large language models (LLMs) with human preferences given budgeted online feedback. We first formulate the LLM alignment problem in the frame of contextual dueling bandits. This formulation, subsuming recent paradigms such as online RLHF and online DPO, inherently quests for sample-efficient algorithms that incorporate online active exploration. Leveraging insights from bandit theory, we introduce a unified algorithm based on Thompson sampling and highlight its applications in two distinct LLM alignment scenarios. The practical agent that efficiently implements this algorithm, named SEA (Sample-Efficient Alignment), is empirically validated through extensive experiments across three model scales (1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The results demonstrate that SEA achieves highly sample-efficient alignment with oracle's preferences, outperforming recent active exploration methods for LLMs. Additionally, we release the implementation of SEA together with an efficient codebase designed for online alignment of LLMs, aiming to accelerate future research in this field.", 'score': 10, 'issue_id': 440, 'pub_date': '2024-11-03', 'pub_date_card': {'ru': '3 ноября', 'en': 'November 3', 'zh': '11月3日'}, 'hash': 'c5bb9727a6ba6119', 'authors': ['Zichen Liu', 'Changyu Chen', 'Chao Du', 'Wee Sun Lee', 'Min Lin'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'Singapore Management University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01493.jpg', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#optimization', '#training', '#open_source', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Эффективное согласование языковых моделей с помощью контекстных дуэльных бандитов', 'desc': 'Исследователи изучают методы эффективного согласования больших языковых моделей (LLM) с человеческими предпочтениями при ограниченной онлайн-обратной связи. Они формулируют проблему согласования LLM в рамках контекстных дуэльных бандитов и предлагают унифицированный алгоритм на основе выборки Томпсона. Практический агент SEA (Sample-Efficient Alignment), реализующий этот алгоритм, показывает высокую эффективность выборки при согласовании с предпочтениями оракула. Исследователи также выпускают реализацию SEA вместе с эффективной кодовой базой для онлайн-согласования LLM.'}, 'en': {'title': 'Efficiently Aligning LLMs with Human Preferences Using SEA', 'desc': 'This paper explores how to align large language models (LLMs) with human preferences using limited online feedback. It frames the alignment challenge as a contextual dueling bandits problem, which seeks efficient algorithms that can learn from active exploration. The authors propose a new algorithm called SEA (Sample-Efficient Alignment) based on Thompson sampling, which is tested across various model sizes and preference learning methods. The results show that SEA is highly effective in aligning LLMs with human preferences while using fewer samples compared to existing methods.'}, 'zh': {'title': '高效对齐大型语言模型与人类偏好', 'desc': '本文研究了如何在有限的在线反馈下高效地将大型语言模型（LLMs）与人类偏好对齐。我们将LLM对齐问题框定为上下文对抗赌博者问题，提出了一种基于汤普森采样的统一算法，并在两个不同的LLM对齐场景中进行了应用。通过大量实验验证，名为SEA（样本高效对齐）的实用代理在不同规模的模型和偏好学习算法中表现出色，显示出其在对齐方面的高样本效率。我们还发布了SEA的实现和高效代码库，以促进该领域未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2411.01602', 'title': 'DreamPolish: Domain Score Distillation With Progressive Geometry Generation', 'url': 'https://huggingface.co/papers/2411.01602', 'abstract': 'We introduce DreamPolish, a text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on a view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add a surface polishing stage with only a few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find a suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce a novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such a domain. We draw inspiration from the classifier-free guidance (CFG) in textconditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods.', 'score': 9, 'issue_id': 439, 'pub_date': '2024-11-03', 'pub_date_card': {'ru': '3 ноября', 'en': 'November 3', 'zh': '11月3日'}, 'hash': '403fd08e60540a0a', 'authors': ['Yean Cheng', 'Ziqi Cai', 'Ming Ding', 'Wendi Zheng', 'Shiyu Huang', 'Yuxiao Dong', 'Jie Tang', 'Boxin Shi'], 'affiliations': ['Zhipu AI', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01602.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#3d', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'DreamPolish: Революция в генерации 3D-объектов с идеальной геометрией и текстурами', 'desc': 'DreamPolish - это модель генерации 3D-объектов из текста, которая превосходит существующие методы в создании утонченной геометрии и высококачественных текстур. Модель использует множественные нейронные представления и дополнительный оценщик нормалей для улучшения стабильности синтеза геометрии. Для генерации текстур авторы предлагают новый метод дистилляции оценок в определенном домене (DSD), используя предобученные модели text-to-image. Эксперименты показывают, что DreamPolish создает 3D-объекты с отполированными поверхностями и фотореалистичными текстурами, превосходя современные методы.'}, 'en': {'title': 'DreamPolish: Elevating 3D Generation with Refined Geometry and Textures', 'desc': 'DreamPolish is a text-to-3D generation model that focuses on creating high-quality 3D objects with refined geometry and textures. It improves the geometry construction by using multiple neural representations and an additional normal estimator to reduce artifacts caused by limited guidance. The model also introduces a surface polishing stage that requires minimal training to enhance the geometric details further. For texture generation, it employs a novel domain score distillation (DSD) method, inspired by classifier-free guidance, to achieve photorealistic and consistent textures in the generated 3D assets.'}, 'zh': {'title': 'DreamPolish：生成高质量3D资产的创新模型', 'desc': '本文介绍了一种名为DreamPolish的文本到3D生成模型，能够生成精细的几何形状和高质量的纹理。在几何构建阶段，我们利用多种神经表示来增强合成过程的稳定性，并引入额外的法线估计器来改善几何细节。纹理生成阶段采用了一种新的评分蒸馏目标，称为领域评分蒸馏（DSD），以引导神经表示朝向包含真实感和一致性渲染的适当领域。实验表明，我们的模型能够生成表面光滑且具有真实感纹理的3D资产，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2411.03312', 'title': 'Inference Optimal VLMs Need Only One Visual Token but Larger Models', 'url': 'https://huggingface.co/papers/2411.03312', 'abstract': 'Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5-10times), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.', 'score': 6, 'issue_id': 446, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '60954fb0c9d4b5fb', 'authors': ['Kevin Y. Li', 'Sachin Goyal', 'Joao D. Semedo', 'J. Zico Kolter'], 'affiliations': ['Carnegie Mellon University', 'Bosch Center for Artificial Intelligence'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03312.jpg', 'data': {'categories': ['#reasoning', '#cv', '#inference', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Меньше токенов, больше модель: новый подход к оптимизации VLM', 'desc': 'Эта статья исследует оптимальный баланс между количеством визуальных токенов и параметрами языковой модели (LLM) в vision-language моделях (VLM). Авторы установили закономерности масштабирования, которые показывают, как производительность VLM меняется в зависимости от этих двух факторов. Результаты демонстрируют, что для задач визуального рассуждения оптимальная производительность достигается при использовании самой большой LLM, которая помещается в бюджет вычислений, при минимизации количества визуальных токенов. Исследование предлагает новый взгляд на оптимизацию VLM для эффективного вывода.'}, 'en': {'title': 'Maximizing VLM Efficiency with Token Compression', 'desc': 'This paper explores the efficiency of Vision Language Models (VLMs) in visual understanding tasks, focusing on the trade-off between the number of visual tokens and the size of the language model (LLM). The authors establish scaling laws to determine how performance varies with these two factors, revealing that optimal performance is achieved with minimal visual tokens, often reducing to just one token. They highlight that traditional token reduction methods may not be sufficient for achieving the best inference performance, suggesting that higher compression ratios are necessary. The paper also proposes initial strategies for adapting VLMs to work effectively under these high token compression conditions.'}, 'zh': {'title': '优化推理：最大化LLM与最小化视觉标记的平衡', 'desc': '视觉语言模型（VLMs）在视觉理解和推理任务中表现出色，但在实际应用中，由于处理大量输入标记（主要来自图像）所需的计算量大，推理延迟较高。为了降低推理成本，可以选择缩小大型语言模型（LLM）或减少输入图像标记的数量，后者是许多近期研究的重点。我们通过建立缩放法则来描述视觉标记数量与LLM参数之间的最佳权衡，发现对于视觉推理任务，最佳推理行为是在推理预算内使用最大的LLM，同时将视觉标记数量减少到最小，通常只需一个标记。我们的研究表明，计算最优的推理模式需要在更高的标记压缩比下进行操作。'}}}, {'id': 'https://huggingface.co/papers/2411.03047', 'title': 'GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details', 'url': 'https://huggingface.co/papers/2411.03047', 'abstract': 'Neural implicit functions have brought impressive advances to the state-of-the-art of clothed human digitization from multiple or even single images. However, despite the progress, current arts still have difficulty generalizing to unseen images with complex cloth deformation and body poses. In this work, we present GarVerseLOD, a new dataset and framework that paves the way to achieving unprecedented robustness in high-fidelity 3D garment reconstruction from a single unconstrained image. Inspired by the recent success of large generative models, we believe that one key to addressing the generalization challenge lies in the quantity and quality of 3D garment data. Towards this end, GarVerseLOD collects 6,000 high-quality cloth models with fine-grained geometry details manually created by professional artists. In addition to the scale of training data, we observe that having disentangled granularities of geometry can play an important role in boosting the generalization capability and inference accuracy of the learned model. We hence craft GarVerseLOD as a hierarchical dataset with levels of details (LOD), spanning from detail-free stylized shape to pose-blended garment with pixel-aligned details. This allows us to make this highly under-constrained problem tractable by factorizing the inference into easier tasks, each narrowed down with smaller searching space. To ensure GarVerseLOD can generalize well to in-the-wild images, we propose a novel labeling paradigm based on conditional diffusion models to generate extensive paired images for each garment model with high photorealism. We evaluate our method on a massive amount of in-the-wild images. Experimental results demonstrate that GarVerseLOD can generate standalone garment pieces with significantly better quality than prior approaches. Project page: https://garverselod.github.io/', 'score': 6, 'issue_id': 444, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': 'eadbb8f8b93d4818', 'authors': ['Zhongjin Luo', 'Haolin Liu', 'Chenghong Li', 'Wanghao Du', 'Zirong Jin', 'Wanhu Sun', 'Yinyu Nie', 'Weikai Chen', 'Xiaoguang Han'], 'affiliations': ['SSE, CUHKSZ, China', 'FNii, CUHKSZ, China', 'Huawei Noahs Ark Lab, UK', 'DCC Algorithm Research Center, Tencent Games, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03047.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#benchmark', '#inference', '#graphs', '#data', '#training', '#dataset', '#open_source', '#3d'], 'emoji': '👚', 'ru': {'title': 'GarVerseLOD: Революция в 3D-реконструкции одежды по одному изображению', 'desc': 'GarVerseLOD - это новый набор данных и фреймворк для реконструкции 3D-одежды по одному изображению. Он включает 6000 высококачественных моделей одежды с детализированной геометрией, созданных профессиональными художниками. Набор данных имеет иерархическую структуру с уровнями детализации, от стилизованных форм до одежды с учетом позы и пиксельно-точными деталями. Для генерации реалистичных парных изображений для каждой модели одежды используется условная диффузионная модель.'}, 'en': {'title': 'Revolutionizing 3D Garment Reconstruction with GarVerseLOD', 'desc': "This paper introduces GarVerseLOD, a new dataset and framework aimed at improving 3D garment reconstruction from single images. The authors highlight the challenges faced by existing methods in generalizing to unseen images with complex clothing and poses. By collecting 6,000 high-quality cloth models and organizing them into a hierarchical dataset with varying levels of detail, they enhance the model's ability to learn and generalize. Additionally, they employ a novel labeling approach using conditional diffusion models to create realistic paired images, resulting in superior garment reconstruction quality compared to previous methods."}, 'zh': {'title': '高保真3D服装重建的新突破', 'desc': '神经隐式函数在从多张或单张图像中数字化穿衣人类方面取得了显著进展。然而，当前的方法在处理复杂的布料变形和身体姿势的未见图像时仍然面临挑战。为了解决这一问题，我们提出了GarVerseLOD，一个新的数据集和框架，旨在从单张无约束图像中实现高保真度的3D服装重建。通过收集6000个高质量的布料模型，并采用分层细节的方式，我们的研究显著提高了模型的泛化能力和推理准确性。'}}}, {'id': 'https://huggingface.co/papers/2411.02657', 'title': 'Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge', 'url': 'https://huggingface.co/papers/2411.02657', 'abstract': "Rare diseases present unique challenges in healthcare, often suffering from delayed diagnosis and fragmented information landscapes. The scarcity of reliable knowledge in these conditions poses a distinct challenge for Large Language Models (LLMs) in supporting clinical management and delivering precise patient information underscoring the need for focused training on these 'zebra' cases. We present Zebra-Llama, a specialized context-aware language model with high precision Retrieval Augmented Generation (RAG) capability, focusing on Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000 individuals, exemplifies the complexities of rare diseases with its diverse symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing a novel context-aware fine-tuning methodology trained on questions derived from medical literature, patient experiences, and clinical resources, along with expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities in handling EDS-related queries. On a test set of real-world questions collected from EDS patients and clinicians, medical experts evaluated the responses generated by both models, revealing Zebra-Llama's substantial improvements over base model (Llama 3.1-8B-Instruct) in thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama not only provides more accessible and reliable EDS information but also establishes a framework for developing specialized AI solutions for other rare conditions. This work represents a crucial step towards democratizing expert-level knowledge in rare disease management, potentially transforming how healthcare providers and patients navigate the complex landscape of rare diseases.", 'score': 5, 'issue_id': 439, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '26c0b7bc39488945', 'authors': ['Karthik Soman', 'Andrew Langdon', 'Catalina Villouta', 'Chinmay Agrawal', 'Lashaw Salta', 'Braian Peetoom', 'Gianmarco Bellucci', 'Orion J Buske'], 'affiliations': ['Dept. of Neurology, University of California San Francisco, California, USA', 'Independent Researcher', 'Plix AI, California, USA', 'Dept. of Neurosciences, Mental Health and Sensory Organs, Sapienza University of Rome, Italy', 'PhenoTips, Toronto, Canada'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02657.jpg', 'data': {'categories': ['#science', '#rag', '#healthcare', '#training', '#open_source'], 'emoji': '🦓', 'ru': {'title': 'Zebra-Llama: ИИ-эксперт по редким заболеваниям', 'desc': 'Статья представляет Zebra-Llama - специализированную языковую модель для редких заболеваний, используя синдром Элерса-Данлоса (EDS) как пример. Модель использует контекстно-зависимую настройку и усиленную генерацию с извлечением (RAG) для обработки запросов, связанных с EDS. Zebra-Llama показала значительные улучшения по сравнению с базовой моделью в полноте, точности, ясности и надежности цитирования при ответе на реальные вопросы пациентов и врачей. Эта работа открывает путь к созданию специализированных ИИ-решений для других редких заболеваний, демократизируя экспертные знания в этой области.'}, 'en': {'title': 'Zebra-Llama: Transforming Rare Disease Management with AI', 'desc': 'This paper introduces Zebra-Llama, a specialized language model designed to improve the management of rare diseases, specifically Ehlers-Danlos Syndrome (EDS). It addresses the challenges posed by limited information and delayed diagnoses in rare conditions by utilizing a context-aware fine-tuning approach. The model employs Retrieval Augmented Generation (RAG) to enhance the precision of responses to EDS-related queries, outperforming the base model in various evaluation metrics. By making Zebra-Llama an open-source resource, the authors aim to democratize access to expert knowledge in rare disease management, paving the way for similar advancements in other rare conditions.'}, 'zh': {'title': 'Zebra-Llama：罕见疾病管理的新突破', 'desc': '罕见疾病在医疗保健中面临独特挑战，常常导致诊断延迟和信息碎片化。针对这些罕见病例，本文提出了一种名为Zebra-Llama的专门语言模型，具备高精度的检索增强生成能力，重点关注Ehlers-Danlos综合症（EDS）。通过一种新颖的上下文感知微调方法，Zebra-Llama在处理与EDS相关的问题时表现出前所未有的能力，显著提高了回答的全面性、准确性和清晰度。该模型作为开源资源发布，不仅提供了更易获取和可靠的EDS信息，还为其他罕见疾病开发专门的人工智能解决方案奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2411.02844', 'title': 'Correlation of Object Detection Performance with Visual Saliency and Depth Estimation', 'url': 'https://huggingface.co/papers/2411.02844', 'abstract': "As object detection techniques continue to evolve, understanding their relationships with complementary visual tasks becomes crucial for optimising model architectures and computational resources. This paper investigates the correlations between object detection accuracy and two fundamental visual tasks: depth prediction and visual saliency prediction. Through comprehensive experiments using state-of-the-art models (DeepGaze IIE, Depth Anything, DPT-Large, and Itti's model) on COCO and Pascal VOC datasets, we find that visual saliency shows consistently stronger correlations with object detection accuracy (mArho up to 0.459 on Pascal VOC) compared to depth prediction (mArho up to 0.283). Our analysis reveals significant variations in these correlations across object categories, with larger objects showing correlation values up to three times higher than smaller objects. These findings suggest incorporating visual saliency features into object detection architectures could be more beneficial than depth information, particularly for specific object categories. The observed category-specific variations also provide insights for targeted feature engineering and dataset design improvements, potentially leading to more efficient and accurate object detection systems.", 'score': 3, 'issue_id': 444, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': 'd17ec72bcba1cd05', 'authors': ['Matthias Bartolo', 'Dylan Seychell'], 'affiliations': ['Dept. of Artificial Intelligence University of Malta'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02844.jpg', 'data': {'categories': ['#cv', '#graphs', '#optimization', '#dataset', '#architecture'], 'emoji': '👁️', 'ru': {'title': 'Визуальная значимость превосходит глубину в улучшении обнаружения объектов', 'desc': 'Исследование изучает связь между точностью обнаружения объектов и задачами предсказания глубины и визуальной значимости. Эксперименты с современными моделями на наборах данных COCO и Pascal VOC показали, что визуальная значимость имеет более сильную корреляцию с точностью обнаружения объектов по сравнению с предсказанием глубины. Обнаружены значительные различия в корреляциях между категориями объектов, причем для более крупных объектов корреляция до трех раз выше. Результаты предполагают, что включение признаков визуальной значимости в архитектуры обнаружения объектов может быть более полезным, чем информация о глубине.'}, 'en': {'title': 'Enhancing Object Detection with Visual Saliency Insights', 'desc': 'This paper explores how object detection accuracy relates to two other visual tasks: depth prediction and visual saliency prediction. The authors conducted experiments with advanced models on popular datasets and found that visual saliency has a stronger correlation with object detection accuracy than depth prediction. They noted that larger objects tend to show much higher correlation values compared to smaller ones. The results suggest that integrating visual saliency features into object detection models could enhance performance, especially for certain object categories.'}, 'zh': {'title': '视觉显著性助力物体检测精度提升', 'desc': '本文研究了物体检测精度与深度预测和视觉显著性预测这两种基本视觉任务之间的关系。通过在COCO和Pascal VOC数据集上使用先进模型进行全面实验，发现视觉显著性与物体检测精度的相关性明显高于深度预测。研究还表明，不同物体类别之间的相关性存在显著差异，大型物体的相关性值可高达小型物体的三倍。结果表明，将视觉显著性特征融入物体检测架构可能比深度信息更有利，尤其是在特定物体类别中。'}}}, {'id': 'https://huggingface.co/papers/2411.11844', 'title': 'Generative World Explorer', 'url': 'https://huggingface.co/papers/2411.11844', 'abstract': 'Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state.In contrast, humans can imagine unseen parts of the world through a mental exploration and revise their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions, without necessitating the physical exploration of the world at all times. To achieve this human-like ability, we introduce the Generative World Explorer (Genex), an egocentric world exploration framework that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train Genex, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) Genex can generate high-quality and consistent observations during long-horizon exploration of a large virtual physical world and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.', 'score': 34, 'issue_id': 655, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 ноября', 'en': 'November 18', 'zh': '11月18日'}, 'hash': 'ad359ab18e626959', 'authors': ['Taiming Lu', 'Tianmin Shu', 'Alan Yuille', 'Daniel Khashabi', 'Jieneng Chen'], 'affiliations': ['Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2411.11844.jpg', 'data': {'categories': ['#agents', '#3d', '#synthetic', '#games', '#dataset', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Мысленное исследование мира для улучшения решений', 'desc': 'В статье рассматривается проблема планирования с частичным наблюдением в embodied AI. Авторы предлагают подход, который позволяет агенту мысленно исследовать 3D мир и обновлять свои представления о нём без физического взаимодействия. Для этого они разработали систему Generative World Explorer (Genex), которая использует синтетические данные для обучения. Эксперименты показали, что Genex может генерировать качественные наблюдения, которые помогают агенту принимать более обоснованные решения.'}, 'en': {'title': 'Imagine to Explore: Enhancing Decision-Making with Mental Simulations', 'desc': 'This paper addresses the challenge of planning in environments where an agent has incomplete information. Unlike traditional methods that rely on physical exploration, the authors propose a framework called Generative World Explorer (Genex) that enables agents to mentally simulate and explore their surroundings. By generating imagined observations, Genex allows agents to update their beliefs about the world without needing to physically navigate it. The results show that this approach leads to improved decision-making in complex environments, demonstrating the potential of mental exploration in embodied AI.'}, 'zh': {'title': '心理探索，智能决策的新方式', 'desc': '在具身人工智能中，部分观察的规划是一个重要挑战。大多数研究通过让智能体物理探索环境来更新对世界状态的信念，而我们提出的生成世界探索器（Genex）则允许智能体通过心理探索来想象未见的世界部分。Genex能够在大型3D世界中生成想象的观察，从而更新信念，帮助智能体在当前步骤做出更明智的决策。我们创建了一个合成城市场景数据集Genex-DB，并通过实验验证了Genex在长时间探索中的高质量观察生成能力。'}}}, {'id': 'https://huggingface.co/papers/2411.10640', 'title': 'BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices', 'url': 'https://huggingface.co/papers/2411.10640', 'abstract': 'The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with leq 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B).', 'score': 29, 'issue_id': 652, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 ноября', 'en': 'November 16', 'zh': '11月16日'}, 'hash': '0366549d4347dfd2', 'authors': ['Xudong Lu', 'Yinghao Chen', 'Cheng Chen', 'Hui Tan', 'Boheng Chen', 'Yina Xie', 'Rui Hu', 'Guanxin Tan', 'Renshou Wu', 'Yan Hu', 'Yi Zeng', 'Lei Wu', 'Liuyang Bian', 'Zhaoxiong Wang', 'Long Liu', 'Yanzhou Yang', 'Han Xiao', 'Aojun Zhou', 'Yafei Wen', 'Xiaoxin Chen', 'Shuai Ren', 'Hongsheng Li'], 'affiliations': ['CUHK MMLab', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.10640.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#small_models', '#multimodal', '#inference', '#optimization'], 'emoji': '📱', 'ru': {'title': 'BlueLM-V-3B: Мощь больших языковых моделей в вашем кармане', 'desc': 'BlueLM-V-3B - это новый подход к развертыванию мультимодальных больших языковых моделей (MLLM) на мобильных устройствах. Он решает проблемы ограниченной памяти и вычислительной мощности смартфонов, оптимизируя алгоритм и систему для эффективной работы. Модель имеет компактный размер (3,1 млрд параметров), высокую скорость генерации (24,4 токена/с) и превосходную производительность по сравнению с более крупными моделями. BlueLM-V-3B демонстрирует потенциал для интеграции продвинутых ИИ-технологий в повседневную жизнь через мобильные устройства.'}, 'en': {'title': 'Efficient MLLMs for Mobile: BlueLM-V-3B Unleashed!', 'desc': 'This paper introduces BlueLM-V-3B, a multimodal large language model (MLLM) designed for efficient deployment on mobile devices. It addresses the challenges of limited memory and computational power by optimizing model inference through a redesigned dynamic resolution scheme and hardware-aware system optimizations. BlueLM-V-3B features a compact architecture with 2.7 billion parameters for the language model and 400 million for the vision encoder, ensuring fast processing speeds. The model achieves impressive performance metrics, outperforming larger models on benchmarks while maintaining a generation speed of 24.4 tokens per second.'}, 'zh': {'title': '高效部署多模态大语言模型的创新方案', 'desc': '这篇论文介绍了一种名为BlueLM-V-3B的多模态大语言模型（MLLM），旨在高效地在移动平台上部署。该模型具有2.7亿参数的语言模型和4亿参数的视觉编码器，能够在移动设备上实现快速生成。通过重新设计动态分辨率方案和进行硬件优化，BlueLM-V-3B在MediaTek Dimensity 9300处理器上达到了每秒24.4个标记的生成速度。该模型在OpenCompass基准测试中获得了66.1的最高平均分，超越了许多参数更大的模型。'}}}, {'id': 'https://huggingface.co/papers/2411.11504', 'title': 'Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering', 'url': 'https://huggingface.co/papers/2411.11504', 'abstract': 'The evolution of machine learning has increasingly prioritized the development of powerful models and more scalable supervision signals. However, the emergence of foundation models presents significant challenges in providing effective supervision signals necessary for further enhancing their capabilities. Consequently, there is an urgent need to explore novel supervision signals and technical approaches. In this paper, we propose verifier engineering, a novel post-training paradigm specifically designed for the era of foundation models. The core of verifier engineering involves leveraging a suite of automated verifiers to perform verification tasks and deliver meaningful feedback to foundation models. We systematically categorize the verifier engineering process into three essential stages: search, verify, and feedback, and provide a comprehensive review of state-of-the-art research developments within each stage. We believe that verifier engineering constitutes a fundamental pathway toward achieving Artificial General Intelligence.', 'score': 12, 'issue_id': 654, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 ноября', 'en': 'November 18', 'zh': '11月18日'}, 'hash': 'a48ecb5e8a1da0ae', 'authors': ['Xinyan Guan', 'Yanjiang Liu', 'Xinyu Lu', 'Boxi Cao', 'Ben He', 'Xianpei Han', 'Le Sun', 'Jie Lou', 'Bowen Yu', 'Yaojie Lu', 'Hongyu Lin'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2411.11504.jpg', 'data': {'categories': ['#rlhf', '#survey', '#training', '#agi'], 'emoji': '🔍', 'ru': {'title': 'Верификационная инженерия: новый путь к совершенствованию ИИ', 'desc': "В статье представлена концепция 'верификационной инженерии' - новой парадигмы для улучшения фундаментальных моделей машинного обучения. Авторы предлагают использовать автоматизированные верификаторы для проверки и обратной связи с моделями. Процесс разделен на три этапа: поиск, верификация и обратная связь. Исследователи считают, что этот подход может стать ключевым на пути к созданию искусственного общего интеллекта."}, 'en': {'title': 'Unlocking Foundation Models with Verifier Engineering', 'desc': 'This paper discusses the challenges of providing effective supervision signals for foundation models in machine learning. It introduces a new approach called verifier engineering, which uses automated verifiers to enhance the capabilities of these models. The process is divided into three stages: search, verify, and feedback, each aimed at improving model performance. The authors argue that this method is crucial for advancing towards Artificial General Intelligence.'}, 'zh': {'title': '验证器工程：迈向人工通用智能的新路径', 'desc': '本论文探讨了在基础模型时代，如何提供有效的监督信号以提升模型能力。我们提出了一种新的后训练范式——验证器工程，旨在利用自动化验证器进行验证任务，并为基础模型提供有意义的反馈。验证器工程的过程分为三个关键阶段：搜索、验证和反馈，并对每个阶段的最新研究进展进行了系统性回顾。我们认为，验证器工程是实现人工通用智能的重要途径。'}}}, {'id': 'https://huggingface.co/papers/2411.09944', 'title': 'SlimLM: An Efficient Small Language Model for On-Device Document Assistance', 'url': 'https://huggingface.co/papers/2411.09944', 'abstract': 'While small language models (SLMs) show promises for mobile deployment, their real-world performance and applications on smartphones remains underexplored. We present SlimLM, a series of SLMs optimized for document assistance tasks on mobile devices. Through extensive experiments on a Samsung Galaxy S24, we identify the optimal trade-offs between model size (ranging from 125M to 7B parameters), context length, and inference time for efficient on-device processing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on DocAssist, our constructed dataset for summarization, question answering and suggestion tasks. Our smallest model demonstrates efficient performance on S24, while larger variants offer enhanced capabilities within mobile constraints. We evaluate SlimLM against existing SLMs, showing comparable or superior performance and offering a benchmark for future research in on-device language models. We also provide an Android application, offering practical insights into SLM deployment. Our findings provide valuable insights and illuminate the capabilities of running advanced language models on high-end smartphones, potentially reducing server costs and enhancing privacy through on-device processing.', 'score': 10, 'issue_id': 655, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': 'ec53cf3813914219', 'authors': ['Thang M. Pham', 'Phat T. Nguyen', 'Seunghyun Yoon', 'Viet Dac Lai', 'Franck Dernoncourt', 'Trung Bui'], 'affiliations': ['Adobe Research', 'Auburn University', 'Georgia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2411.09944.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#small_models', '#open_source', '#dataset', '#benchmark'], 'emoji': '📱', 'ru': {'title': 'Эффективные языковые модели прямо в вашем кармане', 'desc': 'SlimLM - это серия малых языковых моделей, оптимизированных для задач помощи с документами на мобильных устройствах. Модели варьируются от 125 миллионов до 7 миллиардов параметров и оценивались на Samsung Galaxy S24. SlimLM предобучена на датасете SlimPajama-627B и дообучена на специально созданном наборе данных DocAssist для задач суммаризации, вопросно-ответных систем и генерации предложений. Исследование демонстрирует возможности запуска продвинутых языковых моделей на современных смартфонах, что потенциально снижает серверные затраты и повышает приватность за счет обработки на устройстве.'}, 'en': {'title': 'SlimLM: Efficient Language Models for Mobile Document Assistance', 'desc': 'This paper introduces SlimLM, a series of small language models designed specifically for mobile devices, focusing on document assistance tasks. The models are optimized for performance on smartphones, balancing size, context length, and inference time to ensure efficient on-device processing. SlimLM is pre-trained on a large dataset and fine-tuned for tasks like summarization and question answering, demonstrating effective performance even with the smallest model. The research highlights the potential of deploying advanced language models on smartphones, which can lower server costs and improve user privacy.'}, 'zh': {'title': 'SlimLM：移动设备上的高效语言模型', 'desc': '本论文介绍了一种名为SlimLM的小型语言模型系列，专为移动设备上的文档辅助任务优化。我们在三星Galaxy S24上进行了广泛实验，找到了模型大小、上下文长度和推理时间之间的最佳平衡，以实现高效的本地处理。SlimLM在SlimPajama-627B上进行预训练，并在我们构建的DocAssist数据集上进行微调，支持摘要、问答和建议等任务。我们的研究表明，SlimLM在移动设备上表现出色，能够降低服务器成本并增强隐私保护。'}}}, {'id': 'https://huggingface.co/papers/2411.10836', 'title': 'AnimateAnything: Consistent and Controllable Animation for Video Generation', 'url': 'https://huggingface.co/papers/2411.10836', 'abstract': "We present a unified controllable video generation approach AnimateAnything that facilitates precise and consistent video manipulation across various conditions, including camera trajectories, text prompts, and user motion annotations. Specifically, we carefully design a multi-scale control feature fusion network to construct a common motion representation for different conditions. It explicitly converts all control information into frame-by-frame optical flows. Then we incorporate the optical flows as motion priors to guide final video generation. In addition, to reduce the flickering issues caused by large-scale motion, we propose a frequency-based stabilization module. It can enhance temporal coherence by ensuring the video's frequency domain consistency. Experiments demonstrate that our method outperforms the state-of-the-art approaches. For more details and videos, please refer to the webpage: https://yu-shaonian.github.io/Animate_Anything/.", 'score': 10, 'issue_id': 655, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 ноября', 'en': 'November 16', 'zh': '11月16日'}, 'hash': 'b979f7de4cf79a50', 'authors': ['Guojun Lei', 'Chi Wang', 'Hong Li', 'Rong Zhang', 'Yikai Wang', 'Weiwei Xu'], 'affiliations': ['Beihang University', 'State Key Lab of CAD&CG, Zhejiang University', 'Tsinghua University', 'Zhejiang Gongshang University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10836.jpg', 'data': {'categories': ['#video'], 'emoji': '🎬', 'ru': {'title': 'Универсальная система для гибкой и точной генерации видео', 'desc': 'AnimateAnything - это унифицированный подход к управляемой генерации видео, позволяющий точно и последовательно манипулировать видео в различных условиях, включая траектории камеры, текстовые подсказки и пользовательские аннотации движения. Авторы разработали многомасштабную сеть слияния управляющих признаков для создания общего представления движения для различных условий. Метод преобразует всю управляющую информацию в покадровые оптические потоки и использует их в качестве априорной информации о движении для управления финальной генерацией видео. Для уменьшения мерцания при масштабных движениях предложен модуль стабилизации на основе частот, обеспечивающий согласованность видео в частотной области.'}, 'en': {'title': 'AnimateAnything: Mastering Video Generation with Precision Control', 'desc': 'The paper introduces AnimateAnything, a method for generating videos that allows for detailed control over various aspects like camera movement and user inputs. It uses a multi-scale control feature fusion network to create a unified motion representation that can adapt to different conditions. By converting control information into optical flows, the method guides the video generation process effectively. Additionally, a frequency-based stabilization module is implemented to minimize flickering and improve the smoothness of the final video output.'}, 'zh': {'title': '统一可控视频生成，精准操控每一帧', 'desc': '我们提出了一种统一的可控视频生成方法AnimateAnything，能够在不同条件下实现精确和一致的视频操控，包括相机轨迹、文本提示和用户运动注释。该方法设计了一个多尺度控制特征融合网络，以构建不同条件下的共同运动表示。它将所有控制信息显式转换为逐帧的光流，并将光流作为运动先验来指导最终的视频生成。此外，我们还提出了一种基于频率的稳定模块，以减少大规模运动引起的闪烁问题，从而增强视频的时间一致性。'}}}, {'id': 'https://huggingface.co/papers/2411.07641', 'title': 'Top-$nσ$: Not All Logits Are You Need', 'url': 'https://huggingface.co/papers/2411.07641', 'abstract': 'Large language models (LLMs) typically employ greedy decoding or low-temperature sampling for reasoning tasks, reflecting a perceived trade-off between diversity and accuracy. We challenge this convention by introducing top-nsigma, a novel sampling method that operates directly on pre-softmax logits by leveraging a statistical threshold. Our key insight is that logits naturally separate into a Gaussian-distributed noisy region and a distinct informative region, enabling efficient token filtering without complex probability manipulations. Unlike existing methods (e.g., top-p, min-p) that inadvertently include more noise tokens at higher temperatures, top-nsigma maintains a stable sampling space regardless of temperature scaling. We also provide a theoretical analysis of top-nsigma to better understand its behavior. The extensive experimental results across four reasoning-focused datasets demonstrate that our method not only outperforms existing sampling approaches but also surpasses greedy decoding, while maintaining consistent performance even at high temperatures.', 'score': 10, 'issue_id': 651, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': 'd3439bf0c336ac57', 'authors': ['Chenxia Tang', 'Jianchun Liu', 'Hongli Xu', 'Liusheng Huang'], 'affiliations': ['School of Computer Science and Technology, University of Science and Technology of China', 'Suzhou Institute for Advanced Research, University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2411.07641.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#reasoning'], 'emoji': '🎯', 'ru': {'title': 'Top-nsigma: эффективное сэмплирование для улучшения рассуждений языковых моделей', 'desc': 'В статье представлен новый метод сэмплирования для больших языковых моделей под названием top-nsigma. Этот метод работает напрямую с логитами перед софтмаксом, используя статистический порог для разделения шумовых и информативных токенов. Top-nsigma позволяет эффективно фильтровать токены без сложных манипуляций с вероятностями и сохраняет стабильное пространство сэмплирования независимо от температуры. Экспериментальные результаты показывают, что данный метод превосходит существующие подходы к сэмплированию и жадное декодирование на задачах рассуждения.'}, 'en': {'title': 'Top-NSigma: Enhancing Sampling for Better Reasoning in LLMs', 'desc': 'This paper introduces a new sampling method called top-nsigma for large language models (LLMs) that improves reasoning tasks. Unlike traditional methods that use greedy decoding or low-temperature sampling, top-nsigma filters tokens based on their pre-softmax logits, which are divided into informative and noisy regions. This approach allows for better token selection without the complications of probability adjustments, ensuring a stable sampling process across different temperature settings. Experimental results show that top-nsigma outperforms existing methods and maintains high performance even when using higher temperatures.'}, 'zh': {'title': '突破传统，提升推理性能的top-nsigma方法', 'desc': '本文提出了一种新的采样方法top-nsigma，旨在改善大语言模型在推理任务中的表现。该方法直接在预软最大值的logits上操作，通过统计阈值来进行有效的令牌过滤。我们的研究表明，logits可以自然地分为高斯分布的噪声区域和信息丰富的区域，从而避免了复杂的概率操作。实验结果显示，top-nsigma在多个推理数据集上超越了现有的采样方法和贪婪解码，且在高温度下仍能保持稳定的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.10510', 'title': 'SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.10510', 'abstract': 'Diffusion Transformers (DiT) have emerged as powerful generative models for various tasks, including image, video, and speech synthesis. However, their inference process remains computationally expensive due to the repeated evaluation of resource-intensive attention and feed-forward modules. To address this, we introduce SmoothCache, a model-agnostic inference acceleration technique for DiT architectures. SmoothCache leverages the observed high similarity between layer outputs across adjacent diffusion timesteps. By analyzing layer-wise representation errors from a small calibration set, SmoothCache adaptively caches and reuses key features during inference. Our experiments demonstrate that SmoothCache achieves 8% to 71% speed up while maintaining or even improving generation quality across diverse modalities. We showcase its effectiveness on DiT-XL for image generation, Open-Sora for text-to-video, and Stable Audio Open for text-to-audio, highlighting its potential to enable real-time applications and broaden the accessibility of powerful DiT models.', 'score': 8, 'issue_id': 654, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': '991f548fec1ec8c9', 'authors': ['Joseph Liu', 'Joshua Geddes', 'Ziyu Guo', 'Haomiao Jiang', 'Mahesh Kumar Nandwana'], 'affiliations': ['Queens University', 'Roblox'], 'pdf_title_img': 'assets/pdf/title_img/2411.10510.jpg', 'data': {'categories': ['#inference', '#multimodal', '#video', '#optimization', '#audio', '#diffusion'], 'emoji': '⚡', 'ru': {'title': 'SmoothCache: Быстрее и лучше с умным кэшированием для DiT', 'desc': 'Статья представляет SmoothCache - технику ускорения вывода для архитектур Diffusion Transformers (DiT). SmoothCache использует высокое сходство между выходными данными слоев на соседних временных шагах диффузии. Метод адаптивно кэширует и повторно использует ключевые признаки во время вывода, анализируя ошибки представления слоев на небольшом калибровочном наборе. Эксперименты показывают, что SmoothCache достигает ускорения от 8% до 71% при сохранении или даже улучшении качества генерации для различных модальностей.'}, 'en': {'title': 'Accelerating Diffusion Transformers with SmoothCache', 'desc': 'This paper presents SmoothCache, a technique designed to speed up the inference process of Diffusion Transformers (DiT), which are used for generating images, videos, and audio. The traditional inference method is slow because it requires many evaluations of complex attention and feed-forward layers. SmoothCache improves efficiency by caching and reusing similar outputs from adjacent diffusion timesteps, reducing the need for repeated calculations. Experiments show that this method can accelerate inference by 8% to 71% while maintaining or enhancing the quality of the generated content.'}, 'zh': {'title': 'SmoothCache：加速扩散变换器的推理过程', 'desc': '扩散变换器（DiT）是一种强大的生成模型，广泛应用于图像、视频和语音合成等任务。然而，它们的推理过程计算开销较大，因为需要重复评估资源密集型的注意力和前馈模块。为了解决这个问题，我们提出了SmoothCache，这是一种与模型无关的推理加速技术，利用相邻扩散时间步之间层输出的高度相似性。通过分析小型校准集中的层级表示误差，SmoothCache自适应地缓存和重用关键特征，从而在保持或提高生成质量的同时，实现了8%到71%的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2411.11767', 'title': 'Drowning in Documents: Consequences of Scaling Reranker Inference', 'url': 'https://huggingface.co/papers/2411.11767', 'abstract': 'Rerankers, typically cross-encoders, are often used to re-score the documents retrieved by cheaper initial IR systems. This is because, though expensive, rerankers are assumed to be more effective. We challenge this assumption by measuring reranker performance for full retrieval, not just re-scoring first-stage retrieval. Our experiments reveal a surprising trend: the best existing rerankers provide diminishing returns when scoring progressively more documents and actually degrade quality beyond a certain limit. In fact, in this setting, rerankers can frequently assign high scores to documents with no lexical or semantic overlap with the query. We hope that our findings will spur future research to improve reranking.', 'score': 8, 'issue_id': 652, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 ноября', 'en': 'November 18', 'zh': '11月18日'}, 'hash': '3fa06087787bc8d4', 'authors': ['Mathew Jacob', 'Erik Lindgren', 'Matei Zaharia', 'Michael Carbin', 'Omar Khattab', 'Andrew Drozdov'], 'affiliations': ['Databricks', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2411.11767.jpg', 'data': {'categories': ['#benchmark', '#data'], 'emoji': '🔍', 'ru': {'title': 'Неожиданные ограничения ранжировщиков в информационном поиске', 'desc': 'В статье исследуется эффективность ранжировщиков (rerankers) в информационном поиске. Авторы обнаружили, что при оценке большего количества документов качество ранжирования ухудшается. Более того, ранжировщики могут присваивать высокие оценки документам без лексического или семантического сходства с запросом. Исследование ставит под сомнение предположение о превосходстве ранжировщиков над более простыми системами поиска.'}, 'en': {'title': 'Rethinking Rerankers: Diminishing Returns in Document Scoring', 'desc': 'This paper investigates the effectiveness of rerankers, specifically cross-encoders, in the context of information retrieval (IR). Traditionally, rerankers are believed to enhance the quality of document scoring after an initial retrieval phase. However, the authors find that as more documents are scored, the performance of these rerankers diminishes and can even lead to poorer results. The study highlights that rerankers may assign high scores to irrelevant documents, suggesting a need for improved methods in reranking processes.'}, 'zh': {'title': '重排序器的有效性需重新审视', 'desc': '本文探讨了重排序器（通常是交叉编码器）在信息检索中的有效性。我们通过测量重排序器在完整检索中的表现，挑战了它们在初步检索后重新评分的假设。实验结果显示，现有的重排序器在评分越来越多的文档时，效果逐渐减弱，甚至在某个限度后质量下降。我们的发现希望能激励未来的研究，以改进重排序技术。'}}}, {'id': 'https://huggingface.co/papers/2411.11171', 'title': 'LLäMmlein: Compact and Competitive German-Only Language Models from Scratch', 'url': 'https://huggingface.co/papers/2411.11171', 'abstract': 'We create two German-only decoder models, LL\\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models\' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models\' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.', 'score': 7, 'issue_id': 656, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 ноября', 'en': 'November 17', 'zh': '11月17日'}, 'hash': '3eea3fe6bac7ab51', 'authors': ['Jan Pfister', 'Julia Wunderle', 'Andreas Hotho'], 'affiliations': ['Data Science Chair Center for Artificial Intelligence and Data Science (CAIDAS) Julius-Maximilians-Universität Würzburg (JMU)'], 'pdf_title_img': 'assets/pdf/title_img/2411.11171.jpg', 'data': {'categories': ['#benchmark', '#data', '#dataset', '#low_resource', '#open_source', '#training', '#multilingual'], 'emoji': '🇩🇪', 'ru': {'title': 'Немецкие языковые модели: от данных к декодеру', 'desc': 'Исследователи создали две модели декодера на немецком языке: LL"aMmlein 120M и 1B. Процесс включал обширную предобработку данных, создание специального немецкого токенизатора и обучение моделей. Модели оценивались на различных бенчмарках, включая SuperGLEBer, и показали конкурентоспособные результаты по сравнению с современными моделями аналогичного размера. Исследование предоставило ценные insights для будущей разработки языковых моделей.'}, 'en': {'title': 'Empowering German NLP with LL"aMmlein Models', 'desc': 'This paper presents two German-only decoder models, LL"aMmlein 120M and 1B, developed from scratch for the German NLP community. The training process included data preprocessing, a custom tokenizer, and evaluation against benchmarks like SuperGLEBer. The models demonstrated competitive performance, matching or exceeding state-of-the-art models of similar sizes. Insights from the training revealed that while model quality improves with size, some tasks show early performance plateaus, guiding future resource allocation in model development.'}, 'zh': {'title': '德语解码器模型的创新与共享', 'desc': '我们创建了两个仅支持德语的解码器模型，LL"aMmlein 120M和1B，并将其训练数据公开，供德语自然语言处理研究社区使用。模型训练包括多个关键步骤，如数据预处理、自定义德语分词器的创建、实际训练以及在各种基准上的最终模型评估。在训练过程中，我们保存并分析了多个检查点，使用SuperGLEBer基准监测模型的学习动态。与SuperGLEBer基准上的最先进模型相比，两个LL"aMmlein模型表现出竞争力，始终与相似参数大小的模型相匹配或超越。'}}}, {'id': 'https://huggingface.co/papers/2411.10669', 'title': 'Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts', 'url': 'https://huggingface.co/papers/2411.10669', 'abstract': 'As the research of Multimodal Large Language Models (MLLMs) becomes popular, an advancing MLLM model is typically required to handle various textual and visual tasks (e.g., VQA, Detection, OCR, and ChartQA) simultaneously for real-world applications. However, due to the significant differences in representation and distribution among data from various tasks, simply mixing data of all tasks together leads to the well-known``multi-task conflict" issue, resulting in performance degradation across various tasks. To address this issue, we propose Awaker2.5-VL, a Mixture of Experts~(MoE) architecture suitable for MLLM, which acquires the multi-task capabilities through multiple sparsely activated experts. To speed up the training and inference of Awaker2.5-VL, each expert in our model is devised as a low-rank adaptation (LoRA) structure. Extensive experiments on multiple latest benchmarks demonstrate the effectiveness of Awaker2.5-VL. The code and model weight are released in our Project Page: https://github.com/MetabrainAGI/Awaker.', 'score': 7, 'issue_id': 654, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 ноября', 'en': 'November 16', 'zh': '11月16日'}, 'hash': 'f1319420ae85759e', 'authors': ['Jinqiang Long', 'Yanqi Dai', 'Guoxing Yang', 'Hongpeng Lin', 'Nanyi Fei', 'Yizhao Gao', 'Zhiwu Lu'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'Metabrain AGI Lab, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2411.10669.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#benchmark', '#optimization', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Awaker2.5-VL: Мультизадачная MLLM без конфликтов', 'desc': 'Статья представляет Awaker2.5-VL - новую архитектуру мультимодальной большой языковой модели (MLLM), основанную на принципе смеси экспертов (MoE). Модель решает проблему конфликта между задачами при обучении на разнородных данных. Каждый эксперт в системе реализован с использованием низкоранговой адаптации (LoRA) для ускорения обучения и вывода. Эксперименты показывают эффективность Awaker2.5-VL на современных бенчмарках.'}, 'en': {'title': 'Awaker2.5-VL: Mastering Multimodal Tasks with Expert Precision', 'desc': "This paper introduces Awaker2.5-VL, a new model designed to improve the performance of Multimodal Large Language Models (MLLMs) on various tasks like visual question answering and detection. The authors address the 'multi-task conflict' problem, which occurs when different tasks interfere with each other due to their diverse data representations. Awaker2.5-VL uses a Mixture of Experts (MoE) architecture, where only a subset of experts is activated for each task, allowing for better specialization and efficiency. Additionally, the model incorporates low-rank adaptation (LoRA) to enhance training speed and inference performance, showing promising results in extensive experiments."}, 'zh': {'title': '多模态任务的专家混合解决方案', 'desc': '本研究提出了一种名为Awaker2.5-VL的多模态大语言模型（MLLM），旨在同时处理文本和视觉任务，如视觉问答（VQA）、检测、光学字符识别（OCR）和图表问答（ChartQA）。为了克服多任务冲突问题，Awaker2.5-VL采用了专家混合（MoE）架构，通过多个稀疏激活的专家来实现多任务能力。每个专家被设计为低秩适应（LoRA）结构，以加速模型的训练和推理。大量实验结果表明，Awaker2.5-VL在多个最新基准测试中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2411.09213', 'title': 'Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering', 'url': 'https://huggingface.co/papers/2411.09213', 'abstract': "Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs' ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models' limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs' reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.", 'score': 6, 'issue_id': 655, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 ноября', 'en': 'November 14', 'zh': '11月14日'}, 'hash': 'e99e85d88963aa4c', 'authors': ['Nghia Trung Ngo', 'Chien Van Nguyen', 'Franck Dernoncourt', 'Thien Huu Nguyen'], 'affiliations': ['Adobe Research, USA', 'Department of Computer Science, University of Oregon, OR, USA'], 'pdf_title_img': 'assets/pdf/title_img/2411.09213.jpg', 'data': {'categories': ['#rag', '#survey', '#healthcare', '#open_source', '#benchmark', '#reasoning'], 'emoji': '🩺', 'ru': {'title': 'Новый бенчмарк для оценки надежности медицинских RAG-систем', 'desc': 'Статья представляет новый комплексный фреймворк оценки для медицинских систем вопросов и ответов, использующих retrieval-augmented generation (RAG). Авторы вводят бенчмарк MedRGB, который дополняет существующие наборы данных элементами для тестирования различных аспектов надежности системы. Проведены обширные эксперименты с коммерческими и открытыми языковыми моделями в разных условиях поиска. Результаты выявили ограниченные способности современных моделей справляться с шумом и дезинформацией в извлеченных документах.'}, 'en': {'title': 'Enhancing Medical QA with Robust Retrieval-Augmented Generation', 'desc': 'This paper introduces a new evaluation framework called Medical Retrieval-Augmented Generation Benchmark (MedRGB) to improve the performance of large language models (LLMs) in medical question-answering tasks. It highlights the need for accurate and trustworthy systems in the sensitive medical domain, addressing gaps in existing benchmarks that do not consider practical scenarios. The authors conduct extensive evaluations of both commercial and open-source LLMs, revealing their limitations in dealing with noise and misinformation in retrieved documents. The study also analyzes the reasoning processes of these models, providing insights for future improvements in retrieval-augmented generation systems for medical applications.'}, 'zh': {'title': '提升医疗问答系统的可靠性', 'desc': '检索增强生成（RAG）是一种有前景的方法，可以提高大型语言模型（LLMs）在知识密集型任务中的表现，尤其是在医疗领域。然而，医疗领域的敏感性要求系统必须完全准确和可信。现有的RAG基准主要关注标准的检索-回答设置，忽视了许多实际场景，这些场景对可靠医疗系统的关键方面进行了评估。本文提出了医疗检索增强生成基准（MedRGB），为四个医疗问答数据集提供了各种补充元素，以测试LLMs在特定场景下的处理能力。'}}}, {'id': 'https://huggingface.co/papers/2411.11024', 'title': 'VeGaS: Video Gaussian Splatting', 'url': 'https://huggingface.co/papers/2411.11024', 'abstract': 'Implicit Neural Representations (INRs) employ neural networks to approximate discrete data as continuous functions. In the context of video data, such models can be utilized to transform the coordinates of pixel locations along with frame occurrence times (or indices) into RGB color values. Although INRs facilitate effective compression, they are unsuitable for editing purposes. One potential solution is to use a 3D Gaussian Splatting (3DGS) based model, such as the Video Gaussian Representation (VGR), which is capable of encoding video as a multitude of 3D Gaussians and is applicable for numerous video processing operations, including editing. Nevertheless, in this case, the capacity for modification is constrained to a limited set of basic transformations. To address this issue, we introduce the Video Gaussian Splatting (VeGaS) model, which enables realistic modifications of video data. To construct VeGaS, we propose a novel family of Folded-Gaussian distributions designed to capture nonlinear dynamics in a video stream and model consecutive frames by 2D Gaussians obtained as respective conditional distributions. Our experiments demonstrate that VeGaS outperforms state-of-the-art solutions in frame reconstruction tasks and allows realistic modifications of video data. The code is available at: https://github.com/gmum/VeGaS.', 'score': 5, 'issue_id': 661, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 ноября', 'en': 'November 17', 'zh': '11月17日'}, 'hash': '0c6ed02c1597f4ff', 'authors': ['Weronika Smolak-Dyżewska', 'Dawid Malarz', 'Kornel Howil', 'Jan Kaczmarczyk', 'Marcin Mazur', 'Przemysław Spurek'], 'affiliations': ['Jagiellonian University Faculty of Mathematics and Computer Science'], 'pdf_title_img': 'assets/pdf/title_img/2411.11024.jpg', 'data': {'categories': ['#video', '#optimization', '#3d'], 'emoji': '🎬', 'ru': {'title': 'VeGaS: Революция в редактировании видео с помощью гауссовых распределений', 'desc': 'Статья представляет новую модель под названием Video Gaussian Splatting (VeGaS) для реалистичного редактирования видео. VeGaS использует семейство сложенных гауссовых распределений для захвата нелинейной динамики в видеопотоке. Модель превосходит современные решения в задачах реконструкции кадров. VeGaS позволяет проводить реалистичные модификации видеоданных, преодолевая ограничения предыдущих подходов.'}, 'en': {'title': 'VeGaS: Revolutionizing Video Editing with Gaussian Splatting', 'desc': 'Implicit Neural Representations (INRs) use neural networks to represent discrete data as continuous functions, particularly in video processing. They convert pixel coordinates and frame indices into RGB values, enabling effective data compression but limiting editing capabilities. The Video Gaussian Splatting (VeGaS) model enhances this by utilizing a new family of Folded-Gaussian distributions, allowing for realistic video modifications while capturing nonlinear dynamics. Our experiments show that VeGaS surpasses existing methods in frame reconstruction and supports a wider range of editing operations.'}, 'zh': {'title': '视频数据的真实修改新方法', 'desc': '隐式神经表示（INRs）使用神经网络将离散数据近似为连续函数。在视频数据中，这种模型可以将像素位置的坐标和帧出现时间转换为RGB颜色值。虽然INRs在压缩方面表现出色，但不适合编辑。我们提出的视频高斯点云（VeGaS）模型，利用新型的折叠高斯分布，能够实现视频数据的真实修改，并在帧重建任务中超越了现有的最先进解决方案。'}}}, {'id': 'https://huggingface.co/papers/2411.09661', 'title': 'Adaptive Decoding via Latent Preference Optimization', 'url': 'https://huggingface.co/papers/2411.09661', 'abstract': 'During language model decoding, it is known that using higher temperature sampling gives more creative responses, while lower temperatures are more factually accurate. However, such models are commonly applied to general instruction following, which involves both creative and fact seeking tasks, using a single fixed temperature across all examples and tokens. In this work, we introduce Adaptive Decoding, a layer added to the model to select the sampling temperature dynamically at inference time, at either the token or example level, in order to optimize performance. To learn its parameters we introduce Latent Preference Optimization (LPO) a general approach to train discrete latent variables such as choices of temperature. Our method outperforms all fixed decoding temperatures across a range of tasks that require different temperatures, including UltraFeedback, Creative Story Writing, and GSM8K.', 'score': 5, 'issue_id': 659, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 ноября', 'en': 'November 14', 'zh': '11月14日'}, 'hash': '4bb5fff16280c4bc', 'authors': ['Shehzaad Dhuliawala', 'Ilia Kulikov', 'Ping Yu', 'Asli Celikyilmaz', 'Jason Weston', 'Sainbayar Sukhbaatar', 'Jack Lanchantin'], 'affiliations': ['FAIR at Meta', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2411.09661.jpg', 'data': {'categories': ['#training', '#optimization', '#story_generation', '#inference'], 'emoji': '🌡️', 'ru': {'title': 'Адаптивное декодирование: умный выбор температуры для языковых моделей', 'desc': 'Статья представляет новый метод адаптивного декодирования для языковых моделей, который динамически выбирает температуру сэмплирования во время вывода. Авторы вводят технику оптимизации латентных предпочтений (LPO) для обучения дискретных латентных переменных, таких как выбор температуры. Метод превосходит фиксированные температуры декодирования на ряде задач, требующих различных температур. Эксперименты проводились на датасетах UltraFeedback, Creative Story Writing и GSM8K.'}, 'en': {'title': 'Dynamic Temperature for Optimal Language Model Responses', 'desc': "This paper presents a new method called Adaptive Decoding, which allows language models to adjust their sampling temperature dynamically during inference. By varying the temperature, the model can balance creativity and factual accuracy based on the specific task at hand. The authors introduce Latent Preference Optimization (LPO) to effectively train the model's temperature selection process. Their approach shows improved performance over traditional fixed temperature methods across various tasks, demonstrating its versatility and effectiveness."}, 'zh': {'title': '自适应解码：动态选择温度优化模型表现', 'desc': '在语言模型解码过程中，使用较高的温度采样可以产生更具创意的响应，而较低的温度则更准确。本文提出了一种自适应解码方法，通过在推理时动态选择采样温度，优化模型在不同任务中的表现。我们引入了潜在偏好优化（LPO）来训练温度选择的离散潜变量。实验结果表明，我们的方法在需要不同温度的多种任务中表现优于固定解码温度。'}}}, {'id': 'https://huggingface.co/papers/2411.11045', 'title': 'StableV2V: Stablizing Shape Consistency in Video-to-Video Editing', 'url': 'https://huggingface.co/papers/2411.11045', 'abstract': 'Recent advancements of generative AI have significantly promoted content creation and editing, where prevailing studies further extend this exciting progress to video editing. In doing so, these studies mainly transfer the inherent motion patterns from the source videos to the edited ones, where results with inferior consistency to user prompts are often observed, due to the lack of particular alignments between the delivered motions and edited contents. To address this limitation, we present a shape-consistent video editing method, namely StableV2V, in this paper. Our method decomposes the entire editing pipeline into several sequential procedures, where it edits the first video frame, then establishes an alignment between the delivered motions and user prompts, and eventually propagates the edited contents to all other frames based on such alignment. Furthermore, we curate a testing benchmark, namely DAVIS-Edit, for a comprehensive evaluation of video editing, considering various types of prompts and difficulties. Experimental results and analyses illustrate the outperforming performance, visual consistency, and inference efficiency of our method compared to existing state-of-the-art studies.', 'score': 5, 'issue_id': 658, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 ноября', 'en': 'November 17', 'zh': '11月17日'}, 'hash': 'd4f742e7121f2322', 'authors': ['Chang Liu', 'Rui Li', 'Kaidong Zhang', 'Yunwei Lan', 'Dong Liu'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2411.11045.jpg', 'data': {'categories': ['#optimization', '#games', '#video', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'StableV2V: редактирование видео с сохранением формы и согласованностью движения', 'desc': 'Статья представляет метод редактирования видео под названием StableV2V, который обеспечивает согласованность формы при редактировании. Метод разбивает процесс редактирования на несколько последовательных этапов, включая редактирование первого кадра и распространение изменений на остальные кадры. Авторы также создали тестовый набор данных DAVIS-Edit для оценки различных аспектов редактирования видео. Эксперименты показывают, что StableV2V превосходит существующие методы по качеству результатов, визуальной согласованности и эффективности вывода.'}, 'en': {'title': 'StableV2V: Aligning Motion with User Prompts for Consistent Video Editing', 'desc': 'This paper introduces StableV2V, a novel method for video editing that enhances the consistency between user prompts and the resulting video content. The approach involves breaking down the editing process into sequential steps, starting with the first frame and aligning the motion patterns with user instructions. By propagating the edited content across all frames based on this alignment, StableV2V achieves better visual coherence and performance. Additionally, the authors present a new benchmark, DAVIS-Edit, to evaluate the effectiveness of video editing methods under various conditions.'}, 'zh': {'title': '稳定一致的视频编辑方法', 'desc': '本文介绍了一种名为StableV2V的形状一致性视频编辑方法，旨在提高视频编辑的质量和一致性。该方法将整个编辑流程分解为多个顺序步骤，首先编辑第一帧视频，然后在用户提示和传递的运动之间建立对齐，最后根据这种对齐将编辑内容传播到其他帧。我们还创建了一个测试基准DAVIS-Edit，以全面评估视频编辑的效果，考虑了不同类型的提示和难度。实验结果表明，与现有的最先进研究相比，我们的方法在性能、视觉一致性和推理效率上均表现优越。'}}}, {'id': 'https://huggingface.co/papers/2411.10499', 'title': 'FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on', 'url': 'https://huggingface.co/papers/2411.10499', 'abstract': 'Although image-based virtual try-on has made considerable progress, emerging approaches still encounter challenges in producing high-fidelity and robust fitting images across diverse scenarios. These methods often struggle with issues such as texture-aware maintenance and size-aware fitting, which hinder their overall effectiveness. To address these limitations, we propose a novel garment perception enhancement technique, termed FitDiT, designed for high-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more parameters and attention to high-resolution features. First, to further improve texture-aware maintenance, we introduce a garment texture extractor that incorporates garment priors evolution to fine-tune garment feature, facilitating to better capture rich details such as stripes, patterns, and text. Additionally, we introduce frequency-domain learning by customizing a frequency distance loss to enhance high-frequency garment details. To tackle the size-aware fitting issue, we employ a dilated-relaxed mask strategy that adapts to the correct length of garments, preventing the generation of garments that fill the entire mask area during cross-category try-on. Equipped with the above design, FitDiT surpasses all baselines in both qualitative and quantitative evaluations. It excels in producing well-fitting garments with photorealistic and intricate details, while also achieving competitive inference times of 4.57 seconds for a single 1024x768 image after DiT structure slimming, outperforming existing methods.', 'score': 4, 'issue_id': 654, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': 'b142b4be26ef6147', 'authors': ['Boyuan Jiang', 'Xiaobin Hu', 'Donghao Luo', 'Qingdong He', 'Chengming Xu', 'Jinlong Peng', 'Jiangning Zhang', 'Chengjie Wang', 'Yunsheng Wu', 'Yanwei Fu'], 'affiliations': ['Fudan University', 'Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2411.10499.jpg', 'data': {'categories': ['#cv', '#optimization', '#3d', '#diffusion'], 'emoji': '👚', 'ru': {'title': 'FitDiT: Высококачественная виртуальная примерка с сохранением деталей', 'desc': 'Статья представляет новый метод виртуальной примерки одежды под названием FitDiT, основанный на диффузионных трансформерах. FitDiT улучшает сохранение текстур одежды с помощью экстрактора текстур и обучения в частотной области. Для решения проблемы подгонки размера используется стратегия расширенной маски. Метод превосходит существующие подходы по качеству и реалистичности генерируемых изображений.'}, 'en': {'title': 'FitDiT: Revolutionizing Virtual Try-On with High-Fidelity Garment Perception', 'desc': 'This paper presents FitDiT, a new technique for improving virtual try-on systems using Diffusion Transformers. It addresses challenges in producing realistic and well-fitting images by enhancing garment perception through a specialized texture extractor and frequency-domain learning. The method also incorporates a dilated-relaxed mask strategy to ensure garments fit correctly without oversizing. FitDiT demonstrates superior performance in generating high-fidelity images with intricate details while maintaining efficient processing times.'}, 'zh': {'title': 'FitDiT：高保真虚拟试穿的新突破', 'desc': '本文提出了一种新的服装感知增强技术，称为FitDiT，旨在提高虚拟试穿的高保真度。该方法利用扩散变换器（DiT）分配更多参数和注意力于高分辨率特征，以解决纹理感知维护和尺寸感知适配的问题。我们引入了服装纹理提取器和频域学习，增强了服装细节的捕捉能力，并采用扩张放松掩码策略来适应服装的正确长度。FitDiT在定性和定量评估中均超越了所有基线，能够生成具有真实感和复杂细节的合身服装。'}}}, {'id': 'https://huggingface.co/papers/2411.10168', 'title': "Evaluating the role of `Constitutions' for learning from AI feedback", 'url': 'https://huggingface.co/papers/2411.10168', 'abstract': "The growing capabilities of large language models (LLMs) have led to their use as substitutes for human feedback for training and assessing other LLMs. These methods often rely on `constitutions', written guidelines which a critic model uses to provide feedback and improve generations. We investigate how the choice of constitution affects feedback quality by using four different constitutions to improve patient-centered communication in medical interviews. In pairwise comparisons conducted by 215 human raters, we found that detailed constitutions led to better results regarding emotive qualities. However, none of the constitutions outperformed the baseline in learning more practically-oriented skills related to information gathering and provision. Our findings indicate that while detailed constitutions should be prioritised, there are possible limitations to the effectiveness of AI feedback as a reward signal in certain areas.", 'score': 3, 'issue_id': 661, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': '491ed277e2d6a217', 'authors': ['Saskia Redgate', 'Andrew M. Bean', 'Adam Mahdi'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2411.10168.jpg', 'data': {'categories': ['#rlhf', '#interpretability', '#alignment', '#healthcare'], 'emoji': '🤖', 'ru': {'title': 'Конституции LLM: возможности и ограничения в медицинской коммуникации', 'desc': "Исследование посвящено использованию больших языковых моделей (LLM) для обучения и оценки других LLM вместо человеческой обратной связи. Авторы изучали влияние выбора 'конституции' (письменных инструкций) на качество обратной связи в контексте улучшения коммуникации между врачом и пациентом. Результаты показали, что подробные конституции лучше справляются с эмоциональными аспектами, но не превосходят базовую модель в практических навыках сбора и предоставления информации. Исследование указывает на потенциальные ограничения эффективности обратной связи от ИИ в определенных областях."}, 'en': {'title': 'Enhancing AI Feedback with Detailed Guidelines', 'desc': "This paper explores how large language models (LLMs) can be trained and evaluated using human-like feedback, specifically through the use of 'constitutions'—guidelines that help a critic model assess and improve LLM outputs. The study tests four different constitutions to enhance patient-centered communication during medical interviews. Results from 215 human raters show that more detailed constitutions improve the emotive qualities of the communication, but they do not significantly enhance practical skills like information gathering. The findings suggest that while detailed guidelines are beneficial, there are limitations to using AI feedback as a reward signal for certain competencies."}, 'zh': {'title': '详细指导原则提升情感质量', 'desc': '本文探讨了大型语言模型（LLMs）在训练和评估其他LLMs时，如何作为人类反馈的替代品。研究中使用了四种不同的指导原则（constitution），以改善医疗访谈中的以患者为中心的沟通。通过215名人类评审者的对比实验，我们发现详细的指导原则在情感质量方面的反馈效果更好。然而，在信息收集和提供等实际技能的学习上，没有任何指导原则超越了基线表现。'}}}, {'id': 'https://huggingface.co/papers/2411.03562', 'title': 'Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Level', 'url': 'https://huggingface.co/papers/2411.03562', 'abstract': "We introduce Agent K v1.0, an end-to-end autonomous data science agent designed to automate, optimise, and generalise across diverse data science tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle by learning from experience. It leverages a highly flexible structured reasoning framework to enable it to dynamically process memory in a nested structure, effectively learning from accumulated experience stored to handle complex reasoning tasks. It optimises long- and short-term memory by selectively storing and retrieving key information, guiding future decisions based on environmental rewards. This iterative approach allows it to refine decisions without fine-tuning or backpropagation, achieving continuous improvement through experiential learning. We evaluate our agent's apabilities using Kaggle competitions as a case study. Following a fully automated protocol, Agent K v1.0 systematically addresses complex and multimodal data science tasks, employing Bayesian optimisation for hyperparameter tuning and feature engineering. Our new evaluation framework rigorously assesses Agent K v1.0's end-to-end capabilities to generate and send submissions starting from a Kaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\\% success rate across tasks, spanning tabular, computer vision, NLP, and multimodal domains. When benchmarking against 5,856 human Kaggle competitors by calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\\%, demonstrating an overall skill level comparable to Expert-level users. Notably, its Elo-MMR score falls between the first and third quartiles of scores achieved by human Grandmasters. Furthermore, our results indicate that Agent K v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a record of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's progression system.", 'score': 56, 'issue_id': 457, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '1db584382b826315', 'authors': ['Antoine Grosnit', 'Alexandre Maraval', 'James Doran', 'Giuseppe Paolo', 'Albert Thomas', 'Refinath Shahul Hameed Nabeezath Beevi', 'Jonas Gonzalez', 'Khyati Khandelwal', 'Ignacio Iacobacci', 'Abdelhakim Benechehab', 'Hamza Cherkaoui', 'Youssef Attia El-Hili', 'Kun Shao', 'Jianye Hao', 'Jun Yao', 'Balazs Kegl', 'Haitham Bou-Ammar', 'Jun Wang'], 'affiliations': ['Huawei Noahs Ark', 'AI Centre, Department of Computer Science, UCL', 'Technical University of Darmstadt'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03562.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#agi', '#optimization', '#multimodal', '#training', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Agent K v1.0: Автономный ИИ-агент для решения задач в области науки о данных', 'desc': 'Представлен Agent K v1.0 - автономный агент для автоматизации задач в области науки о данных. Он использует структурированную систему рассуждений для обработки памяти и обучения на основе опыта. Agent K v1.0 оптимизирует долгосрочную и краткосрочную память, что позволяет ему улучшать решения без дополнительной настройки. Оценка возможностей агента проводилась на соревнованиях Kaggle, где он продемонстрировал 92.5% успешность в решении разнообразных задач.'}, 'en': {'title': 'Agent K v1.0: Your Autonomous Data Science Expert!', 'desc': "Agent K v1.0 is an autonomous data science agent that automates the entire data science life cycle, learning from its experiences to improve over time. It uses a structured reasoning framework to manage memory and handle complex tasks without the need for traditional fine-tuning methods. By employing Bayesian optimization for hyperparameter tuning and feature engineering, it effectively addresses a variety of data science challenges. The agent's performance has been validated through Kaggle competitions, where it achieved a high success rate and ranked competitively against human experts, demonstrating its advanced capabilities in multiple domains."}, 'zh': {'title': 'Agent K v1.0：自动化数据科学的未来', 'desc': '我们介绍了Agent K v1.0，这是一个端到端的自主数据科学代理，旨在自动化、优化和泛化各种数据科学任务。Agent K v1.0 完全自动化，能够管理整个数据科学生命周期，并通过经验学习来提升能力。它利用灵活的结构化推理框架，动态处理嵌套结构中的记忆，有效地从积累的经验中学习，以应对复杂的推理任务。通过选择性存储和检索关键信息，Agent K v1.0 优化了短期和长期记忆，基于环境奖励指导未来决策，展现出与人类专家相当的技能水平。'}}}, {'id': 'https://huggingface.co/papers/2411.03823', 'title': 'Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination', 'url': 'https://huggingface.co/papers/2411.03823', 'abstract': 'The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced.', 'score': 43, 'issue_id': 457, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '3f0a02ee67213e17', 'authors': ['Dingjie Song', 'Sicheng Lai', 'Shunian Chen', 'Lichao Sun', 'Benyou Wang'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen', 'Lehigh University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03823.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#multimodal', '#training', '#dataset', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Чистота данных - залог надёжности мультимодальных ИИ-моделей', 'desc': 'Статья посвящена проблеме загрязнения данных в мультимодальных больших языковых моделях (MLLM). Авторы представляют фреймворк MM-Detect для обнаружения загрязнения в MLLM. Экспериментальные результаты показывают, что MM-Detect чувствителен к разным степеням загрязнения и может выявлять значительные улучшения производительности из-за утечки обучающего набора. Исследование также рассматривает возможность загрязнения на этапах предобучения и тонкой настройки моделей.'}, 'en': {'title': 'Detecting Contamination in Multimodal Language Models', 'desc': 'This paper addresses the challenge of data contamination in multimodal large language models (MLLMs), which can affect their performance evaluation. The authors propose a new framework called MM-Detect, specifically designed to identify contamination in MLLMs across different modalities and training phases. Their experiments show that MM-Detect can effectively detect varying levels of contamination and reveal how training set leakage impacts performance. Additionally, the study investigates contamination sources during both the pre-training and fine-tuning phases of MLLMs, providing valuable insights into potential contamination points.'}, 'zh': {'title': '多模态数据污染检测的新突破', 'desc': '本研究介绍了一种针对多模态大语言模型（MLLMs）数据污染检测的新框架MM-Detect。该框架能够有效识别在训练过程中可能出现的数据污染问题，尤其是在多模态基准测试中。实验结果表明，MM-Detect对不同程度的数据污染非常敏感，并能显著提升性能评估的准确性。此外，我们还探讨了数据污染可能源自LLMs的预训练阶段和MLLMs的微调阶段，为理解污染引入的时机提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2411.03884', 'title': 'Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models', 'url': 'https://huggingface.co/papers/2411.03884', 'abstract': 'Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.', 'score': 20, 'issue_id': 459, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '6ed1524392784244', 'authors': ['Zhijian Zhuo', 'Ya Wang', 'Yutao Zeng', 'Xiaoqing Li', 'Xun Zhou', 'Jinwen Ma'], 'affiliations': ['School of Mathematical Sciences, Peking University', 'Seed-Foundation-Model, ByteDance', 'Capital University of Economics and Business'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03884.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#open_source', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'PolyCom: Революция в активационных функциях для трансформеров', 'desc': 'Статья представляет новую категорию активационных функций для трансформеров, называемую PolyCom (полиномиальные композитные активации). Авторы теоретически обосновывают, что PolyCom обладает улучшенной выразительностью и эффективностью по сравнению с другими активационными функциями. Эксперименты показывают, что использование PolyCom в больших языковых моделях (LLM) позволяет им лучше улавливать взаимодействия высокого порядка в данных. Результаты демонстрируют значительные улучшения в точности и скорости сходимости по сравнению с традиционными активационными функциями.'}, 'en': {'title': 'Unlocking Transformer Potential with Polynomial Activations', 'desc': 'This paper introduces a new type of activation function called Polynomial Composition Activations (PolyCom) for transformer models. The authors argue that PolyCom enhances the nonlinearity of transformers, which is crucial for improving their representational capacity. Through mathematical analysis, they show that networks using PolyCom can approximate complex functions more efficiently than those using traditional activation functions. Empirical tests on large language models reveal that PolyCom leads to better performance in terms of accuracy and convergence, demonstrating its potential as a superior alternative in machine learning applications.'}, 'zh': {'title': '多项式组合激活函数：提升变换器性能的新方法', 'desc': '本文提出了一种新型的多项式组合激活函数（PolyCom），旨在优化变换器的动态性能。我们通过数学分析证明了PolyCom在表达能力和效率方面优于其他激活函数。实验结果表明，使用PolyCom的网络在逼近光滑函数时所需的参数更少，且在大型语言模型的预训练配置中表现出更高的准确性和收敛速度。我们的研究表明，PolyCom能够有效捕捉数据中的高阶交互，从而显著提升模型性能。'}}}, {'id': 'https://huggingface.co/papers/2411.04109', 'title': 'Self-Consistency Preference Optimization', 'url': 'https://huggingface.co/papers/2411.04109', 'abstract': 'Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. In this work, we extend the self-consistency concept to help train models. We thus introduce self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. We show ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku.', 'score': 13, 'issue_id': 459, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '213f2796c0bc72ae', 'authors': ['Archiki Prasad', 'Weizhe Yuan', 'Richard Yuanzhe Pang', 'Jing Xu', 'Maryam Fazel-Zarandi', 'Mohit Bansal', 'Sainbayar Sukhbaatar', 'Jason Weston', 'Jane Yu'], 'affiliations': ['Meta FAIR', 'UNC Chapel Hill', 'New York University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04109.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#rl', '#optimization', '#training', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Самосогласованность как ключ к улучшению ИИ без учителя', 'desc': 'Статья представляет новый метод самосогласованной оптимизации предпочтений (ScPO) для улучшения моделей машинного обучения без использования аннотированных данных. Этот подход расширяет концепцию самосогласованности, применяемую обычно на этапе вывода, на процесс обучения модели. ScPO итеративно обучает модель предпочитать согласованные ответы над несогласованными на новых задачах без учителя. Метод показывает значительные улучшения в задачах рассуждения, таких как GSM8K и MATH, сокращая разрыв с обучением с учителем.'}, 'en': {'title': 'Empowering Models with Self-Consistency for Better Reasoning', 'desc': "This paper introduces a new method called self-consistency preference optimization (ScPO) to enhance the training of machine learning models without human annotations. ScPO focuses on improving the consistency of answers by iteratively training the model to prefer consistent responses over inconsistent ones. The authors demonstrate that ScPO significantly outperforms traditional reward model training on complex reasoning tasks, such as GSM8K and MATH, and even approaches the performance of supervised training. Additionally, when combined with standard supervised learning, ScPO further boosts the model's performance, achieving superior results on various benchmarks."}, 'zh': {'title': '自我一致性优化，提升模型推理能力！', 'desc': '自我对齐是指模型在没有人工标注的情况下自我改进的过程，近年来这一研究领域迅速发展。然而，现有技术在复杂推理任务中常常无法有效提升性能，因为很难分配正确的奖励。本文提出了一种新的方法——自我一致性偏好优化（ScPO），它通过迭代训练一致的答案，使其优于不一致的答案，从而解决无监督新问题。实验结果表明，ScPO在推理任务上显著优于传统的奖励模型训练，且与标准监督学习结合后效果更佳。'}}}, {'id': 'https://huggingface.co/papers/2411.03590', 'title': 'From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond', 'url': 'https://huggingface.co/papers/2411.03590', 'abstract': "Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI's o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1's performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.", 'score': 9, 'issue_id': 460, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': 'c43efbb9c2b1c150', 'authors': ['Harsha Nori', 'Naoto Usuyama', 'Nicholas King', 'Scott Mayer McKinney', 'Xavier Fernandes', 'Sheng Zhang', 'Eric Horvitz'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.03590.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#inference', '#optimization', '#healthcare', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Новая эра моделей с встроенным reasoning: o1-preview превосходит GPT-4 в медицине', 'desc': 'Статья исследует эффективность модели o1-preview от OpenAI на различных медицинских тестах в сравнении с GPT-4 и стратегией Medprompt. Авторы обнаружили, что o1-preview превосходит GPT-4 даже без специальных техник промптинга, но традиционные методы обучения на нескольких примерах (few-shot learning) снижают её производительность. Исследование выявило, что o1-preview достигает наилучших результатов при более высокой стоимости, в то время как GPT-4 с Medprompt остаётся ценной альтернативой в определённых контекстах. Авторы также отмечают необходимость создания новых, более сложных медицинских тестов, так как o1-preview близка к насыщению на существующих бенчмарках.'}, 'en': {'title': 'Unlocking Medical Mastery: The Power of Run-Time Reasoning in LLMs', 'desc': "This paper explores the effectiveness of run-time steering strategies, particularly focusing on the Medprompt technique, in enhancing the performance of large language models (LLMs) in specialized fields like medicine. It introduces OpenAI's o1-preview model, which employs a new approach to reasoning before generating responses, showing superior performance compared to the GPT-4 series even without additional prompting. The study evaluates various medical benchmarks and finds that traditional few-shot prompting may not be beneficial for reasoning-native models like o1-preview, while ensembling remains a viable but resource-intensive option. The analysis reveals a trade-off between cost and accuracy, highlighting the need for new benchmarks to further challenge these advanced models."}, 'zh': {'title': '运行时推理：提升大型语言模型的关键', 'desc': '本文探讨了运行时引导策略（如Medprompt）在提升大型语言模型（LLMs）在医学等专业领域表现中的重要性。研究表明，o1-preview模型在生成最终响应之前进行运行时推理，能够在多种医学挑战基准上超越GPT-4系列。尽管o1-preview在没有提示技术的情况下表现优异，但经典的提示工程策略在新推理模型中可能不再有效。最后，研究指出o1-preview在现有医学基准上已接近饱和，强调了开发新基准的必要性。'}}}, {'id': 'https://huggingface.co/papers/2410.23218', 'title': 'OS-ATLAS: A Foundation Action Model for Generalist GUI Agents', 'url': 'https://huggingface.co/papers/2410.23218', 'abstract': 'Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiProVision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas - a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested significant engineering effort in developing an open-source toolkit for synthesizing GUI grounding data across multiple platforms, including Windows, Linux, MacOS, Android, and the web. Leveraging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs.', 'score': 46, 'issue_id': 410, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': 'd7a3f0fd08f934d5', 'authors': ['Zhiyong Wu', 'Zhenyu Wu', 'Fangzhi Xu', 'Yian Wang', 'Qiushi Sun', 'Chengyou Jia', 'Kanzhi Cheng', 'Zichen Ding', 'Liheng Chen', 'Paul Pu Liang', 'Yu Qiao'], 'affiliations': ['Shanghai AI Laboratory', 'Shanghai Jiaotong University', 'The University of Hong Kong', 'MIT'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23218.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#cv', '#graphs', '#multiplatform', '#data', '#training', '#dataset', '#open_source', '#agents'], 'emoji': '🖥️', 'ru': {'title': 'OS-Atlas: Открытая модель для универсального взаимодействия с GUI', 'desc': 'Исследователи разработали OS-Atlas - основополагающую модель для взаимодействия с графическим интерфейсом пользователя (GUI). Модель использует инновационный подход к данным и моделированию, что позволяет ей эффективно работать с GUI и решать задачи вне распределения (OOD). Авторы создали открытый набор инструментов для синтеза данных о GUI на различных платформах и выпустили крупнейший открытый кросс-платформенный корпус, содержащий более 13 миллионов элементов GUI. OS-Atlas демонстрирует значительное улучшение производительности по сравнению с предыдущими моделями на шести тестовых наборах, охватывающих мобильные, настольные и веб-платформы.'}, 'en': {'title': 'Empowering Open-Source GUI Agents with OS-Atlas', 'desc': "This paper introduces OS-Atlas, an open-source foundational model designed for GUI grounding and Out-Of-Distribution (OOD) tasks. It addresses the performance gap between commercial Vision-Language Models (VLMs) and open-source alternatives by providing a comprehensive toolkit for synthesizing GUI grounding data across various platforms. The authors present the largest open-source cross-platform GUI grounding dataset, featuring over 13 million GUI elements, which enhances the model's ability to understand and generalize from GUI screenshots. Extensive evaluations show that OS-Atlas outperforms previous models, offering insights for further advancements in open-source VLM capabilities."}, 'zh': {'title': '开源GUI模型OS-Atlas：提升界面理解能力的创新之路', 'desc': '本论文介绍了OS-Atlas，一个开源的GUI动作模型，专注于GUI定位和超出分布（OOD）任务。我们开发了一个工具包，可以在多个平台上合成GUI定位数据，包括Windows、Linux、MacOS、Android和网页。OS-Atlas利用超过1300万个GUI元素的数据集，结合创新的模型训练方法，显著提高了对GUI截图的理解能力。通过在六个基准测试中进行广泛评估，OS-Atlas在移动、桌面和网页平台上表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2411.00027', 'title': 'Personalization of Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2411.00027', 'abstract': 'Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.', 'score': 31, 'issue_id': 409, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'a190b2e727d2d0ad', 'authors': ['Zhehao Zhang', 'Ryan A. Rossi', 'Branislav Kveton', 'Yijia Shao', 'Diyi Yang', 'Hamed Zamani', 'Franck Dernoncourt', 'Joe Barrow', 'Tong Yu', 'Sungchul Kim', 'Ruiyi Zhang', 'Jiuxiang Gu', 'Tyler Derr', 'Hongjie Chen', 'Junda Wu', 'Xiang Chen', 'Zichao Wang', 'Subrata Mitra', 'Nedim Lipka', 'Nesreen Ahmed', 'Yu Wang'], 'affiliations': ['Dartmouth College', 'Adobe Research', 'Stanford University', 'University of Massachusetts Amherst', 'Pattern Data', 'Vanderbilt University', 'Dolby Research', 'University of California San Diego', 'Cisco Research', 'University of Oregon'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00027.jpg', 'data': {'categories': ['#benchmark', '#training', '#dataset', '#survey', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Объединяя подходы: комплексный взгляд на персонализацию больших языковых моделей', 'desc': 'Статья посвящена персонализации больших языковых моделей (LLM) и объединяет два основных направления исследований в этой области. Авторы предлагают таксономию использования персонализированных LLM, формализуют основы и расширяют понятия персонализации. Они систематизируют литературу по различным аспектам, включая методы персонализации, наборы данных и способы оценки. В работе также выделяются нерешенные проблемы и открытые вопросы в данной области исследований.'}, 'en': {'title': 'Bridging Personalization Gaps in Large Language Models', 'desc': 'This paper addresses the growing need for personalization in Large Language Models (LLMs) by creating a comprehensive framework that connects personalized text generation with applications like recommendation systems. It introduces a taxonomy that categorizes various aspects of personalized LLMs, including techniques, datasets, and evaluation methods. The authors formalize the concept of personalization in LLMs, discussing its different dimensions and the challenges faced in this area. By synthesizing existing research and identifying open problems, the paper serves as a guide for researchers and practitioners interested in the personalization of LLMs.'}, 'zh': {'title': '统一个性化大型语言模型的研究', 'desc': '本文探讨了大型语言模型（LLMs）个性化的重要性及其应用。我们首次将个性化文本生成与个性化相关的下游应用（如推荐系统）结合起来，提出了个性化LLMs的分类法。文章对个性化LLMs的基础进行了形式化定义，并讨论了个性化的不同方面、使用场景和需求。最后，我们总结了现有文献，并指出了个性化LLMs面临的挑战和未解决的问题，以帮助研究人员和从业者更好地理解这一领域。'}}}, {'id': 'https://huggingface.co/papers/2411.00322', 'title': 'Constant Acceleration Flow', 'url': 'https://huggingface.co/papers/2411.00322', 'abstract': 'Rectified flow and reflow procedures have significantly advanced fast generation by progressively straightening ordinary differential equation (ODE) flows. They operate under the assumption that image and noise pairs, known as couplings, can be approximated by straight trajectories with constant velocity. However, we observe that modeling with constant velocity and using reflow procedures have limitations in accurately learning straight trajectories between pairs, resulting in suboptimal performance in few-step generation. To address these limitations, we introduce Constant Acceleration Flow (CAF), a novel framework based on a simple constant acceleration equation. CAF introduces acceleration as an additional learnable variable, allowing for more expressive and accurate estimation of the ODE flow. Moreover, we propose two techniques to further improve estimation accuracy: initial velocity conditioning for the acceleration model and a reflow process for the initial velocity. Our comprehensive studies on toy datasets, CIFAR-10, and ImageNet 64x64 demonstrate that CAF outperforms state-of-the-art baselines for one-step generation. We also show that CAF dramatically improves few-step coupling preservation and inversion over Rectified flow. Code is available at https://github.com/mlvlab/CAF{https://github.com/mlvlab/CAF}.', 'score': 22, 'issue_id': 412, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'edca1b3005d37bab', 'authors': ['Dogyun Park', 'Sojin Lee', 'Sihyeon Kim', 'Taehoon Lee', 'Youngjoon Hong', 'Hyunwoo J. Kim'], 'affiliations': ['Korea University', 'KAIST'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00322.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'CAF: Ускоряем генерацию изображений с помощью постоянного ускорения', 'desc': 'Статья представляет новый подход к ускорению генерации изображений в машинном обучении, называемый Constant Acceleration Flow (CAF). В отличие от существующих методов, CAF использует модель постоянного ускорения вместо постоянной скорости для более точного моделирования траекторий между парами изображений и шума. Авторы вводят ускорение как дополнительную обучаемую переменную и предлагают техники улучшения точности оценки, включая обусловливание начальной скорости и процесс рефлоу. Эксперименты на различных наборах данных показывают превосходство CAF над современными методами в одношаговой генерации и сохранении связей при генерации за несколько шагов.'}, 'en': {'title': 'Accelerating Image Generation with Constant Acceleration Flow', 'desc': 'This paper presents a new method called Constant Acceleration Flow (CAF) to improve the generation of images using ordinary differential equations (ODEs). Traditional methods assume that the flow between images and noise can be modeled as straight lines moving at a constant speed, which can lead to inaccuracies. CAF enhances this by introducing acceleration as a learnable variable, allowing for more flexible and precise modeling of the flow. The authors demonstrate that CAF outperforms existing methods in generating images with fewer steps while maintaining better quality and accuracy.'}, 'zh': {'title': '常加速度流：提升图像生成的新方法', 'desc': '本文提出了一种新的框架，称为常加速度流（CAF），用于改进图像生成过程。CAF通过引入加速度作为可学习变量，能够更准确地估计常微分方程（ODE）流。与传统的直线轨迹假设不同，CAF允许在生成过程中考虑加速度，从而提高生成质量。实验结果表明，CAF在少步生成和耦合保持方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2410.23266', 'title': 'TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models', 'url': 'https://huggingface.co/papers/2410.23266', 'abstract': "Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, how well do the models truly perform visual temporal reasoning? Our study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. To systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame Information Disparity. Following these principles, we introduce TOMATO, Temporal Reasoning Multimodal Evaluation, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. TOMATO comprises 1,484 carefully curated, human-annotated questions spanning six tasks (i.e., action count, direction, rotation, shape & trend, velocity & frequency, and visual cues), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. Our comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model. Moreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence. We believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending human world dynamics through the video modality.", 'score': 19, 'issue_id': 416, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '2743c77af808246f', 'authors': ['Ziyao Shangguan', 'Chuhan Li', 'Yuxuan Ding', 'Yanan Zheng', 'Yilun Zhao', 'Tesca Fitzgerald', 'Arman Cohan'], 'affiliations': ['Yale University', 'Allen Institute for AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23266.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#graphs', '#video', '#multimodal', '#survey'], 'emoji': '🍅', 'ru': {'title': 'TOMATO: Новый стандарт для оценки временного рассуждения в видеоанализе', 'desc': 'Исследователи разработали новый бенчмарк TOMATO для оценки способностей мультимодальных фундаментальных моделей (МФМ) к временному рассуждению при анализе видео. Бенчмарк включает 1484 тщательно отобранных вопроса по шести задачам, применяемых к 1417 видео различных сценариев. Авторы выявили значительный разрыв в производительности между человеком и лучшей моделью, а также фундаментальные ограничения существующих МФМ в интерпретации последовательности кадров. TOMATO призван стать важным инструментом для оценки следующего поколения МФМ и стимулировать развитие систем ИИ, способных понимать динамику человеческого мира через видеомодальность.'}, 'en': {'title': 'TOMATO: A New Benchmark for Evaluating Temporal Reasoning in Video Understanding', 'desc': 'This paper investigates the true capabilities of Multimodal Foundation Models (MFMs) in visual temporal reasoning for video understanding. The authors argue that existing benchmarks may overstate the performance of these models, as many tasks can be solved using only a few frames or even out-of-order frames. To address this, they introduce TOMATO, a new benchmark designed to rigorously evaluate MFMs based on three principles: Multi-Frame Gain, Frame Order Sensitivity, and Frame Information Disparity. Their findings reveal a significant performance gap between human understanding and model capabilities, highlighting the need for improved temporal reasoning in future AI systems.'}, 'zh': {'title': '评估多模态模型的时间推理能力', 'desc': '本研究探讨了多模态基础模型（MFM）在视频理解中的视觉时间推理能力。我们发现，现有基准测试可能高估了这些模型的表现，因为许多问题可以通过少量或无序的帧来解决。为此，我们提出了三个原则和相应的评估指标，并引入了TOMATO基准，以系统评估MFM在视频理解中的时间推理能力。我们的评估显示，当前模型与人类表现之间存在57.3%的差距，揭示了MFM在处理连续帧时的基本局限性。'}}}, {'id': 'https://huggingface.co/papers/2411.00776', 'title': 'Randomized Autoregressive Visual Generation', 'url': 'https://huggingface.co/papers/2411.00776', 'abstract': "This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer", 'score': 17, 'issue_id': 408, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '0cc2c0f19f735f79', 'authors': ['Qihang Yu', 'Ju He', 'Xueqing Deng', 'Xiaohui Shen', 'Liang-Chieh Chen'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00776.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#optimization', '#open_source', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Случайная перестановка для улучшения генерации изображений', 'desc': 'Статья представляет метод Randomized AutoRegressive modeling (RAR) для генерации изображений. RAR использует случайную перестановку входной последовательности во время обучения авторегрессионной модели, что позволяет учитывать двунаправленный контекст. Метод сохраняет совместимость с языковыми моделями и достигает нового state-of-the-art результата на бенчмарке ImageNet-256 с показателем FID 1.48. RAR превосходит как авторегрессионные, так и диффузионные методы генерации изображений.'}, 'en': {'title': 'Revolutionizing Image Generation with Randomized AutoRegressive Modeling', 'desc': 'This paper introduces Randomized AutoRegressive modeling (RAR), a novel approach for generating images that enhances performance while remaining compatible with existing language modeling techniques. RAR employs a unique training method where the input sequence is randomly shuffled during the autoregressive training process, allowing the model to learn from various factorization orders. This strategy helps the model to better understand and utilize bidirectional contexts, leading to improved image generation capabilities. The results show that RAR achieves a remarkable FID score of 1.48 on the ImageNet-256 benchmark, outperforming previous state-of-the-art methods in both autoregressive and diffusion-based image generation.'}, 'zh': {'title': '随机自回归建模：图像生成的新突破', 'desc': '本文提出了一种随机自回归建模（RAR）方法用于视觉生成，在图像生成任务上设定了新的最先进性能，同时与语言建模框架完全兼容。RAR方法简单：在标准的自回归训练过程中，输入序列通常按光栅形式排列，但以概率r随机打乱为不同的因子化顺序，r从1开始，随着训练线性衰减到0。这种退火训练策略使模型能够学习最大化所有因子化顺序的期望似然，从而有效提高模型建模双向上下文的能力。RAR保持了自回归建模框架的完整性，确保与语言建模的完全兼容，同时在图像生成中显著提高了性能。'}}}, {'id': 'https://huggingface.co/papers/2411.00660', 'title': 'Physics in Next-token Prediction', 'url': 'https://huggingface.co/papers/2411.00660', 'abstract': "We discovered the underlying physics in Next-token Prediction (NTP). We identified the law of information conservation within NTP and proposed the First Law of Information Capacity (IC-1), demonstrating that the essence of intelligence emergence in auto-regressive models is fundamentally a process of information transfer. We also introduced Landauer's Principle into NTP, formulating the Second Law of Information Capacity (IC-2), which establishes the relationship between auto-regressive model training and energy consumption. Additionally, we presented several corollaries, which hold practical significance for production practices. Finally, we validated the compatibility and complementarity of our findings with existing theories.", 'score': 14, 'issue_id': 416, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '9114a8de1a432c2d', 'authors': ['Hongjun An', 'Yiliang Song', 'Xuelong Li'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00660.jpg', 'data': {'categories': ['#science', '#optimization', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Физика информации раскрывает тайны искусственного интеллекта', 'desc': 'Исследователи обнаружили фундаментальные физические принципы в задаче предсказания следующего токена (NTP). Они сформулировали Первый закон информационной емкости, показывающий, что возникновение интеллекта в авторегрессионных моделях по сути является процессом передачи информации. Введя принцип Ландауэра в NTP, ученые также сформулировали Второй закон информационной емкости, связывающий обучение авторегрессионных моделей с энергопотреблением. Исследование предлагает ряд практических следствий и согласуется с существующими теориями в области машинного обучения.'}, 'en': {'title': 'Unveiling the Physics of Next-Token Prediction', 'desc': "This paper explores the physics behind Next-token Prediction (NTP) in machine learning. It introduces the First Law of Information Capacity (IC-1), which highlights how intelligence in auto-regressive models arises from information transfer. The authors also apply Landauer's Principle to NTP, leading to the Second Law of Information Capacity (IC-2), linking model training to energy consumption. The findings are supported by practical corollaries and align with existing theories in the field."}, 'zh': {'title': '揭示自回归模型中的信息与能量关系', 'desc': '我们发现了下一步预测（NTP）中的基本物理原理。我们识别了NTP中的信息守恒定律，并提出了信息容量第一定律（IC-1），证明了自回归模型中智能出现的本质是信息传递的过程。我们还将朗道原理引入NTP，制定了信息容量第二定律（IC-2），建立了自回归模型训练与能量消耗之间的关系。此外，我们提出了几个具有实际意义的推论，验证了我们的发现与现有理论的兼容性和互补性。'}}}, {'id': 'https://huggingface.co/papers/2410.24159', 'title': 'GPT or BERT: why not both?', 'url': 'https://huggingface.co/papers/2410.24159', 'abstract': 'We present a simple way to merge masked language modeling with causal language modeling. This hybrid training objective results in a model that combines the strengths of both modeling paradigms within a single transformer stack: GPT-BERT can be transparently used like any standard causal or masked language model. We test the pretraining process that enables this flexible behavior on the BabyLM Challenge 2024. The results show that the hybrid pretraining outperforms masked-only or causal-only models. We openly release the models, training corpora and code.', 'score': 13, 'issue_id': 415, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'f46bbe7c538f7a87', 'authors': ['Lucas Georges Gabriel Charpentier', 'David Samuel'], 'affiliations': ['University of Oslo, Language Technology Group'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24159.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Гибридное языковое моделирование: лучшее из двух миров', 'desc': 'Исследователи представили новый метод объединения маскированного и причинно-следственного языкового моделирования. Результатом стала модель GPT-BERT, сочетающая преимущества обоих подходов в единой трансформерной архитектуре. Эксперименты в рамках BabyLM Challenge 2024 показали, что гибридное предобучение превосходит модели, использующие только один из методов. Авторы открыто публикуют модели, обучающие корпуса и код.'}, 'en': {'title': 'Merging Masked and Causal Language Models for Enhanced Performance', 'desc': 'This paper introduces a novel approach that merges masked language modeling (MLM) with causal language modeling (CLM) into a single transformer architecture. The resulting model, named GPT-BERT, leverages the advantages of both paradigms, allowing it to function effectively as either a standard masked or causal language model. The authors evaluate this hybrid training method on the BabyLM Challenge 2024, demonstrating that it outperforms models trained exclusively with either MLM or CLM. Additionally, they provide open access to the models, training data, and code to facilitate further research.'}, 'zh': {'title': '混合预训练，模型性能双赢！', 'desc': '本文提出了一种将掩码语言建模与因果语言建模相结合的简单方法。这种混合训练目标使得模型能够在单个变换器架构中结合两种建模范式的优点。我们在2024年BabyLM挑战赛中测试了这种灵活行为的预训练过程。结果表明，混合预训练的性能优于仅使用掩码或仅使用因果的模型。'}}}, {'id': 'https://huggingface.co/papers/2410.22370', 'title': 'Survey of User Interface Design and Interaction Techniques in Generative AI Applications', 'url': 'https://huggingface.co/papers/2410.22370', 'abstract': 'The applications of generative AI have become extremely impressive, and the interplay between users and AI is even more so. Current human-AI interaction literature has taken a broad look at how humans interact with generative AI, but it lacks specificity regarding the user interface designs and patterns used to create these applications. Therefore, we present a survey that comprehensively presents taxonomies of how a human interacts with AI and the user interaction patterns designed to meet the needs of a variety of relevant use cases. We focus primarily on user-guided interactions, surveying interactions that are initiated by the user and do not include any implicit signals given by the user. With this survey, we aim to create a compendium of different user-interaction patterns that can be used as a reference for designers and developers alike. In doing so, we also strive to lower the entry barrier for those attempting to learn more about the design of generative AI applications.', 'score': 11, 'issue_id': 409, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '9701ceb4e85eeeba', 'authors': ['Reuben Luera', 'Ryan A. Rossi', 'Alexa Siu', 'Franck Dernoncourt', 'Tong Yu', 'Sungchul Kim', 'Ruiyi Zhang', 'Xiang Chen', 'Hanieh Salehy', 'Jian Zhao', 'Samyadeep Basu', 'Puneet Mathur', 'Nedim Lipka'], 'affiliations': ['University of California San Diego', 'Adobe Research', 'University of Waterloo', 'University of Maryland, College Park'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22370.jpg', 'data': {'categories': ['#agents', '#survey', '#architecture', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Путеводитель по взаимодействию человека и ИИ', 'desc': 'Статья представляет обзор таксономий взаимодействия человека с генеративным ИИ и паттернов пользовательского интерфейса для различных сценариев использования. Авторы фокусируются на взаимодействиях, инициируемых пользователем, без учета неявных сигналов. Цель работы - создать справочник паттернов взаимодействия для дизайнеров и разработчиков приложений генеративного ИИ. Исследование направлено на снижение входного барьера для тех, кто хочет изучить дизайн приложений генеративного ИИ.'}, 'en': {'title': 'Enhancing User Interaction with Generative AI', 'desc': 'This paper surveys the ways users interact with generative AI, focusing specifically on user-guided interactions. It identifies and categorizes various user interface designs and interaction patterns that cater to different use cases. By providing a comprehensive taxonomy, the authors aim to serve as a reference for designers and developers in creating more effective generative AI applications. The goal is to make it easier for newcomers to understand and engage with the design aspects of these technologies.'}, 'zh': {'title': '提升人机交互，设计更智能的生成式AI应用', 'desc': '本论文探讨了生成式人工智能（AI）与用户之间的互动，强调了用户界面设计的重要性。我们提供了一份全面的调查，分类了人类与AI的互动方式，特别关注用户主导的交互模式。通过这项调查，我们希望为设计师和开发者提供不同的用户交互模式参考，降低学习生成式AI应用设计的门槛。最终目标是提升人机交互的效率和用户体验。'}}}, {'id': 'https://huggingface.co/papers/2410.23775', 'title': 'In-Context LoRA for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2410.23775', 'abstract': 'Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20sim 100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. We release our code, data, and models at https://github.com/ali-vilab/In-Context-LoRA', 'score': 10, 'issue_id': 407, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '748dab03a37a21a4', 'authors': ['Lianghua Huang', 'Wei Wang', 'Zhi-Fan Wu', 'Yupeng Shi', 'Huanzhang Dou', 'Chen Liang', 'Yutong Feng', 'Yu Liu', 'Jingren Zhou'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23775.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Раскрытие скрытого потенциала DiT для многозадачной генерации изображений', 'desc': 'Исследование предлагает новый подход к использованию диффузионных трансформеров (DiT) для генерации изображений в различных задачах. Авторы обнаружили, что существующие модели DiT для преобразования текста в изображение уже обладают способностью к контекстной генерации без дополнительной настройки. На основе этого наблюдения они разработали простой pipeline, включающий конкатенацию изображений, совместное создание подписей и применение LoRA-тюнинга на небольших датасетах. Предложенный метод, названный IC-LoRA, позволяет генерировать высококачественные наборы изображений, лучше соответствующие заданным промптам.'}, 'en': {'title': 'Unlocking In-Context Generation with IC-LoRA', 'desc': 'This paper investigates the use of diffusion transformers (DiTs) for generating images without being tied to specific tasks. The authors propose that DiTs can generate images effectively with minimal adjustments, leveraging their inherent in-context generation capabilities. They introduce a new method called In-Context LoRA (IC-LoRA), which simplifies the process by concatenating images and using joint captioning, along with small dataset tuning. This approach enhances the quality of generated images while maintaining a flexible architecture that can adapt to various tasks without extensive retraining.'}, 'zh': {'title': '激活上下文生成能力，提升图像生成质量', 'desc': '本研究探讨了扩散变换器（DiTs）在无任务特定的图像生成中的应用。我们提出，文本到图像的DiTs本身具备上下文生成能力，只需少量调整即可激活。通过实验，我们展示了现有的文本到图像DiTs能够在不调整的情况下有效进行上下文生成。我们提出了一种简单的流程，利用DiTs的上下文能力，生成高保真度的图像集，且不需要对原始模型进行修改。'}}}, {'id': 'https://huggingface.co/papers/2411.00771', 'title': 'CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes', 'url': 'https://huggingface.co/papers/2411.00771', 'abstract': 'Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10times compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs. The project page is available at https://dekuliutesla.github.io/CityGaussianV2/.', 'score': 9, 'issue_id': 415, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '00c1f9c65cf89cfe', 'authors': ['Yang Liu', 'Chuanchen Luo', 'Zhongkai Mao', 'Junran Peng', 'Zhaoxiang Zhang'], 'affiliations': ['NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences', 'Centre for Artificial Intelligence and Robotic, HKISI', 'Shandong University', 'University of Science and Technology Beijing'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00771.jpg', 'data': {'categories': ['#benchmark', '#graphs', '#optimization', '#training', '#3d'], 'emoji': '🏙️', 'ru': {'title': 'CityGaussianV2: Эффективная реконструкция крупномасштабных сцен с помощью улучшенного Gaussian Splatting', 'desc': 'Статья представляет CityGaussianV2 - новый подход к реконструкции крупномасштабных сцен, решающий проблемы геометрической точности и эффективности в 3D Gaussian Splatting. Авторы внедряют технику уплотнения на основе декомпозированного градиента и регрессии глубины для устранения размытых артефактов и ускорения сходимости. Для масштабирования вводится фильтр удлинения, снижающий взрывной рост количества гауссианов. Оптимизированный конвейер CityGaussian позволяет достичь 10-кратного сжатия, экономии времени обучения на 25% и снижения использования памяти на 50%.'}, 'en': {'title': 'Revolutionizing 3D Scene Reconstruction with CityGaussianV2', 'desc': 'This paper introduces CityGaussianV2, a new method for reconstructing large-scale 3D scenes using Gaussian splatting techniques. It addresses challenges in geometric accuracy and efficiency that arise from the unstructured nature of traditional 3D Gaussian Splatting. The authors implement a novel densification and depth regression technique to improve image clarity and speed up the training process. Additionally, they optimize the training pipeline to reduce memory usage and training time significantly while maintaining high visual quality and accuracy.'}, 'zh': {'title': '高效精准的大规模场景重建新方法', 'desc': '3D高斯点云（3DGS）在辐射场重建中取得了显著进展，但在复杂场景中准确表示表面仍然是一个挑战。本文提出了CityGaussianV2，一种针对大规模场景重建的新方法，解决了几何精度和效率的问题。我们采用分解梯度的密集化和深度回归技术，消除模糊伪影并加速收敛。通过引入延伸滤波器，我们有效地减少了高斯数量的爆炸，同时优化了CityGaussian管道，实现了训练时间和内存使用的显著节省。'}}}, {'id': 'https://huggingface.co/papers/2411.00680', 'title': 'Zipfian Whitening', 'url': 'https://huggingface.co/papers/2411.00680', 'abstract': "The word embedding space in neural models is skewed, and correcting this can improve task performance. We point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are uniform; in reality, word frequencies follow a highly non-uniform distribution, known as Zipf's law. Surprisingly, simply performing PCA whitening weighted by the empirical word frequency that follows Zipf's law significantly improves task performance, surpassing established baselines. From a theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures. By adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective, and in terms of the loss functions for imbalanced classification. Additionally, our theory corroborates that popular natural language processing methods, such as skip-gram negative sampling, WhiteningBERT, and headless language models, work well just because their word embeddings encode the empirical word frequency into the underlying probabilistic model.", 'score': 9, 'issue_id': 413, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '26e33c177a131240', 'authors': ['Sho Yokoi', 'Han Bao', 'Hiroto Kurita', 'Hidetoshi Shimodaira'], 'affiliations': ['Tohoku University', 'RIKEN', 'Kyoto University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00680.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#math', '#data', '#architecture'], 'emoji': '📊', 'ru': {'title': 'Улучшение векторных представлений слов с учетом закона Ципфа', 'desc': 'Статья посвящена проблеме асимметрии пространства векторных представлений слов в нейронных моделях. Авторы предлагают метод коррекции этой асимметрии с учетом закона Ципфа о распределении частот слов. Простое применение PCA отбеливания, взвешенного по эмпирической частоте слов, значительно улучшает производительность модели на различных задачах. Теоретический анализ показывает, что этот подход естественным образом подчеркивает информативные низкочастотные слова и объясняет эффективность популярных методов обработки естественного языка.'}, 'en': {'title': 'Correcting Skewness in Word Embeddings for Better Performance', 'desc': "This paper discusses how the word embedding space in neural models can be improved by addressing its skewness. It highlights that many existing methods assume uniform word frequencies, while in reality, word frequencies follow Zipf's law, which is highly non-uniform. The authors propose using PCA whitening that is weighted by empirical word frequencies, leading to significant improvements in task performance. Their theoretical framework categorizes word representations based on exponential families, emphasizing the importance of low-frequency words and providing insights into popular NLP methods."}, 'zh': {'title': '校正词嵌入空间，提升任务性能！', 'desc': '这篇论文探讨了神经模型中的词嵌入空间偏斜问题，并提出了通过校正这一偏斜来提高任务性能的方法。研究表明，现有的许多模型假设词频是均匀分布的，但实际上，词频遵循一种称为齐夫定律的高度非均匀分布。通过对词频进行加权的主成分分析（PCA）白化，显著提升了任务性能，超越了已有的基准。理论上，我们的方法与现有方法可以清晰地分类，强调了低频词的重要性，并解释了流行的自然语言处理方法为何有效。'}}}, {'id': 'https://huggingface.co/papers/2411.00412', 'title': 'Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation', 'url': 'https://huggingface.co/papers/2411.00412', 'abstract': "Large Language Models (LLMs) demonstrate promising capabilities in solving simple scientific problems but often produce hallucinations for complex ones. While integrating LLMs with tools can increase reliability, this approach typically results in over-reliance on tools, diminishing the model's ability to solve simple problems through basic reasoning. In contrast, human experts first assess problem complexity using domain knowledge before choosing an appropriate solution approach. Inspired by this human problem-solving process, we propose a novel two-component fine-tuning method. In the first component World Knowledge Distillation (WKD), LLMs learn directly from solutions generated using tool's information to internalize domain knowledge. In the second component Tool Usage Adaptation (TUA), we partition problems into easy and hard categories based on the model's direct answering accuracy. While maintaining the same alignment target for easy problems as in WKD, we train the model to intelligently switch to tool usage for more challenging problems. We validate our method on six scientific benchmark datasets, spanning mathematics, climate science and epidemiology. On average, our models demonstrate a 28.18% improvement in answer accuracy and a 13.89% increase in tool usage precision across all datasets, surpassing state-of-the-art models including GPT-4o and Claude-3.5.", 'score': 9, 'issue_id': 407, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '27e4deefc7d09df0', 'authors': ['Bohan Lyu', 'Yadi Cao', 'Duncan Watson-Parris', 'Leon Bergen', 'Taylor Berg-Kirkpatrick', 'Rose Yu'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00412.jpg', 'data': {'categories': ['#science', '#reasoning', '#rl', '#hallucinations', '#benchmark', '#multilingual', '#math', '#training', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Умное переключение: как научить ИИ эффективно решать задачи разной сложности', 'desc': 'Исследование посвящено улучшению способности больших языковых моделей (LLM) решать научные задачи. Авторы предлагают двухкомпонентный метод дообучения: дистилляция мировых знаний и адаптация использования инструментов. Этот подход позволяет LLM эффективно решать простые задачи с помощью базовых рассуждений, а для сложных - прибегать к инструментам. Метод показал значительное улучшение точности ответов и точности использования инструментов на шести научных наборах данных.'}, 'en': {'title': 'Enhancing LLMs: Smart Tool Use for Complex Problems', 'desc': "This paper addresses the limitations of Large Language Models (LLMs) in solving complex scientific problems, which often lead to inaccuracies or 'hallucinations'. The authors propose a two-component fine-tuning method that mimics human problem-solving strategies by first assessing problem complexity. The first component, World Knowledge Distillation (WKD), allows LLMs to learn from solutions that utilize external tools, while the second component, Tool Usage Adaptation (TUA), helps the model categorize problems as easy or hard and decide when to use tools. The proposed method shows significant improvements in accuracy and tool usage precision across various scientific datasets, outperforming existing models."}, 'zh': {'title': '智能切换，提升模型解决问题的能力', 'desc': '大型语言模型（LLMs）在解决简单科学问题方面表现出色，但在复杂问题上常常出现幻觉。我们提出了一种新颖的双组件微调方法，模仿人类专家的解决问题过程。第一个组件是世界知识蒸馏（WKD），使LLMs从工具生成的解决方案中学习领域知识。第二个组件是工具使用适应（TUA），根据模型的直接回答准确性将问题分为简单和困难两类，从而提高模型在复杂问题上的工具使用能力。'}}}, {'id': 'https://huggingface.co/papers/2411.00225', 'title': 'Fashion-VDM: Video Diffusion Model for Virtual Try-On', 'url': 'https://huggingface.co/papers/2411.00225', 'abstract': "We present Fashion-VDM, a video diffusion model (VDM) for generating virtual try-on videos. Given an input garment image and person video, our method aims to generate a high-quality try-on video of the person wearing the given garment, while preserving the person's identity and motion. Image-based virtual try-on has shown impressive results; however, existing video virtual try-on (VVT) methods are still lacking garment details and temporal consistency. To address these issues, we propose a diffusion-based architecture for video virtual try-on, split classifier-free guidance for increased control over the conditioning inputs, and a progressive temporal training strategy for single-pass 64-frame, 512px video generation. We also demonstrate the effectiveness of joint image-video training for video try-on, especially when video data is limited. Our qualitative and quantitative experiments show that our approach sets the new state-of-the-art for video virtual try-on. For additional results, visit our project page: https://johannakarras.github.io/Fashion-VDM.", 'score': 7, 'issue_id': 418, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'a412815c3df113c6', 'authors': ['Johanna Karras', 'Yingwei Li', 'Nan Liu', 'Luyang Zhu', 'Innfarn Yoo', 'Andreas Lugmayr', 'Chris Lee', 'Ira Kemelmacher-Shlizerman'], 'affiliations': ['Google Research, University of Washington, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00225.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#video', '#optimization', '#training', '#games', '#architecture'], 'emoji': '👚', 'ru': {'title': 'Виртуальная примерка одежды на видео с помощью диффузионных моделей', 'desc': 'Fashion-VDM - это модель видеодиффузии для создания виртуальных примерочных видео. Модель генерирует высококачественное видео человека, одетого в заданный предмет одежды, сохраняя при этом личность и движения человека. Авторы предлагают архитектуру на основе диффузии, разделенное беcклассификаторное управление и прогрессивную стратегию временного обучения для генерации 64-кадровых видео разрешением 512 пикселей за один проход. Эксперименты показывают, что подход устанавливает новый уровень качества в задаче виртуальной примерки одежды на видео.'}, 'en': {'title': 'Revolutionizing Virtual Try-Ons with Fashion-VDM!', 'desc': 'Fashion-VDM is a novel video diffusion model designed to create realistic virtual try-on videos by combining garment images with person videos. The model focuses on maintaining the identity and motion of the person while accurately displaying the garment. It addresses challenges in existing video virtual try-on methods, such as lack of detail and temporal consistency, by utilizing a diffusion-based architecture and a progressive training strategy. Our experiments demonstrate that Fashion-VDM achieves state-of-the-art results in video virtual try-on, even with limited video data.'}, 'zh': {'title': 'Fashion-VDM：视频虚拟试穿的新突破', 'desc': '我们提出了Fashion-VDM，这是一种用于生成虚拟试穿视频的视频扩散模型。该方法可以根据输入的服装图像和人物视频，生成高质量的试穿视频，同时保持人物的身份和动作。与现有的视频虚拟试穿方法相比，我们的方法在服装细节和时间一致性方面有显著提升。我们的实验结果表明，Fashion-VDM在视频虚拟试穿领域达到了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2411.00762', 'title': 'Face Anonymization Made Simple', 'url': 'https://huggingface.co/papers/2411.00762', 'abstract': 'Current face anonymization techniques often depend on identity loss calculated by face recognition models, which can be inaccurate and unreliable. Additionally, many methods require supplementary data such as facial landmarks and masks to guide the synthesis process. In contrast, our approach uses diffusion models with only a reconstruction loss, eliminating the need for facial landmarks or masks while still producing images with intricate, fine-grained details. We validated our results on two public benchmarks through both quantitative and qualitative evaluations. Our model achieves state-of-the-art performance in three key areas: identity anonymization, facial attribute preservation, and image quality. Beyond its primary function of anonymization, our model can also perform face swapping tasks by incorporating an additional facial image as input, demonstrating its versatility and potential for diverse applications. Our code and models are available at https://github.com/hanweikung/face_anon_simple .', 'score': 7, 'issue_id': 415, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': '0948ed1ff23fd58b', 'authors': ['Han-Wei Kung', 'Tuomas Varanka', 'Sanjay Saha', 'Terence Sim', 'Nicu Sebe'], 'affiliations': ['University of Trento', 'University of Oulu', 'National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00762.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#open_source', '#architecture'], 'emoji': '🎭', 'ru': {'title': 'Простая и эффективная анонимизация лиц с помощью диффузионных моделей', 'desc': 'Эта статья представляет новый метод анонимизации лиц с использованием диффузионных моделей. В отличие от существующих подходов, метод не требует дополнительных данных вроде лицевых ориентиров или масок. Модель достигает наилучших результатов в анонимизации личности, сохранении атрибутов лица и качестве изображения. Кроме того, она может выполнять задачи по замене лиц, демонстрируя свою универсальность.'}, 'en': {'title': 'Revolutionizing Face Anonymization with Diffusion Models', 'desc': "This paper presents a novel face anonymization technique that utilizes diffusion models, focusing solely on reconstruction loss without the need for additional data like facial landmarks or masks. The method achieves high-quality image generation while effectively anonymizing identities and preserving facial attributes. The authors demonstrate the model's performance through rigorous evaluations on public benchmarks, showcasing its state-of-the-art results in identity anonymization and image quality. Additionally, the model's versatility is highlighted by its capability to perform face swapping tasks, making it suitable for various applications."}, 'zh': {'title': '创新的面部匿名化与交换技术', 'desc': '当前的面部匿名化技术通常依赖于面部识别模型计算的身份损失，这可能不够准确和可靠。我们的方法使用扩散模型，仅依赖重建损失，省去了面部特征点或面具的需求，同时仍能生成细致的图像。我们的模型在身份匿名化、面部属性保留和图像质量三个关键领域达到了最先进的性能。除了匿名化功能外，我们的模型还可以通过添加额外的面部图像作为输入来执行面部交换任务，展示了其多样性和潜在应用。'}}}, {'id': 'https://huggingface.co/papers/2411.00233', 'title': 'SambaMixer: State of Health Prediction of Li-ion Batteries using Mamba State Space Models', 'url': 'https://huggingface.co/papers/2411.00233', 'abstract': 'The state of health (SOH) of a Li-ion battery is a critical parameter that determines the remaining capacity and the remaining lifetime of the battery. In this paper, we propose SambaMixer a novel structured state space model (SSM) for predicting the state of health of Li-ion batteries. The proposed SSM is based on the MambaMixer architecture, which is designed to handle multi-variate time signals. We evaluate our model on the NASA battery discharge dataset and show that our model outperforms the state-of-the-art on this dataset. We further introduce a novel anchor-based resampling method which ensures time signals are of the expected length while also serving as augmentation technique. Finally, we condition prediction on the sample time and the cycle time difference using positional encodings to improve the performance of our model and to learn recuperation effects. Our results proof that our model is able to predict the SOH of Li-ion batteries with high accuracy and robustness.', 'score': 7, 'issue_id': 410, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '6361ca66f5ca137f', 'authors': ['José Ignacio Olalde-Verano', 'Sascha Kirch', 'Clara Pérez-Molina', 'Sergio Martin'], 'affiliations': ['UNED - Universidad Nacional de Educacion Distancia, Madrid, Spain'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00233.jpg', 'data': {'categories': ['#science', '#optimization', '#healthcare', '#training', '#dataset', '#architecture'], 'emoji': '🔋', 'ru': {'title': 'Точное прогнозирование срока службы аккумуляторов с помощью глубокого обучения', 'desc': 'Статья представляет SambaMixer - новую модель структурированного пространства состояний для прогнозирования состояния здоровья литий-ионных аккумуляторов. Модель основана на архитектуре MambaMixer и способна обрабатывать многомерные временные сигналы. Авторы предлагают метод ресэмплинга на основе якорей для нормализации длины сигналов и аугментации данных. Использование позиционного кодирования времени выборки и разницы циклов позволяет модели учитывать эффекты восстановления аккумуляторов.'}, 'en': {'title': 'SambaMixer: Revolutionizing Li-ion Battery Health Prediction', 'desc': 'This paper introduces SambaMixer, a new structured state space model (SSM) designed to predict the state of health (SOH) of Li-ion batteries. The model utilizes the MambaMixer architecture to effectively process multi-variate time signals, enhancing prediction accuracy. It is evaluated against the NASA battery discharge dataset, demonstrating superior performance compared to existing methods. Additionally, the paper presents an innovative anchor-based resampling technique and employs positional encodings to improve predictions by accounting for time-related factors.'}, 'zh': {'title': '高效预测锂离子电池健康状态的新方法', 'desc': '本文提出了一种新颖的结构化状态空间模型（SSM），用于预测锂离子电池的健康状态（SOH）。该模型基于MambaMixer架构，能够处理多变量时间信号。我们在NASA电池放电数据集上评估了该模型，结果显示其性能优于现有的最先进方法。此外，我们引入了一种新颖的基于锚点的重采样方法，以确保时间信号的预期长度，并作为数据增强技术。'}}}, {'id': 'https://huggingface.co/papers/2410.22901', 'title': 'HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models', 'url': 'https://huggingface.co/papers/2410.22901', 'abstract': 'We propose an effective method for inserting adapters into text-to-image foundation models, which enables the execution of complex downstream tasks while preserving the generalization ability of the base model. The core idea of this method is to optimize the attention mechanism related to 2D feature maps, which enhances the performance of the adapter. This approach was validated on the task of meme video generation and achieved significant results. We hope this work can provide insights for post-training tasks of large text-to-image models. Additionally, as this method demonstrates good compatibility with SD1.5 derivative models, it holds certain value for the open-source community. Therefore, we will release the related code (https://songkey.github.io/hellomeme).', 'score': 7, 'issue_id': 408, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '801963cbdcf75d7b', 'authors': ['Shengkai Zhang', 'Nianhong Jiao', 'Tian Li', 'Chaojie Yang', 'Chenhui Xue', 'Boya Niu', 'Jun Gao'], 'affiliations': ['HelloGroup Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22901.jpg', 'data': {'categories': ['#cv', '#video', '#training', '#open_source', '#games', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'Адаптеры для текст-в-изображение моделей: новый подход к генерации мемов', 'desc': 'Статья представляет эффективный метод внедрения адаптеров в базовые модели преобразования текста в изображение. Этот подход оптимизирует механизм внимания, связанный с двумерными картами признаков, что улучшает производительность адаптера. Метод был успешно применен для генерации мемов в видеоформате. Авторы отмечают совместимость метода с производными моделями SD1.5, что делает его ценным для сообщества открытого исходного кода.'}, 'en': {'title': 'Enhancing Text-to-Image Models with Adapter Integration', 'desc': 'This paper presents a novel method for integrating adapters into text-to-image foundation models, allowing them to perform complex tasks while maintaining their ability to generalize. The method focuses on optimizing the attention mechanism associated with 2D feature maps, which significantly boosts the performance of the adapters. The effectiveness of this approach was demonstrated through the task of meme video generation, yielding impressive results. The authors aim to contribute to the open-source community by sharing their code and providing insights for post-training tasks in large text-to-image models.'}, 'zh': {'title': '适配器插入：提升文本到图像模型的能力', 'desc': '本文提出了一种有效的方法，将适配器插入文本到图像的基础模型中，从而在执行复杂的下游任务时保持基础模型的泛化能力。该方法的核心思想是优化与二维特征图相关的注意力机制，从而增强适配器的性能。我们在生成表情包视频的任务上验证了该方法，并取得了显著的结果。希望这项工作能为大型文本到图像模型的后训练任务提供一些见解，并为开源社区带来价值。'}}}, {'id': 'https://huggingface.co/papers/2411.00369', 'title': 'GRS-QA -- Graph Reasoning-Structured Question Answering Dataset', 'url': 'https://huggingface.co/papers/2411.00369', 'abstract': 'Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.', 'score': 6, 'issue_id': 409, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'b3e4773e065d1bc1', 'authors': ['Anish Pahilajani', 'Devasha Trivedi', 'Jincen Shuai', 'Khin S. Yone', 'Samyak Rajesh Jain', 'Namyong Park', 'Ryan A. Rossi', 'Nesreen K. Ahmed', 'Franck Dernoncourt', 'Yu Wang'], 'affiliations': ['University of California Santa Cruz', 'Adobe Research', 'Cisco Outshift', 'University of Oregon'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00369.jpg', 'data': {'categories': ['#rag', '#reasoning', '#cv', '#graphs', '#dataset', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Графы рассуждений раскрывают возможности языковых моделей', 'desc': 'Статья представляет новый набор данных GRS-QA для оценки способностей больших языковых моделей (LLM) в многоходовых вопросно-ответных задачах. GRS-QA включает в себя как семантические контексты, так и структуры рассуждений для пар вопрос-ответ. Набор данных использует графы рассуждений, где узлы представляют текстовые контексты, а ребра обозначают логические связи. Эмпирический анализ показывает, что LLM по-разному справляются с вопросами, имеющими различные структуры рассуждений.'}, 'en': {'title': 'Unlocking LLMs: Understanding Reasoning Structures in Multi-Hop QA', 'desc': 'This paper discusses the performance of Large Language Models (LLMs) in multi-hop question-answering (M-QA) tasks, focusing on their reasoning abilities. The authors identify a gap in existing datasets that do not provide detailed reasoning structures, which are crucial for evaluating LLM performance. To fill this gap, they introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which features reasoning graphs that clearly outline the logical pathways for answering questions. Their analysis shows that LLMs exhibit varying performance based on the complexity of the reasoning structures involved, highlighting the importance of understanding both textual and semantic elements in M-QA.'}, 'zh': {'title': '揭示推理结构对LLM表现的影响', 'desc': '大型语言模型（LLMs）在多跳问答（M-QA）中表现出色，主要得益于其先进的推理能力。然而，LLM在多跳问答中的表现受固有推理结构的影响尚不明确，主要是因为缺乏提供细粒度推理结构的问答数据集。为了解决这个问题，我们引入了图推理结构问答数据集（GRS-QA），该数据集为问答对提供了语义上下文和推理结构。与现有的M-QA数据集不同，GRS-QA通过构建推理图来明确捕捉复杂的推理路径，节点表示文本上下文，边表示逻辑流，从而实现对LLM推理能力的细粒度评估。'}}}, {'id': 'https://huggingface.co/papers/2410.21157', 'title': 'M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation', 'url': 'https://huggingface.co/papers/2410.21157', 'abstract': 'Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.', 'score': 6, 'issue_id': 408, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'd6a0779456870cae', 'authors': ['Jiaheng Liu', 'Ken Deng', 'Congnan Liu', 'Jian Yang', 'Shukai Liu', 'He Zhu', 'Peng Zhao', 'Linzheng Chai', 'Yanan Wu', 'Ke Jin', 'Ge Zhang', 'Zekun Wang', 'Guoan Zhang', 'Bangyu Xiang', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Alibaba Group', 'University of Waterloo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21157.jpg', 'data': {'categories': ['#benchmark', '#multilingual', '#plp', '#data', '#dataset', '#transfer_learning', '#open_source'], 'emoji': '🖥️', 'ru': {'title': 'Многоязычный бенчмарк для оценки LLM в автодополнении кода', 'desc': 'Статья представляет новый бенчмарк M2RC-EVAL для оценки способностей больших языковых моделей (LLM) в задаче автодополнения кода на уровне репозитория. Бенчмарк охватывает 18 языков программирования и включает детальные аннотации для различных сценариев дополнения. Авторы также создали набор данных M2RC-INSTRUCT для улучшения возможностей существующих LLM в этой задаче. Эксперименты подтверждают эффективность предложенных инструментов.'}, 'en': {'title': 'Empowering Multilingual Code Completion with M2RC-EVAL and M2RC-INSTRUCT', 'desc': 'This paper introduces a new benchmark called M2RC-EVAL for repository-level code completion that supports 18 programming languages, addressing the limitations of existing benchmarks that only cover a few languages. It provides fine-grained annotations based on abstract syntax trees, allowing for a more detailed evaluation of code completion scenarios. Additionally, the authors present the M2RC-INSTRUCT dataset to enhance the performance of code Large Language Models (LLMs) in multilingual contexts. Experimental results show that both M2RC-EVAL and M2RC-INSTRUCT significantly improve the capabilities of existing code LLMs.'}, 'zh': {'title': '多语言代码补全的新基准测试', 'desc': '本论文提出了一种新的多语言代码补全基准测试，称为M2RC-EVAL，涵盖了18种编程语言。现有的基准测试通常只关注少数几种语言，无法全面评估大型语言模型在不同语言中的代码智能能力。此外，M2RC-EVAL提供了细粒度的注释，帮助研究人员更好地理解模型在不同补全场景下的表现。为了进一步提升代码补全能力，我们还构建了一个多语言指令数据集M2RC-INSTRUCT。'}}}, {'id': 'https://huggingface.co/papers/2411.00030', 'title': 'WikiNER-fr-gold: A Gold-Standard NER Corpus', 'url': 'https://huggingface.co/papers/2411.00030', 'abstract': 'We address in this article the the quality of the WikiNER corpus, a multilingual Named Entity Recognition corpus, and provide a consolidated version of it. The annotation of WikiNER was produced in a semi-supervised manner i.e. no manual verification has been carried out a posteriori. Such corpus is called silver-standard. In this paper we propose WikiNER-fr-gold which is a revised version of the French proportion of WikiNER. Our corpus consists of randomly sampled 20% of the original French sub-corpus (26,818 sentences with 700k tokens). We start by summarizing the entity types included in each category in order to define an annotation guideline, and then we proceed to revise the corpus. Finally we present an analysis of errors and inconsistency observed in the WikiNER-fr corpus, and we discuss potential future work directions.', 'score': 4, 'issue_id': 411, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '5f3e739256c5e800', 'authors': ['Danrun Cao', 'Nicolas Béchet', 'Pierre-François Marteau'], 'affiliations': ['Univ. Bretagne Sud, CNRS, IRISA', 'OctopusMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00030.jpg', 'data': {'categories': ['#synthetic', '#multilingual', '#data', '#dataset', '#low_resource'], 'emoji': '🏷️', 'ru': {'title': 'Золотой стандарт для французского NER: улучшение WikiNER', 'desc': 'Статья посвящена улучшению качества корпуса WikiNER для распознавания именованных сущностей на нескольких языках. Авторы создали WikiNER-fr-gold - вручную проверенную версию французской части корпуса, состоящую из 20% оригинальных данных. Они разработали руководство по аннотации, пересмотрели корпус и проанализировали ошибки в исходном WikiNER-fr. Исследование направлено на повышение точности обучения моделей NER на французском языке.'}, 'en': {'title': 'Enhancing Named Entity Recognition with WikiNER-fr-gold', 'desc': 'This paper focuses on improving the quality of the WikiNER corpus, which is used for Named Entity Recognition (NER) in multiple languages. The original WikiNER corpus was created using a semi-supervised approach, resulting in a silver-standard dataset without manual verification. The authors introduce WikiNER-fr-gold, a refined version of the French subset of WikiNER, based on a carefully sampled 20% of the original data. They outline the types of entities included, establish annotation guidelines, and analyze errors in the original corpus to suggest future improvements.'}, 'zh': {'title': '提升WikiNER语料库质量的探索', 'desc': '本文讨论了WikiNER语料库的质量，这是一个多语言命名实体识别语料库，并提供了一个整合版本。WikiNER的标注是以半监督的方式生成的，即没有进行后期的人工验证，因此该语料库被称为银标准。我们提出了WikiNER-fr-gold，这是WikiNER法语部分的修订版本，包含了原法语子语料库的20%随机抽样（26,818个句子，700k个标记）。最后，我们分析了WikiNER-fr语料库中观察到的错误和不一致，并讨论了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2411.10442', 'title': 'Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization', 'url': 'https://huggingface.co/papers/2411.10442', 'abstract': 'Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance. To address this, we introduce a preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset. and (2) on the model side, we explore integrating PO with MLLMs, developing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance. Our approach demonstrates improved performance across multiple benchmarks, particularly in multimodal reasoning tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10x larger InternVL2-76B. We hope this study could inspire further advancements in MLLMs. Code, data, and model shall be publicly released.', 'score': 49, 'issue_id': 722, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': '3cc3675b352b1634', 'authors': ['Weiyun Wang', 'Zhe Chen', 'Wenhai Wang', 'Yue Cao', 'Yangzhou Liu', 'Zhangwei Gao', 'Jinguo Zhu', 'Xizhou Zhu', 'Lewei Lu', 'Yu Qiao', 'Jifeng Dai'], 'affiliations': ['Fudan University', 'Nanjing University', 'OpenGVLab, Shanghai AI Laboratory', 'SenseTime Research', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10442.jpg', 'data': {'categories': ['#training', '#benchmark', '#reasoning', '#open_source', '#dataset', '#multimodal', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Усиление мультимодальных рассуждений ИИ через оптимизацию предпочтений', 'desc': 'Исследователи представили новый метод улучшения мультимодальных языковых моделей (MLLM) с помощью оптимизации предпочтений (PO). Они создали крупномасштабный набор данных MMPR для мультимодальных рассуждений и разработали метод смешанной оптимизации предпочтений (MPO). Их модель InternVL2-8B-MPO показала значительное улучшение производительности в задачах мультимодальных рассуждений, особенно в тестах типа Chain-of-Thought. Результаты демонстрируют потенциал этого подхода для повышения способностей MLLM к мультимодальным рассуждениям.'}, 'en': {'title': 'Boosting Multimodal Reasoning with Preference Optimization', 'desc': 'This paper addresses the limitations of existing multimodal large language models (MLLMs) in reasoning tasks due to distribution shifts. It introduces a preference optimization (PO) process to enhance the Chain-of-Thought (CoT) performance of these models. The authors create a high-quality dataset called MMPR through an automated preference data construction pipeline and develop a method called Mixed Preference Optimization (MPO) to integrate PO with MLLMs. Their model, InternVL2-8B-MPO, shows significant improvements in multimodal reasoning tasks, achieving higher accuracy than previous models and demonstrating the potential for further advancements in this field.'}, 'zh': {'title': '提升多模态推理能力的新方法', 'desc': '本文介绍了一种新的方法来提高多模态大语言模型（MLLMs）的推理能力。我们提出了一种偏好优化（PO）过程，旨在解决现有模型在多模态推理中的分布偏移问题。通过构建高质量的大规模多模态推理偏好数据集MMPR，并将PO与MLLMs结合，我们开发了混合偏好优化（MPO）方法，显著提升了模型在多模态链式思维（CoT）任务中的表现。实验结果表明，我们的模型在多个基准测试中表现优异，特别是在多模态推理任务上。'}}}, {'id': 'https://huggingface.co/papers/2411.14402', 'title': 'Multimodal Autoregressive Pre-training of Large Vision Encoders', 'url': 'https://huggingface.co/papers/2411.14402', 'abstract': 'We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Our encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal image understanding across diverse settings.', 'score': 34, 'issue_id': 723, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '95826a974f0f9bb2', 'authors': ['Enrico Fini', 'Mustafa Shukor', 'Xiujun Li', 'Philipp Dufter', 'Michal Klein', 'David Haldimann', 'Sai Aitharaju', 'Victor Guilherme Turrisi da Costa', 'Louis Béthune', 'Zhe Gan', 'Alexander T Toshev', 'Marcin Eichner', 'Moin Nabi', 'Yinfei Yang', 'Joshua M. Susskind', 'Alaaeldin El-Nouby'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2411.14402.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'AIMV2: универсальный энкодер изображений с мультимодальным предобучением', 'desc': 'Представлен новый метод предобучения крупномасштабных энкодеров изображений под названием AIMV2. Он расширяет авторегрессионное предобучение на мультимодальный контекст, включая изображения и текст. AIMV2 использует мультимодальный декодер для генерации фрагментов изображений и текстовых токенов. Модель демонстрирует высокую производительность в различных задачах компьютерного зрения и мультимодального понимания, превосходя современные контрастивные модели.'}, 'en': {'title': 'AIMV2: Unifying Vision and Text for Superior Multimodal Understanding', 'desc': 'This paper presents AIMV2, a new method for pre-training vision encoders that can handle both images and text. It builds on recent techniques in autoregressive pre-training, allowing the model to generate image patches and text tokens effectively. AIMV2 is designed to be scalable and performs exceptionally well on various tasks, including localization and classification. The results show that AIMV2 outperforms existing models like CLIP in multimodal image understanding, achieving high accuracy on benchmarks like ImageNet-1k.'}, 'zh': {'title': 'AIMV2：多模态视觉编码的创新之路', 'desc': '我们提出了一种新颖的大规模视觉编码器预训练方法。该方法基于最近在自回归视觉模型预训练方面的进展，扩展到多模态设置，即图像和文本。我们介绍了AIMV2，这是一系列通用的视觉编码器，具有简单的预训练过程、可扩展性和在多种下游任务中的卓越表现。我们的编码器在多模态评估和视觉基准测试（如定位、基础和分类）中表现出色，AIMV2-3B编码器在ImageNet-1k上实现了89.5%的准确率。'}}}, {'id': 'https://huggingface.co/papers/2411.13676', 'title': 'Hymba: A Hybrid-head Architecture for Small Language Models', 'url': 'https://huggingface.co/papers/2411.13676', 'abstract': 'We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the "forced-to-attend" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput.', 'score': 32, 'issue_id': 721, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': '24009a4acf67d4c7', 'authors': ['Xin Dong', 'Yonggan Fu', 'Shizhe Diao', 'Wonmin Byeon', 'Zijia Chen', 'Ameya Sunil Mahabaleshwarkar', 'Shih-Yang Liu', 'Matthijs Van Keirsbilck', 'Min-Hung Chen', 'Yoshi Suhara', 'Yingyan Lin', 'Jan Kautz', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.13676.jpg', 'data': {'categories': ['#small_models', '#training', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Гибридная архитектура Hymba: эффективность малых языковых моделей на новом уровне', 'desc': 'Авторы представляют Hymba - семейство малых языковых моделей с гибридной параллельной архитектурой, сочетающей механизмы внимания трансформеров и модели пространства состояний (SSM) для повышения эффективности. В модель добавлены обучаемые мета-токены, хранящие важную информацию и снижающие нагрузку на механизмы внимания. Оптимизация включает межслойное разделение ключей и значений, а также частичное скользящее окно внимания. Модель Hymba-1.5B-Base превосходит все публичные модели до 2 млрд параметров и даже Llama-3.2-3B по точности, при значительном уменьшении размера кэша и увеличении пропускной способности.'}, 'en': {'title': 'Hymba: Efficient Language Models with Hybrid Architecture', 'desc': 'Hymba is a new family of small language models that combines transformer attention with state space models (SSMs) to improve efficiency. The model uses attention heads for detailed recall and SSM heads for summarizing context effectively. It also features learnable meta tokens that help retain important information without overloading the attention mechanism. Our experiments show that Hymba outperforms existing small language models, achieving better accuracy while using significantly less cache and providing faster processing speeds.'}, 'zh': {'title': 'Hymba：高效的小型语言模型新选择', 'desc': '我们提出了Hymba，这是一种小型语言模型，采用混合头并行架构，将变换器注意机制与状态空间模型（SSMs）结合，以提高效率。注意头提供高分辨率的回忆，而SSM头则实现高效的上下文总结。此外，我们引入了可学习的元标记，这些标记被添加到提示前，存储关键信息，减轻了与注意机制相关的“强制关注”负担。我们的模型通过跨层键值（KV）共享和部分滑动窗口注意机制进一步优化，结果是缓存大小紧凑。'}}}, {'id': 'https://huggingface.co/papers/2411.14405', 'title': 'Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions', 'url': 'https://huggingface.co/papers/2411.14405', 'abstract': 'Currently OpenAI o1 has sparked a surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding -- which are well-suited for reinforcement learning (RL) -- but also places greater emphasis on open-ended resolutions. We aim to address the question: "Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?" Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies -- optimized for complex real-world problem-solving tasks.', 'score': 32, 'issue_id': 720, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'ef4a95abeea69237', 'authors': ['Yu Zhao', 'Huifeng Yin', 'Bo Zeng', 'Hao Wang', 'Tianqi Shi', 'Chenyang Lyu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce'], 'pdf_title_img': 'assets/pdf/title_img/2411.14405.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'Расширение границ искусственного интеллекта: от стандартных задач к открытым проблемам', 'desc': 'Статья описывает разработку модели Marco-o1, которая расширяет возможности OpenAI o1 в области рассуждений. Модель нацелена на решение задач с открытым концом, где отсутствуют четкие стандарты и сложно количественно оценить результаты. Marco-o1 использует усовершенствованные методы, такие как обучение с подкреплением, цепочки размышлений и метод Монте-Карло. Основная цель - создать модель, способную эффективно обобщать знания и решать сложные задачи реального мира.'}, 'en': {'title': 'Unlocking Generalization in Large Reasoning Models', 'desc': "This paper explores the capabilities of the Marco-o1 model in handling large reasoning tasks across various domains. It emphasizes the model's ability to generalize beyond traditional areas like mathematics and coding, where answers are clear-cut. The research investigates how Marco-o1 can tackle open-ended problems where standard solutions and quantifiable rewards are not readily available. Key techniques employed include Chain-of-Thought fine-tuning and Monte Carlo Tree Search, which enhance the model's reasoning and problem-solving abilities in complex scenarios."}, 'zh': {'title': '推动推理模型的广泛应用', 'desc': '这篇论文探讨了大型推理模型（LRM）的研究，特别是OpenAI的o1模型。Marco-o1不仅关注数学、物理和编程等有标准答案的学科，还强调开放式问题的解决能力。研究的核心问题是o1模型是否能够有效地推广到缺乏明确标准和难以量化奖励的更广泛领域。Marco-o1结合了链式思维（CoT）微调、蒙特卡洛树搜索（MCTS）、反思机制和创新推理策略，以优化复杂的现实问题解决任务。'}}}, {'id': 'https://huggingface.co/papers/2411.14199', 'title': 'OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs', 'url': 'https://huggingface.co/papers/2411.14199', 'abstract': "Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? We introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, we develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's 32%. We open-source all of our code, models, datastore, data and a public demo.", 'score': 21, 'issue_id': 720, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'f429efe07ec308f2', 'authors': ['Akari Asai', 'Jacqueline He', 'Rulin Shao', 'Weijia Shi', 'Amanpreet Singh', 'Joseph Chee Chang', 'Kyle Lo', 'Luca Soldaini', 'Sergey Feldman', "Mike D'arcy", 'David Wadden', 'Matt Latzke', 'Minyang Tian', 'Pan Ji', 'Shengyan Liu', 'Hao Tong', 'Bohao Wu', 'Yanyu Xiong', 'Luke Zettlemoyer', 'Graham Neubig', 'Dan Weld', 'Doug Downey', 'Wen-tau Yih', 'Pang Wei Koh', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for AI', 'Carnegie Mellon University', 'Meta', 'Stanford University', 'University of Illinois, Urbana-Champaign', 'University of North Carolina, Chapel Hill', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.14199.jpg', 'data': {'categories': ['#science', '#rag', '#open_source', '#multimodal', '#benchmark', '#hallucinations'], 'emoji': '🔬', 'ru': {'title': 'OpenScholar: ИИ-помощник для синтеза научной литературы', 'desc': 'Статья представляет OpenScholar - специализированную языковую модель с расширенным поиском, которая отвечает на научные запросы, используя 45 миллионов научных статей. Для оценки модели авторы разработали ScholarQABench - первый масштабный мультидоменный бенчмарк для поиска литературы. OpenScholar-8B превосходит GPT-4 и PaperQA2 по точности ответов, несмотря на меньший размер модели. Эксперты предпочли ответы OpenScholar-8B и OpenScholar-GPT4o экспертным ответам в 51% и 70% случаев соответственно.'}, 'en': {'title': 'Empowering Scientific Research with OpenScholar: Accurate, Citation-Backed Insights', 'desc': 'This paper presents OpenScholar, a retrieval-augmented language model designed to assist researchers in synthesizing scientific literature. OpenScholar effectively identifies relevant passages from a vast collection of 45 million open-access papers and generates citation-backed responses to scientific queries. The model is evaluated using ScholarQABench, a benchmark that includes expert-written queries and answers across multiple domains, demonstrating superior performance in correctness compared to other models like GPT-4o. Additionally, OpenScholar shows a significant reduction in citation hallucination, achieving accuracy comparable to human experts, and enhances the performance of existing models through its innovative architecture.'}, 'zh': {'title': 'OpenScholar：提升科学文献检索的智能助手', 'desc': '本论文介绍了一种名为OpenScholar的专用检索增强语言模型，旨在帮助科学家从4500万篇开放获取论文中提取相关信息并生成基于引用的回答。我们开发了ScholarQABench，这是第一个大规模的多领域文献搜索基准，包含2967个专家编写的查询和208个长答案，涵盖计算机科学、物理学、神经科学和生物医学等领域。OpenScholar在准确性上超越了GPT-4o和PaperQA2，尽管其模型规模较小，且在引用准确性方面与人类专家相当。我们还开源了所有代码、模型和数据，提供了公共演示，促进了科学文献的检索和理解。'}}}, {'id': 'https://huggingface.co/papers/2411.14251', 'title': 'Natural Language Reinforcement Learning', 'url': 'https://huggingface.co/papers/2411.14251', 'abstract': 'Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases. Our code will be released at https://github.com/waterhorse1/Natural-language-RL.', 'score': 21, 'issue_id': 719, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '351fc2a705b34aff', 'authors': ['Xidong Feng', 'Ziyu Wan', 'Haotian Fu', 'Bo Liu', 'Mengyue Yang', 'Girish A. Koushik', 'Zhiyuan Hu', 'Ying Wen', 'Jun Wang'], 'affiliations': ['Brown University', 'National University of Singapore', 'Shanghai Jiao Tong University', 'University College London', 'University of Bristol', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2411.14251.jpg', 'data': {'categories': ['#interpretability', '#rl', '#rlhf', '#open_source', '#games'], 'emoji': '🗣️', 'ru': {'title': 'Обучение с подкреплением заговорило на естественном языке', 'desc': 'Эта статья представляет новую концепцию - обучение с подкреплением на естественном языке (NLRL). NLRL расширяет традиционные марковские процессы принятия решений, переопределяя основные принципы RL в языковом пространстве. Используя достижения в области больших языковых моделей, NLRL может быть реализовано с помощью промптов или градиентного обучения. Эксперименты на играх Maze, Breakthrough и крестики-нолики демонстрируют эффективность и интерпретируемость этого подхода.'}, 'en': {'title': 'Revolutionizing Decision-Making with Language: Natural Language Reinforcement Learning', 'desc': "This paper introduces Natural Language Reinforcement Learning (NLRL), which adapts traditional Reinforcement Learning (RL) methods to work with natural language representations. By extending the Markov Decision Process (MDP) framework, NLRL redefines key RL concepts such as task objectives, policies, and value functions in the context of language. The approach leverages advancements in large language models (LLMs) to enhance policy and value updates through prompting or gradient-based training. Experimental results on games like Maze, Breakthrough, and Tic-Tac-Toe showcase NLRL's effectiveness and interpretability across various applications."}, 'zh': {'title': '自然语言强化学习：决策的新视角', 'desc': '强化学习（RL）通过马尔可夫决策过程（MDP）来数学化决策制定。本文提出了一种新的可能性，即自然语言强化学习（NLRL），通过将传统的MDP扩展到基于自然语言的表示空间。NLRL创新性地重新定义了强化学习的原则，包括任务目标、策略、价值函数、贝尔曼方程和策略迭代，使其适应语言的对应关系。通过在迷宫、突破和井字棋等游戏中的实验，验证了NLRL框架在多种应用场景中的有效性、效率和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2411.12364', 'title': 'Ultra-Sparse Memory Network', 'url': 'https://huggingface.co/papers/2411.12364', 'abstract': 'It is widely acknowledged that the performance of Transformer models is exponentially related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms traditional models. In our experiments, we train networks with up to 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget.', 'score': 16, 'issue_id': 721, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 ноября', 'en': 'November 19', 'zh': '11月19日'}, 'hash': '090bf8a39ee13838', 'authors': ['Zihao Huang', 'Qiyang Min', 'Hongzhi Huang', 'Defa Zhu', 'Yutao Zeng', 'Ran Guo', 'Xun Zhou'], 'affiliations': ['Seed-Foundation-Model Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2411.12364.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'UltraMem: сверхбыстрые трансформеры с разреженной памятью', 'desc': 'В этой статье представлен новый подход UltraMem, который использует сверхразреженные слои памяти для улучшения производительности трансформеров. Метод позволяет значительно снизить задержку при выводе, сохраняя качество модели. Исследованы законы масштабирования новой архитектуры, показывающие её преимущества перед традиционными моделями. Эксперименты с сетями, содержащими до 20 миллионов ячеек памяти, демонстрируют state-of-the-art скорость вывода и качество модели при заданном вычислительном бюджете.'}, 'en': {'title': 'UltraMem: Speeding Up Transformers with Sparse Memory!', 'desc': 'This paper presents UltraMem, a novel architecture designed to enhance the efficiency of Transformer models by integrating a large-scale, ultra-sparse memory layer. By decoupling the number of parameters from computational complexity, UltraMem addresses the high memory access costs that hinder inference speed in existing models. The authors demonstrate that their approach not only reduces inference latency but also maintains competitive model performance, even with networks containing up to 20 million memory slots. Additionally, the study explores the scaling laws of UltraMem, showing that it outperforms traditional models while adhering to computational constraints.'}, 'zh': {'title': 'UltraMem：提升推理速度的新架构', 'desc': '本论文提出了一种名为UltraMem的新架构，旨在解决Transformer模型在推理时的高内存访问成本问题。通过引入大规模的超稀疏内存层，UltraMem能够在保持模型性能的同时显著降低推理延迟。我们还研究了这种新架构的扩展规律，结果表明其具有良好的扩展性，并且在性能上优于传统模型。实验表明，我们的方法在给定的计算预算内实现了最先进的推理速度和模型性能。'}}}, {'id': 'https://huggingface.co/papers/2411.14432', 'title': 'Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2411.14432', 'abstract': "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.", 'score': 15, 'issue_id': 720, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '0af1bb82d8021d3b', 'authors': ['Yuhao Dong', 'Zuyan Liu', 'Hai-Long Sun', 'Jingkang Yang', 'Winston Hu', 'Yongming Rao', 'Ziwei Liu'], 'affiliations': ['Nanjing University', 'S-Lab, NTU', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14432.jpg', 'data': {'categories': ['#reasoning', '#long_context', '#multimodal', '#data', '#dataset', '#benchmark', '#agents', '#training'], 'emoji': '🧠', 'ru': {'title': 'Улучшение визуальных рассуждений ИИ через длинные цепочки и мультиагентное обучение', 'desc': 'Статья представляет Insight-V - подход к улучшению способностей мультимодальных больших языковых моделей (MLLM) к рассуждениям. Авторы предлагают двухэтапный конвейер для создания длинных и структурированных данных для обучения без участия человека. Они также разрабатывают мультиагентную систему, состоящую из агента рассуждений и агента-резюме, для эффективного обучения MLLM. Результаты показывают значительное улучшение производительности на сложных мультимодальных задачах, требующих визуального рассуждения.'}, 'en': {'title': 'Empowering Multi-Modal Reasoning with Insight-V', 'desc': 'This paper introduces Insight-V, a novel approach to enhance the reasoning capabilities of multi-modal large language models (MLLMs) by generating high-quality long-chain reasoning data. The authors propose a two-step pipeline that creates structured reasoning paths without human intervention and employs a multi-granularity assessment method to ensure the quality of the generated data. They also develop a multi-agent system that includes a reasoning agent for long-chain reasoning and a summary agent to evaluate and condense the reasoning outputs. The results show that Insight-V significantly improves performance on complex multi-modal tasks, particularly those requiring visual reasoning, while maintaining effectiveness on perception-focused tasks.'}, 'zh': {'title': '提升多模态推理能力的创新方法', 'desc': '本文介绍了一种名为Insight-V的系统，旨在提高多模态大语言模型（MLLMs）的推理能力。我们设计了一个两步生成管道，以无人工干预的方式创建长且结构化的推理数据，并采用多粒度评估方法确保数据质量。研究表明，直接用复杂推理数据监督MLLMs并不能达到理想效果，因此我们构建了一个多代理系统，包括专注于长链推理的推理代理和负责评估和总结推理结果的总结代理。通过这种方法，Insight-V在视觉推理等多模态基准测试中表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2411.14430', 'title': 'Stable Flow: Vital Layers for Training-Free Image Editing', 'url': 'https://huggingface.co/papers/2411.14430', 'abstract': 'Diffusion models have revolutionized the field of content synthesis and editing. Recent models have replaced the traditional UNet architecture with the Diffusion Transformer (DiT), and employed flow-matching for improved training and sampling. However, they exhibit limited generation diversity. In this work, we leverage this limitation to perform consistent image edits via selective injection of attention features. The main challenge is that, unlike the UNet-based models, DiT lacks a coarse-to-fine synthesis structure, making it unclear in which layers to perform the injection. Therefore, we propose an automatic method to identify "vital layers" within DiT, crucial for image formation, and demonstrate how these layers facilitate a range of controlled stable edits, from non-rigid modifications to object addition, using the same mechanism. Next, to enable real-image editing, we introduce an improved image inversion method for flow models. Finally, we evaluate our approach through qualitative and quantitative comparisons, along with a user study, and demonstrate its effectiveness across multiple applications. The project page is available at https://omriavrahami.com/stable-flow', 'score': 9, 'issue_id': 723, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '4d5707c1fdd2e4f9', 'authors': ['Omri Avrahami', 'Or Patashnik', 'Ohad Fried', 'Egor Nemchinov', 'Kfir Aberman', 'Dani Lischinski', 'Daniel Cohen-Or'], 'affiliations': ['Reichman University', 'Snap Research', 'Tel Aviv University', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2411.14430.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Стабильное редактирование изображений через выборочное внедрение признаков в DiT', 'desc': 'Эта статья представляет новый подход к редактированию изображений с использованием диффузионных моделей. Авторы предлагают метод выборочного внедрения признаков внимания в ключевые слои Диффузионного Трансформера (DiT) для выполнения стабильных и контролируемых изменений изображений. Они также разрабатывают улучшенный метод инверсии изображений для потоковых моделей, что позволяет редактировать реальные изображения. Эффективность подхода демонстрируется через качественные и количественные сравнения, а также пользовательское исследование.'}, 'en': {'title': 'Enhancing Image Editing with Vital Layer Injection in Diffusion Transformers', 'desc': "This paper discusses advancements in diffusion models for content synthesis and editing, specifically focusing on the Diffusion Transformer (DiT) architecture. The authors address the challenge of limited generation diversity in DiT by proposing a method to selectively inject attention features into vital layers of the model. This approach allows for consistent and controlled image edits, such as non-rigid modifications and object additions, despite DiT's lack of a traditional coarse-to-fine synthesis structure. Additionally, the paper introduces an improved image inversion method for flow models and validates the effectiveness of their technique through various evaluations and user studies."}, 'zh': {'title': '利用扩散变换器实现稳定的图像编辑', 'desc': '扩散模型在内容合成和编辑领域取得了革命性进展。最近的模型用扩散变换器（DiT）替代了传统的UNet架构，并采用流匹配技术以改善训练和采样。然而，这些模型的生成多样性有限。我们利用这一限制，通过选择性注入注意力特征来实现一致的图像编辑，并提出了一种自动识别DiT中关键层的方法，以便进行稳定的图像修改。'}}}, {'id': 'https://huggingface.co/papers/2411.14257', 'title': 'Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models', 'url': 'https://huggingface.co/papers/2411.14257', 'abstract': "Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This suggests that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token.", 'score': 8, 'issue_id': 732, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '765c1d51aaa40d67', 'authors': ['Javier Ferrando', 'Oscar Obeso', 'Senthooran Rajamanoharan', 'Neel Nanda'], 'affiliations': ['ETH Zürich', 'UPC'], 'pdf_title_img': 'assets/pdf/title_img/2411.14257.jpg', 'data': {'categories': ['#rlhf', '#architecture', '#interpretability', '#training', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Самопознание ИИ: ключ к контролю галлюцинаций', 'desc': 'Исследователи обнаружили, что ключевым механизмом галлюцинаций в больших языковых моделях является распознавание сущностей. Используя разреженные автоэнкодеры, они выявили значимые направления в пространстве представлений, которые определяют, распознает ли модель сущность. Эти направления имеют причинно-следственную связь и могут влиять на поведение модели, заставляя ее отказываться отвечать на вопросы о известных сущностях или галлюцинировать атрибуты неизвестных. Исследование также показало, что эти механизмы сохраняются после файнтюнинга чат-моделей.'}, 'en': {'title': 'Understanding Hallucinations: Entity Recognition in Language Models', 'desc': "This paper investigates the issue of hallucinations in large language models, which occur when these models generate incorrect or fabricated information. The authors utilize sparse autoencoders to analyze how these models recognize entities and determine their knowledge about them. They find that the model's internal representations can indicate whether it knows about an entity, influencing its responses to questions. The study reveals that these representations can affect the model's behavior, such as leading it to refuse to answer questions about known entities or to invent details about unknown ones."}, 'zh': {'title': '揭示大型语言模型的自我知识与幻觉机制', 'desc': '在大型语言模型中，幻觉现象普遍存在，但其背后的机制尚不清楚，这限制了我们解决这一问题的能力。通过使用稀疏自编码器作为可解释性工具，我们发现实体识别是这些机制的关键部分，模型能够识别出自己能否回忆起某个实体的事实。稀疏自编码器揭示了表示空间中的有意义方向，这些方向可以检测模型是否认识某个实体，例如识别出它对某个运动员或电影并不了解。这表明模型具有自我知识：关于自身能力的内部表示，这些方向在模型的拒绝回答已知实体问题或对未知实体进行幻觉时具有因果相关性。'}}}, {'id': 'https://huggingface.co/papers/2411.13807', 'title': 'MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control', 'url': 'https://huggingface.co/papers/2411.13807', 'abstract': 'The rapid advancement of diffusion models has greatly improved video synthesis, especially in controllable video generation, which is essential for applications like autonomous driving. However, existing methods are limited by scalability and how control conditions are integrated, failing to meet the needs for high-resolution and long videos for autonomous driving applications. In this paper, we introduce MagicDriveDiT, a novel approach based on the DiT architecture, and tackle these challenges. Our method enhances scalability through flow matching and employs a progressive training strategy to manage complex scenarios. By incorporating spatial-temporal conditional encoding, MagicDriveDiT achieves precise control over spatial-temporal latents. Comprehensive experiments show its superior performance in generating realistic street scene videos with higher resolution and more frames. MagicDriveDiT significantly improves video generation quality and spatial-temporal controls, expanding its potential applications across various tasks in autonomous driving.', 'score': 6, 'issue_id': 728, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '95ef9fbe239921f8', 'authors': ['Ruiyuan Gao', 'Kai Chen', 'Bo Xiao', 'Lanqing Hong', 'Zhenguo Li', 'Qiang Xu'], 'affiliations': ['CUHK', 'HKUST', 'Huawei Cloud', 'Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.13807.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#training', '#video'], 'emoji': '🚗', 'ru': {'title': 'Революция в синтезе видео для беспилотных автомобилей', 'desc': 'Статья представляет MagicDriveDiT - новый подход к генерации видео для автономного вождения, основанный на архитектуре DiT. Метод улучшает масштабируемость с помощью flow matching и прогрессивного обучения для сложных сценариев. MagicDriveDiT использует пространственно-временное кодирование условий для точного контроля над латентными переменными. Эксперименты показывают превосходную производительность в генерации реалистичных уличных сцен с более высоким разрешением и большим количеством кадров.'}, 'en': {'title': 'MagicDriveDiT: Revolutionizing Video Generation for Autonomous Driving', 'desc': 'This paper presents MagicDriveDiT, a new method for generating high-quality videos using diffusion models, specifically designed for applications in autonomous driving. It addresses the limitations of existing techniques by enhancing scalability and integrating control conditions more effectively. The approach utilizes flow matching and a progressive training strategy to handle complex video scenarios, ensuring better performance in generating long, high-resolution videos. By incorporating spatial-temporal conditional encoding, MagicDriveDiT allows for precise control over the generated video content, making it suitable for various autonomous driving tasks.'}, 'zh': {'title': 'MagicDriveDiT：提升视频生成质量与控制能力的创新方法', 'desc': '本论文介绍了一种名为MagicDriveDiT的新方法，基于DiT架构，旨在解决视频合成中的可扩展性和控制条件集成问题。该方法通过流匹配技术增强了可扩展性，并采用渐进式训练策略来处理复杂场景。通过引入时空条件编码，MagicDriveDiT能够精确控制时空潜变量。实验结果表明，该方法在生成高分辨率和更多帧的真实街景视频方面表现优越，显著提升了视频生成质量和时空控制能力。'}}}, {'id': 'https://huggingface.co/papers/2411.14343', 'title': 'UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages', 'url': 'https://huggingface.co/papers/2411.14343', 'abstract': 'Large language models (LLMs) under-perform on low-resource languages due to limited training data. We present a method to efficiently collect text data for low-resource languages from the entire Common Crawl corpus. Our approach, UnifiedCrawl, filters and extracts common crawl using minimal compute resources, yielding mono-lingual datasets much larger than previously available sources. We demonstrate that leveraging this data to fine-tuning multilingual LLMs via efficient adapter methods (QLoRA) significantly boosts performance on the low-resource language, while minimizing VRAM usage. Our experiments show large improvements in language modeling perplexity and an increase in few-shot prompting scores. Our work and released source code provide an affordable approach to improve LLMs for low-resource languages using consumer hardware. Our source code is available here at https://github.com/bethelmelesse/unifiedcrawl.', 'score': 4, 'issue_id': 728, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'e3be7df0af13931a', 'authors': ['Bethel Melesse Tessema', 'Akhil Kedia', 'Tae-Sun Chung'], 'affiliations': ['Ajou University Suwon, South Korea', 'Independent Researcher Seoul, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2411.14343.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#open_source', '#training', '#multilingual', '#data'], 'emoji': '🌍', 'ru': {'title': 'Улучшение языковых моделей для малоресурсных языков с помощью UnifiedCrawl', 'desc': 'Статья представляет метод UnifiedCrawl для эффективного сбора текстовых данных для малоресурсных языков из корпуса Common Crawl. Авторы демонстрируют, что использование этих данных для дообучения многоязычных языковых моделей с помощью эффективных методов адаптации (QLoRA) значительно повышает производительность на малоресурсных языках. Эксперименты показывают улучшение перплексии языкового моделирования и повышение оценок при few-shot промптинге. Предложенный подход позволяет улучшать большие языковые модели для малоресурсных языков, используя доступное оборудование.'}, 'en': {'title': 'Boosting Low-Resource Languages with UnifiedCrawl', 'desc': 'This paper addresses the challenge of large language models (LLMs) performing poorly on low-resource languages due to a lack of training data. The authors introduce a method called UnifiedCrawl, which efficiently collects and filters text data from the Common Crawl corpus, creating larger mono-lingual datasets. They demonstrate that fine-tuning multilingual LLMs with this data using efficient adapter methods like QLoRA leads to significant improvements in language modeling and few-shot prompting scores. The approach is designed to be accessible, allowing enhancements to LLMs for low-resource languages using standard consumer hardware.'}, 'zh': {'title': '提升低资源语言模型性能的新方法', 'desc': '本论文提出了一种名为UnifiedCrawl的方法，用于从Common Crawl数据集中高效收集低资源语言的文本数据。该方法通过最小化计算资源的使用，过滤和提取数据，从而生成比以往更大的单语数据集。我们展示了利用这些数据通过高效的适配器方法（QLoRA）对多语言大语言模型进行微调，可以显著提高低资源语言的性能，同时减少显存使用。实验结果表明，语言建模的困惑度有了显著改善，少量示例提示的得分也有所提高。'}}}, {'id': 'https://huggingface.co/papers/2411.13082', 'title': 'Patience Is The Key to Large Language Model Reasoning', 'url': 'https://huggingface.co/papers/2411.13082', 'abstract': 'Recent advancements in the field of large language models, particularly through the Chain of Thought (CoT) approach, have demonstrated significant improvements in solving complex problems. However, existing models either tend to sacrifice detailed reasoning for brevity due to user preferences, or require extensive and expensive training data to learn complicated reasoning ability, limiting their potential in solving complex tasks. To bridge this gap, following the concept of scaling test-time, we propose a simple method by encouraging models to adopt a more patient reasoning style without the need of introducing new knowledge or skills. To employ a preference optimization approach, we generate detailed reasoning processes as positive examples and simple answers as negative examples, thereby training the model to favor thoroughness in its responses. Our results demonstrate a performance increase of up to 6.7% on GSM8k with training just on a lightweight dataset.', 'score': 4, 'issue_id': 726, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'b18aa77451c249b7', 'authors': ['Yijiong Yu'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.13082.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Терпеливые рассуждения: новый путь к улучшению ИИ', 'desc': 'Статья описывает новый метод улучшения рассуждений больших языковых моделей без необходимости обширного обучения. Авторы предлагают подход, основанный на оптимизации предпочтений, где модель учится выдавать более подробные рассуждения вместо кратких ответов. Метод использует детальные рассуждения как положительные примеры и простые ответы как отрицательные при обучении. Результаты показывают улучшение производительности до 6.7% на наборе данных GSM8k при обучении на небольшом датасете.'}, 'en': {'title': 'Enhancing Reasoning in Language Models with Simple Training Techniques', 'desc': 'This paper discusses improvements in large language models using the Chain of Thought (CoT) method, which enhances their ability to solve complex problems. It identifies a challenge where models often prioritize brevity over detailed reasoning, or require large amounts of training data to develop reasoning skills. The authors propose a new method that encourages models to adopt a more thorough reasoning style without needing additional knowledge. By using preference optimization, they train models with detailed reasoning as positive examples and simple answers as negative examples, achieving a notable performance increase on the GSM8k dataset with minimal training data.'}, 'zh': {'title': '提升推理能力，简化训练过程', 'desc': '本文探讨了大型语言模型在解决复杂问题时的进展，特别是通过思维链（CoT）方法的应用。现有模型往往为了简洁而牺牲详细推理，或者需要大量昂贵的训练数据来学习复杂的推理能力。为了解决这个问题，我们提出了一种简单的方法，鼓励模型采用更耐心的推理风格，而无需引入新知识或技能。通过生成详细的推理过程作为正例和简单答案作为负例，我们训练模型更倾向于全面的回答，结果显示在GSM8k上性能提高了6.7%。'}}}, {'id': 'https://huggingface.co/papers/2411.14347', 'title': 'DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding', 'url': 'https://huggingface.co/papers/2411.14347', 'abstract': "In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model's core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model's open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, both improving the previous SOTA performance by 5.8 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects.", 'score': 3, 'issue_id': 739, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '5805ca9f7ff81a78', 'authors': ['Tianhe Ren', 'Yihao Chen', 'Qing Jiang', 'Zhaoyang Zeng', 'Yuda Xiong', 'Wenlong Liu', 'Zhengyu Ma', 'Junyi Shen', 'Yuan Gao', 'Xiaoke Jiang', 'Xingyu Chen', 'Zhuheng Song', 'Yuhong Zhang', 'Hongjie Huang', 'Han Gao', 'Shilong Liu', 'Hao Zhang', 'Feng Li', 'Kent Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)'], 'pdf_title_img': 'assets/pdf/title_img/2411.14347.jpg', 'data': {'categories': ['#optimization', '#long_context', '#cv', '#benchmark', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'DINO-X: Универсальное обнаружение объектов без подсказок', 'desc': 'DINO-X - это унифицированная объектно-ориентированная модель компьютерного зрения, разработанная IDEA Research, которая демонстрирует лучшую на сегодняшний день производительность в обнаружении объектов в открытом мире. Модель использует архитектуру трансформера типа кодировщик-декодировщик для создания объектного представления и поддерживает различные типы подсказок, включая текстовые, визуальные и пользовательские. DINO-X обучается на крупномасштабном наборе данных Grounding-100M, содержащем более 100 миллионов высококачественных образцов для заземления. Экспериментальные результаты показывают превосходную производительность DINO-X, особенно в задачах обнаружения объектов с длинным хвостом распределения.'}, 'en': {'title': 'DINO-X: Revolutionizing Open-World Object Detection', 'desc': 'DINO-X is a cutting-edge object-centric vision model that excels in open-world object detection. It utilizes a Transformer-based encoder-decoder architecture to create detailed object representations, allowing for flexible input through text, visual, or customized prompts. The model is trained on a massive dataset of over 100 million samples, enhancing its ability to detect a wide range of objects without needing specific prompts. Experimental results show that DINO-X outperforms previous models, especially in recognizing rare objects, achieving significant improvements in detection accuracy across various benchmarks.'}, 'zh': {'title': 'DINO-X：开放世界物体检测的新标杆', 'desc': '本文介绍了DINO-X，这是由IDEA研究团队开发的统一对象中心视觉模型，具有当前最佳的开放世界物体检测性能。DINO-X采用与Grounding DINO 1.5相同的基于Transformer的编码器-解码器架构，旨在实现开放世界物体理解的对象级表示。为了简化长尾物体检测，DINO-X扩展了输入选项，支持文本提示、视觉提示和自定义提示，甚至可以实现无提示的开放世界检测。通过在超过1亿个高质量样本的Grounding-100M数据集上进行预训练，DINO-X能够同时支持多种物体感知和理解任务，实验结果显示其在多个基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2411.14384', 'title': 'Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation', 'url': 'https://huggingface.co/papers/2411.14384', 'abstract': 'Existing feed-forward image-to-3D methods mainly rely on 2D multi-view diffusion models that cannot guarantee 3D consistency. These methods easily collapse when changing the prompt view direction and mainly handle object-centric prompt images. In this paper, we propose a novel single-stage 3D diffusion model, DiffusionGS, for object and scene generation from a single view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to enforce view consistency and allow the model to generate robustly given prompt views of any directions, beyond object-centric inputs. Plus, to improve the capability and generalization ability of DiffusionGS, we scale up 3D training data by developing a scene-object mixed training strategy. Experiments show that our method enjoys better generation quality (2.20 dB higher in PSNR and 23.25 lower in FID) and over 5x faster speed (~6s on an A100 GPU) than SOTA methods. The user study and text-to-3D applications also reveals the practical values of our method. Our Project page at https://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and interactive generation results.', 'score': 2, 'issue_id': 733, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '1f0661cca4948898', 'authors': ['Yuanhao Cai', 'He Zhang', 'Kai Zhang', 'Yixun Liang', 'Mengwei Ren', 'Fujun Luan', 'Qing Liu', 'Soo Ye Kim', 'Jianming Zhang', 'Zhifei Zhang', 'Yuqian Zhou', 'Zhe Lin', 'Alan Yuille'], 'affiliations': ['Adobe Research', 'Hong Kong University of Science and Technology', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14384.jpg', 'data': {'categories': ['#3d', '#diffusion'], 'emoji': '🌟', 'ru': {'title': 'Революция в 3D-генерации: от 2D к реалистичным трехмерным сценам', 'desc': 'DiffusionGS - это новая модель генерации трехмерных объектов и сцен по одному изображению. В отличие от существующих методов, она напрямую генерирует 3D облака гауссовых точек, обеспечивая согласованность с разных ракурсов. Модель обучается на смешанном наборе данных объектов и сцен, что улучшает ее обобщающую способность. Эксперименты показывают, что DiffusionGS превосходит существующие методы по качеству и скорости генерации.'}, 'en': {'title': 'Revolutionizing 3D Generation with DiffusionGS', 'desc': 'This paper introduces DiffusionGS, a new single-stage 3D diffusion model designed to generate 3D representations from a single 2D view. Unlike existing methods that struggle with 3D consistency and are limited to object-centric images, DiffusionGS produces 3D Gaussian point clouds that maintain view consistency across various prompt directions. The model enhances its performance by utilizing a scene-object mixed training strategy, which increases the diversity of the training data. Experimental results demonstrate that DiffusionGS achieves superior generation quality and speed compared to state-of-the-art methods, making it a valuable tool for text-to-3D applications.'}, 'zh': {'title': 'DiffusionGS：从单视角生成一致的3D场景', 'desc': '现有的前馈图像到3D方法主要依赖于2D多视角扩散模型，这些模型无法保证3D一致性。我们提出了一种新颖的单阶段3D扩散模型DiffusionGS，可以从单一视角生成物体和场景。DiffusionGS在每个时间步直接输出3D高斯点云，以确保视角一致性，并能够根据任何方向的提示视图进行稳健生成。通过开发场景-物体混合训练策略，我们扩大了3D训练数据，提高了DiffusionGS的能力和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.15124', 'title': 'TÜLU 3: Pushing Frontiers in Open Language Model Post-Training', 'url': 'https://huggingface.co/papers/2411.15124', 'abstract': 'Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\\"ULU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\\"ULU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\\"ULU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.   In addition to the T\\"ULU 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the T\\"ULU 3 approach to more domains.', 'score': 35, 'issue_id': 755, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '44809ea81d71ef97', 'authors': ['Nathan Lambert', 'Jacob Morrison', 'Valentina Pyatkin', 'Shengyi Huang', 'Hamish Ivison', 'Faeze Brahman', 'Lester James V. Miranda', 'Alisa Liu', 'Nouha Dziri', 'Shane Lyu', 'Yuling Gu', 'Saumya Malik', 'Victoria Graf', 'Jena D. Hwang', 'Jiangjiang Yang', 'Ronan Le Bras', 'Oyvind Tafjord', 'Chris Wilhelm', 'Luca Soldaini', 'Noah A. Smith', 'Yizhong Wang', 'Pradeep Dasigi', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.15124.jpg', 'data': {'categories': ['#dataset', '#training', '#data', '#open_source', '#optimization', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Открытый рецепт пост-тренировки языковых моделей', 'desc': 'Статья представляет T"ULU 3 - семейство открытых моделей, обученных с помощью пост-тренировки. Авторы описывают методы обучения, включая SFT, DPO и новый метод RLVR. T"ULU 3 превосходит многие современные модели, включая инструктированные версии Llama 3.1 и GPT-4o-mini. Исследователи предоставляют полные рецепты обучения, наборы данных и инструменты для воспроизведения результатов.'}, 'en': {'title': 'Unlocking Language Model Potential with T"ULU 3', 'desc': 'This paper presents T"ULU 3, a set of fully-open post-trained language models that enhance performance and capabilities beyond existing models. It emphasizes the importance of transparency in training data and methodologies, providing a comprehensive guide for implementing modern post-training techniques. The models utilize advanced training algorithms, including supervised finetuning and a novel reinforcement learning method, achieving superior results compared to both open and closed counterparts. Additionally, the paper offers extensive resources for replication and adaptation, including datasets, training code, and evaluation tools.'}, 'zh': {'title': 'T"ULU 3：开放的后训练模型新纪元', 'desc': '本文介绍了T"ULU 3，这是一个完全开放的最新后训练模型系列，旨在提高语言模型的行为和技能。我们提供了模型的训练数据、代码和训练配方，填补了开放技术与专有技术之间的透明度差距。T"ULU 3在多个基准测试中超越了现有的语言模型，包括Llama 3.1和GPT-4o-mini等。我们还引入了一种多任务评估方案，并提供了详细的报告，以便于在更多领域中复现和适应T"ULU 3的方法。'}}}, {'id': 'https://huggingface.co/papers/2411.14793', 'title': 'Style-Friendly SNR Sampler for Style-Driven Generation', 'url': 'https://huggingface.co/papers/2411.14793', 'abstract': 'Recent large-scale diffusion models generate high-quality images but struggle to learn new, personalized artistic styles, which limits the creation of unique style templates. Fine-tuning with reference images is the most promising approach, but it often blindly utilizes objectives and noise level distributions used for pre-training, leading to suboptimal style alignment. We propose the Style-friendly SNR sampler, which aggressively shifts the signal-to-noise ratio (SNR) distribution toward higher noise levels during fine-tuning to focus on noise levels where stylistic features emerge. This enables models to better capture unique styles and generate images with higher style alignment. Our method allows diffusion models to learn and share new "style templates", enhancing personalized content creation. We demonstrate the ability to generate styles such as personal watercolor paintings, minimal flat cartoons, 3D renderings, multi-panel images, and memes with text, thereby broadening the scope of style-driven generation.', 'score': 27, 'issue_id': 752, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '03859b57f29683ab', 'authors': ['Jooyoung Choi', 'Chaehun Shin', 'Yeongtak Oh', 'Heeseung Kim', 'Sungroh Yoon'], 'affiliations': ['AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University', 'Data Science and AI Laboratory, ECE, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14793.jpg', 'data': {'categories': ['#synthetic', '#3d', '#multimodal', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Улучшение стилизации изображений с помощью оптимизации шума в диффузионных моделях', 'desc': 'Статья представляет новый метод улучшения генерации изображений в определенном стиле с помощью диффузионных моделей. Авторы предлагают использовать Style-friendly SNR sampler, который смещает распределение соотношения сигнал-шум в сторону более высоких уровней шума при дообучении модели. Это позволяет лучше захватывать уникальные стилистические особенности и генерировать изображения с более высоким соответствием заданному стилю. Метод демонстрирует способность генерировать различные стили, включая акварельные рисунки, минималистичные мультфильмы, 3D-рендеры и мемы с текстом.'}, 'en': {'title': 'Unlocking Unique Artistic Styles with Style-friendly SNR Sampler', 'desc': 'This paper addresses the challenge of adapting large-scale diffusion models to generate personalized artistic styles. The authors introduce the Style-friendly SNR sampler, which modifies the signal-to-noise ratio (SNR) during fine-tuning to emphasize higher noise levels where stylistic features are more prominent. By doing so, the model improves its ability to capture unique styles, resulting in images that align better with the desired artistic expression. The proposed method expands the creative possibilities for generating diverse styles, including watercolor paintings and cartoons, thus enhancing personalized content creation.'}, 'zh': {'title': '提升个性化艺术风格生成的信噪比方法', 'desc': '最近的大规模扩散模型能够生成高质量的图像，但在学习新的个性化艺术风格方面存在困难，这限制了独特风格模板的创建。微调参考图像是最有前景的方法，但通常盲目使用预训练时的目标和噪声水平分布，导致风格对齐不理想。我们提出了风格友好的信噪比（SNR）采样器，在微调过程中积极将信噪比分布向更高的噪声水平转移，以专注于风格特征出现的噪声水平。这使得模型能够更好地捕捉独特风格，并生成具有更高风格对齐的图像。'}}}, {'id': 'https://huggingface.co/papers/2411.15098', 'title': 'OminiControl: Minimal and Universal Control for Diffusion Transformer', 'url': 'https://huggingface.co/papers/2411.15098', 'abstract': 'In this paper, we introduce OminiControl, a highly versatile and parameter-efficient framework that integrates image conditions into pre-trained Diffusion Transformer (DiT) models. At its core, OminiControl leverages a parameter reuse mechanism, enabling the DiT to encode image conditions using itself as a powerful backbone and process them with its flexible multi-modal attention processors. Unlike existing methods, which rely heavily on additional encoder modules with complex architectures, OminiControl (1) effectively and efficiently incorporates injected image conditions with only ~0.1% additional parameters, and (2) addresses a wide range of image conditioning tasks in a unified manner, including subject-driven generation and spatially-aligned conditions such as edges, depth, and more. Remarkably, these capabilities are achieved by training on images generated by the DiT itself, which is particularly beneficial for subject-driven generation. Extensive evaluations demonstrate that OminiControl outperforms existing UNet-based and DiT-adapted models in both subject-driven and spatially-aligned conditional generation. Additionally, we release our training dataset, Subjects200K, a diverse collection of over 200,000 identity-consistent images, along with an efficient data synthesis pipeline to advance research in subject-consistent generation.', 'score': 19, 'issue_id': 754, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '9cd668db99ed0902', 'authors': ['Zhenxiong Tan', 'Songhua Liu', 'Xingyi Yang', 'Qiaochu Xue', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.15098.jpg', 'data': {'categories': ['#open_source', '#synthetic', '#data', '#diffusion', '#dataset', '#architecture', '#multimodal', '#training'], 'emoji': '🎨', 'ru': {'title': 'OminiControl: Универсальное управление генерацией изображений с минимальными затратами', 'desc': 'OminiControl - это новая эффективная система для интеграции изображений в предобученные модели Diffusion Transformer (DiT). Она использует механизм повторного использования параметров, позволяя DiT кодировать условия изображений с помощью собственной архитектуры. OminiControl требует всего 0,1% дополнительных параметров и может решать широкий спектр задач условной генерации изображений. Система превосходит существующие модели на основе UNet и адаптированные DiT как в генерации, управляемой субъектом, так и в пространственно-согласованной условной генерации.'}, 'en': {'title': 'Efficient Image Conditioning with OminiControl', 'desc': 'OminiControl is a new framework that enhances pre-trained Diffusion Transformer (DiT) models by integrating image conditions efficiently. It uses a parameter reuse mechanism, allowing the DiT to process image conditions with minimal additional parameters, specifically around 0.1%. This framework can handle various image conditioning tasks, such as generating images based on specific subjects or aligning them with spatial features like edges and depth. OminiControl has shown superior performance compared to traditional UNet-based models and other DiT adaptations, and it comes with a large dataset, Subjects200K, to support further research.'}, 'zh': {'title': 'OminiControl：高效整合图像条件的创新框架', 'desc': '本文介绍了OminiControl，这是一个高度灵活且参数高效的框架，能够将图像条件集成到预训练的扩散变换器（DiT）模型中。OminiControl利用参数重用机制，使DiT能够使用自身作为强大的基础，编码图像条件，并通过灵活的多模态注意力处理器进行处理。与现有方法不同，OminiControl仅需约0.1%的额外参数，就能有效地整合注入的图像条件，并以统一的方式处理多种图像条件任务。通过在DiT自身生成的图像上进行训练，OminiControl在主题驱动生成和空间对齐条件生成方面的表现优于现有的UNet和DiT适应模型。'}}}, {'id': 'https://huggingface.co/papers/2411.12946', 'title': 'A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection', 'url': 'https://huggingface.co/papers/2411.12946', 'abstract': 'Large Language Models are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety.', 'score': 15, 'issue_id': 756, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'de5ca9118a5cab35', 'authors': ['Gabriel Chua', 'Shing Yee Chan', 'Shaun Khoo'], 'affiliations': ['Government Technology Agency Singapore', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.12946.jpg', 'data': {'categories': ['#data', '#dataset', '#hallucinations', '#synthetic', '#open_source', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'Гибкая защита языковых моделей без реальных данных', 'desc': 'Статья представляет новую методологию разработки защитных механизмов для больших языковых моделей (LLM) от нецелевого использования. Авторы предлагают гибкий подход, не требующий реальных данных, основанный на генерации синтетического датасета с помощью LLM. Разработанные защитные механизмы превосходят эвристические подходы и могут обобщаться на другие категории злоупотреблений. Авторы также открывают доступ к синтетическому датасету и моделям защиты для поддержки будущих исследований в области безопасности LLM.'}, 'en': {'title': 'Building Better Guardrails for Large Language Models', 'desc': 'This paper addresses the issue of off-topic misuse in Large Language Models (LLMs) by proposing a new methodology for developing guardrails without relying on real-world data. The authors create a synthetic dataset by using LLMs to generate diverse prompts based on a well-defined problem space, which helps in training more effective off-topic guardrails. Their approach not only reduces false positives but also enhances adaptability to various misuse scenarios, such as harmful prompts and jailbreak attempts. Furthermore, the authors contribute to the community by open-sourcing their synthetic dataset and guardrail models, promoting further research in LLM safety.'}, 'zh': {'title': '构建灵活的防护措施，提升大型语言模型安全性', 'desc': '本论文探讨了大型语言模型在使用中可能出现的偏离主题的误用问题。我们提出了一种灵活的、无数据的防护措施开发方法，旨在解决现有方法的高误报率和适应性不足的问题。通过对问题空间的定性定义，并利用大型语言模型生成多样化的提示，我们构建了一个合成数据集，用于基准测试和训练防护措施。最后，我们开源了合成数据集和防护模型，为大型语言模型的安全性研究和开发提供了重要资源。'}}}, {'id': 'https://huggingface.co/papers/2411.13543', 'title': 'BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games', 'url': 'https://huggingface.co/papers/2411.13543', 'abstract': 'Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community.', 'score': 11, 'issue_id': 762, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'c4fe6e6278490fc9', 'authors': ['Davide Paglieri', 'Bartłomiej Cupiał', 'Samuel Coward', 'Ulyana Piterbarg', 'Maciej Wolczyk', 'Akbir Khan', 'Eduardo Pignatelli', 'Łukasz Kuciński', 'Lerrel Pinto', 'Rob Fergus', 'Jakob Nicolaus Foerster', 'Jack Parker-Holder', 'Tim Rocktäschel'], 'affiliations': ['AI Centre, University College London', 'Anthropic', 'IDEAS NCBR', 'New York University', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2411.13543.jpg', 'data': {'categories': ['#agents', '#rl', '#benchmark', '#reasoning', '#open_source', '#games'], 'emoji': '🎮', 'ru': {'title': 'BALROG: игровой полигон для испытания искусственного интеллекта', 'desc': 'BALROG - это новый бенчмарк для оценки возможностей больших языковых и визуально-языковых моделей в сложных игровых средах. Он включает ряд задач разной сложности, от простых до экстремально сложных, таких как NetHack Learning Environment. Исследователи провели обширное тестирование популярных моделей и обнаружили, что они справляются с простыми играми, но испытывают значительные трудности с более сложными задачами. Особенно заметны проблемы в принятии решений на основе визуальной информации.'}, 'en': {'title': 'BALROG: Benchmarking Agentic Capabilities of LLMs and VLMs', 'desc': 'This paper introduces BALROG, a new benchmark aimed at evaluating the agentic capabilities of Large Language Models (LLMs) and Vision Language Models (VLMs) in complex environments. The benchmark includes a variety of challenging games that test advanced skills like spatial reasoning and long-term planning, which are essential for real-world tasks. The authors implement fine-grained metrics to assess model performance across different difficulty levels, revealing that current models excel in simpler tasks but struggle with more complex ones, particularly in vision-based decision-making. BALROG is released as an open resource to support further research in enhancing the capabilities of LLMs and VLMs.'}, 'zh': {'title': '评估智能代理能力的新基准：BALROG', 'desc': '大型语言模型（LLMs）和视觉语言模型（VLMs）在知识和推理能力上表现出色，但在复杂动态环境中仍然面临挑战。现实任务需要处理复杂的交互、高级空间推理、长期规划和持续探索新策略，而我们在这些领域缺乏有效的评估方法。为了解决这个问题，我们提出了BALROG，一个新颖的基准，旨在通过多样化的挑战性游戏评估LLMs和VLMs的代理能力。我们的研究表明，尽管当前模型在简单游戏中取得了一定成功，但在更具挑战性的任务中表现显著不足，尤其是在基于视觉的决策方面。'}}}, {'id': 'https://huggingface.co/papers/2411.14982', 'title': 'Large Multi-modal Models Can Interpret Features in Large Multi-modal Models', 'url': 'https://huggingface.co/papers/2411.14982', 'abstract': "Recent advances in Large Multimodal Models (LMMs) lead to significant breakthroughs in both academia and industry. One question that arises is how we, as humans, can understand their internal neural representations. This paper takes an initial step towards addressing this question by presenting a versatile framework to identify and interpret the semantics within LMMs. Specifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the representations into human understandable features. 2) We then present an automatic interpretation framework to interpreted the open-semantic features learned in SAE by the LMMs themselves. We employ this framework to analyze the LLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these features can effectively steer the model's behavior. Our results contribute to a deeper understanding of why LMMs excel in specific tasks, including EQ tests, and illuminate the nature of their mistakes along with potential strategies for their rectification. These findings offer new insights into the internal mechanisms of LMMs and suggest parallels with the cognitive processes of the human brain.", 'score': 11, 'issue_id': 761, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '7d4fae86425adb6c', 'authors': ['Kaichen Zhang', 'Yifei Shen', 'Bo Li', 'Ziwei Liu'], 'affiliations': ['LMMs-Lab Team, S-Lab, NTU, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.14982.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая тайны мышления больших мультимодальных моделей', 'desc': 'Статья представляет новый подход к интерпретации внутренних нейронных представлений в больших мультимодальных моделях (LMM). Авторы применяют разреженный автоэнкодер для выделения понятных человеку признаков, а затем используют автоматическую систему интерпретации для анализа этих признаков. Исследование проводится на модели LLaVA-NeXT-8B с помощью LLaVA-OV-72B, демонстрируя, как выделенные признаки влияют на поведение модели. Результаты помогают лучше понять принципы работы LMM и проводят параллели с когнитивными процессами человеческого мозга.'}, 'en': {'title': 'Unlocking the Secrets of Large Multimodal Models', 'desc': "This paper explores how we can understand the internal workings of Large Multimodal Models (LMMs) by using a Sparse Autoencoder (SAE) to break down their representations into features that humans can comprehend. It introduces a framework for automatically interpreting these features, which are learned by the LMMs themselves. The study specifically analyzes the LLaVA-NeXT-8B model in relation to the LLaVA-OV-72B model, showing that the identified features can influence the model's performance on various tasks. The findings enhance our understanding of LMMs' strengths and weaknesses, drawing parallels to human cognitive processes."}, 'zh': {'title': '揭示大型多模态模型的内部机制', 'desc': '本文探讨了大型多模态模型（LMMs）的内部神经表示如何被理解。我们首先使用稀疏自编码器（SAE）将表示解耦为人类可理解的特征。接着，我们提出了一个自动解释框架，用于解释LMMs自身学习的开放语义特征。我们的研究结果加深了对LMMs在特定任务中表现优异原因的理解，并揭示了它们错误的性质及可能的纠正策略。'}}}, {'id': 'https://huggingface.co/papers/2411.14794', 'title': 'VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection', 'url': 'https://huggingface.co/papers/2411.14794', 'abstract': 'The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities. Our code and dataset will be released at: https://github.com/hshjerry/VideoEspresso', 'score': 8, 'issue_id': 757, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': 'f03437948b77773f', 'authors': ['Songhao Han', 'Wei Huang', 'Hairong Shi', 'Le Zhuo', 'Xiu Su', 'Shifeng Zhang', 'Xu Zhou', 'Xiaojuan Qi', 'Yue Liao', 'Si Liu'], 'affiliations': ['Beihang University', 'CUHK', 'Central South University', 'Sangfor Technologies Inc.', 'Shanghai AI Lab', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2411.14794.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#cv', '#games', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'VideoEspresso: новый подход к созданию данных и рассуждению для задач видеопонимания', 'desc': 'Статья представляет новый набор данных VideoEspresso для задач видеопонимания и видео-вопросно-ответных систем. Авторы разработали метод создания высококачественных пар вопрос-ответ с сохранением пространственно-временной информации и промежуточных шагов рассуждения. Предложена архитектура Hybrid LVLMs Collaboration, включающая селектор ключевых кадров и двухэтапную модель рассуждений на основе больших мультимодальных языковых моделей. Эксперименты показали превосходство предложенного подхода над существующими методами на большинстве задач видеопонимания.'}, 'en': {'title': 'Enhancing VideoQA with VideoEspresso: A New Era of Multimodal Reasoning', 'desc': 'This paper presents VideoEspresso, a new dataset designed to enhance video question-answering (VideoQA) by providing high-quality pairs that maintain spatial and temporal coherence. It addresses the limitations of existing datasets, which often rely on expensive manual annotations or ineffective automatic methods. The authors introduce a semantic-aware construction pipeline that reduces redundancy and generates QA pairs using GPT-4o, along with video Chain-of-Thought (CoT) annotations to improve reasoning. Additionally, they propose a Hybrid LVLMs Collaboration framework that optimally selects frames and performs reasoning, achieving superior performance on various tasks compared to existing models.'}, 'zh': {'title': 'VideoEspresso：提升视频推理的新数据集与框架', 'desc': '本论文介绍了一种新的视频问答数据集VideoEspresso，旨在解决现有数据集在视频推理任务中的不足。该数据集保留了重要的空间细节和时间连贯性，并提供了中间推理步骤的多模态注释。我们提出了一种混合的大型视觉语言模型协作框架，能够自适应选择核心帧并进行连锁思维推理。实验结果表明，我们的方法在多个任务上优于现有基线，展示了更强的视频推理能力。'}}}, {'id': 'https://huggingface.co/papers/2411.14762', 'title': 'Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction', 'url': 'https://huggingface.co/papers/2411.14762', 'abstract': 'Efficient tokenization of videos remains a challenge in training vision models that can process long videos. One promising direction is to develop a tokenizer that can encode long video clips, as it would enable the tokenizer to leverage the temporal coherence of videos better for tokenization. However, training existing tokenizers on long videos often incurs a huge training cost as they are trained to reconstruct all the frames at once. In this paper, we introduce CoordTok, a video tokenizer that learns a mapping from coordinate-based representations to the corresponding patches of input videos, inspired by recent advances in 3D generative models. In particular, CoordTok encodes a video into factorized triplane representations and reconstructs patches that correspond to randomly sampled (x,y,t) coordinates. This allows for training large tokenizer models directly on long videos without requiring excessive training resources. Our experiments show that CoordTok can drastically reduce the number of tokens for encoding long video clips. For instance, CoordTok can encode a 128-frame video with 128times128 resolution into 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar reconstruction quality. We further show that this efficient video tokenization enables memory-efficient training of a diffusion transformer that can generate 128 frames at once.', 'score': 8, 'issue_id': 756, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '9490a40884583735', 'authors': ['Huiwon Jang', 'Sihyun Yu', 'Jinwoo Shin', 'Pieter Abbeel', 'Younggyo Seo'], 'affiliations': ['KAIST', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2411.14762.jpg', 'data': {'categories': ['#long_context', '#3d', '#diffusion', '#video', '#training'], 'emoji': '🎥', 'ru': {'title': 'CoordTok: Эффективная токенизация длинных видео с помощью координатного представления', 'desc': 'CoordTok - это новый токенизатор для видео, который использует координатное представление для кодирования длинных видеоклипов. Он обучается восстанавливать патчи, соответствующие случайно выбранным координатам (x,y,t), что позволяет эффективно обрабатывать длинные видео. CoordTok значительно сокращает количество токенов, необходимых для кодирования видео, по сравнению с базовыми методами. Это делает возможным эффективное по памяти обучение диффузионного трансформера для генерации длинных видео.'}, 'en': {'title': 'Efficient Video Tokenization with CoordTok', 'desc': 'This paper presents CoordTok, a novel video tokenizer designed to efficiently encode long video clips by leveraging coordinate-based representations. Unlike traditional tokenizers that reconstruct all frames simultaneously, CoordTok uses a factorized triplane representation to map (x,y,t) coordinates to video patches, significantly reducing the number of tokens needed. The approach allows for training large models on long videos without incurring high computational costs. Experimental results demonstrate that CoordTok can encode a 128-frame video into just 1280 tokens, outperforming existing methods that require thousands of tokens for similar quality.'}, 'zh': {'title': '高效视频标记化，降低训练成本！', 'desc': '本论文提出了一种名为CoordTok的视频标记器，旨在高效处理长视频的标记化问题。CoordTok通过学习坐标表示与输入视频补丁之间的映射，利用了视频的时间一致性。与传统方法相比，CoordTok显著减少了编码长视频所需的标记数量，从而降低了训练成本。实验表明，CoordTok能够在保持重建质量的同时，将128帧视频的标记数量减少到1280个。'}}}, {'id': 'https://huggingface.co/papers/2411.14521', 'title': 'MyTimeMachine: Personalized Facial Age Transformation', 'url': 'https://huggingface.co/papers/2411.14521', 'abstract': "Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20sim40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), which combines a global aging prior with a personal photo collection (using as few as 50 images) to learn a personalized age transformation. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our approach can also be extended to videos, achieving high-quality, identity-preserving, and temporally consistent aging effects that resemble actual appearances at target ages, demonstrating its superiority over state-of-the-art approaches.", 'score': 7, 'issue_id': 755, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'd104e8a00be886bb', 'authors': ['Luchao Qi', 'Jiaye Wu', 'Bang Gong', 'Annie N. Wang', 'David W. Jacobs', 'Roni Sengupta'], 'affiliations': ['University of Maryland, College Park', 'University of North Carolina at Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.14521.jpg', 'data': {'categories': ['#architecture', '#training', '#multimodal', '#cv', '#video'], 'emoji': '👴', 'ru': {'title': 'Персонализированное старение лиц с помощью глубокого обучения', 'desc': 'Статья представляет новый метод персонализированного старения лиц на изображениях, называемый MyTimeMachine (MyTM). Авторы комбинируют глобальную модель старения с личной коллекцией фотографий для создания индивидуализированной трансформации возраста. В работе вводится Адаптерная Сеть, объединяющая персонализированные и глобальные признаки старения для генерации состаренного изображения с помощью StyleGAN2. Метод также применим к видео, обеспечивая качественные, сохраняющие идентичность и темпорально согласованные эффекты старения.'}, 'en': {'title': 'Personalized Aging: Your Face, Your Time', 'desc': "This paper presents MyTimeMachine (MyTM), a novel approach to facial aging that combines global aging knowledge with personalized photo collections. It addresses the challenge of accurately predicting an individual's appearance at a target age by utilizing as few as 50 personal images. The method employs an Adapter Network that integrates personalized and global aging features, generating realistic re-aged images using StyleGAN2. Additionally, the authors introduce three specialized loss functions to enhance the personalization of the aging process, resulting in high-quality, identity-preserving, and temporally consistent aging effects."}, 'zh': {'title': '个性化老化，真实再现', 'desc': '面部老化是一个复杂的过程，受到性别、种族、生活方式等多种因素的影响，因此很难学习到一个通用的老化模型来准确预测个体的老化情况。现有技术虽然能够生成逼真的老化效果，但重塑的图像往往与目标年龄的外貌不符，因此需要个性化处理。我们提出了MyTimeMachine（MyTM），它结合了全球老化模型和个人照片集（仅需50张图像）来学习个性化的年龄转换。我们的创新在于引入了适配器网络和三种损失函数，使得生成的老化图像能够保持身份一致性，并在视频中实现高质量的老化效果。'}}}, {'id': 'https://huggingface.co/papers/2411.14208', 'title': 'Novel View Extrapolation with Video Diffusion Priors', 'url': 'https://huggingface.co/papers/2411.14208', 'abstract': 'The field of novel view synthesis has made significant strides thanks to the development of radiance field methods. However, most radiance field techniques are far better at novel view interpolation than novel view extrapolation where the synthesis novel views are far beyond the observed training views. We design ViewExtrapolator, a novel view synthesis approach that leverages the generative priors of Stable Video Diffusion (SVD) for realistic novel view extrapolation. By redesigning the SVD denoising process, ViewExtrapolator refines the artifact-prone views rendered by radiance fields, greatly enhancing the clarity and realism of the synthesized novel views. ViewExtrapolator is a generic novel view extrapolator that can work with different types of 3D rendering such as views rendered from point clouds when only a single view or monocular video is available. Additionally, ViewExtrapolator requires no fine-tuning of SVD, making it both data-efficient and computation-efficient. Extensive experiments demonstrate the superiority of ViewExtrapolator in novel view extrapolation. Project page: https://kunhao-liu.github.io/ViewExtrapolator/.', 'score': 6, 'issue_id': 755, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'dddb7a1ecc9850f3', 'authors': ['Kunhao Liu', 'Ling Shao', 'Shijian Lu'], 'affiliations': ['Nanyang Technological University', 'UCAS-Terminus AI Lab, UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2411.14208.jpg', 'data': {'categories': ['#diffusion', '#3d', '#video'], 'emoji': '🔭', 'ru': {'title': 'Реалистичная экстраполяция новых ракурсов с помощью генеративных моделей', 'desc': 'ViewExtrapolator - это новый подход к синтезу видов, использующий генеративные возможности Stable Video Diffusion (SVD) для реалистичной экстраполяции новых ракурсов. Метод улучшает качество изображений, полученных с помощью радиационных полей, значительно повышая четкость и реалистичность синтезированных видов. ViewExtrapolator может работать с различными типами 3D-рендеринга, включая облака точек, и не требует дополнительного обучения SVD. Эксперименты подтверждают превосходство ViewExtrapolator в задаче экстраполяции новых ракурсов.'}, 'en': {'title': 'Enhancing Novel View Extrapolation with ViewExtrapolator', 'desc': 'This paper introduces ViewExtrapolator, a new method for synthesizing novel views that go beyond the original training views. It utilizes the generative capabilities of Stable Video Diffusion (SVD) to improve the quality of these extrapolated views. By modifying the SVD denoising process, the method reduces artifacts and enhances the realism of the generated images. ViewExtrapolator is versatile, working with various 3D rendering types and requiring no fine-tuning, making it efficient in both data and computation.'}, 'zh': {'title': '提升新视图外推的清晰度与真实感', 'desc': '本论文介绍了一种新的视图合成方法，称为ViewExtrapolator，旨在改善新视图外推的效果。传统的辐射场技术在新视图插值方面表现良好，但在新视图外推时效果较差。ViewExtrapolator利用稳定视频扩散（SVD）的生成先验，通过重新设计去噪过程，显著提高了合成新视图的清晰度和真实感。该方法适用于不同类型的3D渲染，并且无需对SVD进行微调，具有数据和计算效率。'}}}, {'id': 'https://huggingface.co/papers/2411.15131', 'title': 'WildLMa: Long Horizon Loco-Manipulation in the Wild', 'url': 'https://huggingface.co/papers/2411.15131', 'abstract': "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world environments, which requires the robot to (1) have skills that generalize across object configurations; (2) be capable of long-horizon task execution in diverse environments; and (3) perform complex manipulation beyond pick-and-place. Quadruped robots with manipulators hold promise for extending the workspace and enabling robust locomotion, but existing results do not investigate such a capability. This paper proposes WildLMa with three components to address these issues: (1) adaptation of learned low-level controller for VR-enabled whole-body teleoperation and traversability; (2) WildLMa-Skill -- a library of generalizable visuomotor skills acquired via imitation learning or heuristics and (3) WildLMa-Planner -- an interface of learned skills that allow LLM planners to coordinate skills for long-horizon tasks. We demonstrate the importance of high-quality training data by achieving higher grasping success rate over existing RL baselines using only tens of demonstrations. WildLMa exploits CLIP for language-conditioned imitation learning that empirically generalizes to objects unseen in training demonstrations. Besides extensive quantitative evaluation, we qualitatively demonstrate practical robot applications, such as cleaning up trash in university hallways or outdoor terrains, operating articulated objects, and rearranging items on a bookshelf.", 'score': 5, 'issue_id': 755, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': 'c38df92e9599db09', 'authors': ['Ri-Zhao Qiu', 'Yuchen Song', 'Xuanbin Peng', 'Sai Aneesh Suryadevara', 'Ge Yang', 'Minghuan Liu', 'Mazeyu Ji', 'Chengzhe Jia', 'Ruihan Yang', 'Xueyan Zou', 'Xiaolong Wang'], 'affiliations': ['MIT', 'NVIDIA', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2411.15131.jpg', 'data': {'categories': ['#rl', '#robotics', '#training', '#agi', '#transfer_learning', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'WildLMa: Универсальный робот-манипулятор для реального мира', 'desc': 'Статья представляет систему WildLMa для мобильной манипуляции роботов в реальных условиях. Система включает адаптивный низкоуровневый контроллер для телеоперации, библиотеку обобщаемых визуомоторных навыков и планировщик на основе языковых моделей для выполнения долгосрочных задач. WildLMa использует имитационное обучение и CLIP для генерализации на новые объекты, демонстрируя высокую точность захвата при небольшом количестве обучающих примеров. Система показала свою эффективность в практических задачах, таких как уборка мусора и перестановка предметов.'}, 'en': {'title': 'Empowering Robots for Real-World Manipulation with WildLMa', 'desc': 'This paper presents WildLMa, a framework designed for mobile manipulation robots to operate effectively in varied real-world settings. It includes a low-level controller for teleoperation, a library of generalizable visuomotor skills learned through imitation, and a planner that coordinates these skills for complex tasks. The approach emphasizes the significance of high-quality training data, achieving better performance in grasping tasks compared to existing reinforcement learning methods. Additionally, WildLMa utilizes CLIP for language-conditioned imitation learning, enabling the robot to adapt to new objects not seen during training.'}, 'zh': {'title': 'WildLMa：提升机器人在真实环境中的操控能力', 'desc': '本论文提出了一种名为WildLMa的移动操控机器人系统，旨在解决在多样化真实环境中执行复杂任务的挑战。该系统包括三个主要组件：适应性低级控制器、通用视觉运动技能库和长时间任务规划接口。通过模仿学习和高质量训练数据，WildLMa在抓取成功率上超越了现有的强化学习基线。该研究展示了机器人在实际应用中的潜力，如清理校园走廊垃圾和操作复杂物体。'}}}, {'id': 'https://huggingface.co/papers/2411.13127', 'title': 'Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images', 'url': 'https://huggingface.co/papers/2411.13127', 'abstract': 'Cloud segmentation is a critical challenge in remote sensing image interpretation, as its accuracy directly impacts the effectiveness of subsequent data processing and analysis. Recently, vision foundation models (VFM) have demonstrated powerful generalization capabilities across various visual tasks. In this paper, we present a parameter-efficient adaptive approach, termed Cloud-Adapter, designed to enhance the accuracy and robustness of cloud segmentation. Our method leverages a VFM pretrained on general domain data, which remains frozen, eliminating the need for additional training. Cloud-Adapter incorporates a lightweight spatial perception module that initially utilizes a convolutional neural network (ConvNet) to extract dense spatial representations. These multi-scale features are then aggregated and serve as contextual inputs to an adapting module, which modulates the frozen transformer layers within the VFM. Experimental results demonstrate that the Cloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the frozen backbone, achieves substantial performance gains. Cloud-Adapter consistently attains state-of-the-art (SOTA) performance across a wide variety of cloud segmentation datasets from multiple satellite sources, sensor series, data processing levels, land cover scenarios, and annotation granularities. We have released the source code and pretrained models at https://github.com/XavierJiezou/Cloud-Adapter to support further research.', 'score': 3, 'issue_id': 765, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': '06d32240370757a2', 'authors': ['Xuechao Zou', 'Shun Zhang', 'Kai Li', 'Shiying Wang', 'Junliang Xing', 'Lei Jin', 'Congyan Lang', 'Pin Tao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.13127.jpg', 'data': {'categories': ['#cv', '#dataset', '#open_source', '#architecture', '#training'], 'emoji': '☁️', 'ru': {'title': 'Эффективная адаптация моделей компьютерного зрения для сегментации облаков', 'desc': 'В статье представлен метод Cloud-Adapter для улучшения сегментации облаков на спутниковых снимках. Подход использует замороженную предобученную модель компьютерного зрения (VFM) и легковесный модуль пространственного восприятия. Cloud-Adapter модулирует слои трансформера VFM, используя контекстные признаки, извлеченные сверточной нейронной сетью. Метод достигает высокой точности на различных наборах данных, используя всего 0,6% обучаемых параметров основной модели.'}, 'en': {'title': 'Enhancing Cloud Segmentation with Efficient Adaptation', 'desc': 'This paper addresses the challenge of cloud segmentation in remote sensing images, which is crucial for accurate data analysis. It introduces a novel method called Cloud-Adapter, which enhances segmentation performance by using a vision foundation model (VFM) that is pretrained and kept frozen. The approach employs a lightweight convolutional neural network to extract spatial features, which are then used to adapt the transformer layers of the VFM without additional training. Experimental results show that Cloud-Adapter achieves state-of-the-art performance while using only a small fraction of the trainable parameters, making it efficient and effective for various cloud segmentation tasks.'}, 'zh': {'title': '云分割的新方法：Cloud-Adapter', 'desc': '云分割在遥感图像解读中是一个重要的挑战，其准确性直接影响后续数据处理和分析的效果。本文提出了一种名为Cloud-Adapter的参数高效自适应方法，旨在提高云分割的准确性和鲁棒性。该方法利用在通用领域数据上预训练的视觉基础模型（VFM），并保持其不变，避免了额外的训练需求。实验结果表明，Cloud-Adapter在多个卫星来源的云分割数据集上实现了最先进的性能，仅使用了冻结主干网络0.6%的可训练参数。'}}}, {'id': 'https://huggingface.co/papers/2411.15115', 'title': 'VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement', 'url': 'https://huggingface.co/papers/2411.15115', 'abstract': 'Recent text-to-video (T2V) diffusion models have demonstrated impressive generation capabilities across various domains. However, these models often generate videos that have misalignments with text prompts, especially when the prompts describe complex scenes with multiple objects and attributes. To address this, we introduce VideoRepair, a novel model-agnostic, training-free video refinement framework that automatically identifies fine-grained text-video misalignments and generates explicit spatial and textual feedback, enabling a T2V diffusion model to perform targeted, localized refinements. VideoRepair consists of four stages: In (1) video evaluation, we detect misalignments by generating fine-grained evaluation questions and answering those questions with MLLM. In (2) refinement planning, we identify accurately generated objects and then create localized prompts to refine other areas in the video. Next, in (3) region decomposition, we segment the correctly generated area using a combined grounding module. We regenerate the video by adjusting the misaligned regions while preserving the correct regions in (4) localized refinement. On two popular video generation benchmarks (EvalCrafter and T2V-CompBench), VideoRepair substantially outperforms recent baselines across various text-video alignment metrics. We provide a comprehensive analysis of VideoRepair components and qualitative examples.', 'score': 3, 'issue_id': 763, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '9d767f888609fdd4', 'authors': ['Daeun Lee', 'Jaehong Yoon', 'Jaemin Cho', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.15115.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#benchmark', '#alignment', '#video'], 'emoji': '🎬', 'ru': {'title': 'Умное исправление видео с помощью ИИ', 'desc': 'VideoRepair - это новая модель для улучшения качества видео, созданных с помощью text-to-video диффузионных моделей. Она автоматически определяет несоответствия между текстовым описанием и сгенерированным видео, особенно для сложных сцен с множеством объектов. VideoRepair генерирует детальные пространственные и текстовые рекомендации для целенаправленного улучшения проблемных областей видео. Модель состоит из четырех этапов: оценка видео, планирование улучшений, декомпозиция регионов и локализованное улучшение.'}, 'en': {'title': 'Enhancing Text-Video Alignment with VideoRepair', 'desc': 'The paper presents VideoRepair, a new framework designed to improve the alignment between text prompts and generated videos in text-to-video (T2V) diffusion models. It identifies and corrects misalignments by generating specific evaluation questions and using a multi-layer language model (MLLM) to assess the video. The framework operates in four stages: evaluating the video for misalignments, planning refinements, segmenting correctly generated areas, and finally refining the misaligned regions while keeping the accurate parts intact. VideoRepair shows significant improvements over existing methods on standard benchmarks, demonstrating its effectiveness in enhancing text-video coherence.'}, 'zh': {'title': 'VideoRepair：提升文本到视频生成的精确度', 'desc': '最近的文本到视频（T2V）扩散模型在多个领域展示了出色的生成能力。然而，这些模型在生成视频时，常常与文本提示存在不一致，尤其是在描述复杂场景时。为了解决这个问题，我们提出了VideoRepair，这是一种新颖的无模型、无训练的视频修复框架，能够自动识别细粒度的文本视频不对齐，并生成明确的空间和文本反馈，从而使T2V扩散模型能够进行有针对性的局部修正。VideoRepair通过视频评估、修复规划、区域分解和局部修复四个阶段来实现视频的优化。'}}}, {'id': 'https://huggingface.co/papers/2411.15033', 'title': 'One to rule them all: natural language to bind communication, perception and action', 'url': 'https://huggingface.co/papers/2411.15033', 'abstract': 'In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have a wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans. This paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback. The Planner Module is the core of the system where LLMs embedded in a modified ReAct framework are employed to interpret and carry out user commands. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances a robot adaptability, task execution, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robot ability to perform tasks. Using a dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue.', 'score': 2, 'issue_id': 758, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': 'f76bbfe3f8854ad7', 'authors': ['Simone Colombani', 'Dimitri Ognibene', 'Giuseppe Boccignone'], 'affiliations': ['Oversonic Robotics, Carate Brianza, Italy', 'University of Milan, Italy', 'University of Milano-Bicocca, Milan, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2411.15033.jpg', 'data': {'categories': ['#games', '#architecture', '#agents', '#robotics', '#optimization', '#agi', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Интеллектуальные роботы: взаимодействие с человеком на новом уровне', 'desc': 'Статья представляет передовую архитектуру для планирования действий роботов, интегрирующую коммуникацию, восприятие и планирование с использованием больших языковых моделей (LLM). Система переводит команды на естественном языке в исполняемые действия робота, учитывая информацию об окружающей среде и динамически обновляя планы на основе обратной связи в реальном времени. Ключевым компонентом является Модуль Планировщика, где LLM, встроенные в модифицированную структуру ReAct, интерпретируют и выполняют команды пользователя. Архитектура повышает адаптивность робота, выполнение задач и взаимодействие с пользователями в динамических средах.'}, 'en': {'title': 'Empowering Robots with Natural Language Understanding and Dynamic Adaptability', 'desc': "This paper discusses a new architecture for robots that helps them understand and follow human instructions in real-time. It combines Large Language Models (LLMs) with a planning system that allows robots to interpret natural language commands and adapt to changing environments. The core of this system is a Planner Module that uses LLMs to translate user requests into actions while considering the current state of the environment. By incorporating feedback loops and a dynamic semantic map, the architecture improves the robot's ability to work alongside humans and adjust its plans as needed."}, 'zh': {'title': '智能机器人：自然语言与动态环境的完美结合', 'desc': '近年来，人机交互领域的研究集中在开发能够理解复杂人类指令并在动态多样环境中执行任务的机器人。这些系统在个人助理和工业机器人等多个应用中具有广泛的应用，强调机器人与人类灵活、自然和安全的互动。本文提出了一种先进的机器人行动规划架构，结合了通信、感知和规划，利用大型语言模型（LLMs）将自然语言表达的命令转化为可执行的机器人动作。通过实时反馈动态更新计划，该系统提高了机器人在共享和动态环境中适应性、任务执行和与人类用户的无缝协作能力。'}}}, {'id': 'https://huggingface.co/papers/2411.15138', 'title': 'Material Anything: Generating Materials for Any 3D Object via Diffusion', 'url': 'https://huggingface.co/papers/2411.15138', 'abstract': 'We present Material Anything, a fully-automated, unified diffusion framework designed to generate physically-based materials for 3D objects. Unlike existing methods that rely on complex pipelines or case-specific optimizations, Material Anything offers a robust, end-to-end solution adaptable to objects under diverse lighting conditions. Our approach leverages a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss to improve stability and material quality. Additionally, we introduce confidence masks as a dynamic switcher within the diffusion model, enabling it to effectively handle both textured and texture-less objects across varying lighting conditions. By employing a progressive material generation strategy guided by these confidence masks, along with a UV-space material refiner, our method ensures consistent, UV-ready material outputs. Extensive experiments demonstrate our approach outperforms existing methods across a wide range of object categories and lighting conditions.', 'score': 14, 'issue_id': 776, 'pub_date': '2024-11-22', 'pub_date_card': {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'}, 'hash': '34b8f6718115f1e3', 'authors': ['Xin Huang', 'Tengfei Wang', 'Ziwei Liu', 'Qing Wang'], 'affiliations': ['Northwestern Polytechnical University', 'S-Lab, Nanyang Technological University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.15138.jpg', 'data': {'categories': ['#3d', '#architecture', '#optimization', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Универсальная генерация материалов для 3D-объектов с помощью диффузии', 'desc': 'В статье представлен Material Anything - полностью автоматизированный унифицированный фреймворк диффузии для генерации физически корректных материалов для 3D-объектов. В отличие от существующих методов, он предлагает надежное сквозное решение, адаптируемое к объектам в различных условиях освещения. Подход использует предобученную модель диффузии изображений с тройной архитектурой и функцией потерь рендеринга для улучшения стабильности и качества материалов. Также вводятся маски уверенности как динамический переключатель в модели диффузии, позволяющий эффективно обрабатывать объекты с текстурами и без них в различных условиях освещения.'}, 'en': {'title': 'Automating Realistic Material Generation for 3D Objects', 'desc': 'Material Anything is a novel framework that automates the generation of realistic materials for 3D objects using a unified diffusion approach. It simplifies the material creation process by eliminating the need for complex workflows and optimizations tailored to specific cases. The framework utilizes a pre-trained image diffusion model, enhanced with a triple-head architecture and rendering loss, to ensure high-quality and stable material outputs. By incorporating confidence masks, it dynamically adapts to different object types and lighting scenarios, resulting in consistent and UV-ready materials across various conditions.'}, 'zh': {'title': '全自动材料生成，适应多种光照条件', 'desc': '本文介绍了一种名为Material Anything的全自动统一扩散框架，旨在为3D物体生成基于物理的材料。与现有方法依赖复杂流程或特定优化不同，Material Anything提供了一种稳健的端到端解决方案，适应不同光照条件下的物体。我们的方法利用了预训练的图像扩散模型，并通过三头架构和渲染损失来提高稳定性和材料质量。此外，我们引入了置信掩码作为扩散模型中的动态切换器，使其能够有效处理有纹理和无纹理的物体。'}}}, {'id': 'https://huggingface.co/papers/2411.15466', 'title': 'Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator', 'url': 'https://huggingface.co/papers/2411.15466', 'abstract': 'Subject-driven text-to-image generation aims to produce images of a new subject within a desired context by accurately capturing both the visual characteristics of the subject and the semantic content of a text prompt. Traditional methods rely on time- and resource-intensive fine-tuning for subject alignment, while recent zero-shot approaches leverage on-the-fly image prompting, often sacrificing subject alignment. In this paper, we introduce Diptych Prompting, a novel zero-shot approach that reinterprets as an inpainting task with precise subject alignment by leveraging the emergent property of diptych generation in large-scale text-to-image models. Diptych Prompting arranges an incomplete diptych with the reference image in the left panel, and performs text-conditioned inpainting on the right panel. We further prevent unwanted content leakage by removing the background in the reference image and improve fine-grained details in the generated subject by enhancing attention weights between the panels during inpainting. Experimental results confirm that our approach significantly outperforms zero-shot image prompting methods, resulting in images that are visually preferred by users. Additionally, our method supports not only subject-driven generation but also stylized image generation and subject-driven image editing, demonstrating versatility across diverse image generation applications. Project page: https://diptychprompting.github.io/', 'score': 12, 'issue_id': 777, 'pub_date': '2024-11-23', 'pub_date_card': {'ru': '23 ноября', 'en': 'November 23', 'zh': '11月23日'}, 'hash': '288600e8c54930f4', 'authors': ['Chaehun Shin', 'Jooyoung Choi', 'Heeseung Kim', 'Sungroh Yoon'], 'affiliations': ['AIIS, ASRI, INMC, ISRC, and Interdisciplinary Program in AI, Seoul National University', 'Data Science and AI Laboratory, ECE, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2411.15466.jpg', 'data': {'categories': ['#synthetic', '#cv', '#multimodal', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Diptych Prompting: точная генерация изображений без дополнительного обучения', 'desc': 'Статья представляет новый метод генерации изображений под названием Diptych Prompting. Этот подход использует свойство диптиха в крупномасштабных моделях text-to-image для точного воспроизведения субъекта в желаемом контексте. Метод интерпретирует задачу как инпейнтинг, размещая исходное изображение в левой части диптиха и генерируя правую часть на основе текстового промпта. Diptych Prompting превосходит существующие zero-shot методы и поддерживает различные приложения генерации изображений.'}, 'en': {'title': 'Diptych Prompting: Zero-Shot Image Generation with Subject Precision', 'desc': 'This paper presents Diptych Prompting, a new method for generating images from text prompts while maintaining accurate subject alignment. Unlike traditional methods that require extensive fine-tuning, this zero-shot approach treats the task as inpainting, using a diptych format with a reference image. By focusing on the left panel for the reference and performing text-conditioned inpainting on the right, the method enhances detail and prevents unwanted content from leaking into the generated image. The results show that Diptych Prompting not only improves visual quality but also allows for versatile applications in stylized image generation and editing.'}, 'zh': {'title': 'Diptych Prompting：精准的主题驱动图像生成新方法', 'desc': '本文提出了一种新的零-shot方法，称为Diptych Prompting，旨在实现主题驱动的文本到图像生成。该方法通过将生成任务重新解释为图像修补，确保了主题的精确对齐。Diptych Prompting利用大型文本到图像模型的双联生成特性，左侧面板展示参考图像，右侧面板进行文本条件的修补。实验结果表明，该方法在视觉效果上优于传统的零-shot图像提示方法，且支持多种图像生成应用。'}}}, {'id': 'https://huggingface.co/papers/2411.14522', 'title': 'GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI', 'url': 'https://huggingface.co/papers/2411.14522', 'abstract': "Despite significant advancements in general artificial intelligence, such as GPT-4, their effectiveness in the medical domain (general medical AI, GMAI) remains constrained due to the absence of specialized medical knowledge. To address this challenge, we present GMAI-VL-5.5M, a comprehensive multimodal medical dataset created by converting hundreds of specialized medical datasets into meticulously constructed image-text pairs. This dataset features comprehensive task coverage, diverse modalities, and high-quality image-text data. Building upon this multimodal dataset, we propose GMAI-VL, a general medical vision-language model with a progressively three-stage training strategy. This approach significantly enhances the model's ability by integrating visual and textual information, thereby improving its ability to process multimodal data and support accurate diagnosis and clinical decision-making. Experimental evaluations demonstrate that GMAI-VL achieves state-of-the-art results across a wide range of multimodal medical tasks, such as visual question answering and medical image diagnosis. Our contributions include the development of the GMAI-VL-5.5M dataset, the introduction of the GMAI-VL model, and the establishment of new benchmarks in multiple medical domains. Code and dataset will be released at https://github.com/uni-medical/GMAI-VL.", 'score': 5, 'issue_id': 778, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'eb0e262f1661d5c8', 'authors': ['Tianbin Li', 'Yanzhou Su', 'Wei Li', 'Bin Fu', 'Zhe Chen', 'Ziyan Huang', 'Guoan Wang', 'Chenglong Ma', 'Ying Chen', 'Ming Hu', 'Yanjun Li', 'Pengcheng Chen', 'Xiaowei Hu', 'Zhongying Deng', 'Yuanfeng Ji', 'Jin Ye', 'Yu Qiao', 'Junjun He'], 'affiliations': ['East China Normal University', 'Fudan University', 'Monash University', 'Nanjing University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Sciences', 'Stanford University', 'University of Cambridge', 'University of Washington', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14522.jpg', 'data': {'categories': ['#agi', '#benchmark', '#multimodal', '#optimization', '#science', '#healthcare', '#dataset'], 'emoji': '🏥', 'ru': {'title': 'GMAI-VL: Мощная мультимодальная модель для медицинского ИИ', 'desc': 'Исследователи представили GMAI-VL-5.5M - обширный мультимодальный медицинский датасет, созданный путем преобразования сотен специализированных медицинских наборов данных в пары изображение-текст. На основе этого датасета была разработана модель GMAI-VL - общая медицинская модель компьютерного зрения и обработки естественного языка, обученная по трехэтапной стратегии. GMAI-VL достигает передовых результатов в широком спектре мультимодальных медицинских задач, таких как визуальные вопросно-ответные системы и диагностика медицинских изображений. Работа вносит вклад в развитие искусственного интеллекта в медицине, предоставляя новый датасет, модель и бенчмарки.'}, 'en': {'title': 'Empowering Medical AI with Multimodal Learning', 'desc': "This paper introduces GMAI-VL-5.5M, a new multimodal medical dataset designed to enhance general medical AI (GMAI) by combining various specialized medical datasets into image-text pairs. The dataset supports a wide range of medical tasks and includes high-quality data that improves the model's understanding of both visual and textual information. The authors propose a three-stage training strategy for the GMAI-VL model, which significantly boosts its performance in tasks like visual question answering and medical image diagnosis. Experimental results show that GMAI-VL sets new benchmarks in the medical domain, demonstrating its effectiveness in aiding clinical decision-making."}, 'zh': {'title': '医学领域的多模态智能突破', 'desc': '尽管通用人工智能（如GPT-4）取得了显著进展，但在医学领域的有效性仍然受到限制，因为缺乏专业的医学知识。为了解决这个问题，我们提出了GMAI-VL-5.5M，这是一个通过将数百个专业医学数据集转换为精心构建的图像-文本对而创建的综合多模态医学数据集。基于这个多模态数据集，我们提出了GMAI-VL，一个具有逐步三阶段训练策略的通用医学视觉-语言模型。实验评估表明，GMAI-VL在视觉问答和医学图像诊断等多种多模态医学任务中达到了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2411.16681', 'title': 'Factorized Visual Tokenization and Generation', 'url': 'https://huggingface.co/papers/2411.16681', 'abstract': 'Visual tokenizers are fundamental to image generation. They convert visual data into discrete tokens, enabling transformer-based models to excel at image generation. Despite their success, VQ-based tokenizers like VQGAN face significant limitations due to constrained vocabulary sizes. Simply expanding the codebook often leads to training instability and diminishing performance gains, making scalability a critical challenge. In this work, we introduce Factorized Quantization (FQ), a novel approach that revitalizes VQ-based tokenizers by decomposing a large codebook into multiple independent sub-codebooks. This factorization reduces the lookup complexity of large codebooks, enabling more efficient and scalable visual tokenization. To ensure each sub-codebook captures distinct and complementary information, we propose a disentanglement regularization that explicitly reduces redundancy, promoting diversity across the sub-codebooks. Furthermore, we integrate representation learning into the training process, leveraging pretrained vision models like CLIP and DINO to infuse semantic richness into the learned representations. This design ensures our tokenizer captures diverse semantic levels, leading to more expressive and disentangled representations. Experiments show that the proposed FQGAN model substantially improves the reconstruction quality of visual tokenizers, achieving state-of-the-art performance. We further demonstrate that this tokenizer can be effectively adapted into auto-regressive image generation. https://showlab.github.io/FQGAN', 'score': 4, 'issue_id': 780, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '966d673404fb7a77', 'authors': ['Zechen Bai', 'Jianxiong Gao', 'Ziteng Gao', 'Pichao Wang', 'Zheng Zhang', 'Tong He', 'Mike Zheng Shou'], 'affiliations': ['Amazon', 'Fudan University', 'Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.16681.jpg', 'data': {'categories': ['#optimization', '#architecture', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Факторизованная квантизация: новый шаг в масштабируемой токенизации изображений', 'desc': 'Статья представляет новый подход к визуальным токенизаторам для генерации изображений - Факторизованную Квантизацию (FQ). Этот метод разбивает большой кодбук на несколько независимых под-кодбуков, что позволяет эффективно масштабировать процесс токенизации. Авторы предлагают регуляризацию для уменьшения избыточности и повышения разнообразия между под-кодбуками. Интеграция предобученных моделей компьютерного зрения, таких как CLIP и DINO, обогащает семантическое представление токенов.'}, 'en': {'title': 'Revitalizing Image Generation with Factorized Quantization', 'desc': 'This paper presents a new method called Factorized Quantization (FQ) to improve visual tokenizers used in image generation. Traditional VQ-based tokenizers struggle with limited vocabulary sizes, which can hinder performance and scalability. FQ addresses this by breaking down a large codebook into smaller, independent sub-codebooks, reducing complexity and enhancing efficiency. Additionally, the method incorporates disentanglement regularization and representation learning to ensure diverse and rich semantic representations, leading to better image generation results.'}, 'zh': {'title': '因子化量化：提升视觉标记器的效率与表现', 'desc': '视觉标记器是图像生成的基础，它将视觉数据转换为离散的标记，使基于变换器的模型在图像生成方面表现出色。尽管VQGAN等基于VQ的标记器取得了一定成功，但由于词汇量有限，它们面临着显著的局限性。我们提出了一种新方法——因子化量化（FQ），通过将大型代码本分解为多个独立的子代码本，来解决这一问题，从而提高了视觉标记化的效率和可扩展性。实验表明，FQGAN模型显著提高了视觉标记器的重建质量，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.16594', 'title': 'From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge', 'url': 'https://huggingface.co/papers/2411.16594', 'abstract': 'Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). However, traditional methods, whether matching-based or embedding-based, often fall short of judging subtle attributes and delivering satisfactory results. Recent advancements in Large Language Models (LLMs) inspire the "LLM-as-a-judge" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection across various tasks and applications. This paper provides a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to advance this emerging field. We begin by giving detailed definitions from both input and output perspectives. Then we introduce a comprehensive taxonomy to explore LLM-as-a-judge from three dimensions: what to judge, how to judge and where to judge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and highlight key challenges and promising directions, aiming to provide valuable insights and inspire future research in this promising research area. Paper list and more resources about LLM-as-a-judge can be found at https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and https://llm-as-a-judge.github.io.', 'score': 4, 'issue_id': 778, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '56883eb77dcb5fa3', 'authors': ['Dawei Li', 'Bohan Jiang', 'Liangjie Huang', 'Alimohammad Beigi', 'Chengshuai Zhao', 'Zhen Tan', 'Amrita Bhattacharjee', 'Yuxuan Jiang', 'Canyu Chen', 'Tianhao Wu', 'Kai Shu', 'Lu Cheng', 'Huan Liu'], 'affiliations': ['Arizona State University', 'Emory University', 'Illinois Institute of Technology', 'University of California, Berkeley', 'University of Illinois Chicago', 'University of Maryland, Baltimore County'], 'pdf_title_img': 'assets/pdf/title_img/2411.16594.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#survey'], 'emoji': '⚖️', 'ru': {'title': 'LLM как судья: новая парадигма оценки в AI и NLP', 'desc': "Статья представляет собой обзор использования больших языковых моделей (LLM) в качестве судей для оценки и ранжирования в задачах искусственного интеллекта и обработки естественного языка. Авторы предлагают подробную таксономию подхода 'LLM-as-a-judge', рассматривая что, как и где оценивать. В работе также представлены бенчмарки для оценки эффективности LLM в роли судей. Статья завершается обсуждением ключевых проблем и перспективных направлений исследований в этой области."}, 'en': {'title': 'Harnessing LLMs for Enhanced AI Evaluation', 'desc': 'This paper discusses the challenges of assessment and evaluation in artificial intelligence and natural language processing, particularly focusing on the limitations of traditional methods. It introduces the innovative concept of using Large Language Models (LLMs) as judges to score, rank, or select outputs in various tasks. The authors provide a detailed framework that categorizes LLM-based judgment into three key areas: what to judge, how to judge, and where to judge. Additionally, the paper compiles benchmarks for evaluating these models and identifies future research directions to enhance the effectiveness of LLMs in assessment tasks.'}, 'zh': {'title': '大型语言模型：评判的新力量', 'desc': '本论文探讨了大型语言模型（LLM）在评估和判断中的应用，提出了“LLM作为评判者”的新范式。传统的评估方法往往无法有效判断细微的属性，而LLM能够在多种任务中进行打分、排名和选择。我们从输入和输出的角度详细定义了评判的概念，并建立了一个全面的分类法，探讨了评判的内容、方式和场所。最后，我们编制了评估LLM作为评判者的基准，并强调了关键挑战和未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2411.16657', 'title': 'DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation', 'url': 'https://huggingface.co/papers/2411.16657', 'abstract': "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.", 'score': 4, 'issue_id': 777, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '02cf3312e8d1f6ca', 'authors': ['Zun Wang', 'Jialu Li', 'Han Lin', 'Jaehong Yoon', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2411.16657.jpg', 'data': {'categories': ['#multimodal', '#3d', '#video', '#story_generation'], 'emoji': '🎬', 'ru': {'title': 'DreamRunner: От сценария к видео с помощью ИИ', 'desc': 'DreamRunner - это новый метод генерации видео по текстовому сценарию, который использует большую языковую модель для структурирования входных данных. Он применяет адаптацию на основе извлечения информации для захвата целевых движений объектов в каждой сцене. DreamRunner также предлагает новый модуль пространственно-временного 3D-внимания и внедрения приоров для точного связывания объектов и движений. Метод демонстрирует передовые результаты в согласованности персонажей, соответствии тексту и плавных переходах.'}, 'en': {'title': 'DreamRunner: Crafting Seamless Storytelling Videos from Text', 'desc': 'This paper introduces DreamRunner, a new method for generating storytelling videos from text scripts. It addresses challenges in creating complex motions and maintaining object consistency across scenes by using a large language model for scene planning and a retrieval-augmented approach for motion customization. The method incorporates a novel spatial-temporal region-based attention module to ensure precise object-motion binding and semantic control in video frames. DreamRunner outperforms existing models in character consistency and smooth transitions, showcasing its effectiveness in generating multi-object interactions and adhering to compositional text prompts.'}, 'zh': {'title': 'DreamRunner：创新的故事视频生成方法', 'desc': '故事视频生成（SVG）是一项新兴任务，旨在根据输入文本脚本创建长篇、多动作、多场景的视频。该方法面临着多个挑战，包括对象需要展现复杂的细微动作，以及多个对象在不同场景中的一致性。为了解决这些问题，我们提出了DreamRunner，这是一种新颖的故事到视频生成方法，利用大型语言模型（LLM）进行场景规划和对象布局。DreamRunner还引入了空间-时间区域基础的3D注意力机制，能够实现细粒度的对象动作绑定和逐帧语义控制，展现出在角色一致性和文本对齐方面的先进性能。'}}}, {'id': 'https://huggingface.co/papers/2411.16205', 'title': 'MH-MoE:Multi-Head Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2411.16205', 'abstract': 'Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by using the multi-head mechanism to collectively attend to information from various representation spaces within different experts. In this paper, we present a novel implementation of MH-MoE that maintains both FLOPs and parameter parity with sparse Mixture of Experts models. Experimental results on language models show that the new implementation yields quality improvements over both vanilla MoE and fine-grained MoE models. Additionally, our experiments demonstrate that MH-MoE is compatible with 1-bit Large Language Models (LLMs) such as BitNet.', 'score': 3, 'issue_id': 778, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': 'b684b6d745cb66ff', 'authors': ['Shaohan Huang', 'Xun Wu', 'Shuming Ma', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2411.16205.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Мультиголовая смесь экспертов: эффективность без компромиссов', 'desc': 'Статья представляет новую реализацию мультиголовой смеси экспертов (MH-MoE), которая сохраняет паритет по FLOP и параметрам с разреженными моделями смеси экспертов. Эксперименты на языковых моделях показывают, что новая реализация улучшает качество по сравнению с обычными MoE и мелкозернистыми MoE моделями. MH-MoE использует механизм мультиголовности для совместного внимания к информации из различных пространств представлений в разных экспертах. Исследование также демонстрирует совместимость MH-MoE с 1-битными большими языковыми моделями, такими как BitNet.'}, 'en': {'title': 'Unlocking Performance with Multi-Head Mixture-of-Experts', 'desc': 'The Multi-Head Mixture-of-Experts (MH-MoE) model enhances performance by allowing multiple heads to focus on different aspects of data from various experts. This paper introduces a new way to implement MH-MoE that keeps the same computational cost and number of parameters as traditional sparse Mixture of Experts models. Experiments conducted on language models reveal that this new approach provides better results compared to standard MoE and fine-grained MoE models. Furthermore, the findings indicate that MH-MoE can effectively work with 1-bit Large Language Models like BitNet.'}, 'zh': {'title': '多头混合专家：提升模型性能的新方法', 'desc': '多头混合专家模型（MH-MoE）通过多头机制，能够同时关注来自不同专家的多种表示空间的信息，从而展现出优越的性能。本文提出了一种新颖的MH-MoE实现，能够在计算量（FLOPs）和参数数量上与稀疏混合专家模型保持一致。实验结果表明，该新实现相较于传统的MoE和细粒度MoE模型在语言模型上有显著的质量提升。此外，我们的实验还表明MH-MoE与1位大语言模型（如BitNet）兼容。'}}}, {'id': 'https://huggingface.co/papers/2411.14486', 'title': 'The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI Quiz', 'url': 'https://huggingface.co/papers/2411.14486', 'abstract': "This research introduces a novel evaluation framework designed to assess large language models' (LLMs) ability to acknowledge uncertainty on 675 fundamentally unsolvable problems. Using a curated dataset of graduate-level grand challenge questions with intentionally unknowable answers, we evaluated twelve state-of-the-art LLMs, including both open and closed-source models, on their propensity to admit ignorance rather than generate plausible but incorrect responses. The best models scored in 62-68% accuracy ranges for admitting the problem solution was unknown in fields ranging from biology to philosophy and mathematics. We observed an inverse relationship between problem difficulty and model accuracy, with GPT-4 demonstrating higher rates of uncertainty acknowledgment on more challenging problems (35.8%) compared to simpler ones (20.0%). This pattern indicates that models may be more prone to generate speculative answers when problems appear more tractable. The study also revealed significant variations across problem categories, with models showing difficulty in acknowledging uncertainty in invention and NP-hard problems while performing relatively better on philosophical and psychological challenges. These results contribute to the growing body of research on artificial general intelligence (AGI) assessment by highlighting the importance of uncertainty recognition as a critical component of future machine intelligence evaluation. This impossibility test thus extends previous theoretical frameworks for universal intelligence testing by providing empirical evidence of current limitations in LLMs' ability to recognize their own knowledge boundaries, suggesting new directions for improving model training architectures and evaluation approaches.", 'score': 3, 'issue_id': 777, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'c30a94b30cace49a', 'authors': ['David Noever', 'Forrest McKee'], 'affiliations': ['PeopleTec, Inc., Huntsville, AL'], 'pdf_title_img': 'assets/pdf/title_img/2411.14486.jpg', 'data': {'categories': ['#architecture', '#training', '#benchmark', '#interpretability', '#agi', '#dataset'], 'emoji': '🤔', 'ru': {'title': 'Признание незнания: ключевой аспект оценки искусственного интеллекта', 'desc': 'Это исследование представляет новую систему оценки способности больших языковых моделей (LLM) признавать неопределенность на 675 принципиально нерешаемых проблемах. Двенадцать современных LLM были оценены на их склонность признавать незнание, а не генерировать правдоподобные, но неверные ответы. Лучшие модели показали точность 62-68% в признании неизвестности решения проблемы в различных областях. Исследование выявило обратную зависимость между сложностью проблемы и точностью модели, а также значительные различия между категориями проблем.'}, 'en': {'title': 'Evaluating Uncertainty: A New Benchmark for Language Models', 'desc': "This research presents a new framework to evaluate how well large language models (LLMs) recognize their own uncertainty when faced with unsolvable problems. By testing twelve advanced LLMs on a set of graduate-level questions that have no answers, the study found that the best models could admit ignorance 62-68% of the time. Interestingly, the models were more likely to acknowledge uncertainty on harder problems, with GPT-4 showing a 35.8% acknowledgment rate on difficult questions. The findings emphasize the need for better training and evaluation methods to enhance LLMs' ability to recognize their knowledge limits, which is crucial for advancing artificial general intelligence (AGI)."}, 'zh': {'title': '承认不确定性：评估大型语言模型的新视角', 'desc': '本研究提出了一种新的评估框架，用于评估大型语言模型（LLMs）在675个根本无法解决的问题上承认不确定性的能力。我们使用了一组经过精心挑选的研究生级别的重大挑战问题数据集，评估了包括开源和闭源模型在内的十二个最先进的LLMs，观察它们承认无知的倾向。结果显示，最佳模型在承认问题解决方案未知的准确率范围为62%到68%，并且在更具挑战性的问题上（如生物学、哲学和数学）表现出更高的不确定性承认率。研究还发现，不同问题类别之间存在显著差异，模型在承认发明和NP难题的不确定性时表现较差，而在哲学和心理学挑战中表现相对较好。'}}}, {'id': 'https://huggingface.co/papers/2411.16443', 'title': 'SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis', 'url': 'https://huggingface.co/papers/2411.16443', 'abstract': "Text-based generation and editing of 3D scenes hold significant potential for streamlining content creation through intuitive user interactions. While recent advances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time rendering, existing methods are often specialized and task-focused, lacking a unified framework for both generation and editing. In this paper, we introduce SplatFlow, a comprehensive framework that addresses this gap by enabling direct 3DGS generation and editing. SplatFlow comprises two main components: a multi-view rectified flow (RF) model and a Gaussian Splatting Decoder (GSDecoder). The multi-view RF model operates in latent space, generating multi-view images, depths, and camera poses simultaneously, conditioned on text prompts, thus addressing challenges like diverse scene scales and complex camera trajectories in real-world settings. Then, the GSDecoder efficiently translates these latent outputs into 3DGS representations through a feed-forward 3DGS method. Leveraging training-free inversion and inpainting techniques, SplatFlow enables seamless 3DGS editing and supports a broad range of 3D tasks-including object editing, novel view synthesis, and camera pose estimation-within a unified framework without requiring additional complex pipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K datasets, demonstrating its versatility and effectiveness in various 3D generation, editing, and inpainting-based tasks.", 'score': 2, 'issue_id': 779, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '8447f5d110a89105', 'authors': ['Hyojun Go', 'Byeongjun Park', 'Jiho Jang', 'Jin-Young Kim', 'Soonwoo Kwon', 'Changick Kim'], 'affiliations': ['KAIST', 'Twelve Labs'], 'pdf_title_img': 'assets/pdf/title_img/2411.16443.jpg', 'data': {'categories': ['#3d'], 'emoji': '🎨', 'ru': {'title': 'Универсальная система для интуитивного создания и редактирования 3D-сцен', 'desc': 'SplatFlow - это комплексная система для генерации и редактирования 3D-сцен на основе текстовых запросов. Она состоит из двух основных компонентов: многоракурсной модели выпрямленного потока и декодера гауссовского сплаттинга. Система позволяет генерировать многоракурсные изображения, карты глубины и положения камер одновременно, а затем эффективно преобразовывать их в 3D-представления. SplatFlow поддерживает широкий спектр задач 3D-моделирования, включая редактирование объектов, синтез новых ракурсов и оценку положения камеры, в рамках единой системы.'}, 'en': {'title': 'SplatFlow: Unifying 3D Scene Generation and Editing', 'desc': 'This paper presents SplatFlow, a unified framework for generating and editing 3D scenes using 3D Gaussian Splatting (3DGS). It features a multi-view rectified flow model that generates images, depths, and camera poses from text prompts, addressing challenges in scene diversity and camera movement. The framework also includes a Gaussian Splatting Decoder that converts latent outputs into 3DGS representations efficiently. SplatFlow supports various 3D tasks like object editing and novel view synthesis, demonstrating its effectiveness on multiple datasets without the need for complex pipelines.'}, 'zh': {'title': 'SplatFlow：统一的3D生成与编辑框架', 'desc': '本文介绍了一种名为SplatFlow的综合框架，旨在简化3D场景的生成和编辑。SplatFlow结合了多视角整流流模型和高斯点云解码器，能够根据文本提示同时生成多视角图像、深度和相机姿态。该框架通过无训练反演和修补技术，实现了无缝的3D高斯点云编辑，支持多种3D任务，如物体编辑和新视角合成。我们在MVImgNet和DL3DV-7K数据集上验证了SplatFlow的能力，展示了其在多种3D生成和编辑任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.16035', 'title': 'Predicting Emergent Capabilities by Finetuning', 'url': 'https://huggingface.co/papers/2411.16035', 'abstract': 'A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models. In this work, we first pose the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? We then discover a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, we can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., "emergence laws"). We validate this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, we find that, in some cases, we can accurately predict whether models trained with up to 4x more compute have emerged. Finally, we present a case study of two realistic uses for emergence prediction.', 'score': 2, 'issue_id': 779, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': 'a3e818d78a8bea58', 'authors': ['Charlie Snell', 'Eric Wallace', 'Dan Klein', 'Sergey Levine'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2411.16035.jpg', 'data': {'categories': ['#optimization', '#training', '#agi', '#benchmark', '#open_source'], 'emoji': '🔮', 'ru': {'title': 'Предсказание будущего ИИ: раскрытие тайн эмерджентности в языковых моделях', 'desc': 'Статья посвящена проблеме предсказания возникновения новых способностей у крупномасштабных языковых моделей (LLM). Авторы предлагают метод предсказания эмерджентности, основанный на дообучении моделей на конкретных задачах. Они обнаружили, что дообучение может сдвинуть точку возникновения эмерджентности в сторону менее мощных моделей. Используя этот подход, исследователи смогли точно предсказать эмерджентность для моделей, обученных с использованием в 4 раза больше вычислительных ресурсов.'}, 'en': {'title': 'Predicting Emergence: Unlocking Future LLM Capabilities', 'desc': 'This paper addresses the challenge of predicting emergent capabilities in large language models (LLMs) as they scale. While the pretraining loss of LLMs is predictable based on compute resources, their downstream performance can show unexpected jumps in capability. The authors introduce the concept of emergence prediction, which involves fine-tuning LLMs on specific tasks to determine when these emergent capabilities will appear. They validate their approach using standard NLP benchmarks and demonstrate that it is possible to predict the emergence of capabilities in future models based on the performance of smaller models.'}, 'zh': {'title': '预测语言模型能力的突破', 'desc': '本研究探讨了大型语言模型（LLM）在扩展时出现的能力预测问题。我们发现，通过对特定任务进行微调，可以提前预测未来模型在该任务上的表现。研究表明，微调可以将能力出现的临界点向能力较低的模型移动。我们在多个自然语言处理基准上验证了这一方法，并展示了如何利用这一预测能力进行实际应用。'}}}, {'id': 'https://huggingface.co/papers/2411.16341', 'title': 'From CISC to RISC: language-model guided assembly transpilation', 'url': 'https://huggingface.co/papers/2411.16341', 'abstract': "The transition from x86 to ARM architecture is becoming increasingly common across various domains, primarily driven by ARM's energy efficiency and improved performance across traditional sectors. However, this ISA shift poses significant challenges, mainly due to the extensive legacy ecosystem of x86 software and lack of portability across proprietary ecosystems and software stacks. This paper introduces CRT, a lightweight LLM-based transpiler that automatically converts x86 assembly to ARM assembly. Our approach bridges the fundamental architectural gap between x86's CISC-based and ARM's RISC-based computing paradigms while preserving program semantics and optimizing performance. We evaluate CRT on diverse real-world applications, achieving 79.25% translation accuracy from x86 to ARMv5 on our comprehensive test suite, and an 88.68% accuracy from x86 to RISC-V. In practical deployments on Apple M2 hardware (ARMv8), our transpiled code achieves 1.73times speedup compared to Apple's Rosetta 2 virtualization engine, while delivering 2.41times memory efficiency and 1.47times better energy consumption. Through testing and analysis, we show that CRT successfully navigates the CISC/RISC divide and generates correctly executable RISC code despite machine ``language'' barriers. We release our code, models, training datasets, and benchmarks at: https://ahmedheakl.github.io/asm2asm/.", 'score': 2, 'issue_id': 778, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '1d28eaae89074f91', 'authors': ['Ahmed Heakl', 'Chaimaa Abi', 'Rania Hossam', 'Abdulrahman Mahmoud'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE'], 'pdf_title_img': 'assets/pdf/title_img/2411.16341.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#benchmark'], 'emoji': '🔄', 'ru': {'title': 'Преодоление барьера CISC/RISC: автоматическая трансляция x86 в ARM с помощью ИИ', 'desc': 'Статья представляет CRT - лёгкий транспилятор на основе большой языковой модели, который автоматически конвертирует ассемблерный код x86 в ассемблер ARM. Этот подход преодолевает фундаментальный архитектурный разрыв между вычислительными парадигмами CISC (x86) и RISC (ARM), сохраняя семантику программ и оптимизируя производительность. Авторы оценивают CRT на различных реальных приложениях, достигая 79.25% точности перевода с x86 на ARMv5. В практических развертываниях на оборудовании Apple M2 (ARMv8) транспилированный код достигает ускорения в 1.73 раза по сравнению с виртуализационным движком Apple Rosetta 2.'}, 'en': {'title': 'Bridging the CISC/RISC Divide with CRT Transpiler', 'desc': "This paper presents CRT, a lightweight LLM-based transpiler designed to convert x86 assembly code into ARM assembly code. The transition from x86 to ARM architecture is challenging due to the differences in their instruction set architectures (ISAs), specifically x86's Complex Instruction Set Computing (CISC) and ARM's Reduced Instruction Set Computing (RISC). CRT effectively bridges this gap while maintaining the original program's functionality and optimizing performance. The evaluation shows that CRT achieves high translation accuracy and outperforms existing solutions in terms of speed, memory efficiency, and energy consumption on ARM hardware."}, 'zh': {'title': '轻松跨越架构鸿沟，提升性能与效率', 'desc': '本论文介绍了一种名为CRT的轻量级LLM基础的转译器，能够自动将x86汇编代码转换为ARM汇编代码。该方法解决了x86的复杂指令集（CISC）与ARM的精简指令集（RISC）之间的架构差异，同时保持程序语义并优化性能。通过在真实应用上的评估，CRT在x86到ARMv5的转换准确率达到79.25%，在x86到RISC-V的转换准确率达到88.68%。在Apple M2硬件上的实际部署中，转译后的代码相比于Apple的Rosetta 2虚拟化引擎实现了1.73倍的速度提升和2.41倍的内存效率。'}}}, {'id': 'https://huggingface.co/papers/2411.16489', 'title': 'O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?', 'url': 'https://huggingface.co/papers/2411.16489', 'abstract': "This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount.", 'score': 1, 'issue_id': 780, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '9d7613aad6cae404', 'authors': ['Zhen Huang', 'Haoyang Zou', 'Xuefeng Li', 'Yixiu Liu', 'Yuxiang Zheng', 'Ethan Chern', 'Shijie Xia', 'Yiwei Qin', 'Weizhe Yuan', 'Pengfei Liu'], 'affiliations': ['Generative AI Research Lab (GAIR)', 'NYU', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2411.16489.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#data', '#math', '#benchmark', '#transfer_learning', '#hallucinations', '#training'], 'emoji': '🧠', 'ru': {'title': 'Дистилляция знаний: скрытый путь к репликации передовых языковых моделей', 'desc': 'Эта статья представляет критический анализ текущих подходов к репликации возможностей модели O1 от OpenAI, уделяя особое внимание широко распространенному, но часто нераскрываемому использованию методов дистилляции знаний. Исследование показывает, как простая дистилляция из API O1 в сочетании с контролируемой тонкой настройкой может достичь превосходной производительности в сложных математических задачах. Эксперименты демонстрируют, что базовая модель, настроенная на десятках тысяч образцов дистиллированных из O1 цепочек рассуждений, превосходит O1-preview на американском математическом экзамене AIME при минимальной технической сложности. Авторы также исследуют возможности обобщения моделей, дистиллированных из O1, на различные задачи, включая галлюцинации, безопасность и открытые вопросно-ответные системы.'}, 'en': {'title': 'Unlocking AI Potential: The Power of Transparent Distillation', 'desc': "This paper critically analyzes the methods used to replicate the capabilities of OpenAI's O1 model, emphasizing the often hidden use of knowledge distillation techniques. The authors demonstrate that fine-tuning a base model with data distilled from O1's API can lead to better performance on complex mathematical reasoning tasks compared to the original O1 model. Their experiments reveal that models trained on O1-distilled data not only excel in math but also generalize well to other tasks, showing reduced biases and improved safety. The study advocates for transparency in AI research and highlights the importance of foundational understanding in developing advanced AI systems."}, 'zh': {'title': '知识蒸馏：提升AI模型性能的关键', 'desc': '本文对当前复制OpenAI O1模型能力的方法进行了深入分析，特别关注知识蒸馏技术的广泛使用。研究表明，通过简单的从O1 API进行蒸馏，并结合监督微调，可以在复杂的数学推理任务上实现优越的性能。我们的实验显示，经过微调的基础模型在美国邀请数学考试（AIME）中表现优于O1预览，且技术复杂性较低。此外，尽管模型仅在数学问题解决数据上进行训练，但在开放式问答任务中也展现出强大的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2411.15671', 'title': 'Best of Both Worlds: Advantages of Hybrid Graph Sequence Models', 'url': 'https://huggingface.co/papers/2411.15671', 'abstract': 'Modern sequence models (e.g., Transformers, linear RNNs, etc.) emerged as dominant backbones of recent deep learning frameworks, mainly due to their efficiency, representational power, and/or ability to capture long-range dependencies. Adopting these sequence models for graph-structured data has recently gained popularity as the alternative to Message Passing Neural Networks (MPNNs). There is, however, a lack of a common foundation about what constitutes a good graph sequence model, and a mathematical description of the benefits and deficiencies in adopting different sequence models for learning on graphs. To this end, we first present Graph Sequence Model (GSM), a unifying framework for adopting sequence models for graphs, consisting of three main steps: (1) Tokenization, which translates the graph into a set of sequences; (2) Local Encoding, which encodes local neighborhoods around each node; and (3) Global Encoding, which employs a scalable sequence model to capture long-range dependencies within the sequences. This framework allows us to understand, evaluate, and compare the power of different sequence model backbones in graph tasks. Our theoretical evaluations of the representation power of Transformers and modern recurrent models through the lens of global and local graph tasks show that there are both negative and positive sides for both types of models. Building on this observation, we present GSM++, a fast hybrid model that uses the Hierarchical Affinity Clustering (HAC) algorithm to tokenize the graph into hierarchical sequences, and then employs a hybrid architecture of Transformer to encode these sequences. Our theoretical and experimental results support the design of GSM++, showing that GSM++ outperforms baselines in most benchmark evaluations.', 'score': 1, 'issue_id': 779, 'pub_date': '2024-11-23', 'pub_date_card': {'ru': '23 ноября', 'en': 'November 23', 'zh': '11月23日'}, 'hash': '540358181ae0274e', 'authors': ['Ali Behrouz', 'Ali Parviz', 'Mahdi Karami', 'Clayton Sanford', 'Bryan Perozzi', 'Vahab Mirrokni'], 'affiliations': ['Google Research', 'New Jersey Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2411.15671.jpg', 'data': {'categories': ['#optimization', '#math', '#architecture', '#benchmark', '#graphs'], 'emoji': '🕸️', 'ru': {'title': 'Унифицированный подход к обработке графов с помощью последовательностных моделей', 'desc': 'Статья представляет унифицированный фреймворк Graph Sequence Model (GSM) для применения последовательностных моделей к графовым данным. GSM состоит из трех этапов: токенизация графа в набор последовательностей, локальное кодирование окрестностей узлов и глобальное кодирование для захвата дальних зависимостей. Авторы теоретически оценивают репрезентативную мощность трансформеров и современных рекуррентных моделей для графовых задач. На основе этого анализа предлагается гибридная модель GSM++, использующая иерархическую кластеризацию для токенизации и архитектуру трансформера для кодирования.'}, 'en': {'title': 'Unifying Sequence Models for Enhanced Graph Learning', 'desc': 'This paper introduces the Graph Sequence Model (GSM), a framework that integrates sequence models with graph-structured data. It consists of three steps: tokenization of graphs into sequences, local encoding of node neighborhoods, and global encoding to capture long-range dependencies. The authors evaluate the strengths and weaknesses of different sequence models, particularly Transformers and recurrent models, in graph tasks. They also propose GSM++, a hybrid model that enhances performance by using hierarchical tokenization and a Transformer architecture, demonstrating superior results in benchmark tests.'}, 'zh': {'title': '图序列模型：结合序列与图的力量', 'desc': '现代序列模型（如变换器和线性递归神经网络）在深度学习框架中占据主导地位，因其高效性、表示能力和捕捉长距离依赖的能力。将这些序列模型应用于图结构数据的研究逐渐受到关注，作为消息传递神经网络（MPNNs）的替代方案。本文提出了图序列模型（GSM），这是一个统一框架，包含图的标记化、局部编码和全局编码三个主要步骤。我们还提出了GSM++，一种快速混合模型，利用层次亲和聚类算法对图进行分层序列标记，并采用变换器的混合架构进行编码，实验结果表明GSM++在大多数基准评估中优于其他模型。'}}}, {'id': 'https://huggingface.co/papers/2411.14525', 'title': 'SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation', 'url': 'https://huggingface.co/papers/2411.14525', 'abstract': 'Computed Tomography (CT) is one of the most popular modalities for medical imaging. By far, CT images have contributed to the largest publicly available datasets for volumetric medical segmentation tasks, covering full-body anatomical structures. Large amounts of full-body CT images provide the opportunity to pre-train powerful models, e.g., STU-Net pre-trained in a supervised fashion, to segment numerous anatomical structures. However, it remains unclear in which conditions these pre-trained models can be transferred to various downstream medical segmentation tasks, particularly segmenting the other modalities and diverse targets. To address this problem, a large-scale benchmark for comprehensive evaluation is crucial for finding these conditions. Thus, we collected 87 public datasets varying in modality, target, and sample size to evaluate the transfer ability of full-body CT pre-trained models. We then employed a representative model, STU-Net with multiple model scales, to conduct transfer learning across modalities and targets. Our experimental results show that (1) there may be a bottleneck effect concerning the dataset size in fine-tuning, with more improvement on both small- and large-scale datasets than medium-size ones. (2) Models pre-trained on full-body CT demonstrate effective modality transfer, adapting well to other modalities such as MRI. (3) Pre-training on the full-body CT not only supports strong performance in structure detection but also shows efficacy in lesion detection, showcasing adaptability across target tasks. We hope that this large-scale open evaluation of transfer learning can direct future research in volumetric medical image segmentation.', 'score': 1, 'issue_id': 778, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'eb68ad946d69ba56', 'authors': ['Jin Ye', 'Ying Chen', 'Yanjun Li', 'Haoyu Wang', 'Zhongying Deng', 'Ziyan Huang', 'Yanzhou Su', 'Chenglong Ma', 'Yuanfeng Ji', 'Junjun He'], 'affiliations': ['East China Normal University', 'Shanghai AI Laboratory', 'Stanford University', 'University of Cambridge', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14525.jpg', 'data': {'categories': ['#benchmark', '#healthcare', '#open_source', '#training', '#transfer_learning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Универсальность предобученных КТ-моделей в медицинской сегментации', 'desc': "Статья посвящена исследованию переноса обучения моделей, предобученных на полноразмерных КТ-изображениях, на другие задачи сегментации медицинских изображений. Авторы создали масштабный бенчмарк из 87 публичных датасетов для оценки эффективности такого переноса. Результаты показывают, что предобученные модели хорошо адаптируются к другим модальностям (например, МРТ) и различным целевым задачам. Исследование выявило эффект 'бутылочного горлышка' в зависимости от размера датасета при тонкой настройке моделей."}, 'en': {'title': 'Unlocking Transfer Learning in Medical Imaging with CT Datasets', 'desc': 'This paper investigates the transferability of models pre-trained on full-body CT images for various medical segmentation tasks. It highlights the effectiveness of the STU-Net model in adapting to different imaging modalities, such as MRI, and diverse anatomical targets. The study reveals that dataset size impacts fine-tuning performance, with both small and large datasets yielding better results than medium-sized ones. Overall, the findings emphasize the potential of using large-scale CT datasets to enhance model performance in medical image segmentation tasks.'}, 'zh': {'title': '全身CT预训练模型的迁移学习能力研究', 'desc': '计算机断层扫描（CT）是医学成像中最常用的技术之一。本文探讨了在不同条件下，基于全身CT预训练模型的迁移学习能力，特别是在其他成像模态和多样化目标的分割任务中。我们收集了87个公共数据集进行评估，结果表明，数据集大小对微调有瓶颈效应，且全身CT预训练模型在迁移到其他模态（如MRI）时表现良好。我们的研究希望为未来的体积医学图像分割研究提供指导。'}}}, {'id': 'https://huggingface.co/papers/2411.16085', 'title': 'Cautious Optimizers: Improving Training with One Line of Code', 'url': 'https://huggingface.co/papers/2411.16085', 'abstract': "AdamW has been the default optimizer for transformer pretraining. For many years, our community searches for faster and more stable optimizers with only constraint positive outcomes. In this work, we propose a single-line modification in Pytorch to any momentum-based optimizer, which we rename Cautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that this modification preserves Adam's Hamiltonian function and it does not break the convergence guarantee under the Lyapunov analysis. In addition, a whole new family of optimizers is revealed by our theoretical insight. Among them, we pick the simplest one for empirical experiments, showing speed-up on Llama and MAE pretraining up to 1.47times. Code is available at https://github.com/kyleliang919/C-Optim", 'score': 1, 'issue_id': 777, 'pub_date': '2024-11-25', 'pub_date_card': {'ru': '25 ноября', 'en': 'November 25', 'zh': '11月25日'}, 'hash': '48a2e1454fdde298', 'authors': ['Kaizhao Liang', 'Lizhang Chen', 'Bo Liu', 'Qiang Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.16085.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Осторожная оптимизация: простое изменение, большой результат', 'desc': 'Статья представляет модификацию оптимизаторов на основе импульса, названную Cautious Optimizer. Авторы предлагают простое изменение в коде PyTorch, которое сохраняет функцию Гамильтона для Adam и гарантирует сходимость по анализу Ляпунова. Теоретический анализ раскрывает целое семейство новых оптимизаторов. Эмпирические эксперименты показывают ускорение предобучения Llama и MAE до 1.47 раз.'}, 'en': {'title': 'Cautious Optimizer: Speeding Up Transformer Pretraining!', 'desc': 'This paper introduces a new optimizer called the Cautious Optimizer, which is a simple modification of existing momentum-based optimizers like AdamW and Lion. The modification preserves the Hamiltonian function of Adam, ensuring that the convergence properties remain intact according to Lyapunov stability analysis. The authors demonstrate that this new optimizer can significantly speed up the pretraining of models like Llama and MAE, achieving improvements of up to 1.47 times. Additionally, the research opens the door to a new family of optimizers, expanding the options available for machine learning practitioners.'}, 'zh': {'title': '提升变换器预训练速度的新优化器', 'desc': '本文提出了一种新的优化器，称为Cautious Optimizer，旨在提高变换器预训练的速度和稳定性。通过对现有的动量优化器进行简单的修改，我们的理论分析表明，这种修改保持了Adam的哈密顿函数，并且在Lyapunov分析下不破坏收敛性保证。我们还揭示了一系列新的优化器，并选择了其中最简单的进行实证实验，结果显示在Llama和MAE预训练中速度提升可达1.47倍。相关代码已在GitHub上发布。'}}}, {'id': 'https://huggingface.co/papers/2411.12372', 'title': 'RedPajama: an Open Dataset for Training Large Language Models', 'url': 'https://huggingface.co/papers/2411.12372', 'abstract': "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.", 'score': 20, 'issue_id': 682, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 ноября', 'en': 'November 19', 'zh': '11月19日'}, 'hash': '9393337102332466', 'authors': ['Maurice Weber', 'Daniel Fu', 'Quentin Anthony', 'Yonatan Oren', 'Shane Adams', 'Anton Alexandrov', 'Xiaozhong Lyu', 'Huu Nguyen', 'Xiaozhe Yao', 'Virginia Adams', 'Ben Athiwaratkun', 'Rahul Chalamala', 'Kezhen Chen', 'Max Ryabinin', 'Tri Dao', 'Percy Liang', 'Christopher Ré', 'Irina Rish', 'Ce Zhang'], 'affiliations': ['Caltech', 'ETH Zurich', 'EleutherAI', 'Mila, Montréal, Canada', 'Ohio State University', 'Ontocord.ai', 'Princeton University', 'Stanford University', 'Together AI', 'University of Chicago', 'Université de Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2411.12372.jpg', 'data': {'categories': ['#science', '#data', '#open_source', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'RedPajama: открытый путь к прозрачным и мощным языковым моделям', 'desc': 'Статья посвящена проблемам создания открытых языковых моделей и представляет набор данных RedPajama для их обучения. Авторы выделяют три ключевых вызова: прозрачность разработки, доступ к качественным данным и наличие метаданных для курации датасетов. RedPajama-V1 воспроизводит датасет LLaMA, а RedPajama-V2 содержит необработанные веб-данные с сигналами качества. Исследования показывают, как эти сигналы могут быть использованы для создания высококачественных подмножеств данных.'}, 'en': {'title': 'Advancing Open-Source Language Models with RedPajama Datasets', 'desc': 'This paper discusses the importance of transparency and quality in the datasets used for training large language models. It identifies three main challenges: the need for clear data curation processes, access to high-quality data, and the availability of metadata for better dataset analysis. To tackle these issues, the authors introduce the RedPajama datasets, which include a comprehensive reproduction of the LLaMA training dataset and a large web-only dataset with quality signals. The findings highlight how these datasets can improve the development of open-source language models by providing high-quality data and insights into effective data curation practices.'}, 'zh': {'title': '推动开放语言模型的透明与高效', 'desc': '本论文探讨了大型语言模型在数据集构建和过滤方面的挑战，强调了透明度、数据质量和元数据可用性的重要性。我们发布了RedPajama-V1和RedPajama-V2数据集，旨在解决这些问题，提供高质量的开放数据。RedPajama数据集包含超过100万亿个标记，涵盖多个领域，并提供质量信号以帮助数据过滤。我们的研究表明，利用网络数据的质量信号可以有效地构建高质量的数据子集，推动透明且高效的语言模型的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.11925', 'title': 'Continuous Speculative Decoding for Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2411.11925', 'abstract': 'Continuous-valued Autoregressive (AR) image generation models have demonstrated notable superiority over their discrete-token counterparts, showcasing considerable reconstruction quality and higher generation fidelity. However, the computational demands of the autoregressive framework result in significant inference overhead. While speculative decoding has proven effective in accelerating Large Language Models (LLMs), their adaptation to continuous-valued visual autoregressive models remains unexplored. This work generalizes the speculative decoding algorithm from discrete tokens to continuous space. By analyzing the intrinsic properties of output distribution, we establish a tailored acceptance criterion for the diffusion distributions prevalent in such models. To overcome the inconsistency that occurred in speculative decoding output distributions, we introduce denoising trajectory alignment and token pre-filling methods. Additionally, we identify the hard-to-sample distribution in the rejection phase. To mitigate this issue, we propose a meticulous acceptance-rejection sampling method with a proper upper bound, thereby circumventing complex integration. Experimental results show that our continuous speculative decoding achieves a remarkable 2.33times speed-up on off-the-shelf models while maintaining the output distribution. Codes will be available at https://github.com/MarkXCloud/CSpD', 'score': 12, 'issue_id': 674, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 ноября', 'en': 'November 18', 'zh': '11月18日'}, 'hash': '17049106ecc06192', 'authors': ['Zili Wang', 'Robert Zhang', 'Kun Ding', 'Qi Yang', 'Fei Li', 'Shiming Xiang'], 'affiliations': ['China Tower Corporation Limited', 'Institute of Automation, Chinese Academy of Sciences, China', 'University of Chinese Academy of Sciences, China'], 'pdf_title_img': 'assets/pdf/title_img/2411.11925.jpg', 'data': {'categories': ['#optimization', '#inference', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Ускорение генерации изображений: от дискретного к непрерывному', 'desc': 'Статья представляет новый метод ускорения генерации изображений с помощью авторегрессионных моделей с непрерывными значениями. Авторы адаптируют алгоритм спекулятивного декодирования, ранее применявшийся для ускорения больших языковых моделей, к непрерывному пространству. Они вводят специальный критерий принятия для диффузионных распределений и предлагают методы выравнивания траектории шумоподавления и предварительного заполнения токенов. Экспериментальные результаты показывают 2.33-кратное ускорение без ухудшения качества выходных данных.'}, 'en': {'title': 'Speeding Up Image Generation with Continuous Speculative Decoding', 'desc': 'This paper presents a new approach to improve the speed of continuous-valued autoregressive image generation models. It adapts speculative decoding, a technique previously used in large language models, to work with continuous data. The authors introduce methods to align denoising trajectories and pre-fill tokens to enhance the output quality during the decoding process. Their experiments demonstrate that this new method can significantly speed up the generation process by over two times while preserving the quality of the generated images.'}, 'zh': {'title': '加速连续值自回归图像生成的推测解码', 'desc': '本文提出了一种针对连续值自回归图像生成模型的推测解码算法，旨在提高生成速度。通过分析输出分布的内在特性，建立了适合扩散分布的接受标准。为了解决推测解码输出分布的不一致性，本文引入了去噪轨迹对齐和令牌预填充方法。实验结果表明，该方法在保持输出分布的同时，实现了2.33倍的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2411.12044', 'title': 'ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements', 'url': 'https://huggingface.co/papers/2411.12044', 'abstract': "Recent advances in foundational Vision Language Models (VLMs) have reshaped the evaluation paradigm in computer vision tasks. These foundational models, especially CLIP, have accelerated research in open-vocabulary computer vision tasks, including Open-Vocabulary Semantic Segmentation (OVSS). Although the initial results are promising, the dense prediction capabilities of VLMs still require further improvement. In this study, we enhance the semantic segmentation performance of CLIP by introducing new modules and modifications: 1) architectural changes in the last layer of ViT and the incorporation of attention maps from the middle layers with the last layer, 2) Image Engineering: applying data augmentations to enrich input image representations, and 3) using Large Language Models (LLMs) to generate definitions and synonyms for each class name to leverage CLIP's open-vocabulary capabilities. Our training-free method, ITACLIP, outperforms current state-of-the-art approaches on segmentation benchmarks such as COCO-Stuff, COCO-Object, Pascal Context, and Pascal VOC. Our code is available at https://github.com/m-arda-aydn/ITACLIP.", 'score': 10, 'issue_id': 684, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 ноября', 'en': 'November 18', 'zh': '11月18日'}, 'hash': 'd123699ae0dacdaa', 'authors': ['M. Arda Aydın', 'Efe Mert Çırpar', 'Elvin Abdinli', 'Gozde Unal', 'Yusuf H. Sahin'], 'affiliations': ['Bilkent University', 'Istanbul Technical University', 'RWTH Aachen University', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2411.12044.jpg', 'data': {'categories': ['#open_source', '#optimization', '#training', '#architecture', '#benchmark', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'ITACLIP: Улучшение семантической сегментации без дополнительного обучения', 'desc': 'Эта статья представляет новый метод улучшения семантической сегментации изображений с использованием модели CLIP. Авторы предлагают архитектурные изменения в последнем слое ViT и включение карт внимания из средних слоев. Они также применяют аугментацию данных для обогащения представлений входных изображений. Кроме того, исследователи используют большие языковые модели для генерации определений и синонимов каждого класса, чтобы лучше использовать возможности CLIP по работе с открытым словарем.'}, 'en': {'title': 'Enhancing CLIP for Open-Vocabulary Semantic Segmentation', 'desc': "This paper discusses improvements to Vision Language Models (VLMs), particularly CLIP, for better performance in Open-Vocabulary Semantic Segmentation (OVSS). The authors propose enhancements through architectural modifications, including changes to the Vision Transformer (ViT) layers and the integration of attention maps. They also introduce data augmentation techniques to improve image representation and utilize Large Language Models (LLMs) to generate class definitions and synonyms, enhancing CLIP's open-vocabulary capabilities. The proposed method, ITACLIP, shows superior performance on various segmentation benchmarks compared to existing methods."}, 'zh': {'title': '提升CLIP的语义分割性能', 'desc': '本研究针对基础视觉语言模型（VLMs）在计算机视觉任务中的应用进行了改进，特别是CLIP模型在开放词汇语义分割（OVSS）中的表现。我们通过引入新的模块和修改，提升了CLIP的语义分割性能，包括对ViT最后一层的架构调整和中间层注意力图的结合。我们还通过图像工程技术增强输入图像的表示，并利用大型语言模型（LLMs）生成每个类别名称的定义和同义词，以充分利用CLIP的开放词汇能力。我们的无训练方法ITACLIP在COCO-Stuff、COCO-Object、Pascal Context和Pascal VOC等分割基准上超越了当前的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2411.12734', 'title': 'Soft Robotic Dynamic In-Hand Pen Spinning', 'url': 'https://huggingface.co/papers/2411.12734', 'abstract': "Dynamic in-hand manipulation remains a challenging task for soft robotic systems that have demonstrated advantages in safe compliant interactions but struggle with high-speed dynamic tasks. In this work, we present SWIFT, a system for learning dynamic tasks using a soft and compliant robotic hand. Unlike previous works that rely on simulation, quasi-static actions and precise object models, the proposed system learns to spin a pen through trial-and-error using only real-world data without requiring explicit prior knowledge of the pen's physical attributes. With self-labeled trials sampled from the real world, the system discovers the set of pen grasping and spinning primitive parameters that enables a soft hand to spin a pen robustly and reliably. After 130 sampled actions per object, SWIFT achieves 100% success rate across three pens with different weights and weight distributions, demonstrating the system's generalizability and robustness to changes in object properties. The results highlight the potential for soft robotic end-effectors to perform dynamic tasks including rapid in-hand manipulation. We also demonstrate that SWIFT generalizes to spinning items with different shapes and weights such as a brush and a screwdriver which we spin with 10/10 and 5/10 success rates respectively. Videos, data, and code are available at https://soft-spin.github.io.", 'score': 8, 'issue_id': 676, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 ноября', 'en': 'November 19', 'zh': '11月19日'}, 'hash': 'f75a2283a0a8a06f', 'authors': ['Yunchao Yao', 'Uksang Yoo', 'Jean Oh', 'Christopher G. Atkeson', 'Jeffrey Ichnowski'], 'affiliations': ['Robotics Institute at Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2411.12734.jpg', 'data': {'categories': ['#robotics'], 'emoji': '🤖', 'ru': {'title': 'Мягкая роботизированная рука осваивает динамичные манипуляции', 'desc': 'Статья представляет систему SWIFT для обучения динамическим задачам с использованием мягкой роботизированной руки. В отличие от предыдущих подходов, система учится вращать ручку методом проб и ошибок, используя только данные реального мира без предварительных знаний о физических свойствах объекта. После 130 пробных действий SWIFT достигает 100% успеха при вращении трех ручек с разным весом и распределением массы. Система также демонстрирует обобщающую способность на предметах другой формы и веса.'}, 'en': {'title': 'SWIFT: Mastering Dynamic Manipulation with Soft Robotics', 'desc': "This paper introduces SWIFT, a novel system designed for dynamic in-hand manipulation using a soft robotic hand. Unlike traditional methods that depend on simulations or precise object models, SWIFT learns to perform tasks like spinning a pen through real-world trial-and-error. The system effectively identifies optimal grasping and spinning parameters without needing prior knowledge of the object's characteristics. SWIFT demonstrates impressive generalizability, achieving a 100% success rate in spinning various pens and also successfully manipulating other objects like brushes and screwdrivers."}, 'zh': {'title': '软机器人动态操作的新突破', 'desc': '本研究提出了一种名为SWIFT的系统，用于学习动态任务，特别是在软机器人手中进行快速的物体操作。与以往依赖于模拟和精确物体模型的方法不同，SWIFT通过真实世界的数据进行试错学习，能够在没有物体物理属性先验知识的情况下，成功地旋转笔。经过130次采样操作，SWIFT在三种不同重量和分布的笔上实现了100%的成功率，展示了其在物体属性变化下的通用性和鲁棒性。该系统还能够推广到其他形状和重量的物体，如刷子和螺丝刀，分别实现了10/10和5/10的成功率，显示了软机器人在动态任务中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2411.10818', 'title': 'FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations', 'url': 'https://huggingface.co/papers/2411.10818', 'abstract': 'Sketch animations offer a powerful medium for visual storytelling, from simple flip-book doodles to professional studio productions. While traditional animation requires teams of skilled artists to draw key frames and in-between frames, existing automation attempts still demand significant artistic effort through precise motion paths or keyframe specification. We present FlipSketch, a system that brings back the magic of flip-book animation -- just draw your idea and describe how you want it to move! Our approach harnesses motion priors from text-to-video diffusion models, adapting them to generate sketch animations through three key innovations: (i) fine-tuning for sketch-style frame generation, (ii) a reference frame mechanism that preserves visual integrity of input sketch through noise refinement, and (iii) a dual-attention composition that enables fluid motion without losing visual consistency. Unlike constrained vector animations, our raster frames support dynamic sketch transformations, capturing the expressive freedom of traditional animation. The result is an intuitive system that makes sketch animation as simple as doodling and describing, while maintaining the artistic essence of hand-drawn animation.', 'score': 6, 'issue_id': 687, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 ноября', 'en': 'November 16', 'zh': '11月16日'}, 'hash': '2aee0f98e4694b74', 'authors': ['Hmrishav Bandyopadhyay', 'Yi-Zhe Song'], 'affiliations': ['SketchX, CVSSP, University of Surrey, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2411.10818.jpg', 'data': {'categories': ['#story_generation', '#cv', '#video', '#multimodal', '#diffusion'], 'emoji': '✏️', 'ru': {'title': 'Оживляем скетчи силой слова и ИИ', 'desc': 'FlipSketch - это система, которая автоматизирует создание анимированных скетчей. Она использует диффузионные модели для генерации последовательности кадров на основе исходного рисунка и текстового описания движения. Ключевые инновации включают дообучение модели на скетчах, механизм сохранения визуальной целостности и двойное внимание для плавности движения. Система позволяет создавать выразительные анимации так же просто, как рисовать и описывать идею.'}, 'en': {'title': 'Doodle Your Dreams: Effortless Sketch Animation with FlipSketch!', 'desc': 'FlipSketch is a novel system that simplifies the process of creating sketch animations by allowing users to draw their ideas and describe desired movements. It leverages motion priors from text-to-video diffusion models, which are fine-tuned for generating sketch-style frames. The system incorporates a reference frame mechanism to ensure the visual integrity of the sketches and employs dual-attention composition for smooth motion while preserving consistency. This approach enables dynamic transformations in raster frames, making sketch animation accessible and intuitive, akin to traditional hand-drawn techniques.'}, 'zh': {'title': '让草图动画创作变得简单如涂鸦', 'desc': 'FlipSketch 是一个新系统，旨在简化草图动画的制作过程。用户只需绘制草图并描述动画的运动方式，无需复杂的关键帧设置。该系统利用文本到视频扩散模型的运动先验，通过三项创新来生成草图动画。最终，FlipSketch 使得草图动画的创作变得像涂鸦一样简单，同时保留了手绘动画的艺术性。'}}}, {'id': 'https://huggingface.co/papers/2411.12275', 'title': 'Building Trust: Foundations of Security, Safety and Transparency in AI', 'url': 'https://huggingface.co/papers/2411.12275', 'abstract': 'This paper explores the rapidly evolving ecosystem of publicly available AI models, and their potential implications on the security and safety landscape. As AI models become increasingly prevalent, understanding their potential risks and vulnerabilities is crucial. We review the current security and safety scenarios while highlighting challenges such as tracking issues, remediation, and the apparent absence of AI model lifecycle and ownership processes. Comprehensive strategies to enhance security and safety for both model developers and end-users are proposed. This paper aims to provide some of the foundational pieces for more standardized security, safety, and transparency in the development and operation of AI models and the larger open ecosystems and communities forming around them.', 'score': 6, 'issue_id': 682, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 ноября', 'en': 'November 19', 'zh': '11月19日'}, 'hash': '628941b4647bf155', 'authors': ['Huzaifa Sidhpurwala', 'Garth Mollett', 'Emily Fox', 'Mark Bestavros', 'Huamin Chen'], 'affiliations': ['Red Hat'], 'pdf_title_img': 'assets/pdf/title_img/2411.12275.jpg', 'data': {'categories': ['#open_source', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Безопасность публичных моделей ИИ: вызовы и решения', 'desc': 'Статья исследует экосистему публично доступных моделей искусственного интеллекта и их потенциальное влияние на безопасность. Авторы рассматривают текущие сценарии безопасности, выделяя проблемы отслеживания, устранения уязвимостей и отсутствия процессов жизненного цикла моделей ИИ. Предлагаются комплексные стратегии повышения безопасности как для разработчиков моделей, так и для конечных пользователей. Цель работы - заложить основу для стандартизации безопасности и прозрачности в разработке и эксплуатации моделей ИИ.'}, 'en': {'title': 'Securing the Future of Open AI Models', 'desc': 'This paper examines the growing availability of AI models and their impact on security and safety. It emphasizes the importance of identifying risks and vulnerabilities associated with these models as they become more common. The authors discuss challenges like tracking model usage, addressing security issues, and the lack of clear ownership and lifecycle management for AI models. They propose strategies to improve security and safety for developers and users, aiming to establish standards for transparency in AI model development and operation.'}, 'zh': {'title': '提升人工智能模型的安全与透明性', 'desc': '本文探讨了公开可用的人工智能模型快速发展的生态系统及其对安全和安全性影响的潜在含义。随着人工智能模型的普及，理解其潜在风险和脆弱性变得至关重要。我们回顾了当前的安全和安全场景，并强调了跟踪问题、修复措施以及人工智能模型生命周期和所有权流程缺失等挑战。本文提出了增强模型开发者和最终用户安全与安全性的综合策略，旨在为人工智能模型及其周围开放生态系统和社区的发展提供更标准化的安全性、透明性和安全性的基础。'}}}, {'id': 'https://huggingface.co/papers/2411.10161', 'title': 'SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning', 'url': 'https://huggingface.co/papers/2411.10161', 'abstract': "Existing Image Quality Assessment (IQA) methods achieve remarkable success in analyzing quality for overall image, but few works explore quality analysis for Regions of Interest (ROIs). The quality analysis of ROIs can provide fine-grained guidance for image quality improvement and is crucial for scenarios focusing on region-level quality. This paper proposes a novel network, SEAGULL, which can SEe and Assess ROIs quality with GUidance from a Large vision-Language model. SEAGULL incorporates a vision-language model (VLM), masks generated by Segment Anything Model (SAM) to specify ROIs, and a meticulously designed Mask-based Feature Extractor (MFE) to extract global and local tokens for specified ROIs, enabling accurate fine-grained IQA for ROIs. Moreover, this paper constructs two ROI-based IQA datasets, SEAGULL-100w and SEAGULL-3k, for training and evaluating ROI-based IQA. SEAGULL-100w comprises about 100w synthetic distortion images with 33 million ROIs for pre-training to improve the model's ability of regional quality perception, and SEAGULL-3k contains about 3k authentic distortion ROIs to enhance the model's ability to perceive real world distortions. After pre-training on SEAGULL-100w and fine-tuning on SEAGULL-3k, SEAGULL shows remarkable performance on fine-grained ROI quality assessment. Code and datasets are publicly available at the https://github.com/chencn2020/Seagull.", 'score': 4, 'issue_id': 684, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': 'f2475fbc98477ad9', 'authors': ['Zewen Chen', 'Juan Wang', 'Wen Wang', 'Sunhan Xu', 'Hang Xiong', 'Yun Zeng', 'Jian Guo', 'Shuxun Wang', 'Chunfeng Yuan', 'Bing Li', 'Weiming Hu'], 'affiliations': ['Beijing Jiaotong University', 'Beijing Union University', 'China University of Petroleum', 'PeopleAI Inc.', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'School of Information Science and Technology, ShanghaiTech University', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.10161.jpg', 'data': {'categories': ['#dataset', '#open_source', '#cv', '#synthetic'], 'emoji': '🦅', 'ru': {'title': 'SEAGULL: Точная оценка качества регионов интереса в изображениях', 'desc': 'Статья представляет SEAGULL - новую нейронную сеть для оценки качества изображений в регионах интереса (ROI). SEAGULL использует модель машинного зрения и языка, маски из Segment Anything Model и специально разработанный экстрактор признаков на основе масок для точной оценки качества ROI. Авторы также создали два набора данных для обучения и оценки: SEAGULL-100w с синтетическими искажениями и SEAGULL-3k с реальными искажениями. После предобучения и тонкой настройки SEAGULL показывает выдающиеся результаты в детальной оценке качества ROI.'}, 'en': {'title': 'SEAGULL: Fine-Grained Quality Assessment for Image Regions', 'desc': 'This paper introduces SEAGULL, a novel network designed for assessing the quality of Regions of Interest (ROIs) in images, which is often overlooked by traditional Image Quality Assessment (IQA) methods. By leveraging a vision-language model and masks from the Segment Anything Model, SEAGULL effectively extracts both global and local features to provide detailed quality evaluations for specified ROIs. The authors also present two new datasets, SEAGULL-100w and SEAGULL-3k, which are used to train and evaluate the model, enhancing its ability to perceive both synthetic and real-world distortions. The results demonstrate that SEAGULL significantly improves fine-grained ROI quality assessment, making it a valuable tool for image quality enhancement.'}, 'zh': {'title': '精细化区域质量评估的新方法', 'desc': '现有的图像质量评估方法在整体图像质量分析上取得了显著成功，但对感兴趣区域（ROI）的质量分析研究较少。本文提出了一种新颖的网络SEAGULL，能够通过大型视觉-语言模型的指导来评估ROI的质量。SEAGULL结合了视觉-语言模型（VLM）、由Segment Anything Model（SAM）生成的掩码来指定ROI，以及精心设计的基于掩码的特征提取器（MFE），实现了对ROI的精细质量评估。此外，本文构建了两个基于ROI的质量评估数据集SEAGULL-100w和SEAGULL-3k，用于训练和评估ROI的质量评估。'}}}, {'id': 'https://huggingface.co/papers/2411.12240', 'title': 'Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages', 'url': 'https://huggingface.co/papers/2411.12240', 'abstract': "Large Language Models (LLMs) based on transformer architectures have revolutionized a variety of domains, with tokenization playing a pivotal role in their pre-processing and fine-tuning stages. In multilingual models, particularly those tailored for Indic languages, effective tokenization is crucial for optimizing performance. This paper presents a comprehensive evaluation of tokenizers used by 12 LLMs across all 22 official languages of India, with a focus on comparing the efficiency of their tokenization processes. We employed the Normalized Sequence Length (NSL) as a key metric in our analysis. Our findings reveal that the SUTRA tokenizer outperforms all other models, including several Indic-specific models, excelling in 14 languages. Notable insights include the SUTRA tokenizer's superior handling of Indic languages, GPT-4o's advancement over its predecessor GPT-4 in processing Indian languages, and the limited performance of Project Indus in certain languages. This study underscores the critical importance of developing targeted tokenization strategies for multilingual and Indic-centric models, laying the groundwork for future improvements in tokenizer design to enhance linguistic coverage and model efficiency.", 'score': 2, 'issue_id': 675, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 ноября', 'en': 'November 19', 'zh': '11月19日'}, 'hash': 'aee934b73b340b71', 'authors': ['S. Tamang', 'D. J. Bora'], 'affiliations': ['Department of IT The Assam Kaziranga University Jorhat, India'], 'pdf_title_img': 'assets/pdf/title_img/2411.12240.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#data', '#dataset'], 'emoji': '🇮🇳', 'ru': {'title': 'SUTRA: Лидер токенизации для индийских языков в больших языковых моделях', 'desc': 'В статье представлен анализ токенизаторов, используемых в 12 больших языковых моделях (LLM) для 22 официальных языков Индии. Исследователи использовали метрику нормализованной длины последовательности (NSL) для оценки эффективности токенизации. Результаты показали, что токенизатор SUTRA превзошел другие модели, включая специализированные для индийских языков, показав лучшие результаты для 14 языков. Исследование подчеркивает важность разработки целевых стратегий токенизации для многоязычных моделей и моделей, ориентированных на индийские языки.'}, 'en': {'title': 'Optimizing Tokenization for Multilingual Mastery', 'desc': 'This paper evaluates the effectiveness of tokenizers used in Large Language Models (LLMs) for all 22 official languages of India, emphasizing the importance of tokenization in multilingual contexts. The study introduces the Normalized Sequence Length (NSL) as a metric to assess the efficiency of different tokenizers. Results indicate that the SUTRA tokenizer significantly outperforms other models, particularly in handling Indic languages. The findings highlight the need for specialized tokenization strategies to improve the performance of LLMs in diverse linguistic settings.'}, 'zh': {'title': '优化多语言模型的分词策略', 'desc': '本论文探讨了基于变换器架构的大型语言模型（LLMs）中的分词技术，特别是在印度官方语言中的应用。我们对12种LLMs使用的分词器进行了全面评估，重点比较了它们的分词效率。研究结果显示，SUTRA分词器在14种语言中表现优于其他模型，尤其是在处理印度语言方面。该研究强调了为多语言和以印度语言为中心的模型开发针对性分词策略的重要性，以提高语言覆盖率和模型效率。'}}}, {'id': 'https://huggingface.co/papers/2410.24024', 'title': 'AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents', 'url': 'https://huggingface.co/papers/2410.24024', 'abstract': 'Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.', 'score': 48, 'issue_id': 422, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '4ba16ad433c7511f', 'authors': ['Yifan Xu', 'Xiao Liu', 'Xueqiao Sun', 'Siyi Cheng', 'Hao Yu', 'Hanyu Lai', 'Shudan Zhang', 'Dan Zhang', 'Jie Tang', 'Yuxiao Dong'], 'affiliations': ['Tsinghua University', 'Peking University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24024.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#training', '#dataset', '#open_source', '#games', '#agents'], 'emoji': '🤖', 'ru': {'title': 'AndroidLab: Революция в обучении Android-агентов', 'desc': 'Статья представляет AndroidLab - систематическую среду для разработки и оценки агентов на базе Android. Эта среда включает в себя операционное окружение с различными модальностями, пространством действий и воспроизводимым эталоном. AndroidLab поддерживает как большие языковые модели (LLM), так и мультимодальные модели (LMM) в одном пространстве действий. С помощью AndroidLab авторы разработали набор данных Android Instruction и обучили шесть моделей с открытым исходным кодом, значительно повысив их эффективность.'}, 'en': {'title': 'Empowering Android Agents with AndroidLab: A New Benchmark Framework', 'desc': 'This paper introduces AndroidLab, a comprehensive framework designed for training and evaluating Android agents. It provides a structured environment that includes various modalities and a defined action space, allowing for systematic testing of both open-source and closed-source models. The framework supports large language models (LLMs) and multimodal models (LMMs), facilitating their development and benchmarking across 138 tasks in nine different applications. By utilizing AndroidLab, the authors significantly improved the success rates of LLMs and LMMs, demonstrating its effectiveness in enhancing the performance of Android agents.'}, 'zh': {'title': 'AndroidLab：提升安卓代理的训练与评估', 'desc': '自主代理在与现实世界互动中变得越来越重要，尤其是安卓代理。现有的安卓代理训练和评估研究缺乏系统性，无法全面比较开源和闭源模型。我们提出了AndroidLab，这是一个系统化的安卓代理框架，支持多种操作环境和动作空间。通过AndroidLab，我们开发了安卓指令数据集，并显著提高了大语言模型和多模态模型的成功率。'}}}, {'id': 'https://huggingface.co/papers/2411.02355', 'title': '"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization', 'url': 'https://huggingface.co/papers/2411.02355', 'abstract': 'Despite the popularity of large language model (LLM) quantization for inference acceleration, significant uncertainty remains regarding the accuracy-performance trade-offs associated with various quantization formats. We present a comprehensive empirical study of quantized accuracy, evaluating popular quantization formats (FP8, INT8, INT4) across academic benchmarks and real-world tasks, on the entire Llama-3.1 model family. Additionally, our study examines the difference in text generated by quantized models versus their uncompressed counterparts. Beyond benchmarks, we also present a couple of quantization improvements which allowed us to obtain state-of-the-art accuracy recovery results. Our investigation, encompassing over 500,000 individual evaluations, yields several key findings: (1) FP8 weight and activation quantization (W8A8-FP) is lossless across all model scales, (2) INT8 weight and activation quantization (W8A8-INT), when properly tuned, incurs surprisingly low 1-3% accuracy degradation, and (3) INT4 weight-only quantization (W4A16-INT) is competitive with 8-bit integer weight and activation quantization. To address the question of the "best" format for a given deployment environment, we conduct inference performance analysis using the popular open-source vLLM framework on various GPU architectures. We find that W4A16 offers the best cost-efficiency for synchronous deployments, and for asynchronous deployment on mid-tier GPUs. At the same time, W8A8 formats excel in asynchronous "continuous batching" deployment of mid- and large-size models on high-end GPUs. Our results provide a set of practical guidelines for deploying quantized LLMs across scales and performance requirements.', 'score': 44, 'issue_id': 431, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '8cc3629c4f7e76a1', 'authors': ['Eldar Kurtic', 'Alexandre Marques', 'Shubhra Pandit', 'Mark Kurtz', 'Dan Alistarh'], 'affiliations': ['Neural Magic', 'Institute of Science and Technology Austria'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02355.jpg', 'data': {'categories': ['#benchmark', '#inference', '#optimization', '#training', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'Оптимальное квантование LLM: баланс между точностью и производительностью', 'desc': 'Это исследование посвящено квантованию больших языковых моделей (LLM) для ускорения вывода. Авторы провели комплексный эмпирический анализ различных форматов квантования (FP8, INT8, INT4) на семействе моделей Llama-3.1, оценивая их точность на академических тестах и реальных задачах. Результаты показывают, что квантование FP8 практически безошибочно для всех размеров моделей, а INT8 при правильной настройке дает снижение точности всего на 1-3%. Исследование также включает анализ производительности на различных GPU архитектурах, предоставляя практические рекомендации по развертыванию квантованных LLM.'}, 'en': {'title': 'Optimizing Quantization for Efficient Large Language Model Deployment', 'desc': 'This paper investigates the accuracy and performance trade-offs of different quantization formats for large language models (LLMs), specifically focusing on FP8, INT8, and INT4 quantization methods. The authors conducted extensive evaluations on the Llama-3.1 model family, analyzing over 500,000 instances to determine the impact of quantization on model accuracy and text generation. Key findings indicate that FP8 quantization is lossless, while INT8 can achieve minimal accuracy loss with proper tuning, and INT4 quantization remains competitive. The study also provides practical guidelines for selecting the optimal quantization format based on deployment scenarios and GPU architectures.'}, 'zh': {'title': '量化模型的最佳选择与性能优化', 'desc': '本文研究了大语言模型（LLM）量化对推理加速的影响，特别关注不同量化格式（如FP8、INT8、INT4）在准确性和性能之间的权衡。我们对Llama-3.1模型系列进行了全面的实证研究，评估了量化模型在学术基准和实际任务中的表现。研究发现，FP8量化在所有模型规模上都是无损的，而经过适当调优的INT8量化仅有1-3%的准确性下降。此外，我们还提出了一些量化改进方法，帮助实现了最先进的准确性恢复结果。'}}}, {'id': 'https://huggingface.co/papers/2411.02337', 'title': 'WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning', 'url': 'https://huggingface.co/papers/2411.02337', 'abstract': "Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL's effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems.", 'score': 36, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'a9af7d20e52c1f39', 'authors': ['Zehan Qi', 'Xiao Liu', 'Iat Long Iong', 'Hanyu Lai', 'Xueqiao Sun', 'Xinyue Yang', 'Jiadai Sun', 'Yu Yang', 'Shuntian Yao', 'Tianjie Zhang', 'Wei Xu', 'Jie Tang', 'Yuxiao Dong'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02337.jpg', 'data': {'categories': ['#small_models', '#reasoning', '#rl', '#benchmark', '#optimization', '#training', '#open_source', '#agents', '#architecture'], 'emoji': '🕸️', 'ru': {'title': 'WebRL: Открытые языковые модели превосходят проприетарные в веб-задачах', 'desc': 'Статья представляет WebRL - фреймворк обучения с подкреплением для создания веб-агентов на основе открытых языковых моделей. WebRL решает проблемы нехватки обучающих задач, разреженной обратной связи и смещения распределения политики в онлайн-обучении. Фреймворк включает самоэволюционирующий учебный план, модель вознаграждения на основе результатов и адаптивные стратегии обучения с подкреплением. Применение WebRL значительно улучшило производительность открытых моделей Llama-3.1 и GLM-4 в задачах веб-взаимодействия, превзойдя даже проприетарные модели GPT-4.'}, 'en': {'title': 'Empowering Open LLMs for Superior Web Performance with WebRL', 'desc': 'This paper presents WebRL, a novel framework that enhances open large language models (LLMs) for web-based tasks through reinforcement learning. It tackles challenges such as limited training tasks, sparse feedback, and policy drift by implementing a self-evolving curriculum that generates new tasks from failures, a robust outcome-supervised reward model, and adaptive learning strategies. The framework significantly improves the performance of open models like Llama-3.1 and GLM-4, achieving success rates that surpass proprietary models like GPT-4-Turbo. Overall, WebRL demonstrates a promising approach to making powerful web agents more accessible by leveraging open-source LLMs.'}, 'zh': {'title': 'WebRL：开放LLM的自我进化网络代理训练框架', 'desc': '大型语言模型（LLMs）在网络任务中展现了出色的潜力，但现有的LLM网络代理依赖昂贵的专有API，而开放的LLM缺乏决策能力。本文提出了WebRL，一个自我进化的在线课程强化学习框架，旨在利用开放的LLM训练高性能的网络代理。WebRL解决了构建LLM网络代理的三个关键挑战，包括训练任务稀缺、反馈信号稀疏和在线学习中的策略分布漂移。通过WebRL，我们将开放的Llama-3.1和GLM-4模型转变为高效的网络代理，显著提高了它们的成功率，超越了现有的专有模型。'}}}, {'id': 'https://huggingface.co/papers/2411.02385', 'title': 'How Far is Video Generation from World Model: A Physical Law Perspective', 'url': 'https://huggingface.co/papers/2411.02385', 'abstract': 'OpenAI\'s Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization. We developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws. This provides an unlimited supply of data for large-scale experimentation and enables quantitative evaluation of whether the generated videos adhere to physical laws. We trained diffusion-based video generation models to predict object movements based on initial frames. Our scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios. Further experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit "case-based" generalization behavior, i.e., mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color > size > velocity > shape. Our study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora\'s broader success. See our project page at https://phyworld.github.io', 'score': 32, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '771395452deb397f', 'authors': ['Bingyi Kang', 'Yang Yue', 'Rui Lu', 'Zhijie Lin', 'Yang Zhao', 'Kaixin Wang', 'Gao Huang', 'Jiashi Feng'], 'affiliations': ['Bytedance Research', 'Tsinghua University', 'Technion'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02385.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#benchmark', '#cv', '#video', '#training', '#open_source', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Генеративные видеомодели не раскрывают физические законы при масштабировании', 'desc': 'Исследование оценивает способность моделей генерации видео изучать фундаментальные физические законы из визуальных данных. Авторы разработали 2D симуляцию для создания видео, управляемых законами классической механики. Эксперименты показали, что модели достигают идеальной генерализации в рамках распределения, но не справляются с экстраполяцией на новые сценарии. Результаты указывают на то, что модели не абстрагируют общие физические правила, а скорее демонстрируют обобщение на основе конкретных примеров.'}, 'en': {'title': 'Unlocking Video Generation: Beyond Scaling to Understand Physics', 'desc': 'This paper investigates the ability of video generation models to learn and predict physical laws from visual data without human input. The authors created a 2D simulation environment to generate videos based on classical mechanics, allowing for extensive testing of model performance. They found that while the models excelled in familiar scenarios, they struggled with new, unseen situations, indicating a reliance on specific examples rather than generalizing physical principles. The study concludes that simply increasing model size is not enough for these systems to grasp fundamental laws of physics, highlighting the need for improved generalization techniques.'}, 'zh': {'title': '视频生成模型与物理法则的探索', 'desc': '本文探讨了视频生成模型在学习物理法则方面的潜力。我们开发了一个二维模拟测试平台，用于生成受经典力学法则支配的视频数据。通过对模型在不同场景下的表现进行评估，我们发现模型在已知分布内表现良好，但在未知分布中则出现失败。研究表明，模型在推广新案例时，优先考虑的因素依次为颜色、大小、速度和形状，而不是抽象出一般的物理规则。'}}}, {'id': 'https://huggingface.co/papers/2411.02336', 'title': 'MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D', 'url': 'https://huggingface.co/papers/2411.02336', 'abstract': 'Texturing is a crucial step in the 3D asset production workflow, which enhances the visual appeal and diversity of 3D assets. Despite recent advancements in Text-to-Texture (T2T) generation, existing methods often yield subpar results, primarily due to local discontinuities, inconsistencies across multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle these challenges, we propose a novel generation-refinement 3D texturing framework called MVPaint, which can generate high-resolution, seamless textures while emphasizing multi-view consistency. MVPaint mainly consists of three key modules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model, MVPaint first simultaneously generates multi-view images by employing an SMG model, which leads to coarse texturing results with unpainted parts due to missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete 3D texturing, we introduce the S3I method, specifically designed to effectively texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore, MVPaint employs a UVR module to improve the texture quality in the UV space, which first performs a UV-space Super-Resolution, followed by a Spatial-aware Seam-Smoothing algorithm for revising spatial texturing discontinuities caused by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the Objaverse T2T benchmark and the GSO T2T benchmark, based on selected high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset, respectively. Extensive experimental results demonstrate that MVPaint surpasses existing state-of-the-art methods. Notably, MVPaint could generate high-fidelity textures with minimal Janus issues and highly enhanced cross-view consistency.', 'score': 23, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '0f7de0fb0e5cdd86', 'authors': ['Wei Cheng', 'Juncheng Mu', 'Xianfang Zeng', 'Xin Chen', 'Anqi Pang', 'Chi Zhang', 'Zhibin Wang', 'Bin Fu', 'Gang Yu', 'Ziwei Liu', 'Liang Pan'], 'affiliations': ['Tencent PCG', 'Shanghai AI Laboratory', 'S-Lab, NTU', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02336.jpg', 'data': {'categories': ['#benchmark', '#cv', '#optimization', '#dataset', '#games', '#3d'], 'emoji': '🎨', 'ru': {'title': 'MVPaint: революция в автоматическом текстурировании 3D-моделей', 'desc': 'В статье представлена новая система MVPaint для генерации высококачественных текстур для 3D-моделей. Система состоит из трех ключевых модулей: синхронизированной многоракурсной генерации, пространственно-ориентированного 3D-инпейнтинга и уточнения UV-развертки. MVPaint решает проблемы локальных разрывов, несогласованности между ракурсами и зависимости от UV-развертки, характерные для существующих методов текстурирования. Авторы также создали два новых бенчмарка для оценки качества генерации текстур по тексту.'}, 'en': {'title': 'MVPaint: Revolutionizing 3D Texturing with Multi-View Consistency', 'desc': 'This paper introduces MVPaint, a new framework for generating high-quality textures for 3D models. It addresses common issues in Text-to-Texture (T2T) generation, such as local discontinuities and inconsistencies across different views. MVPaint consists of three main components: Synchronized Multi-view Generation for initial texture creation, Spatial-aware 3D Inpainting for filling in unpainted areas, and UV Refinement for enhancing texture quality in UV space. The framework outperforms existing methods, providing seamless textures with improved visual consistency across multiple perspectives.'}, 'zh': {'title': 'MVPaint：提升3D纹理生成的一体化解决方案', 'desc': '本文提出了一种新的3D纹理生成与优化框架MVPaint，旨在解决现有文本到纹理生成方法中的局部不连续性和多视图一致性问题。MVPaint包含三个主要模块：同步多视图生成（SMG）、空间感知3D修补（S3I）和UV优化（UVR），能够生成高分辨率、无缝的纹理。通过SMG模块，MVPaint可以同时生成多视图图像，而S3I模块则专注于填补未观察到的区域。最后，UVR模块通过超分辨率和缝合平滑算法来提升UV空间中的纹理质量，实验结果表明MVPaint在纹理生成方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2411.00860', 'title': 'Survey of Cultural Awareness in Language Models: Text and Beyond', 'url': 'https://huggingface.co/papers/2411.00860', 'abstract': 'Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology. In this paper, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking the definitions of culture from anthropology and psychology as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of Human-Computer Interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature.', 'score': 23, 'issue_id': 425, 'pub_date': '2024-10-30', 'pub_date_card': {'ru': '30 октября', 'en': 'October 30', 'zh': '10月30日'}, 'hash': '8e4a6348db0215e3', 'authors': ['Siddhesh Pawar', 'Junyeong Park', 'Jiho Jin', 'Arnav Arora', 'Junho Myung', 'Srishti Yadav', 'Faiz Ghifari Haznitrama', 'Inhwa Song', 'Alice Oh', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen, Denmark', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'University of Copenhagen, Denmark', 'University of Copenhagen, Denmark', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'KAIST, Republic of Korea', 'University of Copenhagen, Denmark'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00860.jpg', 'data': {'categories': ['#science', '#benchmark', '#multilingual', '#multimodal', '#ethics', '#training', '#dataset', '#survey', '#architecture', '#alignment'], 'emoji': '🌍', 'ru': {'title': 'Культурная инклюзивность в эпоху больших языковых моделей', 'desc': 'Эта статья посвящена внедрению культурной осведомленности в крупномасштабные языковые модели (LLM). Авторы рассматривают методологии создания кросс-культурных датасетов и стратегии включения культурных аспектов в задачи обработки естественного языка. Обсуждаются этические последствия культурной адаптации LLM и роль взаимодействия человека с компьютером в этом процессе. Статья также предлагает направления для будущих исследований в области культурной осведомленности искусственного интеллекта.'}, 'en': {'title': 'Enhancing Cultural Sensitivity in Language Models', 'desc': 'This paper explores the importance of cultural sensitivity in large language models (LLMs) used in applications like chatbots. It reviews existing research on how to make LLMs more inclusive by integrating insights from psychology and anthropology. The authors define cultural awareness in LLMs and discuss methods for creating diverse datasets and evaluating cultural inclusivity. Additionally, they highlight ethical considerations and suggest future research directions to enhance cultural alignment in LLMs.'}, 'zh': {'title': '让大型语言模型更具文化敏感性', 'desc': '本论文探讨了在大型语言模型（LLMs）中融入文化敏感性的重要性，以确保用户的包容性。我们首先定义了LLMs中的文化意识，并基于人类学和心理学的定义进行讨论。接着，我们分析了创建跨文化数据集的方法、在下游任务中实现文化包容的策略，以及用于评估LLMs文化意识的方法论。最后，我们讨论了文化对齐的伦理影响、人机交互在推动文化包容中的作用，以及未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2411.02395', 'title': 'Training-free Regional Prompting for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02395', 'abstract': 'Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.', 'score': 23, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '2a0401bfd2cb136b', 'authors': ['Anthony Chen', 'Jianjin Xu', 'Wenzhao Zheng', 'Gaole Dai', 'Yida Wang', 'Renrui Zhang', 'Haofan Wang', 'Shanghang Zhang'], 'affiliations': ['Peking University', 'InstantX Team', 'Carnegie Mellon University', 'UC Berkeley', 'Li Auto Inc.', 'CUHK'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02395.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#open_source', '#cv'], 'emoji': '🎨', 'ru': {'title': 'Точная генерация изображений по сложным текстам: новый метод для DiT', 'desc': 'Статья представляет новый метод регионального промптинга для архитектуры Diffusion Transformer (DiT), применимый к моделям FLUX.1. Этот подход позволяет улучшить генерацию изображений по сложным текстовым описаниям без дополнительного обучения модели. Авторы реализовали метод манипуляции вниманием, что позволяет DiT более точно следовать детальным композиционным промптам. Работа направлена на преодоление ограничений существующих моделей в обработке длинных текстовых описаний с множеством объектов и пространственных отношений.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Regional Prompting in Diffusion Transformers', 'desc': 'This paper discusses the limitations of current diffusion models in generating images from long and complex text prompts. It highlights the challenges these models face when dealing with multiple objects and their attributes in a spatial context. The authors propose a new method called regional prompting for the Diffusion Transformer (DiT) architecture, specifically for the FLUX.1 model, which enhances its ability to generate images based on detailed text descriptions without requiring additional training. This approach utilizes attention manipulation to achieve fine-grained compositional generation.'}, 'zh': {'title': '区域提示提升扩散模型的文本到图像生成能力', 'desc': '扩散模型在文本到图像生成方面表现出色，尤其是在理解语义方面得到了很大提升。尽管已有许多区域提示方法被提出，但现有模型仍无法完美处理长且复杂的文本提示。本文提出了一种基于注意力操作的区域提示方法，专门针对FLUX.1架构进行实现。该方法使得扩散变换器能够在无需训练的情况下，实现细粒度的组合文本到图像生成能力。'}}}, {'id': 'https://huggingface.co/papers/2411.02265', 'title': 'Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent', 'url': 'https://huggingface.co/papers/2411.02265', 'abstract': "In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.   Codes: https://github.com/Tencent/Hunyuan-Large   Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large", 'score': 23, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '775afc5ff4d7fbdf', 'authors': ['Xingwu Sun', 'Yanfeng Chen', 'Yiqing Huang', 'Ruobing Xie', 'Jiaqi Zhu', 'Kai Zhang', 'Shuaipeng Li', 'Zhen Yang', 'Jonny Han', 'Xiaobo Shu', 'Jiahao Bu', 'Zhongzhi Chen', 'Xuemeng Huang', 'Fengzong Lian', 'Saiyong Yang', 'Jianfeng Yan', 'Yuyuan Zeng', 'Xiaoqin Ren', 'Chao Yu', 'Lulu Wu', 'Yue Mao', 'Jun Xia', 'Tao Yang', 'Suncong Zheng', 'Kan Wu', 'Dian Jiao', 'Jinbao Xue', 'Xipeng Zhang', 'Decheng Wu', 'Kai Liu', 'Dengpeng Wu', 'Guanghui Xu', 'Shaohua Chen', 'Shuang Chen', 'Xiao Feng', 'Yigeng Hong', 'Junqiang Zheng', 'Chengcheng Xu', 'Zongwei Li', 'Xiong Kuang', 'Jianglu Hu', 'Yiqi Chen', 'Yuchi Deng', 'Guiyang Li', 'Ao Liu', 'Chenchen Zhang', 'Shihui Hu', 'Zilong Zhao', 'Zifan Wu', 'Yao Ding', 'Weichao Wang', 'Han Liu', 'Roberts Wang', 'Hao Fei', 'Peijie Yu', 'Ze Zhao', 'Xun Cao', 'Hai Wang', 'Fusheng Xiang', 'Mengyuan Huang', 'Zhiyuan Xiong', 'Bin Hu', 'Xuebin Hou', 'Lei Jiang', 'Jianqiang Ma', 'Jiajia Wu', 'Yaping Deng', 'Yi Shen', 'Qian Wang', 'Weijie Liu', 'Jie Liu', 'Meng Chen', 'Liang Dong', 'Weiwen Jia', 'Hu Chen', 'Feifei Liu', 'Rui Yuan', 'Huilin Xu', 'Zhenxiang Yan', 'Tengfei Cao', 'Zhichao Hu', 'Xinhua Feng', 'Dong Du', 'Tinghao Yu', 'Yangyu Tao', 'Feng Zhang', 'Jianchen Zhu', 'Chengzhong Xu', 'Xirui Li', 'Chong Zha', 'Wen Ouyang', 'Yinben Xia', 'Xiang Li', 'Zekun He', 'Rongpeng Chen', 'Jiawei Song', 'Ruibin Chen', 'Fan Jiang', 'Chongqing Zhao', 'Bo Wang', 'Hao Gong', 'Rong Gan', 'Winston Hu', 'Zhanhui Kang', 'Yong Yang', 'Yuhong Liu', 'Di Wang', 'Jie Jiang'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02265.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#synthetic', '#benchmark', '#optimization', '#math', '#plp', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Hunyuan-Large: Гигантский шаг вперед в области моделей смеси экспертов', 'desc': 'Статья представляет Hunyuan-Large - крупнейшую открытую модель на основе трансформеров с архитектурой смеси экспертов, содержащую 389 миллиардов параметров. Модель демонстрирует превосходную производительность в различных задачах, включая понимание и генерацию языка, логические рассуждения и программирование. Ключевые особенности Hunyuan-Large включают использование масштабных синтетических данных и смешанную стратегию маршрутизации экспертов. Авторы также исследуют законы масштабирования и графики скорости обучения для моделей смеси экспертов.'}, 'en': {'title': 'Unlocking New Frontiers in AI with Hunyuan-Large', 'desc': 'Hunyuan-Large is a groundbreaking Transformer-based mixture of experts model with an impressive 389 billion parameters, designed to process up to 256K tokens. It demonstrates exceptional performance in various tasks such as language understanding, logical reasoning, and coding, surpassing the capabilities of LLama3.1-70B and rivaling the larger LLama3.1-405B. The model employs innovative techniques like large-scale synthetic data generation, a mixed expert routing strategy, and expert-specific learning rates to enhance its efficiency. Furthermore, the paper explores scaling laws and learning rate schedules, offering insights for future advancements in model optimization.'}, 'zh': {'title': 'Hunyuan-Large：超大规模专家混合模型的创新之路', 'desc': '本文介绍了Hunyuan-Large，这是目前最大的开源基于Transformer的专家混合模型，拥有3890亿个参数和520亿个激活参数，能够处理多达256K的token。我们对Hunyuan-Large在语言理解与生成、逻辑推理、数学问题解决、编码、长上下文和聚合任务等多个基准上的优越性能进行了全面评估，结果显示其优于LLama3.1-70B，并且在与更大模型LLama3.1-405B的比较中表现相当。Hunyuan-Large的关键实践包括大规模合成数据、混合专家路由策略、键值缓存压缩技术和专家特定学习率策略。此外，我们还研究了专家混合模型的扩展规律和学习率调度，为未来模型的开发和优化提供了宝贵的见解和指导。'}}}, {'id': 'https://huggingface.co/papers/2411.02397', 'title': 'Adaptive Caching for Faster Video Generation with Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.02397', 'abstract': 'Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs) -- despite making significant headway in this context -- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a training-free method to accelerate video DiTs, termed Adaptive Caching (AdaCache), which is motivated by the fact that "not all videos are created equal": meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization (MoReg) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines.', 'score': 20, 'issue_id': 421, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'd7fa22d791789900', 'authors': ['Kumara Kahatapitiya', 'Haozhe Liu', 'Sen He', 'Ding Liu', 'Menglin Jia', 'Chenyang Zhang', 'Michael S. Ryoo', 'Tian Xie'], 'affiliations': ['Meta AI', 'Stony Brook University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02397.jpg', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'Адаптивное кэширование ускоряет генерацию видео без потери качества', 'desc': 'Статья представляет метод AdaCache для ускорения видео-диффузионных трансформеров (DiT) без переобучения. AdaCache адаптивно кэширует вычисления в процессе диффузии, учитывая, что разным видео требуется разное количество шагов шумоподавления. Также предложена схема регуляризации движения (MoReg) для оптимизации распределения вычислений на основе содержания движения в видео. Метод значительно ускоряет генерацию видео (до 4.7 раз на Open-Sora 720p) без потери качества.'}, 'en': {'title': 'Accelerating Video Generation with Adaptive Caching!', 'desc': 'This paper presents a method called Adaptive Caching (AdaCache) to improve the speed of video generation using Diffusion Transformers (DiTs). The authors argue that different videos have varying requirements for denoising steps, allowing for a more efficient computation process. By caching computations and creating a tailored caching schedule for each video, they enhance the balance between quality and speed. Additionally, they introduce a Motion Regularization (MoReg) technique to optimize resource allocation based on the motion content of the video, achieving significant speed improvements without compromising quality.'}, 'zh': {'title': '加速视频生成，提升质量与效率！', 'desc': '本论文提出了一种名为自适应缓存（AdaCache）的方法，用于加速视频生成中的扩散变换器（DiTs）。该方法通过缓存计算过程，针对每个视频生成制定缓存计划，从而优化质量与延迟的平衡。我们还引入了运动正则化（MoReg）方案，根据视频中的运动内容来控制计算分配。整体而言，这些创新显著提高了推理速度，同时保持了生成质量。'}}}, {'id': 'https://huggingface.co/papers/2411.02319', 'title': 'GenXD: Generating Any 3D and 4D Scenes', 'url': 'https://huggingface.co/papers/2411.02319', 'abstract': "Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, GenXD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, GenXD employs masked latent conditions to support a variety of conditioning views. GenXD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating GenXD's effectiveness and versatility compared to previous methods in 3D and 4D generation.", 'score': 19, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '51444eddaf6bbbe7', 'authors': ['Yuyang Zhao', 'Chung-Ching Lin', 'Kevin Lin', 'Zhiwen Yan', 'Linjie Li', 'Zhengyuan Yang', 'Jianfeng Wang', 'Gim Hee Lee', 'Lijuan Wang'], 'affiliations': ['National University of Singapore', 'Microsoft Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02319.jpg', 'data': {'categories': ['#diffusion', '#synthetic', '#cv', '#video', '#data', '#dataset', '#3d', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'GenXD: Универсальный генератор 3D и 4D сцен', 'desc': 'Исследователи представили новый подход к генерации 3D и 4D сцен под названием GenXD. Они создали большой набор данных реальных 4D сцен CamVid-30K, используя специальный конвейер для извлечения информации о движении камеры и объектов из видео. GenXD использует мультиракурсно-временные модули для разделения движений камеры и объектов, а также маскированные латентные условия для гибкого задания входных ракурсов. Метод позволяет генерировать видео с заданной траекторией камеры и согласованные 3D виды, которые можно преобразовать в 3D-представления.'}, 'en': {'title': 'Unlocking 3D and 4D Generation with GenXD', 'desc': 'This paper addresses the challenges of generating 3D and 4D visual content, which are more complex than 2D generation due to limited data and model design issues. The authors introduce a new dataset called CamVid-30K, created by extracting camera poses and object movements from videos, which helps in training models effectively. They present a framework named GenXD that utilizes multiview-temporal modules to differentiate between camera and object movements, enabling the generation of realistic 3D and 4D scenes. Extensive evaluations show that GenXD outperforms existing methods, demonstrating its capability to create coherent videos and 3D representations from diverse inputs.'}, 'zh': {'title': '突破3D与4D生成的瓶颈', 'desc': '本文探讨了3D和4D视觉生成的挑战，尤其是在缺乏大规模4D数据的情况下。我们提出了一种数据整理流程，从视频中获取相机姿态和物体运动强度，并引入了一个大型的4D场景数据集CamVid-30K。基于这些数据，我们开发了GenXD框架，能够生成任意3D或4D场景。通过多视角时间模块，GenXD能够有效地学习相机和物体的运动，并生成与相机轨迹一致的视频和可提升为3D表示的视图。'}}}, {'id': 'https://huggingface.co/papers/2411.02394', 'title': 'AutoVFX: Physically Realistic Video Editing from Natural Language Instructions', 'url': 'https://huggingface.co/papers/2411.02394', 'abstract': "Modern visual effects (VFX) software has made it possible for skilled artists to create imagery of virtually anything. However, the creation process remains laborious, complex, and largely inaccessible to everyday users. In this work, we present AutoVFX, a framework that automatically creates realistic and dynamic VFX videos from a single video and natural language instructions. By carefully integrating neural scene modeling, LLM-based code generation, and physical simulation, AutoVFX is able to provide physically-grounded, photorealistic editing effects that can be controlled directly using natural language instructions. We conduct extensive experiments to validate AutoVFX's efficacy across a diverse spectrum of videos and instructions. Quantitative and qualitative results suggest that AutoVFX outperforms all competing methods by a large margin in generative quality, instruction alignment, editing versatility, and physical plausibility.", 'score': 15, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '96ed53359fbc24a5', 'authors': ['Hao-Yu Hsu', 'Zhi-Hao Lin', 'Albert Zhai', 'Hongchi Xia', 'Shenlong Wang'], 'affiliations': ['University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02394.jpg', 'data': {'categories': ['#reasoning', '#cv', '#video', '#multimodal', '#games', '#architecture', '#alignment'], 'emoji': '🎬', 'ru': {'title': 'AutoVFX: Создание спецэффектов силой мысли', 'desc': 'AutoVFX - это новая система, которая автоматически создает реалистичные видео с визуальными эффектами на основе одного видео и текстовых инструкций на естественном языке. Она объединяет нейронное моделирование сцен, генерацию кода с помощью больших языковых моделей и физическое моделирование. AutoVFX позволяет создавать фотореалистичные эффекты с физически корректным поведением. Эксперименты показали, что система значительно превосходит аналоги по качеству генерации, соответствию инструкциям, универсальности редактирования и физической достоверности.'}, 'en': {'title': 'Transforming VFX Creation with Natural Language and AI', 'desc': 'AutoVFX is a novel framework designed to simplify the creation of visual effects (VFX) by allowing users to generate realistic videos from a single input video and natural language commands. It combines advanced techniques such as neural scene modeling, large language model (LLM)-based code generation, and physical simulation to produce high-quality, dynamic effects. The framework is evaluated through extensive experiments, demonstrating superior performance in generative quality, alignment with user instructions, versatility in editing, and adherence to physical realism. Overall, AutoVFX makes sophisticated VFX creation accessible to a broader audience, reducing the complexity of traditional methods.'}, 'zh': {'title': '自动化视觉特效，轻松创作真实影像', 'desc': '现代视觉特效软件使得熟练的艺术家能够创造几乎任何图像，但创作过程仍然繁琐且复杂，普通用户难以接触。本文提出了AutoVFX，一个框架可以根据单个视频和自然语言指令自动创建逼真且动态的视觉特效视频。通过精心整合神经场景建模、基于大语言模型的代码生成和物理仿真，AutoVFX能够提供物理基础的、照片级真实感的编辑效果，并可以通过自然语言指令直接控制。大量实验表明，AutoVFX在生成质量、指令对齐、编辑多样性和物理合理性方面显著优于所有竞争方法。'}}}, {'id': 'https://huggingface.co/papers/2411.00836', 'title': 'DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models', 'url': 'https://huggingface.co/papers/2411.00836', 'abstract': "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that SOTA VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 SOTA VLMs with 5,010 generated concrete questions. Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.", 'score': 15, 'issue_id': 420, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'e73ae00a5621a2b9', 'authors': ['Chengke Zou', 'Xingang Guo', 'Rui Yang', 'Junyu Zhang', 'Bin Hu', 'Huan Zhang'], 'affiliations': ['University of Illinois at Urbana-Champaign', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00836.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#benchmark', '#cv', '#graphs', '#optimization', '#math', '#dataset', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'DynaMath: Новый подход к оценке математических способностей ИИ', 'desc': 'Статья представляет DynaMath - динамический визуальный математический бенчмарк для оценки робастности рассуждений Vision-Language Models (VLM) в задачах математики. Исследователи обнаружили, что современные VLM, такие как GPT-4V, часто не способны применять шаги решения к похожим задачам с небольшими изменениями. DynaMath включает 501 исходный вопрос в виде Python-программ, позволяющих генерировать множество вариаций для тестирования обобщающей способности моделей. Результаты оценки 14 современных VLM показали, что их точность в худшем случае значительно ниже средней точности.'}, 'en': {'title': 'Enhancing VLMs: Unveiling Mathematical Reasoning Limitations with DynaMath', 'desc': "This paper explores the limitations of Vision-Language Models (VLMs) in performing mathematical reasoning tasks that involve visual elements. It highlights that while humans can adapt their problem-solving strategies to slight changes in problems, current state-of-the-art VLMs like GPT-4o struggle with this adaptability. To address this issue, the authors introduce DynaMath, a dynamic benchmark that generates a wide variety of mathematical questions to rigorously test VLMs' reasoning capabilities. The findings reveal that VLMs exhibit significantly lower accuracy in worst-case scenarios compared to average cases, underscoring the importance of evaluating their robustness in mathematical reasoning."}, 'zh': {'title': '提升视觉语言模型的数学推理能力', 'desc': '本文探讨了视觉语言模型（VLMs）在数学推理任务中的表现，尤其是在视觉上下文的影响下。研究发现，尽管人类能够灵活应对相似问题的变化，当前的最先进模型如GPT-4o在面对这些变化时却表现不佳，显示出其数学推理能力的局限性。为了解决这一问题，本文提出了DynaMath，一个动态视觉数学基准，旨在深入评估VLMs的推理稳健性。通过对501个高质量种子问题的自动生成，DynaMath能够评估模型在不同输入条件下的泛化能力，结果显示模型在最坏情况下的准确率显著低于平均情况。'}}}, {'id': 'https://huggingface.co/papers/2411.01747', 'title': 'DynaSaur: Large Language Agents Beyond Predefined Actions', 'url': 'https://huggingface.co/papers/2411.01747', 'abstract': 'Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in https://github.com/adobe-research/dynasaur{https://github.com/adobe-research/dynasaur}.', 'score': 13, 'issue_id': 423, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '772b11b15cab80a0', 'authors': ['Dang Nguyen', 'Viet Dac Lai', 'Seunghyun Yoon', 'Ryan A. Rossi', 'Handong Zhao', 'Ruiyi Zhang', 'Puneet Mathur', 'Nedim Lipka', 'Yu Wang', 'Trung Bui', 'Franck Dernoncourt', 'Tianyi Zhou'], 'affiliations': ['University of Maryland', 'Adobe Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01747.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#agi', '#plp', '#open_source', '#agents', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Динамическое создание действий для агентов на базе LLM: шаг к адаптивному ИИ', 'desc': 'Эта статья представляет новую структуру агентов на основе больших языковых моделей (LLM), которая позволяет динамически создавать и комбинировать действия в режиме реального времени. В отличие от существующих систем с фиксированным набором действий, предлагаемый подход генерирует и выполняет программы на языке общего назначения на каждом шаге взаимодействия с окружающей средой. Авторы демонстрируют, что их метод обеспечивает большую гибкость и превосходит предыдущие подходы в экспериментах на бенчмарке GAIA. Особенно эффективно система работает в сценариях, где предопределенные действия отсутствуют или неприменимы.'}, 'en': {'title': 'Empowering LLM Agents with Dynamic Action Generation', 'desc': 'This paper introduces a new framework for large language model (LLM) agents that allows them to dynamically create and execute actions in real-time, rather than relying on a fixed set of predefined actions. This approach enhances the planning and acting capabilities of LLM agents, making them more adaptable to complex and unpredictable environments. By generating programs in a general-purpose programming language, the agents can respond to unforeseen situations and accumulate useful actions for future tasks. The experiments conducted on the GAIA benchmark show that this framework significantly outperforms traditional methods, providing greater flexibility and effectiveness in real-world applications.'}, 'zh': {'title': '动态创建与组合动作的LLM代理框架', 'desc': '现有的大型语言模型（LLM）代理系统通常在每一步从固定的预定义动作集中选择动作。这种方法在封闭的、狭窄的环境中有效，但在现实世界中应用时面临两个主要挑战：一是从固定动作集中选择限制了LLM代理的规划和执行能力，二是需要大量人力来列举和实现所有可能的动作，这在复杂环境中变得不切实际。我们提出了一种LLM代理框架，允许在线动态创建和组合动作，代理通过生成和执行通用编程语言编写的程序与环境互动。我们的实验表明，该框架提供了更大的灵活性，并在GAIA基准测试中表现优于以往方法。'}}}, {'id': 'https://huggingface.co/papers/2411.02327', 'title': 'PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance', 'url': 'https://huggingface.co/papers/2411.02327', 'abstract': "The past year has witnessed the significant advancement of video-based large language models. However, the challenge of developing a unified model for both short and long video understanding remains unresolved. Most existing video LLMs cannot handle hour-long videos, while methods custom for long videos tend to be ineffective for shorter videos and images. In this paper, we identify the key issue as the redundant content in videos. To address this, we propose a novel pooling strategy that simultaneously achieves token compression and instruction-aware visual feature aggregation. Our model is termed Prompt-guided Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three core components: the CLIP-based visual-prompt alignment that extracts visual information relevant to the user's instructions, the prompt-guided pooling that compresses the visual sequence to arbitrary scales using convolution-style pooling, and the clip context extension designed for lengthy prompt common in visual dialogue. Moreover, our codebase also integrates the most advanced video Direct Preference Optimization (DPO) and visual interleave training. Extensive experiments have validated the performance of our model. With superior throughput and only 1024 visual context, PPLLaVA achieves better results on image benchmarks as a video LLM, while achieving state-of-the-art performance across various video benchmarks, excelling in tasks ranging from caption generation to multiple-choice questions, and handling video lengths from seconds to hours. Codes have been available at https://github.com/farewellthree/PPLLaVA.", 'score': 11, 'issue_id': 432, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'ed9d356ccf75d780', 'authors': ['Ruyang Liu', 'Haoran Tang', 'Haibo Liu', 'Yixiao Ge', 'Ying Shan', 'Chen Li', 'Jiankun Yang'], 'affiliations': ['Peking University', 'Applied Research Center (ARC), Tencent PCG', 'Peng Cheng Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02327.jpg', 'data': {'categories': ['#long_context', '#rlhf', '#benchmark', '#video', '#optimization', '#training', '#open_source', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Универсальная обработка видео любой длины с помощью инновационного пулинга', 'desc': 'Статья представляет новую модель PPLLaVA для обработки как коротких, так и длинных видео. Ключевая инновация заключается в использовании стратегии пулинга, которая сжимает токены и агрегирует визуальные признаки с учетом инструкций. Модель включает выравнивание визуальных подсказок на основе CLIP, пулинг с учетом подсказок и расширение контекста клипов. PPLLaVA демонстрирует высокую производительность на различных бенчмарках для изображений и видео разной длительности.'}, 'en': {'title': 'Unified Video Understanding with PPLLaVA', 'desc': 'This paper presents a new model called Prompt-guided Pooling LLaVA (PPLLaVA) that improves video understanding for both short and long videos. The authors identify redundant content in videos as a key challenge and propose a pooling strategy that compresses tokens while aggregating visual features based on user instructions. PPLLaVA includes components for visual-prompt alignment, convolution-style pooling, and context extension for lengthy prompts. The model demonstrates superior performance across various benchmarks, effectively handling video lengths from seconds to hours.'}, 'zh': {'title': '视频理解的新突破：PPLLaVA模型', 'desc': '本论文提出了一种新的视频大语言模型，称为PPLLaVA，旨在解决短视频和长视频理解的统一模型问题。我们发现视频中的冗余内容是主要挑战，因此提出了一种新的池化策略，实现了令牌压缩和指令感知的视觉特征聚合。PPLLaVA包含三个核心组件，分别是基于CLIP的视觉提示对齐、提示引导池化和剪辑上下文扩展。经过广泛实验验证，PPLLaVA在处理从几秒到几小时的视频时，表现出色，超越了现有的视频基准。'}}}, {'id': 'https://huggingface.co/papers/2411.02335', 'title': 'Sparsing Law: Towards Large Language Models with Greater Activation Sparsity', 'url': 'https://huggingface.co/papers/2411.02335', 'abstract': 'Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-p% sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1-sparsity ratio) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable.', 'score': 11, 'issue_id': 422, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '3c9152f4d267fc7b', 'authors': ['Yuqi Luo', 'Chenyang Song', 'Xu Han', 'Yingfa Chen', 'Chaojun Xiao', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02335.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Разреженность активаций в LLM: ключ к эффективности и интерпретируемости', 'desc': 'Статья исследует разреженность активаций в больших языковых моделях (LLM). Авторы предлагают новую метрику PPL-p% для измерения разреженности активаций. Исследование показывает, что различные функции активации демонстрируют противоположные тренды разреженности во время обучения. Обнаружено, что соотношение ширины и глубины модели влияет на разреженность активаций, а масштаб параметров модели оказывает слабое влияние.'}, 'en': {'title': 'Unlocking Efficiency: The Power of Activation Sparsity in LLMs', 'desc': 'This paper investigates activation sparsity in decoder-only Transformer-based large language models (LLMs), focusing on how weakly-contributed activation outputs can be reduced for efficiency. The authors introduce a new metric called PPL-p% sparsity to quantitatively measure activation sparsity across different activation functions. Their experiments reveal that while ReLU and SiLU functions perform similarly, they exhibit opposite trends in training-time sparsity, with ReLU being more efficient. Additionally, the study finds that activation sparsity is largely unaffected by the parameter scale, suggesting that deeper architectures may enhance efficiency without requiring more parameters.'}, 'zh': {'title': '激活稀疏性：提升大型语言模型效率的关键', 'desc': '激活稀疏性指的是在激活输出中存在大量贡献较弱的元素，这些元素可以被消除，从而对大型语言模型（LLMs）的许多重要应用有益。本文对基于解码器的Transformer LLM中的激活稀疏性进行了全面的定量研究，提出了一种新的激活稀疏性度量标准PPL-p%稀疏性，适用于任何激活函数。研究发现，不同的激活函数在性能上相似，但在训练时间的稀疏性趋势上却相反，ReLU激活函数在利用更多训练数据方面更为高效。最后，研究表明，激活稀疏性的极限值与参数规模的变化关系不大，这为提高LLMs的效率和可解释性提供了重要的启示。'}}}, {'id': 'https://huggingface.co/papers/2411.01798', 'title': 'SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF', 'url': 'https://huggingface.co/papers/2411.01798', 'abstract': "In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.", 'score': 8, 'issue_id': 435, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': 'e5a60efe079c6136', 'authors': ['Atoosa Chegini', 'Hamid Kazemi', 'Iman Mirzadeh', 'Dong Yin', 'Maxwell Horton', 'Moin Nabi', 'Mehrdad Farajtabar', 'Keivan Alizadeh'], 'affiliations': ['University of Maryland', 'Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01798.jpg', 'data': {'categories': ['#small_models', '#rl', '#rlhf', '#benchmark', '#optimization', '#training', '#open_source', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'SALSA: Гибкое обучение для лучшей адаптации языковых моделей', 'desc': 'Статья представляет новый подход SALSA для обучения больших языковых моделей с подкреплением от обратной связи человека (RLHF). В отличие от традиционного метода PPO, использующего KL-дивергенцию с замороженной исходной моделью, SALSA создает более гибкую опорную модель путем усреднения весов двух независимо обученных моделей. Это позволяет исследовать более широкую область пространства параметров, не жертвуя стабильностью. Эксперименты показывают, что SALSA превосходит PPO по многим метрикам, включая награды, обобщение вне распределения и общую производительность.'}, 'en': {'title': 'SALSA: Enhancing LLM Alignment through Flexible Exploration', 'desc': "This paper introduces SALSA, a new method for improving the alignment of Large Language Models (LLMs) with human preferences using Reinforcement Learning from Human Feedback (RLHF). Traditional RLHF methods use a fixed reference policy, which can restrict the model's ability to explore and find better solutions due to the penalty imposed by Kullback-Leibler (KL) divergence. SALSA addresses this issue by averaging weights from two independently fine-tuned models, creating a more flexible reference that allows for greater exploration of the reward landscape. The results show that SALSA enhances model performance, robustness, and generalization across various benchmarks compared to traditional methods like Proximal Policy Optimization (PPO)."}, 'zh': {'title': 'SALSA：提升大型语言模型对齐与探索的创新方法', 'desc': '在大型语言模型（LLM）的开发中，基于人类反馈的强化学习（RLHF）对于使模型与人类价值观和偏好保持一致至关重要。传统的RLHF依赖于当前策略与冻结初始策略之间的Kullback-Leibler（KL）散度作为参考，这在策略优化算法中作为惩罚项使用。本文提出了一种新方法SALSA（基于模型集合的对齐学习），通过对两个独立的监督微调模型进行权重空间平均，创建一个更灵活的参考模型，从而克服了传统方法的局限性。SALSA通过更好的探索能力，显著提高了模型的鲁棒性和性能，验证了其在多个基准测试中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.00785', 'title': 'IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI', 'url': 'https://huggingface.co/papers/2411.00785', 'abstract': 'We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can "migrate" the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control.', 'score': 8, 'issue_id': 434, 'pub_date': '2024-10-17', 'pub_date_card': {'ru': '17 октября', 'en': 'October 17', 'zh': '10月17日'}, 'hash': 'a7e2824d0e474c0b', 'authors': ['Xiaoyu Chen', 'Junliang Guo', 'Tianyu He', 'Chuheng Zhang', 'Pushi Zhang', 'Derek Cathera Yang', 'Li Zhao', 'Jiang Bian'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00785.jpg', 'data': {'categories': ['#science', '#cv', '#graphs', '#video', '#multimodal', '#training', '#dataset', '#robotics', '#transfer_learning', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Единое пространство действий для людей и роботов', 'desc': 'Исследователи представляют IGOR - систему для создания единого семантически согласованного пространства действий для людей и роботов. IGOR сжимает визуальные изменения между начальным и целевым изображением в латентные действия, что позволяет генерировать метки действий для масштабных видеоданных. Это единое латентное пространство действий позволяет обучать базовые политики и модели мира для разнообразных задач, выполняемых как роботами, так и людьми. IGOR демонстрирует возможность переноса движений объектов между видео, даже между людьми и роботами, а также интеграцию с естественным языком и низкоуровневым управлением роботами.'}, 'en': {'title': 'Unified Action Space for Human-Robot Interaction', 'desc': 'The paper presents Image-GOal Representations (IGOR), a framework designed to create a unified action space that is semantically consistent for both humans and robots. By compressing the visual changes between an initial image and its goal state into latent actions, IGOR facilitates knowledge transfer across diverse datasets of human and robot activities. This approach allows for the generation of latent action labels from large-scale video data, enabling the training of foundational policy and world models for various tasks. Ultimately, IGOR enhances the ability to transfer movement knowledge between humans and robots, aligning actions with natural language for improved robot control.'}, 'zh': {'title': '统一动作空间，连接人类与机器人', 'desc': '本文介绍了图像目标表示（IGOR），旨在学习一个统一且语义一致的动作空间，适用于人类和各种机器人。通过这个统一的潜在动作空间，IGOR能够在大规模机器人和人类活动数据之间进行知识转移。我们通过压缩初始图像与目标状态之间的视觉变化来实现这一点，从而生成潜在动作标签。IGOR为机器人控制和人机交互开辟了新的可能性。'}}}, {'id': 'https://huggingface.co/papers/2411.00918', 'title': 'LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models', 'url': 'https://huggingface.co/papers/2411.00918', 'abstract': 'Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.', 'score': 8, 'issue_id': 420, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'a406640433a3de34', 'authors': ['Nam V. Nguyen', 'Thong T. Doan', 'Luong Tran', 'Van Nguyen', 'Quang Pham'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00918.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#dataset', '#open_source', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'LibMoE: Демократизация исследований Mixture of Experts в больших языковых моделях', 'desc': 'Статья представляет LibMoE - комплексную модульную систему для исследования, обучения и оценки алгоритмов Mixture of Experts (MoE) в больших языковых моделях (LLM). LibMoE основан на трех принципах: модульный дизайн, эффективное обучение и всесторонняя оценка. Используя LibMoE, авторы провели обширное сравнение пяти современных алгоритмов MoE на трех различных LLM и 11 наборах данных в режиме zero-shot. Результаты показали, что, несмотря на уникальные характеристики, все алгоритмы MoE показывают примерно одинаковые результаты при усреднении по широкому спектру задач.'}, 'en': {'title': 'LibMoE: Streamlining Mixture of Experts for Large Language Models', 'desc': 'This paper introduces LibMoE, a framework designed to facilitate research on Mixture of Experts (MoE) algorithms for large language models (LLMs). It emphasizes a modular design, efficient training processes, and comprehensive evaluation methods to make MoE more accessible to researchers. The authors benchmark five leading MoE algorithms across three LLMs and 11 datasets, revealing that their performance is generally similar across various tasks. LibMoE aims to standardize the training and evaluation of MoE, helping researchers advance the development of future LLMs.'}, 'zh': {'title': 'LibMoE：让混合专家算法更易于研究和应用', 'desc': '混合专家（MoE）在大型语言模型（LLMs）的高效和有效发展中扮演着重要角色。由于资源需求巨大，许多研究者难以研究大规模的MoE算法。本文开发了LibMoE，这是一个全面且模块化的框架，旨在简化MoE算法的研究、训练和评估。通过模块化设计、高效训练和全面评估，LibMoE使得MoE在LLMs中的应用对更多研究者变得可及。'}}}, {'id': 'https://huggingface.co/papers/2411.00743', 'title': 'Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models', 'url': 'https://huggingface.co/papers/2411.00743', 'abstract': 'Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and L_0 sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5\\% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.', 'score': 6, 'issue_id': 421, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'af234e3c99f935ec', 'authors': ['Aashiq Muhamed', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University', 'Machine Learning Department, Carnegie Mellon University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00743.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#ethics', '#data', '#training', '#dataset', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Специализированные автоэнкодеры раскрывают скрытые концепты в фундаментальных моделях', 'desc': 'Статья представляет новый метод интерпретации фундаментальных моделей - Специализированные разреженные автоэнкодеры (SSAE). SSAE фокусируются на конкретных поддоменах и эффективно выявляют редкие, но важные концепты в данных. Авторы демонстрируют преимущества плотного поиска для выбора данных и использования Tilted Empirical Risk Minimization в качестве целевой функции обучения. Эффективность SSAE показана на стандартных метриках и в практическом исследовании на наборе данных Bias in Bios.'}, 'en': {'title': 'Illuminating Hidden Concepts in Foundation Models with SSAEs', 'desc': 'This paper discusses the challenges of understanding and mitigating risks in foundation models (FMs) through effective interpretability methods. It introduces Specialized Sparse Autoencoders (SSAEs), which are designed to better identify rare but important concepts in data by focusing on specific subdomains. The authors provide a training approach that utilizes dense retrieval for data selection and Tilted Empirical Risk Minimization to enhance concept recall. The results show that SSAEs outperform traditional Sparse Autoencoders in capturing these critical features, as demonstrated in a case study that improved classification accuracy by addressing bias in gender information.'}, 'zh': {'title': '专注子领域的稀疏自编码器：揭示基础模型的潜在特征', 'desc': '本论文探讨了基础模型（FMs）潜在风险的理解与缓解，强调了有效的可解释性方法的重要性。我们提出了一种新的稀疏自编码器（SSAEs），旨在揭示数据中稀有但重要的概念，特别关注特定子领域。通过密集检索和倾斜经验风险最小化等方法，我们展示了SSAEs在捕捉子领域尾部概念方面的优势。案例研究表明，SSAEs在去除虚假性别信息时，分类准确率提高了12.5%。'}}}, {'id': 'https://huggingface.co/papers/2411.00359', 'title': 'Constrained Diffusion Implicit Models', 'url': 'https://huggingface.co/papers/2411.00359', 'abstract': 'This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose constrained diffusion implicit models (CDIM) that modify the diffusion updates to enforce a constraint upon the final output. For noiseless inverse problems, CDIM exactly satisfies the constraints; in the noisy case, we generalize CDIM to satisfy an exact constraint on the residual distribution of the noise. Experiments across a variety of tasks and metrics show strong performance of CDIM, with analogous inference acceleration to unconstrained DDIM: 10 to 50 times faster than previous conditional diffusion methods. We demonstrate the versatility of our approach on many problems including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reconstruction.', 'score': 5, 'issue_id': 434, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'e85bdeb4e14858fd', 'authors': ['Vivek Jayaram', 'Ira Kemelmacher-Shlizerman', 'Steven M. Seitz', 'John Thickstun'], 'affiliations': ['University of Washington', 'Cornell University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00359.jpg', 'data': {'categories': ['#diffusion', '#cv', '#inference', '#optimization', '#3d', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Ускоренное решение обратных задач с помощью ограниченных диффузионных моделей', 'desc': 'Статья описывает эффективный алгоритм для решения зашумленных линейных обратных задач с использованием предобученных диффузионных моделей. Авторы предлагают constrained diffusion implicit models (CDIM), которые модифицируют обновления диффузии для обеспечения ограничений на конечный результат. CDIM точно удовлетворяет ограничениям для задач без шума, а для зашумленных задач обобщается для удовлетворения точного ограничения на распределение остаточного шума. Эксперименты показывают высокую производительность CDIM с ускорением вывода в 10-50 раз по сравнению с предыдущими условными диффузионными методами.'}, 'en': {'title': 'Accelerating Inverse Problem Solutions with Constrained Diffusion Models', 'desc': "This paper introduces a new algorithm called Constrained Diffusion Implicit Models (CDIM) for tackling noisy linear inverse problems. CDIM builds on the existing Denoising Diffusion Implicit Models (DDIM) by incorporating constraints into the diffusion process, ensuring that the final output adheres to specific requirements. In scenarios without noise, CDIM perfectly meets these constraints, while in noisy situations, it adapts to maintain an exact constraint on the noise's residual distribution. The results demonstrate that CDIM not only performs well across various tasks like super-resolution and denoising but also accelerates inference significantly, achieving speeds 10 to 50 times faster than traditional methods."}, 'zh': {'title': '高效解决带噪声线性逆问题的约束扩散模型', 'desc': '本文提出了一种高效的算法，用于解决带噪声的线性逆问题，利用预训练的扩散模型。我们扩展了去噪扩散隐式模型（DDIM）的范式，提出了约束扩散隐式模型（CDIM），通过修改扩散更新来强制最终输出满足约束条件。在无噪声的逆问题中，CDIM能够精确满足约束；而在有噪声的情况下，我们将CDIM推广到满足噪声残差分布的精确约束。实验结果表明，CDIM在多种任务和指标上表现出色，其推理速度比之前的条件扩散方法快10到50倍，展示了我们方法在超分辨率、去噪、修复、去模糊和3D点云重建等问题上的多样性。'}}}, {'id': 'https://huggingface.co/papers/2411.00492', 'title': 'Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models', 'url': 'https://huggingface.co/papers/2411.00492', 'abstract': 'We present Multi-expert Prompting, a novel enhancement of ExpertPrompting (Xu et al., 2023), designed to improve the large language model (LLM) generation. Specifically, it guides an LLM to fulfill an input instruction by simulating multiple experts, aggregating their responses, and selecting the best among individual and aggregated responses. This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (Ven and Delbecq, 1974), a well-established decision-making framework. Our evaluations demonstrate that Multi-expert Prompting significantly outperforms ExpertPrompting and comparable baselines in enhancing the truthfulness, factuality, informativeness, and usefulness of responses while reducing toxicity and hurtfulness. It further achieves state-of-the-art truthfulness by outperforming the best baseline by 8.69% with ChatGPT. Multi-expert Prompting is efficient, explainable, and highly adaptable to diverse scenarios, eliminating the need for manual prompt construction.', 'score': 5, 'issue_id': 433, 'pub_date': '2024-11-01', 'pub_date_card': {'ru': '1 ноября', 'en': 'November 1', 'zh': '11月1日'}, 'hash': 'd4857ddaa86c9914', 'authors': ['Do Xuan Long', 'Duong Ngoc Yen', 'Anh Tuan Luu', 'Kenji Kawaguchi', 'Min-Yen Kan', 'Nancy F. Chen'], 'affiliations': ['National University of Singapore', 'Institute for Infocomm Research (I2R), A*STAR', 'Nanyang Technological University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.00492.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#interpretability', '#architecture', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Коллективный разум экспертов для улучшения ответов ИИ', 'desc': 'В статье представлен метод Multi-expert Prompting, улучшающий генерацию текста большими языковыми моделями (LLM). Этот подход симулирует работу нескольких экспертов, объединяет их ответы и выбирает лучший результат. Процесс реализуется в виде единой цепочки рассуждений с использованием семи специально разработанных подзадач, основанных на методике Nominal Group Technique. Эксперименты показывают, что Multi-expert Prompting значительно превосходит базовые методы по ряду критериев, включая достоверность и информативность ответов.'}, 'en': {'title': 'Harnessing Collective Expertise for Superior Language Model Responses', 'desc': 'Multi-expert Prompting is an advanced technique that enhances the performance of large language models (LLMs) by simulating the input of multiple experts. It aggregates responses from these simulated experts and selects the best one, ensuring that the output is more accurate and informative. This method is structured around seven subtasks inspired by the Nominal Group Technique, which aids in effective decision-making. Evaluations show that this approach significantly improves the truthfulness and usefulness of LLM responses while minimizing negative outputs like toxicity.'}, 'zh': {'title': '多专家提示：提升语言模型生成的全新方法', 'desc': '本文提出了一种新的增强方法，称为多专家提示（Multi-expert Prompting），旨在改善大型语言模型（LLM）的生成效果。该方法通过模拟多个专家来指导LLM执行输入指令，聚合各个专家的响应，并选择最佳的单个和聚合响应。我们设计了七个子任务，基于成熟的决策框架——名义小组技术（Nominal Group Technique），以实现这一过程。评估结果表明，多专家提示在提高响应的真实性、事实性、信息量和实用性方面显著优于专家提示（ExpertPrompting）及其他基线，同时减少了有害性和攻击性。'}}}, {'id': 'https://huggingface.co/papers/2411.01106', 'title': 'LoRA-Contextualizing Adaptation of Large Multimodal Models for Long Document Understanding', 'url': 'https://huggingface.co/papers/2411.01106', 'abstract': 'Large multimodal models (LMMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page, visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to LMMs leads to inefficiencies, especially with lengthy documents. In this work, we present a novel framework named LoRA-Contextualizing Adaptation of Large multimodal models (LoCAL), which broadens the capabilities of any LMM to support long-document understanding. We demonstrate that LMMs can effectively serve as multimodal retrievers, fetching relevant pages to answer user questions based on these pages. LoCAL is implemented with two specific LMM adapters: one for evidence page retrieval and another for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of LoCAL.', 'score': 4, 'issue_id': 433, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 ноября', 'en': 'November 2', 'zh': '11月2日'}, 'hash': '9ac41ff2238a6f8d', 'authors': ['Jian Chen', 'Ruiyi Zhang', 'Yufan Zhou', 'Tong Yu', 'Franck Dernoncourt', 'Jiuxiang Gu', 'Ryan A. Rossi', 'Changyou Chen', 'Tong Sun'], 'affiliations': ['University at Buffalo', 'Adobe Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01106.jpg', 'data': {'categories': ['#science', '#rag', '#benchmark', '#multimodal', '#architecture', '#long_context'], 'emoji': '📄', 'ru': {'title': 'LoCAL: Революция в понимании длинных документов с помощью LMM', 'desc': 'Статья представляет новый фреймворк LoCAL для улучшения понимания длинных документов большими мультимодальными моделями (LMM). LoCAL использует два адаптера LMM: один для поиска релевантных страниц, другой для ответов на вопросы. Этот подход позволяет эффективно обрабатывать сложные многостраничные документы, преодолевая ограничения традиционных методов. Эмпирические результаты показывают превосходную производительность LoCAL на публичных бенчмарках.'}, 'en': {'title': 'Enhancing Long-Document Understanding with LoCAL', 'desc': 'This paper introduces LoCAL, a new framework designed to enhance large multimodal models (LMMs) for understanding long and complex documents. Traditional methods struggle with efficiency when processing multiple pages, but LoCAL allows LMMs to act as multimodal retrievers, selecting relevant pages to answer questions. The framework includes two specialized adapters: one for retrieving evidence pages and another for answering questions based on those pages. Empirical results indicate that LoCAL achieves state-of-the-art performance on public benchmarks, showcasing its effectiveness in long-document comprehension.'}, 'zh': {'title': '提升大型多模态模型的长文档理解能力', 'desc': '大型多模态模型（LMMs）在理解文本丰富的图像方面取得了显著进展，但在处理复杂的多页视觉文档时仍然面临挑战。传统的文档解析方法在检索增强生成中存在性能和效率的限制，而直接将所有页面呈现给LMMs则导致效率低下，尤其是在处理较长文档时。我们提出了一种新框架，称为LoRA-上下文适应的大型多模态模型（LoCAL），它扩展了任何LMM支持长文档理解的能力。实验结果表明，LoCAL在公共基准测试中表现出色，证明了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.01192', 'title': 'Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks', 'url': 'https://huggingface.co/papers/2411.01192', 'abstract': 'We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research.', 'score': 3, 'issue_id': 427, 'pub_date': '2024-11-02', 'pub_date_card': {'ru': '2 ноября', 'en': 'November 2', 'zh': '11月2日'}, 'hash': 'df042a726644eac5', 'authors': ['Gagan Bhatia', 'El Moatez Billah Nagoudi', 'Abdellah El Mekki', 'Fakhraddin Alwajih', 'Muhammad Abdul-Mageed'], 'affiliations': ['MBZUAI', 'The University of British Columbia', 'Invertible AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.01192.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#multilingual', '#dataset', '#open_source', '#low_resource', '#architecture'], 'emoji': '🦢', 'ru': {'title': 'Swan: Прорыв в обработке арабского языка', 'desc': 'Представлены модели семейства Swan для встраивания текстов на арабском языке. Разработаны два варианта: Swan-Small на основе ARBERTv2 и Swan-Large на базе ArMistral. Для оценки создан бенчмарк ArabicMTEB, охватывающий 8 задач и 94 набора данных. Swan-Large превосходит Multilingual-E5-large по большинству арабских задач, демонстрируя диалектную и культурную осведомленность.'}, 'en': {'title': 'Swan: Advancing Arabic Language Embeddings for Diverse Applications', 'desc': 'This paper presents Swan, a new family of embedding models specifically designed for the Arabic language, which includes two versions: Swan-Small and Swan-Large. Swan-Small is based on ARBERTv2, while Swan-Large utilizes the ArMistral model, a large pretrained Arabic language model. The authors introduce ArabicMTEB, a benchmark suite that evaluates the performance of these models across various Arabic text tasks, ensuring they are effective in different dialects and cultural contexts. The results show that Swan-Large outperforms existing models in most Arabic tasks, highlighting its efficiency and effectiveness in Arabic natural language processing.'}, 'zh': {'title': 'Swan：阿拉伯语嵌入模型的创新之路', 'desc': '本文介绍了Swan，一个以阿拉伯语为中心的嵌入模型家族，适用于小规模和大规模的应用场景。Swan包括两个变体：基于ARBERTv2的Swan-Small和基于预训练阿拉伯大型语言模型ArMistral的Swan-Large。我们提出了ArabicMTEB，一个全面的基准套件，用于评估阿拉伯文本嵌入在跨语言、多方言、多领域和多文化方面的表现，涵盖八个不同任务和94个数据集。Swan-Large在大多数阿拉伯任务中表现优异，超越了Multilingual-E5-large，而Swan-Small也始终优于Multilingual-E5 base。'}}}, {'id': 'https://huggingface.co/papers/2410.22366', 'title': 'Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2410.22366', 'abstract': "Sparse autoencoders (SAEs) have become a core ingredient in the reverse engineering of large-language models (LLMs). For LLMs, they have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigated the possibility of using SAEs to learn interpretable features for a few-step text-to-image diffusion models, such as SDXL Turbo. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net. We find that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find one block that deals mainly with image composition, one that is mainly responsible for adding local details, and one for color, illumination, and style. Therefore, our work is an important first step towards better understanding the internals of generative text-to-image models like SDXL Turbo and showcases the potential of features learned by SAEs for the visual domain.   Code is available at https://github.com/surkovv/sdxl-unbox", 'score': 73, 'issue_id': 366, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'bb45ca9d1b89342a', 'authors': ['Viacheslav Surkov', 'Chris Wendler', 'Mikhail Terekhov', 'Justin Deschenaux', 'Robert West', 'Caglar Gulcehre'], 'affiliations': ['School of Computer and Communication Sciences EPFL Lausanne, Switzerland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22366.jpg', 'data': {'categories': ['#diffusion', '#cv', '#interpretability', '#open_source', '#3d', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая секреты SDXL Turbo: разреженные автоэнкодеры в действии', 'desc': 'Исследователи применили разреженные автоэнкодеры (SAE) для анализа внутренних представлений модели генерации изображений по тексту SDXL Turbo. Они обнаружили, что SAE способны выделять интерпретируемые признаки из промежуточных слоев модели. Анализ показал специализацию различных блоков модели на композиции изображения, локальных деталях и цвете/освещении. Это исследование открывает путь к лучшему пониманию и контролю генеративных моделей изображений.'}, 'en': {'title': 'Unlocking Interpretability in Text-to-Image Models with Sparse Autoencoders', 'desc': "This paper explores the use of Sparse Autoencoders (SAEs) to analyze and interpret the inner workings of text-to-image diffusion models, specifically SDXL Turbo. By training SAEs on the updates from transformer blocks in the model's denoising U-net, the authors demonstrate that these autoencoders can extract interpretable features that influence the image generation process. The study reveals that different transformer blocks specialize in various aspects of image creation, such as composition, local details, and color. This research marks a significant advancement in understanding generative models and highlights the utility of SAEs in the visual domain."}, 'zh': {'title': '稀疏自编码器助力文本到图像模型的理解', 'desc': '稀疏自编码器（SAEs）在大型语言模型（LLMs）的逆向工程中发挥了重要作用。本文探讨了将SAEs应用于文本到图像模型的可能性，特别是针对SDXL Turbo的几步扩散模型。我们发现，SAEs学习到的特征具有可解释性，并且对生成过程有因果影响，揭示了模型内部块之间的专业化。我们的研究为更好地理解生成文本到图像模型的内部机制迈出了重要的一步。'}}}, {'id': 'https://huggingface.co/papers/2410.23743', 'title': 'What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective', 'url': 'https://huggingface.co/papers/2410.23743', 'abstract': 'What makes a difference in the post-training of LLMs? We investigate the training patterns of different layers in large language models (LLMs), through the lens of gradient, when training with different responses and initial models. We are specifically interested in how fast vs. slow thinking affects the layer-wise gradients, given the recent popularity of training LLMs on reasoning paths such as chain-of-thoughts (CoT) and process rewards. In our study, fast thinking without CoT leads to larger gradients and larger differences of gradients across layers than slow thinking (Detailed CoT), indicating the learning stability brought by the latter. Moreover, pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. Additionally, we study whether the gradient patterns can reflect the correctness of responses when training different LLMs using slow vs. fast thinking paths. The results show that the gradients of slow thinking can distinguish correct and irrelevant reasoning paths. As a comparison, we conduct similar gradient analyses on non-reasoning knowledge learning tasks, on which, however, trivially increasing the response length does not lead to similar behaviors of slow thinking. Our study strengthens fundamental understandings of LLM training and sheds novel insights on its efficiency and stability, which pave the way towards building a generalizable System-2 agent. Our code, data, and gradient statistics can be found in: https://github.com/MingLiiii/Layer_Gradient.', 'score': 58, 'issue_id': 364, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '6af756426d4b0064', 'authors': ['Ming Li', 'Yanhong Li', 'Tianyi Zhou'], 'affiliations': ['University of Chicago', 'University of Maryland'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23743.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training', '#open_source', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Градиенты раскрывают тайны обучения языковых моделей', 'desc': 'Исследование анализирует паттерны обучения различных слоев больших языковых моделей (LLM) через призму градиентов. Авторы изучают влияние быстрого и медленного мышления на градиенты слоев, учитывая популярность обучения LLM на цепочках рассуждений (CoT) и процессных вознаграждениях. Результаты показывают, что медленное мышление с детальным CoT приводит к большей стабильности обучения по сравнению с быстрым мышлением. Исследование также выявляет различия в градиентных паттернах между предобученными и инструктированными LLM, а также между задачами рассуждения и обучения знаниям.'}, 'en': {'title': 'Unlocking Stability: The Power of Slow Thinking in LLM Training', 'desc': 'This paper explores how different training patterns in large language models (LLMs) affect their learning stability and efficiency. It focuses on the impact of fast versus slow thinking on layer-wise gradients during training, particularly when using reasoning techniques like chain-of-thoughts (CoT). The findings reveal that fast thinking generates larger gradients and more variability across layers, while slow thinking promotes stability and better distinguishes correct reasoning paths. Additionally, the study highlights that pre-trained LLMs are more resilient to the instability caused by fast thinking compared to instruction-tuned models.'}, 'zh': {'title': '快速与慢速思维对大语言模型训练的影响', 'desc': '本研究探讨了大语言模型（LLMs）在后训练阶段的不同层次的训练模式，特别关注快速思维与慢速思维对梯度的影响。研究发现，快速思维在没有链式思维（CoT）的情况下，会导致更大的梯度和层间梯度差异，而慢速思维则带来了更好的学习稳定性。预训练的LLMs对快速思维的不稳定性影响较小，而指令调优的LLMs则更为敏感。此外，慢速思维的梯度模式能够有效区分正确与无关的推理路径，增强了对LLM训练的基本理解。'}}}, {'id': 'https://huggingface.co/papers/2410.22476', 'title': 'A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents', 'url': 'https://huggingface.co/papers/2410.22476', 'abstract': 'In task-oriented dialogue systems, intent detection is crucial for interpreting user queries and providing appropriate responses. Existing research primarily addresses simple queries with a single intent, lacking effective systems for handling complex queries with multiple intents and extracting different intent spans. Additionally, there is a notable absence of multilingual, multi-intent datasets. This study addresses three critical tasks: extracting multiple intent spans from queries, detecting multiple intents, and developing a multi-lingual multi-label intent dataset. We introduce a novel multi-label multi-class intent detection dataset (MLMCID-dataset) curated from existing benchmark datasets. We also propose a pointer network-based architecture (MLMCID) to extract intent spans and detect multiple intents with coarse and fine-grained labels in the form of sextuplets. Comprehensive analysis demonstrates the superiority of our pointer network-based system over baseline approaches in terms of accuracy and F1-score across various datasets.', 'score': 24, 'issue_id': 370, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '22f1775d93baf7e1', 'authors': ['Ankan Mullick', 'Sombit Bose', 'Abhilash Nandy', 'Gajula Sai Chaitanya', 'Pawan Goyal'], 'affiliations': ['Computer Science and Engineering Department, IIT Kharagpur, India', 'Qualcomm, India'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22476.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#multilingual', '#graphs', '#optimization', '#dataset', '#transfer_learning', '#games', '#low_resource', '#machine_translation', '#architecture'], 'emoji': '🎯', 'ru': {'title': 'Точное определение множественных намерений в диалоговых системах', 'desc': 'Статья представляет новый подход к обнаружению намерений в диалоговых системах. Авторы предлагают архитектуру на основе указательной сети для извлечения нескольких намерений из запросов пользователей. Также создан новый многоязычный датасет для задачи многометочной классификации намерений. Эксперименты показывают превосходство предложенного метода над базовыми подходами по точности и F1-мере.'}, 'en': {'title': 'Enhancing Intent Detection for Complex Queries in Multilingual Dialogue Systems', 'desc': 'This paper focuses on improving task-oriented dialogue systems by enhancing intent detection, especially for complex queries that involve multiple intents. The authors identify a gap in existing research, which often only addresses simple queries, and they highlight the need for multilingual datasets that can handle multiple intents. To tackle this, they introduce the MLMCID-dataset, a new multi-label multi-class intent detection dataset, and a pointer network-based architecture designed to extract intent spans and detect multiple intents effectively. Their experiments show that this new approach outperforms traditional methods, achieving better accuracy and F1-scores across different datasets.'}, 'zh': {'title': '多意图检测的新突破', 'desc': '在任务导向对话系统中，意图检测对于理解用户查询和提供合适的响应至关重要。现有研究主要集中于处理单一意图的简单查询，缺乏有效的系统来处理复杂的多意图查询和提取不同的意图范围。此外，缺乏多语言和多意图的数据集。本研究提出了一个新的多标签多类意图检测数据集（MLMCID-dataset），并提出了一种基于指针网络的架构（MLMCID），能够提取意图范围并检测多种意图，分析结果显示该系统在准确性和F1分数上优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2410.24198', 'title': 'SelfCodeAlign: Self-Alignment for Code Generation', 'url': 'https://huggingface.co/papers/2410.24198', 'abstract': "Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component's effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance.", 'score': 20, 'issue_id': 372, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '76a150925181ec52', 'authors': ['Yuxiang Wei', 'Federico Cassano', 'Jiawei Liu', 'Yifeng Ding', 'Naman Jain', 'Zachary Mueller', 'Harm de Vries', 'Leandro von Werra', 'Arjun Guha', 'Lingming Zhang'], 'affiliations': ['Cursor AI', 'Hugging Face', 'Northeastern University', 'Roblox', 'ServiceNow Research', 'University of California, Berkeley', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24198.jpg', 'data': {'categories': ['#small_models', '#synthetic', '#benchmark', '#plp', '#data', '#training', '#dataset', '#open_source', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'SelfCodeAlign: самообучение ИИ-программиста без человеческих аннотаций', 'desc': 'SelfCodeAlign - это новый метод самонастройки языковых моделей для программирования без использования обширных человеческих аннотаций. Он генерирует разнообразные задачи программирования, создает и проверяет решения, а затем использует успешные примеры для дообучения модели. Эксперименты показывают, что SelfCodeAlign значительно улучшает производительность моделей разного размера в задачах программирования. Метод превосходит предыдущие подходы к инструктивной настройке без участия человека и дистилляции.'}, 'en': {'title': 'SelfCodeAlign: Revolutionizing Code LLMs with Minimal Human Input', 'desc': "This paper introduces SelfCodeAlign, a novel method for improving large language models (LLMs) specifically for coding tasks without needing extensive human input. The approach involves generating diverse coding tasks from high-quality code snippets and validating the model's responses in a controlled environment. By fine-tuning the model on a dataset of 74,000 instruction-response pairs, SelfCodeAlign significantly enhances the model's ability to follow instructions, achieving superior performance compared to larger models. The results demonstrate that this method is effective across various model sizes and establishes a new standard for self-aligned code LLMs."}, 'zh': {'title': '自我对齐，提升代码模型性能！', 'desc': '本论文提出了一种名为SelfCodeAlign的自我对齐代码大语言模型（LLM）的方法，旨在提高模型遵循人类指令的能力。该方法无需大量人工标注或蒸馏，利用相同的基础模型在数据生成过程中进行推理。SelfCodeAlign通过从高质量的种子代码片段中提取多样化的编码概念，生成新的任务，并在沙箱环境中验证每个任务的多个响应。实验结果表明，使用SelfCodeAlign生成的数据集进行微调后，模型在多个基准测试中表现优于之前的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2410.23918', 'title': 'BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments', 'url': 'https://huggingface.co/papers/2410.23918', 'abstract': 'Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from capability to availability, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce BitStack, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code is available at https://github.com/xinghaow99/BitStack.', 'score': 17, 'issue_id': 371, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '7699f83913665aca', 'authors': ['Xinghao Wang', 'Pengyu Wang', 'Bo Wang', 'Dong Zhang', 'Yunhua Zhou', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23918.jpg', 'data': {'categories': ['#inference', '#optimization', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'BitStack: эффективное сжатие весов для гибкого развертывания языковых моделей', 'desc': 'В статье представлен новый метод сжатия весов моделей машинного обучения под названием BitStack. Этот подход позволяет динамически регулировать размер модели с минимальной передачей данных между оперативной памятью и устройствами хранения. BitStack использует декомпозицию весов, учитывая значимость каждого параметра, что приводит к созданию резидуальных блоков размером примерно 1 бит на параметр в каждой итерации декомпозиции. Эксперименты показывают, что BitStack соответствует или превосходит сильные базовые линии квантования, особенно при экстремальных коэффициентах сжатия.'}, 'en': {'title': 'Dynamic Memory Management for Large Language Models with BitStack', 'desc': 'This paper presents BitStack, a new method for compressing large language models (LLMs) without the need for prior training. It addresses the challenge of deploying LLMs on devices with limited memory by allowing dynamic adjustments to model size based on available memory. BitStack uses weight decomposition to create small, efficient blocks of model parameters, which can be easily transmitted and loaded as needed. The results show that BitStack not only provides fine control over memory usage but also performs as well or better than traditional quantization methods, especially under high compression scenarios.'}, 'zh': {'title': 'BitStack：高效的权重压缩方法', 'desc': '大型语言模型（LLMs）在许多应用中取得了革命性进展，但在本地设备上的部署仍面临内存限制的挑战。本文提出了一种名为BitStack的新方法，它是一种无训练的权重压缩技术，能够在内存使用和模型性能之间实现灵活的权衡。通过权重分解，BitStack可以动态调整模型大小，减少运行内存与存储设备之间的传输。实验表明，尽管提供了细粒度的大小控制，BitStack在极端压缩比下的性能仍然与强量化基线相当或更优。'}}}, {'id': 'https://huggingface.co/papers/2410.20650', 'title': 'NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks', 'url': 'https://huggingface.co/papers/2410.20650', 'abstract': 'The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available.', 'score': 16, 'issue_id': 380, 'pub_date': '2024-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '2878f8bac8da005d', 'authors': ['Yongchang Hao', 'Yanshuai Cao', 'Lili Mou'], 'affiliations': ['Borealis AI', 'Dept. Computing Science & Alberta Machine Intelligence Institute (Amii), University of Alberta', 'Canada CIFAR AI Chair'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.20650.jpg', 'data': {'categories': ['#small_models', '#inference', '#optimization', '#training', '#open_source'], 'emoji': '🗜️', 'ru': {'title': 'NeuZip: эффективное сжатие нейросетей без потери качества', 'desc': 'NeuZip - это новая схема сжатия весов нейронных сетей, основанная на энтропии чисел с плавающей запятой. Метод позволяет значительно уменьшить объем памяти, необходимой для обучения и вывода крупных моделей, без потери производительности. Например, NeuZip сокращает использование памяти при обучении модели Llama-3 8B с 31 ГБ до менее чем 16 ГБ. При выводе метод может уменьшить использование памяти более чем вдвое, сохраняя почти безупречную производительность.'}, 'en': {'title': 'NeuZip: Compressing Neural Networks Without Compromise', 'desc': 'This paper presents NeuZip, a novel weight compression technique designed for neural networks. NeuZip leverages the entropy of floating-point numbers to compress model weights, allowing for more efficient use of on-device memory during both training and inference. The method significantly reduces the memory requirements of large models, such as the Llama-3 8B, without compromising their performance. By implementing NeuZip, users can achieve a memory footprint reduction of over 50% while maintaining the integrity of the training dynamics and inference results.'}, 'zh': {'title': 'NeuZip：高效内存压缩，性能不打折！', 'desc': '本论文介绍了一种新的权重压缩方案，称为NeuZip，旨在提高神经网络的内存效率。通过利用浮点数的熵，NeuZip能够在不牺牲性能的情况下，显著减少模型的内存占用。具体来说，我们将Llama-3 8B模型的训练内存需求从31GB降低到16GB以下，同时保持训练动态不变。在推理阶段，我们的方法可以将内存使用减少一半以上，同时保持接近无损的性能。'}}}, {'id': 'https://huggingface.co/papers/2410.23933', 'title': 'Language Models can Self-Lengthen to Generate Long Texts', 'url': 'https://huggingface.co/papers/2410.23933', 'abstract': 'Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to process long contexts, yet a notable gap remains in generating long, aligned outputs. This limitation stems from a training gap where pre-training lacks effective instructions for long-text generation, and post-training data primarily consists of short query-response pairs. Current approaches, such as instruction backtranslation and behavior imitation, face challenges including data quality, copyright issues, and constraints on proprietary model usage. In this paper, we introduce an innovative iterative training framework called Self-Lengthen that leverages only the intrinsic knowledge and skills of LLMs without the need for auxiliary data or proprietary models. The framework consists of two roles: the Generator and the Extender. The Generator produces the initial response, which is then split and expanded by the Extender. This process results in a new, longer response, which is used to train both the Generator and the Extender iteratively. Through this process, the models are progressively trained to handle increasingly longer responses. Experiments on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long-text generation, when applied to top open-source LLMs such as Qwen2 and LLaMA3. Our code is publicly available at https://github.com/QwenLM/Self-Lengthen.', 'score': 15, 'issue_id': 363, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '2ba3bbe4b8a9836d', 'authors': ['Shanghaoran Quan', 'Tianyi Tang', 'Bowen Yu', 'An Yang', 'Dayiheng Liu', 'Bofei Gao', 'Jianhong Tu', 'Yichang Zhang', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Qwen Team, Alibaba Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23933.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#optimization', '#training', '#open_source', '#architecture', '#long_context'], 'emoji': '📏', 'ru': {'title': 'Самоудлинение: революция в генерации длинных текстов для LLM', 'desc': 'Статья представляет новый метод обучения больших языковых моделей (LLM) для генерации длинных текстов, называемый Self-Lengthen. Этот подход использует итеративное обучение с двумя ролями: Генератор и Расширитель, которые работают вместе для создания и улучшения длинных ответов. Метод не требует дополнительных данных или проприетарных моделей, опираясь только на внутренние знания и навыки LLM. Эксперименты показывают, что Self-Lengthen превосходит существующие методы в генерации длинных текстов при применении к ведущим открытым LLM.'}, 'en': {'title': 'Empowering LLMs to Generate Longer, Coherent Texts with Self-Lengthen', 'desc': "This paper addresses the challenge of generating long, coherent outputs from Large Language Models (LLMs), which often struggle due to a lack of effective training for long-text generation. The authors propose a novel iterative training framework called Self-Lengthen, which utilizes the models' existing capabilities without relying on external data or proprietary models. The framework involves two components: a Generator that creates an initial response and an Extender that expands this response into a longer format. Through iterative training, the models improve their ability to generate longer, aligned outputs, demonstrating superior performance compared to existing methods in experiments and human evaluations."}, 'zh': {'title': '自我延长：提升长文本生成的创新框架', 'desc': '最近，大型语言模型（LLMs）的进步显著提升了处理长文本的能力，但在生成长且一致的输出方面仍存在不足。这一限制源于训练过程中的缺陷，预训练缺乏有效的长文本生成指令，而后期训练的数据主要是短问答对。本文提出了一种创新的迭代训练框架Self-Lengthen，利用LLMs的内在知识和技能，无需辅助数据或专有模型。该框架包括生成器和扩展器两个角色，通过生成初始响应并进行扩展，逐步训练模型以处理更长的响应。'}}}, {'id': 'https://huggingface.co/papers/2410.24175', 'title': 'Constraint Back-translation Improves Complex Instruction Following of Large Language Models', 'url': 'https://huggingface.co/papers/2410.24175', 'abstract': "Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.", 'score': 15, 'issue_id': 363, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '6550f79d46b1945c', 'authors': ['Yunjia Qi', 'Hao Peng', 'Xiaozhi Wang', 'Bin Xu', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Department of Computer Science and Technology, BNRist, Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24175.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#optimization', '#data', '#training', '#dataset', '#open_source'], 'emoji': '🦀', 'ru': {'title': 'Улучшение следования сложным инструкциям в LLM через обратный перевод ограничений', 'desc': "Эта статья представляет новый метод улучшения способности больших языковых моделей (LLM) следовать сложным инструкциям. Авторы предлагают технику 'обратного перевода ограничений', которая использует существующие наборы данных для генерации высококачественных пар инструкция-ответ со сложными ограничениями. Они создали набор данных CRAB, используя эту технику, и показали, что дообучение на нем улучшает способность LLM следовать сложным инструкциям. Исследователи также обнаружили, что обратный перевод ограничений может служить полезной вспомогательной целью обучения."}, 'en': {'title': 'Enhancing LLMs with Constraint Back-Translation for Complex Instructions', 'desc': 'This paper addresses the challenges that large language models (LLMs) face when following complex instructions. It critiques traditional instruction-tuning methods that rely on generating complex instruction-response pairs, which often leads to poor performance due to the limitations of LLMs. The authors introduce a new technique called constraint back-translation, which enhances existing high-quality instruction-response pairs by adding implicit complex constraints. Their experiments show that using this method to create a dataset, named CRAB, significantly improves the ability of various LLMs to follow complex instructions, demonstrating its effectiveness as a training strategy.'}, 'zh': {'title': '提升复杂指令跟随能力的新方法', 'desc': '大型语言模型（LLMs）在处理复杂指令时表现不佳，尤其是在格式和长度等约束方面。以往的研究通过对复杂指令-响应对进行后训练来改进模型，但效果有限。本文提出了一种新颖的数据生成技术，称为约束反向翻译，利用现有数据集中的高质量指令-响应对，添加已满足的复杂约束，从而提高数据质量。实验表明，在新生成的数据集CRAB上进行后训练，可以显著提升多个基础LLM在复杂指令跟随能力上的表现。'}}}, {'id': 'https://huggingface.co/papers/2410.24213', 'title': 'Learning Video Representations without Natural Videos', 'url': 'https://huggingface.co/papers/2410.24213', 'abstract': 'In this paper, we show that useful video representations can be learned from synthetic videos and natural images, without incorporating natural videos in the training. We propose a progression of video datasets synthesized by simple generative processes, that model a growing set of natural video properties (e.g. motion, acceleration, and shape transformations). The downstream performance of video models pre-trained on these generated datasets gradually increases with the dataset progression. A VideoMAE model pre-trained on our synthetic videos closes 97.2% of the performance gap on UCF101 action classification between training from scratch and self-supervised pre-training from natural videos, and outperforms the pre-trained model on HMDB51. Introducing crops of static images to the pre-training stage results in similar performance to UCF101 pre-training and outperforms the UCF101 pre-trained model on 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the low-level properties of the datasets, we identify correlations between frame diversity, frame similarity to natural data, and downstream performance. Our approach provides a more controllable and transparent alternative to video data curation processes for pre-training.', 'score': 14, 'issue_id': 362, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'a01960d8f855aede', 'authors': ['Xueyang Yu', 'Xinlei Chen', 'Yossi Gandelsman'], 'affiliations': ['Meta AI', 'ShanghaiTech University', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24213.jpg', 'data': {'categories': ['#synthetic', '#cv', '#video', '#training', '#dataset', '#transfer_learning'], 'emoji': '🎞️', 'ru': {'title': 'Синтетические видео для эффективного предобучения видеомоделей', 'desc': 'Исследователи показали, что полезные представления видео можно обучить на синтетических видео и естественных изображениях, без использования реальных видео. Они предложили последовательность синтетических видеонаборов данных, моделирующих различные свойства естественных видео. Модель VideoMAE, предобученная на этих синтетических данных, показала результаты, близкие к предобучению на реальных видео для задачи классификации действий. Анализ свойств наборов данных выявил корреляции между разнообразием кадров, их схожестью с естественными данными и эффективностью на целевых задачах.'}, 'en': {'title': 'Synthetic Videos: A New Path to Effective Video Representation Learning', 'desc': 'This paper demonstrates that effective video representations can be learned using synthetic videos and natural images, without needing natural videos for training. The authors introduce a series of progressively complex synthetic video datasets that capture essential characteristics of natural videos, such as motion and shape changes. They show that a VideoMAE model pre-trained on these synthetic datasets significantly narrows the performance gap compared to models trained on natural videos. Additionally, incorporating static image crops during pre-training enhances performance on various action classification tasks, suggesting a strong link between dataset properties and model effectiveness.'}, 'zh': {'title': '合成视频助力视频表示学习的突破', 'desc': '本文展示了如何从合成视频和自然图像中学习有用的视频表示，而无需在训练中使用自然视频。我们提出了一系列通过简单生成过程合成的视频数据集，这些数据集模拟了越来越多的自然视频特性，如运动、加速度和形状变换。通过在这些合成数据集上预训练的VideoMAE模型，在UCF101动作分类任务中，性能差距缩小了97.2%，并在HMDB51上表现优于预训练模型。我们的研究表明，数据集的帧多样性和与自然数据的相似性与下游性能之间存在相关性，为视频数据的预训练提供了更可控和透明的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2410.22394', 'title': "AAAR-1.0: Assessing AI's Potential to Assist Research", 'url': 'https://huggingface.co/papers/2410.22394', 'abstract': 'Numerous studies have assessed the proficiency of AI systems, particularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and creative content generation. However, researchers face unique challenges and opportunities in leveraging LLMs for their own work, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EquationInference, assessing the correctness of equations based on the contextual information in paper submissions; (ii) ExperimentDesign, designing experiments to validate research ideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper submissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews is deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new versions.', 'score': 13, 'issue_id': 379, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'cbb8d9bd8efa2823', 'authors': ['Renze Lou', 'Hanzi Xu', 'Sijia Wang', 'Jiangshu Du', 'Ryo Kamoi', 'Xiaoxin Lu', 'Jian Xie', 'Yuxuan Sun', 'Yusen Zhang', 'Jihyun Janice Ahn', 'Hongchao Fang', 'Zhuoyang Zou', 'Wenchao Ma', 'Xi Li', 'Kai Zhang', 'Congying Xia', 'Lifu Huang', 'Wenpeng Yin'], 'affiliations': ['Fudan University', 'Netflix', 'Ohio State University', 'Pennsylvania State University', 'Salesforce Research', 'University of Alabama at Birmingham', 'University of California, Davis', 'University of Illinois Chicago', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.22394.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#plp', '#dataset', '#open_source', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'AAAR-1.0: Новый рубеж в оценке ИИ для научных исследований', 'desc': 'Представлен новый бенчмарк AAAR-1.0 для оценки производительности больших языковых моделей (LLM) в исследовательских задачах. Он включает четыре задания: вывод уравнений, разработку экспериментов, выявление слабых мест в статьях и анализ рецензий. AAAR-1.0 отличается от существующих бенчмарков ориентацией на исследовательскую деятельность и необходимостью глубоких экспертных знаний. Оценка различных LLM на этом бенчмарке выявила их потенциал и ограничения в выполнении сложных исследовательских задач.'}, 'en': {'title': 'Empowering Researchers with AI: Introducing AAAR-1.0', 'desc': 'This paper presents AAAR-1.0, a new benchmark dataset aimed at evaluating the performance of large language models (LLMs) in research-related tasks. The dataset focuses on four key tasks: assessing equation correctness, designing experiments, identifying weaknesses in papers, and critiquing reviews. Unlike previous benchmarks, AAAR-1.0 is specifically tailored for research activities that require specialized knowledge. The study highlights both the capabilities and limitations of current LLMs in handling complex research tasks, with plans for future iterations of the benchmark.'}, 'zh': {'title': 'AAAR-1.0：提升研究效率的语言模型基准', 'desc': '本研究介绍了AAAR-1.0，这是一个用于评估大型语言模型（LLMs）在研究任务中的表现的基准数据集。该数据集专注于三个关键任务：方程推理、实验设计和论文弱点识别，旨在帮助研究人员更好地利用LLMs。与以往的基准不同，AAAR-1.0强调研究导向，要求深厚的领域专业知识。通过对开源和专有LLMs的评估，我们揭示了它们在复杂研究任务中的潜力和局限性。'}}}, {'id': 'https://huggingface.co/papers/2410.24032', 'title': 'Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks', 'url': 'https://huggingface.co/papers/2410.24032', 'abstract': "The rise of large language models (LLMs) has revolutionized user interactions with knowledge-based systems, enabling chatbots to synthesize vast amounts of information and assist with complex, exploratory tasks. However, LLM-based chatbots often struggle to provide personalized support, particularly when users start with vague queries or lack sufficient contextual information. This paper introduces the Collaborative Assistant for Personalized Exploration (CARE), a system designed to enhance personalization in exploratory tasks by combining a multi-agent LLM framework with a structured user interface. CARE's interface consists of a Chat Panel, Solution Panel, and Needs Panel, enabling iterative query refinement and dynamic solution generation. The multi-agent framework collaborates to identify both explicit and implicit user needs, delivering tailored, actionable solutions. In a within-subject user study with 22 participants, CARE was consistently preferred over a baseline LLM chatbot, with users praising its ability to reduce cognitive load, inspire creativity, and provide more tailored solutions. Our findings highlight CARE's potential to transform LLM-based systems from passive information retrievers to proactive partners in personalized problem-solving and exploration.", 'score': 8, 'issue_id': 368, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'b4fbb07af8c8665c', 'authors': ['Yingzhe Peng', 'Xiaoting Qin', 'Zhiyang Zhang', 'Jue Zhang', 'Qingwei Lin', 'Xu Yang', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Microsoft, China', 'Microsoft, USA', 'Southeast University, China', 'State Key Laboratory for Novel Software Technology, Nanjing University, China'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24032.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#interpretability', '#agents', '#architecture', '#alignment'], 'emoji': '🤝', 'ru': {'title': 'CARE: Персонализированный ИИ-помощник для исследовательских задач', 'desc': 'Статья представляет систему CARE (Collaborative Assistant for Personalized Exploration), которая улучшает персонализацию в исследовательских задачах, сочетая мультиагентную архитектуру на основе больших языковых моделей (LLM) со структурированным пользовательским интерфейсом. CARE состоит из панелей чата, решений и потребностей, что позволяет итеративно уточнять запросы и динамически генерировать решения. Система использует коллаборативный подход для выявления явных и неявных потребностей пользователя, предоставляя индивидуальные, действенные решения. Пользовательское исследование показало, что CARE предпочтительнее базового чат-бота на основе LLM, снижая когнитивную нагрузку и стимулируя креативность.'}, 'en': {'title': 'Transforming LLMs into Proactive Personal Assistants', 'desc': 'This paper presents the Collaborative Assistant for Personalized Exploration (CARE), a system that improves how users interact with large language models (LLMs) during exploratory tasks. CARE uses a multi-agent framework that works together to understand both the explicit and implicit needs of users, allowing for more personalized assistance. The system features a structured user interface with different panels that help users refine their queries and generate solutions dynamically. A user study showed that participants preferred CARE over traditional LLM chatbots, noting its effectiveness in reducing cognitive load and providing tailored support.'}, 'zh': {'title': '个性化探索的协作助手CARE', 'desc': '这篇论文介绍了一种名为CARE的系统，旨在提高大型语言模型（LLM）在探索性任务中的个性化支持。CARE结合了多代理LLM框架和结构化用户界面，帮助用户在模糊查询时更好地获取信息。系统的界面包括聊天面板、解决方案面板和需求面板，支持用户迭代优化查询和动态生成解决方案。研究表明，CARE在用户体验上优于传统的LLM聊天机器人，能够减轻认知负担，激发创造力，并提供更具针对性的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2410.24211', 'title': 'DELTA: Dense Efficient Long-range 3D Tracking for any video', 'url': 'https://huggingface.co/papers/2410.24211', 'abstract': 'Tracking dense 3D motion from monocular videos remains challenging, particularly when aiming for pixel-level precision over long sequences. We introduce \\Approach, a novel method that efficiently tracks every pixel in 3D space, enabling accurate motion estimation across entire videos. Our approach leverages a joint global-local attention mechanism for reduced-resolution tracking, followed by a transformer-based upsampler to achieve high-resolution predictions. Unlike existing methods, which are limited by computational inefficiency or sparse tracking, \\Approach delivers dense 3D tracking at scale, running over 8x faster than previous methods while achieving state-of-the-art accuracy. Furthermore, we explore the impact of depth representation on tracking performance and identify log-depth as the optimal choice. Extensive experiments demonstrate the superiority of \\Approach on multiple benchmarks, achieving new state-of-the-art results in both 2D and 3D dense tracking tasks. Our method provides a robust solution for applications requiring fine-grained, long-term motion tracking in 3D space.', 'score': 8, 'issue_id': 368, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': 'e9fe68be8b772de2', 'authors': ['Tuan Duc Ngo', 'Peiye Zhuang', 'Chuang Gan', 'Evangelos Kalogerakis', 'Sergey Tulyakov', 'Hsin-Ying Lee', 'Chaoyang Wang'], 'affiliations': ['Snap Inc.', 'TU Crete', 'UMass Amherst'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24211.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#cv', '#video', '#optimization', '#3d', '#architecture'], 'emoji': '🎥', 'ru': {'title': 'Революция в плотном 3D-трекинге: быстро, точно, масштабируемо', 'desc': 'Статья представляет новый метод для точного отслеживания движения каждого пикселя в 3D-пространстве на протяжении длинных видеопоследовательностей. Авторы используют механизм совместного глобально-локального внимания для отслеживания на пониженном разрешении, а затем применяют трансформер для повышения разрешения предсказаний. Метод работает в 8 раз быстрее предыдущих подходов, достигая при этом наилучшей точности. Эксперименты показывают превосходство предложенного метода на нескольких эталонных наборах данных для задач плотного отслеживания в 2D и 3D.'}, 'en': {'title': 'Efficient Dense 3D Motion Tracking with State-of-the-Art Precision', 'desc': 'This paper presents a new method called \\Approach for tracking dense 3D motion from monocular videos with high precision. It utilizes a joint global-local attention mechanism to perform efficient tracking at reduced resolution, followed by a transformer-based upsampler to enhance the predictions to high resolution. The method significantly improves computational efficiency, running over 8 times faster than previous techniques while achieving state-of-the-art accuracy in both 2D and 3D tracking tasks. Additionally, the study identifies log-depth as the best depth representation for enhancing tracking performance.'}, 'zh': {'title': '高效稠密三维运动追踪的新方法', 'desc': '本论文介绍了一种新方法\textit{Approach}，用于从单目视频中高效追踪每个像素的三维运动。该方法结合了全局和局部注意力机制，先进行低分辨率追踪，然后使用基于变换器的上采样器实现高分辨率预测。与现有方法相比，\textit{Approach}在计算效率和稠密追踪方面表现出色，速度比之前的方法快8倍，同时保持了最先进的准确性。通过广泛的实验，我们证明了\textit{Approach}在多个基准测试中的优越性，提供了在三维空间中进行精细、长期运动追踪的强大解决方案。'}}}, {'id': 'https://huggingface.co/papers/2410.21969', 'title': 'BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays', 'url': 'https://huggingface.co/papers/2410.21969', 'abstract': 'Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.', 'score': 8, 'issue_id': 362, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': 'f3fe2798fa53bad0', 'authors': ['Yang Zhou', 'Tan Li Hui Faith', 'Yanyu Xu', 'Sicong Leng', 'Xinxing Xu', 'Yong Liu', 'Rick Siow Mong Goh'], 'affiliations': ['C-FAIR, Shandong University, China', 'Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore', 'Microsoft Research Asia Singapore', 'Nanyang Technological University, Singapore', 'National University of Singapore, Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.21969.jpg', 'data': {'categories': ['#benchmark', '#cv', '#graphs', '#healthcare', '#transfer_learning', '#dataset', '#open_source', '#survey'], 'emoji': '〰️', 'ru': {'title': 'Единый стандарт для оценки моделей анализа медицинских данных', 'desc': 'Статья представляет BenchX - унифицированную систему оценки методов предобучения моделей для анализа медицинских изображений и текстов (MedVLP). BenchX включает обширные наборы данных рентгеновских снимков грудной клетки, стандартизированные протоколы предобработки и разделения данных, а также унифицированные методы дообучения моделей. С помощью BenchX авторы провели сравнительный анализ девяти современных методов MedVLP, выявив неожиданные результаты и необходимость пересмотра некоторых выводов предыдущих исследований.'}, 'en': {'title': 'Standardizing Medical Vision-Language Pretraining Evaluation with BenchX', 'desc': 'The paper introduces MedVLP, a method for learning visual representations from medical images and reports, which can be applied to various medical tasks. It highlights the challenges in evaluating MedVLP methods due to inconsistencies in datasets and implementations. To address this, the authors propose BenchX, a benchmark framework that standardizes evaluation across multiple MedVLP methods using public chest X-ray datasets. BenchX includes comprehensive datasets, standardized preprocessing, and unified finetuning protocols, allowing for systematic comparisons and improved performance assessments of MedVLP techniques.'}, 'zh': {'title': '统一基准，提升医疗视觉语言预训练的比较与评估', 'desc': '本文提出了一个名为BenchX的统一基准框架，旨在评估医疗视觉语言预训练（MedVLP）方法的性能。BenchX包含三个主要组成部分：涵盖九个数据集和四个医疗任务的综合数据集、标准化的数据预处理和训练测试划分的基准套件，以及适应不同MedVLP方法的一致微调协议。通过使用BenchX，我们为九种最先进的MedVLP方法建立了基线，并发现一些早期的MedVLP方法的性能可以提升，超过更近期的方法。这一发现促使我们重新审视MedVLP领域的研究进展和结论。'}}}, {'id': 'https://huggingface.co/papers/2410.21666', 'title': 'Minimum Entropy Coupling with Bottleneck', 'url': 'https://huggingface.co/papers/2410.21666', 'abstract': 'This paper investigates a novel lossy compression framework operating under logarithmic loss, designed to handle situations where the reconstruction distribution diverges from the source distribution. This framework is especially relevant for applications that require joint compression and retrieval, and in scenarios involving distributional shifts due to processing. We show that the proposed formulation extends the classical minimum entropy coupling framework by integrating a bottleneck, allowing for a controlled degree of stochasticity in the coupling. We explore the decomposition of the Minimum Entropy Coupling with Bottleneck (MEC-B) into two distinct optimization problems: Entropy-Bounded Information Maximization (EBIM) for the encoder, and Minimum Entropy Coupling (MEC) for the decoder. Through extensive analysis, we provide a greedy algorithm for EBIM with guaranteed performance, and characterize the optimal solution near functional mappings, yielding significant theoretical insights into the structural complexity of this problem. Furthermore, we illustrate the practical application of MEC-B through experiments in Markov Coding Games (MCGs) under rate limits. These games simulate a communication scenario within a Markov Decision Process, where an agent must transmit a compressed message from a sender to a receiver through its actions. Our experiments highlight the trade-offs between MDP rewards and receiver accuracy across various compression rates, showcasing the efficacy of our method compared to conventional compression baseline.', 'score': 4, 'issue_id': 379, 'pub_date': '2024-10-29', 'pub_date_card': {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'}, 'hash': '9e1b09f8eff55094', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#rl', '#benchmark', '#optimization', '#math', '#data', '#training', '#games'], 'emoji': '🗜️', 'ru': {'title': 'Сжатие данных с контролируемыми потерями для задач с изменением распределения', 'desc': "Статья представляет новую систему сжатия с потерями, работающую в условиях логарифмических потерь, когда распределение реконструкции отличается от исходного распределения. Это расширение классической задачи минимального энтропийного связывания, включающее 'бутылочное горлышко' для контроля стохастичности. Авторы разделяют задачу на две оптимизационные подзадачи: максимизацию информации с ограничением энтропии (EBIM) для кодировщика и минимальное энтропийное связывание (MEC) для декодера. Практическое применение метода демонстрируется на примере марковских игр кодирования (MCG) с ограничениями скорости передачи данных."}, 'en': {'title': 'Revolutionizing Compression with Controlled Stochasticity', 'desc': 'This paper presents a new lossy compression framework that uses logarithmic loss to effectively manage cases where the output distribution differs from the original input distribution. It is particularly useful for tasks that involve both compression and retrieval, especially when there are changes in the data distribution. The authors enhance the traditional minimum entropy coupling method by adding a bottleneck, which allows for a controlled level of randomness in the data coupling process. They break down the new framework, called Minimum Entropy Coupling with Bottleneck (MEC-B), into two optimization tasks: one for maximizing information during encoding and another for minimizing entropy during decoding, providing a greedy algorithm with proven performance.'}, 'zh': {'title': '新型有损压缩框架：控制随机性的最小熵耦合', 'desc': '本文研究了一种新颖的有损压缩框架，该框架在对数损失下运行，旨在处理重建分布与源分布不一致的情况。该框架特别适用于需要联合压缩和检索的应用，以及由于处理导致的分布变化场景。我们展示了所提出的公式如何通过集成瓶颈扩展经典的最小熵耦合框架，从而在耦合中允许控制的随机性程度。通过对最小熵耦合与瓶颈（MEC-B）的分解，我们提出了编码器的熵约束信息最大化（EBIM）和解码器的最小熵耦合（MEC）两个优化问题，并提供了保证性能的贪婪算法。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2410.24218', 'title': 'Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use', 'url': 'https://huggingface.co/papers/2410.24218', 'abstract': "In real-world scenarios, it is desirable for embodied agents to have the ability to leverage human language to gain explicit or implicit knowledge for learning tasks. Despite recent progress, most previous approaches adopt simple low-level instructions as language inputs, which may not reflect natural human communication. It's not clear how to incorporate rich language use to facilitate task learning. To address this question, this paper studies different types of language inputs in facilitating reinforcement learning (RL) embodied agents. More specifically, we examine how different levels of language informativeness (i.e., feedback on past behaviors and future guidance) and diversity (i.e., variation of language expressions) impact agent learning and inference. Our empirical results based on four RL benchmarks demonstrate that agents trained with diverse and informative language feedback can achieve enhanced generalization and fast adaptation to new tasks. These findings highlight the pivotal role of language use in teaching embodied agents new tasks in an open world. Project website: https://github.com/sled-group/Teachable_RL", 'score': 4, 'issue_id': 375, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '3f784be6816fb87a', 'authors': ['Jiajun Xi', 'Yinong He', 'Jianing Yang', 'Yinpei Dai', 'Joyce Chai'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.24218.jpg', 'data': {'categories': ['#reasoning', '#rl', '#rlhf', '#benchmark', '#transfer_learning', '#games', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Язык как ключ к обучению ИИ-агентов в реальном мире', 'desc': 'Статья исследует влияние различных типов языковых входных данных на обучение агентов с подкреплением в воплощенных средах. Авторы изучают, как информативность и разнообразие языка влияют на обучение и вывод агентов. Эксперименты на четырех эталонных задачах показывают, что агенты, обученные с разнообразной и информативной языковой обратной связью, достигают лучшей обобщаемости и быстрой адаптации к новым задачам. Результаты подчеркивают ключевую роль использования естественного языка в обучении воплощенных агентов новым задачам.'}, 'en': {'title': 'Empowering Agents with Rich Language for Smarter Learning', 'desc': 'This paper explores how embodied agents can use human language to improve their learning in real-world tasks. It focuses on the impact of different types of language inputs, specifically looking at how informative and diverse language can enhance reinforcement learning (RL) processes. The study shows that agents receiving varied and detailed language feedback perform better in adapting to new tasks and generalizing their knowledge. These findings emphasize the importance of natural language communication in training intelligent agents to operate effectively in dynamic environments.'}, 'zh': {'title': '语言助力智能体学习新任务', 'desc': '本论文研究了如何利用人类语言来帮助具身智能体学习任务。我们探讨了不同类型的语言输入对强化学习（RL）智能体的影响，特别是语言信息量和多样性如何影响学习效果。实验证明，使用多样且信息丰富的语言反馈训练的智能体，能够更好地适应新任务并提高泛化能力。这些发现强调了语言在教导具身智能体新任务中的重要性。'}}}, {'id': 'https://huggingface.co/papers/2410.23825', 'title': 'GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages', 'url': 'https://huggingface.co/papers/2410.23825', 'abstract': 'The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it - including the pipeline, language identification model, and filters - available to the research community. Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1, Pipeline v. 3.0 https://github.com/cisnlp/GlotCC.', 'score': 3, 'issue_id': 375, 'pub_date': '2024-10-31', 'pub_date_card': {'ru': '31 октября', 'en': 'October 31', 'zh': '10月31日'}, 'hash': '7d1bf1bef5a430e0', 'authors': ['Amir Hossein Kargaran', 'François Yvon', 'Hinrich Schütze'], 'affiliations': ['LMU Munich & Munich Center for Machine Learning, Munich, Germany', 'Sorbonne Université & CNRS, ISIR, Paris, France'], 'pdf_title_img': 'assets\\pdf\\title_img\\2410.23825.jpg', 'data': {'categories': ['#multilingual', '#data', '#dataset', '#open_source', '#low_resource'], 'emoji': '🌐', 'ru': {'title': 'GlotCC: Многоязычный корпус для обучения языковых моделей', 'desc': 'Статья представляет GlotCC - чистый корпус текстов объемом 2 ТБ, охватывающий более 1000 языков, включая миноритарные. Корпус создан на основе CommonCrawl с использованием воспроизводимого открытого конвейера обработки данных. GlotCC тщательно очищен от шума, что делает его надежным для использования в обучении языковых моделей. Авторы предоставляют доступ к корпусу и системе его создания для исследовательского сообщества.'}, 'en': {'title': 'Empowering Minority Languages with GlotCC: A Clean, Large-Scale Corpus', 'desc': 'This paper introduces GlotCC, a large and clean text corpus designed for minority languages, addressing the gap in available data for these languages in the context of pretrained language models. The corpus is derived from CommonCrawl and spans over 2TB, covering more than 1000 languages, making it a valuable resource for researchers. The authors emphasize the importance of having a reproducible and open-source pipeline for generating such corpora, which includes a language identification model and noise filters. By providing GlotCC and its generation system to the research community, the authors aim to enhance the development of language models for underrepresented languages.'}, 'zh': {'title': 'GlotCC：多语言研究的新资源', 'desc': '随着预训练语言模型的发展，对大规模文本语料库的需求不断增加。现有的语料库主要集中在大型语言社区，缺乏覆盖广泛少数语言的资源。我们提出了GlotCC，这是一个干净的、文档级的2TB通用语料库，来源于CommonCrawl，涵盖了1000多种语言。我们将GlotCC及其生成系统（包括管道、语言识别模型和过滤器）提供给研究社区，以促进多语言研究。'}}}, {'id': 'https://huggingface.co/papers/2411.10958', 'title': 'SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration', 'url': 'https://huggingface.co/papers/2411.10958', 'abstract': 'Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. SageAttention utilizes 8-bit matrix multiplication, 16-bit matrix multiplication with 16-bit accumulator, and precision-enhancing methods, implementing an accurate and 2x speedup kernel compared to FlashAttention2. To further enhance the efficiency of attention computation while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrixes (Q, K) to INT4 in a warp-level granularity and quantize matrixes (widetilde P, V) to FP8. Second, we propose a method to smooth Q and V, enhancing the accuracy of attention with INT4 QK and FP8 PV. Third, we analyze the quantization accuracy across timesteps and layers, then propose an adaptive quantization method to ensure the end-to-end metrics over various models. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention.', 'score': 33, 'issue_id': 697, 'pub_date': '2024-11-17', 'pub_date_card': {'ru': '17 ноября', 'en': 'November 17', 'zh': '11月17日'}, 'hash': '3500eddd4100341a', 'authors': ['Jintao Zhang', 'Haofeng Huang', 'Pengle Zhang', 'Jia Wei', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10958.jpg', 'data': {'categories': ['#video', '#inference', '#training', '#optimization', '#cv'], 'emoji': '⚡', 'ru': {'title': 'Быстрее и точнее: революция в механизме внимания', 'desc': 'SageAttention2 - это новый метод для ускорения вычисления внимания в нейронных сетях. Он использует 4-битное матричное умножение и дополнительные техники повышения точности. Метод включает квантизацию матриц Q и K до INT4, а матриц P и V до FP8, а также сглаживание Q и V для улучшения точности. SageAttention2 превосходит FlashAttention2 и xformers по скорости операций примерно в 3 и 5 раз соответственно на RTX4090.'}, 'en': {'title': 'Accelerating Attention with Precision: SageAttention2', 'desc': 'This paper introduces SageAttention, a novel approach to improve the efficiency of attention mechanisms in machine learning models by utilizing quantization techniques. It employs 8-bit and 16-bit matrix multiplications, achieving a 2x speedup over existing methods like FlashAttention2. The follow-up, SageAttention2, further enhances performance by implementing 4-bit matrix multiplication while maintaining accuracy through advanced precision-enhancing methods. The results demonstrate significant improvements in operations per second (OPS) and minimal loss in performance across various model types, including those used for language processing and image generation.'}, 'zh': {'title': '提升注意力计算效率的新方法', 'desc': '本论文提出了一种新的注意力机制SageAttention2，旨在提高计算效率并保持精度。通过使用4位矩阵乘法和其他精度增强技术，SageAttention2在性能上显著优于现有方法。我们还提出了一种自适应量化方法，以确保在不同模型上的端到端指标保持稳定。实验结果表明，SageAttention2在多种任务中表现出色，尤其是在大语言处理和图像生成方面。'}}}, {'id': 'https://huggingface.co/papers/2411.13503', 'title': 'VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models', 'url': 'https://huggingface.co/papers/2411.13503', 'abstract': 'Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects "video generation quality" into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has several appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). The evaluation metrics with fine-grained levels reveal individual models\' strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks\' alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models\' ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models. 4) Versatile Benchmarking: VBench++ supports evaluating text-to-video and image-to-video. We introduce a high-quality Image Suite with an adaptive aspect ratio to enable fair evaluations across different image-to-video generation settings. Beyond assessing technical quality, VBench++ evaluates the trustworthiness of video generative models, providing a more holistic view of model performance. 5) Full Open-Sourcing: We fully open-source VBench++ and continually add new video generation models to our leaderboard to drive forward the field of video generation.', 'score': 23, 'issue_id': 700, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'b7c2e10d1b01293e', 'authors': ['Ziqi Huang', 'Fan Zhang', 'Xiaojie Xu', 'Yinan He', 'Jiashuo Yu', 'Ziyue Dong', 'Qianli Ma', 'Nattapol Chanpaisit', 'Chenyang Si', 'Yuming Jiang', 'Yaohui Wang', 'Xinyuan Chen', 'Ying-Cong Chen', 'Limin Wang', 'Dahua Lin', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Hong Kong University of Science and Technology', 'Nanjing University', 'Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2411.13503.jpg', 'data': {'categories': ['#open_source', '#alignment', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'VBench: Новый стандарт оценки качества генерации видео', 'desc': 'VBench представляет собой комплексный набор инструментов для оценки качества генерации видео. Он разбивает качество на 16 иерархических и независимых измерений, каждое с собственными методами оценки. VBench включает набор данных с аннотациями человеческих предпочтений для проверки соответствия метрик человеческому восприятию. Бенчмарк предоставляет ценные insights о возможностях современных моделей и поддерживает оценку генерации видео как из текста, так и из изображений.'}, 'en': {'title': 'VBench: A Comprehensive Benchmark for Evaluating Video Generation Models', 'desc': 'This paper introduces VBench, a new evaluation benchmark for video generation models that addresses the shortcomings of existing metrics. VBench breaks down video generation quality into 16 specific dimensions, such as motion smoothness and subject identity inconsistency, allowing for a detailed analysis of model performance. It includes human preference annotations to ensure that the evaluation aligns with human perceptions, providing valuable insights into the strengths and weaknesses of different models. Additionally, VBench++ supports various types of video generation, including text-to-video and image-to-video, and is fully open-sourced to encourage ongoing improvements in the field.'}, 'zh': {'title': 'VBench：视频生成评估的新标准', 'desc': '视频生成技术取得了显著进展，但评估这些模型仍然是一个挑战。为了解决这个问题，我们提出了VBench，一个全面的评估基准，能够将“视频生成质量”分解为具体的、分层的和解耦的维度。VBench包含16个视频生成维度，并提供与人类感知一致的评估方法，帮助识别模型的优缺点。通过全面的评估，VBench不仅评估技术质量，还关注视频生成模型的可信度，推动视频生成领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.13281', 'title': 'VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation', 'url': 'https://huggingface.co/papers/2411.13281', 'abstract': "Large multimodal models (LMMs) with advanced video analysis capabilities have recently garnered significant attention. However, most evaluations rely on traditional methods like multiple-choice questions in benchmarks such as VideoMME and LongVideoBench, which are prone to lack the depth needed to capture the complex demands of real-world users. To address this limitation-and due to the prohibitive cost and slow pace of human annotation for video tasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS Chatbot Arena's framework, designed to automatically assess LMMs' video analysis abilities. VideoAutoArena utilizes user simulation to generate open-ended, adaptive questions that rigorously assess model performance in video understanding. The benchmark features an automated, scalable evaluation framework, incorporating a modified ELO Rating System for fair and continuous comparisons across multiple LMMs. To validate our automated judging system, we construct a 'gold standard' using a carefully curated subset of human annotations, demonstrating that our arena strongly aligns with human judgment while maintaining scalability. Additionally, we introduce a fault-driven evolution strategy, progressively increasing question complexity to push models toward handling more challenging video analysis scenarios. Experimental results demonstrate that VideoAutoArena effectively differentiates among state-of-the-art LMMs, providing insights into model strengths and areas for improvement. To further streamline our evaluation, we introduce VideoAutoBench as an auxiliary benchmark, where human annotators label winners in a subset of VideoAutoArena battles. We use GPT-4o as a judge to compare responses against these human-validated answers. Together, VideoAutoArena and VideoAutoBench offer a cost-effective, and scalable framework for evaluating LMMs in user-centric video analysis.", 'score': 15, 'issue_id': 702, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'e8e997ff5dc00260', 'authors': ['Ziyang Luo', 'Haoning Wu', 'Dongxu Li', 'Jing Ma', 'Mohan Kankanhalli', 'Junnan Li'], 'affiliations': ['Hong Kong Baptist University', 'National University of Singapore', 'Rhymes AI'], 'pdf_title_img': 'assets/pdf/title_img/2411.13281.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#optimization', '#benchmark', '#video', '#games'], 'emoji': '🎥', 'ru': {'title': 'Революция в оценке видеоанализа: автоматизированная арена для мультимодальных моделей', 'desc': 'Статья представляет VideoAutoArena - новый подход к оценке мультимодальных моделей в анализе видео. В отличие от традиционных методов, VideoAutoArena использует симуляцию пользователей для генерации адаптивных вопросов, что позволяет более глубоко оценить способности моделей. Система включает автоматизированную оценку с использованием модифицированной системы рейтинга ELO для сравнения различных мультимодальных моделей. Дополнительно представлен VideoAutoBench - вспомогательный бенчмарк, использующий GPT-4 для сравнения ответов моделей с ответами, валидированными людьми.'}, 'en': {'title': 'Revolutionizing Video Analysis Evaluation with VideoAutoArena', 'desc': 'This paper presents VideoAutoArena, a new benchmark for evaluating large multimodal models (LMMs) in video analysis. Unlike traditional evaluation methods that use multiple-choice questions, VideoAutoArena employs user simulation to create open-ended, adaptive questions that better reflect real-world user needs. The framework includes an automated judging system based on a modified ELO Rating System, allowing for continuous and fair comparisons among various LMMs. Additionally, the paper introduces VideoAutoBench, which incorporates human annotations to validate the automated assessments, ensuring that the evaluation process is both scalable and aligned with human judgment.'}, 'zh': {'title': 'VideoAutoArena：视频分析的新评估标准', 'desc': '本文介绍了一种新的视频分析基准测试工具，名为VideoAutoArena，旨在评估大型多模态模型（LMMs）的能力。与传统的多项选择题评估方法不同，VideoAutoArena通过用户模拟生成开放式和自适应的问题，以更深入地测试模型在视频理解方面的表现。该基准采用自动化的评估框架，并结合修改后的ELO评分系统，实现对多个LMMs的公平比较。实验结果表明，VideoAutoArena能够有效区分不同的LMMs，并提供有关模型优缺点的见解。'}}}, {'id': 'https://huggingface.co/papers/2411.11922', 'title': 'SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory', 'url': 'https://huggingface.co/papers/2411.11922', 'abstract': 'The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of memories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incorporating temporal motion cues with the proposed motion-aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, showcasing its ability to generalize without fine-tuning. In evaluations, SAMURAI achieves significant improvements in success rate and precision over existing trackers, with a 7.1% AUC gain on LaSOT_{ext} and a 3.5% AO gain on GOT-10k. Moreover, it achieves competitive results compared to fully supervised methods on LaSOT, underscoring its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments. Code and results are available at https://github.com/yangchris11/samurai.', 'score': 12, 'issue_id': 697, 'pub_date': '2024-11-18', 'pub_date_card': {'ru': '18 ноября', 'en': 'November 18', 'zh': '11月18日'}, 'hash': '8b3456a33d980c50', 'authors': ['Cheng-Yen Yang', 'Hsiang-Wei Huang', 'Wenhao Chai', 'Zhongyu Jiang', 'Jenq-Neng Hwang'], 'affiliations': ['University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.11922.jpg', 'data': {'categories': ['#transfer_learning', '#video', '#benchmark', '#optimization', '#cv'], 'emoji': '🥷', 'ru': {'title': 'SAMURAI: Точное отслеживание объектов без дополнительного обучения', 'desc': 'SAMURAI - это улучшенная версия модели SAM 2, разработанная специально для визуального отслеживания объектов. Она использует временные сигналы движения и механизм выбора памяти с учетом движения для эффективного предсказания движения объектов и уточнения выбора масок. SAMURAI работает в режиме реального времени и демонстрирует высокую производительность в режиме zero-shot на различных наборах данных. Модель достигает значительных улучшений в показателях успешности и точности по сравнению с существующими трекерами, не требуя дополнительного обучения или тонкой настройки.'}, 'en': {'title': 'SAMURAI: Real-Time Tracking with Motion-Aware Memory', 'desc': 'The Segment Anything Model 2 (SAM 2) excels in object segmentation but struggles with visual object tracking in crowded and dynamic scenes. This paper presents SAMURAI, an improved version of SAM 2 that integrates temporal motion cues and a motion-aware memory selection mechanism to enhance tracking accuracy. SAMURAI effectively predicts object motion and refines mask selection, allowing it to operate in real-time without the need for retraining. The model shows significant performance improvements over existing trackers, achieving higher success rates and precision on various benchmark datasets, making it suitable for real-world applications.'}, 'zh': {'title': 'SAMURAI：实时视觉目标跟踪的新突破', 'desc': 'SAMURAI是对Segment Anything Model 2（SAM 2）的增强版本，专门用于视觉目标跟踪。它通过引入时间运动线索和运动感知记忆选择机制，解决了在拥挤场景中快速移动或自遮挡物体的跟踪挑战。与原始模型的固定窗口记忆方法不同，SAMURAI能够有效预测物体运动并优化掩膜选择，从而实现实时、准确的跟踪。评估结果显示，SAMURAI在多个基准数据集上表现出色，成功率和精度显著提高，展示了其在动态环境中的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2411.06559', 'title': 'Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents', 'url': 'https://huggingface.co/papers/2411.06559', 'abstract': 'Language agents have demonstrated promising capabilities in automating web-based tasks, though their current reactive approaches still underperform largely compared to humans. While incorporating advanced planning algorithms, particularly tree search methods, could enhance these agents\' performance, implementing tree search directly on live websites poses significant safety risks and practical constraints due to irreversible actions such as confirming a purchase. In this paper, we introduce a novel paradigm that augments language agents with model-based planning, pioneering the innovative use of large language models (LLMs) as world models in complex web environments. Our method, WebDreamer, builds on the key insight that LLMs inherently encode comprehensive knowledge about website structures and functionalities. Specifically, WebDreamer uses LLMs to simulate outcomes for each candidate action (e.g., "what would happen if I click this button?") using natural language descriptions, and then evaluates these imagined outcomes to determine the optimal action at each step. Empirical results on two representative web agent benchmarks with online interaction -- VisualWebArena and Mind2Web-live -- demonstrate that WebDreamer achieves substantial improvements over reactive baselines. By establishing the viability of LLMs as world models in web environments, this work lays the groundwork for a paradigm shift in automated web interaction. More broadly, our findings open exciting new avenues for future research into 1) optimizing LLMs specifically for world modeling in complex, dynamic environments, and 2) model-based speculative planning for language agents.', 'score': 9, 'issue_id': 698, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 ноября', 'en': 'November 10', 'zh': '11月10日'}, 'hash': '6ce3ac73a0235409', 'authors': ['Yu Gu', 'Boyuan Zheng', 'Boyu Gou', 'Kai Zhang', 'Cheng Chang', 'Sanjari Srivastava', 'Yanan Xie', 'Peng Qi', 'Huan Sun', 'Yu Su'], 'affiliations': ['Orby AI', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2411.06559.jpg', 'data': {'categories': ['#architecture', '#agents', '#optimization', '#benchmark'], 'emoji': '🌐', 'ru': {'title': 'WebDreamer: LLM как модель мира для веб-агентов', 'desc': 'Статья представляет новый подход WebDreamer для улучшения работы языковых агентов при выполнении веб-задач. Метод использует большие языковые модели (LLM) в качестве моделей мира для симуляции результатов действий агента на веб-сайтах. WebDreamer применяет планирование на основе модели, оценивая воображаемые исходы для выбора оптимальных действий. Эксперименты показали значительное улучшение производительности по сравнению с реактивными базовыми линиями на двух эталонных наборах данных.'}, 'en': {'title': 'Revolutionizing Web Agents with Model-Based Planning', 'desc': "This paper presents a new approach called WebDreamer that enhances language agents' ability to perform web-based tasks by integrating model-based planning with large language models (LLMs). Unlike traditional reactive methods, WebDreamer allows agents to simulate potential outcomes of actions in a web environment, helping them to make better decisions. By leveraging the inherent knowledge of LLMs about website structures, the agents can evaluate different actions and choose the most effective one. The results show that WebDreamer significantly outperforms existing reactive methods in two benchmark tests, indicating a promising direction for future research in automated web interaction."}, 'zh': {'title': '用模型驱动规划提升语言代理的能力', 'desc': '本文介绍了一种新颖的范式，通过模型驱动的规划增强语言代理的能力，特别是在复杂的网络环境中。我们提出的WebDreamer方法利用大型语言模型（LLMs）作为世界模型，模拟每个候选动作的结果，并评估这些结果以确定最佳行动。实验结果表明，WebDreamer在两个代表性的网络代理基准测试中显著优于传统的反应式方法。此研究为自动化网络交互的范式转变奠定了基础，并为未来的研究开辟了新的方向。'}}}, {'id': 'https://huggingface.co/papers/2411.13476', 'title': 'When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training', 'url': 'https://huggingface.co/papers/2411.13476', 'abstract': "Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16's limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\% compared to standard full attention mechanisms, while preserving the original LLM's capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.", 'score': 6, 'issue_id': 704, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'e463af90b96a6aa1', 'authors': ['Haonan Wang', 'Qian Liu', 'Chao Du', 'Tongyao Zhu', 'Cunxiao Du', 'Kenji Kawaguchi', 'Tianyu Pang'], 'affiliations': ['National University of Singapore', 'SSea AI Lab, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.13476.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#long_context'], 'emoji': '⚓', 'ru': {'title': 'AnchorAttention: Эффективное решение для обработки длинных контекстов в LLM', 'desc': 'Статья представляет новый метод внимания под названием AnchorAttention для улучшения обработки длинных контекстов в больших языковых моделях (LLM). Авторы обнаружили численные проблемы при использовании Rotary Positional Embedding (RoPE) с форматом BFloat16, особенно в сценариях с длинным контекстом. AnchorAttention решает эту проблему, рассматривая первый токен как общий якорь с постоянным идентификатором позиции. Эксперименты показали, что AnchorAttention значительно улучшает производительность при работе с длинными контекстами и сокращает время обучения более чем на 50% по сравнению со стандартными механизмами полного внимания.'}, 'en': {'title': 'AnchorAttention: Enhancing Long-Context Performance in LLMs', 'desc': "This paper introduces AnchorAttention, a new attention mechanism designed to improve the performance of large language models (LLMs) when processing long sequences. The authors identify that using Rotary Positional Embedding (RoPE) with BFloat16 format leads to numerical issues, particularly affecting relative positional encoding in long-context scenarios. AnchorAttention addresses these issues by treating the first token as a shared anchor, which enhances computational efficiency and maintains semantic coherence. Experimental results show that this method significantly boosts long-context capabilities and reduces training time by over 50% without compromising the model's overall performance."}, 'zh': {'title': 'AnchorAttention：提升长上下文处理能力的创新方法', 'desc': '本文提出了一种新的注意力机制，称为AnchorAttention，旨在解决使用BFloat16格式时的数值问题。传统的旋转位置嵌入（RoPE）在长上下文训练中表现良好，但在BFloat16格式下会出现精度不足的问题，影响相对位置编码的效果。AnchorAttention通过将第一个token视为共享锚点，减少了不必要的注意力计算，提高了长上下文处理能力，并加快了训练速度。实验表明，AnchorAttention在长上下文性能上显著优于标准的全注意力机制，同时保持了原有大语言模型在一般任务上的能力。'}}}, {'id': 'https://huggingface.co/papers/2411.12811', 'title': 'Stylecodes: Encoding Stylistic Information For Image Generation', 'url': 'https://huggingface.co/papers/2411.12811', 'abstract': 'Diffusion models excel in image generation, but controlling them remains a challenge. We focus on the problem of style-conditioned image generation. Although example images work, they are cumbersome: srefs (style-reference codes) from MidJourney solve this issue by expressing a specific image style in a short numeric code. These have seen widespread adoption throughout social media due to both their ease of sharing and the fact they allow using an image for style control, without having to post the source images themselves. However, users are not able to generate srefs from their own images, nor is the underlying training procedure public. We propose StyleCodes: an open-source and open-research style encoder architecture and training procedure to express image style as a 20-symbol base64 code. Our experiments show that our encoding results in minimal loss in quality compared to traditional image-to-style techniques.', 'score': 5, 'issue_id': 706, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 ноября', 'en': 'November 19', 'zh': '11月19日'}, 'hash': '55d7513a0c99c789', 'authors': ['Ciara Rowles'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.12811.jpg', 'data': {'categories': ['#cv', '#architecture', '#open_source', '#diffusion', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'StyleCodes: открытый способ кодирования стиля изображений', 'desc': 'Эта статья посвящена проблеме генерации изображений с контролируемым стилем в диффузионных моделях. Авторы предлагают архитектуру StyleCodes - энкодер стиля, который преобразует изображение в короткий числовой код из 20 символов. Этот подход позволяет легко делиться и применять стили изображений без необходимости публиковать сами исходные изображения. Эксперименты показывают, что качество генерации с использованием StyleCodes сопоставимо с традиционными методами передачи стиля от изображения к изображению.'}, 'en': {'title': 'Unlocking Style Control in Image Generation with StyleCodes', 'desc': 'This paper addresses the challenge of controlling diffusion models for style-conditioned image generation. It introduces StyleCodes, an open-source architecture that encodes image styles into a compact 20-symbol base64 code, making it easier for users to generate styles from their own images. The proposed method allows for efficient style sharing without the need to post original images, enhancing usability in social media contexts. Experimental results indicate that StyleCodes maintain high image quality, comparable to traditional methods of style encoding.'}, 'zh': {'title': '风格编码，轻松生成图像风格！', 'desc': '扩散模型在图像生成方面表现出色，但控制这些模型仍然是一个挑战。我们关注风格条件的图像生成问题，提出了一种名为StyleCodes的开放源代码风格编码器架构。StyleCodes能够将图像风格表达为一个20个符号的base64代码，用户可以方便地生成自己的风格参考代码。我们的实验表明，与传统的图像到风格技术相比，这种编码方法在质量上几乎没有损失。'}}}, {'id': 'https://huggingface.co/papers/2411.12925', 'title': 'Loss-to-Loss Prediction: Scaling Laws for All Datasets', 'url': 'https://huggingface.co/papers/2411.12925', 'abstract': 'While scaling laws provide a reliable methodology for predicting train loss across compute scales for a single data distribution, less is known about how these predictions should change as we change the distribution. In this paper, we derive a strategy for predicting one loss from another and apply it to predict across different pre-training datasets and from pre-training data to downstream task data. Our predictions extrapolate well even at 20x the largest FLOP budget used to fit the curves. More precisely, we find that there are simple shifted power law relationships between (1) the train losses of two models trained on two separate datasets when the models are paired by training compute (train-to-train), (2) the train loss and the test loss on any downstream distribution for a single model (train-to-test), and (3) the test losses of two models trained on two separate train datasets (test-to-test). The results hold up for pre-training datasets that differ substantially (some are entirely code and others have no code at all) and across a variety of downstream tasks. Finally, we find that in some settings these shifted power law relationships can yield more accurate predictions than extrapolating single-dataset scaling laws.', 'score': 2, 'issue_id': 716, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 ноября', 'en': 'November 19', 'zh': '11月19日'}, 'hash': '2c87616b49396672', 'authors': ['David Brandfonbrener', 'Nikhil Anand', 'Nikhil Vyas', 'Eran Malach', 'Sham Kakade'], 'affiliations': ['Kempner Institute, Harvard University', 'SEAS, Harvard University'], 'pdf_title_img': 'assets/pdf/title_img/2411.12925.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#data', '#training', '#dataset'], 'emoji': '📈', 'ru': {'title': 'Универсальные законы масштабирования для различных распределений данных', 'desc': 'Статья исследует, как меняются предсказания масштабируемости моделей машинного обучения при изменении распределения данных. Авторы предлагают стратегию для предсказания потерь одной модели на основе другой. Они обнаружили простые степенные зависимости между потерями моделей, обученных на разных наборах данных, а также между обучающими и тестовыми потерями. Результаты применимы для существенно различающихся наборов данных и различных задач.'}, 'en': {'title': 'Predicting Loss Across Data Distributions with Power Laws', 'desc': 'This paper explores how to predict training loss when changing data distributions in machine learning models. It introduces a method to derive relationships between losses from different datasets, allowing for effective predictions across pre-training and downstream tasks. The authors discover that simple shifted power law relationships exist between train losses of models trained on different datasets, as well as between train and test losses. Their findings suggest that these relationships can provide more accurate predictions than traditional scaling laws, even when datasets vary significantly.'}, 'zh': {'title': '跨数据分布的损失预测新策略', 'desc': '本文探讨了如何在不同数据分布下预测训练损失。我们提出了一种策略，可以从一个损失预测另一个损失，适用于不同的预训练数据集和下游任务数据。研究发现，训练损失之间、训练损失与测试损失之间、以及测试损失之间存在简单的偏移幂律关系。我们的结果表明，在某些情况下，这些偏移幂律关系的预测精度优于单一数据集的扩展法则。'}}}, {'id': 'https://huggingface.co/papers/2411.13025', 'title': 'ORID: Organ-Regional Information Driven Framework for Radiology Report Generation', 'url': 'https://huggingface.co/papers/2411.13025', 'abstract': 'The objective of Radiology Report Generation (RRG) is to automatically generate coherent textual analyses of diseases based on radiological images, thereby alleviating the workload of radiologists. Current AI-based methods for RRG primarily focus on modifications to the encoder-decoder model architecture. To advance these approaches, this paper introduces an Organ-Regional Information Driven (ORID) framework which can effectively integrate multi-modal information and reduce the influence of noise from unrelated organs. Specifically, based on the LLaVA-Med, we first construct an RRG-related instruction dataset to improve organ-regional diagnosis description ability and get the LLaVA-Med-RRG. After that, we propose an organ-based cross-modal fusion module to effectively combine the information from the organ-regional diagnosis description and radiology image. To further reduce the influence of noise from unrelated organs on the radiology report generation, we introduce an organ importance coefficient analysis module, which leverages Graph Neural Network (GNN) to examine the interconnections of the cross-modal information of each organ region. Extensive experiments an1d comparisons with state-of-the-art methods across various evaluation metrics demonstrate the superior performance of our proposed method.', 'score': 2, 'issue_id': 702, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'e62215e28a0bdfdb', 'authors': ['Tiancheng Gu', 'Kaicheng Yang', 'Xiang An', 'Ziyong Feng', 'Dongnan Liu', 'Weidong Cai'], 'affiliations': ['DeepGlint', 'University of Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2411.13025.jpg', 'data': {'categories': ['#multimodal', '#graphs', '#dataset', '#games', '#architecture'], 'emoji': '🏥', 'ru': {'title': 'Умная генерация радиологических отчетов с фокусом на органы', 'desc': 'Статья представляет новый подход к автоматической генерации радиологических отчетов, называемый ORID (Organ-Regional Information Driven). Авторы улучшают модель LLaVA-Med, создавая специализированный датасет инструкций для описания органов. Они также предлагают модуль кросс-модального слияния информации об органах и модуль анализа важности органов на основе графовых нейронных сетей. Эксперименты показывают превосходство предложенного метода над существующими подходами.'}, 'en': {'title': 'Enhancing Radiology Reports with Organ-Regional Insights', 'desc': 'This paper presents a novel framework called Organ-Regional Information Driven (ORID) for improving Radiology Report Generation (RRG) by effectively integrating multi-modal data from radiological images and textual descriptions. The approach enhances the encoder-decoder model by introducing an organ-based cross-modal fusion module, which combines relevant information while minimizing noise from unrelated organs. Additionally, it employs a Graph Neural Network (GNN) to analyze the importance of different organ regions, ensuring that only the most relevant data influences the report generation. Experimental results show that the ORID framework outperforms existing state-of-the-art methods in generating coherent and accurate radiology reports.'}, 'zh': {'title': '提升放射学报告生成的智能化水平', 'desc': '放射学报告生成（RRG）的目标是根据放射图像自动生成连贯的疾病分析文本，从而减轻放射科医生的工作负担。目前基于人工智能的RRG方法主要集中在对编码器-解码器模型架构的修改。本文提出了一种有机区域信息驱动（ORID）框架，能够有效整合多模态信息，并减少来自无关器官的噪声影响。通过构建与RRG相关的指令数据集和引入基于器官的跨模态融合模块，我们的方法在各种评估指标上表现出优越的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.10867', 'title': 'ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models', 'url': 'https://huggingface.co/papers/2411.10867', 'abstract': 'Latest developments in Large Multimodal Models (LMMs) have broadened their capabilities to include video understanding. Specifically, Text-to-video (T2V) models have made significant progress in quality, comprehension, and duration, excelling at creating videos from simple textual prompts. Yet, they still frequently produce hallucinated content that clearly signals the video is AI-generated. We introduce ViBe: a large-scale Text-to-Video Benchmark of hallucinated videos from T2V models. We identify five major types of hallucination: Vanishing Subject, Numeric Variability, Temporal Dysmorphia, Omission Error, and Physical Incongruity. Using 10 open-source T2V models, we developed the first large-scale dataset of hallucinated videos, comprising 3,782 videos annotated by humans into these five categories. ViBe offers a unique resource for evaluating the reliability of T2V models and provides a foundation for improving hallucination detection and mitigation in video generation. We establish classification as a baseline and present various ensemble classifier configurations, with the TimeSFormer + CNN combination yielding the best performance, achieving 0.345 accuracy and 0.342 F1 score. This benchmark aims to drive the development of robust T2V models that produce videos more accurately aligned with input prompts.', 'score': 1, 'issue_id': 715, 'pub_date': '2024-11-16', 'pub_date_card': {'ru': '16 ноября', 'en': 'November 16', 'zh': '11月16日'}, 'hash': 'be2a7eb4c0e51788', 'authors': ['Vipula Rawte', 'Sarthak Jain', 'Aarush Sinha', 'Garv Kaushik', 'Aman Bansal', 'Prathiksha Rumale Vishwanath', 'Samyak Rajesh Jain', 'Aishwarya Naresh Reganti', 'Vinija Jain', 'Aman Chadha', 'Amit P. Sheth', 'Amitava Das'], 'affiliations': ['AI Institute, University of South Carolina, USA', 'Amazon GenAI, USA', 'Amazon Web Services, USA', 'Guru Gobind Singh Indraprastha University, India', 'Indian Institute of Technology (BHU), India', 'Meta, USA', 'University of California, Santa Cruz, USA', 'University of Massachusetts Amherst, USA', 'Vellore Institute of Technology, India'], 'pdf_title_img': 'assets/pdf/title_img/2411.10867.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#video', '#hallucinations', '#open_source', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'ViBe: новый стандарт для борьбы с галлюцинациями в генерации видео', 'desc': 'В статье представлен ViBe - крупномасштабный бенчмарк для оценки галлюцинаций в моделях преобразования текста в видео (T2V). Авторы выделили пять основных типов галлюцинаций и создали датасет из 3782 видео, размеченных людьми. Бенчмарк предназначен для оценки надежности T2V моделей и улучшения обнаружения галлюцинаций. В качестве базового решения предложена классификация с использованием ансамбля моделей, где комбинация TimeSFormer + CNN показала лучший результат.'}, 'en': {'title': 'ViBe: Benchmarking Hallucinations in Text-to-Video Models', 'desc': 'This paper presents ViBe, a benchmark designed to evaluate hallucinations in videos generated by Text-to-Video (T2V) models. It identifies five types of hallucinations that can occur in AI-generated videos, such as Vanishing Subject and Temporal Dysmorphia. The authors created a large dataset of 3,782 annotated videos from various T2V models to facilitate this evaluation. By establishing a baseline for classification and testing different ensemble classifiers, they aim to enhance the reliability of T2V models and reduce the occurrence of hallucinated content.'}, 'zh': {'title': '提升文本到视频模型的可靠性', 'desc': '本文介绍了ViBe，一个针对文本到视频（T2V）模型生成的幻觉视频的大规模基准。我们识别了五种主要的幻觉类型，包括消失主体、数值变异、时间畸形、遗漏错误和物理不一致。通过使用10个开源T2V模型，我们创建了第一个包含3782个幻觉视频的大规模数据集，并对其进行了人工标注。ViBe为评估T2V模型的可靠性提供了独特的资源，并为改善幻觉检测和缓解奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2411.08147', 'title': 'Large Language Models Can Self-Improve in Long-context Reasoning', 'url': 'https://huggingface.co/papers/2411.08147', 'abstract': 'Large language models (LLMs) have achieved substantial progress in processing long contexts but still struggle with long-context reasoning. Existing approaches typically involve fine-tuning LLMs with synthetic data, which depends on annotations from human experts or advanced models like GPT-4, thus restricting further advancements. To address this issue, we investigate the potential for LLMs to self-improve in long-context reasoning and propose SEALONG, an approach specifically designed for this purpose. This approach is straightforward: we sample multiple outputs for each question, score them with Minimum Bayes Risk, and then apply supervised fine-tuning or preference optimization based on these outputs. Extensive experiments on several leading LLMs demonstrate the effectiveness of SEALONG, with an absolute improvement of 4.2 points for Llama-3.1-8B-Instruct. Furthermore, SEALONG achieves superior performance compared to prior approaches that depend on data produced by human experts or advanced models. We anticipate that this work will open new avenues for self-improvement techniques in long-context scenarios, which are essential for the continual advancement of LLMs.', 'score': 47, 'issue_id': 567, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': 'ea4232d9ddd5ef31', 'authors': ['Siheng Li', 'Cheng Yang', 'Zesen Cheng', 'Lemao Liu', 'Mo Yu', 'Yujiu Yang', 'Wai Lam'], 'affiliations': ['The Chinese University of Hong Kong', 'Peking University', 'Tsinghua University', 'Tencent'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08147.jpg', 'data': {'categories': ['#training', '#synthetic', '#long_context', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Самообучение LLMs для длинных контекстов', 'desc': 'В статье рассматривается проблема улучшения способности больших языковых моделей (LLMs) к рассуждению в длинных контекстах. Авторы предлагают метод SEALONG, который позволяет моделям самостоятельно улучшаться без необходимости в данных от экспертов или продвинутых моделей. Метод заключается в генерации нескольких ответов на вопрос, их оценке с помощью минимального байесовского риска и последующей оптимизации. Эксперименты показывают, что SEALONG значительно улучшает производительность моделей, таких как Llama-3.1-8B-Instruct.'}, 'en': {'title': 'Empowering LLMs to Self-Improve Long-Context Reasoning', 'desc': 'This paper addresses the challenges that large language models (LLMs) face in reasoning over long contexts. The authors propose a new method called SEALONG, which allows LLMs to enhance their long-context reasoning capabilities without relying on human-annotated data. Instead, the approach involves generating multiple outputs for each question, scoring them using Minimum Bayes Risk, and then fine-tuning the model based on these scores. Experimental results show that SEALONG significantly improves performance on Llama-3.1-8B-Instruct, outperforming previous methods that depend on expert-generated data.'}, 'zh': {'title': '让大型语言模型自我提升长文本推理能力', 'desc': '大型语言模型（LLMs）在处理长文本方面取得了显著进展，但在长文本推理上仍然存在困难。现有的方法通常依赖于人工专家或先进模型（如GPT-4）提供的合成数据进行微调，这限制了进一步的发展。为了解决这个问题，我们提出了一种名为SEALONG的方法，旨在让LLMs在长文本推理中自我改进。通过对每个问题采样多个输出，并使用最小贝叶斯风险进行评分，我们可以基于这些输出进行监督微调或偏好优化，从而显著提高模型性能。'}}}, {'id': 'https://huggingface.co/papers/2411.08380', 'title': 'EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation', 'url': 'https://huggingface.co/papers/2411.08380', 'abstract': 'Video generation has emerged as a promising tool for world simulation, leveraging visual data to replicate real-world environments. Within this context, egocentric video generation, which centers on the human perspective, holds significant potential for enhancing applications in virtual reality, augmented reality, and gaming. However, the generation of egocentric videos presents substantial challenges due to the dynamic nature of egocentric viewpoints, the intricate diversity of actions, and the complex variety of scenes encountered. Existing datasets are inadequate for addressing these challenges effectively. To bridge this gap, we present EgoVid-5M, the first high-quality dataset specifically curated for egocentric video generation. EgoVid-5M encompasses 5 million egocentric video clips and is enriched with detailed action annotations, including fine-grained kinematic control and high-level textual descriptions. To ensure the integrity and usability of the dataset, we implement a sophisticated data cleaning pipeline designed to maintain frame consistency, action coherence, and motion smoothness under egocentric conditions. Furthermore, we introduce EgoDreamer, which is capable of generating egocentric videos driven simultaneously by action descriptions and kinematic control signals. The EgoVid-5M dataset, associated action annotations, and all data cleansing metadata will be released for the advancement of research in egocentric video generation.', 'score': 21, 'issue_id': 565, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '7c82b1fc5a785fe0', 'authors': ['Xiaofeng Wang', 'Kang Zhao', 'Feng Liu', 'Jiayu Wang', 'Guosheng Zhao', 'Xiaoyi Bao', 'Zheng Zhu', 'Yingya Zhang', 'Xingang Wang'], 'affiliations': ['CASIA', 'Tsinghua University', 'Alibaba', 'UCAS'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08380.jpg', 'data': {'categories': ['#data', '#synthetic', '#games', '#video', '#dataset'], 'emoji': '👀', 'ru': {'title': 'EgoVid-5M: революция в генерации эгоцентрических видео', 'desc': 'Статья представляет EgoVid-5M - первый крупномасштабный датасет для генерации эгоцентрических видео, содержащий 5 миллионов клипов с подробными аннотациями действий. Авторы разработали сложный процесс очистки данных для обеспечения целостности и удобства использования датасета. Также представлена модель EgoDreamer, способная генерировать эгоцентрические видео на основе описаний действий и кинематических сигналов управления. Датасет, аннотации и метаданные будут опубликованы для продвижения исследований в области генерации эгоцентрических видео.'}, 'en': {'title': 'EgoVid-5M: Revolutionizing Egocentric Video Generation', 'desc': 'This paper introduces EgoVid-5M, a new dataset designed for generating egocentric videos, which are videos captured from a first-person perspective. The dataset contains 5 million video clips with detailed annotations that describe the actions and movements occurring in each clip. The authors also present EgoDreamer, a model that can create egocentric videos based on both action descriptions and kinematic controls. This work aims to address the challenges of existing datasets and improve the quality and usability of egocentric video generation for applications in virtual and augmented reality.'}, 'zh': {'title': 'EgoVid-5M：自我视角视频生成的新突破', 'desc': '视频生成是一种有前景的世界模拟工具，可以利用视觉数据复制现实环境。以自我视角为中心的视频生成在虚拟现实、增强现实和游戏应用中具有重要潜力。然而，自我视角视频的生成面临着动态视角、复杂动作和多样场景的挑战。为了解决这些问题，我们提出了EgoVid-5M，这是第一个专门为自我视角视频生成而创建的高质量数据集，包含500万个视频片段和详细的动作注释。'}}}, {'id': 'https://huggingface.co/papers/2411.07618', 'title': 'Direct Preference Optimization Using Sparse Feature-Level Constraints', 'url': 'https://huggingface.co/papers/2411.07618', 'abstract': 'The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.', 'score': 13, 'issue_id': 565, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': 'b7ab0d7ebab29360', 'authors': ['Qingyu Yin', 'Chak Tou Leong', 'Hongbo Zhang', 'Minjun Zhu', 'Hanqi Yan', 'Qiang Zhang', 'Yulan He', 'Wenjie Li', 'Jun Wang', 'Yue Zhang', 'Linyi Yang'], 'affiliations': ['Westlake University', 'Zhejiang University', 'The Hong Kong Polytechnic University', 'Kings College London', 'University College London'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07618.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': '🎯', 'ru': {'title': 'Эффективное выравнивание языковых моделей с помощью разреженных признаков', 'desc': 'Статья представляет новый метод для выравнивания больших языковых моделей (LLM) с человеческими предпочтениями - Feature-level constrained Preference Optimization (FPO). FPO использует предобученные разреженные автоэнкодеры и вводит ограничения на уровне признаков для эффективного выравнивания. Метод показывает улучшение на 5.08% по сравнению с современными базовыми методами при значительно меньших вычислительных затратах. FPO предлагает перспективное решение для эффективного и контролируемого выравнивания LLM.'}, 'en': {'title': 'Efficient Alignment of Language Models with Human Preferences', 'desc': 'This paper addresses the challenge of aligning large language models (LLMs) with human preferences. It introduces a new method called Feature-level constrained Preference Optimization (FPO), which aims to improve the alignment process while maintaining stability and efficiency. FPO utilizes pre-trained Sparse Autoencoders (SAEs) and applies feature-level constraints to enhance the alignment without incurring high computational costs. Experimental results show that FPO outperforms existing methods, achieving a significant improvement in performance while being more resource-efficient.'}, 'zh': {'title': '高效稳定的语言模型对齐新方法', 'desc': '本文提出了一种新的方法，称为特征级约束偏好优化（FPO），旨在简化大型语言模型（LLM）与人类偏好的对齐过程。FPO利用预训练的稀疏自编码器（SAE）并引入特征级约束，从而实现高效且稳定的对齐。通过激活稀疏特征和使用特征级离线参考，FPO在计算成本上显著降低，同时提高了模型的性能。实验结果表明，FPO在基准数据集上相较于最先进的基线方法，赢率提高了5.08%。'}}}, {'id': 'https://huggingface.co/papers/2411.08868', 'title': 'CamemBERT 2.0: A Smarter French Language Model Aged to Perfection', 'url': 'https://huggingface.co/papers/2411.08868', 'abstract': 'French language models, such as CamemBERT, have been widely adopted across industries for natural language processing (NLP) tasks, with models like CamemBERT seeing over 4 million downloads per month. However, these models face challenges due to temporal concept drift, where outdated training data leads to a decline in performance, especially when encountering new topics and terminology. This issue emphasizes the need for updated models that reflect current linguistic trends. In this paper, we introduce two new versions of the CamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these challenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use of the Replaced Token Detection (RTD) objective for better contextual understanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked Language Modeling (MLM) objective. Both models are trained on a significantly larger and more recent dataset with longer context length and an updated tokenizer that enhances tokenization performance for French. We evaluate the performance of these models on both general-domain NLP tasks and domain-specific applications, such as medical field tasks, demonstrating their versatility and effectiveness across a range of use cases. Our results show that these updated models vastly outperform their predecessors, making them valuable tools for modern NLP systems. All our new models, as well as intermediate checkpoints, are made openly available on Huggingface.', 'score': 11, 'issue_id': 569, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '6be618c24defa5fb', 'authors': ['Wissam Antoun', 'Francis Kulumba', 'Rian Touchent', 'Éric de la Clergerie', 'Benoît Sagot', 'Djamé Seddah'], 'affiliations': ['Inria, Paris, France'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08868.jpg', 'data': {'categories': ['#training', '#multilingual', '#healthcare', '#long_context', '#architecture', '#dataset', '#low_resource', '#open_source'], 'emoji': '🇫🇷', 'ru': {'title': 'Обновленные французские языковые модели для современных задач NLP', 'desc': 'В статье представлены две новые версии французской языковой модели CamemBERT - CamemBERTav2 и CamemBERTv2. Эти модели призваны решить проблему временного дрейфа концепций, из-за которого устаревшие данные для обучения приводят к снижению производительности. CamemBERTav2 основана на архитектуре DeBERTaV3 и использует метод Replaced Token Detection, а CamemBERTv2 построена на RoBERTa с применением Masked Language Modeling. Обе модели обучены на более обширном и современном наборе данных с увеличенной длиной контекста и улучшенным токенизатором для французского языка.'}, 'en': {'title': 'Revamping CamemBERT for Modern NLP Challenges', 'desc': 'This paper presents two updated versions of the French language model CamemBERT, named CamemBERTav2 and CamemBERTv2, to tackle the issue of temporal concept drift in natural language processing. CamemBERTav2 utilizes the DeBERTaV3 architecture with a Replaced Token Detection objective, while CamemBERTv2 is based on RoBERTa and employs Masked Language Modeling. Both models are trained on a larger, more recent dataset, improving their contextual understanding and tokenization performance for French. The evaluation shows that these models significantly outperform previous versions in various NLP tasks, including general and domain-specific applications.'}, 'zh': {'title': '更新模型，提升法语NLP性能', 'desc': '本文介绍了两种新的法语语言模型CamemBERTav2和CamemBERTv2，旨在解决由于过时训练数据导致的性能下降问题。这些模型基于DeBERTaV3和RoBERTa架构，分别采用了替换标记检测（RTD）和掩蔽语言建模（MLM）目标，以提高上下文理解能力。它们在更大且更新的数据集上进行训练，具有更长的上下文长度和改进的分词器，提升了法语的分词性能。实验结果表明，这些更新的模型在通用和特定领域的自然语言处理任务中表现优异，成为现代NLP系统的重要工具。'}}}, {'id': 'https://huggingface.co/papers/2411.08307', 'title': 'PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation', 'url': 'https://huggingface.co/papers/2411.08307', 'abstract': 'Music generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains a significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io.', 'score': 6, 'issue_id': 575, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '1f1b5e7081062b6f', 'authors': ['Yungang Yi', 'Weihua Li', 'Matthew Kuo', 'Quan Bai'], 'affiliations': ['Auckland University of Technology, Auckland, New Zealand', 'University of Tasmania, Tasmania, Australia'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08307.jpg', 'data': {'categories': ['#audio', '#architecture', '#story_generation'], 'emoji': '🎼', 'ru': {'title': 'PerceiverS: Новый подход к генерации структурированной и выразительной музыки', 'desc': 'Статья представляет PerceiverS - новую архитектуру для генерации символической музыки. Модель использует механизмы эффективной сегментации и многомасштабного внимания для улучшения структуры и выразительности генерируемой музыки. PerceiverS сочетает кросс-внимание и самовнимание для захвата как долгосрочных музыкальных структур, так и кратковременных нюансов исполнения. Эксперименты на наборах данных вроде Maestro показывают улучшение в генерации связной и разнообразной музыки.'}, 'en': {'title': 'Enhancing Symbolic Music Generation with PerceiverS', 'desc': 'This paper introduces PerceiverS, a new architecture aimed at improving symbolic music generation. It utilizes Effective Segmentation and Multi-Scale attention mechanisms to learn both long-term structures and short-term expressive details in music. By integrating cross-attention and self-attention, the model effectively captures the overall musical structure while maintaining nuanced performance elements. Evaluations on datasets like Maestro show that PerceiverS generates more coherent and diverse music, achieving better structural consistency and expressive variation.'}, 'zh': {'title': 'PerceiverS：生成富有表现力的符号音乐新方法', 'desc': '本文提出了一种新的音乐生成架构，称为PerceiverS（分段与尺度），旨在解决生成长结构和富有表现力的符号音乐的挑战。该模型结合了有效的分段和多尺度注意力机制，能够同时学习长期结构依赖和短期表现细节。通过在多尺度设置中结合交叉注意力和自注意力，PerceiverS能够捕捉长距离的音乐结构，同时保留演奏的细微差别。实验结果表明，该模型在生成连贯且多样化的音乐方面表现出色，具有结构一致性和表现变化。'}}}, {'id': 'https://huggingface.co/papers/2411.08790', 'title': 'Can sparse autoencoders be used to decompose and interpret steering vectors?', 'url': 'https://huggingface.co/papers/2411.08790', 'abstract': 'Steering vectors are a promising approach to control the behaviour of large language models. However, their underlying mechanisms remain poorly understood. While sparse autoencoders (SAEs) may offer a potential method to interpret steering vectors, recent findings show that SAE-reconstructed vectors often lack the steering properties of the original vectors. This paper investigates why directly applying SAEs to steering vectors yields misleading decompositions, identifying two reasons: (1) steering vectors fall outside the input distribution for which SAEs are designed, and (2) steering vectors can have meaningful negative projections in feature directions, which SAEs are not designed to accommodate. These limitations hinder the direct use of SAEs for interpreting steering vectors.', 'score': 6, 'issue_id': 570, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '68e55764db274419', 'authors': ['Harry Mayne', 'Yushi Yang', 'Adam Mahdi'], 'affiliations': ['University of Oxford'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08790.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#training'], 'emoji': '🧭', 'ru': {'title': 'Ограничения автоэнкодеров в расшифровке векторов управления ИИ', 'desc': 'Статья исследует проблемы применения разреженных автоэнкодеров (SAE) для интерпретации векторов управления в больших языковых моделях. Авторы выявляют два ключевых ограничения: несоответствие векторов управления входному распределению SAE и наличие значимых отрицательных проекций. Эти факторы препятствуют прямому использованию SAE для анализа векторов управления. Исследование подчеркивает необходимость разработки новых методов для понимания механизмов работы векторов управления в языковых моделях.'}, 'en': {'title': 'Understanding Steering Vectors: Limitations of Sparse Autoencoders', 'desc': 'This paper explores the use of steering vectors in controlling large language models and highlights the challenges in understanding their mechanisms. It specifically examines the limitations of sparse autoencoders (SAEs) when applied to these steering vectors. The authors identify that steering vectors do not fit the input distribution that SAEs are designed for, and they can have significant negative projections that SAEs cannot handle. These findings suggest that using SAEs for interpreting steering vectors may lead to inaccurate results, indicating a need for alternative methods.'}, 'zh': {'title': '揭示引导向量的奥秘', 'desc': '本论文探讨了引导向量在控制大型语言模型行为中的应用潜力，但其机制尚不清楚。研究发现，稀疏自编码器（SAE）在解释引导向量时存在问题，重构的向量往往缺乏原始向量的引导特性。论文指出，直接应用SAE于引导向量会导致误导性的分解，原因包括引导向量超出了SAE设计的输入分布，以及引导向量在特征方向上可能具有有意义的负投影。由于这些限制，SAE在解释引导向量时的直接应用受到阻碍。'}}}, {'id': 'https://huggingface.co/papers/2411.08328', 'title': 'Motion Control for Enhanced Complex Action Video Generation', 'url': 'https://huggingface.co/papers/2411.08328', 'abstract': "Existing text-to-video (T2V) models often struggle with generating videos with sufficiently pronounced or complex actions. A key limitation lies in the text prompt's inability to precisely convey intricate motion details. To address this, we propose a novel framework, MVideo, designed to produce long-duration videos with precise, fluid actions. MVideo overcomes the limitations of text prompts by incorporating mask sequences as an additional motion condition input, providing a clearer, more accurate representation of intended actions. Leveraging foundational vision models such as GroundingDINO and SAM2, MVideo automatically generates mask sequences, enhancing both efficiency and robustness. Our results demonstrate that, after training, MVideo effectively aligns text prompts with motion conditions to produce videos that simultaneously meet both criteria. This dual control mechanism allows for more dynamic video generation by enabling alterations to either the text prompt or motion condition independently, or both in tandem. Furthermore, MVideo supports motion condition editing and composition, facilitating the generation of videos with more complex actions. MVideo thus advances T2V motion generation, setting a strong benchmark for improved action depiction in current video diffusion models. Our project page is available at https://mvideo-v1.github.io/.", 'score': 2, 'issue_id': 575, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': 'ff99307fadb40120', 'authors': ['Qiang Zhou', 'Shaofeng Zhang', 'Nianzu Yang', 'Ye Qian', 'Hao Li'], 'affiliations': ['INF Tech', 'Shanghai Jiao Tong University', 'Fudan University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08328.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#video', '#benchmark', '#games'], 'emoji': '🎬', 'ru': {'title': 'Точные движения в генерируемых видео с помощью масок', 'desc': 'MVideo - это новая система для генерации видео по текстовому описанию, которая решает проблему недостаточно выраженных или сложных действий в существующих моделях. Она использует последовательности масок в качестве дополнительного входа для более точного представления движений. MVideo автоматически генерирует эти маски с помощью моделей компьютерного зрения, что повышает эффективность и надежность. Система позволяет независимо изменять текстовое описание и условия движения, а также поддерживает редактирование и композицию условий движения.'}, 'en': {'title': 'MVideo: Revolutionizing Text-to-Video with Precise Motion Control', 'desc': 'The paper introduces MVideo, a new framework for text-to-video (T2V) generation that enhances the depiction of complex actions in videos. It addresses the limitations of traditional text prompts by using mask sequences as an additional input, which provides clearer motion details. MVideo utilizes advanced vision models to automatically create these mask sequences, improving the efficiency and accuracy of video generation. The framework allows for independent or combined adjustments to text prompts and motion conditions, enabling the creation of more dynamic and intricate videos.'}, 'zh': {'title': 'MVideo：提升文本到视频生成的动态性与精确性', 'desc': '现有的文本到视频（T2V）模型在生成复杂动作的视频时常常面临挑战。主要问题在于文本提示无法准确传达复杂的运动细节。为了解决这个问题，我们提出了一种新框架MVideo，旨在生成具有精确流畅动作的长时视频。MVideo通过引入掩码序列作为额外的运动条件输入，克服了文本提示的局限性，从而实现更动态的视频生成。'}}}, {'id': 'https://huggingface.co/papers/2411.09595', 'title': 'LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models', 'url': 'https://huggingface.co/papers/2411.09595', 'abstract': 'This work explores expanding the capabilities of large language models (LLMs) pretrained on text to generate 3D meshes within a unified model. This offers key advantages of (1) leveraging spatial knowledge already embedded in LLMs, derived from textual sources like 3D tutorials, and (2) enabling conversational 3D generation and mesh understanding. A primary challenge is effectively tokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly. To address this, we introduce LLaMA-Mesh, a novel approach that represents the vertex coordinates and face definitions of 3D meshes as plain text, allowing direct integration with LLMs without expanding the vocabulary. We construct a supervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate 3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs as required, and (3) understand and interpret 3D meshes. Our work is the first to demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge for 3D mesh generation in a text-based format, effectively unifying the 3D and text modalities. LLaMA-Mesh achieves mesh generation quality on par with models trained from scratch while maintaining strong text generation performance.', 'score': 55, 'issue_id': 588, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 ноября', 'en': 'November 14', 'zh': '11月14日'}, 'hash': 'ec4879add3585cee', 'authors': ['Zhengyi Wang', 'Jonathan Lorraine', 'Yikai Wang', 'Hang Su', 'Jun Zhu', 'Sanja Fidler', 'Xiaohui Zeng'], 'affiliations': ['Tsinghua University', 'NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.09595.jpg', 'data': {'categories': ['#optimization', '#games', '#3d', '#multimodal', '#training'], 'emoji': '🧠', 'ru': {'title': 'Языковые модели учатся создавать 3D-миры', 'desc': 'Это исследование посвящено расширению возможностей больших языковых моделей (LLM) для генерации 3D-моделей. Авторы представляют LLaMA-Mesh - новый подход, который позволяет представлять 3D-сетки в виде обычного текста, что делает возможной их обработку языковыми моделями. Модель обучается генерировать 3D-модели по текстовым запросам, создавать комбинированные текстовые и 3D-выходы, а также понимать и интерпретировать 3D-сетки. LLaMA-Mesh достигает качества генерации 3D-моделей на уровне специализированных моделей, сохраняя при этом высокую производительность в генерации текста.'}, 'en': {'title': 'Unifying Text and 3D: LLaMA-Mesh Transforms Language into Spatial Creations', 'desc': 'This paper presents LLaMA-Mesh, a novel method that enables large language models (LLMs) to generate 3D meshes from text inputs. By tokenizing 3D mesh data into a text format, the model can leverage existing spatial knowledge from its text training. The approach allows for conversational interactions where users can request 3D mesh generation and receive outputs in both text and mesh formats. LLaMA-Mesh demonstrates that LLMs can effectively learn complex spatial relationships, achieving high-quality mesh generation comparable to models specifically trained for this task.'}, 'zh': {'title': '统一文本与3D生成的创新之路', 'desc': '本研究探讨了如何扩展大型语言模型（LLMs）的能力，使其能够在统一模型中生成3D网格。我们提出了一种新方法LLaMA-Mesh，将3D网格的顶点坐标和面定义表示为普通文本，从而实现与LLMs的直接集成。通过构建一个监督微调（SFT）数据集，使预训练的LLMs能够根据文本提示生成3D网格，并理解和解释3D网格。我们的工作首次证明了LLMs可以通过微调获得复杂的空间知识，从而在文本基础上生成3D网格，成功实现了3D和文本模态的统一。'}}}, {'id': 'https://huggingface.co/papers/2411.09703', 'title': 'MagicQuill: An Intelligent Interactive Image Editing System', 'url': 'https://huggingface.co/papers/2411.09703', 'abstract': 'Image editing involves a variety of complex tasks and requires efficient and precise manipulation techniques. In this paper, we present MagicQuill, an integrated image editing system that enables swift actualization of creative ideas. Our system features a streamlined yet functionally robust interface, allowing for the articulation of editing operations (e.g., inserting elements, erasing objects, altering color) with minimal input. These interactions are monitored by a multimodal large language model (MLLM) to anticipate editing intentions in real time, bypassing the need for explicit prompt entry. Finally, we apply a powerful diffusion prior, enhanced by a carefully learned two-branch plug-in module, to process editing requests with precise control. Experimental results demonstrate the effectiveness of MagicQuill in achieving high-quality image edits. Please visit https://magic-quill.github.io to try out our system.', 'score': 40, 'issue_id': 586, 'pub_date': '2024-11-14', 'pub_date_card': {'ru': '14 ноября', 'en': 'November 14', 'zh': '11月14日'}, 'hash': '530cdb8733e44b9e', 'authors': ['Zichen Liu', 'Yue Yu', 'Hao Ouyang', 'Qiuyu Wang', 'Ka Leong Cheng', 'Wen Wang', 'Zhiheng Liu', 'Qifeng Chen', 'Yujun Shen'], 'affiliations': ['HKUST', 'Ant Group', 'ZJU', 'HKU'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.09703.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#cv'], 'emoji': '🖌️', 'ru': {'title': 'MagicQuill: Интуитивное редактирование изображений с помощью ИИ', 'desc': 'MagicQuill - это интегрированная система редактирования изображений, использующая мультимодальную большую языковую модель (MLLM) для предсказания намерений пользователя в реальном времени. Система имеет упрощенный интерфейс, позволяющий выполнять сложные операции редактирования с минимальным вводом. Для обработки запросов на редактирование применяется мощный диффузионный приор, усиленный специально обученным двухветвевым модулем. Экспериментальные результаты демонстрируют эффективность MagicQuill в достижении высококачественного редактирования изображений.'}, 'en': {'title': 'Effortless Image Editing with MagicQuill', 'desc': 'MagicQuill is an advanced image editing system designed to facilitate quick and precise editing tasks. It utilizes a multimodal large language model (MLLM) to understand user intentions in real time, allowing for seamless interactions without the need for explicit commands. The system incorporates a diffusion prior and a two-branch plug-in module to ensure high-quality and controlled image modifications. Experimental results indicate that MagicQuill significantly enhances the efficiency and effectiveness of image editing processes.'}, 'zh': {'title': 'MagicQuill：高效精准的图像编辑系统', 'desc': '本文介绍了一种名为MagicQuill的集成图像编辑系统，旨在高效、精确地实现创意构思。该系统具有简洁而强大的界面，用户可以通过最少的输入进行编辑操作，如插入元素、擦除对象和改变颜色。系统通过多模态大语言模型（MLLM）实时监测用户的编辑意图，省去了显式输入提示的需求。最后，结合强大的扩散先验和精心设计的双分支插件模块，MagicQuill能够以精确的控制处理编辑请求，实验结果表明其在高质量图像编辑方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.09009', 'title': 'Cut Your Losses in Large-Vocabulary Language Models', 'url': 'https://huggingface.co/papers/2411.09009', 'abstract': 'As language models grow ever larger, so do their vocabularies. This has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation. Cross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory. Rather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly. We implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e., below numerical precision) contribution to the gradient. Experiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.', 'score': 27, 'issue_id': 588, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': 'a3ceb77f367c961a', 'authors': ['Erik Wijmans', 'Brody Huval', 'Alexander Hertzberg', 'Vladlen Koltun', 'Philipp Krähenbühl'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.09009.jpg', 'data': {'categories': ['#optimization', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Революция в памяти: эффективное обучение больших языковых моделей', 'desc': 'Эта статья представляет метод Cut Cross-Entropy (CCE) для эффективного вычисления функции потерь в крупных языковых моделях. CCE значительно снижает потребление памяти при обучении, вычисляя только необходимые логиты вместо полной матрицы. Авторы реализовали специальный kernel для выполнения матричных умножений и редукции log-sum-exp в быстрой памяти. Эксперименты показывают, что CCE drastically снижает потребление памяти без ущерба для скорости обучения или сходимости модели.'}, 'en': {'title': 'Revolutionizing Memory Efficiency in Language Model Training', 'desc': 'This paper introduces Cut Cross-Entropy (CCE), a novel method designed to reduce the memory usage of large language models (LLMs) during training. Traditional cross-entropy loss computation requires storing a large logit matrix, which can consume excessive memory, especially as model vocabularies grow. CCE addresses this by calculating the loss without creating the full logit matrix, instead focusing only on the correct token and dynamically evaluating the necessary computations. The implementation of CCE significantly decreases memory requirements, allowing for efficient training without compromising performance or speed.'}, 'zh': {'title': 'Cut Cross-Entropy：显著降低内存占用的创新方法', 'desc': '随着语言模型的规模不断扩大，它们的词汇量也在增加。这导致在训练过程中，交叉熵的内存占用在一个单独的层中显著增加。我们提出了一种名为Cut Cross-Entropy（CCE）的方法，它在不将所有标记的logits存储到全局内存中的情况下计算交叉熵损失。通过这种方法，CCE显著减少了内存占用，同时保持了训练速度和收敛性。'}}}, {'id': 'https://huggingface.co/papers/2411.06469', 'title': 'ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?', 'url': 'https://huggingface.co/papers/2411.06469', 'abstract': "Large Language Models (LLMs) hold great promise to revolutionize current clinical systems for their superior capacities on medical text processing tasks and medical licensing exams. Meanwhile, traditional ML models such as SVM and XGBoost have still been mainly adopted in clinical prediction tasks. An emerging question is Can LLMs beat traditional ML models in clinical prediction? Thus, we build a new benchmark ClinicalBench to comprehensively study the clinical predictive modeling capacities of both general-purpose and medical LLMs, and compare them with traditional ML models. ClinicalBench embraces three common clinical prediction tasks, two databases, 14 general-purpose LLMs, 8 medical LLMs, and 11 traditional ML models. Through extensive empirical investigation, we discover that both general-purpose and medical LLMs, even with different model scales, diverse prompting or fine-tuning strategies, still cannot beat traditional ML models in clinical prediction yet, shedding light on their potential deficiency in clinical reasoning and decision-making. We call for caution when practitioners adopt LLMs in clinical applications. ClinicalBench can be utilized to bridge the gap between LLMs' development for healthcare and real-world clinical practice.", 'score': 13, 'issue_id': 586, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 ноября', 'en': 'November 10', 'zh': '11月10日'}, 'hash': '933f68c6c7b9c7f5', 'authors': ['Canyu Chen', 'Jian Yu', 'Shan Chen', 'Che Liu', 'Zhongwei Wan', 'Danielle Bitterman', 'Fei Wang', 'Kai Shu'], 'affiliations': ['Illinois Institute of Technology', 'Emory University', 'Mass General Brigham & Boston Childrens Hospital, Harvard Medical School', 'Weill Cornell Medicine, Cornell University', 'Imperial College London', 'Ohio State University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06469.jpg', 'data': {'categories': ['#science', '#benchmark', '#healthcare', '#reasoning'], 'emoji': '🏥', 'ru': {'title': 'LLM vs традиционное ML: кто победит в клиническом прогнозировании?', 'desc': 'Данная статья представляет новый бенчмарк ClinicalBench для сравнения больших языковых моделей (LLM) и традиционных моделей машинного обучения в клинических задачах прогнозирования. Исследователи провели обширное эмпирическое исследование, включающее различные LLM общего назначения и медицинские LLM, а также традиционные модели машинного обучения. Результаты показали, что LLM пока не превосходят традиционные модели в клиническом прогнозировании, несмотря на различные стратегии промптинга и файн-тюнинга. Авторы призывают к осторожности при внедрении LLM в клинические приложения и предлагают использовать ClinicalBench для преодоления разрыва между разработкой LLM для здравоохранения и реальной клинической практикой.'}, 'en': {'title': 'LLMs vs. Traditional Models: A Cautionary Tale in Clinical Prediction', 'desc': 'This paper investigates the effectiveness of Large Language Models (LLMs) in clinical prediction tasks compared to traditional machine learning models like SVM and XGBoost. The authors created a benchmark called ClinicalBench, which includes various clinical prediction tasks and a wide range of models for comparison. Their findings reveal that, despite the advanced capabilities of LLMs in processing medical text, they still fall short of outperforming traditional models in clinical prediction scenarios. The study emphasizes the need for caution in using LLMs for clinical applications, highlighting their limitations in clinical reasoning and decision-making.'}, 'zh': {'title': '大型语言模型在临床预测中的挑战', 'desc': '大型语言模型（LLMs）在医学文本处理和医学执照考试中表现出色，具有革命性的潜力。然而，传统的机器学习模型如支持向量机（SVM）和XGBoost仍然是临床预测任务的主要选择。我们建立了一个新的基准测试ClinicalBench，全面研究通用和医学LLMs在临床预测建模中的能力，并与传统机器学习模型进行比较。研究发现，尽管LLMs在不同规模和策略下进行实验，但在临床预测中仍未能超越传统模型，这表明它们在临床推理和决策方面可能存在不足。'}}}, {'id': 'https://huggingface.co/papers/2411.08768', 'title': 'Sharingan: Extract User Action Sequence from Desktop Recordings', 'url': 'https://huggingface.co/papers/2411.08768', 'abstract': 'Video recordings of user activities, particularly desktop recordings, offer a rich source of data for understanding user behaviors and automating processes. However, despite advancements in Vision-Language Models (VLMs) and their increasing use in video analysis, extracting user actions from desktop recordings remains an underexplored area. This paper addresses this gap by proposing two novel VLM-based methods for user action extraction: the Direct Frame-Based Approach (DF), which inputs sampled frames directly into VLMs, and the Differential Frame-Based Approach (DiffF), which incorporates explicit frame differences detected via computer vision techniques. We evaluate these methods using a basic self-curated dataset and an advanced benchmark adapted from prior work. Our results show that the DF approach achieves an accuracy of 70% to 80% in identifying user actions, with the extracted action sequences being re-playable though Robotic Process Automation. We find that while VLMs show potential, incorporating explicit UI changes can degrade performance, making the DF approach more reliable. This work represents the first application of VLMs for extracting user action sequences from desktop recordings, contributing new methods, benchmarks, and insights for future research.', 'score': 6, 'issue_id': 618, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '1179d90820efd445', 'authors': ['Yanting Chen', 'Yi Ren', 'Xiaoting Qin', 'Jue Zhang', 'Kehong Yuan', 'Lu Han', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Tsinghua University', 'Microsoft'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08768.jpg', 'data': {'categories': ['#agents', '#training', '#optimization', '#video', '#cv', '#benchmark', '#games'], 'emoji': '🖥️', 'ru': {'title': 'Извлечение действий пользователя из видеозаписей с помощью Vision-Language Models', 'desc': 'Статья представляет два новых метода на основе Vision-Language Models для извлечения действий пользователя из записей рабочего стола. Прямой подход на основе кадров (DF) подает выбранные кадры напрямую в VLM, в то время как дифференциальный подход (DiffF) использует явные различия между кадрами. Результаты показывают, что DF-подход достигает точности 70-80% в идентификации действий пользователя. Это первое применение VLM для извлечения последовательностей действий пользователя из записей рабочего стола.'}, 'en': {'title': 'Unlocking User Actions from Desktop Videos with VLMs', 'desc': 'This paper explores the use of Vision-Language Models (VLMs) to extract user actions from desktop video recordings, an area that has not been thoroughly investigated. It introduces two innovative methods: the Direct Frame-Based Approach (DF), which uses sampled video frames directly, and the Differential Frame-Based Approach (DiffF), which analyzes differences between frames. The study evaluates these methods on a self-curated dataset and a benchmark, revealing that the DF method achieves a 70% to 80% accuracy in identifying user actions. The findings suggest that while VLMs are promising, the inclusion of explicit user interface changes can negatively impact performance, highlighting the DF approach as a more dependable option.'}, 'zh': {'title': '从桌面录制中提取用户行为的新方法', 'desc': '本论文探讨了从桌面录制视频中提取用户行为的挑战，提出了两种基于视觉语言模型（VLM）的方法。第一种是直接帧输入方法（DF），它将采样的帧直接输入VLM进行分析；第二种是差异帧输入方法（DiffF），它利用计算机视觉技术检测的帧差异来增强分析。实验结果表明，DF方法在识别用户行为方面的准确率达到70%到80%，并且提取的行为序列可以通过机器人流程自动化（RPA）重放。尽管VLM显示出潜力，但显式用户界面变化的引入可能会降低性能，因此DF方法被认为更为可靠。'}}}, {'id': 'https://huggingface.co/papers/2411.08954', 'title': 'Inconsistencies In Consistency Models: Better ODE Solving Does Not Imply Better Samples', 'url': 'https://huggingface.co/papers/2411.08954', 'abstract': 'Although diffusion models can generate remarkably high-quality samples, they are intrinsically bottlenecked by their expensive iterative sampling procedure. Consistency models (CMs) have recently emerged as a promising diffusion model distillation method, reducing the cost of sampling by generating high-fidelity samples in just a few iterations. Consistency model distillation aims to solve the probability flow ordinary differential equation (ODE) defined by an existing diffusion model. CMs are not directly trained to minimize error against an ODE solver, rather they use a more computationally tractable objective. As a way to study how effectively CMs solve the probability flow ODE, and the effect that any induced error has on the quality of generated samples, we introduce Direct CMs, which directly minimize this error. Intriguingly, we find that Direct CMs reduce the ODE solving error compared to CMs but also result in significantly worse sample quality, calling into question why exactly CMs work well in the first place. Full code is available at: https://github.com/layer6ai-labs/direct-cms.', 'score': 5, 'issue_id': 618, 'pub_date': '2024-11-13', 'pub_date_card': {'ru': '13 ноября', 'en': 'November 13', 'zh': '11月13日'}, 'hash': '9cff9dea78bd5669', 'authors': ['Noël Vouitsis', 'Rasa Hosseinzadeh', 'Brendan Leigh Ross', 'Valentin Villecroze', 'Satya Krishna Gorti', 'Jesse C. Cresswell', 'Gabriel Loaiza-Ganem'], 'affiliations': ['Layer 6 AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08954.jpg', 'data': {'categories': ['#diffusion', '#training', '#optimization', '#cv'], 'emoji': '🔬', 'ru': {'title': 'Парадокс точности: когда меньшая ошибка не гарантирует лучшего результата', 'desc': 'Статья исследует методы улучшения эффективности диффузионных моделей в машинном обучении. Авторы представляют новый подход под названием Direct Consistency Models (Direct CMs), который напрямую минимизирует ошибку решения обыкновенного дифференциального уравнения (ODE) потока вероятности. Несмотря на то, что Direct CMs уменьшают ошибку решения ODE по сравнению с обычными Consistency Models (CMs), они приводят к значительно худшему качеству сэмплов. Это исследование ставит под вопрос причины эффективности CMs и открывает новые направления для изучения в области дистилляции диффузионных моделей.'}, 'en': {'title': 'Streamlining Diffusion: The Paradox of Consistency Models', 'desc': 'This paper discusses the limitations of diffusion models in generating high-quality samples due to their costly iterative sampling process. It introduces consistency models (CMs) as a method to distill diffusion models, allowing for high-fidelity sample generation in fewer iterations. The authors explore the relationship between CMs and the probability flow ordinary differential equation (ODE) that governs diffusion models, proposing Direct CMs that aim to minimize the error in solving this ODE. Surprisingly, while Direct CMs reduce the ODE solving error, they produce lower quality samples, raising questions about the underlying reasons for the effectiveness of CMs.'}, 'zh': {'title': '提升扩散模型效率的新方法', 'desc': '扩散模型能够生成高质量的样本，但其迭代采样过程非常耗时。最近出现的连续性模型（CMs）通过在少量迭代中生成高保真样本，成为一种有前景的扩散模型蒸馏方法。连续性模型蒸馏旨在解决由现有扩散模型定义的概率流常微分方程（ODE），但并不是直接通过最小化ODE求解器的误差来训练。我们引入了直接连续性模型（Direct CMs），直接最小化该误差，发现虽然Direct CMs减少了ODE求解误差，但生成样本的质量显著下降，这引发了对CMs有效性的质疑。'}}}, {'id': 'https://huggingface.co/papers/2411.06490', 'title': 'Hermes: A Large Language Model Framework on the Journey to Autonomous Networks', 'url': 'https://huggingface.co/papers/2411.06490', 'abstract': 'The drive toward automating cellular network operations has grown with the increasing complexity of these systems. Despite advancements, full autonomy currently remains out of reach due to reliance on human intervention for modeling network behaviors and defining policies to meet target requirements. Network Digital Twins (NDTs) have shown promise in enhancing network intelligence, but the successful implementation of this technology is constrained by use case-specific architectures, limiting its role in advancing network autonomy. A more capable network intelligence, or "telecommunications brain", is needed to enable seamless, autonomous management of cellular network. Large Language Models (LLMs) have emerged as potential enablers for this vision but face challenges in network modeling, especially in reasoning and handling diverse data types. To address these gaps, we introduce Hermes, a chain of LLM agents that uses "blueprints" for constructing NDT instances through structured and explainable logical steps. Hermes allows automatic, reliable, and accurate network modeling of diverse use cases and configurations, thus marking progress toward fully autonomous network operations.', 'score': 5, 'issue_id': 618, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 ноября', 'en': 'November 10', 'zh': '11月10日'}, 'hash': '9ee1aeb9095ebee4', 'authors': ['Fadhel Ayed', 'Ali Maatouk', 'Nicola Piovesan', 'Antonio De Domenico', 'Merouane Debbah', 'Zhi-Quan Luo'], 'affiliations': ['Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France', 'Khalifa University of Science and Technology, Abu Dhabi, UAE', 'The Chinese University of Hong Kong, Shenzhen, China', 'Yale University, New Haven, Connecticut, USA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06490.jpg', 'data': {'categories': ['#reasoning', '#agents', '#agi', '#multimodal'], 'emoji': '🧠', 'ru': {'title': "Hermes: автономный 'мозг' для сетей связи на основе ИИ", 'desc': "Статья представляет Hermes - систему агентов на основе больших языковых моделей для автоматического создания цифровых двойников сетей связи. Hermes использует 'чертежи' для построения цифровых двойников через структурированные логические шаги. Это позволяет моделировать разнообразные сценарии использования сетей без вмешательства человека. Система призвана приблизить полную автономность управления сетями связи."}, 'en': {'title': 'Towards Fully Autonomous Cellular Networks with Hermes', 'desc': 'This paper discusses the challenges of automating cellular network operations due to their complexity and the need for human intervention in modeling and policy definition. It highlights the potential of Network Digital Twins (NDTs) to improve network intelligence but notes that their effectiveness is limited by specific architectures. The authors propose Hermes, a system of Large Language Model (LLM) agents that create NDT instances using structured logical steps, enhancing network modeling capabilities. By enabling automatic and accurate modeling for various use cases, Hermes aims to advance the goal of fully autonomous cellular networks.'}, 'zh': {'title': '迈向完全自主的蜂窝网络管理', 'desc': '随着蜂窝网络操作的复杂性增加，自动化的需求也在增长。尽管已有进展，但由于依赖人工干预来建模网络行为和定义政策，完全自主仍然难以实现。网络数字双胞胎（NDT）在提升网络智能方面显示出潜力，但其成功实施受到特定用例架构的限制。为了解决这些问题，我们提出了Hermes，一个利用大型语言模型（LLM）代理的系统，通过结构化和可解释的逻辑步骤构建NDT实例，从而实现多样化用例和配置的自动、可靠和准确的网络建模。'}}}, {'id': 'https://huggingface.co/papers/2411.07232', 'title': 'Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models', 'url': 'https://huggingface.co/papers/2411.07232', 'abstract': 'Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models\' attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed "Additing Affordance Benchmark" for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics.', 'score': 56, 'issue_id': 523, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': '5e344b551de578a9', 'authors': ['Yoad Tewel', 'Rinon Gal', 'Dvir Samuel', 'Yuval Atzmon', 'Lior Wolf', 'Gal Chechik'], 'affiliations': ['NVIDIA', 'Tel-Aviv University', 'Bar-Ilan University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07232.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#cv', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Add-it: Умное добавление объектов в изображения без дополнительного обучения', 'desc': 'В статье представлен метод Add-it для добавления объектов в изображения на основе текстовых инструкций. Он использует расширенный механизм внимания в диффузионных моделях, учитывая информацию из сцены, текстового запроса и генерируемого изображения. Add-it превосходит современные методы на реальных и сгенерированных бенчмарках без специального обучения. Метод предпочтителен в более чем 80% случаев по оценкам людей.'}, 'en': {'title': 'Seamless Object Insertion with Add-it: No Fine-Tuning Needed!', 'desc': "This paper presents Add-it, a novel approach for adding objects to images based on text instructions, addressing the challenge of maintaining the original scene's integrity while ensuring the new object is placed naturally. The method leverages diffusion models' attention mechanisms, integrating information from the scene image, text prompt, and generated image to achieve seamless object insertion. By employing a weighted extended-attention mechanism, Add-it preserves structural consistency and fine details, resulting in more plausible object placements. Remarkably, Add-it does not require task-specific fine-tuning and outperforms existing supervised methods on various benchmarks, including a new evaluation standard for object placement plausibility."}, 'zh': {'title': '无缝图像编辑的新突破', 'desc': '这篇论文介绍了一种名为Add-it的方法，用于根据文本指令将物体添加到图像中。该方法利用扩散模型的注意力机制，结合场景图像、文本提示和生成图像的信息，以实现自然的物体放置。Add-it在不进行特定任务微调的情况下，达到了图像插入基准测试的最先进结果，并在80%以上的情况下被人类评估者所偏好。该方法保持了结构一致性和细节，同时确保了物体的自然位置。'}}}, {'id': 'https://huggingface.co/papers/2411.07199', 'title': 'OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision', 'url': 'https://huggingface.co/papers/2411.07199', 'abstract': 'Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \\omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \\omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \\omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/', 'score': 42, 'issue_id': 522, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': '89d7bedc1b5241ac', 'authors': ['Cong Wei', 'Zheyang Xiong', 'Weiming Ren', 'Xinrun Du', 'Ge Zhang', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'University of Wisconsin-Madison', 'Vector Institute', 'M-A-P'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07199.jpg', 'data': {'categories': ['#dataset', '#data', '#optimization', '#cv', '#architecture', '#open_source', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'OmniEdit: универсальный редактор изображений', 'desc': 'В статье рассматриваются методы редактирования изображений с использованием моделей диффузии, которые обучаются на автоматически синтезированных или вручную аннотированных парах изображений. Основные проблемы существующих моделей включают ограниченные навыки редактирования, шумные и артефактные данные, а также ограничение по разрешению и соотношению сторон изображений. Представлена новая модель OmniEdit, способная выполнять семь различных задач редактирования изображений с любым соотношением сторон. OmniEdit использует обучение от семи специализированных моделей, улучшает качество данных с помощью важностной выборки и предлагает новую архитектуру EditNet для повышения успешности редактирования.'}, 'en': {'title': 'OmniEdit: The All-in-One Image Editing Solution', 'desc': "This paper introduces \textit{omniedit}, a versatile image editing model designed to tackle multiple editing tasks with varying aspect ratios. The authors address key challenges in existing methods, such as biased synthesis, noisy datasets, and fixed resolutions, which limit practical applications. By leveraging supervision from multiple specialist models and employing advanced importance sampling techniques, \textit{omniedit} enhances data quality and editing performance. The proposed EditNet architecture further improves the model's success rate, making it a powerful tool for real-world image editing scenarios."}, 'zh': {'title': '全能图像编辑，打破现实应用的限制', 'desc': '本文介绍了一种名为\textit{omniedit}的全能图像编辑器，旨在解决现有图像编辑方法在实际应用中的局限性。我们识别出三个主要挑战，包括偏见合成过程导致的编辑能力有限、训练数据集中的噪声和伪影问题，以及数据集的低分辨率和固定宽高比限制。通过利用七个不同专业模型的监督，\textit{omniedit}能够处理七种不同的图像编辑任务，并且支持任意宽高比。我们的实验结果表明，\textit{omniedit}在自动评估和人工评估中均显著优于现有模型。'}}}, {'id': 'https://huggingface.co/papers/2411.06176', 'title': 'M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework', 'url': 'https://huggingface.co/papers/2411.06176', 'abstract': 'The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models. Our data, code, and models are available at https://multimodal-documents.github.io.', 'score': 42, 'issue_id': 521, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 ноября', 'en': 'November 9', 'zh': '11月9日'}, 'hash': '950719af940fd8d0', 'authors': ['Yew Ken Chia', 'Liying Cheng', 'Hou Pong Chan', 'Chaoqun Liu', 'Maojia Song', 'Sharifah Mahani Aljunied', 'Soujanya Poria', 'Lidong Bing'], 'affiliations': ['DAMO Academy, Alibaba Group, Singapore', 'Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06176.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#dataset', '#multimodal', '#open_source', '#training'], 'emoji': '📄', 'ru': {'title': 'M-LongDoc: прорыв в понимании длинных мультимодальных документов', 'desc': 'Статья представляет M-LongDoc - новый набор данных и фреймворк для оценки мультимодальных моделей в задаче понимания длинных документов. Авторы предлагают метод дообучения, учитывающий особенности поиска в мультимодальных документах. Набор данных включает 851 образец современных многостраничных документов, требующих генеративных, а не только экстрактивных ответов. Эксперименты показывают, что предложенный подход улучшает корректность ответов модели на 4.6% по сравнению с базовыми моделями с открытым исходным кодом.'}, 'en': {'title': 'Enhancing Multimodal Document Understanding with M-LongDoc', 'desc': 'This paper presents M-LongDoc, a benchmark designed to evaluate large multimodal models on lengthy documents that include text, figures, and tables. The authors introduce a retrieval-aware tuning method that enhances the efficiency and effectiveness of multimodal document reading, particularly for open-ended question-answering tasks. Unlike previous benchmarks, M-LongDoc features more recent and extensive documents, requiring models to generate comprehensive answers rather than just extractive responses. Experimental results indicate that the proposed tuning approach improves the accuracy of model responses by 4.6% compared to existing baseline models.'}, 'zh': {'title': '提升多模态文档理解的效率与效果', 'desc': '本文介绍了一种名为M-LongDoc的基准数据集，包含851个样本，旨在评估大型多模态模型在文档理解和问答任务中的表现。由于文档通常包含文本、图形和表格等多种内容，人工阅读耗时较长，因此需要开发有效的自动化方法来辅助人类。我们提出了一种检索感知的调优方法，以提高多模态文档阅读的效率和效果。实验结果表明，该方法在模型响应的正确性上相较于基线开源模型有4.6%的相对提升。'}}}, {'id': 'https://huggingface.co/papers/2411.07140', 'title': 'Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models', 'url': 'https://huggingface.co/papers/2411.07140', 'abstract': 'New LLM evaluation benchmarks are important to align with the rapid development of Large Language Models (LLMs). In this work, we present Chinese SimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions, and Chinese SimpleQA mainly has five properties (i.e., Chinese, Diverse, High-quality, Static, Easy-to-evaluate). Specifically, first, we focus on the Chinese language over 6 major topics with 99 diverse subtopics. Second, we conduct a comprehensive quality control process to achieve high-quality questions and answers, where the reference answers are static and cannot be changed over time. Third, following SimpleQA, the questions and answers are very short, and the grading process is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we perform a comprehensive evaluation on the factuality abilities of existing LLMs. Finally, we hope that Chinese SimpleQA could guide the developers to better understand the Chinese factuality abilities of their models and facilitate the growth of foundation models.', 'score': 33, 'issue_id': 522, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': 'ffca97b13123516b', 'authors': ['Yancheng He', 'Shilong Li', 'Jiaheng Liu', 'Yingshui Tan', 'Weixun Wang', 'Hui Huang', 'Xingyuan Bu', 'Hangyu Guo', 'Chengwei Hu', 'Boren Zheng', 'Zhuoran Lin', 'Xuepeng Liu', 'Dekai Sun', 'Shirong Lin', 'Zhicheng Zheng', 'Xiaoyong Zhu', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Taobao & Tmall Group of Alibaba'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07140.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#low_resource', '#multilingual'], 'emoji': '🇨🇳', 'ru': {'title': 'Новый китайский бенчмарк для оценки фактологических способностей языковых моделей', 'desc': 'Статья представляет новый бенчмарк Chinese SimpleQA для оценки фактологических способностей языковых моделей на китайском языке. Этот бенчмарк охватывает 6 основных тем и 99 подтем, с коротким форматом вопросов и ответов. Авторы провели тщательный контроль качества для обеспечения высокого уровня вопросов и ответов. Chinese SimpleQA позволяет легко оценивать модели с помощью API OpenAI и призван помочь разработчикам лучше понять возможности своих моделей в работе с китайским языком.'}, 'en': {'title': 'Empowering Chinese LLMs with SimpleQA Factuality Benchmark', 'desc': 'This paper introduces Chinese SimpleQA, a new benchmark designed to evaluate the factuality of Large Language Models (LLMs) specifically for the Chinese language. It features a diverse set of questions across six major topics, ensuring high-quality and static reference answers for consistency in evaluation. The benchmark emphasizes short questions and answers, making the grading process straightforward and efficient, particularly using the OpenAI API. The authors aim for Chinese SimpleQA to help developers assess and improve the factuality capabilities of their models in the Chinese context.'}, 'zh': {'title': '中文SimpleQA：提升语言模型事实能力的基准', 'desc': '本文介绍了中文SimpleQA，这是第一个全面评估语言模型回答短问题的事实能力的基准。该基准专注于中文，涵盖六个主要主题和99个多样化的子主题。我们通过严格的质量控制过程，确保问题和答案的高质量，并且参考答案是静态的，不会随时间变化。希望中文SimpleQA能够帮助开发者更好地理解其模型的中文事实能力，促进基础模型的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.07126', 'title': 'Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models', 'url': 'https://huggingface.co/papers/2411.07126', 'abstract': 'We introduce Edify Image, a family of diffusion models capable of generating photorealistic image content with pixel-perfect accuracy. Edify Image utilizes cascaded pixel-space diffusion models trained using a novel Laplacian diffusion process, in which image signals at different frequency bands are attenuated at varying rates. Edify Image supports a wide range of applications, including text-to-image synthesis, 4K upsampling, ControlNets, 360 HDR panorama generation, and finetuning for image customization.', 'score': 27, 'issue_id': 521, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': 'a7486a925b416669', 'authors': ['NVIDIA', ':', 'Yuval Atzmon', 'Maciej Bala', 'Yogesh Balaji', 'Tiffany Cai', 'Yin Cui', 'Jiaojiao Fan', 'Yunhao Ge', 'Siddharth Gururani', 'Jacob Huffman', 'Ronald Isaac', 'Pooya Jannaty', 'Tero Karras', 'Grace Lam', 'J. P. Lewis', 'Aaron Licata', 'Yen-Chen Lin', 'Ming-Yu Liu', 'Qianli Ma', 'Arun Mallya', 'Ashlee Martino-Tarr', 'Doug Mendez', 'Seungjun Nah', 'Chris Pruett', 'Fitsum Reda', 'Jiaming Song', 'Ting-Chun Wang', 'Fangyin Wei', 'Xiaohui Zeng', 'Yu Zeng', 'Qinsheng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07126.jpg', 'data': {'categories': ['#3d', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Фотореалистичная генерация изображений с пиксельной точностью', 'desc': 'Edify Image - это семейство диффузионных моделей, способных генерировать фотореалистичный контент с пиксельной точностью. Модель использует каскадные диффузионные модели в пространстве пикселей, обученные с помощью нового процесса лапласовской диффузии. В этом процессе сигналы изображения на разных частотных диапазонах затухают с разной скоростью. Edify Image поддерживает широкий спектр приложений, включая синтез изображений по тексту, апскейлинг до 4K, ControlNets и генерацию панорам HDR 360°. Модель также позволяет осуществлять тонкую настройку для кастомизации изображений.'}, 'en': {'title': 'Edify Image: Revolutionizing Photorealistic Image Generation with Precision', 'desc': 'Edify Image is a new set of diffusion models designed to create highly realistic images with precise detail. It employs a unique Laplacian diffusion process that adjusts the diffusion rates for different frequency bands of image signals. This allows for versatile applications such as generating images from text, enhancing image resolution to 4K, and creating panoramic images. Additionally, it offers customization options through finetuning, making it adaptable for various image generation tasks.'}, 'zh': {'title': 'Edify Image：生成真实感图像的新突破', 'desc': 'Edify Image是一种扩散模型，能够生成像素级精确的真实感图像内容。它采用级联像素空间扩散模型，并使用新颖的拉普拉斯扩散过程进行训练，能够以不同的速率衰减不同频率带的图像信号。该模型支持多种应用，包括文本到图像合成、4K超分辨率、ControlNets、360 HDR全景生成以及图像定制的微调。Edify Image在图像生成领域展现了强大的灵活性和高质量的输出。'}}}, {'id': 'https://huggingface.co/papers/2411.05830', 'title': 'GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models', 'url': 'https://huggingface.co/papers/2411.05830', 'abstract': "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\GitChameleon{}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.  is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, GPT-4o achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries,  serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.", 'score': 20, 'issue_id': 528, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': '86b9530c6bdf431e', 'authors': ['Nizar Islah', 'Justine Gehring', 'Diganta Misra', 'Eilif Muller', 'Irina Rish', 'Terry Yue Zhuo', 'Massimo Caccia'], 'affiliations': ['Mila - Quebec AI Institute', 'MPI-IS Tubingen, ELLIS Tubingen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05830.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#open_source', '#plp', '#dataset'], 'emoji': '🦎', 'ru': {'title': 'GitChameleon: Тест на адаптивность моделей к версиям библиотек', 'desc': 'Статья представляет GitChameleon - новый набор данных для оценки способности языковых моделей генерировать код, совместимый с конкретными версиями библиотек. Датасет содержит 116 задач по автодополнению кода на Python с исполняемыми юнит-тестами. Эксперименты показали, что современные большие языковые модели (LLM) испытывают трудности с этой задачей - GPT-4 достигает лишь 39.9% успешных решений из 10 попыток. GitChameleon призван стимулировать разработку более адаптивных моделей генерации кода.'}, 'en': {'title': 'Adapting Code Generation to Evolving Libraries with GitChameleon', 'desc': 'This paper addresses the challenges faced by code generation models due to the rapid evolution of software libraries. It introduces \textit{GitChameleon}, a new dataset with 116 Python code completion problems that are specifically tied to different library versions and include executable unit tests. The dataset allows for a more rigorous evaluation of large language models (LLMs) in generating code that is both syntactically correct and functionally accurate. The findings indicate that current state-of-the-art models, like GPT-4o, struggle with this task, underscoring the need for improved adaptability in code generation systems.'}, 'zh': {'title': '应对软件库版本更新的代码生成挑战', 'desc': '随着软件库的快速发展，代码生成模型面临着适应频繁版本更新的挑战，同时还需保持与旧版本的兼容性。现有的代码补全基准测试往往忽视了这一动态特性，而考虑到这一点的测试又依赖于静态代码预测任务，缺乏基于执行的评估，限制了模型的实际可用性。为了解决这个问题，我们引入了\textit{GitChameleon}，这是一个新颖的手动整理数据集，包含116个Python代码补全问题，每个问题都基于特定的库版本，并附有可执行的单元测试。我们的评估显示，当前最先进的大型语言模型在生成版本特定代码方面表现不佳，强调了这一问题的复杂性和现有模型的局限性。'}}}, {'id': 'https://huggingface.co/papers/2411.07231', 'title': 'Watermark Anything with Localized Messages', 'url': 'https://huggingface.co/papers/2411.07231', 'abstract': 'Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. We introduce a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). The WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked. The models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks. Experiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. Moreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions - no larger than 10% of the image surface - even for small 256times 256 images.', 'score': 18, 'issue_id': 524, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': '0f19ada656cd9116', 'authors': ['Tom Sander', 'Pierre Fernandez', 'Alain Durmus', 'Teddy Furon', 'Matthijs Douze'], 'affiliations': ['Meta FAIR', 'École polytechnique', 'Inria Rennes'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07231.jpg', 'data': {'categories': ['#training', '#cv', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'WAM: Локализованные водяные знаки для любой части изображения', 'desc': 'Эта статья представляет новую модель глубокого обучения для локализованного водяного знака изображений, названную Watermark Anything Model (WAM). WAM способна незаметно встраивать водяные знаки в определенные области изображения и извлекать их, даже если изображение было отредактировано или объединено с другими. Модель обучается сначала на изображениях низкого разрешения, а затем дообучается для улучшения незаметности водяных знаков и работы с несколькими метками. Эксперименты показывают, что WAM конкурентоспособна с современными методами по незаметности и устойчивости, особенно против вставок и склеек изображений.'}, 'en': {'title': 'Watermark Anything: Revolutionizing Localized Image Watermarking', 'desc': 'The paper presents a novel deep-learning model called the Watermark Anything Model (WAM) designed for localized image watermarking. Unlike traditional methods, WAM effectively handles small watermarked areas, making it suitable for images with diverse sources or edits. The model consists of an embedder that subtly alters the image and an extractor that identifies watermarked regions to recover hidden messages. Experimental results demonstrate that WAM achieves high imperceptibility and robustness against common image manipulations, outperforming existing techniques, especially in challenging scenarios involving splicing and inpainting.'}, 'zh': {'title': '局部水印的新突破：水印任何模型', 'desc': '本文提出了一种新的深度学习模型，称为水印任何模型（WAM），用于局部图像水印。该模型能够在不显著改变图像的情况下，嵌入水印并提取隐藏信息。WAM通过联合训练和后期训练，确保水印的隐蔽性和多重水印的能力。实验结果表明，WAM在隐蔽性和鲁棒性方面与最先进的方法相当，尤其在处理拼接和修复时表现优异。'}}}, {'id': 'https://huggingface.co/papers/2411.06208', 'title': 'IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization', 'url': 'https://huggingface.co/papers/2411.06208', 'abstract': 'In the realm of large language models (LLMs), the ability of models to accurately follow instructions is paramount as more agents and applications leverage LLMs for construction, where the complexity of instructions are rapidly increasing. However, on the one hand, there is only a certain amount of complex instruction evaluation data; on the other hand, there are no dedicated algorithms to improve the ability to follow complex instructions. To this end, this paper introduces TRACE, a benchmark for improving and evaluating the complex instructionfollowing ability, which consists of 120K training data and 1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference Optimization) alignment method which takes both input and output preference pairs into consideration, where LLMs not only rapidly align with response preferences but also meticulously explore the instruction preferences. Extensive experiments on both in-domain and outof-domain datasets confirm the effectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and 6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.', 'score': 18, 'issue_id': 521, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 ноября', 'en': 'November 9', 'zh': '11月9日'}, 'hash': 'de83b5a8e14da36e', 'authors': ['Xinghua Zhang', 'Haiyang Yu', 'Cheng Fu', 'Fei Huang', 'Yongbin Li'], 'affiliations': ['Alibaba Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06208.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'TRACE и IOPO: новый подход к обучению языковых моделей сложным инструкциям', 'desc': 'Эта статья представляет TRACE - новый эталонный тест для улучшения и оценки способности языковых моделей следовать сложным инструкциям. Авторы также предлагают метод IOPO (оптимизация предпочтений ввода-вывода) для более эффективного обучения моделей. TRACE включает 120 тысяч тренировочных и 1000 оценочных примеров. Эксперименты показывают значительное улучшение результатов по сравнению с существующими методами как на целевых, так и на сторонних данных.'}, 'en': {'title': 'Enhancing LLMs with TRACE and IOPO for Complex Instructions', 'desc': 'This paper addresses the challenge of large language models (LLMs) in following complex instructions, which is becoming increasingly important as their applications grow. It introduces TRACE, a benchmark designed to enhance and evaluate the ability of LLMs to handle complex instructions, featuring a substantial dataset of 120K training examples and 1K evaluation cases. The authors propose a novel alignment method called IOPO (Input-Output Preference Optimization), which focuses on both input and output preferences to improve LLM responses. Experimental results demonstrate that IOPO significantly enhances performance on both in-domain and out-of-domain datasets, outperforming existing methods like SFT and DPO.'}, 'zh': {'title': '提升复杂指令跟随能力的创新方法', 'desc': '在大型语言模型（LLMs）领域，模型准确遵循指令的能力至关重要，尤其是在指令复杂性迅速增加的情况下。本文提出了TRACE，一个用于提高和评估复杂指令跟随能力的基准，包含12万条训练数据和1000条评估数据。我们还提出了IOPO（输入-输出偏好优化）对齐方法，考虑了输入和输出偏好对，帮助LLMs快速对齐响应偏好并深入探索指令偏好。通过在领域内和领域外数据集上的广泛实验，验证了IOPO的有效性，显示出在领域内数据上分别提高了8.15%和2.18%，在领域外数据上提高了6.29%和3.13%。'}}}, {'id': 'https://huggingface.co/papers/2411.05902', 'title': 'Autoregressive Models in Vision: A Survey', 'url': 'https://huggingface.co/papers/2411.05902', 'abstract': 'Autoregressive modeling has been a huge success in the field of natural language processing (NLP). Recently, autoregressive models have emerged as a significant area of focus in computer vision, where they excel in producing high-quality visual content. Autoregressive models in NLP typically operate on subword tokens. However, the representation strategy in computer vision can vary in different levels, i.e., pixel-level, token-level, or scale-level, reflecting the diverse and hierarchical nature of visual data compared to the sequential structure of language. This survey comprehensively examines the literature on autoregressive models applied to vision. To improve readability for researchers from diverse research backgrounds, we start with preliminary sequence representation and modeling in vision. Next, we divide the fundamental frameworks of visual autoregressive models into three general sub-categories, including pixel-based, token-based, and scale-based models based on the strategy of representation. We then explore the interconnections between autoregressive models and other generative models. Furthermore, we present a multi-faceted categorization of autoregressive models in computer vision, including image generation, video generation, 3D generation, and multi-modal generation. We also elaborate on their applications in diverse domains, including emerging domains such as embodied AI and 3D medical AI, with about 250 related references. Finally, we highlight the current challenges to autoregressive models in vision with suggestions about potential research directions. We have also set up a Github repository to organize the papers included in this survey at: https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey.', 'score': 12, 'issue_id': 532, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': '49d4d6e55fb35ca2', 'authors': ['Jing Xiong', 'Gongye Liu', 'Lun Huang', 'Chengyue Wu', 'Taiqiang Wu', 'Yao Mu', 'Yuan Yao', 'Hui Shen', 'Zhongwei Wan', 'Jinfa Huang', 'Chaofan Tao', 'Shen Yan', 'Huaxiu Yao', 'Lingpeng Kong', 'Hongxia Yang', 'Mi Zhang', 'Guillermo Sapiro', 'Jiebo Luo', 'Ping Luo', 'Ngai Wong'], 'affiliations': ['The University of Hong Kong', 'University of Rochester', 'The University of North Carolina at Chapel Hill', 'The Hong Kong Polytechnic University', 'Tsinghua University', 'The Ohio State University', 'Apple', 'Princeton University', 'Duke University', 'Bytedance'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05902.jpg', 'data': {'categories': ['#healthcare', '#survey', '#video', '#3d', '#open_source', '#multimodal', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Авторегрессия в компьютерном зрении: от пикселей до 3D', 'desc': 'Эта статья представляет собой обзор авторегрессионных моделей в компьютерном зрении. Авторы рассматривают различные стратегии представления визуальных данных: на уровне пикселей, токенов и масштабов. В работе анализируются связи между авторегрессионными моделями и другими генеративными моделями, а также их применение в различных областях, включая воплощенный ИИ и 3D медицинский ИИ. Статья завершается обсуждением текущих проблем и потенциальных направлений исследований авторегрессионных моделей в компьютерном зрении.'}, 'en': {'title': 'Exploring Autoregressive Models: Bridging NLP and Computer Vision', 'desc': 'This paper surveys the application of autoregressive models in computer vision, highlighting their success in generating high-quality visual content. It categorizes these models into three main types: pixel-based, token-based, and scale-based, reflecting the hierarchical nature of visual data. The authors also discuss the relationship between autoregressive models and other generative models, as well as their applications in various fields such as embodied AI and 3D medical AI. Additionally, the paper addresses current challenges and suggests future research directions, providing a comprehensive resource for researchers in the area.'}, 'zh': {'title': '自回归模型：视觉生成的新前沿', 'desc': '自回归建模在自然语言处理领域取得了巨大成功，最近在计算机视觉中也成为了一个重要的研究方向。自回归模型在视觉内容生成方面表现出色，能够生成高质量的图像和视频。本文综述了自回归模型在视觉中的应用，探讨了像素级、标记级和尺度级等不同的表示策略。我们还分析了自回归模型与其他生成模型的关系，并提出了在新兴领域中的应用和未来研究方向。'}}}, {'id': 'https://huggingface.co/papers/2411.05990', 'title': 'Game-theoretic LLM: Agent Workflow for Negotiation Games', 'url': 'https://huggingface.co/papers/2411.05990', 'abstract': "This paper investigates the rationality of large language models (LLMs) in strategic decision-making contexts, specifically within the framework of game theory. We evaluate several state-of-the-art LLMs across a spectrum of complete-information and incomplete-information games. Our findings reveal that LLMs frequently deviate from rational strategies, particularly as the complexity of the game increases with larger payoff matrices or deeper sequential trees.   To address these limitations, we design multiple game-theoretic workflows that guide the reasoning and decision-making processes of LLMs. These workflows aim to enhance the models' ability to compute Nash Equilibria and make rational choices, even under conditions of uncertainty and incomplete information. Experimental results demonstrate that the adoption of these workflows significantly improves the rationality and robustness of LLMs in game-theoretic tasks. Specifically, with the workflow, LLMs exhibit marked improvements in identifying optimal strategies, achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility to exploitation during negotiations. Furthermore, we explore the meta-strategic considerations of whether it is rational for agents to adopt such workflows, recognizing that the decision to use or forgo the workflow constitutes a game-theoretic issue in itself.   Our research contributes to a deeper understanding of LLMs' decision-making capabilities in strategic contexts and provides insights into enhancing their rationality through structured workflows. The findings have implications for the development of more robust and strategically sound AI agents capable of navigating complex interactive environments. Code and data supporting this study are available at https://github.com/Wenyueh/game_theory.", 'score': 6, 'issue_id': 522, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': 'aae23469f2886f4c', 'authors': ['Wenyue Hua', 'Ollie Liu', 'Lingyao Li', 'Alfonso Amayuelas', 'Julie Chen', 'Lucas Jiang', 'Mingyu Jin', 'Lizhou Fan', 'Fei Sun', 'William Wang', 'Xintong Wang', 'Yongfeng Zhang'], 'affiliations': ['Rutgers University, New Brunswick', 'University of Southern California', 'University of South Florida', 'University of California, Santa Barbara', 'Independent Researcher', 'Harvard University', 'Institute of Computing Technology'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05990.jpg', 'data': {'categories': ['#games', '#agents', '#rl', '#math', '#reasoning'], 'emoji': '🎲', 'ru': {'title': 'Повышение рациональности языковых моделей в теории игр', 'desc': 'Статья исследует рациональность больших языковых моделей (LLM) в контексте теории игр. Авторы оценивают современные LLM в различных играх с полной и неполной информацией, обнаруживая, что модели часто отклоняются от рациональных стратегий. Для решения этой проблемы предложены специальные рабочие процессы, улучшающие способность моделей вычислять равновесия Нэша и принимать рациональные решения. Результаты показывают значительное повышение рациональности и устойчивости LLM в игровых задачах при использовании этих методов.'}, 'en': {'title': 'Enhancing LLM Rationality in Strategic Decision-Making', 'desc': "This paper examines how large language models (LLMs) make decisions in strategic situations using game theory. It finds that LLMs often do not follow rational strategies, especially in complex games with larger payoff matrices. To improve their decision-making, the authors propose game-theoretic workflows that help LLMs better compute Nash Equilibria and make rational choices under uncertainty. The results show that these workflows significantly enhance the models' ability to identify optimal strategies and perform better in negotiation scenarios."}, 'zh': {'title': '提升大型语言模型的博弈理性', 'desc': '本文研究了大型语言模型（LLMs）在战略决策中的理性，特别是在博弈论框架下。我们评估了多种先进的LLMs在完全信息和不完全信息博弈中的表现，发现随着博弈复杂性的增加，LLMs常常偏离理性策略。为了解决这些问题，我们设计了多种博弈论工作流程，以指导LLMs的推理和决策过程，从而提高其计算纳什均衡和在不确定条件下做出理性选择的能力。实验结果表明，采用这些工作流程显著提高了LLMs在博弈任务中的理性和稳健性。'}}}, {'id': 'https://huggingface.co/papers/2411.06424', 'title': 'Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction', 'url': 'https://huggingface.co/papers/2411.06424', 'abstract': 'Safety fine-tuning algorithms are commonly used to fine-tune language models to reduce harmful outputs, but the exact internal mechanisms of how those models achieve this remain unclear. In studying direct preference optimisation (DPO) for toxicity reduction, current explanations claim that DPO works by dampening the most toxic MLP neurons to learn an offset to avert toxic regions in the residual stream. However, by ablating the most toxic neurons and applying activation patching, we find this explanation incomplete. By projecting neuron activation changes onto a toxicity probe, we find that only 31.8\\% of toxicity reduction comes from dampened toxic neurons. Instead, DPO reduces toxicity by accumulating effects across multiple neuron groups, both reducing writing in the toxic direction and promoting anti-toxicity in the residual stream. Moreover, DPO gives noisy adjustments to neuron activations, with many neurons actually increasing toxicity. This indicates that DPO is a balancing process between opposing neuron effects to achieve toxicity reduction.', 'score': 5, 'issue_id': 524, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 ноября', 'en': 'November 10', 'zh': '11月10日'}, 'hash': '045698e0e102aa83', 'authors': ['Yushi Yang', 'Filip Sondej', 'Harry Mayne', 'Adam Mahdi'], 'affiliations': ['University of Oxford', 'Jagiellonian University', 'University of Oxford'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06424.jpg', 'data': {'categories': ['#ethics', '#training', '#hallucinations', '#rlhf', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'DPO: тонкая настройка нейронов для снижения токсичности', 'desc': 'Статья исследует механизмы работы алгоритмов оптимизации предпочтений (DPO) для снижения токсичности языковых моделей. Авторы опровергают существующие объяснения, согласно которым DPO работает путем подавления наиболее токсичных нейронов. Исследование показывает, что только 31.8% снижения токсичности происходит за счет подавленных токсичных нейронов. DPO снижает токсичность, накапливая эффекты в разных группах нейронов, как уменьшая токсичные активации, так и усиливая анти-токсичные в остаточном потоке.'}, 'en': {'title': 'DPO: Balancing Neuron Effects for Safer Language Models', 'desc': 'This paper investigates how direct preference optimization (DPO) helps reduce harmful outputs in language models. It challenges the common belief that DPO primarily works by dampening the most toxic neurons. Instead, the study reveals that only a small portion of toxicity reduction is due to this dampening effect, with most of the reduction coming from a complex interplay of multiple neuron groups. The findings suggest that DPO functions as a balancing act, where both toxic and anti-toxic neuron activations are adjusted to achieve an overall decrease in toxicity.'}, 'zh': {'title': 'DPO：在对立神经元效应中实现毒性减少的平衡过程', 'desc': '安全微调算法常用于调整语言模型，以减少有害输出，但其内部机制仍不清楚。本文研究了直接偏好优化（DPO）在减少毒性方面的作用，发现现有解释不够全面。通过对最毒神经元的消融和激活修补，我们发现只有31.8%的毒性减少来自于抑制毒性神经元。DPO通过多个神经元组的累积效应来减少毒性，同时在残差流中促进反毒性，表明DPO是一个在对立神经元效应之间平衡的过程。'}}}, {'id': 'https://huggingface.co/papers/2411.05966', 'title': 'Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation', 'url': 'https://huggingface.co/papers/2411.05966', 'abstract': 'Large language models (LLMs) have demonstrated significant success in natural language processing (NLP) tasks and have shown promising results in other domains such as protein sequence generation. However, there remain salient differences between LLMs used for NLP, which effectively handle multiple tasks and are available in small sizes, and protein language models that are often specialized for specific tasks and only exist in larger sizes. In this work, we introduce two small protein language models, based on Llama-3-8B and Phi-3-mini, that are capable of both uncontrollable and controllable protein generation. For the uncontrollable generation task, our best model achieves an average pLDDT score of 69.75, demonstrating robust performance in generating viable protein structures. For the controllable generation task, in which the model generates proteins according to properties specified in the prompt, we achieve a remarkable average TM-Score of 0.84, indicating high structural similarity to target proteins. We chose 10 properties, including six classes of enzymes, to extend the capabilities of prior protein language models. Our approach utilizes the Low-Rank Adaptor (LoRA) technique, reducing trainable parameters to just 4% of the original model size, lowering computational requirements. By using a subset of the UniRef50 dataset and small models, we reduced the overall training time by 70% without compromising performance. Notably, Phi-3-mini reduced trainable parameters by 60%, decreasing training cost by 30% compared to Llama 3. Consequently, Phi-3 achieved a comparable TM-Score of 0.81, demonstrating that smaller models can match the performance of larger ones, like Llama 3. We also demonstrate the deployment of our models on the energy efficient ET-SoC-1 chip, significantly improving the TPS/W by a factor of 3.', 'score': 4, 'issue_id': 534, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': '7cf32991005092d0', 'authors': ['Aayush Shah', 'Shankar Jayaratnam'], 'affiliations': ['Esperanto Technologies'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05966.jpg', 'data': {'categories': ['#low_resource', '#optimization', '#science', '#dataset', '#small_models', '#training', '#inference'], 'emoji': '🧬', 'ru': {'title': 'Маленькие модели - большие возможности в генерации белков', 'desc': 'В этой работе представлены две небольшие языковые модели для работы с белками, основанные на Llama-3-8B и Phi-3-mini. Модели способны как к неконтролируемой, так и к контролируемой генерации белков, достигая высоких показателей pLDDT и TM-Score соответственно. Исследователи использовали технику Low-Rank Adaptor (LoRA) для уменьшения количества обучаемых параметров и сокращения вычислительных требований. Результаты показывают, что небольшие модели могут достигать производительности, сравнимой с более крупными моделями, при значительном снижении затрат на обучение и развертывание.'}, 'en': {'title': 'Small Models, Big Impact: Efficient Protein Generation', 'desc': 'This paper presents two small protein language models, Llama-3-8B and Phi-3-mini, which excel in both uncontrollable and controllable protein generation tasks. The models achieve impressive performance metrics, with an average pLDDT score of 69.75 for generating viable protein structures and a TM-Score of 0.84 for generating proteins based on specified properties. By employing the Low-Rank Adaptor (LoRA) technique, the models significantly reduce the number of trainable parameters, leading to a 70% decrease in training time and a 30% reduction in training costs. The results indicate that smaller models can achieve comparable performance to larger models while being more efficient, especially when deployed on energy-efficient hardware.'}, 'zh': {'title': '小型模型也能生成高质量蛋白质', 'desc': '本研究介绍了两种小型蛋白质语言模型，基于Llama-3-8B和Phi-3-mini，能够进行不可控和可控的蛋白质生成。我们的最佳模型在不可控生成任务中达到了69.75的平均pLDDT分数，显示出生成可行蛋白质结构的强大性能。在可控生成任务中，模型根据提示生成特定属性的蛋白质，平均TM-Score达到了0.84，表明与目标蛋白质的结构相似性很高。通过使用低秩适配器（LoRA）技术，我们将可训练参数减少到原模型的4%，显著降低了计算需求。'}}}, {'id': 'https://huggingface.co/papers/2411.05945', 'title': 'NeKo: Toward Post Recognition Generative Correction Large Language Models with Task-Oriented Experts', 'url': 'https://huggingface.co/papers/2411.05945', 'abstract': "Construction of a general-purpose post-recognition error corrector poses a crucial question: how can we most effectively train a model on a large mixture of domain datasets? The answer would lie in learning dataset-specific features and digesting their knowledge in a single model. Previous methods achieve this by having separate correction language models, resulting in a significant increase in parameters. In this work, we present Mixture-of-Experts as a solution, highlighting that MoEs are much more than a scalability tool. We propose a Multi-Task Correction MoE, where we train the experts to become an ``expert'' of speech-to-text, language-to-text and vision-to-text datasets by learning to route each dataset's tokens to its mapped expert. Experiments on the Open ASR Leaderboard show that we explore a new state-of-the-art performance by achieving an average relative 5.0% WER reduction and substantial improvements in BLEU scores for speech and translation tasks. On zero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with 15.5% to 27.6% relative WER reduction in the Hyporadise benchmark. NeKo performs competitively on grammar and post-OCR correction as a multi-task model.", 'score': 4, 'issue_id': 528, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': '1306ade09b2fc5c5', 'authors': ['Yen-Ting Lin', 'Chao-Han Huck Yang', 'Zhehuai Chen', 'Piotr Zelasko', 'Xuesong Yang', 'Zih-Ching Chen', 'Krishna C Puvvada', 'Szu-Wei Fu', 'Ke Hu', 'Jun Wei Chiu', 'Jagadeesh Balam', 'Boris Ginsburg', 'Yu-Chiang Frank Wang'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05945.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#machine_translation', '#transfer_learning', '#multimodal', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Мультизадачная коррекция ошибок: один MoE для всех доменов', 'desc': 'Статья представляет новый подход к обучению модели для коррекции ошибок распознавания текста в различных областях. Авторы предлагают использовать архитектуру Mixture-of-Experts (MoE) для создания мультизадачной модели коррекции. Эксперты в MoE обучаются на специфических для каждого набора данных особенностях, что позволяет эффективно обрабатывать различные типы входных данных. Эксперименты показывают, что предложенная модель NeKo достигает нового уровня производительности в задачах распознавания речи и машинного перевода, превосходя существующие модели.'}, 'en': {'title': 'Harnessing Mixture-of-Experts for Superior Post-Recognition Error Correction', 'desc': "This paper introduces a novel approach to post-recognition error correction using a Mixture-of-Experts (MoE) model. The authors propose a Multi-Task Correction MoE that effectively learns from diverse datasets, such as speech-to-text and language-to-text, by routing tokens to specialized experts. This method reduces the need for separate correction models, leading to fewer parameters while maintaining high performance. Experimental results demonstrate significant improvements in word error rate (WER) and BLEU scores, showcasing the model's effectiveness across various tasks, including zero-shot evaluations against leading models."}, 'zh': {'title': '混合专家模型：提升多任务学习的利器', 'desc': '本文探讨了如何有效地训练一个通用的后识别错误修正模型，特别是在处理多种领域数据集时。我们提出了一种混合专家模型（Mixture-of-Experts），通过让专家学习特定数据集的特征，从而在一个模型中整合这些知识。实验结果显示，该模型在语音转文本、语言转文本和视觉转文本任务中表现出色，显著降低了错误率。我们的研究在多个基准测试中超越了现有的最先进技术，展示了混合专家模型的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2411.07180', 'title': 'Counterfactual Generation from Language Models', 'url': 'https://huggingface.co/papers/2411.07180', 'abstract': "Understanding and manipulating the causal generation mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery -- e.g., model ablations or manipulation of linear subspaces tied to specific concepts -- to intervene on these models. To understand the impact of interventions precisely, it is useful to examine counterfactuals -- e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equation. Models using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Our experiments demonstrate that the approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.", 'score': 4, 'issue_id': 521, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': '6b57fa07bdf242ce', 'authors': ['Shauli Ravfogel', 'Anej Svete', 'Vésteinn Snæbjarnarson', 'Ryan Cotterell'], 'affiliations': ['ETH Zurich', 'University of Copenhagen'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07180.jpg', 'data': {'categories': ['#math', '#interpretability', '#reasoning', '#training', '#data'], 'emoji': '🔀', 'ru': {'title': 'Контрфактическое рассуждение в языковых моделях: новый взгляд на причинно-следственные связи', 'desc': 'Данная статья представляет новый подход к генерации контрфактических примеров в языковых моделях. Авторы предлагают рассматривать языковые модели как обобщенные структурные уравнения, используя трюк Гумбеля-макса. Это позволяет моделировать совместное распределение оригинальных строк и их контрфактических вариантов. Разработанный алгоритм, основанный на выборке Гумбеля с учетом последствий, позволяет выводить скрытые переменные шума и генерировать контрфактические примеры для наблюдаемых строк.'}, 'en': {'title': 'Harnessing Counterfactuals for Better Control of Language Models', 'desc': 'This paper focuses on understanding how to control language models by manipulating their causal generation mechanisms. It critiques existing methods like representation surgery, which alter model behavior but may not provide precise insights into their effects. The authors introduce a new framework that uses counterfactual reasoning to generate true string counterfactuals, distinguishing it from traditional interventions. Their approach employs Generalized Structural-equation Models and Gumbel-max sampling to effectively model the relationship between original strings and their counterfactuals, revealing the limitations of current intervention techniques.'}, 'zh': {'title': '掌握语言模型的因果生成机制', 'desc': '本文探讨了在语言模型中理解和操控因果生成机制的重要性。以往的研究主要依赖于表示手术等技术来干预模型，但我们强调反事实推理与干预是不同的概念。我们提出了一种框架，通过将语言模型重构为广义结构方程模型，生成真实的字符串反事实。实验表明，该方法能够生成有意义的反事实，同时揭示了常用干预技术的显著副作用。'}}}, {'id': 'https://huggingface.co/papers/2411.06481', 'title': 'KMM: Key Frame Mask Mamba for Extended Motion Generation', 'url': 'https://huggingface.co/papers/2411.06481', 'abstract': "Human motion generation is a cut-edge area of research in generative computer vision, with promising applications in video creation, game development, and robotic manipulation. The recent Mamba architecture shows promising results in efficiently modeling long and complex sequences, yet two significant challenges remain: Firstly, directly applying Mamba to extended motion generation is ineffective, as the limited capacity of the implicit memory leads to memory decay. Secondly, Mamba struggles with multimodal fusion compared to Transformers, and lack alignment with textual queries, often confusing directions (left or right) or omitting parts of longer text queries. To address these challenges, our paper presents three key contributions: Firstly, we introduce KMM, a novel architecture featuring Key frame Masking Modeling, designed to enhance Mamba's focus on key actions in motion segments. This approach addresses the memory decay problem and represents a pioneering method in customizing strategic frame-level masking in SSMs. Additionally, we designed a contrastive learning paradigm for addressing the multimodal fusion problem in Mamba and improving the motion-text alignment. Finally, we conducted extensive experiments on the go-to dataset, BABEL, achieving state-of-the-art performance with a reduction of more than 57% in FID and 70% parameters compared to previous state-of-the-art methods. See project website: https://steve-zeyu-zhang.github.io/KMM", 'score': 3, 'issue_id': 524, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 ноября', 'en': 'November 10', 'zh': '11月10日'}, 'hash': '970f532cd0007e6c', 'authors': ['Zeyu Zhang', 'Hang Gao', 'Akide Liu', 'Qi Chen', 'Feng Chen', 'Yiran Wang', 'Danning Li', 'Hao Tang'], 'affiliations': ['Peking University', 'The University of Adelaide', 'The Australian National University', 'The University of Sydney', 'Monash University', 'McGill University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06481.jpg', 'data': {'categories': ['#cv', '#optimization', '#games', '#multimodal', '#long_context', '#architecture', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'KMM: Прорыв в генерации движений человека с помощью улучшенной архитектуры Mamba', 'desc': 'Статья представляет новую архитектуру KMM для генерации движений человека, основанную на модели Mamba. KMM решает проблемы затухания памяти и мультимодального слияния, характерные для Mamba, с помощью маскирования ключевых кадров и контрастивного обучения. Эксперименты на датасете BABEL показали значительное улучшение качества генерации по сравнению с предыдущими методами. Предложенный подход демонстрирует перспективы в области компьютерного зрения и генеративных моделей для создания реалистичных движений человека.'}, 'en': {'title': 'Enhancing Motion Generation with Key Frame Masking', 'desc': "This paper focuses on improving human motion generation using a new architecture called KMM, which stands for Key frame Masking Modeling. The KMM architecture enhances the Mamba model's ability to handle long motion sequences by addressing memory decay and improving focus on key actions. Additionally, it introduces a contrastive learning approach to better fuse multimodal data and align motion with textual queries, overcoming limitations seen in previous models. The authors demonstrate the effectiveness of their method through extensive experiments on the BABEL dataset, achieving significant performance improvements over existing techniques."}, 'zh': {'title': '提升人类动作生成的关键技术', 'desc': '人类动作生成是生成计算机视觉中的前沿研究领域，具有视频创作、游戏开发和机器人操作等应用前景。本文提出了一种新的KMM架构，通过关键帧掩蔽建模来增强Mamba在动作片段中的关键动作关注能力，从而解决了内存衰减问题。我们还设计了一种对比学习范式，以改善Mamba的多模态融合和运动-文本对齐问题。通过在BABEL数据集上的广泛实验，我们的模型在FID指标上减少了超过57%，并且参数量减少了70%，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.06272', 'title': 'Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models', 'url': 'https://huggingface.co/papers/2411.06272', 'abstract': 'As large language models become increasingly prevalent in the financial sector, there is a pressing need for a standardized method to comprehensively assess their performance. However, existing finance benchmarks often suffer from limited language and task coverage, as well as challenges such as low-quality datasets and inadequate adaptability for LLM evaluation. To address these limitations, we propose "Golden Touchstone", the first comprehensive bilingual benchmark for financial LLMs, which incorporates representative datasets from both Chinese and English across eight core financial NLP tasks. Developed from extensive open source data collection and industry-specific demands, this benchmark includes a variety of financial tasks aimed at thoroughly assessing models\' language understanding and generation capabilities. Through comparative analysis of major models on the benchmark, such as GPT-4o Llama3, FinGPT and FinMA, we reveal their strengths and limitations in processing complex financial information. Additionally, we open-sourced Touchstone-GPT, a financial LLM trained through continual pre-training and financial instruction tuning, which demonstrates strong performance on the bilingual benchmark but still has limitations in specific tasks.This research not only provides the financial large language models with a practical evaluation tool but also guides the development and optimization of future research. The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at https://github.com/IDEA-FinAI/Golden-Touchstone, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.', 'score': 3, 'issue_id': 520, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 ноября', 'en': 'November 9', 'zh': '11月9日'}, 'hash': '2559c023f673c9b4', 'authors': ['Xiaojun Wu', 'Junxi Liu', 'Huanyi Su', 'Zhouchi Lin', 'Yiyan Qi', 'Chengjin Xu', 'Jiajun Su', 'Jiajie Zhong', 'Fuwei Wang', 'Saizhuo Wang', 'Fengrui Hua', 'Jia Li', 'Jian Guo'], 'affiliations': ['IDEA Research', 'The Hong Kong University of Science and Technology (Guangzhou)', 'The Hong Kong University of Science and Technology', 'Nanjing University', 'South China Normal University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06272.jpg', 'data': {'categories': ['#low_resource', '#optimization', '#open_source', '#multilingual', '#benchmark'], 'emoji': '💹', 'ru': {'title': 'Эталон для оценки финансовых языковых моделей', 'desc': 'В статье обсуждается необходимость создания стандартизированного метода оценки производительности больших языковых моделей (LLM) в финансовом секторе. Авторы предлагают "Golden Touchstone", первый двуязычный эталон для финансовых LLM, который включает в себя наборы данных на китайском и английском языках для восьми основных финансовых задач NLP. Этот эталон позволяет более полно оценивать способности моделей в понимании и генерации финансовой информации. Исследование также включает открытый доступ к коду и весам модели Touchstone-GPT, что способствует дальнейшему развитию и оптимизации финансовых LLM.'}, 'en': {'title': 'Golden Touchstone: Elevating Financial LLM Evaluation', 'desc': "This paper introduces 'Golden Touchstone', a new bilingual benchmark designed to evaluate the performance of large language models (LLMs) in the financial sector. It addresses the shortcomings of existing benchmarks by providing a comprehensive assessment across eight key financial NLP tasks in both Chinese and English. The benchmark is built from high-quality datasets and reflects industry needs, allowing for a thorough evaluation of models like GPT-4o, Llama3, FinGPT, and FinMA. Additionally, the paper presents Touchstone-GPT, a financial LLM that has been fine-tuned for better performance on this benchmark, while also making the resources publicly available to support further research in financial LLMs."}, 'zh': {'title': '金融领域的标准化评估工具——金色基准', 'desc': '随着大型语言模型在金融领域的广泛应用，评估其性能的标准化方法变得尤为重要。现有的金融基准测试存在语言和任务覆盖面有限、数据集质量低以及适应性不足等问题。为了解决这些问题，我们提出了“金色基准”，这是第一个全面的双语金融基准，涵盖了中英文的八个核心金融自然语言处理任务。通过对主要模型的比较分析，我们揭示了它们在处理复杂金融信息时的优缺点，并开源了Touchstone-GPT模型，以促进未来的研究和优化。'}}}, {'id': 'https://huggingface.co/papers/2411.04997', 'title': 'LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation', 'url': 'https://huggingface.co/papers/2411.04997', 'abstract': "CLIP is one of the most important multimodal foundational models today. What powers CLIP's capabilities? The rich supervision signals provided by natural language, the carrier of human knowledge, shape a powerful cross-modal representation space. However, with the rapid advancements in large language models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs' strong textual understanding can fundamentally improve CLIP's ability to handle image captions, drastically enhancing its ability to process long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP's visual encoder. Thanks to the LLM's presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP's text encoder's context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks.", 'score': 31, 'issue_id': 513, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '5e5b851688791e8a', 'authors': ['Weiquan Huang', 'Aoqi Wu', 'Yifan Yang', 'Xufang Luo', 'Yuqing Yang', 'Liang Hu', 'Qi Dai', 'Xiyang Dai', 'Dongdong Chen', 'Chong Luo', 'Lili Qiu'], 'affiliations': ['Tongji University', 'Microsoft Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04997.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#games', '#training'], 'emoji': '🔗', 'ru': {'title': 'Усиление CLIP с помощью LLM для лучшего понимания изображений и текста', 'desc': 'LLM2CLIP - это новый подход к улучшению мультимодальной модели CLIP с помощью больших языковых моделей (LLM). Авторы предлагают дообучать LLM в пространстве подписей к изображениям с помощью контрастивного обучения, чтобы улучшить текстовые возможности CLIP. Затем обученная LLM выступает в роли учителя для визуального энкодера CLIP. Этот метод позволяет использовать более длинные и сложные подписи к изображениям, преодолевая ограничения стандартного текстового энкодера CLIP.'}, 'en': {'title': "Unlocking CLIP's Potential with LLMs", 'desc': "This paper introduces LLM2CLIP, a new method that enhances the CLIP model by integrating large language models (LLMs) like GPT-4. By fine-tuning the LLM in the caption space using contrastive learning, the model improves its ability to understand and generate complex image captions. The LLM acts as a teacher for CLIP's visual encoder, allowing it to process longer and more intricate texts than the original CLIP could handle. The results show significant advancements in cross-modal tasks, demonstrating the effectiveness of combining LLMs with multimodal representation learning."}, 'zh': {'title': '利用大型语言模型提升CLIP的多模态学习能力', 'desc': 'CLIP是一个重要的多模态基础模型，本文提出了一种新方法LLM2CLIP，旨在利用大型语言模型（LLMs）来增强CLIP的能力。通过对LLM进行微调并结合对比学习，我们能够提取其文本能力，从而显著提高CLIP在处理图像标题时的表现。LLM的强大文本理解能力使得CLIP能够处理更长和更复杂的文本，克服了传统CLIP的局限性。实验结果表明，这种方法在跨模态任务中带来了显著的改进。'}}}, {'id': 'https://huggingface.co/papers/2411.04282', 'title': 'Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding', 'url': 'https://huggingface.co/papers/2411.04282', 'abstract': 'Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO.', 'score': 25, 'issue_id': 518, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': 'e566070395107bc0', 'authors': ['Haolin Chen', 'Yihao Feng', 'Zuxin Liu', 'Weiran Yao', 'Akshara Prabhakar', 'Shelby Heinecke', 'Ricky Ho', 'Phil Mui', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04282.jpg', 'data': {'categories': ['#architecture', '#dataset', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие скрытого потенциала рассуждений в языковых моделях', 'desc': 'Статья представляет новый метод LaTRO для улучшения способностей больших языковых моделей (LLM) к рассуждению. LaTRO формулирует процесс рассуждения как выборку из латентного распределения и оптимизирует его с помощью вариационных подходов. Метод позволяет LLM одновременно улучшать процесс рассуждения и способность оценивать качество рассуждений без необходимости внешней обратной связи. Эксперименты на наборах данных GSM8K и ARC-Challenge показали значительное улучшение точности по сравнению с базовыми моделями и обычной дообучением.'}, 'en': {'title': 'Unlocking Reasoning Potential in Large Language Models with LaTRO', 'desc': 'This paper presents LaTent Reasoning Optimization (LaTRO), a new framework designed to enhance the reasoning abilities of large language models (LLMs) during training. LaTRO treats reasoning as a process of sampling from a latent distribution and uses variational methods to optimize this process. The framework allows LLMs to improve their reasoning skills and assess the quality of their reasoning simultaneously, without needing external feedback. Experimental results show that LaTRO significantly boosts the performance of LLMs on reasoning tasks, indicating that pre-trained models have untapped reasoning potential that can be further developed through this method.'}, 'zh': {'title': '解锁大型语言模型的潜在推理能力', 'desc': '大型语言模型（LLMs）在处理复杂推理任务时仍然面临挑战。我们提出了一种新的框架，称为LaTent推理优化（LaTRO），它通过变分方法将推理过程视为从潜在分布中采样。LaTRO可以在训练过程中同时提高模型的推理能力和评估推理质量的能力，而无需外部反馈。实验结果表明，LaTRO显著提高了模型在GSM8K和ARC-Challenge数据集上的表现，证明了预训练LLMs的潜在推理能力可以通过我们的优化方法得到增强。'}}}, {'id': 'https://huggingface.co/papers/2411.05288', 'title': 'Balancing Pipeline Parallelism with Vocabulary Parallelism', 'url': 'https://huggingface.co/papers/2411.05288', 'abstract': 'Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .', 'score': 18, 'issue_id': 509, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': '19accdb712f507d9', 'authors': ['Man Tsung Yeung', 'Penghui Qi', 'Min Lin', 'Xinyi Wan'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05288.jpg', 'data': {'categories': ['#architecture', '#inference', '#open_source', '#optimization', '#training'], 'emoji': '⚡', 'ru': {'title': 'Эффективное распараллеливание словарных слоев для ускорения обучения больших языковых моделей', 'desc': 'Статья предлагает новый метод распараллеливания обучения больших языковых моделей, фокусируясь на эффективном распределении слоев словаря. Авторы разработали алгоритмы для равномерного распределения вычислений и памяти между устройствами в конвейере, что позволяет уменьшить простои и оптимизировать использование памяти. Метод интегрируется с существующими схемами конвейерного параллелизма и особенно эффективен в сочетании с методами балансировки активационной памяти. Эксперименты показывают значительное улучшение пропускной способности и снижение пикового использования памяти, особенно для моделей с большими словарями.'}, 'en': {'title': 'Balancing Memory and Computation in Pipeline Parallelism for Language Models', 'desc': 'This paper focuses on improving the efficiency of training large language models by addressing the imbalance caused by vocabulary layers in pipeline parallelism. The authors propose a method to evenly distribute vocabulary layers across different pipeline devices, which helps to balance computation and memory usage. They introduce algorithms to minimize communication delays within these layers and integrate their approach with existing pipeline schedules. The results show significant improvements in throughput and reduced memory usage, making the training process more efficient, especially for models with large vocabularies.'}, 'zh': {'title': '优化词汇层以平衡计算与内存', 'desc': '本文探讨了在训练大型语言模型时，词汇层导致的计算和内存不平衡问题。我们提出了一种方法，将词汇层均匀分配到管道设备上，并将计算分组到管道传递中。为了减少激活内存开销，我们设计了几种算法来降低词汇层内的通信障碍。通过结合这些技术，我们的方法在计算和参数内存之间实现了有效平衡，并在大词汇场景下显著降低了峰值内存使用。'}}}, {'id': 'https://huggingface.co/papers/2411.05738', 'title': 'StdGEN: Semantic-Decomposed 3D Character Generation from Single Images', 'url': 'https://huggingface.co/papers/2411.05738', 'abstract': 'We present StdGEN, an innovative pipeline for generating semantically decomposed high-quality 3D characters from single images, enabling broad applications in virtual reality, gaming, and filmmaking, etc. Unlike previous methods which struggle with limited decomposability, unsatisfactory quality, and long optimization times, StdGEN features decomposability, effectiveness and efficiency; i.e., it generates intricately detailed 3D characters with separated semantic components such as the body, clothes, and hair, in three minutes. At the core of StdGEN is our proposed Semantic-aware Large Reconstruction Model (S-LRM), a transformer-based generalizable model that jointly reconstructs geometry, color and semantics from multi-view images in a feed-forward manner. A differentiable multi-layer semantic surface extraction scheme is introduced to acquire meshes from hybrid implicit fields reconstructed by our S-LRM. Additionally, a specialized efficient multi-view diffusion model and an iterative multi-layer surface refinement module are integrated into the pipeline to facilitate high-quality, decomposable 3D character generation. Extensive experiments demonstrate our state-of-the-art performance in 3D anime character generation, surpassing existing baselines by a significant margin in geometry, texture and decomposability. StdGEN offers ready-to-use semantic-decomposed 3D characters and enables flexible customization for a wide range of applications. Project page: https://stdgen.github.io', 'score': 13, 'issue_id': 507, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': 'b23d3650ace21f86', 'authors': ['Yuze He', 'Yanning Zhou', 'Wang Zhao', 'Zhongkai Wu', 'Kaiwen Xiao', 'Wei Yang', 'Yong-Jin Liu', 'Xiao Han'], 'affiliations': ['Tencent AI Lab', 'Tsinghua University', 'Beihang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05738.jpg', 'data': {'categories': ['#3d', '#diffusion', '#games'], 'emoji': '🎭', 'ru': {'title': 'StdGEN: Революция в создании 3D-персонажей с семантическим разделением', 'desc': 'StdGEN - это инновационный конвейер для генерации семантически декомпозированных трехмерных персонажей высокого качества из одиночных изображений. В основе StdGEN лежит предложенная авторами Semantic-aware Large Reconstruction Model (S-LRM), трансформер-модель, которая реконструирует геометрию, цвет и семантику из многоракурсных изображений. Конвейер включает дифференцируемую схему извлечения многослойной семантической поверхности, специализированную эффективную многоракурсную диффузионную модель и модуль итеративного уточнения многослойной поверхности. Эксперименты показывают значительное превосходство StdGEN над существующими методами в генерации 3D-персонажей аниме по качеству геометрии, текстур и возможностям декомпозиции.'}, 'en': {'title': 'Revolutionizing 3D Character Generation with StdGEN', 'desc': 'StdGEN is a novel pipeline designed to create high-quality 3D characters from single images, focusing on semantic decomposition. It overcomes limitations of previous methods by generating detailed characters with distinct components like body, clothing, and hair in just three minutes. The core of StdGEN is the Semantic-aware Large Reconstruction Model (S-LRM), which uses a transformer architecture to reconstruct geometry, color, and semantics efficiently. With additional features like a multi-layer surface extraction and a diffusion model, StdGEN achieves superior performance in 3D character generation, particularly for anime, allowing for easy customization and broad application.'}, 'zh': {'title': 'StdGEN：高效生成可分解3D角色的创新管道', 'desc': 'StdGEN是一种创新的管道，能够从单张图像生成语义分解的高质量3D角色，广泛应用于虚拟现实、游戏和电影制作等领域。与以往方法相比，StdGEN在可分解性、有效性和效率上表现出色，能够在三分钟内生成细致的3D角色，且各个语义组件如身体、衣服和头发分离。其核心是语义感知的大型重建模型（S-LRM），该模型基于变换器，能够从多视图图像中联合重建几何、颜色和语义。通过引入可微分的多层语义表面提取方案，StdGEN实现了高质量、可分解的3D角色生成，实验结果显示其在3D动漫角色生成方面的性能超越了现有基准。'}}}, {'id': 'https://huggingface.co/papers/2411.02462', 'title': 'Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study', 'url': 'https://huggingface.co/papers/2411.02462', 'abstract': "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.", 'score': 9, 'issue_id': 508, 'pub_date': '2024-11-04', 'pub_date_card': {'ru': '4 ноября', 'en': 'November 4', 'zh': '11月4日'}, 'hash': '38beaabd86eeaa88', 'authors': ['André Storhaug', 'Jingyue Li'], 'affiliations': ['Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.02462.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#plp'], 'emoji': '🧪', 'ru': {'title': 'Эффективная настройка языковых моделей для генерации модульных тестов', 'desc': 'Эта статья исследует применение методов эффективной настройки параметров (PEFT) для больших языковых моделей (LLM) в задаче генерации модульных тестов. Авторы сравнивают полную тонкую настройку с различными методами PEFT, включая LoRA, (IA)^3 и настройку промптов, на разных архитектурах и размерах моделей. Результаты показывают, что методы PEFT могут обеспечить производительность, сравнимую с полной тонкой настройкой, при этом значительно снижая вычислительные затраты. Особенно эффективной оказалась настройка промптов, а LoRA в некоторых случаях приближается к эффективности полной тонкой настройки.'}, 'en': {'title': 'Unlocking Cost-Effective Fine-Tuning for Unit Test Generation', 'desc': 'This paper explores the use of parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs) in the context of unit test generation. It highlights the challenges of full fine-tuning, which can be costly and resource-intensive, especially as LLMs increase in size. The authors evaluate various PEFT techniques, such as LoRA and prompt tuning, to determine their effectiveness compared to full fine-tuning. The results indicate that PEFT methods can achieve performance similar to full fine-tuning, with prompt tuning being the most efficient option for resource utilization.'}, 'zh': {'title': '参数高效微调：提升单元测试生成的经济性与有效性', 'desc': '这篇论文探讨了大型语言模型（LLMs）在单元测试生成中的应用，特别是参数高效微调（PEFT）方法。传统的全量微调虽然有效，但成本高昂，PEFT方法通过只微调部分参数来降低计算开销。研究表明，PEFT方法在单元测试生成中能够达到与全量微调相当的性能，尤其是提示微调在成本和资源利用上最为有效。论文还比较了不同模型架构和大小下的多种PEFT方法，展示了其在特定任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.04425', 'title': 'DELIFT: Data Efficient Language model Instruction Fine Tuning', 'url': 'https://huggingface.co/papers/2411.04425', 'abstract': "Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model's responses to other samples, effectively measuring the informational value relative to the model's current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.", 'score': 8, 'issue_id': 509, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '397d7c5c26dfad0f', 'authors': ['Ishika Agarwal', 'Krishnateja Killamsetty', 'Lucian Popa', 'Marina Danilevksy'], 'affiliations': ['University of Illinois Urbana-Champaign', 'IBM Research'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04425.jpg', 'data': {'categories': ['#data', '#optimization', '#training'], 'emoji': '🔍', 'ru': {'title': 'Эффективный файн-тюнинг языковых моделей с меньшими данными', 'desc': 'Статья представляет новый алгоритм DELIFT для оптимизации выбора данных при файн-тюнинге больших языковых моделей (LLM). DELIFT использует попарную метрику полезности для оценки информативности образцов данных относительно текущих возможностей модели. Алгоритм эффективно работает на всех этапах файн-тюнинга: инструктирование, обучение специфичным задачам и непрерывное обучение. Эксперименты показали, что DELIFT может сократить объем данных для файн-тюнинга на 70% без ущерба для производительности.'}, 'en': {'title': 'Optimize Data, Maximize Performance with DELIFT!', 'desc': 'This paper presents DELIFT, a new algorithm designed to improve the fine-tuning process of large language models (LLMs) by optimizing data selection. DELIFT focuses on three stages of fine-tuning: instruction tuning, task-specific fine-tuning, and continual fine-tuning, making it more efficient than traditional methods. It uses a pairwise utility metric to evaluate the informational value of data samples, ensuring that only the most beneficial data is selected. The results show that DELIFT can significantly reduce the amount of data needed for fine-tuning by up to 70%, while maintaining or even enhancing model performance.'}, 'zh': {'title': '高效微调：DELIFT算法的创新之路', 'desc': '本论文提出了一种名为DELIFT的新算法，用于提高大型语言模型（LLMs）在特定任务上的性能，同时减少冗余和无效数据的使用。DELIFT通过优化数据选择，系统性地改进了三个关键的微调阶段：指令微调、任务特定微调和持续微调。该方法使用了一种成对效用度量，量化数据样本对模型响应其他样本的改善程度，从而有效评估信息价值。实验结果表明，DELIFT能够在不降低性能的情况下，将微调数据量减少多达70%，显著提高了计算效率。'}}}, {'id': 'https://huggingface.co/papers/2411.04954', 'title': 'CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM', 'url': 'https://huggingface.co/papers/2411.04954', 'abstract': "This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user's inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models' vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points. The project page and more visualizations can be found at: https://cad-mllm.github.io/", 'score': 7, 'issue_id': 518, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'f3ddd073293c6b26', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#data', '#dataset', '#benchmark', '#multimodal', '#optimization', '#open_source'], 'emoji': '🏗️', 'ru': {'title': 'CAD-MLLM: Революция в генерации CAD-моделей с помощью мультимодального ИИ', 'desc': 'CAD-MLLM - это первая система, способная генерировать параметрические CAD-модели на основе мультимодальных входных данных, включая текст, изображения и облака точек. Система использует последовательности команд CAD-моделей и продвинутые большие языковые модели для согласования пространства признаков между различными модальностями и векторными представлениями CAD-моделей. Для обучения модели был создан набор данных Omni-CAD, содержащий около 450 тысяч CAD-моделей с соответствующими мультимодальными данными. Экспериментальные результаты показывают, что CAD-MLLM значительно превосходит существующие методы условной генерации и остается устойчивой к шумам и отсутствующим точкам.'}, 'en': {'title': 'Revolutionizing CAD Generation with Multimodal Inputs', 'desc': 'This paper presents CAD-MLLM, a novel system designed to generate Computer-Aided Design (CAD) models from various user inputs, including text, images, and point clouds. It utilizes large language models (LLMs) to effectively align different types of input data with the vectorized representations of CAD models. The authors introduce the Omni-CAD dataset, which is the first of its kind, containing around 450,000 instances of multimodal data paired with CAD construction sequences. The evaluation of the generated models includes new metrics for topology and surface quality, showing that CAD-MLLM outperforms existing methods and is resilient to data noise.'}, 'zh': {'title': '统一CAD生成系统：多模态输入的创新应用', 'desc': '本文旨在设计一个统一的计算机辅助设计（CAD）生成系统，能够根据用户的文本描述、图像、点云或其组合轻松生成CAD模型。我们介绍了CAD-MLLM，这是第一个能够基于多模态输入生成参数化CAD模型的系统。该框架利用CAD模型的命令序列，并采用先进的大型语言模型（LLMs）对不同模态数据和CAD模型的向量表示进行特征空间对齐。我们构建了一个名为Omni-CAD的综合数据集，包含文本描述、多视图图像、点和命令序列，约有45万个实例，评估结果显示CAD-MLLM在生成质量上显著优于现有方法。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2411.04097', 'title': 'RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned Vision-Language Models', 'url': 'https://huggingface.co/papers/2411.04097', 'abstract': 'Fine-tuned vision-language models (VLMs) often capture spurious correlations between image features and textual attributes, resulting in degraded zero-shot performance at test time. Existing approaches for addressing spurious correlations (i) primarily operate at the global image-level rather than intervening directly on fine-grained image features and (ii) are predominantly designed for unimodal settings. In this work, we present RaVL, which takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features rather than operating at the global image level. Given a fine-tuned VLM, RaVL first discovers spurious correlations by leveraging a region-level clustering approach to identify precise image features contributing to zero-shot classification errors. Then, RaVL mitigates the identified spurious correlation with a novel region-aware loss function that enables the VLM to focus on relevant regions and ignore spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with various model architectures, data domains, and learned spurious correlations. Our results show that RaVL accurately discovers (191% improvement over the closest baseline) and mitigates (8.2% improvement on worst-group image classification accuracy) spurious correlations. Qualitative evaluations on general-domain and medical-domain VLMs confirm our findings.', 'score': 5, 'issue_id': 516, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '62101343e5b62784', 'authors': ['Maya Varma', 'Jean-Benoit Delbrouck', 'Zhihong Chen', 'Akshay Chaudhari', 'Curtis Langlotz'], 'affiliations': ['Stanford University', 'Hugging Face'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04097.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#cv', '#training', '#healthcare', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Точное обнаружение и устранение ложных корреляций в мультимодальных моделях', 'desc': 'Эта статья представляет метод RaVL для обнаружения и смягчения ложных корреляций в мультимодальных моделях компьютерного зрения и языка (VLM). RaVL использует кластеризацию на уровне регионов изображения для выявления конкретных визуальных признаков, вызывающих ошибки классификации. Затем применяется новая функция потерь, учитывающая регионы, чтобы модель фокусировалась на релевантных областях при дообучении. Эксперименты на 654 VLM показали значительное улучшение в обнаружении и смягчении ложных корреляций по сравнению с базовыми методами.'}, 'en': {'title': 'Enhancing VLM Robustness by Targeting Spurious Correlations at the Local Level', 'desc': 'This paper introduces RaVL, a method designed to improve the robustness of fine-tuned vision-language models (VLMs) by addressing spurious correlations between image features and text attributes. Unlike existing methods that focus on global image-level features, RaVL operates on fine-grained local image features to identify and mitigate these correlations. It employs a region-level clustering approach to pinpoint specific image features that lead to classification errors in zero-shot scenarios. The results demonstrate that RaVL significantly enhances the discovery and mitigation of spurious correlations, leading to improved classification accuracy across various VLM architectures and domains.'}, 'zh': {'title': '提升视觉语言模型鲁棒性的RaVL方法', 'desc': '本文提出了一种名为RaVL的模型，旨在提高视觉语言模型（VLM）的鲁棒性。RaVL通过局部图像特征来发现和减轻虚假相关性，而不是仅在全局图像层面进行干预。该方法利用区域级聚类来识别导致零-shot分类错误的精确图像特征，并通过新的区域感知损失函数来减轻这些虚假相关性。实验结果表明，RaVL在发现和减轻虚假相关性方面均显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2411.04986', 'title': 'The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities', 'url': 'https://huggingface.co/papers/2411.04986', 'abstract': 'Modern language models can process inputs across diverse languages and modalities. We hypothesize that models acquire this capability through learning a shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. We term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through a transmodal semantic "hub" which integrates information from various modality-specific "spokes" regions. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the model\'s dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply a vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing.', 'score': 4, 'issue_id': 515, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '9a27aef11b630a04', 'authors': ['Zhaofeng Wu', 'Xinyan Velocity Yu', 'Dani Yogatama', 'Jiasen Lu', 'Yoon Kim'], 'affiliations': ['MIT', 'University of Southern California', 'Allen Institute for AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04986.jpg', 'data': {'categories': ['#multilingual', '#multimodal', '#transfer_learning', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Единое семантическое пространство как ключ к мультимодальности ИИ', 'desc': 'Статья рассматривает гипотезу семантического хаба в современных языковых моделях. Исследователи предполагают, что модели обрабатывают разнородные входные данные через общее семантическое пространство представлений. Эксперименты показывают, что семантически эквивалентные входы на разных языках имеют схожие представления в промежуточных слоях модели. Это свойство распространяется на различные типы данных, включая арифметические выражения, код и аудиовизуальные входы.'}, 'en': {'title': 'Unlocking Multimodal Understanding: The Semantic Hub Hypothesis', 'desc': "This paper explores how modern language models can understand and process different languages and types of data by learning a shared representation space. The authors propose the 'semantic hub hypothesis', which suggests that these models organize information similarly to how the human brain integrates knowledge from various sources. They demonstrate that representations of semantically similar inputs, even from different languages or modalities, are closely aligned in the model's intermediate layers. Additionally, they show that changes in one type of data representation can influence outputs in other types, indicating that this shared space is actively used by the model rather than being a mere artifact of training."}, 'zh': {'title': '共享表示空间：跨模态理解的关键', 'desc': '现代语言模型能够处理多种语言和模态的输入。我们假设模型通过学习一个共享的表示空间来获得这一能力，这个空间将语义相似的输入放在一起，即使它们来自不同的模态或语言。我们称之为语义中心假设，类似于神经科学中的中心-辐射模型，认为人脑中的语义知识是通过一个跨模态的语义“中心”组织的。研究表明，不同语言中语义等价的输入在模型的中间层具有相似的表示，这种共享表示空间在处理输入时被模型积极利用。'}}}, {'id': 'https://huggingface.co/papers/2411.05457', 'title': 'Improving the detection of technical debt in Java source code with an enriched dataset', 'url': 'https://huggingface.co/papers/2411.05457', 'abstract': 'Technical debt (TD) is a term used to describe the additional work and costs that emerge when developers have opted for a quick and easy solution to a problem, rather than a more effective and well-designed, but time-consuming approach. Self-Admitted Technical Debts (SATDs) are a specific type of technical debts that developers intentionally document and acknowledge, typically via textual comments. While these self-admitted comments are a useful tool for identifying technical debts, most of the existing approaches focus on capturing crucial tokens associated with various categories of TD, neglecting the rich information embedded within the source code itself. Recent research has focused on detecting SATDs by analyzing comments embedded in source code, and there has been little work dealing with technical debts contained in the source code. To fill such a gap, in this study, through the analysis of comments and their associated source code from 974 Java projects hosted in the Stack corpus, we curated the first ever dataset of TD identified by code comments, coupled with its associated source code. Through an empirical evaluation, we found out that the comments of the resulting dataset help enhance the prediction performance of state-of-the-art SATD detection models. More importantly, including the classified source code significantly improves the accuracy in predicting various types of technical debt. In this respect, our work is two-fold: (i) We believe that our dataset will catalyze future work in the domain, inspiring various research issues related to the recognition of technical debt; (ii) The proposed classifiers may serve as baselines for other studies on the detection of TD by means of the curated dataset.', 'score': 2, 'issue_id': 510, 'pub_date': '2024-11-08', 'pub_date_card': {'ru': '8 ноября', 'en': 'November 8', 'zh': '11月8日'}, 'hash': 'bc9c84b19f317115', 'authors': ['Nam Le Hai', 'Anh M. T. Bui', 'Phuong T. Nguyen', 'Davide Di Ruscio', 'Rick Kazman'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05457.jpg', 'data': {'categories': ['#data', '#training', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Новый взгляд на технический долг: анализ кода и комментариев', 'desc': 'Эта статья посвящена исследованию технического долга (ТД) в разработке программного обеспечения. Авторы создали первый датасет, содержащий примеры ТД, идентифицированного в комментариях к коду, вместе с соответствующим исходным кодом. Исследование показало, что включение классифицированного исходного кода значительно улучшает точность предсказания различных типов технического долга. Работа предлагает новый подход к обнаружению ТД, который может служить основой для будущих исследований в этой области.'}, 'en': {'title': 'Bridging Comments and Code: Enhancing Technical Debt Detection', 'desc': 'This paper addresses the issue of Technical Debt (TD) in software development, particularly focusing on Self-Admitted Technical Debts (SATDs) documented by developers. It highlights the limitations of existing methods that primarily analyze comment tokens, overlooking the valuable information within the source code itself. The authors present a novel dataset created from 974 Java projects, linking SATD comments to their corresponding source code, which enhances the detection of technical debts. Their empirical evaluation demonstrates that incorporating both comments and source code significantly improves the performance of SATD detection models, paving the way for future research in this area.'}, 'zh': {'title': '技术债务识别的新视角', 'desc': '技术债务（TD）是指开发者为了快速解决问题而选择的简单方案所带来的额外工作和成本。自我承认的技术债务（SATD）是开发者通过文本注释主动记录和承认的一种特定类型的技术债务。本文通过分析来自974个Java项目的注释和相关源代码，创建了首个由代码注释识别的技术债务数据集，并发现这些注释能显著提高现有SATD检测模型的预测性能。我们的研究不仅为技术债务的识别提供了新的数据集，还提出了可作为基线的分类器，推动未来相关研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.04905', 'title': 'OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models', 'url': 'https://huggingface.co/papers/2411.04905', 'abstract': "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems.While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an ``open cookbook'' for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.", 'score': 102, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '799dedd6597ce7ab', 'authors': ['Siming Huang', 'Tianhao Cheng', 'J. K. Liu', 'Jiaran Hao', 'Liuyihan Song', 'Yang Xu', 'J. Yang', 'J. H. Liu', 'Chenchen Zhang', 'Linzheng Chai', 'Ruifeng Yuan', 'Zhaoxiang Zhang', 'Jie Fu', 'Qian Liu', 'Ge Zhang', 'Zili Wang', 'Yuan Qi', 'Yinghui Xu', 'Wei Chu'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04905.jpg', 'data': {'categories': ['#dataset', '#data', '#training', '#synthetic', '#agents', '#plp'], 'emoji': '🧑\u200d💻', 'ru': {'title': 'OpenCoder: открытая книга рецептов для создания топовых языковых моделей кода', 'desc': 'OpenCoder - это высококачественная языковая модель для работы с кодом, сопоставимая по производительности с ведущими моделями. Авторы не только предоставляют веса модели и код для инференса, но и полный набор воспроизводимых данных для обучения, конвейер обработки данных и подробные протоколы обучения. Ключевыми ингредиентами для создания модели такого уровня являются оптимизированные эвристические правила очистки данных, методы дедупликации, использование текстового корпуса, связанного с кодом, и высококачественные синтетические данные. Эта открытость призвана ускорить исследования и обеспечить воспроизводимый прогресс в области ИИ для работы с кодом.'}, 'en': {'title': 'OpenCoder: Unlocking Code AI with Transparency and Reproducibility', 'desc': 'This paper introduces OpenCoder, a high-performance large language model (LLM) specifically designed for code generation and reasoning tasks. It addresses the lack of open-access models that provide reproducible data processing and transparent training protocols, which are essential for scientific research. OpenCoder not only matches the performance of proprietary models but also shares its model weights, inference code, and detailed training methodologies. By emphasizing data cleaning, corpus recall, and synthetic data generation, OpenCoder aims to enhance accessibility and foster reproducible advancements in the field of code AI.'}, 'zh': {'title': 'OpenCoder：开放的顶级代码大语言模型', 'desc': '本文介绍了OpenCoder，一个高质量的代码大语言模型（LLM），旨在为科学研究提供开放的资源。与其他模型不同，OpenCoder不仅提供模型权重和推理代码，还包括可重复的训练数据和完整的数据处理流程。我们识别出构建顶级代码LLM的关键要素，包括数据清洗的启发式规则、与代码相关的文本语料库的回忆以及高质量的合成数据。通过这种开放性，我们希望加速代码人工智能的研究和可重复的进展。'}}}, {'id': 'https://huggingface.co/papers/2411.05003', 'title': 'ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning', 'url': 'https://huggingface.co/papers/2411.05003', 'abstract': 'Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique.', 'score': 65, 'issue_id': 464, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'f71f2e0f1addbe57', 'authors': ['David Junhao Zhang', 'Roni Paiss', 'Shiran Zada', 'Nikhil Karnad', 'David E. Jacobs', 'Yael Pritch', 'Inbar Mosseri', 'Mike Zheng Shou', 'Neal Wadhwa', 'Nataniel Ruiz'], 'affiliations': ['Google', 'National University of Singapore'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05003.jpg', 'data': {'categories': ['#video', '#diffusion', '#hallucinations'], 'emoji': '🎥', 'ru': {'title': 'Переснимаем реальность: новые ракурсы для любого видео', 'desc': 'ReCapture - это метод генерации новых видео с измененными траекториями камеры на основе одного пользовательского видео. Он позволяет перегенерировать исходное видео с другими углами обзора и кинематографическим движением камеры, включая правдоподобное восстановление невидимых частей сцены. Метод работает в два этапа: сначала создается шумное опорное видео с новой траекторией камеры, а затем оно очищается и делается временно согласованным. ReCapture использует мультивидовые диффузионные модели и рендеринг облака точек на основе глубины.'}, 'en': {'title': 'ReCapture: Transforming User Videos with New Camera Perspectives', 'desc': 'This paper introduces ReCapture, a novel method for generating new videos with different camera angles from a single user-provided video. It leverages multiview diffusion models and depth-based point cloud rendering to create an initial noisy video with a new camera trajectory. The method then refines this video using a masked video fine-tuning technique to ensure temporal consistency and clarity. Additionally, ReCapture can convincingly generate parts of the scene that were not visible in the original video, enhancing the overall viewing experience.'}, 'zh': {'title': 'ReCapture：从用户视频生成新视角的魔法', 'desc': '最近在视频建模方面取得了突破，使得生成视频中的相机轨迹可控。然而，这些方法无法直接应用于用户提供的非生成视频。本文提出了一种名为ReCapture的方法，可以从单个用户提供的视频生成具有新相机轨迹的新视频。该方法不仅能够从不同角度重新生成参考视频，还能合理地幻觉出参考视频中不可见的场景部分。'}}}, {'id': 'https://huggingface.co/papers/2411.04965', 'title': 'BitNet a4.8: 4-bit Activations for 1-bit LLMs', 'url': 'https://huggingface.co/papers/2411.04965', 'abstract': 'Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58, presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.', 'score': 63, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'dcfd440f9caf6714', 'authors': ['Hongyu Wang', 'Shuming Ma', 'Furu Wei'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04965.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'BitNet a4.8: Эффективность и производительность в мире языковых моделей', 'desc': 'Статья представляет BitNet a4.8 - усовершенствованную версию 1-битной языковой модели. Эта модель использует 4-битные активации для входных данных в слоях внимания и прямой связи, а также применяет разреживание и 8-битное квантование для промежуточных состояний. Эксперименты показывают, что BitNet a4.8 достигает производительности, сравнимой с BitNet b1.58, при эквивалентных затратах на обучение, но с более быстрым выводом. Модель активирует только 55% параметров и поддерживает 3-битный KV-кэш, повышая эффективность развертывания и вывода крупномасштабных языковых моделей.'}, 'en': {'title': 'Efficient Inference with 4-bit Activations in LLMs', 'desc': 'This paper introduces BitNet a4.8, a new model that enhances the efficiency of 1-bit Large Language Models (LLMs) by using 4-bit activations. It combines hybrid quantization and sparsification techniques to reduce errors from outlier channels, allowing for better performance. The model uses 4-bit activations in key layers while applying 8-bit quantization to intermediate states, resulting in faster inference times. Experiments show that BitNet a4.8 matches the performance of its predecessor, BitNet b1.58, while being more efficient in terms of parameter activation and supporting a 3-bit key-value cache.'}, 'zh': {'title': '提升大型语言模型推理效率的新方法', 'desc': '最近关于1位大型语言模型（LLMs）的研究，如BitNet b1.58，展示了在保持性能的同时降低推理成本的前景。本文介绍了BitNet a4.8，支持1位LLMs的4位激活。BitNet a4.8采用混合量化和稀疏化策略，以减轻由异常通道引入的量化误差。实验表明，BitNet a4.8在训练成本相当的情况下，推理速度更快，且仅激活55%的参数，支持3位KV缓存，进一步提高了大规模LLM的部署和推理效率。'}}}, {'id': 'https://huggingface.co/papers/2411.04996', 'title': 'Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models', 'url': 'https://huggingface.co/papers/2411.04996', 'abstract': "The development of large language models (LLMs) has expanded to multi-modal systems capable of processing text, images, and speech within a unified framework. Training these models demands significantly larger datasets and computational resources compared to text-only LLMs. To address the scaling challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that significantly reduces pretraining computational costs. MoT decouples non-embedding parameters of the model by modality -- including feed-forward networks, attention matrices, and layer normalization -- enabling modality-specific processing with global self-attention over the full input sequence. We evaluate MoT across multiple settings and model scales. In the Chameleon 7B setting (autoregressive text-and-image generation), MoT matches the dense baseline's performance using only 55.8\\% of the FLOPs. When extended to include speech, MoT reaches speech performance comparable to the dense baseline with only 37.2\\% of the FLOPs. In the Transfusion setting, where text and image are trained with different objectives, a 7B MoT model matches the image modality performance of the dense baseline with one third of the FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image generation metrics. System profiling further highlights MoT's practical benefits, achieving dense baseline image quality in 47.2\\% of the wall-clock time and text quality in 75.6\\% of the wall-clock time (measured on AWS p4de.24xlarge instances with NVIDIA A100 GPUs).", 'score': 46, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '53d29fd65eda072e', 'authors': ['Weixin Liang', 'Lili Yu', 'Liang Luo', 'Srinivasan Iyer', 'Ning Dong', 'Chunting Zhou', 'Gargi Ghosh', 'Mike Lewis', 'Wen-tau Yih', 'Luke Zettlemoyer', 'Xi Victoria Lin'], 'affiliations': ['FAIR at Meta', 'Stanford University, Department of Computer Science'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04996.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективные мультимодальные трансформеры: меньше вычислений, та же мощность', 'desc': 'Эта статья представляет Mixture-of-Transformers (MoT) - новую архитектуру для мультимодальных языковых моделей, которая значительно снижает вычислительные затраты при предобучении. MoT разделяет параметры модели по модальностям, позволяя эффективно обрабатывать текст, изображения и речь в единой системе. Эксперименты показывают, что MoT достигает производительности плотных базовых моделей, используя значительно меньше вычислительных ресурсов. Это позволяет создавать более эффективные мультимодальные системы искусственного интеллекта.'}, 'en': {'title': 'Efficient Multi-Modal Processing with Mixture-of-Transformers', 'desc': 'This paper presents Mixture-of-Transformers (MoT), a novel sparse multi-modal transformer architecture designed to efficiently handle text, images, and speech. By decoupling non-embedding parameters by modality, MoT allows for specialized processing while maintaining global self-attention across the entire input. The architecture significantly reduces computational costs, achieving comparable performance to dense models with fewer floating point operations (FLOPs). Evaluations show that MoT not only matches but often outperforms dense baselines in various settings, demonstrating its effectiveness in multi-modal tasks.'}, 'zh': {'title': '混合变换器：高效的多模态学习新方案', 'desc': '本论文介绍了一种新的稀疏多模态变换器架构，称为混合变换器（MoT），旨在降低大语言模型的预训练计算成本。MoT通过模态解耦模型的非嵌入参数，使得不同模态（如文本、图像和语音）可以进行特定处理，同时保持全局自注意力机制。实验结果表明，MoT在多个设置下表现出色，能够以更少的计算资源达到与密集基线相当的性能。该模型在图像生成和语音处理任务中均展现了显著的效率优势，证明了其在多模态学习中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2411.04928', 'title': 'DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion', 'url': 'https://huggingface.co/papers/2411.04928', 'abstract': 'In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. Our approach begins with the insight that both the spatial structure of a 3D scene and the temporal evolution of a 4D scene can be effectively represented through sequences of video frames. While recent video diffusion models have shown remarkable success in producing vivid visuals, they face limitations in directly recovering 3D/4D scenes due to limited spatial and temporal controllability during generation. To overcome this, we propose ST-Director, which decouples spatial and temporal factors in video diffusion by learning dimension-aware LoRAs from dimension-variant data. This controllable video diffusion approach enables precise manipulation of spatial structure and temporal dynamics, allowing us to reconstruct both 3D and 4D representations from sequential frames with the combination of spatial and temporal dimensions. Additionally, to bridge the gap between generated videos and real-world scenes, we introduce a trajectory-aware mechanism for 3D generation and an identity-preserving denoising strategy for 4D generation. Extensive experiments on various real-world and synthetic datasets demonstrate that DimensionX achieves superior results in controllable video generation, as well as in 3D and 4D scene generation, compared with previous methods.', 'score': 43, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'b0958f934bc56a95', 'authors': ['Wenqiang Sun', 'Shuo Chen', 'Fangfu Liu', 'Zilong Chen', 'Yueqi Duan', 'Jun Zhang', 'Yikai Wang'], 'affiliations': ['HKUST', 'Tsinghua University', 'ShengShu'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04928.jpg', 'data': {'categories': ['#3d', '#video', '#synthetic'], 'emoji': '🎭', 'ru': {'title': 'От 2D к 4D: DimensionX раздвигает границы генеративных моделей', 'desc': 'DimensionX - это фреймворк для генерации фотореалистичных 3D и 4D сцен из одного изображения с помощью видео-диффузии. Ключевым компонентом является ST-Director, который разделяет пространственные и временные факторы в видео-диффузии, обучая размерно-зависимые LoRA на данных с различными измерениями. Для 3D генерации используется механизм, учитывающий траекторию, а для 4D - стратегия шумоподавления, сохраняющая идентичность. Эксперименты показывают превосходство DimensionX в контролируемой генерации видео и создании 3D/4D сцен по сравнению с существующими методами.'}, 'en': {'title': 'Transforming Single Images into Stunning 3D and 4D Worlds!', 'desc': 'DimensionX is a novel framework that generates photorealistic 3D and 4D scenes from a single image using video diffusion techniques. It addresses the limitations of existing video diffusion models by introducing ST-Director, which separates spatial and temporal factors, allowing for better control over the generated scenes. By learning dimension-aware Low-Rank Adaptations (LoRAs) from diverse data, DimensionX enhances the manipulation of both spatial structures and temporal dynamics. The framework also incorporates a trajectory-aware mechanism and an identity-preserving denoising strategy to improve the realism of generated scenes, outperforming previous methods in extensive experiments.'}, 'zh': {'title': 'DimensionX：从单图像生成真实3D和4D场景的创新框架', 'desc': '本文介绍了DimensionX，一个框架，旨在通过单张图像和视频扩散生成逼真的3D和4D场景。我们的方法利用视频帧序列有效表示3D场景的空间结构和4D场景的时间演变。为了解决现有视频扩散模型在生成过程中空间和时间可控性不足的问题，我们提出了ST-Director，通过从维度变化的数据中学习维度感知的LoRA，解耦空间和时间因素。实验结果表明，DimensionX在可控视频生成以及3D和4D场景生成方面优于以往的方法。'}}}, {'id': 'https://huggingface.co/papers/2411.04952', 'title': 'M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding', 'url': 'https://huggingface.co/papers/2411.04952', 'abstract': 'Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images.', 'score': 26, 'issue_id': 477, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '56e0d2f2775dbda9', 'authors': ['Jaemin Cho', 'Debanjan Mahata', 'Ozan Irsoy', 'Yujie He', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill', 'Bloomberg'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04952.jpg', 'data': {'categories': ['#benchmark', '#rag', '#dataset', '#multimodal', '#games', '#long_context'], 'emoji': '📄', 'ru': {'title': 'M3DocRAG: Мультимодальные ответы на вопросы по документам нового поколения', 'desc': 'Статья представляет M3DocRAG - новую мультимодальную систему для ответов на вопросы по документам. Она использует мультимодальный ретривер и мультимодальную языковую модель для эффективной обработки одного или нескольких документов с сохранением визуальной информации. M3DocRAG способна работать с различными контекстами документов, типами вопросов и модальностями доказательств. Авторы также представляют новый бенчмарк M3DocVQA для оценки систем открытого домена на более чем 3000 PDF-документах.'}, 'en': {'title': 'M3DocRAG: Revolutionizing Document Visual Question Answering', 'desc': 'The paper presents M3DocRAG, a new framework for Document Visual Question Answering (DocVQA) that addresses limitations of existing methods. Unlike traditional approaches that focus on single-page documents and often overlook visual elements, M3DocRAG can process multiple pages and various document types while retaining important visual information. It utilizes a multi-modal retriever and a multi-modal language model (MLM) to efficiently find relevant documents and answer questions, accommodating both text and visual evidence. The authors also introduce M3DocVQA, a benchmark for evaluating open-domain DocVQA, demonstrating that their framework outperforms existing models in several benchmarks.'}, 'zh': {'title': 'M3DocRAG：跨页文档问答的新突破', 'desc': '本文介绍了一种新的多模态检索增强生成框架M3DocRAG，旨在解决文档视觉问答（DocVQA）中的挑战。现有方法主要处理单页文档，无法有效应对跨页或多文档的信息检索。M3DocRAG能够灵活处理不同的文档上下文和问题跳跃，同时保留重要的视觉信息，如图表和图像。通过在超过3000个PDF文档上进行评估，M3DocRAG在多个基准测试中表现优异，超越了许多强基线。'}}}, {'id': 'https://huggingface.co/papers/2411.04709', 'title': 'TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2411.04709', 'abstract': 'Video generation models are revolutionizing content creation, with image-to-video models drawing increasing attention due to their enhanced controllability, visual consistency, and practical applications. However, despite their popularity, these models rely on user-provided text and image prompts, and there is currently no dedicated dataset for studying these prompts. In this paper, we introduce TIP-I2V, the first large-scale dataset of over 1.70 million unique user-provided Text and Image Prompts specifically for Image-to-Video generation. Additionally, we provide the corresponding generated videos from five state-of-the-art image-to-video models. We begin by outlining the time-consuming and costly process of curating this large-scale dataset. Next, we compare TIP-I2V to two popular prompt datasets, VidProM (text-to-video) and DiffusionDB (text-to-image), highlighting differences in both basic and semantic information. This dataset enables advancements in image-to-video research. For instance, to develop better models, researchers can use the prompts in TIP-I2V to analyze user preferences and evaluate the multi-dimensional performance of their trained models; and to enhance model safety, they may focus on addressing the misinformation issue caused by image-to-video models. The new research inspired by TIP-I2V and the differences with existing datasets emphasize the importance of a specialized image-to-video prompt dataset. The project is publicly available at https://tip-i2v.github.io.', 'score': 24, 'issue_id': 464, 'pub_date': '2024-11-05', 'pub_date_card': {'ru': '5 ноября', 'en': 'November 5', 'zh': '11月5日'}, 'hash': 'fcc8e4daf79a82b9', 'authors': ['Wenhao Wang', 'Yi Yang'], 'affiliations': ['University of Technology Sydney', 'Zhejiang University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04709.jpg', 'data': {'categories': ['#dataset', '#video'], 'emoji': '🎬', 'ru': {'title': 'TIP-I2V: Революция в изучении промптов для генерации видео из изображений', 'desc': 'Исследователи представили TIP-I2V - первый крупномасштабный набор данных, содержащий более 1,70 миллиона уникальных пользовательских текстовых и изображений-промптов для генерации видео из изображений. Датасет также включает соответствующие сгенерированные видео от пяти современных моделей преобразования изображений в видео. TIP-I2V позволяет анализировать предпочтения пользователей, оценивать многомерную производительность обученных моделей и решать проблемы безопасности, связанные с дезинформацией. Этот набор данных подчеркивает важность специализированного датасета промптов для генерации видео из изображений и открывает новые возможности для исследований в этой области.'}, 'en': {'title': 'Empowering Image-to-Video Generation with TIP-I2V Dataset', 'desc': 'This paper presents TIP-I2V, the first large-scale dataset containing over 1.70 million unique user-provided text and image prompts for image-to-video generation. The dataset aims to enhance the controllability and visual consistency of video generation models by providing a rich source of prompts. It also includes generated videos from five advanced image-to-video models, facilitating comparative analysis and model evaluation. By addressing the lack of dedicated datasets, TIP-I2V supports research in user preferences and model safety, particularly in mitigating misinformation issues.'}, 'zh': {'title': 'TIP-I2V：图像到视频生成的新数据集', 'desc': '视频生成模型正在改变内容创作，图像到视频模型因其更好的可控性和视觉一致性而受到关注。尽管这些模型很受欢迎，但目前缺乏专门用于研究用户提供的文本和图像提示的数据集。本文介绍了TIP-I2V，这是第一个大规模的数据集，包含超过170万个独特的用户提供的文本和图像提示，专门用于图像到视频生成。该数据集的推出将推动图像到视频研究的进展，帮助研究人员分析用户偏好并评估模型性能。'}}}, {'id': 'https://huggingface.co/papers/2411.05000', 'title': 'Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?', 'url': 'https://huggingface.co/papers/2411.05000', 'abstract': 'As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data.', 'score': 20, 'issue_id': 474, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '72ef4dc00d41e203', 'authors': ['Jonathan Roberts', 'Kai Han', 'Samuel Albanie'], 'affiliations': ['University of Cambridge', 'The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05000.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset', '#long_context'], 'emoji': '🧵', 'ru': {'title': 'Языковые модели в лабиринте длинного контекста: нити информации и границы понимания', 'desc': 'Исследование посвящено анализу способностей современных языковых моделей (LLM) эффективно использовать увеличенный контекст. Авторы провели серию экспериментов по извлечению информации, оценивая 17 ведущих LLM на способность следовать нескольким информационным потокам в контексте. Результаты показали, что многие модели способны эффективно обрабатывать несколько потоков одновременно, но для некоторых моделей эффективная длина контекста оказалась значительно меньше заявленной. Исследование также подчеркивает важность учета различий в токенизации при сравнении моделей.'}, 'en': {'title': 'Unlocking the Potential of Long-Context LLMs', 'desc': 'This paper investigates how well large language models (LLMs) can utilize their extended context capabilities for complex information retrieval and reasoning tasks. The authors conduct experiments with 17 leading LLMs to assess their ability to track multiple threads of information within their context windows. They discover that while many models can maintain performance across various threads, the effective context limit is often shorter than the maximum supported length, leading to decreased accuracy with larger contexts. Additionally, the study emphasizes the need for caution when comparing token counts from different tokenizers, as they can represent different amounts of text.'}, 'zh': {'title': '长上下文模型的潜力与挑战', 'desc': '随着大型语言模型（LLMs）上下文限制的增加，应用范围和下游功能也在扩大。许多现实任务的决策依赖于分散在不同文档中的细节，这些文档通常包含大量无关信息。长上下文LLMs在复杂信息检索和推理方面表现出色，但我们对它们如何有效利用上下文的理解仍然滞后。我们的实验表明，尽管许多模型在跟踪信息线程方面表现良好，但有效的上下文限制往往比支持的上下文长度要短，且随着上下文窗口的增大，准确性会下降。'}}}, {'id': 'https://huggingface.co/papers/2411.04923', 'title': 'VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos', 'url': 'https://huggingface.co/papers/2411.04923', 'abstract': 'Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed for fine-grained pixel-level grounding in videos based on user-provided textual inputs. Our design seamlessly connects three key components: a Large Language Model, a dual vision encoder that emphasizes both spatial and temporal details, and a spatio-temporal decoder for accurate mask generation. This connection is facilitated via tunable V-L and L-V adapters that enable close Vision-Language (VL) alignment. The architecture is trained to synchronize both spatial and temporal elements of video content with textual instructions. To enable fine-grained grounding, we curate a multimodal dataset featuring detailed visually-grounded conversations using a semiautomatic annotation pipeline, resulting in a diverse set of 38k video-QA triplets along with 83k objects and 671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation. Experimental results show that our model consistently outperforms existing approaches across all three tasks.', 'score': 20, 'issue_id': 474, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '3db2b6994e9c5047', 'authors': ['Shehan Munasinghe', 'Hanan Gani', 'Wenqi Zhu', 'Jiale Cao', 'Eric Xing', 'Fahad Shahbaz Khan', 'Salman Khan'], 'affiliations': ['Mohamed bin Zayed University of AI', 'Tianjin University', 'Carnegie Mellon University', 'Linköping University', 'Australian National University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04923.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#architecture', '#games', '#alignment', '#cv'], 'emoji': '🎥', 'ru': {'title': 'Точная локализация в видео с помощью мультимодального ИИ', 'desc': 'VideoGLaMM - это новая модель крупномасштабного мультимодального обучения, разработанная для точной пиксельной локализации в видео на основе текстовых запросов пользователя. Модель объединяет языковую модель, двойной энкодер зрения и пространственно-временной декодер для генерации масок. Для обучения модели был создан специальный набор данных с детальными визуально-обоснованными диалогами. Эксперименты показали превосходство VideoGLaMM над существующими подходами в задачах генерации обоснованных диалогов, визуальной локализации и сегментации видео по запросу.'}, 'en': {'title': 'Achieving Pixel-Level Precision in Video-Text Alignment', 'desc': 'This paper presents VideoGLaMM, a Large Multimodal Model (LMM) that enhances the alignment between videos and text at a fine-grained level. It addresses the challenges of pixel-level grounding by integrating a Large Language Model with a dual vision encoder that captures both spatial and temporal dynamics of video content. The model employs tunable adapters for effective Vision-Language alignment and is trained on a comprehensive multimodal dataset with 38k video-QA triplets. Experimental results demonstrate that VideoGLaMM outperforms existing models in tasks such as Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation.'}, 'zh': {'title': '视频与文本的精细对齐新突破', 'desc': '本论文介绍了一种名为VideoGLaMM的多模态模型，旨在实现视频与文本之间的精细像素级对齐。该模型结合了大型语言模型、双重视觉编码器和时空解码器，能够处理视频中的复杂空间和时间动态。通过可调的视觉-语言适配器，VideoGLaMM实现了视觉与语言的紧密对齐。实验结果表明，该模型在生成对话、视觉对齐和视频分割等任务上均优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2411.04496', 'title': 'Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large Language Model', 'url': 'https://huggingface.co/papers/2411.04496', 'abstract': 'To increase social bonding with interlocutors, humans naturally acquire the ability to respond appropriately in a given situation by considering which conversational skill is most suitable for the response - a process we call skill-of-mind. For large language model (LLM)-based conversational agents, planning appropriate conversational skills, as humans do, is challenging due to the complexity of social dialogue, especially in interactive scenarios. To address this, we propose a skill-of-mind-annotated conversation dataset, named Multifaceted Skill-of-Mind, which includes multi-turn and multifaceted conversational skills across various interactive scenarios (e.g., long-term, counseling, task-oriented), grounded in diverse social contexts (e.g., demographics, persona, rules of thumb). This dataset consists of roughly 100K conversations. Using this dataset, we introduce a new family of skill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B parameters. With extensive experiments, these models successfully demonstrate the skill-of-mind process and exhibit strong generalizability in inferring multifaceted skills across a variety of domains. Moreover, we show that Thanos significantly enhances the quality of responses generated by LLM-based conversational agents and promotes prosocial behavior in human evaluations.', 'score': 20, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'bf7486353434568f', 'authors': ['Young-Jun Lee', 'Dokyong Lee', 'Junyoung Youn', 'Kyeongjin Oh', 'Ho-Jin Choi'], 'affiliations': ['KAIST', 'KT Corporation'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04496.jpg', 'data': {'categories': ['#dataset', '#agents', '#multimodal'], 'emoji': '🗣️', 'ru': {'title': 'Языковые модели учатся искусству общения', 'desc': 'Исследователи представили новый набор данных под названием Multifaceted Skill-of-Mind, содержащий аннотации навыков ведения диалога в различных интерактивных сценариях. На основе этого набора данных была разработана серия языковых моделей Thanos, способных выбирать подходящие навыки общения в зависимости от контекста. Эксперименты показали, что модели Thanos успешно демонстрируют процесс выбора навыков и обладают хорошей обобщающей способностью в различных областях. Кроме того, использование Thanos значительно улучшило качество ответов диалоговых агентов на основе больших языковых моделей и способствовало более просоциальному поведению по оценкам людей.'}, 'en': {'title': 'Enhancing Conversational Skills in AI with Thanos', 'desc': 'This paper introduces a new dataset called Multifaceted Skill-of-Mind, which helps large language models (LLMs) learn how to respond appropriately in social conversations. The dataset contains around 100,000 conversations that cover various interactive scenarios and social contexts, allowing LLMs to understand different conversational skills. The authors also present a new family of LLMs named Thanos, which are designed to incorporate these skills into their responses. Through experiments, Thanos models show improved response quality and promote positive social interactions in conversations.'}, 'zh': {'title': '提升对话质量的技能思维模型', 'desc': '本文提出了一种名为多面技能思维的对话数据集，旨在帮助大型语言模型（LLM）更好地理解和应用社交对话中的适当回应技能。该数据集包含约10万条对话，涵盖了多轮和多方面的对话技能，适用于不同的互动场景，如长期对话、咨询和任务导向。我们还介绍了一种新的LLM家族，名为Thanos，具有1B、3B和8B参数规模，能够有效地展示技能思维过程，并在多种领域中推断多面技能。实验结果表明，Thanos显著提高了LLM对话代理生成的回应质量，并在人工评估中促进了亲社会行为。'}}}, {'id': 'https://huggingface.co/papers/2411.05001', 'title': 'Analyzing The Language of Visual Tokens', 'url': 'https://huggingface.co/papers/2411.05001', 'abstract': 'With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models.', 'score': 19, 'issue_id': 482, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '75768d92bd8ce17a', 'authors': ['David M. Chan', 'Rodolfo Corona', 'Joonyong Park', 'Cheol Jun Cho', 'Yutong Bai', 'Trevor Darrell'], 'affiliations': ['University of California, Berkeley', 'The University of Tokyo, Tokyo'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05001.jpg', 'data': {'categories': ['#multimodal', '#cv', '#interpretability', '#alignment'], 'emoji': '🖼️', 'ru': {'title': 'Визуальные языки: между Ципфом и хаосом', 'desc': 'Статья исследует статистические свойства дискретных визуальных языков, используемых в современных моделях компьютерного зрения. Авторы обнаружили, что визуальные языки следуют распределению Ципфа, но имеют более высокую энтропию и меньшую сжимаемость по сравнению с естественными языками. Визуальные токены в основном представляют части объектов, а не целые объекты или сцены. Исследование также показало, что визуальным языкам не хватает связной грамматической структуры и иерархической организации, характерной для естественных языков.'}, 'en': {'title': 'Unlocking the Secrets of Visual Languages in AI', 'desc': 'This paper explores the statistical properties of discrete visual languages used in transformer-based models for vision and language tasks. It reveals that these visual languages exhibit Zipfian distributions, but their token innovation leads to higher entropy and lower compression, primarily representing object parts. The study finds that visual languages lack cohesive grammatical structures, resulting in higher perplexity and less hierarchical organization compared to natural languages. Ultimately, the research highlights the importance of understanding these properties to improve the design of computer vision models.'}, 'zh': {'title': '揭示视觉语言的统计特性', 'desc': '本文探讨了基于变换器的视觉和语言任务模型（如LLaVA和Chameleon）中图像的离散标记表示。研究发现，视觉语言虽然遵循Zipf分布，但更高的标记创新会导致更大的熵和更低的压缩率，标记主要代表物体部分，显示出中间粒度。与自然语言相比，视觉语言缺乏连贯的语法结构，导致更高的困惑度和较弱的层次组织。通过这些实验，我们揭示了离散视觉语言的统计特性如何影响计算机视觉模型的设计。'}}}, {'id': 'https://huggingface.co/papers/2411.04999', 'title': 'DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation', 'url': 'https://huggingface.co/papers/2411.04999', 'abstract': "Significant progress has been made in open-vocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in real-world scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70% on non-stationary objects, which is more than a 2x improvement over state-of-the-art static systems. Our code as well as our experiment and deployment videos are open sourced and can be found on our project website: https://dynamem.github.io/", 'score': 16, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '47171ef52d95552a', 'authors': ['Peiqi Liu', 'Zhanqiu Guo', 'Mohit Warke', 'Soumith Chintala', 'Chris Paxton', 'Nur Muhammad Mahi Shafiullah', 'Lerrel Pinto'], 'affiliations': ['New York University', 'Hello Robot Inc.', 'Meta Inc.'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04999.jpg', 'data': {'categories': ['#robotics', '#3d', '#multimodal', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Динамическая память для роботов в изменяющемся мире', 'desc': 'DynaMem - новый подход к мобильной манипуляции с открытым словарем в динамических средах. Система использует динамическую пространственно-семантическую память для представления окружения робота, построенную на основе 3D структуры данных облаков точек. DynaMem применяет мультимодальные языковые модели и модели компьютерного зрения для локализации объектов по запросам на естественном языке. Эксперименты показали двукратное улучшение успешности захвата и перемещения нестационарных объектов по сравнению с современными статическими системами.'}, 'en': {'title': 'Empowering Robots with Dynamic Memory for Open-World Manipulation', 'desc': "This paper introduces DynaMem, a novel approach for open-vocabulary mobile manipulation that allows robots to adapt to dynamic environments. Unlike traditional systems that rely on static environments, DynaMem utilizes a dynamic spatio-semantic memory to keep track of changes in the robot's surroundings. It employs a 3D data structure to manage point clouds and leverages multimodal large language models (LLMs) for object localization. The results show that DynaMem significantly improves the robot's ability to interact with non-stationary objects, achieving a 70% success rate in pick-and-drop tasks, which is more than double the performance of existing static systems."}, 'zh': {'title': '动态记忆，智能操作！', 'desc': '本研究提出了一种新的动态空间语义记忆方法DynaMem，用于开放词汇的移动操作。与传统静态环境系统不同，DynaMem能够在不断变化的环境中进行物体定位和操作。该方法利用三维数据结构维护动态记忆，并通过多模态大语言模型进行查询。实验结果表明，DynaMem在非静态物体的抓取和放置任务中成功率达70%，显著优于现有静态系统。'}}}, {'id': 'https://huggingface.co/papers/2411.05007', 'title': 'SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models', 'url': 'https://huggingface.co/papers/2411.05007', 'abstract': 'Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, na\\"{\\i}vely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-Sigma, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5times, achieving 3.0times speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs. Our quantization library and inference engine are open-sourced.', 'score': 15, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '636ee9cbe15eefb6', 'authors': ['Muyang Li', 'Yujun Lin', 'Zhekai Zhang', 'Tianle Cai', 'Xiuyu Li', 'Junxian Guo', 'Enze Xie', 'Chenlin Meng', 'Jun-Yan Zhu', 'Song Han'], 'affiliations': ['MIT', 'UC Berkeley', 'NVIDIA', 'CMU', 'Princeton', 'SJTU', 'Pika Labs'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05007.jpg', 'data': {'categories': ['#inference', '#optimization', '#diffusion'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных моделей с помощью 4-битного квантования', 'desc': 'Статья представляет новый метод квантования под названием SVDQuant для ускорения диффузионных моделей. Метод использует низкоранговую ветвь для поглощения выбросов в весах и активациях, позволяя эффективно квантовать их до 4 бит. Авторы также разработали движок вывода Nunchaku, который оптимизирует выполнение квантованных моделей. Эксперименты показали значительное снижение использования памяти и ускорение работы крупных моделей генерации изображений.'}, 'en': {'title': 'Accelerating Diffusion Models with SVDQuant: Efficient 4-Bit Quantization', 'desc': 'This paper presents SVDQuant, a novel 4-bit quantization method designed to enhance the efficiency of diffusion models for image generation. As these models increase in size, they face challenges related to memory usage and latency, which SVDQuant aims to address by effectively managing outliers in weights and activations. The method utilizes Singular Value Decomposition (SVD) to absorb outliers into a low-rank branch, improving the quantization process without compromising image quality. Additionally, the co-designed inference engine, Nunchaku, optimizes memory access, resulting in significant reductions in memory usage and increased processing speed for large models.'}, 'zh': {'title': '加速扩散模型的4位量化新方法', 'desc': '扩散模型在生成高质量图像方面非常有效，但随着模型规模的增大，它们需要更多的内存并且延迟更高，这给部署带来了挑战。本文提出了一种新的4位量化方法SVDQuant，通过量化权重和激活值来加速扩散模型。与传统的平滑方法不同，SVDQuant通过低秩分支吸收异常值，从而减轻量化过程中的困难。实验结果表明，SVDQuant在保持图像质量的同时，显著减少了内存使用和提高了速度，适用于更多互动应用。'}}}, {'id': 'https://huggingface.co/papers/2411.04752', 'title': 'RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval', 'url': 'https://huggingface.co/papers/2411.04752', 'abstract': 'Code-mixing, the integration of lexical and grammatical elements from multiple languages within a single sentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman script, especially among migrant communities who form online groups to share relevant local information. This paper focuses on the challenges of extracting relevant information from code-mixed conversations, specifically within Roman transliterated Bengali mixed with English. This study presents a novel approach to address these challenges by developing a mechanism to automatically identify the most relevant answers from code-mixed conversations. We have experimented with a dataset comprising of queries and documents from Facebook, and Query Relevance files (QRels) to aid in this task. Our results demonstrate the effectiveness of our approach in extracting pertinent information from complex, code-mixed digital conversations, contributing to the broader field of natural language processing in multilingual and informal text environments. We use GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant documents to frame a mathematical model which helps to detect relevant documents corresponding to a query.', 'score': 15, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'e2dbb14c8f2ca6ef', 'authors': ['Aniket Deroy', 'Subhankar Maity'], 'affiliations': ['IIT Kharagpur, Kharagpur, India'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04752.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#data'], 'emoji': '🗣️', 'ru': {'title': 'Извлечение информации из многоязычных разговоров: новый подход к обработке смешанных текстов', 'desc': 'Статья посвящена проблеме извлечения релевантной информации из смешанных языковых разговоров в социальных сетях Индии. Авторы разработали новый подход для автоматического определения наиболее релевантных ответов в разговорах на смеси бенгальского и английского языков, записанных латиницей. В исследовании использовались данные из Facebook и файлы Query Relevance, а также применялась языковая модель GPT-3.5 Turbo. Результаты демонстрируют эффективность предложенного метода в обработке сложных многоязычных текстов в неформальной цифровой среде.'}, 'en': {'title': 'Enhancing Information Retrieval in Code-Mixed Conversations', 'desc': 'This paper addresses the challenge of extracting relevant information from code-mixed conversations, particularly in Roman transliterated Bengali and English. It presents a novel mechanism that utilizes GPT-3.5 Turbo and a mathematical model to identify pertinent answers from social media interactions. The study experiments with a dataset from Facebook, focusing on queries and documents to enhance information retrieval. The results indicate that the proposed approach effectively navigates the complexities of multilingual and informal text, contributing to advancements in natural language processing.'}, 'zh': {'title': '从代码混合对话中提取信息的新方法', 'desc': '本文研究了在多语言环境中，如何从代码混合的对话中提取相关信息。特别是在印度，社交媒体用户常用罗马字母书写的孟加拉语与英语混合交流。我们提出了一种新方法，通过自动识别代码混合对话中的相关答案来解决这一挑战。实验结果表明，该方法在提取复杂数字对话中的相关信息方面是有效的，推动了自然语言处理领域的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.04075', 'title': 'M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models', 'url': 'https://huggingface.co/papers/2411.04075', 'abstract': 'Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address this gap, we introduce M3SciQA, a multi-modal, multi-document scientific question answering benchmark designed for a more comprehensive evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, where each cluster represents a primary paper along with all its cited documents, mirroring the workflow of comprehending a single paper by requiring multi-modal and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of 18 foundation models. Our results indicate that current foundation models still significantly underperform compared to human experts in multi-modal information retrieval and in reasoning across multiple scientific documents. Additionally, we explore the implications of these findings for the future advancement of applying foundation models in multi-modal scientific literature analysis.', 'score': 14, 'issue_id': 480, 'pub_date': '2024-11-06', 'pub_date_card': {'ru': '6 ноября', 'en': 'November 6', 'zh': '11月6日'}, 'hash': '654e787a9f0ab7ff', 'authors': ['Chuhan Li', 'Ziyao Shangguan', 'Yilun Zhao', 'Deyuan Li', 'Yixin Liu', 'Arman Cohan'], 'affiliations': [], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04075.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#science'], 'emoji': '🧠', 'ru': {'title': 'M3SciQA: Новый рубеж в оценке фундаментальных моделей для научного анализа', 'desc': 'M3SciQA - это новый бенчмарк для оценки фундаментальных моделей в области научных вопросов и ответов. Он включает в себя мультимодальные и мультидокументные задачи, охватывающие 70 кластеров статей по обработке естественного языка. Результаты тестирования 18 моделей показали, что их производительность значительно уступает экспертам-людям в задачах мультимодального поиска информации и рассуждений по нескольким научным документам. Этот бенчмарк открывает новые перспективы для развития применения фундаментальных моделей в анализе научной литературы.'}, 'en': {'title': 'M3SciQA: Bridging the Gap in Multi-Modal Scientific Understanding', 'desc': 'This paper introduces M3SciQA, a new benchmark for evaluating foundation models in the context of scientific question answering. Unlike existing benchmarks that focus solely on single-document, text-only tasks, M3SciQA incorporates multi-modal and multi-document elements to better reflect real research workflows. The benchmark includes 1,452 expert-annotated questions related to clusters of natural language processing papers, requiring models to interpret both textual and non-textual data. Evaluation results show that current foundation models still lag behind human experts in retrieving multi-modal information and reasoning across multiple documents, highlighting the need for further advancements in this area.'}, 'zh': {'title': 'M3SciQA：多模态科学问答的新基准', 'desc': '现有的基础模型评估基准主要集中在单文档、文本任务上，无法全面捕捉研究工作流程的复杂性。为了解决这个问题，我们提出了M3SciQA，这是一个多模态、多文档的科学问答基准，旨在更全面地评估基础模型。M3SciQA包含1452个专家注释的问题，涵盖70个自然语言处理论文集群，模拟理解单篇论文的工作流程。我们的评估结果表明，当前的基础模型在多模态信息检索和跨多个科学文档推理方面仍显著低于人类专家的表现。'}}}, {'id': 'https://huggingface.co/papers/2411.04335', 'title': 'GazeGen: Gaze-Driven User Interaction for Visual Content Generation', 'url': 'https://huggingface.co/papers/2411.04335', 'abstract': "We present GazeGen, a user interaction system that generates visual content (images and videos) for locations indicated by the user's eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and generative AI, GazeGen performs gaze-controlled image adding/deleting, repositioning, and surface material changes of image objects, and converts static images into videos. Central to GazeGen is the DFT Gaze (Distilled and Fine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters, performing accurate real-time gaze predictions tailored to individual users' eyes on small edge devices. GazeGen is the first system to combine visual content generation with real-time gaze estimation, made possible exclusively by DFT Gaze. This real-time gaze estimation enables various visual content generation tasks, all controlled by the user's gaze. The input for DFT Gaze is the user's eye images, while the inputs for visual content generation are the user's view and the predicted gaze point from DFT Gaze. To achieve efficient gaze predictions, we derive the small model from a large model (10x larger) via novel knowledge distillation and personal adaptation techniques. We integrate knowledge distillation with a masked autoencoder, developing a compact yet powerful gaze estimation model. This model is further fine-tuned with Adapters, enabling highly accurate and personalized gaze predictions with minimal user input. DFT Gaze ensures low-latency and precise gaze tracking, supporting a wide range of gaze-driven tasks. We validate the performance of DFT Gaze on AEA and OpenEDS2020 benchmarks, demonstrating low angular gaze error and low latency on the edge device (Raspberry Pi 4). Furthermore, we describe applications of GazeGen, illustrating its versatility and effectiveness in various usage scenarios.", 'score': 14, 'issue_id': 466, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'caa85d9f2385fc2a', 'authors': ['He-Yen Hsieh', 'Ziyun Li', 'Sai Qian Zhang', 'Wei-Te Mark Ting', 'Kao-Den Chang', 'Barbara De Salvo', 'Chiao Liu', 'H. T. Kung'], 'affiliations': ['Reality Labs Research, Meta', 'New York University', 'Harvard University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04335.jpg', 'data': {'categories': ['#agents', '#cv', '#video', '#edge_computing', '#training'], 'emoji': '👁️', 'ru': {'title': 'Взглядом управляй: революция в генерации визуального контента', 'desc': 'Исследователи представляют систему GazeGen, которая генерирует визуальный контент на основе направления взгляда пользователя. В основе системы лежит сверхлегкая модель DFT Gaze, выполняющая точное предсказание направления взгляда в реальном времени на небольших устройствах. GazeGen позволяет интуитивно манипулировать визуальным контентом, используя передовые методы обнаружения объектов и генеративного ИИ. Система была протестирована на эталонных наборах данных, продемонстрировав низкую угловую ошибку определения взгляда и низкую задержку на периферийных устройствах.'}, 'en': {'title': 'GazeGen: Transforming Eye Gaze into Visual Content Control', 'desc': 'GazeGen is an innovative user interaction system that generates visual content based on where a user is looking. It utilizes advanced object detection and generative AI techniques to allow users to manipulate images and videos by simply gazing at specific areas. The core of GazeGen is the DFT Gaze agent, a lightweight model that accurately predicts gaze in real-time, making it suitable for use on small devices like the Raspberry Pi 4. By combining gaze estimation with visual content generation, GazeGen enables intuitive and personalized interactions with digital media.'}, 'zh': {'title': '眼动控制的视觉内容生成系统', 'desc': 'GazeGen是一个用户交互系统，可以根据用户的眼动生成视觉内容（图像和视频）。它利用先进的物体检测和生成AI技术，实现了基于视线的图像添加、删除、重新定位和表面材质变化。系统的核心是DFT Gaze代理，这是一个轻量级模型，能够在小型边缘设备上进行实时的眼动预测。GazeGen是首个将视觉内容生成与实时眼动估计相结合的系统，展示了其在多种应用场景中的灵活性和有效性。'}}}, {'id': 'https://huggingface.co/papers/2411.05005', 'title': 'Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models', 'url': 'https://huggingface.co/papers/2411.05005', 'abstract': 'Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or as mere feature extractors. In contrast to these isolated and thus sub-optimal efforts, we introduce a unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through a unique exploitation of the diffusion-denoising process. Within this framework, we further enhance discriminative visual perception via multi-modal generation, by utilizing the denoising network to create multi-modal data that mirror the distribution of the original training set. Importantly, Diff-2-in-1 optimizes the utilization of the created diverse and faithful data by leveraging a novel self-improving learning mechanism. Comprehensive experimental evaluations validate the effectiveness of our framework, showcasing consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation characterized by both realism and usefulness.', 'score': 13, 'issue_id': 477, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '249deab8440380fc', 'authors': ['Shuhong Zheng', 'Zhipeng Bao', 'Ruoyu Zhao', 'Martial Hebert', 'Yu-Xiong Wang'], 'affiliations': ['University of Illinois Urbana-Champaign', 'Carnegie Mellon University', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05005.jpg', 'data': {'categories': ['#training', '#diffusion', '#data', '#multimodal', '#optimization', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Diff-2-in-1: Объединение генерации и восприятия в диффузионных моделях', 'desc': 'Статья представляет новый универсальный фреймворк на основе диффузионных моделей под названием Diff-2-in-1. Этот фреймворк способен одновременно выполнять мультимодальную генерацию данных и плотное визуальное восприятие, используя процесс диффузионного шумоподавления. Diff-2-in-1 улучшает дискриминативное визуальное восприятие путем генерации мультимодальных данных, отражающих распределение исходного обучающего набора. Фреймворк также использует новый механизм самосовершенствующегося обучения для оптимизации использования созданных разнообразных и достоверных данных.'}, 'en': {'title': 'Unifying Data Generation and Visual Perception with Diff-2-in-1', 'desc': 'This paper presents a new framework called Diff-2-in-1 that integrates diffusion models for both data generation and visual perception tasks. Unlike previous approaches that used diffusion models in isolation, this framework leverages the diffusion-denoising process to enhance the performance of visual perception. It generates multi-modal data that closely resembles the original training data, improving the discriminative capabilities of the model. The framework also includes a self-improving learning mechanism to optimize the use of the generated data, leading to better performance across various tasks.'}, 'zh': {'title': 'Diff-2-in-1：多模态生成与视觉感知的统一框架', 'desc': '本文提出了一种新的框架，称为Diff-2-in-1，旨在同时处理多模态数据生成和密集视觉感知。与以往将扩散模型视为独立组件的做法不同，该框架利用扩散去噪过程的独特特性，增强了视觉感知的判别能力。通过生成与原始训练集分布相似的多模态数据，Diff-2-in-1优化了生成数据的使用，采用了一种新颖的自我改进学习机制。实验结果表明，该框架在多种判别模型上均表现出一致的性能提升，并生成了高质量的多模态数据，具有真实感和实用性。'}}}, {'id': 'https://huggingface.co/papers/2411.04989', 'title': 'SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2411.04989', 'abstract': 'Methods for image-to-video generation have achieved impressive, photo-realistic quality. However, adjusting specific elements in generated videos, such as object motion or camera movement, is often a tedious process of trial and error, e.g., involving re-generating videos with different random seeds. Recent techniques address this issue by fine-tuning a pre-trained model to follow conditioning signals, such as bounding boxes or point trajectories. Yet, this fine-tuning procedure can be computationally expensive, and it requires datasets with annotated object motion, which can be difficult to procure. In this work, we introduce SG-I2V, a framework for controllable image-to-video generation that is self-guidedx2013offering zero-shot control by relying solely on the knowledge present in a pre-trained image-to-video diffusion model without the need for fine-tuning or external knowledge. Our zero-shot method outperforms unsupervised baselines while being competitive with supervised models in terms of visual quality and motion fidelity.', 'score': 13, 'issue_id': 465, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': 'a707043470b8dffd', 'authors': ['Koichi Namekata', 'Sherwin Bahmani', 'Ziyi Wu', 'Yash Kant', 'Igor Gilitschenski', 'David B. Lindell'], 'affiliations': ['University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.04989.jpg', 'data': {'categories': ['#video', '#diffusion', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Управляемая генерация видео без дополнительного обучения', 'desc': 'SG-I2V - это новый фреймворк для управляемой генерации видео из изображений. Он использует предобученную диффузионную модель без необходимости дополнительного обучения или внешних данных. Метод превосходит неконтролируемые базовые модели и конкурирует с контролируемыми по качеству изображения и точности движения. SG-I2V позволяет легко настраивать конкретные элементы генерируемых видео, такие как движение объектов или камеры.'}, 'en': {'title': 'Zero-Shot Control in Image-to-Video Generation', 'desc': 'This paper presents SG-I2V, a novel framework for generating videos from images with controllable features. Unlike traditional methods that require fine-tuning on annotated datasets, SG-I2V operates in a zero-shot manner, leveraging a pre-trained image-to-video diffusion model. This approach allows for easier manipulation of elements like object motion and camera movement without the computational costs associated with fine-tuning. The results show that SG-I2V achieves high visual quality and motion fidelity, outperforming unsupervised methods and competing with supervised ones.'}, 'zh': {'title': 'SG-I2V：高效的图像到视频生成方法', 'desc': '本文介绍了一种名为SG-I2V的框架，用于可控的图像到视频生成。该方法利用预训练的图像到视频扩散模型，提供零-shot控制，避免了繁琐的微调过程。与无监督基线相比，我们的方法在视觉质量和运动保真度上表现优越，并且与监督模型相竞争。此研究为视频生成提供了一种更高效的解决方案，减少了对标注数据集的依赖。'}}}, {'id': 'https://huggingface.co/papers/2411.10440', 'title': 'LLaVA-o1: Let Vision Language Models Reason Step-by-Step', 'url': 'https://huggingface.co/papers/2411.10440', 'abstract': "Large language models have demonstrated substantial advancements in reasoning capabilities, particularly through inference-time scaling, as illustrated by models such as OpenAI's o1. However, current Vision-Language Models (VLMs) often struggle to perform systematic and structured reasoning, especially when handling complex visual question-answering tasks. In this work, we introduce LLaVA-o1, a novel VLM designed to conduct autonomous multistage reasoning. Unlike chain-of-thought prompting, LLaVA-o1 independently engages in sequential stages of summarization, visual interpretation, logical reasoning, and conclusion generation. This structured approach enables LLaVA-o1 to achieve marked improvements in precision on reasoning-intensive tasks. To accomplish this, we compile the LLaVA-o1-100k dataset, integrating samples from various visual question answering sources and providing structured reasoning annotations. Besides, we propose an inference-time stage-level beam search method, which enables effective inference-time scaling. Remarkably, with only 100k training samples and a simple yet effective inference time scaling method, LLaVA-o1 not only outperforms its base model by 8.9% on a wide range of multimodal reasoning benchmarks, but also surpasses the performance of larger and even closed-source models, such as Gemini-1.5-pro, GPT-4o-mini, and Llama-3.2-90B-Vision-Instruct.", 'score': 60, 'issue_id': 627, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': 'a706d5cae9964c9e', 'authors': ['Guowei Xu', 'Peng Jin', 'Li Hao', 'Yibing Song', 'Lichao Sun', 'Li Yuan'], 'affiliations': ['School of Electronic and Computer Engineering, Peking University', 'Institute for Interdisciplinary Information Sciences, Tsinghua University', 'Peng Cheng Laboratory', 'AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School', 'Alibaba DAMO Academy', 'Computer Science and Engineering, Lehigh University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10440.jpg', 'data': {'categories': ['#dataset', '#inference', '#reasoning', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'LLaVA-o1: Прорыв в автономном визуальном рассуждении', 'desc': 'LLaVA-o1 - это новая мультимодальная языковая модель, способная к автономному многоступенчатому рассуждению. В отличие от других подходов, LLaVA-o1 самостоятельно выполняет последовательные этапы обобщения, визуальной интерпретации, логического рассуждения и формулирования выводов. Для обучения модели был создан датасет LLaVA-o1-100k со структурированными аннотациями рассуждений. Несмотря на небольшой объем обучающих данных, LLaVA-o1 превзошла более крупные модели на ряде тестов мультимодальных рассуждений.'}, 'en': {'title': 'LLaVA-o1: Revolutionizing Visual Reasoning with Structured Multistage Processing', 'desc': 'This paper presents LLaVA-o1, a new Vision-Language Model (VLM) that enhances reasoning capabilities in visual question-answering tasks. Unlike traditional methods that rely on chain-of-thought prompting, LLaVA-o1 autonomously performs multistage reasoning, which includes summarization, visual interpretation, logical reasoning, and conclusion generation. The model is trained on the LLaVA-o1-100k dataset, which contains diverse visual question-answering samples with structured reasoning annotations. With an innovative inference-time stage-level beam search method, LLaVA-o1 achieves significant improvements in precision, outperforming both its base model and larger closed-source models with only 100k training samples.'}, 'zh': {'title': 'LLaVA-o1：自主多阶段推理的视觉语言模型', 'desc': '本论文介绍了一种新型的视觉语言模型LLaVA-o1，旨在进行自主的多阶段推理。与传统的链式思维提示不同，LLaVA-o1能够独立进行总结、视觉解读、逻辑推理和结论生成等多个阶段。通过这种结构化的方法，LLaVA-o1在推理密集型任务上显著提高了准确性。我们还构建了LLaVA-o1-100k数据集，并提出了一种有效的推理时间阶段级束搜索方法，以实现推理时间的有效扩展。'}}}, {'id': 'https://huggingface.co/papers/2411.06558', 'title': 'Region-Aware Text-to-Image Generation via Hard Binding and Soft Refinement', 'url': 'https://huggingface.co/papers/2411.06558', 'abstract': 'In this paper, we present RAG, a Regional-Aware text-to-image Generation method conditioned on regional descriptions for precise layout composition. Regional prompting, or compositional generation, which enables fine-grained spatial control, has gained increasing attention for its practicality in real-world applications. However, previous methods either introduce additional trainable modules, thus only applicable to specific models, or manipulate on score maps within cross-attention layers using attention masks, resulting in limited control strength when the number of regions increases. To handle these limitations, we decouple the multi-region generation into two sub-tasks, the construction of individual region (Regional Hard Binding) that ensures the regional prompt is properly executed, and the overall detail refinement (Regional Soft Refinement) over regions that dismiss the visual boundaries and enhance adjacent interactions. Furthermore, RAG novelly makes repainting feasible, where users can modify specific unsatisfied regions in the last generation while keeping all other regions unchanged, without relying on additional inpainting models. Our approach is tuning-free and applicable to other frameworks as an enhancement to the prompt following property. Quantitative and qualitative experiments demonstrate that RAG achieves superior performance over attribute binding and object relationship than previous tuning-free methods.', 'score': 23, 'issue_id': 641, 'pub_date': '2024-11-10', 'pub_date_card': {'ru': '10 ноября', 'en': 'November 10', 'zh': '11月10日'}, 'hash': 'f4d24a7a6c0b27a0', 'authors': ['Zhennan Chen', 'Yajie Li', 'Haofan Wang', 'Zhibo Chen', 'Zhengkai Jiang', 'Jun Li', 'Qian Wang', 'Jian Yang', 'Ying Tai'], 'affiliations': ['China Mobile', 'HKUST', 'InstantX', 'Liblib AI', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2411.06558.jpg', 'data': {'categories': ['#rag', '#cv', '#multimodal', '#optimization', '#games'], 'emoji': '🎨', 'ru': {'title': 'Точный контроль над генерацией изображений с помощью региональных описаний', 'desc': 'В этой статье представлен метод RAG (Regional-Aware Generation) для генерации изображений по текстовому описанию с учетом региональных особенностей. RAG разделяет процесс на два этапа: создание отдельных регионов и общую доработку деталей. Метод позволяет точно контролировать пространственное расположение объектов без дополнительного обучения модели. RAG также дает возможность перерисовывать отдельные неудовлетворительные области изображения, сохраняя остальные неизменными.'}, 'en': {'title': 'RAG: Precision in Image Generation through Regional Awareness', 'desc': 'This paper introduces RAG, a method for generating images from text that focuses on specific regions of the image for better layout control. It addresses limitations of previous methods by breaking down the generation process into two tasks: creating individual regions accurately and refining the overall image details. RAG allows users to modify specific areas of an image without affecting others, making it user-friendly and flexible. The method is designed to work without additional training and shows improved performance in generating images with clear attributes and relationships compared to earlier techniques.'}, 'zh': {'title': '区域感知生成，精确布局新方法', 'desc': '本文提出了一种名为RAG的区域感知文本到图像生成方法，旨在通过区域描述实现精确的布局组合。该方法通过区域提示和组合生成，提供了细粒度的空间控制，适用于实际应用。与以往方法不同，RAG将多区域生成分解为两个子任务，确保区域提示的有效执行和整体细节的优化。RAG还支持用户在最后生成中修改特定区域，而无需依赖额外的修复模型，展现了优越的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.08033', 'title': 'GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation', 'url': 'https://huggingface.co/papers/2411.08033', 'abstract': 'While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our framework employs a Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single/multi-view image inputs. Notably, the newly proposed latent space naturally enables geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate the effectiveness of our approach on multiple datasets, outperforming existing methods in both text- and image-conditioned 3D generation.', 'score': 19, 'issue_id': 627, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': '9c28ee6de37c05b2', 'authors': ['Yushi Lan', 'Shangchen Zhou', 'Zhaoyang Lyu', 'Fangzhou Hong', 'Shuai Yang', 'Bo Dai', 'Xingang Pan', 'Chen Change Loy'], 'affiliations': ['S-Lab, Nanyang Technological University, Singapore', 'Shanghai Artificial Intelligence Laboratory', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2411.08033.jpg', 'data': {'categories': ['#synthetic', '#3d', '#multimodal', '#diffusion'], 'emoji': '🧊', 'ru': {'title': 'Революция в 3D-генерации: от точек к реальности', 'desc': 'Эта статья представляет новый фреймворк для генерации 3D-контента, который решает существующие проблемы с форматами ввода, дизайном латентного пространства и представлением выходных данных. Предложенный метод, названный GaussianAnything, использует вариационный автоэнкодер (VAE) с многоракурсными RGB-D-N рендерингами в качестве входных данных и уникальным дизайном латентного пространства, сохраняющим информацию о 3D-форме. Фреймворк поддерживает мультимодальную условную 3D-генерацию, позволяя использовать облака точек, текстовые описания и одиночные/многоракурсные изображения в качестве входных данных. Экспериментальные результаты показывают эффективность подхода на нескольких наборах данных, превосходя существующие методы в 3D-генерации, обусловленной текстом и изображениями.'}, 'en': {'title': 'Revolutionizing 3D Generation with GaussianAnything', 'desc': 'This paper presents a new framework for generating 3D content that overcomes limitations in current methods related to input formats and latent space design. It utilizes a Variational Autoencoder (VAE) that takes multi-view RGB-D-Normal renderings as input, creating a structured latent space that maintains essential 3D shape details. The framework also features a cascaded latent diffusion model to separate shape and texture effectively, enhancing the quality of generated 3D models. The proposed method, called GaussianAnything, allows for flexible 3D generation based on various input types, demonstrating superior performance in experiments compared to existing techniques.'}, 'zh': {'title': '创新3D生成框架：解耦形状与纹理', 'desc': '本论文提出了一种新颖的3D生成框架，旨在解决现有方法在输入格式、潜在空间设计和输出表示方面的挑战。该框架采用变分自编码器（VAE），以多视角的RGB-D（深度）-N（法线）渲染作为输入，独特的潜在空间设计能够保留3D形状信息。通过引入级联潜在扩散模型，改进了形状与纹理的解耦。实验结果表明，该方法在多个数据集上表现优异，超越了现有的文本和图像条件下的3D生成方法。'}}}, {'id': 'https://huggingface.co/papers/2411.10323', 'title': 'The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use', 'url': 'https://huggingface.co/papers/2411.10323', 'abstract': "The recently released model, Claude 3.5 Computer Use, stands out as the first frontier AI model to offer computer use in public beta as a graphical user interface (GUI) agent. As an early beta, its capability in the real-world complex environment remains unknown. In this case study to explore Claude 3.5 Computer Use, we curate and organize a collection of carefully designed tasks spanning a variety of domains and software. Observations from these cases demonstrate Claude 3.5 Computer Use's unprecedented ability in end-to-end language to desktop actions. Along with this study, we provide an out-of-the-box agent framework for deploying API-based GUI automation models with easy implementation. Our case studies aim to showcase a groundwork of capabilities and limitations of Claude 3.5 Computer Use with detailed analyses and bring to the fore questions about planning, action, and critic, which must be considered for future improvement. We hope this preliminary exploration will inspire future research into the GUI agent community. All the test cases in the paper can be tried through the project: https://github.com/showlab/computer_use_ootb.", 'score': 14, 'issue_id': 631, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': 'fc2174371c27ab64', 'authors': ['Siyuan Hu', 'Mingyu Ouyang', 'Difei Gao', 'Mike Zheng Shou'], 'affiliations': ['Shou Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2411.10323.jpg', 'data': {'categories': ['#dataset', '#agents'], 'emoji': '🖥️', 'ru': {'title': 'Claude 3.5: Революция в автоматизации пользовательского интерфейса', 'desc': 'В статье представлен анализ модели Claude 3.5 Computer Use - первого публично доступного ИИ-агента с графическим интерфейсом. Авторы провели ряд тестов в различных доменах и программах для оценки возможностей модели. Результаты демонстрируют беспрецедентные способности Claude 3.5 в преобразовании естественного языка в действия на рабочем столе. Также предложена готовая к использованию фреймворк для развертывания моделей автоматизации GUI на основе API.'}, 'en': {'title': 'Exploring the Future of GUI Agents with Claude 3.5', 'desc': "The paper introduces Claude 3.5 Computer Use, a pioneering AI model that functions as a graphical user interface (GUI) agent in public beta. It presents a case study that evaluates the model's performance across various tasks and software applications, highlighting its ability to translate natural language commands into desktop actions. The study also provides a framework for deploying API-based GUI automation models, making it easier for developers to implement similar systems. Through detailed analyses, the paper discusses the capabilities and limitations of Claude 3.5, raising important questions for future research in the GUI agent field."}, 'zh': {'title': '探索Claude 3.5：前沿AI的GUI代理能力', 'desc': 'Claude 3.5计算机使用模型是首个提供图形用户界面（GUI）代理的前沿人工智能模型。本文通过设计一系列任务，探索其在复杂环境中的表现。研究表明，Claude 3.5在语言到桌面操作的端到端能力上具有前所未有的表现。我们还提供了一个易于实现的API基础的GUI自动化模型框架，以便于部署和测试。'}}}, {'id': 'https://huggingface.co/papers/2411.10332', 'title': 'Number it: Temporal Grounding Videos like Flipping Manga', 'url': 'https://huggingface.co/papers/2411.10332', 'abstract': 'Video Large Language Models (Vid-LLMs) have made remarkable advancements in comprehending video content for QA dialogue. However, they struggle to extend this visual understanding to tasks requiring precise temporal localization, known as Video Temporal Grounding (VTG). To address this gap, we introduce Number-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual comprehension with temporal grounding by adding unique numerical identifiers to each video frame. Treating a video as a sequence of numbered frame images, NumPro transforms VTG into an intuitive process: flipping through manga panels in sequence. This allows Vid-LLMs to "read" event timelines, accurately linking visual content with corresponding temporal information. Our experiments demonstrate that NumPro significantly boosts VTG performance of top-tier Vid-LLMs without additional computational cost. Furthermore, fine-tuning on a NumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing previous top-performing methods by up to 6.9\\% in mIoU for moment retrieval and 8.5\\% in mAP for highlight detection. The code will be available at https://github.com/yongliang-wu/NumPro.', 'score': 9, 'issue_id': 631, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': '92d2bcabe629d81c', 'authors': ['Yongliang Wu', 'Xinting Hu', 'Yuyang Sun', 'Yizhou Zhou', 'Wenbo Zhu', 'Fengyun Rao', 'Bernt Schiele', 'Xu Yang'], 'affiliations': ['WeChat, Tencent Inc.', 'Max Planck Institute for Informatics', 'University of California, Berkeley', 'Southeast University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10332.jpg', 'data': {'categories': ['#multimodal', '#training', '#optimization', '#video', '#games'], 'emoji': '🎬', 'ru': {'title': 'NumPro: Помогаем ИИ ориентироваться во времени видео', 'desc': "Video-LLM достигли значительных успехов в понимании видеоконтента для диалогов вопросов и ответов, но испытывают трудности с точной временной локализацией. Для решения этой проблемы авторы предлагают метод Number-Prompt (NumPro), который добавляет уникальные числовые идентификаторы к каждому кадру видео. Это позволяет Video-LLM 'читать' временные линии событий, точно связывая визуальный контент с соответствующей временной информацией. Эксперименты показывают, что NumPro значительно повышает производительность Video-LLM в задачах временной локализации видео без дополнительных вычислительных затрат."}, 'en': {'title': 'Bridging Visual Understanding and Temporal Grounding in Videos', 'desc': 'This paper presents a new method called Number-Prompt (NumPro) to improve Video Temporal Grounding (VTG) in Video Large Language Models (Vid-LLMs). Vid-LLMs have difficulty linking visual content with specific timeframes, which is crucial for tasks like question answering about videos. NumPro addresses this by assigning unique numerical identifiers to each video frame, allowing the model to process video as a sequence of numbered images, similar to reading manga panels. The results show that NumPro enhances the performance of Vid-LLMs in VTG tasks significantly, achieving state-of-the-art results without increasing computational costs.'}, 'zh': {'title': '数字提示：提升视频时间定位的创新方法', 'desc': '视频大型语言模型（Vid-LLMs）在理解视频内容方面取得了显著进展，但在需要精确时间定位的任务（视频时间定位，VTG）中仍然存在困难。为了解决这个问题，我们提出了一种新方法——数字提示（NumPro），通过为每个视频帧添加独特的数字标识符，帮助Vid-LLMs将视觉理解与时间定位结合起来。NumPro将VTG转化为一种直观的过程，类似于按顺序翻阅漫画面板，使Vid-LLMs能够“阅读”事件时间线，准确地将视觉内容与相应的时间信息关联。实验表明，NumPro显著提升了顶级Vid-LLMs在VTG任务中的表现，且没有额外的计算成本。'}}}, {'id': 'https://huggingface.co/papers/2411.10083', 'title': 'Xmodel-1.5: An 1B-scale Multilingual LLM', 'url': 'https://huggingface.co/papers/2411.10083', 'abstract': "We introduce Xmodel-1.5, a novel 1-billion-parameter multilingual large model pretrained on approximately 2 trillion tokens. The model demonstrates strong performance across several languages, with particularly notable results in Thai, Arabic, and French, alongside its effectiveness in Chinese and English. In addition, we contribute to the research community by releasing a Thai evaluation dataset, which includes hundreds of questions annotated by students from Chulalongkorn University's School of Integrated Innovation. While the results are promising, we acknowledge that there is still room for improvement. We hope this work advances ongoing efforts in multilingual AI research and promotes better cross-linguistic understanding in various natural language processing tasks. Our models and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelLM.", 'score': 7, 'issue_id': 641, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': '745c93b05bda9f01', 'authors': ['Wang Qun', 'Liu Yang', 'Lin Qingquan', 'Jiang Ling'], 'affiliations': ['XiaoduoAI'], 'pdf_title_img': 'assets/pdf/title_img/2411.10083.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#open_source', '#small_models', '#low_resource'], 'emoji': '🌍', 'ru': {'title': 'Xmodel-1.5: Мультиязычный ИИ для лучшего понимания между культурами', 'desc': 'Исследователи представили Xmodel-1.5 - новую мультиязычную большую языковую модель с 1 миллиардом параметров, обученную на 2 триллионах токенов. Модель демонстрирует высокую производительность на нескольких языках, особенно на тайском, арабском и французском, наряду с китайским и английским. Авторы также выпустили набор данных для оценки на тайском языке, содержащий сотни вопросов, аннотированных студентами. Модель и код доступны в открытом доступе на GitHub.'}, 'en': {'title': 'Empowering Multilingual AI with Xmodel-1.5', 'desc': 'Xmodel-1.5 is a large multilingual model with 1 billion parameters, trained on a massive dataset of 2 trillion tokens. It shows impressive performance in multiple languages, especially in Thai, Arabic, and French, while also being effective in Chinese and English. The authors provide a new evaluation dataset for Thai, created with input from students, to aid in further research. This work aims to enhance multilingual AI capabilities and foster better understanding across different languages in natural language processing tasks.'}, 'zh': {'title': '推动多语言AI研究的前沿', 'desc': '我们介绍了Xmodel-1.5，这是一个新型的十亿参数多语言大模型，预训练于大约2万亿个标记上。该模型在多种语言上表现出色，尤其在泰语、阿拉伯语和法语方面的结果尤为显著，同时在中文和英文中也表现良好。此外，我们还为研究社区贡献了一个泰语评估数据集，包含由朱拉隆功大学综合创新学院的学生标注的数百个问题。尽管结果令人鼓舞，但我们承认仍有改进的空间，希望这项工作能推动多语言人工智能研究的进展，并促进各种自然语言处理任务中的跨语言理解。'}}}, {'id': 'https://huggingface.co/papers/2411.10438', 'title': 'MARS: Unleashing the Power of Variance Reduction for Training Large Models', 'url': 'https://huggingface.co/papers/2411.10438', 'abstract': 'Training deep neural networks--and more recently, large models--demands efficient and scalable optimizers. Adaptive gradient algorithms like Adam, AdamW, and their variants have been central to this task. Despite the development of numerous variance reduction algorithms in the past decade aimed at accelerating stochastic optimization in both convex and nonconvex settings, variance reduction has not found widespread success in training deep neural networks or large language models. Consequently, it has remained a less favored approach in modern AI. In this paper, to unleash the power of variance reduction for efficient training of large models, we propose a unified optimization framework, MARS (Make vAriance Reduction Shine), which reconciles preconditioned gradient methods with variance reduction via a scaled stochastic recursive momentum technique. Within our framework, we introduce three instances of MARS that leverage preconditioned gradient updates based on AdamW, Lion, and Shampoo, respectively. We also draw a connection between our algorithms and existing optimizers. Experimental results on training GPT-2 models indicate that MARS consistently outperforms AdamW by a large margin.', 'score': 1, 'issue_id': 646, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': 'c08911f01fcd1d00', 'authors': ['Huizhuo Yuan', 'Yifeng Liu', 'Shuang Wu', 'Xun Zhou', 'Quanquan Gu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2411.10438.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'MARS: Новый подход к оптимизации для эффективного обучения больших моделей', 'desc': 'Статья представляет унифицированный оптимизационный фреймворк MARS, объединяющий методы предобусловленного градиента с редукцией дисперсии. Авторы предлагают три варианта MARS, основанные на AdamW, Lion и Shampoo. Фреймворк использует масштабированную стохастическую рекурсивную технику импульса для эффективного обучения больших моделей. Эксперименты на GPT-2 показывают, что MARS значительно превосходит AdamW по производительности.'}, 'en': {'title': 'Unleashing Variance Reduction for Efficient Model Training', 'desc': 'This paper addresses the need for efficient optimizers in training deep neural networks and large models. It highlights the limitations of existing variance reduction techniques in this context and introduces a new framework called MARS (Make vAriance Reduction Shine). MARS combines preconditioned gradient methods with variance reduction using a scaled stochastic recursive momentum approach. The authors demonstrate that MARS significantly outperforms the popular AdamW optimizer in training large models like GPT-2.'}, 'zh': {'title': '让方差减少技术在大模型训练中大放异彩', 'desc': '本论文提出了一种新的优化框架MARS（让方差减少闪耀），旨在提高大模型的训练效率。我们将预处理梯度方法与方差减少技术结合，采用缩放随机递归动量技术。通过引入基于AdamW、Lion和Shampoo的三种MARS实例，我们展示了方差减少在深度神经网络训练中的潜力。实验结果表明，MARS在训练GPT-2模型时显著优于AdamW。'}}}, {'id': 'https://huggingface.co/papers/2411.07133', 'title': 'Stronger Models are NOT Stronger Teachers for Instruction Tuning', 'url': 'https://huggingface.co/papers/2411.07133', 'abstract': "Instruction tuning has been widely adopted to ensure large language models (LLMs) follow user instructions effectively. The resulting instruction-following capabilities of LLMs heavily rely on the instruction datasets used for tuning. Recently, synthetic instruction datasets have emerged as an economically viable solution to provide LLMs diverse and high-quality instructions. However, existing approaches typically assume that larger or stronger models are stronger teachers for instruction tuning, and hence simply adopt these models as response generators to the synthetic instructions. In this paper, we challenge this commonly-adopted assumption. Our extensive experiments across five base models and twenty response generators reveal that larger and stronger models are not necessarily stronger teachers of smaller models. We refer to this phenomenon as the Larger Models' Paradox. We observe that existing metrics cannot precisely predict the effectiveness of response generators since they ignore the compatibility between teachers and base models being fine-tuned. We thus develop a novel metric, named as Compatibility-Adjusted Reward (CAR) to measure the effectiveness of response generators. Our experiments across five base models demonstrate that CAR outperforms almost all baselines.", 'score': 25, 'issue_id': 546, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': 'be2fc1cdad8aa9f3', 'authors': ['Zhangchen Xu', 'Fengqing Jiang', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran'], 'affiliations': ['University of Washington', 'Allen Institute for AI'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07133.jpg', 'data': {'categories': ['#dataset', '#optimization', '#synthetic', '#training', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Парадокс больших моделей: не всегда лучшие учителя', 'desc': 'В статье рассматривается проблема настройки больших языковых моделей (LLMs) с помощью синтетических наборов инструкций. Авторы выявили парадокс, заключающийся в том, что более крупные модели не всегда являются лучшими учителями для более мелких моделей. Для решения этой проблемы они разработали новый метрик, называемый Compatibility-Adjusted Reward (CAR), который учитывает совместимость между учителем и базовой моделью. Эксперименты показали, что CAR превосходит почти все существующие подходы.'}, 'en': {'title': "Rethinking Instruction Tuning: Size Isn't Everything!", 'desc': "This paper investigates the effectiveness of instruction tuning in large language models (LLMs) and challenges the assumption that larger models are better teachers for smaller models. The authors introduce the concept of the Larger Models' Paradox, showing that bigger models do not always enhance the instruction-following capabilities of smaller models. They highlight the limitations of existing metrics in evaluating response generators, which fail to consider the compatibility between the teacher and the base model. To address this, they propose a new metric called Compatibility-Adjusted Reward (CAR), which shows improved performance in assessing the effectiveness of response generators across various models."}, 'zh': {'title': '大模型不一定是好教师！', 'desc': '本论文探讨了指令调优在大型语言模型（LLMs）中的应用，强调了指令数据集对模型性能的重要性。我们发现，较大或较强的模型并不一定是较小模型的更好教师，这一现象被称为“大模型悖论”。此外，现有的评估指标无法准确预测响应生成器的有效性，因为它们忽略了教师模型与被调优基础模型之间的兼容性。为此，我们提出了一种新的评估指标——兼容性调整奖励（CAR），并通过实验验证了其优越性。'}}}, {'id': 'https://huggingface.co/papers/2411.07184', 'title': 'SAMPart3D: Segment Any Part in 3D Objects', 'url': 'https://huggingface.co/papers/2411.07184', 'abstract': '3D part segmentation is a crucial and challenging task in 3D perception, playing a vital role in applications such as robotics, 3D generation, and 3D editing. Recent methods harness the powerful Vision Language Models (VLMs) for 2D-to-3D knowledge distillation, achieving zero-shot 3D part segmentation. However, these methods are limited by their reliance on text prompts, which restricts the scalability to large-scale unlabeled datasets and the flexibility in handling part ambiguities. In this work, we introduce SAMPart3D, a scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities, without requiring predefined part label sets as text prompts. For scalability, we use text-agnostic vision foundation models to distill a 3D feature extraction backbone, allowing scaling to large unlabeled 3D datasets to learn rich 3D priors. For flexibility, we distill scale-conditioned part-aware 3D features for 3D part segmentation at multiple granularities. Once the segmented parts are obtained from the scale-conditioned part-aware 3D features, we use VLMs to assign semantic labels to each part based on the multi-view renderings. Compared to previous methods, our SAMPart3D can scale to the recent large-scale 3D object dataset Objaverse and handle complex, non-ordinary objects. Additionally, we contribute a new 3D part segmentation benchmark to address the lack of diversity and complexity of objects and parts in existing benchmarks. Experiments show that our SAMPart3D significantly outperforms existing zero-shot 3D part segmentation methods, and can facilitate various applications such as part-level editing and interactive segmentation.', 'score': 24, 'issue_id': 541, 'pub_date': '2024-11-11', 'pub_date_card': {'ru': '11 ноября', 'en': 'November 11', 'zh': '11月11日'}, 'hash': 'b4e58a99e4a7e86c', 'authors': ['Yunhan Yang', 'Yukun Huang', 'Yuan-Chen Guo', 'Liangjun Lu', 'Xiaoyang Wu', 'Edmund Y. Lam', 'Yan-Pei Cao', 'Xihui Liu'], 'affiliations': ['The University of Hong Kong'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07184.jpg', 'data': {'categories': ['#games', '#3d', '#benchmark', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'SAMPart3D: гибкая сегментация 3D-объектов без предварительного обучения', 'desc': 'Статья представляет SAMPart3D - масштабируемый фреймворк для сегментации частей 3D-объектов без предварительного обучения. Авторы используют безтекстовые модели компьютерного зрения для извлечения признаков из 3D-данных, что позволяет обучаться на больших наборах неразмеченных 3D-объектов. Метод способен сегментировать объекты на части с разной степенью детализации, а затем присваивать семантические метки с помощью мультимодальных языковых моделей. SAMPart3D превосходит существующие методы и может применяться для редактирования и интерактивной сегментации 3D-объектов.'}, 'en': {'title': 'Revolutionizing 3D Part Segmentation with SAMPart3D', 'desc': 'This paper presents SAMPart3D, a novel framework for zero-shot 3D part segmentation that does not depend on predefined text prompts. It utilizes text-agnostic vision foundation models to extract 3D features, enabling it to scale effectively to large unlabeled datasets. The framework also incorporates scale-conditioned part-aware features, allowing for segmentation at various levels of detail. SAMPart3D outperforms existing methods and introduces a new benchmark to enhance the diversity and complexity of 3D part segmentation tasks.'}, 'zh': {'title': 'SAMPart3D：无文本提示的3D部件分割新框架', 'desc': '3D部件分割是3D感知中的一项重要且具有挑战性的任务，广泛应用于机器人技术、3D生成和3D编辑等领域。本文提出了SAMPart3D框架，它能够在不依赖预定义文本提示的情况下，对任意3D对象进行多粒度的语义部件分割。该框架利用无文本依赖的视觉基础模型，从大规模未标记的3D数据集中提取丰富的3D特征，并通过条件化的部件感知特征实现灵活的分割。实验结果表明，SAMPart3D在处理复杂对象时显著优于现有的零样本3D部件分割方法，并能支持多种应用，如部件级编辑和交互式分割。'}}}, {'id': 'https://huggingface.co/papers/2411.07975', 'title': 'JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2411.07975', 'abstract': 'We present JanusFlow, a powerful framework that unifies image understanding and generation in a single model. JanusFlow introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. To further improve the performance of our unified model, we adopt two key strategies: (i) decoupling the understanding and generation encoders, and (ii) aligning their representations during unified training. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.', 'score': 20, 'issue_id': 544, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': '294dc65a01cd1218', 'authors': ['Yiyang Ma', 'Xingchao Liu', 'Xiaokang Chen', 'Wen Liu', 'Chengyue Wu', 'Zhiyu Wu', 'Zizheng Pan', 'Zhenda Xie', 'Haowei Zhang', 'Xingkai yu', 'Liang Zhao', 'Yisong Wang', 'Jiaying Liu', 'Chong Ruan'], 'affiliations': ['DeepSeek-AI', 'Peking University', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07975.jpg', 'data': {'categories': ['#benchmark', '#alignment', '#architecture', '#diffusion', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'Единая модель для понимания и генерации изображений', 'desc': 'JanusFlow - это новая архитектура, объединяющая понимание и генерацию изображений в одной модели. Она интегрирует авторегрессионные языковые модели с методом rectified flow для генеративного моделирования. Ключевое преимущество - возможность обучать rectified flow в рамках больших языковых моделей без сложных модификаций архитектуры. Для улучшения производительности используется разделение энкодеров понимания и генерации, а также выравнивание их представлений при обучении.'}, 'en': {'title': 'JanusFlow: Unifying Image Understanding and Generation Efficiently', 'desc': 'JanusFlow is a novel framework that combines image understanding and generation into one cohesive model. It utilizes a simple architecture that merges autoregressive language models with rectified flow, enhancing generative modeling capabilities. The study reveals that rectified flow can be effectively trained within the large language model context without needing complex changes to the architecture. By decoupling the encoders for understanding and generation and aligning their representations during training, JanusFlow demonstrates superior performance compared to specialized models and existing unified approaches.'}, 'zh': {'title': 'JanusFlow：图像理解与生成的统一模型', 'desc': 'JanusFlow是一个强大的框架，将图像理解和生成统一在一个模型中。它采用了简约的架构，将自回归语言模型与修正流结合，这是生成建模中的一种先进方法。研究发现，修正流可以在大型语言模型框架内轻松训练，无需复杂的架构修改。通过解耦理解和生成编码器以及在统一训练中对齐它们的表示，JanusFlow在标准基准测试中表现出色，超越了现有的统一方法。'}}}, {'id': 'https://huggingface.co/papers/2411.07461', 'title': 'BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions', 'url': 'https://huggingface.co/papers/2411.07461', 'abstract': 'We introduce BLIP3-KALE, a dataset of 218 million image-text pairs that bridges the gap between descriptive synthetic captions and factual web-scale alt-text. KALE augments synthetic dense image captions with web-scale alt-text to generate factually grounded image captions. Our two-stage approach leverages large vision-language models and language models to create knowledge-augmented captions, which are then used to train a specialized VLM for scaling up the dataset. We train vision-language models on KALE and demonstrate improvements on vision-language tasks. Our experiments show the utility of KALE for training more capable and knowledgeable multimodal models. We release the KALE dataset at https://huggingface.co/datasets/Salesforce/blip3-kale', 'score': 18, 'issue_id': 552, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': '08fb959148999629', 'authors': ['Anas Awadalla', 'Le Xue', 'Manli Shu', 'An Yan', 'Jun Wang', 'Senthil Purushwalkam', 'Sheng Shen', 'Hannah Lee', 'Oscar Lo', 'Jae Sung Park', 'Etash Guha', 'Silvio Savarese', 'Ludwig Schmidt', 'Yejin Choi', 'Caiming Xiong', 'Ran Xu'], 'affiliations': ['University of Washington', 'Salesforce Research', 'Stanford University', 'University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.07461.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#synthetic', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'KALE: Обогащение мультимодальных моделей фактическими знаниями', 'desc': 'В статье представлен новый набор данных BLIP3-KALE, состоящий из 218 миллионов пар изображений и текстов. Этот датасет объединяет синтетические описательные подписи с фактическими альтернативными текстами из веб-источников. Авторы используют двухэтапный подход с применением крупных мультимодальных и языковых моделей для создания подписей, обогащенных знаниями. Эксперименты показывают, что обучение на KALE улучшает результаты в задачах компьютерного зрения и обработки естественного языка.'}, 'en': {'title': 'Bridging Descriptive and Factual Captions with KALE', 'desc': 'The paper presents BLIP3-KALE, a new dataset containing 218 million image-text pairs that enhances the quality of image captions by combining synthetic captions with factual web-based alt-text. This dataset is created using a two-stage method that employs large vision-language models and language models to produce captions that are both descriptive and factually accurate. By training specialized vision-language models on the KALE dataset, the authors demonstrate significant improvements in various vision-language tasks. The findings highlight the effectiveness of KALE in developing more advanced multimodal models that can better understand and generate content across different modalities.'}, 'zh': {'title': '知识增强的图像标题生成', 'desc': '我们介绍了BLIP3-KALE，这是一个包含2.18亿对图像-文本的数据集，旨在弥合描述性合成标题与事实性网络规模替代文本之间的差距。KALE通过将合成的密集图像标题与网络规模的替代文本相结合，生成基于事实的图像标题。我们采用两阶段的方法，利用大型视觉-语言模型和语言模型创建知识增强的标题，并用这些标题训练专门的视觉语言模型，以扩大数据集规模。实验结果表明，KALE在训练更强大和更具知识性的多模态模型方面具有重要价值。'}}}, {'id': 'https://huggingface.co/papers/2411.08034', 'title': 'Scaling Properties of Diffusion Models for Perceptual Tasks', 'url': 'https://huggingface.co/papers/2411.08034', 'abstract': 'In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and segmentation under image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perception tasks. Through a careful analysis of these scaling behaviors, we present various techniques to efficiently train diffusion models for visual perception tasks. Our models achieve improved or comparable performance to state-of-the-art methods using significantly less data and compute. To use our code and models, see https://scaling-diffusion-perception.github.io .', 'score': 12, 'issue_id': 557, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': '4be3b9af89108627', 'authors': ['Rahul Ravishankar', 'Zeeshan Patel', 'Jathushan Rajasegaran', 'Jitendra Malik'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08034.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#cv'], 'emoji': '🔬', 'ru': {'title': 'Диффузионные модели: новый взгляд на задачи визуального восприятия', 'desc': 'В этой статье исследуются возможности итеративных вычислений с диффузионными моделями для задач визуального восприятия. Авторы объединяют такие задачи как оценка глубины, оптический поток и сегментация в рамках преобразования изображения в изображение. Они демонстрируют, как диффузионные модели выигрывают от увеличения вычислительных ресурсов на этапах обучения и вывода. Предложенные методы позволяют эффективно обучать диффузионные модели для задач визуального восприятия, достигая улучшенных результатов при меньшем объеме данных и вычислений.'}, 'en': {'title': 'Scaling Diffusion Models for Enhanced Visual Perception', 'desc': 'This paper explores how diffusion models can be effectively used for both generating images and performing visual perception tasks like depth estimation and segmentation. It presents a unified approach that treats these tasks as image-to-image translation problems, highlighting the advantages of scaling in training and testing. The authors analyze the scaling behaviors of diffusion models and propose techniques to enhance their efficiency in visual perception tasks. As a result, their models demonstrate competitive performance compared to leading methods while requiring less data and computational resources.'}, 'zh': {'title': '扩散模型：视觉感知的新力量', 'desc': '本文提出了扩散模型在生成和视觉感知任务中的强大能力。我们将深度估计、光流和分割等任务统一为图像到图像的转换，并展示了扩散模型在这些感知任务中如何通过扩展训练和测试计算来获益。通过对这些扩展行为的仔细分析，我们提出了多种高效训练扩散模型的技术。我们的模型在使用显著更少的数据和计算的情况下，达到了与最先进方法相当或更好的性能。'}}}, {'id': 'https://huggingface.co/papers/2411.08017', 'title': 'Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings', 'url': 'https://huggingface.co/papers/2411.08017', 'abstract': "Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required to model the generative models effectively. To address this, we introduce a novel approach called Wavelet Latent Diffusion, or WaLa, that encodes 3D shapes into wavelet-based, compact latent encodings. Specifically, we compress a 256^3 signed distance field into a 12^3 times 4 latent grid, achieving an impressive 2427x compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing the inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at 256^3 resolution. Moreover, WaLa offers rapid inference, producing shapes within two to four seconds depending on the condition, despite the model's scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. We open-source our code and, to the best of our knowledge, release the largest pretrained 3D generative models across different modalities.", 'score': 11, 'issue_id': 547, 'pub_date': '2024-11-12', 'pub_date_card': {'ru': '12 ноября', 'en': 'November 12', 'zh': '11月12日'}, 'hash': '0af1f4c0dc38cc5b', 'authors': ['Aditya Sanghi', 'Aliasghar Khani', 'Pradyumna Reddy', 'Arianna Rampini', 'Derek Cheung', 'Kamal Rahimi Malekshan', 'Kanika Madan', 'Hooman Shayani'], 'affiliations': ['Autodesk AI Lab'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.08017.jpg', 'data': {'categories': ['#inference', '#open_source', '#3d', '#dataset', '#diffusion', '#architecture'], 'emoji': '🌊', 'ru': {'title': 'Эффективная генерация 3D-моделей с помощью вейвлет-сжатия', 'desc': 'Статья представляет новый подход к генерации 3D-моделей под названием Wavelet Latent Diffusion (WaLa). Метод использует вейвлет-кодирование для создания компактных латентных представлений 3D-форм, достигая степени сжатия 2427x. Это позволяет эффективно обучать крупномасштабные генеративные нейронные сети без увеличения времени вывода. WaLa демонстрирует улучшенное качество генерации, разнообразие и вычислительную эффективность по сравнению с существующими методами.'}, 'en': {'title': 'Efficient 3D Shape Generation with Wavelet Latent Diffusion', 'desc': 'This paper presents a new method called Wavelet Latent Diffusion (WaLa) for generating high-quality 3D shapes efficiently. It addresses the limitations of existing 3D generative models by using wavelet-based latent encodings, which compress 3D shapes significantly while preserving detail. The method achieves a remarkable 2427x compression ratio, allowing for the training of large-scale generative networks without increasing inference time. WaLa demonstrates state-of-the-art performance in generating diverse and high-quality 3D shapes, and the authors have made their code and pretrained models available to the public.'}, 'zh': {'title': '高效压缩，快速生成3D形状的创新方法', 'desc': '本论文提出了一种新的方法，称为Wavelet Latent Diffusion（WaLa），旨在提高大规模3D生成模型的效率。通过将3D形状编码为基于小波的紧凑潜在编码，WaLa实现了高达2427倍的压缩比，同时保持了细节的完整性。该方法使得训练大型生成网络变得更加高效，并且在推理时不会显著增加时间。我们的模型在多个数据集上表现出色，生成高质量的3D形状，并且开源了代码，推动了3D生成模型的发展。'}}}, {'id': 'https://huggingface.co/papers/2411.06307', 'title': 'Acoustic Volume Rendering for Neural Impulse Response Fields', 'url': 'https://huggingface.co/papers/2411.06307', 'abstract': "Realistic audio synthesis that captures accurate acoustic phenomena is essential for creating immersive experiences in virtual and augmented reality. Synthesizing the sound received at any position relies on the estimation of impulse response (IR), which characterizes how sound propagates in one scene along different paths before arriving at the listener's position. In this paper, we present Acoustic Volume Rendering (AVR), a novel approach that adapts volume rendering techniques to model acoustic impulse responses. While volume rendering has been successful in modeling radiance fields for images and neural scene representations, IRs present unique challenges as time-series signals. To address these challenges, we introduce frequency-domain volume rendering and use spherical integration to fit the IR measurements. Our method constructs an impulse response field that inherently encodes wave propagation principles and achieves state-of-the-art performance in synthesizing impulse responses for novel poses. Experiments show that AVR surpasses current leading methods by a substantial margin. Additionally, we develop an acoustic simulation platform, AcoustiX, which provides more accurate and realistic IR simulations than existing simulators. Code for AVR and AcoustiX are available at https://zitonglan.github.io/avr.", 'score': 4, 'issue_id': 554, 'pub_date': '2024-11-09', 'pub_date_card': {'ru': '9 ноября', 'en': 'November 9', 'zh': '11月9日'}, 'hash': 'f6935a265562d416', 'authors': ['Zitong Lan', 'Chenhao Zheng', 'Zhiwei Zheng', 'Mingmin Zhao'], 'affiliations': ['University of Pennsylvania', 'University of Washington'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.06307.jpg', 'data': {'categories': ['#audio'], 'emoji': '🔊', 'ru': {'title': 'Реалистичный синтез звука с помощью объемного рендеринга', 'desc': 'Статья представляет новый метод синтеза акустических импульсных характеристик под названием Acoustic Volume Rendering (AVR). AVR адаптирует технику рендеринга объема для моделирования распространения звука в пространстве. Метод использует рендеринг в частотной области и сферическую интеграцию для точного воспроизведения измеренных импульсных характеристик. Эксперименты показывают, что AVR значительно превосходит существующие методы в синтезе импульсных характеристик для новых позиций.'}, 'en': {'title': 'Revolutionizing Sound: Acoustic Volume Rendering for Immersive Audio Experiences', 'desc': 'This paper introduces Acoustic Volume Rendering (AVR), a new method for synthesizing realistic audio in virtual and augmented reality. AVR uses volume rendering techniques to model acoustic impulse responses (IRs), which describe how sound travels in a scene. The authors tackle the unique challenges of IRs as time-series signals by employing frequency-domain volume rendering and spherical integration. Their approach not only improves the accuracy of sound synthesis but also outperforms existing methods, demonstrating significant advancements in acoustic simulation with their platform, AcoustiX.'}, 'zh': {'title': '声学体积渲染：提升虚拟现实中的音频体验', 'desc': '本文提出了一种新的声学体积渲染（AVR）方法，用于合成真实的声学脉冲响应（IR），以增强虚拟和增强现实中的沉浸体验。AVR通过适应体积渲染技术，解决了声波传播的独特挑战，特别是将时间序列信号建模为脉冲响应场。我们引入了频域体积渲染和球面积分技术，以更准确地拟合IR测量。实验结果表明，AVR在合成新姿态的脉冲响应方面超越了现有的领先方法，并且我们还开发了一个声学模拟平台AcoustiX，提供更准确的IR模拟。'}}}, {'id': 'https://huggingface.co/papers/2411.05197', 'title': 'Hardware and Software Platform Inference', 'url': 'https://huggingface.co/papers/2411.05197', 'abstract': "It is now a common business practice to buy access to large language model (LLM) inference rather than self-host, because of significant upfront hardware infrastructure and energy costs. However, as a buyer, there is no mechanism to verify the authenticity of the advertised service including the serving hardware platform, e.g. that it is actually being served using an NVIDIA H100. Furthermore, there are reports suggesting that model providers may deliver models that differ slightly from the advertised ones, often to make them run on less expensive hardware. That way, a client pays premium for a capable model access on more expensive hardware, yet ends up being served by a (potentially less capable) cheaper model on cheaper hardware. In this paper we introduce \\textbf{hardware and software platform inference (HSPI)} -- a method for identifying the underlying  architecture and software stack of a (black-box) machine learning model solely based on its input-output behavior. Our method leverages the inherent differences of various  architectures and compilers to distinguish between different  types and software stacks. By analyzing the numerical patterns in the model's outputs, we propose a classification framework capable of accurately identifying the  used for model inference as well as the underlying software configuration. Our findings demonstrate the feasibility of inferring  type from black-box models. We evaluate HSPI against models served on different real hardware and find that in a white-box setting we can distinguish between different s with between 83.9% and 100% accuracy. Even in a black-box setting we are able to achieve results that are up to three times higher than random guess accuracy.", 'score': 3, 'issue_id': 550, 'pub_date': '2024-11-07', 'pub_date_card': {'ru': '7 ноября', 'en': 'November 7', 'zh': '11月7日'}, 'hash': '7685c8e74f6dbc6b', 'authors': ['Cheng Zhang', 'Hanna Foerster', 'Robert D. Mullins', 'Yiren Zhao', 'Ilia Shumailov'], 'affiliations': ['Imperial College London', 'University of Cambridge', 'Google DeepMind'], 'pdf_title_img': 'assets\\pdf\\title_img\\2411.05197.jpg', 'data': {'categories': ['#leakage', '#security', '#architecture', '#inference'], 'emoji': '🕵️', 'ru': {'title': 'Разоблачение обмана: как определить реальное оборудование для языковой модели', 'desc': 'Статья представляет метод HSPI (идентификация аппаратной и программной платформы), позволяющий определить архитектуру и программный стек модели машинного обучения только на основе ее поведения при вводе-выводе. Метод использует присущие различным архитектурам и компиляторам отличия для распознавания типов оборудования и программных конфигураций. Анализируя числовые паттерны в выводе модели, предложена классификационная система, способная точно идентифицировать оборудование, используемое для вывода модели. Результаты показывают возможность определения типа оборудования для черного ящика модели с точностью до 100% в белом ящике и до трех раз выше случайного угадывания в черном ящике.'}, 'en': {'title': 'Verify Your Model: Uncovering the Truth Behind LLM Inference', 'desc': "This paper addresses the challenge of verifying the authenticity of large language model (LLM) services purchased by businesses. It introduces a novel method called hardware and software platform inference (HSPI), which analyzes the input-output behavior of machine learning models to identify their underlying hardware and software configurations. By examining the numerical patterns in the outputs, HSPI can distinguish between different architectures and software stacks, even in a black-box scenario. The results show that HSPI can achieve high accuracy in identifying the model's hardware and software, significantly improving upon random guessing."}, 'zh': {'title': '验证大型语言模型的真实性', 'desc': '本文介绍了一种名为硬件和软件平台推理（HSPI）的方法，用于识别机器学习模型的底层架构和软件堆栈。该方法通过分析模型的输入输出行为，利用不同架构和编译器的固有差异来区分不同类型的硬件和软件配置。研究表明，在白盒环境下，我们可以以83.9%到100%的准确率区分不同的硬件，而在黑盒环境下，准确率也能达到随机猜测的三倍以上。此方法为验证大型语言模型的真实性提供了一种有效的手段。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (28)', '#agents (22)', '#agi (10)', '#alignment (20)', '#architecture (81)', '#audio (3)', '#benchmark (88)', '#cv (59)', '#data (32)', '#dataset (82)', '#diffusion (43)', '#ethics (4)', '#games (29)', '#graphs (13)', '#hallucinations (10)', '#healthcare (12)', '#inference (25)', '#interpretability (21)', '#leakage (2)', '#long_context (18)', '#low_resource (12)', '#machine_translation (2)', '#math (11)', '#multilingual (16)', '#multimodal (59)', '#open_source (71)', '#optimization (101)', '#plp (8)', '#rag (8)', '#reasoning (35)', '#rl (13)', '#rlhf (11)', '#robotics (6)', '#science (17)', '#security (4)', '#small_models (13)', '#story_generation (4)', '#survey (9)', '#synthetic (25)', '#training (104)', '#transfer_learning (14)', '#video (37)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-11-26 06:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-26 06:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-26 06:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    