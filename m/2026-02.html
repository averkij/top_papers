
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 252 papers. February 2026.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">–§–µ–≤—Ä–∞–ª—å 2026</span> | <span id="title-articles-count">252 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2026-01.html">‚¨ÖÔ∏è <span id="prev-date">01.2026</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2026-03.html">‚û°Ô∏è <span id="next-date">03.2026</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">üìà <span id='top-day-label'>–î–µ–Ω—å</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '–§–µ–≤—Ä–∞–ª—å 2026', 'en': 'February 2026', 'zh': '2Êúà2026Âπ¥'};
        let feedDateNext = {'ru': '03.2026', 'en': '03/2026', 'zh': '3Êúà2026Âπ¥'};
        let feedDatePrev = {'ru': '01.2026', 'en': '01/2026', 'zh': '1Êúà2026Âπ¥'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2602.04705', 'title': 'ERNIE 5.0 Technical Report', 'url': 'https://huggingface.co/papers/2602.04705', 'abstract': 'ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.  \t\t\t\t\tAI-generated summary \t\t\t\t In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.', 'score': 195, 'issue_id': 914, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '2765e822381335a3', 'authors': ['Haifeng Wang', 'Hua Wu', 'Tian Wu', 'Yu Sun', 'Jing Liu', 'Dianhai Yu', 'Yanjun Ma', 'Jingzhou He', 'Zhongjun He', 'Dou Hong', 'Qiwen Liu', 'Shuohuan Wang', 'Junyuan Shang', 'Zhenyu Zhang', 'Yuchen Ding', 'Jinle Zeng', 'Jiabin Yang', 'Liang Shen', 'Ruibiao Chen', 'Weichong Yin', 'Siyu Ding', 'Dai Dai', 'Shikun Feng', 'Siqi Bao', 'Bolei He', 'Yan Chen', 'Zhenyu Jiao', 'Ruiqing Zhang', 'Zeyu Chen', 'Qingqing Dang', 'Kaipeng Deng', 'Jiajun Jiang', 'Enlei Gong', 'Guoxia Wang', 'Yanlin Sha', 'Yi Liu', 'Yehan Zheng', 'Weijian Xu', 'Jiaxiang Liu', 'Zengfeng Zeng', 'Yingqi Qu', 'Zhongli Li', 'Zhengkun Zhang', 'Xiyang Wang', 'Zixiang Xu', 'Xinchao Xu', 'Zhengjie Huang', 'Dong Wang', 'Bingjin Chen', 'Yue Chang', 'Xing Yuan', 'Shiwei Huang', 'Qiao Zhao', 'Xinzhe Ding', 'Shuangshuang Qiao', 'Baoshan Yang', 'Bihong Tang', 'Bin Li', 'Bingquan Wang', 'Binhan Tang', 'Binxiong Zheng', 'Bo Cui', 'Bo Ke', 'Bo Zhang', 'Bowen Zhang', 'Boyan Zhang', 'Boyang Liu', 'Caiji Zhang', 'Can Li', 'Chang Xu', 'Chao Pang', 'Chao Zhang', 'Chaoyi Yuan', 'Chen Chen', 'Cheng Cui', 'Chenlin Yin', 'Chun Gan', 'Chunguang Chai', 'Chuyu Fang', 'Cuiyun Han', 'Dan Zhang', 'Danlei Feng', 'Danxiang Zhu', 'Dong Sun', 'Dongbo Li', 'Dongdong Li', 'Dongdong Liu', 'Dongxue Liu', 'Fan Ding', 'Fan Hu', 'Fan Li', 'Fan Mo', 'Feisheng Wu', 'Fengwei Liu', 'Gangqiang Hu', 'Gaofeng Lu', 'Gaopeng Yong', 'Gexiao Tian', 'Guan Wang', 'Guangchen Ni', 'Guangshuo Wu', 'Guanzhong Wang', 'Guihua Liu', 'Guishun Li', 'Haibin Li', 'Haijian Liang', 'Haipeng Ming', 'Haisu Wang', 'Haiyang Lu', 'Haiye Lin', 'Han Zhou', 'Hangting Lou', 'Hanwen Du', 'Hanzhi Zhang', 'Hao Chen', 'Hao Du', 'Hao Liu', 'Hao Zhou', 'Haochen Jiang', 'Haodong Tian', 'Haoshuang Wang', 'Haozhe Geng', 'Heju Yin', 'Hong Chen', 'Hongchen Xue', 'Hongen Liu', 'Honggeng Zhang', 'Hongji Xu', 'Hongwei Chen', 'Hongyang Zhang', 'Hongyuan Zhang', 'Hua Lu', 'Huan Chen', 'Huan Wang', 'Huang He', 'Hui Liu', 'Hui Zhong', 'Huibin Ruan', 'Jiafeng Lu', 'Jiage Liang', 'Jiahao Hu', 'Jiahao Hu', 'Jiajie Yang', 'Jialin Li', 'Jian Chen', 'Jian Wu', 'Jianfeng Yang', 'Jianguang Jiang', 'Jianhua Wang', 'Jianye Chen', 'Jiaodi Liu', 'Jiarui Zhou', 'Jiawei Lv', 'Jiaxin Zhou', 'Jiaxuan Liu', 'Jie Han', 'Jie Sun', 'Jiefan Fang', 'Jihan Liu', 'Jihua Liu', 'Jing Hu', 'Jing Qian', 'Jing Yan', 'Jingdong Du', 'Jingdong Wang', 'Jingjing Wu', 'Jingyong Li', 'Jinheng Wang', 'Jinjin Li', 'Jinliang Lu', 'Jinlin Yu', 'Jinnan Liu', 'Jixiang Feng', 'Jiyi Huang', 'Jiyuan Zhang', 'Jun Liang', 'Jun Xia', 'Jun Yu', 'Junda Chen', 'Junhao Feng', 'Junhong Xiang', 'Junliang Li', 'Kai Liu', 'Kailun Chen', 'Kairan Su', 'Kang Hu', 'Kangkang Zhou', 'Ke Chen', 'Ke Wei', 'Kui Huang', 'Kun Wu', 'Kunbin Chen', 'Lei Han', 'Lei Sun', 'Lei Wen', 'Linghui Meng', 'Linhao Yu', 'Liping Ouyang', 'Liwen Zhang', 'Longbin Ji', 'Longzhi Wang', 'Meng Sun', 'Meng Tian', 'Mengfei Li', 'Mengqi Zeng', 'Mengyu Zhang', 'Ming Hong', 'Mingcheng Zhou', 'Mingming Huang', 'Mingxin Chen', 'Mingzhu Cai', 'Naibin Gu', 'Nemin Qiu', 'Nian Wang', 'Peng Qiu', 'Peng Zhao', 'Pengyu Zou', 'Qi Wang', 'Qi Xin', 'Qian Wang', 'Qiang Zhu', 'Qianhui Luo', 'Qianwei Yang', 'Qianyue He', 'Qifei Wu', 'Qinrui Li', 'Qiwen Bao', 'Quan Zhang', 'Quanxiang Liu', 'Qunyi Xie', 'Rongrui Zhan', 'Rufeng Dai', 'Rui Peng', 'Ruian Liu', 'Ruihao Xu', 'Ruijie Wang', 'Ruixi Zhang', 'Ruixuan Liu', 'Runsheng Shi', 'Ruting Wang', 'Senbo Kang', 'Shan Lu', 'Shaofei Yu', 'Shaotian Gong', 'Shenwei Hu', 'Shifeng Zheng', 'Shihao Guo', 'Shilong Fan', 'Shiqin Liu', 'Shiwei Gu', 'Shixi Zhang', 'Shuai Yao', 'Shuang Zhang', 'Shuangqiao Liu', 'Shuhao Liang', 'Shuwei He', 'Shuwen Yang', 'Sijun He', 'Siming Dai', 'Siming Wu', 'Siyi Long', 'Songhe Deng', 'Suhui Dong', 'Suyin Liang', 'Teng Hu', 'Tianchan Xu', 'Tianliang Lv', 'Tianmeng Yang', 'Tianyi Wei', 'Tiezhu Gao', 'Ting Sun', 'Ting Zhang', 'Tingdan Luo', 'Wei He', 'Wei Luan', 'Wei Yin', 'Wei Zhang', 'Wei Zhou', 'Weibao Gong', 'Weibin Li', 'Weicheng Huang', 'Weichong Dang', 'Weiguo Zhu', 'Weilong Zhang', 'Weiqi Tan', 'Wen Huang', 'Wenbin Chang', 'Wenjing Du', 'Wenlong Miao', 'Wenpei Luo', 'Wenquan Wu', 'Xi Shi', 'Xi Zhao', 'Xiang Gao', 'Xiangguo Zhang', 'Xiangrui Yu', 'Xiangsen Wang', 'Xiangzhe Wang', 'Xianlong Luo', 'Xianying Ma', 'Xiao Tan', 'Xiaocong Lin', 'Xiaofei Wang', 'Xiaofeng Peng', 'Xiaofeng Wu', 'Xiaojian Xu', 'Xiaolan Yuan', 'Xiaopeng Cui', 'Xiaotian Han', 'Xiaoxiong Liu', 'Xiaoxu Fei', 'Xiaoxuan Wu', 'Xiaoyu Wang', 'Xiaoyu Zhang', 'Xin Sun', 'Xin Wang', 'Xinhui Huang', 'Xinming Zhu', 'Xintong Yu', 'Xinyi Xu', 'Xinyu Wang', 'Xiuxian Li', 'XuanShi Zhu', 'Xue Xu', 'Xueying Lv', 'Xuhong Li', 'Xulong Wei', 'Xuyi Chen', 'Yabing Shi', 'Yafeng Wang', 'Yamei Li', 'Yan Liu', 'Yanfu Cheng', 'Yang Gao', 'Yang Liang', 'Yang Wang', 'Yang Wang', 'Yang Yang', 'Yanlong Liu', 'Yannian Fu', 'Yanpeng Wang', 'Yanzheng Lin', 'Yao Chen', 'Yaozong Shen', 'Yaqian Han', 'Yehua Yang', 'Yekun Chai', 'Yesong Wang', 'Yi Song', 'Yichen Zhang', 'Yifei Wang', 'Yifeng Guo', 'Yifeng Kou', 'Yilong Chen', 'Yilong Guo', 'Yiming Wang', 'Ying Chen', 'Ying Wang', 'Yingsheng Wu', 'Yingzhan Lin', 'Yinqi Yang', 'Yiran Xing', 'Yishu Lei', 'Yixiang Tu', 'Yiyan Chen', 'Yong Zhang', 'Yonghua Li', 'Yongqiang Ma', 'Yongxing Dai', 'Yongyue Zhang', 'Yu Ran', 'Yu Sun', 'Yu-Wen Michael Zhang', 'Yuang Liu', 'Yuanle Liu', 'Yuanyuan Zhou', 'Yubo Zhang', 'Yuchen Han', 'Yucheng Wang', 'Yude Gao', 'Yuedong Luo', 'Yuehu Dong', 'Yufeng Hu', 'Yuhui Cao', 'Yuhui Yun', 'Yukun Chen', 'Yukun Gao', 'Yukun Li', 'Yumeng Zhang', 'Yun Fan', 'Yun Ma', 'Yunfei Zhang', 'Yunshen Xie', 'Yuping Xu', 'Yuqin Zhang', 'Yuqing Liu', 'Yurui Li', 'Yuwen Wang', 'Yuxiang Lu', 'Zefeng Cai', 'Zelin Zhao', 'Zelun Zhang', 'Zenan Lin', 'Zezhao Dong', 'Zhaowu Pan', 'Zhaoyu Liu', 'Zhe Dong', 'Zhe Zhang', 'Zhen Zhang', 'Zhengfan Wu', 'Zhengrui Wei', 'Zhengsheng Ning', 'Zhenxing Li', 'Zhenyu Li', 'Zhenyu Qian', 'Zhenyun Li', 'Zhi Li', 'Zhichao Chen', 'Zhicheng Dong', 'Zhida Feng', 'Zhifan Feng', 'Zhihao Deng', 'Zhijin Yu', 'Zhiyang Chen', 'Zhonghui Zheng', 'Zhuangzhuang Guo', 'Zhujun Zhang', 'Zhuo Sun', 'Zichang Liu', 'Zihan Lin', 'Zihao Huang', 'Zihe Zhu', 'Ziheng Zhao', 'Ziping Chen', 'Zixuan Zhu', 'Ziyang Xu', 'Ziyi Liang', 'Ziyuan Gao'], 'affiliations': ['Baidu'], 'pdf_title_img': 'assets/pdf/title_img/2602.04705.jpg', 'data': {'categories': ['#multimodal', '#training', '#inference', '#architecture'], 'emoji': 'üé¨', 'ru': {'title': '–ï–¥–∏–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∏–ª–ª–∏–æ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ —Å –≥–∏–±–∫–∏–º –æ–±—É—á–µ–Ω–∏–µ–º –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏', 'desc': 'ERNIE 5.0 ‚Äî —ç—Ç–æ —Ç—Ä–∏–ª–ª–∏–æ–Ω–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤–∞—è –∞–≤—Ç–µ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE). –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å –Ω—É–ª—è –Ω–∞ –∑–∞–¥–∞—á–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–π –≥—Ä—É–ø–ø—ã —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –Ω–µ–∑–∞–≤–∏—Å–∏–º—É—é –æ—Ç —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö. –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö –º–æ–¥–µ–ª—å –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —ç–ª–∞—Å—Ç–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å —Å–µ–º–µ–π—Å—Ç–≤–æ –ø–æ–¥–º–æ–¥–µ–ª–µ–π —Å —Ä–∞–∑–Ω–æ–π –≥–ª—É–±–∏–Ω–æ–π –∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å—é –≤ –æ–¥–Ω–æ–º —Ü–∏–∫–ª–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ —Ä–µ—à–∏–ª–∏ —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –æ–±–µ—Å–ø–µ—á–∏–≤ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏–µ –≤ —É–ª—å—Ç—Ä–∞—Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ MoE.'}, 'en': {'title': 'ERNIE 5.0: Unifying Multimodal AI with Elastic Training', 'desc': 'ERNIE 5.0 is a groundbreaking autoregressive model that integrates understanding and generation of multiple data types, including text, images, videos, and audio. It utilizes a sparse mixture-of-experts (MoE) architecture, allowing for efficient routing of experts based on the modality of the input. The model employs an elastic training approach, which enables it to adaptively learn various sub-models with different capacities and depths, optimizing performance while managing resource constraints. Extensive testing shows that ERNIE 5.0 excels in delivering balanced performance across all modalities, marking a significant advancement in the field of multimodal machine learning.'}, 'zh': {'title': 'ERNIE 5.0Ôºö‰∏á‰∫øÂèÇÊï∞ÁöÑÂ§öÊ®°ÊÄÅÁªü‰∏ÄÊ®°Âûã', 'desc': 'ERNIE 5.0 ÊòØ‰∏Ä‰∏™ÂÖ∑Êúâ‰∏á‰∫øÂèÇÊï∞ÁöÑËá™ÂõûÂΩíÊ®°ÂûãÔºåÊó®Âú®Áªü‰∏ÄÂ§ÑÁêÜÊñáÊú¨„ÄÅÂõæÂÉè„ÄÅËßÜÈ¢ëÂíåÈü≥È¢ëÁ≠âÂ§öÊ®°ÊÄÅÊï∞ÊçÆ„ÄÇËØ•Ê®°ÂûãÈááÁî®Ë∂ÖÁ®ÄÁñèÁöÑ‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâÊû∂ÊûÑÔºåÈÄöËøáÂºπÊÄßËÆ≠ÁªÉÊñπÊ≥ïËß£ÂÜ≥Â§ßËßÑÊ®°ÈÉ®ÁΩ≤‰∏≠ÁöÑËµÑÊ∫êÈôêÂà∂ÈóÆÈ¢ò„ÄÇERNIE 5.0 Âú®ÂçïÊ¨°È¢ÑËÆ≠ÁªÉ‰∏≠Â≠¶‰π†Â§ö‰∏™Â≠êÊ®°ÂûãÔºåËÉΩÂ§üÂú®ÊÄßËÉΩ„ÄÅÊ®°ÂûãÂ§ßÂ∞èÂíåÊé®ÁêÜÂª∂Ëøü‰πãÈó¥ÁÅµÊ¥ªÊùÉË°°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåERNIE 5.0 Âú®Â§öÁßçÊ®°ÊÄÅ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÊ†áÂøóÁùÄËá™ÂõûÂΩíÊ®°ÂûãÂú®Â§öÊ®°ÊÄÅÁêÜËß£ÂíåÁîüÊàêÊñπÈù¢ÁöÑÈ¶ñÊ¨°Â§ßËßÑÊ®°Â∫îÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03152', 'title': 'FASA: Frequency-aware Sparse Attention', 'url': 'https://huggingface.co/papers/2602.03152', 'abstract': 'FASA is a novel framework that uses query-aware token eviction and functional sparsity in RoPE to reduce KV cache memory usage while maintaining high performance in long-context LLM tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of "dominant" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56times speedup using just 18.9\\% of the cache on AIME24.', 'score': 100, 'issue_id': 920, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '930179326b649b72', 'authors': ['Yifei Wang', 'Yueqi Wang', 'Zhenrui Yue', 'Huimin Zeng', 'Yong Wang', 'Ismini Lourentzou', 'Zhengzhong Tu', 'Xiangxiang Chu', 'Julian McAuley'], 'affiliations': ['AMAP, Alibaba Group', 'Texas A&M University', 'UCSD', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2602.03152.jpg', 'data': {'categories': ['#optimization', '#long_context', '#training', '#inference'], 'emoji': '‚ö°', 'ru': {'title': '–£–º–Ω–æ–µ –ø—Ä–æ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ –∫–µ—à–∞ —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å RoPE', 'desc': 'FASA ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ KV-–∫–µ—à–∞ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ä–µ–∂–∏–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ—Ç–∫—Ä—ã—Ç–∏–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è RoPE –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ ¬´–¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏—Ö¬ª —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö–æ—Ä–æ—à–æ —Å–æ–≥–ª–∞—Å—É–µ—Ç—Å—è —Å –ø–æ–ª–Ω–æ–π –≥–æ–ª–æ–≤–æ–π –≤–Ω–∏–º–∞–Ω–∏—è –∏ –º–æ–∂–µ—Ç —Å–ª—É–∂–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–º –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FASA –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –±–ª–∏–∑–∫–æ–π –∫ –æ—Ä–∞–∫—É–ª—É, –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞–±–æ—Ç—ã —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏.'}, 'en': {'title': 'FASA: Efficient Memory Management for Long-Context LLMs', 'desc': "FASA is a new framework designed to optimize memory usage in Large Language Models (LLMs) by implementing query-aware token eviction and leveraging functional sparsity in the RoPE method. It addresses the challenge of high memory consumption in Key Value (KV) caches when processing long inputs by dynamically predicting the importance of tokens based on the current query. Unlike previous methods that either risk losing important information or rely on insufficient heuristics, FASA identifies a small set of 'dominant' frequency chunks that correlate well with the full attention head. This allows FASA to significantly reduce memory bandwidth and computational costs while maintaining high performance across various long-context tasks, achieving near-optimal accuracy with minimal token retention."}, 'zh': {'title': 'FASAÔºöÈ´òÊïàÁöÑÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜÊ°ÜÊû∂', 'desc': 'FASAÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÊü•ËØ¢ÊÑüÁü•ÁöÑ‰ª§ÁâåÈ©±ÈÄêÂíåRoPE‰∏≠ÁöÑÂäüËÉΩÁ®ÄÁñèÊÄßÊù•ÂáèÂ∞ëKVÁºìÂ≠òÁöÑÂÜÖÂ≠ò‰ΩøÁî®ÔºåÂêåÊó∂Âú®Èïø‰∏ä‰∏ãÊñáÁöÑLLM‰ªªÂä°‰∏≠‰øùÊåÅÈ´òÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂä®ÊÄÅÈ¢ÑÊµã‰ª§ÁâåÁöÑÈáçË¶ÅÊÄßÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ïÂú®Â§ÑÁêÜÈïøËæìÂÖ•Êó∂ÁöÑÂÜÖÂ≠òÁì∂È¢àÈóÆÈ¢ò„ÄÇFASAÁöÑÂÖ≥ÈîÆÂèëÁé∞ÊòØÔºåÊüê‰∫õ‚Äú‰∏ªÂØº‚ÄùÈ¢ëÁéáÂùóÂú®‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄßÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂèØ‰ª•ÊúâÊïàËØÜÂà´ÈáçË¶Å‰ª§Áâå„ÄÇÈÄöËøá‰ªÖÂØπËøô‰∫õÁªèËøá‰øÆÂâ™ÁöÑ‰ª§ÁâåËøõË°åÈõÜ‰∏≠Ê≥®ÊÑèÂäõËÆ°ÁÆóÔºåFASAÊòæËëóÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂ∏¶ÂÆΩÈúÄÊ±ÇÂíåËÆ°ÁÆóÊàêÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04634', 'title': 'WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.04634', 'abstract': 'Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.', 'score': 71, 'issue_id': 914, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'dad5d6d10152daac', 'authors': ['Zelai Xu', 'Zhexuan Xu', 'Ruize Zhang', 'Chunyang Zhu', 'Shi Yu', 'Weilin Liu', 'Quanlu Zhang', 'Wenbo Ding', 'Chao Yu', 'Yu Wang'], 'affiliations': ['EE, Tsinghua University', 'IIIS, Tsinghua University', 'Infinigence AI', 'SIGS, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.04634.jpg', 'data': {'categories': ['#benchmark', '#rl', '#small_models', '#agents'], 'emoji': 'ü§ñ', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —à–∏—Ä–∏–Ω–µ: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –≤–º–µ—Å—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ WideSeek-R1, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —à–∏—Ä–æ–∫–∏—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-–ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–¥–∞—á –ø—É—Ç—ë–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –≤–µ–¥—É—â–µ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—á–∏–Ω—ë–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–±—É—á–∞—é—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—é –∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–∑ 20 —Ç—ã—Å—è—á –∑–∞–¥–∞—á. –ú–æ–¥–µ–ª—å WideSeek-R1-4B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å—Ä–∞–≤–Ω–∏–º—ã—Ö —Å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–æ–π –æ–¥–Ω–∞–≥–µ–Ω—Ç–Ω–æ–π –º–æ–¥–µ–ª—å—é DeepSeek-R1-671B, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ —à–∏—Ä–∏–Ω–µ –≤–º–µ—Å—Ç–æ –≥–ª—É–±–∏–Ω—ã. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç—Å—è –ø–æ –º–µ—Ä–µ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —á–∏—Å–ª–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ–¥–∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Scaling Information Seeking with Multi-Agent Reinforcement Learning', 'desc': 'This paper introduces WideSeek-R1, a multi-agent system designed to enhance information seeking through parallel execution using reinforcement learning. Unlike traditional systems that depend on fixed workflows, WideSeek-R1 employs a lead-agent-subagent framework that allows for scalable orchestration of tasks. By leveraging a shared Large Language Model (LLM) and specialized tools, the system optimizes the collaboration between the lead agent and its subagents. The results demonstrate that WideSeek-R1 can achieve performance levels similar to larger single-agent systems while benefiting from increased efficiency as more subagents are added.'}, 'zh': {'title': 'Â§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÔºö‰ø°ÊÅØËé∑ÂèñÁöÑÊñ∞Áª¥Â∫¶', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫WideSeek-R1ÁöÑÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ÂÆûÁé∞‰ø°ÊÅØÁöÑÂπ∂Ë°åËé∑Âèñ„ÄÇ‰∏é‰º†ÁªüÁöÑÂçï‰∏ÄÊô∫ËÉΩ‰ΩìÊñπÊ≥ïÁõ∏ÊØîÔºåWideSeek-R1ÈÄöËøáÂºïÂÖ•‰∏ªÊô∫ËÉΩ‰ΩìÂíåÂ≠êÊô∫ËÉΩ‰ΩìÁöÑÊ°ÜÊû∂Ôºå‰ºòÂåñ‰∫Ü‰ªªÂä°ÁöÑÁªÑÁªáËÉΩÂäõ„ÄÇËØ•Á≥ªÁªü‰ΩøÁî®ÂÖ±‰∫´ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíå‰∏ìÁî®Â∑•ÂÖ∑ÔºåËÉΩÂ§üÊúâÊïàÂ§ÑÁêÜÂπøÊ≥õÁöÑ‰ø°ÊÅØÊêúÁ¥¢‰ªªÂä°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWideSeek-R1Âú®Â§ö‰∏™‰ªªÂä°‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåËØÅÊòé‰∫ÜÂÆΩÂ∫¶Êâ©Â±ïÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04145', 'title': 'Training Data Efficiency in Multimodal Process Reward Models', 'url': 'https://huggingface.co/papers/2602.04145', 'abstract': 'Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.', 'score': 70, 'issue_id': 913, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '04258fd37246fe76', 'authors': ['Jinyuan Li', 'Chengsong Huang', 'Langlin Huang', 'Shaoyang Xu', 'Haolin Liu', 'Wenxuan Zhang', 'Jiaxin Huang'], 'affiliations': ['Singapore University of Technology and Design', 'University of Virginia', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2602.04145.jpg', 'data': {'categories': ['#data', '#benchmark', '#training', '#multimodal'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–£–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ (MPRM), –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∞—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ–±—ä—è—Å–Ω—è—é—â–∏–π, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Å–º–µ—Å–∏ –º–µ—Ç–æ–∫ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö –≤—ã–≤–æ–¥–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –º–µ—Ç–æ–¥ Balanced-Information Score (BIS), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –æ–±—É—á–µ–Ω–∏—è –ø–æ –¥–≤—É–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, –æ—Ç–æ–±—Ä–∞–Ω–Ω–æ–µ –º–µ—Ç–æ–¥–æ–º BIS, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ 10% –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Efficient MPRM Training: 10% Data, Full Performance!', 'desc': 'This paper introduces a method for training Multimodal Process Reward Models (MPRMs) more efficiently by using a Balanced-Information Score (BIS). The BIS focuses on optimizing the mixture of positive and negative labels and their reliability, which helps in reducing the amount of training data needed. The authors demonstrate that their approach can achieve the same performance as using the full dataset by only utilizing 10% of the training data. This method not only saves resources but also enhances the training process by addressing redundancy in existing Monte Carlo-annotated corpora.'}, 'zh': {'title': 'È´òÊïàËÆ≠ÁªÉÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÁöÑÂπ≥Ë°°‰ø°ÊÅØËØÑÂàÜ', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜÂ§öÊ®°ÊÄÅËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÔºàMPRMÔºâÁöÑÈ´òÊïàËÆ≠ÁªÉÊñπÊ≥ïÔºåÈáçÁÇπÂú®‰∫éÂ¶Ç‰ΩïÈÄöËøáÂπ≥Ë°°‰ø°ÊÅØËØÑÂàÜÊù•ÊèêÈ´òÊï∞ÊçÆÂà©Áî®Áéá„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåMPRMÁöÑËÆ≠ÁªÉÂú®ÈöèÊú∫ÊäΩÊ†∑Êó∂‰ºöËøÖÈÄüÈ•±ÂíåÔºåË°®ÊòéÁé∞ÊúâÁöÑËíôÁâπÂç°Ê¥õÊ†áÊ≥®Êï∞ÊçÆÂ≠òÂú®ÂÜó‰Ωô„ÄÇ‰∏∫‰∫ÜËß£ÈáäËøô‰∏ÄÁé∞Ë±°ÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÁêÜËÆ∫Ê°ÜÊû∂ÔºåÂº∫Ë∞ÉÊ†áÁ≠æÊ∑∑ÂêàÂíåÊ†áÁ≠æÂèØÈù†ÊÄßÂØπ‰ø°ÊÅØÊ¢ØÂ∫¶Êõ¥Êñ∞ÁöÑÈáçË¶ÅÊÄß„ÄÇÈÄöËøáÂºïÂÖ•Âπ≥Ë°°‰ø°ÊÅØËØÑÂàÜÔºàBISÔºâÔºåÊàë‰ª¨ËÉΩÂ§üÂú®‰∏çÂ¢ûÂä†È¢ùÂ§ñÊàêÊú¨ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰ºòÂÖàËÄÉËôëÊ†áÁ≠æÁöÑÊ∑∑ÂêàÊÄßÂíåÂèØÈù†ÊÄßÔºå‰ªéËÄåÂú®‰ªÖ‰ΩøÁî®10%ÁöÑËÆ≠ÁªÉÊï∞ÊçÆÊó∂ÂÆûÁé∞‰∏éÂÖ®Êï∞ÊçÆÁõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04804', 'title': 'OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models', 'url': 'https://huggingface.co/papers/2602.04804', 'abstract': 'OmniSIFT is a modality-asymmetric token compression framework for Omni-LLMs that reduces computational overhead through spatio-temporal video pruning and vision-guided audio selection while maintaining superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.', 'score': 41, 'issue_id': 914, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '72a18b31bbdad9fb', 'authors': ['Yue Ding', 'Yiyan Ji', 'Jungang Li', 'Xuyang Liu', 'Xinlong Chen', 'Junfei Wu', 'Bozhou Li', 'Bohan Zeng', 'Yang Shi', 'Yushuo Guan', 'Yuanxing Zhang', 'Jiaheng Liu', 'Qiang Liu', 'Pengfei Wan', 'Liang Wang'], 'affiliations': ['Kling Team, Kuaishou Technology', 'Nanjing University', 'New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)', 'Peking University', 'Sichuan University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2602.04804.jpg', 'data': {'categories': ['#audio', '#multimodal', '#video', '#inference'], 'emoji': '‚ö°', 'ru': {'title': '–£–º–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': 'OmniSIFT ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (Omni-LLM), –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é: –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –≤–∏–¥–µ–æ —É–¥–∞–ª—è–µ—Ç –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ –∏ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏, –∞ –º–æ–¥—É–ª—å –≤—ã–±–æ—Ä–∞ –∞—É–¥–∏–æ, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π, —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –∞—É–¥–∏–æ—Ç–æ–∫–µ–Ω—ã. –í—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç—Å—è —Å–∫–≤–æ–∑–Ω—ã–º –æ–±—Ä–∞–∑–æ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–≥–æ –ø—Ä—è–º–æ–≥–æ –æ—Ü–µ–Ω—â–∏–∫–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ OmniSIFT –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 25% –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Efficient Token Compression for Omni-LLMs with OmniSIFT', 'desc': 'OmniSIFT is a new framework designed to compress tokens in Omni-modal Large Language Models (Omni-LLMs) while keeping their performance high. It uses a two-step approach: first, it prunes unnecessary video data to reduce redundancy, and second, it selects relevant audio tokens based on visual information. This method significantly lowers the computational load by using only a fraction of the original tokens, yet it still achieves better results than existing compression techniques. The framework is optimized to work efficiently, making it a valuable tool for improving the efficiency of multimodal AI systems.'}, 'zh': {'title': 'OmniSIFTÔºöÈ´òÊïàÁöÑÂÖ®Ê®°ÊÄÅ‰ª§ÁâåÂéãÁº©Ê°ÜÊû∂', 'desc': 'OmniSIFTÊòØ‰∏ÄÁßçÈíàÂØπÂÖ®Ê®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàOmni-LLMsÔºâÁöÑÈùûÂØπÁß∞‰ª§ÁâåÂéãÁº©Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÊó∂Á©∫ËßÜÈ¢ëÂâ™ÊûùÂíåËßÜËßâÂºïÂØºÈü≥È¢ëÈÄâÊã©Êù•ÂáèÂ∞ëËÆ°ÁÆóÂºÄÈîÄÔºåÂêåÊó∂‰øùÊåÅÂçìË∂äÁöÑÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∏§Èò∂ÊÆµÂéãÁº©Á≠ñÁï•ÔºöÈ¶ñÂÖàÔºåÈÄöËøáÊó∂Á©∫ËßÜÈ¢ëÂâ™ÊûùÊ®°ÂùóÂéªÈô§ËßÜÈ¢ë‰∏≠ÁöÑÂÜó‰Ωô‰ø°ÊÅØÔºåÂÖ∂Ê¨°ÔºåÈÄöËøáËßÜËßâÂºïÂØºÈü≥È¢ëÈÄâÊã©Ê®°ÂùóËøáÊª§Èü≥È¢ë‰ª§Áâå„ÄÇOmniSIFTÈÄöËøáÂèØÂæÆÂàÜÁöÑÁõ¥ÈÄö‰º∞ËÆ°Âô®ËøõË°åÁ´ØÂà∞Á´Ø‰ºòÂåñÔºåÁ°Æ‰øù‰∫ÜÈ´òÊïàÊÄßÂíåÈ≤ÅÊ£íÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOmniSIFTÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå‰∏îÂú®ÂèÇÊï∞ÈáèÂíåÂª∂Ëøü‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÂéãÁº©Âü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03560', 'title': 'HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing', 'url': 'https://huggingface.co/papers/2602.03560', 'abstract': "Hybrid Sparse Attention architecture interleaves full and sparse attention layers, using full attention output to guide sparse layer token selection and cache reuse for improved efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.", 'score': 35, 'issue_id': 914, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'd66619c54b016c43', 'authors': ['Yizhao Gao', 'Jianyu Wei', 'Qihao Zhang', 'Yu Cheng', 'Shimao Chen', 'Zhengju Tang', 'Zihan Jiang', 'Yifan Song', 'Hailin Zhang', 'Liang Zhao', 'Bo Yang', 'Gang Wang', 'Shijie Cao', 'Fuli Luo'], 'affiliations': ['Xiaomi'], 'pdf_title_img': 'assets/pdf/title_img/2602.03560.jpg', 'data': {'categories': ['#inference', '#architecture'], 'emoji': '‚ö°', 'ru': {'title': '–ü–æ–ª–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫–∞–∫ —É—á–∏—Ç–µ–ª—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Hybrid Sparse Attention (HySparse), –∫–æ—Ç–æ—Ä–∞—è —á–µ—Ä–µ–¥—É–µ—Ç —Å–ª–æ–∏ –ø–æ–ª–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ª–æ—è–º–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—ã—Ö–æ–¥–æ–≤ –ø–æ–ª–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∫–∞–∫ –æ—Ä–∞–∫—É–ª–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è KV-–∫–µ—à–µ–π –º–µ–∂–¥—É —Å–ª–æ—è–º–∏, —á—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏-—Ñ—É–Ω–∫—Ü–∏—è—Ö. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –∫–∞–∫ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã, —Ç–∞–∫ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è KV-–∫–µ—à–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –º–æ–¥–µ–ª—è—Ö —Ä–∞–∑–º–µ—Ä–æ–º 7B –∏ 80B –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ HySparse –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ–ª–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –¥—Ä—É–≥–∏–µ –≥–∏–±—Ä–∏–¥–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –¥–æ—Å—Ç–∏–≥–∞—è 10-–∫—Ä–∞—Ç–Ω–æ–≥–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —á–∏—Å–ª–µ –ø–æ–ª–Ω—ã—Ö —Å–ª–æ–µ–≤.'}, 'en': {'title': 'Efficient Attention with Hybrid Sparse Architecture', 'desc': 'The Hybrid Sparse Attention (HySparse) architecture combines full and sparse attention layers to enhance efficiency and performance in machine learning models. By using the output from full attention layers to guide the selection of tokens in sparse layers, HySparse eliminates the need for additional proxies that complicate token importance prediction. This method not only improves the accuracy of token selection but also allows for the reuse of key-value (KV) caches, significantly reducing both computational load and memory usage. Evaluations show that HySparse outperforms traditional attention methods, achieving better results with fewer full attention layers while minimizing KV cache storage requirements.'}, 'zh': {'title': 'Ê∑∑ÂêàÁ®ÄÁñèÊ≥®ÊÑèÂäõÔºöÈ´òÊïà‰∏éÊÄßËÉΩÁöÑÂÆåÁæéÁªìÂêà', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ∑∑ÂêàÁ®ÄÁñèÊ≥®ÊÑèÂäõÊû∂ÊûÑÔºàHySparseÔºâÔºåÂÆÉÂ∞ÜÂÖ®Ê≥®ÊÑèÂäõÂ±Ç‰∏éÂ§ö‰∏™Á®ÄÁñèÊ≥®ÊÑèÂäõÂ±Ç‰∫§Êõø‰ΩøÁî®„ÄÇHySparseÈÄöËøáÂâçÈù¢ÁöÑÂÖ®Ê≥®ÊÑèÂäõÂ±ÇÊù•ÊåáÂØºÁ®ÄÁñèÂ±ÇÁöÑÊ†áËÆ∞ÈÄâÊã©ÂíåKVÁºìÂ≠òÁöÑÈáçÁî®Ôºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊïàÁéáÂíåÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïÁõ∏ÊØîÔºåHySparseÈÅøÂÖç‰∫Ü‰ΩøÁî®È¢ùÂ§ñÁöÑ‰ª£ÁêÜÊù•È¢ÑÊµãÊ†áËÆ∞ÈáçË¶ÅÊÄßÔºåÁõ¥Êé•Âà©Áî®ÂÖ®Ê≥®ÊÑèÂäõÂ±Ç‰Ωú‰∏∫ÂáÜÁ°ÆÁöÑÂèÇËÄÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåHySparseÂú®Â§ö‰∏™Ê®°ÂûãËÆæÁΩÆ‰∏≠Âùá‰ºò‰∫éÂÖ®Ê≥®ÊÑèÂäõÂíåÊ∑∑ÂêàSWAÂü∫Á∫øÔºåÊòæËëóÂáèÂ∞ë‰∫ÜKVÁºìÂ≠òÁöÑÂ≠òÂÇ®ÈúÄÊ±Ç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02958', 'title': 'Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization', 'url': 'https://huggingface.co/papers/2602.02958', 'abstract': 'Quant VideoGen addresses KV cache memory limitations in autoregressive video diffusion models through semantic-aware smoothing and progressive residual quantization, achieving significant memory reduction with minimal latency impact.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.', 'score': 31, 'issue_id': 913, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '26882eb2c6324f51', 'authors': ['Haocheng Xi', 'Shuo Yang', 'Yilong Zhao', 'Muyang Li', 'Han Cai', 'Xingyang Li', 'Yujun Lin', 'Zhuoyang Zhang', 'Jintao Zhang', 'Xiuyu Li', 'Zhiying Xu', 'Jun Wu', 'Chenfeng Xu', 'Ion Stoica', 'Song Han', 'Kurt Keutzer'], 'affiliations': ['MIT', 'MIT-IBM Watson AI Lab', 'Stanford University', 'Tsinghua University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2602.02958.jpg', 'data': {'categories': ['#optimization', '#inference', '#long_context', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Quant VideoGen –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ KV –∫–µ—à–∞ –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä—ã–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –æ–±—ä–µ–º KV –∫–µ—à–∞ –≤ 7 —Ä–∞–∑ –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–∞—Ö –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å (–º–µ–Ω–µ–µ 4%), —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏. –ü–æ–¥—Ö–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ–µ –ø—Ä–µ–æ–±–ª–∞–¥–∞–Ω–∏–µ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏, –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∏ –¥–≤–∏–∂–µ–Ω–∏–∏.'}, 'en': {'title': 'Revolutionizing Video Generation with Efficient Memory Management', 'desc': 'Quant VideoGen (QVG) is a novel framework designed to overcome the limitations of KV cache memory in autoregressive video diffusion models. It introduces Semantic Aware Smoothing to effectively manage video spatiotemporal redundancy, resulting in lower magnitude residuals that are easier to quantize. Additionally, QVG employs Progressive Residual Quantization, which systematically reduces quantization errors while balancing memory usage and video quality. This approach significantly decreases KV cache memory requirements by up to 7 times with minimal impact on latency, while enhancing the overall quality of video generation compared to existing methods.'}, 'zh': {'title': 'ÈáèÂåñËßÜÈ¢ëÁîüÊàêÔºöÂÜÖÂ≠ò‰∏éË¥®ÈáèÁöÑÂÆåÁæéÂπ≥Ë°°', 'desc': 'Quant VideoGenÔºàQVGÔºâÊòØ‰∏ÄÁßçÈíàÂØπËá™ÂõûÂΩíËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÁöÑKVÁºìÂ≠òÈáèÂåñÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥KVÁºìÂ≠òÂÜÖÂ≠òÈôêÂà∂ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáËØ≠‰πâÊÑüÁü•Âπ≥ÊªëÊäÄÊúØÔºåQVGËÉΩÂ§üÊúâÊïàÂà©Áî®ËßÜÈ¢ëÁöÑÊó∂Á©∫ÂÜó‰ΩôÔºåÁîüÊàê‰ΩéÂπÖÂ∫¶„ÄÅÈÄÇÂêàÈáèÂåñÁöÑÊÆãÂ∑Æ„ÄÇÂÆÉËøòÂºïÂÖ•‰∫ÜÊ∏êËøõÂºèÊÆãÂ∑ÆÈáèÂåñÊñπÊ°àÔºåÂáèÂ∞ëÈáèÂåñËØØÂ∑ÆÔºåÂêåÊó∂ÂÆûÁé∞Ë¥®Èáè‰∏éÂÜÖÂ≠òÁöÑÂπ≥Ë°°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQVGÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÈôç‰Ωé‰∫ÜKVÁºìÂ≠òÂÜÖÂ≠òÔºå‰∏îÁîüÊàêË¥®Èáè‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04515', 'title': 'EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models', 'url': 'https://huggingface.co/papers/2602.04515', 'abstract': 'EgoActor is a unified vision-language model that translates high-level instructions into precise humanoid robot actions through integrated perception and execution across simulated and real-world environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.', 'score': 30, 'issue_id': 913, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'bea7b12e4a91de7b', 'authors': ['Yu Bai', 'MingMing Yu', 'Chaojie Li', 'Ziyi Bai', 'Xinlong Wang', 'B√∂rje F. Karlsson'], 'affiliations': ['Beijing Academy of Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2602.04515.jpg', 'data': {'categories': ['#training', '#robotics', '#cv', '#multimodal'], 'emoji': 'ü§ñ', 'ru': {'title': '–û—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∫ –¥–µ–π—Å—Ç–≤–∏—è–º: —è–∑—ã–∫ —Ä–æ–±–æ—Ç–∞ –ø—Ä—è–º–æ –∏–∑ –≤–∏–¥–µ–Ω–∏—è', 'desc': 'EgoActor ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤ —Ç–æ—á–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –≥—É–º–∞–Ω–æ–∏–¥–Ω–æ–≥–æ —Ä–æ–±–æ—Ç–∞. –ú–æ–¥–µ–ª—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∏ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ, —Ä–∞–±–æ—Ç–∞—è –∫–∞–∫ –≤ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ RGB-–≤–∏–¥–µ–æ —Å —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–æ–π —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã –¥–µ–π—Å—Ç–≤–∏–π: –ª–æ–∫–æ–º–æ—Ü–∏—é, –¥–≤–∏–∂–µ–Ω–∏—è –≥–æ–ª–æ–≤—ã, –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å —á–µ–ª–æ–≤–µ–∫–æ–º, –ø—Ä–∏–Ω–∏–º–∞—è —Ä–µ—à–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —á–∞—Å—Ç–∏—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –º–µ–Ω—è—é—â–µ–π—Å—è —Å—Ä–µ–¥—ã. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è, —Ä–∞–±–æ—Ç–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å 4B –∏ 8B –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.'}, 'en': {'title': 'EgoActor: Bridging Instructions to Humanoid Actions', 'desc': 'EgoActor is a vision-language model designed to convert high-level instructions into specific actions for humanoid robots. It addresses the challenges of integrating perception, movement, and manipulation in dynamic environments with limited information. The model can predict various actions, such as walking and turning, while coordinating real-time perception and execution. Extensive testing shows that EgoActor can effectively link abstract planning with practical execution across different tasks and environments.'}, 'zh': {'title': 'EgoActorÔºöÂ∞ÜÈ´òÂ±ÇÊåá‰ª§ËΩ¨Âåñ‰∏∫‰∫∫ÂΩ¢Êú∫Âô®‰∫∫Âä®‰ΩúÁöÑÁªü‰∏ÄÊ®°Âûã', 'desc': 'EgoActor ÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂ∞ÜÈ´òÂ±ÇÊåá‰ª§ËΩ¨Âåñ‰∏∫Á≤æÁ°ÆÁöÑ‰∫∫ÂΩ¢Êú∫Âô®‰∫∫Âä®‰Ωú„ÄÇÂÆÉÈÄöËøáÈõÜÊàêÊÑüÁü•ÂíåÊâßË°åÔºåËß£ÂÜ≥‰∫ÜÂú®Âä®ÊÄÅÁéØÂ¢É‰∏≠ËøõË°åËøêÂä®ÂíåÊìç‰ΩúÁöÑÊåëÊàò„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üÈ¢ÑÊµãÂêÑÁßçËøêÂä®ÂéüËØ≠Âíå‰∫∫Êú∫‰∫§‰∫íÔºåÂÆûÊó∂ÂçèË∞ÉÊÑüÁü•‰∏éÊâßË°å„ÄÇÈÄöËøáÂØπÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÁöÑÂπøÊ≥õÁõëÁù£ÔºåEgoActor ËÉΩÂ§üÂú®‰∏çÂêå‰ªªÂä°ÂíåÊú™Áü•ÁéØÂ¢É‰∏≠ËøõË°åÊúâÊïàÁöÑÂÜ≥Á≠ñÂíåÊµÅÁïÖÁöÑÂä®‰ΩúÊé®ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02402', 'title': 'SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation', 'url': 'https://huggingface.co/papers/2602.02402', 'abstract': 'SoMA is a 3D Gaussian Splat simulator that enables stable, long-horizon manipulation of soft bodies by coupling deformable dynamics, environmental forces, and robot actions in a unified latent neural space.  \t\t\t\t\tAI-generated summary \t\t\t\t Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.', 'score': 29, 'issue_id': 913, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'f63bd3f7711df065', 'authors': ['Mu Huang', 'Hui Wang', 'Kerui Ren', 'Linning Xu', 'Yunsong Zhou', 'Mulin Yu', 'Bo Dai', 'Jiangmiao Pang'], 'affiliations': ['Fudan University, China', 'Shanghai Artificial Intelligence Laboratory, China', 'Shanghai Jiao Tong University, China', 'The Chinese University of Hong Kong, China', 'The University of Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.02402.jpg', 'data': {'categories': [], 'emoji': 'ü§ñ', 'ru': {'title': '–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π —Å–∏–º—É–ª—è—Ç–æ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º—è–≥–∫–∏–º–∏ —Ç–µ–ª–∞–º–∏ –±–µ–∑ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'SoMA ‚Äî —ç—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π —Å–∏–º—É–ª—è—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–≥–æ Gaussian Splat, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ñ–æ—Ä–º–∏—Ä—É–µ–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–π –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –∏ –¥–µ–π—Å—Ç–≤–∏–π —Ä–æ–±–æ—Ç–∞ –≤ –µ–¥–∏–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. –°–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –º—è–≥–∫–∏–º–∏ —Ç–µ–ª–∞–º–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤, –ø–æ–ª–∞–≥–∞—è—Å—å –Ω–∞ –æ–±—É—á–∞–µ–º—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –ü–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∏—è —Å–∏–º—É–ª—è—Ü–∏–∏ —Å —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –º–∏—Ä –Ω–∞ 20% –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ –Ω–æ–≤—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. SoMA –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Å–∫–ª–∞–¥—ã–≤–∞–Ω–∏–µ —Ç–∫–∞–Ω–∏ –Ω–∞ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö.'}, 'en': {'title': 'SoMA: Revolutionizing Soft Body Manipulation with Neural Simulation', 'desc': 'SoMA is a novel simulator designed for manipulating soft bodies in three dimensions. It integrates deformable dynamics, environmental forces, and robot actions into a single neural framework, allowing for more accurate and stable simulations. By using learned Gaussian splats, SoMA can model complex interactions without relying on predefined physics, enhancing its ability to generalize beyond previously observed scenarios. This results in improved performance in real-world tasks, such as cloth folding, with a significant increase in accuracy and stability during long-horizon manipulations.'}, 'zh': {'title': 'SoMAÔºöÁ®≥ÂÆöÁöÑËΩØ‰ΩìÊìçÊéßÊñ∞ÊñπÊ≥ï', 'desc': 'SoMAÊòØ‰∏ÄÁßç3DÈ´òÊñØÁÇπÊ®°ÊãüÂô®ÔºåÊó®Âú®ÂÆûÁé∞ËΩØ‰ΩìÁâ©‰ΩìÁöÑÁ®≥ÂÆöÂíåÈïøÊó∂Èó¥ÊìçÊéß„ÄÇÂÆÉÂ∞ÜÂèØÂèòÂΩ¢Âä®ÂäõÂ≠¶„ÄÅÁéØÂ¢ÉÂäõÂíåÊú∫Âô®‰∫∫Âä®‰ΩúÁªìÂêàÂú®‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊΩúÂú®Á•ûÁªèÁ©∫Èó¥‰∏≠„ÄÇ‰∏é‰º†ÁªüÊ®°ÊãüÂô®‰∏çÂêåÔºåSoMA‰∏ç‰æùËµñ‰∫éÈ¢ÑÂÆö‰πâÁöÑÁâ©ÁêÜÊ®°ÂûãÔºåËÄåÊòØÈÄöËøáÂ≠¶‰π†ÁöÑÈ´òÊñØÁÇπÊù•Âª∫Ê®°‰∫§‰∫íÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈáçÊ®°ÊãüÁöÑÂáÜÁ°ÆÊÄßÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂú®ÁúüÂÆû‰∏ñÁïåÁöÑÊú∫Âô®‰∫∫ÊìçÊéß‰∏≠ÊèêÈ´ò‰∫Ü20%ÁöÑÂáÜÁ°ÆÊÄßÔºå‰ΩøÂæóÂ§çÊùÇ‰ªªÂä°Â¶ÇÈïøÊó∂Èó¥ÁöÑÂ∏ÉÊñôÊäòÂè†ÂèòÂæóÂèØË°å„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02196', 'title': 'TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents', 'url': 'https://huggingface.co/papers/2602.02196', 'abstract': 'Test-Time Improvement (TTI) in autonomous LLM agents involves iterative environmental interaction that enhances performance, but current evaluation methods inadequately capture task optimization efficiency and memory utilization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.', 'score': 29, 'issue_id': 914, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'b5317920aaabb251', 'authors': ['Hang Yan', 'Xinyu Che', 'Fangzhi Xu', 'Qiushi Sun', 'Zichen Ding', 'Kanzhi Cheng', 'Jian Zhang', 'Tao Qin', 'Jun Liu', 'Qika Lin'], 'affiliations': ['Nanjing University', 'National University of Singapore', 'Shanghai AI Laboratory', 'The University of Hong Kong', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02196.jpg', 'data': {'categories': [], 'emoji': 'üîÑ', 'ru': {'title': '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è: –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è LLM –∞–≥–µ–Ω—Ç–æ–≤ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç Test-Time Improvement (TTI) ‚Äî –ø—Ä–æ—Ü–µ—Å—Å, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ LLM –∞–≥–µ–Ω—Ç—ã —É–ª—É—á—à–∞—é—Ç —Å–≤–æ—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç TIDE ‚Äî –Ω–æ–≤—É—é –æ—Ü–µ–Ω–æ—á–Ω—É—é –º–µ—Ç—Ä–∏–∫—É, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–±–∏—Ä–∞–µ—Ç TTI –Ω–∞ —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏—è: –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏–Ω–∞–º–∏–∫—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á, —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ –Ω–µ–∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ç—Ä–∞–∂–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —É—Ç–∏–ª–∏–∑–∞—Ü–∏—é –ø–∞–º—è—Ç–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞ —Ç—Ä–µ–±—É–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –Ω–æ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–æ–º –∏ –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Enhancing LLM Performance Through Test-Time Improvement', 'desc': 'This paper introduces Test-Time Improvement (TTI) for autonomous LLM agents, which enhances their performance through iterative interactions with their environment. It identifies shortcomings in current evaluation methods that fail to adequately measure task optimization efficiency and memory usage. To address these issues, the authors propose the Test-time Improvement Diagnostic Evaluation (TIDE) framework, which analyzes TTI through three key dimensions: task completion dynamics, recursive looping behaviors, and memory constraints. The findings suggest that optimizing agent performance involves improving the interaction dynamics rather than just increasing internal reasoning capabilities.'}, 'zh': {'title': '‰ºòÂåñ‰ª£ÁêÜ‰∏éÁéØÂ¢É‰∫íÂä®ÔºåÊèêÂçáÊÄßËÉΩÔºÅ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜËá™‰∏ªÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂú®ÊµãËØïÊó∂ÊîπËøõÔºàTTIÔºâ‰∏≠ÁöÑË°®Áé∞ÔºåÂº∫Ë∞É‰∫ÜÈÄöËøá‰∏éÁéØÂ¢ÉÁöÑËø≠‰ª£‰∫íÂä®Êù•ÊèêÂçáÊÄßËÉΩÁöÑÈáçË¶ÅÊÄß„ÄÇÂΩìÂâçÁöÑËØÑ‰º∞ÊñπÊ≥ïÊó†Ê≥ïÊúâÊïàÊçïÊçâ‰ªªÂä°‰ºòÂåñÊïàÁéáÂíåÂÜÖÂ≠òÂà©Áî®ÊÉÖÂÜµÔºåÂõ†Ê≠§Êàë‰ª¨ÊèêÂá∫‰∫ÜÊµãËØïÊó∂ÊîπËøõËØäÊñ≠ËØÑ‰º∞ÔºàTIDEÔºâÊ°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂Â∞ÜTTIÂàÜËß£‰∏∫‰∏â‰∏™Áõ∏‰∫íÂÖ≥ËÅîÁöÑÁª¥Â∫¶ÔºåËØÑ‰º∞‰ªªÂä°ÂÆåÊàêÁöÑÊó∂Èó¥Âä®ÊÄÅ‰ª•ÂèäÊÄßËÉΩÂèóÈôêÁöÑÂéüÂõ†„ÄÇÈÄöËøáÂπøÊ≥õÁöÑÂÆûÈ™åÔºåTIDEË°®ÊòéÔºåÊèêÂçá‰ª£ÁêÜÊÄßËÉΩ‰∏ç‰ªÖÈúÄË¶ÅÊâ©Â±ïÂÜÖÈÉ®Êé®ÁêÜËÉΩÂäõÔºåËøòÈúÄË¶Å‰ºòÂåñ‰ª£ÁêÜ‰∏éÁéØÂ¢É‰πãÈó¥ÁöÑ‰∫íÂä®Âä®ÊÄÅ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22954', 'title': 'Residual Context Diffusion Language Models', 'url': 'https://huggingface.co/papers/2601.22954', 'abstract': 'Residual Context Diffusion (RCD) enhances diffusion large language models by recycling discarded token information through contextual residuals, improving accuracy with minimal computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.', 'score': 27, 'issue_id': 913, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'a191f56a527a7d38', 'authors': ['Yuezhou Hu', 'Harman Singh', 'Monishwaran Maheswaran', 'Haocheng Xi', 'Coleman Hooper', 'Jintao Zhang', 'Aditya Tomar', 'Michael W. Mahoney', 'Sewon Min', 'Mehrdad Farajtabar', 'Kurt Keutzer', 'Amir Gholami', 'Chenfeng Xu'], 'affiliations': ['Apple', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2601.22954.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#reasoning'], 'emoji': '‚ôªÔ∏è', 'ru': {'title': '–í–æ–∑—Ä–æ–∂–¥–µ–Ω–∏–µ –æ—Ç–±—Ä–æ—à–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Residual Context Diffusion (RCD), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—É—Ç—ë–º –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø–µ—Ä–µmasking –≤ –±–ª–æ—á–Ω—ã—Ö –¥LLM-–º–æ–¥–µ–ª—è—Ö –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, —Ö–æ—Ç—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –æ—Å—Ç–∞—Ç–∫–∏ –∏ –≤–≤–æ–¥–∏—Ç –∏—Ö –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ —ç—Ç–∞–ø–µ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —É–∑–∫–∏—Ö –º–µ—Å—Ç –ø–∞–º—è—Ç–∏. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ 5-10 –ø—É–Ω–∫—Ç–æ–≤ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞–∫–ª–∞–¥–Ω—ã–º–∏ —Ä–∞—Å—Ö–æ–¥–∞–º–∏ –∏ –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —É–¥–≤–∞–∏–≤–∞—è –±–∞–∑–æ–≤—É—é —Ç–æ—á–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Recycling Discarded Tokens for Enhanced Language Model Performance', 'desc': 'Residual Context Diffusion (RCD) is a novel approach that enhances diffusion large language models (dLLMs) by reusing information from discarded tokens during the decoding process. Traditional dLLMs often discard less confident tokens, which leads to wasted computational resources. RCD addresses this by converting these discarded tokens into contextual residuals, which are then reintegrated into the decoding steps, improving overall model accuracy. This method not only boosts performance significantly but also maintains low computational costs, making it an efficient upgrade for existing dLLMs.'}, 'zh': {'title': 'ÊèêÂçáÊâ©Êï£Ê®°ÂûãÁöÑÊÆãÂ∑Æ‰∏ä‰∏ãÊñáÂà©Áî®', 'desc': 'ÊÆãÂ∑Æ‰∏ä‰∏ãÊñáÊâ©Êï£ÔºàRCDÔºâÈÄöËøáÂõûÊî∂Ë¢´‰∏¢ÂºÉÁöÑÊ†áËÆ∞‰ø°ÊÅØÔºåÂ¢ûÂº∫‰∫ÜÊâ©Êï£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®‰∏ä‰∏ãÊñáÊÆãÂ∑ÆÔºåÂ∞Ü‰∏¢ÂºÉÁöÑÊ†áËÆ∞Ë°®Á§∫ËΩ¨Âåñ‰∏∫ÊúâÁî®ÁöÑ‰ø°ÊÅØÔºåÊ≥®ÂÖ•Âà∞ÂêéÁª≠ÁöÑÂéªÂô™Ê≠•È™§‰∏≠Ôºå‰ªéËÄåÊèêÈ´òËß£Á†ÅÁöÑÂáÜÁ°ÆÊÄß„ÄÇRCDÈááÁî®Ëß£ËÄ¶ÁöÑ‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊµÅÁ®ãÔºåÈÅøÂÖç‰∫ÜÂèçÂêë‰º†Êí≠Â∏¶Êù•ÁöÑÂÜÖÂ≠òÁì∂È¢à„ÄÇÂÆûÈ™åË°®ÊòéÔºåRCDÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÔºåËÉΩÂ§üÂú®ËÆ°ÁÆóÂºÄÈîÄÊûÅÂ∞èÁöÑÊÉÖÂÜµ‰∏ãÔºåÊèêÂçáÂâçÊ≤øÊâ©Êï£Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04879', 'title': 'Rethinking the Trust Region in LLM Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.04879', 'abstract': 'DPPO addresses limitations in PPO for LLM fine-tuning by replacing ratio clipping with direct policy divergence constraints, improving training stability and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.', 'score': 25, 'issue_id': 913, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'fe26031fa8cb13a1', 'authors': ['Penghui Qi', 'Xiangxin Zhou', 'Zichen Liu', 'Tianyu Pang', 'Chao Du', 'Min Lin', 'Wee Sun Lee'], 'affiliations': ['School of Computing, National University of Singapore', 'Sea AI Lab, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2602.04879.jpg', 'data': {'categories': ['#rlhf', '#rl', '#training'], 'emoji': 'üéØ', 'ru': {'title': '–û—Ç –æ–±—Ä–µ–∑–∞–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∫ –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω—ã–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º: —Å—Ç–∞–±–∏–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LLM —á–µ—Ä–µ–∑ DPPO', 'desc': 'DPPO ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ PPO –ø—Ä–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –í–º–µ—Å—Ç–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –æ–±—Ä–µ–∑–∞–Ω–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä—è–º–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ–∂–¥—É –ø–æ–ª–∏—Ç–∏–∫–∞–º–∏, —Ç–∞–∫–æ–µ –∫–∞–∫ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –ö—É–ª—å–±–∞–∫–∞-–õ–µ–π–±–ª–µ—Ä–∞. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∏ –≤—ã—Å–æ–∫–æ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –ø—Ä–∏–≤–æ–¥—è –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è Binary –∏ Top-K, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –∏–∑–±–µ–∂–∞—Ç—å –±–æ–ª—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'DPPO: A Smarter Way to Fine-Tune Language Models', 'desc': 'This paper introduces Divergence Proximal Policy Optimization (DPPO), a new method for fine-tuning Large Language Models (LLMs) that improves upon the traditional Proximal Policy Optimization (PPO) algorithm. The authors argue that the ratio clipping mechanism in PPO is not effective for LLMs due to their large vocabularies, leading to inefficient and unstable training. DPPO replaces this clipping with direct policy divergence constraints, which provide a more accurate measure of policy changes. The paper also presents efficient approximations to manage memory usage, demonstrating that DPPO significantly enhances training stability and efficiency in reinforcement learning applications for LLMs.'}, 'zh': {'title': 'DPPOÔºöÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂæÆË∞ÉÁöÑÁ®≥ÂÆöÊÄß‰∏éÊïàÁéá', 'desc': 'DPPOÔºàDivergence Proximal Policy OptimizationÔºâÈÄöËøáÁî®Áõ¥Êé•ÁöÑÁ≠ñÁï•ÂèëÊï£Á∫¶ÊùüÊõø‰ª£PPO‰∏≠ÁöÑÊØîÁéáË£ÅÂâ™ÔºåËß£ÂÜ≥‰∫ÜPPOÂú®Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂæÆË∞É‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇËøôÁßçÊñπÊ≥ïÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊïàÁéáÔºåÁâπÂà´ÊòØÂú®Â§ÑÁêÜÂ§ßËØçÊ±áÈáèÊó∂„ÄÇ‰º†ÁªüÁöÑPPOÊñπÊ≥ïÂú®Êõ¥Êñ∞‰ΩéÊ¶ÇÁéátokenÊó∂ËøáÂ∫¶ÊÉ©ÁΩöÔºåËÄåÂØπÈ´òÊ¶ÇÁéátokenÁöÑÂèòÂåñÂàôÁ∫¶Êùü‰∏çË∂≥ÔºåÂØºËá¥ËÆ≠ÁªÉ‰∏çÁ®≥ÂÆö„ÄÇDPPOÈÄöËøáÂºïÂÖ•È´òÊïàÁöÑ‰∫åËøõÂà∂ÂíåTop-KËøë‰ººÊñπÊ≥ïÔºåËÉΩÂ§üÂú®‰øùÊåÅ‰ΩéÂÜÖÂ≠òÂç†Áî®ÁöÑÂêåÊó∂ÔºåÂáÜÁ°ÆÊçïÊçâÁ≠ñÁï•ÂèëÊï£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03510', 'title': 'Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers', 'url': 'https://huggingface.co/papers/2602.03510', 'abstract': "Text conditioning in DiT-based models is enhanced through a unified normalized convex fusion framework that optimizes multi-layer LLM hidden states via depth-wise semantic routing, improving text-image alignment and compositional generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.", 'score': 23, 'issue_id': 915, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '56a039064235d5b2', 'authors': ['Bozhou Li', 'Yushuo Guan', 'Haolin Li', 'Bohan Zeng', 'Yiyan Ji', 'Yue Ding', 'Pengfei Wan', 'Kun Gai', 'Yuanxing Zhang', 'Wentao Zhang'], 'affiliations': ['Fudan University', 'Kling Team, Kuaishou Technology', 'Nanjing University', 'Peking University', 'School of Artificial Intelligence, University of'], 'pdf_title_img': 'assets/pdf/title_img/2602.03510.jpg', 'data': {'categories': ['#diffusion'], 'emoji': 'üé®', 'ru': {'title': '–ì–ª—É–±–∏–Ω–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –¥–∏—Ñ—Ñ—É–∑–∏–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —É—Å–ª–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ DiT-–º–æ–¥–µ–ª—è—Ö –ø—É—Ç—ë–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ—ë–≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –º–µ—Ö–∞–Ω–∏–∑–º depth-wise semantic routing, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≥–ª—É–±–∏–Ω—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –µ—ë –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –¥–∏–Ω–∞–º–∏–∫–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, –∞ —Ç–∞–∫–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –¢–∞–∫–∂–µ –≤—ã—è–≤–ª–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ classifier-free guidance, –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–µ–±—É–µ—Ç —É—á—ë—Ç–∞ —Ä–µ–∞–ª—å–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è.'}, 'en': {'title': 'Enhancing Text-Image Alignment with Depth-wise Semantic Routing', 'desc': 'This paper presents a new framework for improving text conditioning in DiT-based text-to-image models by using a method called Depth-wise Semantic Routing. The framework optimizes the hidden states of large language models (LLMs) across multiple layers, allowing for better alignment between text and images during the generation process. By employing lightweight gates for organizing these hidden states, the model enhances its ability to generate coherent and contextually relevant images. The findings indicate that this depth-wise approach significantly outperforms traditional methods, particularly in tasks requiring compositional understanding.'}, 'zh': {'title': 'Ê∑±Â∫¶ËØ≠‰πâË∑ØÁî±ÔºöÊèêÂçáÊñáÊú¨‰∏éÂõæÂÉèÁîüÊàêÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÂΩí‰∏ÄÂåñÂá∏ËûçÂêàÊ°ÜÊû∂Ôºå‰ª•Â¢ûÂº∫Âü∫‰∫éDiTÁöÑÊñáÊú¨ÁîüÊàêÊ®°Âûã‰∏≠ÁöÑÊñáÊú¨Êù°‰ª∂Âåñ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊ∑±Â∫¶ËØ≠‰πâË∑ØÁî±‰ºòÂåñÂ§öÂ±ÇÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÈöêËóèÁä∂ÊÄÅÔºå‰ªéËÄåÊîπÂñÑÊñáÊú¨‰∏éÂõæÂÉèÁöÑÂØπÈΩêÂíåÁªÑÂêàÁîüÊàêËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊ∑±Â∫¶ËØ≠‰πâË∑ØÁî±ÊòØ‰ºòË∂äÁöÑÊù°‰ª∂Á≠ñÁï•ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊñáÊú¨‰∏éÂõæÂÉèÁöÑÂåπÈÖçÂ∫¶„ÄÇÊàë‰ª¨ËøòÂèëÁé∞ÔºåÂçïÁ∫ØÁöÑÊó∂Èó¥ËûçÂêàÂèØËÉΩ‰ºöÈôç‰ΩéËßÜËßâÁîüÊàêÁöÑË¥®ÈáèÔºåÂõ†Ê≠§ÈúÄË¶ÅÂÖ≥Ê≥®Êó∂Èó¥‰æùËµñÁöÑ‰ø°Âè∑‰ª•ÂÆûÁé∞Êõ¥Á®≥ÂÅ•ÁöÑÊù°‰ª∂Âåñ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03907', 'title': 'HY3D-Bench: Generation of 3D Assets', 'url': 'https://huggingface.co/papers/2602.03907', 'abstract': 'HY3D-Bench presents an open-source ecosystem for 3D content creation that provides high-fidelity 3D objects and synthetic assets to advance 3D generation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.', 'score': 21, 'issue_id': 914, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '8be299843765de41', 'authors': ['Team Hunyuan3D', ':', 'Bowen Zhang', 'Chunchao Guo', 'Dongyuan Guo', 'Haolin Liu', 'Hongyu Yan', 'Huiwen Shi', 'Jiaao Yu', 'Jiachen Xu', 'Jingwei Huang', 'Kunhong Li', 'Lifu Wang', 'Linus', 'Penghao Wang', 'Qingxiang Lin', 'Ruining Tang', 'Xianghui Yang', 'Yang Li', 'Yirui Guan', 'Yunfei Zhao', 'Yunhan Yang', 'Zeqiang Lai', 'Zhihao Liang', 'Zibo Zhao'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2602.03907.jpg', 'data': {'categories': ['#robotics', '#dataset', '#data', '#synthetic', '#open_source', '#3d'], 'emoji': 'üé®', 'ru': {'title': '–ï–¥–∏–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–≤–æ–ª—é—Ü–∏–∏ –≤ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏', 'desc': 'HY3D-Bench ‚Äî —ç—Ç–æ –æ—Ç–∫—Ä—ã—Ç—ã–π —ç–∫–æ—Å–∏—Å—Ç–µ–º –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–π –±–æ–ª—å—à—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∞—Å—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏ 250 —Ç—ã—Å—è—á 3D-–º–æ–¥–µ–ª–µ–π –∏–∑ –∫—Ä—É–ø–Ω—ã—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤, —Å–æ–∑–¥–∞–≤ –≥–æ—Ç–æ–≤—ã–µ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤–∫–ª—é—á–∞—è –∑–∞–º–∫–Ω—É—Ç—ã–µ —Å–µ—Ç–∫–∏ –∏ –º–Ω–æ–≥–æ—Ä–∞–∫—É—Ä—Å–Ω—ã–µ —Ä–µ–Ω–¥–µ—Ä—ã. –ö–ª—é—á–µ–≤–æ–π –≤–∫–ª–∞–¥ —Ä–∞–±–æ—Ç—ã ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —á–∞—Å—Ç–µ–π –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –¥–æ—Å—Ç–∏—á—å —Ç–æ—á–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ —É–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è 3D-–∫–æ–Ω—Ç–µ–Ω—Ç–∞. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–æ 125 —Ç—ã—Å—è—á –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞—Å—Å–µ—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏.'}, 'en': {'title': 'Empowering 3D Creation with HY3D-Bench', 'desc': 'HY3D-Bench is an open-source platform that enhances 3D content creation by providing a large library of high-quality 3D objects and synthetic assets. It addresses data processing challenges in the field by offering 250,000 meticulously curated 3D objects, which are ready for training with features like watertight meshes and multi-view renderings. The platform also introduces structured part-level decomposition, allowing for detailed perception and editing of 3D models. Additionally, it includes a scalable AIGC synthesis pipeline that generates 125,000 synthetic assets, improving diversity in 3D categories and supporting advancements in robotics and digital content creation.'}, 'zh': {'title': 'HY3D-BenchÔºöÊé®Âä®3DÂàõ‰ΩúÁöÑÂºÄÊ∫êÁîüÊÄÅÁ≥ªÁªü', 'desc': 'HY3D-BenchÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁîüÊÄÅÁ≥ªÁªüÔºåÊó®Âú®Êé®Âä®3DÂÜÖÂÆπÂàõ‰ΩúÁöÑËÉΩÂäõ„ÄÇÂÆÉÊèê‰æõ‰∫Ü25‰∏á‰∏™È´ò‰øùÁúü3DÂØπË±°ÂíåÂêàÊàêËµÑ‰∫ßÔºåËß£ÂÜ≥‰∫ÜÊï∞ÊçÆÂ§ÑÁêÜÁì∂È¢àÁöÑÈóÆÈ¢ò„ÄÇËØ•Á≥ªÁªüÈÄöËøáÁªìÊûÑÂåñÁöÑÈÉ®‰ª∂Á∫ßÂàÜËß£ÔºåÊîØÊåÅÁªÜÁ≤íÂ∫¶ÁöÑÊÑüÁü•ÂíåÂèØÊéßÁºñËæë„ÄÇHY3D-BenchËøòÈÄöËøáÂèØÊâ©Â±ïÁöÑAIGCÂêàÊàêÁÆ°ÈÅìÔºåÂ¢ûÂä†‰∫Ü12.5‰∏á‰∏™ÂêàÊàêËµÑ‰∫ßÔºåÊèêÂçá‰∫ÜÈïøÂ∞æÁ±ªÂà´ÁöÑÂ§öÊ†∑ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03828', 'title': 'AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations', 'url': 'https://huggingface.co/papers/2602.03828', 'abstract': '', 'score': 19, 'issue_id': 921, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '6493ae5590ba73a3', 'authors': ['Minjun Zhu', 'Zhen Lin', 'Yixuan Weng', 'Panzhong Lu', 'Qiujie Xie', 'Yifan Wei', 'Sifan Liu', 'Qiyao Sun', 'Yue Zhang'], 'affiliations': ['Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2602.03828.jpg', 'data': {'categories': [], 'emoji': 'üöÄ', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è LLM –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, —Å–æ—á–µ—Ç–∞—é—â—É—é –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–∞–ª–æ–≥–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏. –≠—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è AI –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.'}, 'en': {'title': 'Hybrid Models: Bridging Spatial and Temporal Learning', 'desc': 'This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the training process and the importance of regularization techniques to prevent overfitting.'}, 'zh': {'title': 'Ê∑±Â∫¶Â≠¶‰π†Êñ∞Ê®°ÂûãÔºåÊèêÂçáÂõæÂÉèÂàÜÁ±ªÁ≤æÂ∫¶ÔºÅ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÂõæÂÉèÂàÜÁ±ªÁöÑÂáÜÁ°ÆÊÄß„ÄÇËØ•Ê®°ÂûãÁªìÂêà‰∫ÜÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNÔºâÂíåÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºàRNNÔºâÁöÑ‰ºòÁÇπÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÊçïÊçâÂõæÂÉè‰∏≠ÁöÑÁ©∫Èó¥ÂíåÊó∂Èó¥ÁâπÂæÅ„ÄÇÈÄöËøáÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äËøõË°åÂÆûÈ™åÔºåÁªìÊûúË°®ÊòéËØ•Ê®°ÂûãÂú®ÂàÜÁ±ª‰ªªÂä°‰∏≠‰ºò‰∫éÁé∞ÊúâÁöÑ‰∏ªÊµÅÊñπÊ≥ï„ÄÇÊ≠§Á†îÁ©∂‰∏∫ÂõæÂÉèÂ§ÑÁêÜÈ¢ÜÂüüÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑ØÂíåÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03143', 'title': 'Self-Hinting Language Models Enhance Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.03143', 'abstract': "SAGE is an on-policy reinforcement learning framework that enhances GRPO by injecting self-hints during training to increase outcome diversity under sparse rewards, improving alignment of large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt x, the model samples a compact hint h (e.g., a plan or decomposition) and then generates a solution œÑ conditioned on (x,h). Crucially, the task reward R(x,œÑ) is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set h=varnothing and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.", 'score': 19, 'issue_id': 913, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '2755aeac8258da3c', 'authors': ['Baohao Liao', 'Hanze Dong', 'Xinxing Xu', 'Christof Monz', 'Jiang Bian'], 'affiliations': ['Language Technology Lab, University of Amsterdam', 'Microsoft', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2602.03143.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#alignment', '#open_source', '#training', '#rl'], 'emoji': 'üéØ', 'ru': {'title': '–£—Å–∏–ª–µ–Ω–∏–µ GRPO —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'SAGE ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º GRPO –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –≤–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥, –∫–æ–≥–¥–∞ –æ—Ç–∫–∞—Ç—ã –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø—ã –ø–æ–ª—É—á–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω—É–ª–µ–≤—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–ª–∞–Ω—ã –∏–ª–∏ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è), –∫–æ—Ç–æ—Ä—ã–µ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø–µ, –Ω–æ –Ω–µ –≤–ª–∏—è—é—Ç –Ω–∞ –∏—Å—Ö–æ–¥–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ù–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è, –∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —É—á–µ–±–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —É–∑–∫–∏—Ö –º–µ—Å—Ç–∞—Ö –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'SAGE: Enhancing Learning Diversity with Self-Hints', 'desc': "SAGE is a new reinforcement learning framework that builds on Group Relative Policy Optimization (GRPO) by introducing self-hints during training. These self-hints help to diversify the outcomes when rewards are sparse, which prevents the model from getting stuck due to identical rewards in a group. By sampling hints that guide the model's learning process, SAGE reshapes the rollout distribution while keeping the task reward unchanged. Experiments show that SAGE outperforms GRPO across multiple benchmarks, demonstrating its effectiveness in improving the alignment of large language models."}, 'zh': {'title': 'Ëá™ÊàëÊèêÁ§∫ÊèêÂçáÂº∫ÂåñÂ≠¶‰π†ÊïàÊûú', 'desc': 'SAGEÊòØ‰∏ÄÁßçÂü∫‰∫éÁ≠ñÁï•ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ê≥®ÂÖ•Ëá™ÊàëÊèêÁ§∫Êù•Â¢ûÂº∫GRPOÁöÑÊïàÊûúÔºå‰ªéËÄåÊèêÈ´òÂú®Á®ÄÁñèÂ•ñÂä±‰∏ãÁöÑÁªìÊûúÂ§öÊ†∑ÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰∏∫ÊØè‰∏™ÊèêÁ§∫ÁîüÊàêÁ¥ßÂáëÁöÑÊèêÁ§∫ÔºåÂ∏ÆÂä©Ê®°ÂûãÂú®Áõ∏ÂêåÁöÑÁªàÁ´ØÈ™åËØÅÂ•ñÂä±‰∏ãÈáçÂ°ëÂõûÊªöÂàÜÂ∏É„ÄÇSAGEÁöÑÂÖ≥ÈîÆÂú®‰∫éÔºåÂÆÉÂú®ÊµãËØïÊó∂‰∏ç‰ΩøÁî®‰ªª‰ΩïÊèêÁ§∫ÔºåÁ°Æ‰øùÊ®°ÂûãÂú®Ê≤°ÊúâÁâπÊùÉ‰ø°ÊÅØÁöÑÊÉÖÂÜµ‰∏ã‰ªçËÉΩË°®Áé∞ËâØÂ•Ω„ÄÇÊ≠§Â§ñÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSAGEÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Âùá‰ºò‰∫éGRPOÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®Â§ßËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩêÊñπÈù¢ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04575', 'title': 'Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration', 'url': 'https://huggingface.co/papers/2602.04575', 'abstract': "Vibe AIGC introduces a new generative AI paradigm where users provide high-level aesthetic and functional preferences, which are then orchestrated through multi-agent workflows to bridge the gap between human intent and machine execution.  \t\t\t\t\tAI-generated summary \t\t\t\t For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the Vibe AIGC, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows.   Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.", 'score': 17, 'issue_id': 913, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '78e831000eb7b72d', 'authors': ['Jiaheng Liu', 'Yuanxing Zhang', 'Shihao Li', 'Xinping Lei'], 'affiliations': ['Kling Team, Kuaishou Technology', 'NJU-LINK Team, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2602.04575.jpg', 'data': {'categories': ['#agents', '#multimodal'], 'emoji': 'üé≠', 'ru': {'title': '–û—Ç —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –∑–∞–º—ã—Å–ª–∞ –∏ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è', 'desc': 'Vibe AIGC –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ AI, –≥–¥–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —ç—Å—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–º Meta-Planner, –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—èIntent-Execution Gap ‚Äî —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –∑–∞–º—ã—Å–ª–æ–º –∏ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–º –≤—ã—Ö–æ–¥–æ–º —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –í–º–µ—Å—Ç–æ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∏–∑ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å–∏—Å—Ç–µ–º–∞ –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫—É—é –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—é –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–æ–∑–¥–∞–≤–∞—è –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –∫–æ–Ω–≤–µ–π–µ—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç AI –∏–∑ —Ö—Ä—É–ø–∫–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤—ã–≤–æ–¥–∞ –≤ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–≥–æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–∫—Ç–∏–≤–æ–≤.'}, 'en': {'title': 'Bridging Human Intent and Machine Execution with Vibe AIGC', 'desc': "Vibe AIGC presents a new approach to generative AI that allows users to express their aesthetic and functional preferences more effectively. Instead of relying solely on traditional models, it utilizes multi-agent workflows to better align human intent with machine output. Users act as Commanders, providing a high-level 'Vibe' that guides the content generation process. This method aims to overcome the Intent-Execution Gap by transforming AI into a reliable partner in creating complex digital assets."}, 'zh': {'title': 'Vibe AIGCÔºöÈáçÂ°ë‰∫∫Êú∫Âçè‰ΩúÁöÑÁîüÊàêÊÄßAIÊñ∞ËåÉÂºè', 'desc': 'Vibe AIGCÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩËåÉÂºèÔºåÁî®Êà∑ÂèØ‰ª•Êèê‰æõÈ´òÂ±ÇÊ¨°ÁöÑÁæéÂ≠¶ÂíåÂäüËÉΩÂÅèÂ•Ω„ÄÇËøô‰∫õÂÅèÂ•ΩÈÄöËøáÂ§ö‰ª£ÁêÜÂ∑•‰ΩúÊµÅËøõË°åÂçèË∞ÉÔºåÊó®Âú®Áº©Â∞è‰∫∫Á±ªÊÑèÂõæ‰∏éÊú∫Âô®ÊâßË°å‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇÁî®Êà∑ÁöÑËßíËâ≤‰ªé‰º†ÁªüÁöÑÊèêÁ§∫Â∑•Á®ãÂ∏àËΩ¨Âèò‰∏∫ÊåáÊå•ÂÆòÔºåÊèê‰æõ‰∏Ä‰∏™ÂåÖÂê´ÁæéÂ≠¶ÂíåÂäüËÉΩÈÄªËæëÁöÑÈ´òÂ±ÇÊ¨°Ë°®Á§∫„ÄÇÈÄöËøáÈÄªËæëÁºñÊéíÁöÑËΩ¨ÂèòÔºåVibe AIGCÂ∞Ü‰∫∫Á±ªÊÉ≥Ë±°‰∏éÊú∫Âô®ÊâßË°åËøûÊé•Ëµ∑Êù•ÔºåÈáçÊñ∞ÂÆö‰πâ‰∫∫Êú∫Âçè‰ΩúÁªèÊµé„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03973', 'title': 'VLS: Steering Pretrained Robot Policies via Vision-Language Models', 'url': 'https://huggingface.co/papers/2602.03973', 'abstract': 'Pretrained diffusion and flow-matching policies fail under test-time shifts due to tight coupling with training configurations, prompting the development of Vision-Language Steering (VLS) for training-free inference-time adaptation through vision-language model-guided trajectory steering.  \t\t\t\t\tAI-generated summary \t\t\t\t Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/', 'score': 17, 'issue_id': 914, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '23559ab6c18e6f3d', 'authors': ['Shuo Liu', 'Ishneet Sukhvinder Singh', 'Yiqing Xu', 'Jiafei Duan', 'Ranjay Krishna'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'National University of Singapore', 'University of Oxford', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2602.03973.jpg', 'data': {'categories': ['#robotics', '#optimization', '#diffusion', '#training', '#multimodal', '#inference'], 'emoji': 'ü§ñ', 'ru': {'title': '–ê–¥–∞–ø—Ç–∞—Ü–∏—è –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã—Ö –ø–æ–ª–∏—Ç–∏–∫ —Ä–æ–±–æ—Ç–∞ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ —á–µ—Ä–µ–∑ –≤–∏–¥–µ–æ—è–∑—ã–∫–æ–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞, –∫–æ–≥–¥–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ flow-matching –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ —É—Å–ª–æ–≤–∏–π –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑-–∑–∞ —Ç–µ—Å–Ω–æ–π —Å–≤—è–∑–∏ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Vision-Language Steering (VLS) ‚Äî –º–µ—Ç–æ–¥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–∏—Ç–∏–∫ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∏–¥–µ–æ—è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è. –ü–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã, —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ–±—ã –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–µ–π—Å—Ç–≤–∏–π –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 31% –≤ CALVIN –∏ 13% –≤ LIBERO-PRO, –∞ —Ç–∞–∫–∂–µ —É—Å–ø–µ—à–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º —Ä–æ–±–æ—Ç–µ Franka.'}, 'en': {'title': 'Adapt and Overcome: Vision-Language Steering for Robust Robotics', 'desc': "This paper addresses the limitations of pretrained diffusion and flow-matching policies in robotics when faced with changes in the environment during testing. These policies often fail due to their reliance on specific training conditions, which do not generalize well to new situations. To overcome this, the authors introduce Vision-Language Steering (VLS), a method that allows for real-time adaptation of robot actions without retraining. VLS utilizes vision-language models to adjust the robot's trajectory based on new observations, significantly improving performance in varied environments."}, 'zh': {'title': 'ËßÜËßâ-ËØ≠Ë®ÄÂºïÂØºÔºöÊó†ËÆ≠ÁªÉÁöÑÊé®ÁêÜÊó∂ÈÄÇÂ∫îÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ËßÜËßâ-ËØ≠Ë®ÄÂºïÂØºÔºàVLSÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éÂú®Êé®ÁêÜÊó∂ÈÄÇÂ∫îÂÜªÁªìÁöÑÁîüÊàêÊú∫Âô®‰∫∫Á≠ñÁï•„ÄÇ‰º†ÁªüÁöÑÈ¢ÑËÆ≠ÁªÉÊâ©Êï£ÊàñÊµÅÂåπÈÖçÁ≠ñÁï•Âú®ÊµãËØïÊó∂ÈÅáÂà∞ÁéØÂ¢ÉÂèòÂåñÊó∂Ë°®Áé∞‰∏ç‰Ω≥Ôºå‰∏ªË¶ÅÊòØÂõ†‰∏∫ÂÆÉ‰ª¨‰∏éËÆ≠ÁªÉÈÖçÁΩÆÁ¥ßÂØÜËÄ¶Âêà„ÄÇVLSÈÄöËøáÂà©Áî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂèØÂæÆÂàÜÁöÑÂ•ñÂä±ÂáΩÊï∞ÔºåÊåáÂØºÂéªÂô™ËøáÁ®ãÔºå‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®‰∏ç‰øÆÊîπÁ≠ñÁï•ÂèÇÊï∞ÁöÑÊÉÖÂÜµ‰∏ãÔºåÈÄÇÂ∫îÊñ∞ÁöÑÁ©∫Èó¥Âíå‰ªªÂä°Ë¶ÅÊ±Ç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVLSÂú®Ê®°ÊãüÂíåÁúüÂÆû‰∏ñÁïåËØÑ‰º∞‰∏≠Âùá‰ºò‰∫é‰ª•ÂæÄÁöÑÊñπÊ≥ïÔºåÊòæËëóÊèêÈ´ò‰∫ÜÈÄÇÂ∫îËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03587', 'title': 'CL-bench: A Benchmark for Context Learning', 'url': 'https://huggingface.co/papers/2602.03587', 'abstract': 'Language models struggle with context learning, requiring new knowledge and reasoning beyond pre-training, as demonstrated by a comprehensive benchmark revealing poor performance on real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.', 'score': 17, 'issue_id': 920, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '66980819a7bff281', 'authors': ['Shihan Dou', 'Ming Zhang', 'Zhangyue Yin', 'Chenhao Huang', 'Yujiong Shen', 'Junzhe Wang', 'Jiayi Chen', 'Yuchen Ni', 'Junjie Ye', 'Cheng Zhang', 'Huaibing Xie', 'Jianglu Hu', 'Shaolei Wang', 'Weichao Wang', 'Yanling Xiao', 'Yiting Liu', 'Zenan Xu', 'Zhen Guo', 'Pluto Zhou', 'Tao Gui', 'Zuxuan Wu', 'Xipeng Qiu', 'Qi Zhang', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Di Wang', 'Shunyu Yao'], 'affiliations': ['Hunyuan Team, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2602.03587.jpg', 'data': {'categories': ['#long_context', '#reasoning', '#dataset', '#benchmark'], 'emoji': 'üìö', 'ru': {'title': '–Ø–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —É–º–µ—é—Ç –ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É —É—á–∏—Ç—å—Å—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ CL-bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –æ–±—É—á–µ–Ω–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 500 —Å–ª–æ–∂–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏ 1899 –∑–∞–¥–∞—á, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏, –≥–¥–µ —Ä–µ—à–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç —É—Å–≤–æ–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π, –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏. –û—Ü–µ–Ω–∫–∞ –¥–µ—Å—è—Ç–∏ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö LLM –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞—é—Ç —Ç–æ–ª—å–∫–æ 23,7% –∑–∞–¥–∞—á, —á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é. –≠—Ç–∞ —Ä–∞–±–æ—Ç–∞ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–±–µ–ª –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–∏—é –±–æ–ª–µ–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, —Å–ø–æ—Å–æ–±–Ω—ã—Ö —Ä–µ—à–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏.'}, 'en': {'title': 'Unlocking Context Learning for Real-World AI', 'desc': 'This paper addresses the limitations of current language models (LMs) in context learning, which is essential for understanding and solving complex real-world tasks. It introduces CL-bench, a new benchmark designed to evaluate LMs on their ability to learn from specific contexts and apply new knowledge beyond their pre-training. The benchmark includes 500 complex contexts and nearly 2,000 tasks, highlighting the need for models to integrate domain-specific knowledge and reasoning skills. Evaluations show that even the best models struggle with context learning, achieving only 23.7% task success, indicating a significant gap in their capabilities for real-world applications.'}, 'zh': {'title': 'ÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÂ≠¶‰π†ËÉΩÂäõ', 'desc': 'ÂΩìÂâçÁöÑËØ≠Ë®ÄÊ®°ÂûãÂú®‰ΩøÁî®È¢ÑËÆ≠ÁªÉÁü•ËØÜËøõË°åÊé®ÁêÜÊó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Â§ÑÁêÜÂ§çÊùÇÁöÑÁé∞ÂÆû‰ªªÂä°Êó∂Âç¥Èù¢‰∏¥ÊåëÊàò„ÄÇËøô‰∫õ‰ªªÂä°ÈúÄË¶ÅÊ®°Âûã‰ªéÁâπÂÆöÁöÑ‰∏ä‰∏ãÊñá‰∏≠Â≠¶‰π†Êñ∞Áü•ËØÜÔºåÂπ∂ËøõË°åÊé®ÁêÜ‰ª•Ëß£ÂÜ≥ÈóÆÈ¢òÔºåËøôÁßçËÉΩÂäõË¢´Áß∞‰∏∫‰∏ä‰∏ãÊñáÂ≠¶‰π†„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜCL-benchÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´500‰∏™Â§çÊùÇ‰∏ä‰∏ãÊñáÂíå1899‰∏™‰ªªÂä°ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ËØÑ‰º∞Ê®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÂ≠¶‰π†ËÉΩÂäõ„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÁé∞ÊúâÊ®°ÂûãÂú®Ëøô‰∫õ‰ªªÂä°‰∏äÁöÑÂπ≥ÂùáËß£ÂÜ≥Áéá‰ªÖ‰∏∫17.2%ÔºåË°®Êòé‰∏ä‰∏ãÊñáÂ≠¶‰π†‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÁöÑÁì∂È¢à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03442', 'title': 'A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces', 'url': 'https://huggingface.co/papers/2602.03442', 'abstract': "Agentic RAG framework enables models to dynamically adapt retrieval decisions across multiple granularities, outperforming traditional approaches while scaling efficiently with model improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.", 'score': 17, 'issue_id': 913, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '317cbc0d4fd16984', 'authors': ['Mingxuan Du', 'Benfeng Xu', 'Chiwei Zhu', 'Shaohan Wang', 'Pengyu Wang', 'Xiaorui Wang', 'Zhendong Mao'], 'affiliations': ['Metastone Technology, Beijing, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.03442.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#rag', '#agents', '#reasoning'], 'emoji': 'üîç', 'ru': {'title': '–£–º–Ω—ã–π –∞–≥–µ–Ω—Ç —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø–æ–∏—Å–∫–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ A-RAG ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –æ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ª–∏–±–æ –∏–∑–≤–ª–µ–∫–∞—é—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–¥–Ω–æ—Ä–∞–∑–æ–≤–æ, –ª–∏–±–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º, A-RAG –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ç—Ä–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ —á—Ç–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –≠—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç—É –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –∏—Å–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å —É–ª—É—á—à–µ–Ω–∏–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ A-RAG –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø—Ä–∏ —Å—Ä–∞–≤–Ω–∏–º–æ–º –∏–ª–∏ –¥–∞–∂–µ –º–µ–Ω—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.'}, 'en': {'title': 'Empowering Models with Dynamic Retrieval Decisions', 'desc': 'The paper introduces the Agentic RAG (A-RAG) framework, which allows language models to make dynamic retrieval decisions at different levels of detail. Unlike traditional retrieval-augmented generation (RAG) systems that use fixed algorithms or predefined workflows, A-RAG empowers models to adaptively utilize retrieval tools such as keyword search, semantic search, and chunk reading. This flexibility enables the model to efficiently scale its performance as it improves, leading to better results in open-domain question answering tasks. Experiments show that A-RAG outperforms existing methods while using fewer tokens, highlighting its effectiveness in leveraging advanced model capabilities.'}, 'zh': {'title': 'Âä®ÊÄÅÈÄÇÂ∫îÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÊ°ÜÊû∂', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫A-RAGÁöÑ‰ª£ÁêÜÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÊ°ÜÊû∂ÔºåÊó®Âú®‰ΩøÊ®°ÂûãËÉΩÂ§üÂä®ÊÄÅÈÄÇÂ∫îÂ§öÂ±ÇÊ¨°ÁöÑÊ£ÄÁ¥¢ÂÜ≥Á≠ñ„ÄÇ‰º†ÁªüÁöÑRAGÁ≥ªÁªüÊó†Ê≥ïÂÖÖÂàÜÂà©Áî®ÂâçÊ≤øËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÂíåÂ∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÔºåÈÄöÂ∏∏‰æùËµñ‰∫éÂçï‰∏ÄÊ£ÄÁ¥¢ÊàñÈ¢ÑÂÆö‰πâÂ∑•‰ΩúÊµÅ„ÄÇA-RAGÈÄöËøáÊèê‰æõÂÖ≥ÈîÆÂ≠óÊêúÁ¥¢„ÄÅËØ≠‰πâÊêúÁ¥¢ÂíåÂùóËØªÂèñÁ≠â‰∏âÁßçÊ£ÄÁ¥¢Â∑•ÂÖ∑Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®Â§ö‰∏™Á≤íÂ∫¶‰∏äËá™ÈÄÇÂ∫îÂú∞ÊêúÁ¥¢ÂíåÊ£ÄÁ¥¢‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåA-RAGÂú®Â§ö‰∏™ÂºÄÊîæÈ¢ÜÂüüÈóÆÁ≠îÂü∫ÂáÜ‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂêåÊó∂Ê£ÄÁ¥¢ÁöÑ‰ª§ÁâåÊï∞ÈáèÁõ∏ÂΩìÊàñÊõ¥Â∞ëÔºåËØÅÊòé‰∫ÜÂÖ∂ÊúâÊïàÂà©Áî®Ê®°ÂûãËÉΩÂäõÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.18207', 'title': 'PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR', 'url': 'https://huggingface.co/papers/2601.18207', 'abstract': 'Search agents trained on scientific paper corpora demonstrate advanced reasoning capabilities for technical question-answering tasks, outperforming traditional retrieval methods through reinforcement learning with verifiable rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.', 'score': 15, 'issue_id': 913, 'pub_date': '2026-01-26', 'pub_date_card': {'ru': '26 —è–Ω–≤–∞—Ä—è', 'en': 'January 26', 'zh': '1Êúà26Êó•'}, 'hash': 'bd0f45c9106cc44d', 'authors': ['James Burgess', 'Jan N. Hansen', 'Duo Peng', 'Yuhui Zhang', 'Alejandro Lozano', 'Min Woo Sun', 'Emma Lundberg', 'Serena Yeung-Levy'], 'affiliations': ['Chan Zuckerberg Biohub Network', 'KTH Royal Institute of Technology', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2601.18207.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#rag', '#science', '#dataset', '#agents', '#reasoning', '#rl'], 'emoji': 'üî¨', 'ru': {'title': '–ê–≥–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∞—Ç—Å—è –∏—Å–∫–∞—Ç—å –∏—Å—Ç–∏–Ω—É –≤ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–µ—Ç–æ–¥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –ø—É—Ç—ë–º –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–∞—É—á–Ω—ã–º —Å—Ç–∞—Ç—å—è–º. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR) –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–∞. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –∫–æ—Ä–ø—É—Å –∏–∑ 16 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –±–∏–æ–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π –∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö PaperSearchQA —Å 60 —Ç—ã—Å—è—á–∞–º–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –û–±—É—á–µ–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –ø—Ä–µ–≤–∑–æ—à–ª–∏ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ —Ç–∞–∫–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏, –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞.'}, 'en': {'title': 'Empowering AI with Advanced Reasoning for Scientific Question-Answering', 'desc': 'This paper presents a novel approach to training search agents that utilize reinforcement learning with verifiable rewards (RLVR) to improve technical question-answering in scientific domains. By focusing on a corpus of 16 million biomedical paper abstracts, the authors create a dataset called PaperSearchQA, which includes 60,000 factoid questions relevant to the scientific literature. The trained agents demonstrate advanced reasoning capabilities, outperforming traditional retrieval methods and showcasing behaviors such as planning and self-verification. This work not only enhances the relevance of AI in scientific research but also provides scalable methods for future applications in various scientific fields.'}, 'zh': {'title': 'ÁßëÂ≠¶ËÆ∫ÊñáÈóÆÁ≠îÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊêúÁ¥¢‰ª£ÁêÜÔºå‰∏ìÈó®Áî®‰∫éÁßëÂ≠¶ËÆ∫ÊñáÁöÑÊäÄÊúØÈóÆÁ≠î‰ªªÂä°„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†‰∏éÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÊñπÊ≥ïÔºåËøô‰∫õ‰ª£ÁêÜÂú®ÂõûÁ≠îÊäÄÊúØÊÄßÈóÆÈ¢òÊó∂Ë°®Áé∞‰ºò‰∫é‰º†ÁªüÁöÑÊ£ÄÁ¥¢ÊñπÊ≥ï„ÄÇÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂåÖÂê´1600‰∏áÁØáÁîüÁâ©ÂåªÂ≠¶ËÆ∫ÊñáÊëòË¶ÅÁöÑÊêúÁ¥¢ËØ≠ÊñôÂ∫ìÔºåÂπ∂ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫PaperSearchQAÁöÑÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÂåÖÂê´6‰∏á‰∏™ÂèØÂõûÁ≠îÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËøô‰∫õÊêúÁ¥¢‰ª£ÁêÜËÉΩÂ§üËøõË°åËßÑÂàí„ÄÅÊé®ÁêÜÂíåËá™ÊàëÈ™åËØÅÔºåÂÖ∑ÊúâÈáçË¶ÅÁöÑÂ∫îÁî®ÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04816', 'title': 'Horizon-LM: A RAM-Centric Architecture for LLM Training', 'url': 'https://huggingface.co/papers/2602.04816', 'abstract': 'Horizon-LM enables large-model training on single GPUs by redefining CPU-GPU roles and eliminating persistent GPU memory usage through explicit recomputation and pipelined execution.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5\\,TB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2times higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.', 'score': 13, 'issue_id': 914, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '5562f2949c282a62', 'authors': ['Zhengqing Yuan', 'Lichao Sun', 'Yanfang', 'Ye'], 'affiliations': ['Lehigh University', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2602.04816.jpg', 'data': {'categories': [], 'emoji': 'üéØ', 'ru': {'title': '–ü–µ—Ä–µ–≤–æ—Ä–æ—Ç —Ä–æ–ª–µ–π: –æ–±—É—á–µ–Ω–∏–µ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º GPU —á–µ—Ä–µ–∑ —Ö–æ—Å—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É', 'desc': 'Horizon-LM ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º GPU, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–æ–ª–∏ CPU –∏ GPU, —Å–¥–µ–ª–∞–≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é —Ö–æ—Å—Ç-–º–∞—à–∏–Ω—É, –∞ GPU ‚Äî –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —è–≤–Ω–æ–µ –ø–µ—Ä–µ–æ—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∏ –∫–æ–Ω–≤–µ–π–µ—Ä–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –≥—Ä–∞—Ñ–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ GPU. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç–∞–∫–æ–º—É –ø–æ–¥—Ö–æ–¥—É —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ 120 –º–ª—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º H200 GPU —Å 1.5 TB –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏. Horizon-LM –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –æ–±—ä—ë–º —Ö–æ—Å—Ç-–ø–∞–º—è—Ç–∏, –∞ –Ω–µ –ø–∞–º—è—Ç–∏ GPU, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º —É–∑–ª–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': 'Revolutionizing Large-Model Training with Horizon-LM', 'desc': 'Horizon-LM is a novel training system designed to optimize large language models (LLMs) on single GPUs by changing how CPUs and GPUs interact. It shifts the primary role of memory management from GPUs to CPUs, allowing GPUs to function only as temporary computation units. This approach eliminates the need for persistent GPU memory, enabling the training of models with up to 120 billion parameters on a single GPU. By using techniques like explicit recomputation and pipelined execution, Horizon-LM significantly increases training efficiency and reduces memory constraints, making large-model training more accessible.'}, 'zh': {'title': 'Horizon-LMÔºöÂçï GPU ‰∏äÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÊñ∞Á™ÅÁ†¥', 'desc': 'Horizon-LM ÊòØ‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÁ≥ªÁªüÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßÊ®°ÂûãËÆ≠ÁªÉ‰∏≠ GPU ÂÜÖÂ≠ò‰∏çË∂≥ÁöÑÈóÆÈ¢ò„ÄÇÂÆÉÈÄöËøáÈáçÊñ∞ÂÆö‰πâ CPU Âíå GPU ÁöÑËßíËâ≤ÔºåÂ∞Ü‰∏ªÂ≠òÂÇ®Âô®ËßÜ‰∏∫‰∏ªË¶ÅÂèÇÊï∞Â≠òÂÇ®ÔºåÂπ∂Â∞Ü GPU ‰ªÖÁî®‰Ωú‰∏¥Êó∂ËÆ°ÁÆóÂºïÊìé„ÄÇËØ•Á≥ªÁªüÈááÁî®ÊòæÂºèÈáçËÆ°ÁÆóÂíåÊµÅÊ∞¥Á∫øÊâßË°åÔºåÊ∂àÈô§‰∫Ü GPU ‰∏äÁöÑÊåÅ‰πÖÊ®°ÂùóÔºå‰ªéËÄå‰ΩøÊ®°ÂûãËßÑÊ®°‰∏é GPU Êï∞ÈáèËß£ËÄ¶„ÄÇHorizon-LM Âú®Âçï‰∏™ GPU ‰∏äËÉΩÂ§üÈ´òÊïàËÆ≠ÁªÉÈ´òËææ 120B ÂèÇÊï∞ÁöÑÊ®°ÂûãÔºåÊòæËëóÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÂêûÂêêÈáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04735', 'title': 'From Data to Behavior: Predicting Unintended Model Behaviors Before Training', 'url': 'https://huggingface.co/papers/2602.04735', 'abstract': 'Data2Behavior predicts unintended model behaviors before training using MDF, a lightweight method that analyzes data features to reveal potential biases without parameter updates.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) can acquire unintended biases from seemingly benign training data even without explicit cues or malicious content. Existing methods struggle to detect such risks before fine-tuning, making post hoc evaluation costly and inefficient. To address this challenge, we introduce Data2Behavior, a new task for predicting unintended model behaviors prior to training. We also propose Manipulating Data Features (MDF), a lightweight approach that summarizes candidate data through their mean representations and injects them into the forward pass of a base model, allowing latent statistical signals in the data to shape model activations and reveal potential biases and safety risks without updating any parameters. MDF achieves reliable prediction while consuming only about 20% of the GPU resources required for fine-tuning. Experiments on Qwen3-14B, Qwen2.5-32B-Instruct, and Gemma-3-12b-it confirm that MDF can anticipate unintended behaviors and provide insight into pre-training vulnerabilities.', 'score': 13, 'issue_id': 914, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '71bcc623c3779bfc', 'authors': ['Mengru Wang', 'Zhenqian Xu', 'Junfeng Fang', 'Yunzhi Yao', 'Shumin Deng', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.04735.jpg', 'data': {'categories': [], 'emoji': 'üîç', 'ru': {'title': '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–∞ –¥–æ –æ–±—É—á–µ–Ω–∏—è: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–π –æ—Ü–µ–Ω–∫–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Data2Behavior ‚Äî –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–æ —ç—Ç–∞–ø–∞ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Manipulating Data Features (MDF), –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –∏—Ö —Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –≤–Ω–µ–¥—Ä—è–µ—Ç –∏—Ö –≤ –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. MDF –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å–º–µ—â–µ–Ω–∏—è –∏ —Ä–∏—Å–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏ —ç—Ç–æ–º –≤—Å–µ–≥–æ 20% GPU-—Ä–µ—Å—É—Ä—Å–æ–≤, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è —Ñ–∞–π–Ω-—Ç—é–Ω–∏–Ω–≥–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã—Ö –ø–æ–≤–µ–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Predicting Biases Before Training with Data2Behavior', 'desc': 'Data2Behavior is a novel approach that predicts unintended behaviors in machine learning models before they are trained. It utilizes a method called Manipulating Data Features (MDF), which analyzes the features of training data to identify potential biases without needing to adjust model parameters. This proactive analysis helps in revealing latent statistical signals that could lead to safety risks. By using MDF, researchers can efficiently assess model vulnerabilities while significantly reducing computational costs compared to traditional fine-tuning methods.'}, 'zh': {'title': 'Âú®ËÆ≠ÁªÉÂâçÈ¢ÑÊµãÊ®°ÂûãÂÅèËßÅÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Data2Behavior ÊòØ‰∏ÄÁßçÂú®ËÆ≠ÁªÉ‰πãÂâçÈ¢ÑÊµãÊ®°ÂûãÊÑèÂ§ñË°å‰∏∫ÁöÑÊñπÊ≥ï„ÄÇÂÆÉ‰ΩøÁî®‰∫Ü‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫ Manipulating Data Features (MDF)ÔºåÈÄöËøáÂàÜÊûêÊï∞ÊçÆÁâπÂæÅÊù•Êè≠Á§∫ÊΩúÂú®ÁöÑÂÅèËßÅÔºåËÄåÊó†ÈúÄÊõ¥Êñ∞Ê®°ÂûãÂèÇÊï∞„ÄÇMDF ÈÄöËøáÂ∞ÜÂÄôÈÄâÊï∞ÊçÆÁöÑÂùáÂÄºË°®Á§∫Ê≥®ÂÖ•Âà∞Âü∫Á°ÄÊ®°ÂûãÁöÑÂâçÂêë‰º†Êí≠‰∏≠ÔºåËÉΩÂ§üÊúâÊïàÂú∞È¢ÑÊµãÊ®°ÂûãÁöÑÊΩúÂú®È£éÈô©„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMDF Âú®Ê∂àËÄóÁ∫¶ 20% ÁöÑ GPU ËµÑÊ∫êÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂèØÈù†Âú∞È¢ÑÊµãÊÑèÂ§ñË°å‰∏∫Âπ∂Êèê‰æõÈ¢ÑËÆ≠ÁªÉÁöÑËÑÜÂº±ÊÄßÊ¥ûÂØü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22859', 'title': 'MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering', 'url': 'https://huggingface.co/papers/2601.22859', 'abstract': 'MEnvAgent is a multi-language framework that automates environment construction for software engineering tasks using a planning-execution-verification architecture and environment reuse mechanism, achieving improved performance on a new benchmark and creating the largest open-source polyglot dataset of verifiable Docker environments.  \t\t\t\t\tAI-generated summary \t\t\t\t The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.', 'score': 13, 'issue_id': 915, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '2d065538f3f284b2', 'authors': ['Chuanzhe Guo', 'Jingjing Wu', 'Sijun He', 'Yang Chen', 'Zhaoqi Kuang', 'Shilong Fan', 'Bingjin Chen', 'Siqi Bao', 'Jing Liu', 'Hua Wu', 'Qingfu Zhu', 'Wanxiang Che', 'Haifeng Wang'], 'affiliations': ['Baidu Inc., Shenzhen, China', 'Research Center for Social Computing and Interactive Robotics, Harbin Institute of Technology, Harbin, China'], 'pdf_title_img': 'assets/pdf/title_img/2601.22859.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#open_source', '#benchmark', '#plp', '#agents', '#low_resource'], 'emoji': 'üê≥', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∏—Å–ø–æ–ª–Ω—è–µ–º—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–ª—è –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è', 'desc': 'MEnvAgent ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–ª—è –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è-–∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è-–ø—Ä–æ–≤–µ—Ä–∫–∏. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—É—Ç—ë–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ä–µ–¥. –ù–∞ –Ω–æ–≤–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ MEnvBench, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º 1000 –∑–∞–¥–∞—á –Ω–∞ 10 —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, MEnvAgent –ø–æ–∫–∞–∑–∞–ª —É–ª—É—á—à–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ Fail-to-Pass –Ω–∞ 8,6% –ø—Ä–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ 43%. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ —Ç–∞–∫–∂–µ MEnvData-SWE ‚Äî –∫—Ä—É–ø–Ω–µ–π—à–∏–π –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö Docker –æ–∫—Ä—É–∂–µ–Ω–∏–π —Å —Ä–µ—à–µ–Ω–∏—è–º–∏ –¥–ª—è –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ü–û.'}, 'en': {'title': 'Automating Multi-Language Environment Construction for Software Engineering', 'desc': 'MEnvAgent is a framework designed to automate the creation of software engineering environments across multiple programming languages. It uses a Planning-Execution-Verification architecture to efficiently handle the construction of these environments, addressing common failures autonomously. The framework also incorporates an Environment Reuse Mechanism, which minimizes computational costs by reusing previously built environments. Evaluations show that MEnvAgent significantly improves performance on a new benchmark, while also providing the largest open-source dataset of verifiable Docker environments for software engineering tasks.'}, 'zh': {'title': 'MEnvAgentÔºöËá™Âä®ÂåñËΩØ‰ª∂Â∑•Á®ãÁéØÂ¢ÉÊûÑÂª∫ÁöÑÂ§öËØ≠Ë®ÄÊ°ÜÊû∂', 'desc': 'MEnvAgentÊòØ‰∏Ä‰∏™Â§öËØ≠Ë®ÄÊ°ÜÊû∂ÔºåÊó®Âú®Ëá™Âä®ÂåñËΩØ‰ª∂Â∑•Á®ã‰ªªÂä°ÁöÑÁéØÂ¢ÉÊûÑÂª∫„ÄÇÂÆÉÈááÁî®ËßÑÂàí-ÊâßË°å-È™åËØÅÊû∂ÊûÑÂíåÁéØÂ¢ÉÈáçÁî®Êú∫Âà∂ÔºåËÉΩÂ§üÊúâÊïàËß£ÂÜ≥ÊûÑÂª∫Â§±Ë¥•ÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂú®MEnvBenchÂü∫ÂáÜ‰∏äËøõË°åËØÑ‰º∞ÔºåMEnvAgentÂú®Â§ö‰∏™ËØ≠Ë®Ä‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåFail-to-PassÁéáÊèêÈ´ò‰∫Ü8.6%ÔºåÊó∂Èó¥ÊàêÊú¨Èôç‰Ωé‰∫Ü43%„ÄÇÊ≠§Â§ñÔºåMEnvAgentËøòÂàõÂª∫‰∫ÜMEnvData-SWEÔºåËøôÊòØËøÑ‰ªä‰∏∫Ê≠¢ÊúÄÂ§ßÁöÑÂºÄÊ∫êÂèØÈ™åËØÅDockerÁéØÂ¢ÉÊï∞ÊçÆÈõÜÔºå‰øÉËøõ‰∫ÜËΩØ‰ª∂Â∑•Á®ã‰ªªÂä°ÁöÑÊåÅÁª≠ÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04284', 'title': 'Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.04284', 'abstract': "Agent-Omit is a training framework that enables LLM agents to adaptively omit redundant thoughts and observations during multi-turn interactions, achieving superior effectiveness-efficiency trade-offs compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.", 'score': 12, 'issue_id': 913, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'cd96815789a76d79', 'authors': ['Yansong Ning', 'Jun Fang', 'Naiqiang Tan', 'Hao Liu'], 'affiliations': ['AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)', 'CSE, The Hong Kong University of Science and Technology', 'Didichuxing Co. Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2602.04284.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agents', '#training', '#rl'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': '–£–º–Ω–æ–µ –ø—Ä–æ–ø—É—Å–∫–∞–Ω–∏–µ: –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –æ–ø—É—Å–∫–∞—Ç—å –ª–∏—à–Ω–∏–µ –º—ã—Å–ª–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Agent-Omit, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –ª–∏—à–Ω–∏–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –º—ã—Å–ª–µ–π –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞, –≤—ã—è–≤–ª—è—è, —á—Ç–æ –∏—Ö –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —ç—Ç–∞–ø–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ–∑ —Ö–æ–ª–æ–¥–Ω–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–º–∏—Å—Å–∏–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –¥–≤–æ–π–Ω—ã–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞ –∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–º—É –ø—Ä–æ–ø—É—Å–∫—É –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ Agent-Omit-8B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ LLM-–∞–≥–µ–Ω—Ç–∞–º–∏ –ø—Ä–∏ –ª—É—á—à–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Optimize Agent Interactions with Adaptive Omission', 'desc': 'Agent-Omit is a novel training framework designed for large language model (LLM) agents to improve their efficiency during multi-turn interactions by selectively omitting unnecessary thoughts and observations. The framework addresses the issue that previous methods treated all interaction data equally, failing to recognize that the importance of thoughts and observations can vary from one turn to another. By conducting quantitative analyses, the authors demonstrate how these factors influence both the effectiveness and efficiency of agents. The proposed method includes a unique reinforcement learning approach that encourages agents to adaptively omit redundant information, leading to better performance compared to existing LLM agent strategies.'}, 'zh': {'title': 'Agent-OmitÔºöÊèêÂçáÂ§öËΩÆ‰∫§‰∫íÊïàÁéáÁöÑÊô∫ËÉΩÁúÅÁï•Ê°ÜÊû∂', 'desc': 'Agent-OmitÊòØ‰∏ÄÁßçËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊó®Âú®‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂú®Â§öËΩÆ‰∫§‰∫í‰∏≠Ëá™ÈÄÇÂ∫îÂú∞ÁúÅÁï•ÂÜó‰ΩôÁöÑÊÄùÁª¥ÂíåËßÇÂØüÔºå‰ªéËÄåÂú®ÊïàÊûúÂíåÊïàÁéá‰πãÈó¥ÂÆûÁé∞Êõ¥Â•ΩÁöÑÂπ≥Ë°°„ÄÇÁé∞ÊúâÁ†îÁ©∂Êú™ËÉΩËÄÉËôë‰∏çÂêåËΩÆÊ¨°‰∏≠ÊÄùÁª¥ÁöÑÂøÖË¶ÅÊÄßÂíåËßÇÂØüÁöÑÂÆûÁî®ÊÄßÔºåÂõ†Ê≠§Agent-OmitÈÄöËøáÂÆöÈáèÁ†îÁ©∂Ëøô‰∫õÂõ†Á¥†ÂØπ‰ª£ÁêÜÊïàÊûúÂíåÊïàÁéáÁöÑÂΩ±ÂìçÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑËÆ≠ÁªÉÊ°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂêàÊàêÂ∞ëÈáèÂÜ∑ÂêØÂä®Êï∞ÊçÆÊù•ÂæÆË∞É‰ª£ÁêÜÁöÑÁúÅÁï•Ë°å‰∏∫ÔºåÂπ∂ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÁúÅÁï•ÊÑüÁü•ÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºå‰ª•ÊøÄÂä±‰ª£ÁêÜÁöÑËá™ÈÄÇÂ∫îÁúÅÁï•ËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåAgent-OmitÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåËææÂà∞‰∫Ü‰∏éÂâçÊ≤øLLM‰ª£ÁêÜÁõ∏ÂΩìÁöÑÊÄßËÉΩÔºåÂπ∂Âú®ÊïàÁéáÊñπÈù¢‰ºò‰∫éÂÖ∂‰ªñÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02160', 'title': 'D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use', 'url': 'https://huggingface.co/papers/2602.02160', 'abstract': '', 'score': 11, 'issue_id': 921, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'a9b20d119694b3bd', 'authors': ['Bowen Xu', 'Shaoyu Wu', 'Hao Jiang', 'Kai Liu', 'Xin Chen', 'Lulu Hu', 'Bin Yang'], 'affiliations': ['Alibaba Cloud Computing, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2602.02160.jpg', 'data': {'categories': [], 'emoji': 'üß†', 'ru': {'title': '–ù–æ–≤–∞—è —ç—Ä–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å LLM', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LLM, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á—ë—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏—Ö —Å–ª–æ—ë–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–∞–ª–æ–≥–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Hybrid Models: Bridging Spatial and Temporal Learning', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': '‰ºòÂåñÁâπÂæÅÈÄâÊã©ÔºåÊèêÂçáÊ®°ÂûãÊïàÁéáÔºÅ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàõÊñ∞ÁöÑÊñπÊ≥ïÔºåÈÄöËøá‰ºòÂåñÁâπÂæÅÈÄâÊã©Êù•ÂáèÂ∞ëËÆ°ÁÆóÂ§çÊùÇÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÁÆóÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊäÄÊúØ„ÄÇÊúÄÁªàÔºåËøôÈ°πÁ†îÁ©∂‰∏∫Êú∫Âô®Â≠¶‰π†È¢ÜÂüüÊèê‰æõ‰∫ÜÊñ∞ÁöÑÊÄùË∑ØÂíåÂ∑•ÂÖ∑„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03916', 'title': 'SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?', 'url': 'https://huggingface.co/papers/2602.03916', 'abstract': "SpatiaLab presents a comprehensive benchmark for evaluating vision-language models' spatial reasoning capabilities across realistic, diverse scenarios, revealing significant gaps compared to human performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.", 'score': 8, 'issue_id': 918, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '1db96f60865a4e5a', 'authors': ['Azmine Toushik Wasi', 'Wahid Faisal', 'Abdur Rahman', 'Mahfuz Ahmed Anik', 'Munem Shahriar', 'Mohsin Mahmud Topu', 'Sadia Tasnim Meem', 'Rahatun Nesa Priti', 'Sabrina Afroz Mitu', 'Md. Iqramul Hoque', 'Shahriyar Zaman Ridoy', 'Mohammed Eunus Ali', 'Majd Hawasly', 'Mohammad Raza', 'Md Rizwan Parvez'], 'affiliations': ['BRAC University', 'Computational Intelligence and Operations Laboratory (CIOL)', 'Monash University', 'North South University (NSU)', 'Qatar Computing Research Institute (QCRI)', 'Shahjalal University of Science and Technology (SUST)'], 'pdf_title_img': 'assets/pdf/title_img/2602.03916.jpg', 'data': {'categories': ['#cv', '#multimodal', '#dataset', '#reasoning', '#benchmark', '#survey'], 'emoji': 'üß≠', 'ru': {'title': '–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ: —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ –ø—Ä–æ–ø–∞—Å—Ç–∏ –º–µ–∂–¥—É –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏ –∏ —á–µ–ª–æ–≤–µ–∫–æ–º', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç SpatiaLab ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 1400 –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —à–µ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è: –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –≥–ª—É–±–∏–Ω–∞ –∏ –æ–∫–∫–ª—é–∑–∏—è, –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è, –º–∞—Å—à—Ç–∞–± –∏ —Ä–∞–∑–º–µ—Ä, –Ω–∞–≤–∏–≥–∞—Ü–∏—è –∏ —Ç—Ä—ë—Ö–º–µ—Ä–Ω–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤—ã—è–≤–∏–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –∏ —á–µ–ª–æ–≤–µ–∫–∞ ‚Äî –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Ç–æ–ª—å–∫–æ 54.93% —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ç–∏–≤ 87.57% —É –ª—é–¥–µ–π –≤ —Ä–µ–∂–∏–º–µ –≤—ã–±–æ—Ä–∞ –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö vision-language –º–æ–¥–µ–ª–µ–π –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.'}, 'en': {'title': 'Bridging the Gap in Spatial Reasoning for Vision-Language Models', 'desc': 'SpatiaLab is a new benchmark designed to test how well vision-language models (VLMs) can understand spatial reasoning in real-world scenarios. Unlike previous studies that used simple or artificial environments, SpatiaLab includes 1,400 visual question-answer pairs across various categories like positioning, depth, and navigation. The results show that current VLMs significantly underperform compared to humans, with accuracy rates revealing a gap in understanding complex spatial relationships. This benchmark aims to highlight the limitations of VLMs and guide future research to improve their spatial reasoning abilities.'}, 'zh': {'title': 'SpatiaLabÔºöÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁ©∫Èó¥Êé®ÁêÜËÉΩÂäõÁöÑÂü∫ÂáÜ', 'desc': 'SpatiaLabÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Á©∫Èó¥Êé®ÁêÜËÉΩÂäõÊñπÈù¢ÁöÑË°®Áé∞„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´1400‰∏™ËßÜËßâÈóÆÁ≠îÂØπÔºåÊ∂µÁõñÂÖ≠‰∏™‰∏ªË¶ÅÁ±ªÂà´ÔºåÊó®Âú®ÂèçÊò†Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÂ§çÊùÇÊÄßÂíåÂ§öÊ†∑ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Á©∫Èó¥Êé®ÁêÜËÉΩÂäõ‰∏ä‰∏é‰∫∫Á±ªÂ≠òÂú®ÊòæËëóÂ∑ÆË∑ùÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜÂ§çÊùÇÁ©∫Èó¥ÂÖ≥Á≥ªÂíåÊ∑±Â∫¶ÊÑüÁü•ÊñπÈù¢„ÄÇSpatiaLab‰∏∫Êú™Êù•ÁöÑÁ†îÁ©∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÈáçË¶ÅÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÂ∏ÆÂä©Êé®Âä®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁ©∫Èó¥ÁêÜËß£ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02140', 'title': 'Quantifying the Gap between Understanding and Generation within Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2602.02140', 'abstract': 'Unified multimodal models exhibit a persistent gap between understanding and generation capabilities, indicating only surface-level integration rather than deep cognitive convergence.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two "unified" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model\'s bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.', 'score': 8, 'issue_id': 914, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '3baaec9a0a6e1234', 'authors': ['Chenlong Wang', 'Yuhang Chen', 'Zhihan Hu', 'Dongping Chen', 'Wenhu Chen', 'Sarah Wiegreffe', 'Tianyi Zhou'], 'affiliations': ['MBZUAI', 'University of Maryland', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2602.02140.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark'], 'emoji': 'üîÄ', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –Ω–æ –Ω–µ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –≥–ª—É–±–æ–∫–æ–π –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –º–µ–∂–¥—É —ç—Ç–∏–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –±–µ–Ω—á–º–∞—Ä–∫ GapEval –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑—Ä—ã–≤–∞ –º–µ–∂–¥—É –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Ä–µ—à–∞—Ç—å –≤ –æ–±–µ–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ç–µ–∫—Å—Ç). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —É—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ –≤—Å–µ—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–µ–π. –ê–Ω–∞–ª–∏–∑ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∑–Ω–∞–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –æ—Å—Ç–∞—é—Ç—Å—è —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–º–∏, –∞ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∏ –∑–Ω–∞–Ω–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ.'}, 'en': {'title': 'Bridging the Gap: Understanding vs. Generating in Unified Multimodal Models', 'desc': 'This paper discusses the limitations of unified multimodal models (UMMs) in integrating understanding and generation tasks. It introduces GapEval, a benchmark that measures the coherence between these two capabilities in a model. The study finds that current UMMs only achieve superficial integration, with a significant gap between how they understand and generate information. Additionally, the research highlights that knowledge across different modalities often remains disconnected, indicating a need for deeper cognitive convergence in future models.'}, 'zh': {'title': 'ÁêÜËß£‰∏éÁîüÊàêËÉΩÂäõÁöÑÁªü‰∏Ä‰πãË∑Ø‰ªçÈúÄÊé¢Á¥¢', 'desc': 'Áªü‰∏ÄÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàUMMÔºâÂú®ÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõ‰πãÈó¥Â≠òÂú®ÊòéÊòæÂ∑ÆË∑ùÔºåË°®ÊòéÂÆÉ‰ª¨ÁöÑÊï¥Âêà‰ªÖÂÅúÁïôÂú®Ë°®Èù¢ÔºåËÄåÈùûÊ∑±Â±ÇÊ¨°ÁöÑËÆ§Áü•ËûçÂêà„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜGapEvalÔºåËøôÊòØ‰∏Ä‰∏™ÂèåÂêëÂü∫ÂáÜÊµãËØïÔºåÊó®Âú®ÈáèÂåñÁêÜËß£ÂíåÁîüÊàêËÉΩÂäõ‰πãÈó¥ÁöÑÂ∑ÆË∑ùÔºåÂπ∂ÊµãÈáèËøô‰∏§Áßç‚ÄúÁªü‰∏Ä‚ÄùÊñπÂêëÁöÑËÆ§Áü•‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰∏çÂêåÊû∂ÊûÑÁöÑUMMÂú®Ëøô‰∏§‰∏™ÊñπÂêë‰∏äÂßãÁªàÂ≠òÂú®Â∑ÆË∑ùÔºåË°®ÊòéÂΩìÂâçÊ®°Âûã‰ªÖÂÆûÁé∞‰∫ÜË°®Â±ÇÁªü‰∏ÄÔºåËÄåÊú™ËÉΩËææÂà∞Ê∑±Â±ÇÊ¨°ÁöÑËÆ§Áü•ËûçÂêà„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂ËøòË°®ÊòéÔºåUMM‰∏≠ÁöÑÁü•ËØÜÂæÄÂæÄÊòØÂàÜÁ¶ªÁöÑÔºåË∑®Ê®°ÊÄÅÁöÑËÉΩÂäõÂíåÁü•ËØÜÂπ∂‰∏çÂêåÊ≠•ÔºåËøô‰∏∫Ëøõ‰∏ÄÊ≠•Êé¢Á¥¢Êèê‰æõ‰∫ÜÊñπÂêë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02554', 'title': 'BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation', 'url': 'https://huggingface.co/papers/2602.02554', 'abstract': "BatCoder is a self-supervised reinforcement learning framework that jointly optimizes code and documentation generation through back-translation, achieving superior performance on code-related benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.", 'score': 8, 'issue_id': 913, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'b594dc20d8c25615', 'authors': ['Jingwen Xu', 'Yiyang Lu', 'Zisu Huang', 'Changze Lv', 'Xiaohua Wang', 'Shizheng Li', 'Zhibo Xu', 'Zhengkang Guo', 'Zhengyuan Wang', 'Muzhao Tian', 'Xuanjing Huang', 'Xiaoqing Zheng'], 'affiliations': ['Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02554.jpg', 'data': {'categories': ['#open_source', '#optimization', '#low_resource', '#plp', '#training', '#rl'], 'emoji': 'üîÑ', 'ru': {'title': '–î–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –ø–µ—Ä–µ–≤–æ–¥ –∫–æ–¥–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏', 'desc': 'BatCoder –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º reinforcement learning, –∫–æ—Ç–æ—Ä—ã–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ back-translation. –ü–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –∏—Å—Ö–æ–¥–Ω—ã–º –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º –∫–æ–¥–æ–º –≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–µ—è–≤–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å LLM —Ç–æ–ª—å–∫–æ –Ω–∞ –∫–æ–¥–µ, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö HumanEval –∏ MBPP –º–æ–¥–µ–ª—å —Å 7B –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–æ—Å—Ç–∏–≥–ª–∞ 83.5% –∏ 81.0% —Ç–æ—á–Ω–æ—Å—Ç–∏ pass@1, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ baseline —Ä–µ—à–µ–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing Code and Documentation Generation with BatCoder', 'desc': "BatCoder is a novel self-supervised reinforcement learning framework that enhances the generation of code and its corresponding documentation. It utilizes a back-translation method where documentation is created from code, and then this documentation is used to regenerate the original code, allowing the model to learn from the semantic similarities. This process provides an implicit reward signal for reinforcement learning, improving the model's ability to generate accurate code and documentation. By leveraging only code for training, BatCoder significantly increases the amount of available training data, leading to impressive performance on code-related benchmarks."}, 'zh': {'title': 'BatCoderÔºö‰ºòÂåñ‰ª£Á†Å‰∏éÊñáÊ°£ÁîüÊàêÁöÑËá™ÁõëÁù£Â≠¶‰π†Ê°ÜÊû∂', 'desc': 'BatCoderÊòØ‰∏ÄÁßçËá™ÁõëÁù£Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂèçÂêëÁøªËØëÂÖ±Âêå‰ºòÂåñ‰ª£Á†ÅÁîüÊàêÂíåÊñáÊ°£ÁîüÊàê„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÂÖà‰ªé‰ª£Á†ÅÁîüÊàêÊñáÊ°£ÔºåÁÑ∂ÂêéÂà©Áî®ÁîüÊàêÁöÑÊñáÊ°£ÈáçÊûÑÂéüÂßã‰ª£Á†ÅÔºå‰∫åËÄÖ‰πãÈó¥ÁöÑËØ≠‰πâÁõ∏‰ººÊÄß‰Ωú‰∏∫ÈöêÂºèÂ•ñÂä±Ôºå‰øÉËøõÊ®°ÂûãÊÄßËÉΩÁöÑÊèêÂçá„ÄÇÈÄöËøá‰ªÖ‰ΩøÁî®‰ª£Á†ÅËøõË°åËÆ≠ÁªÉÔºåBatCoderÊòæËëóÂ¢ûÂä†‰∫ÜÂèØÁî®ÁöÑËÆ≠ÁªÉÊ†∑Êú¨ÔºåËß£ÂÜ≥‰∫ÜÈ´òË¥®Èáè‰ª£Á†Å-ÊñáÊ°£ÂØπÁ®ÄÁº∫ÁöÑÈóÆÈ¢ò„ÄÇÂú®HumanEvalÂíåMBPPÂü∫ÂáÜÊµãËØï‰∏≠ÔºåBatCoderÁöÑË°®Áé∞‰ºò‰∫éËÆ∏Â§öÂº∫Â§ßÁöÑÂºÄÊ∫êÂü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03979', 'title': 'Likelihood-Based Reward Designs for General LLM Reasoning', 'url': 'https://huggingface.co/papers/2602.03979', 'abstract': "Log-probability rewards derived from the reference answer's likelihood outperform binary rewards in chain-of-thought fine-tuning across both verifiable and non-verifiable reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function, often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards. Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining. In verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and yield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short, verifiable and long, non-verifiable answer settings.", 'score': 7, 'issue_id': 914, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'a6bd9b45a789ae4d', 'authors': ['Ariel Kwiatkowski', 'Natasha Butt', 'Ismail Labiad', 'Julia Kempe', 'Yann Ollivier'], 'affiliations': ['Meta FAIR', 'New York University', 'University of Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2602.03979.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#training', '#benchmark', '#rl'], 'emoji': 'üéØ', 'ru': {'title': '–õ–æ–≥–∞—Ä–∏—Ñ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é', 'desc': '–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –º–µ—Ç–æ–¥–æ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–∞—Ä–∏—Ñ–º–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –ª–æ–≥–∞—Ä–∏—Ñ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –Ω–∞ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫ –∏ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö –±–µ–∑ –≤–Ω–µ—à–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–º —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ –≤—Å–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º.'}, 'en': {'title': 'Log-Probability Rewards: A Game Changer for Fine-Tuning Reasoning in LLMs', 'desc': "This paper explores the effectiveness of using log-probability rewards for fine-tuning large language models (LLMs) in reasoning tasks. Unlike traditional binary rewards, log-probability rewards are derived from the likelihood of generating the correct answer, which allows for more nuanced feedback and is scalable. The authors demonstrate that log-probability rewards outperform binary rewards in both verifiable and non-verifiable reasoning benchmarks, leading to better performance in chain-of-thought (CoT) learning. This approach aligns with the next-token log-likelihood loss used during pretraining, making it a promising method for enhancing LLMs' reasoning capabilities."}, 'zh': {'title': 'ÂØπÊï∞Ê¶ÇÁéáÂ•ñÂä±ÔºöÈìæÂºèÊÄùÁª¥ÂæÆË∞ÉÁöÑÊñ∞ÈÄâÊã©', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®Êé®ÁêÜÂü∫ÂáÜ‰∏äÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞ÉÊó∂Ôºå‰ΩøÁî®Âü∫‰∫éÂØπÊï∞Ê¶ÇÁéáÁöÑÂ•ñÂä±ÂáΩÊï∞ÁöÑ‰ºòÂäø„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∫åÂÖÉÂ•ñÂä±Áõ∏ÊØîÔºåËøôÁßçÊñπÊ≥ï‰∏ç‰æùËµñ‰∫éÁâπÂÆöÁöÑÈ™åËØÅÂô®ÔºåÂπ∂‰∏îÂèØ‰ª•Â§ßËßÑÊ®°Â∫îÁî®„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®ÂèÇËÄÉÁ≠îÊ°àÁöÑÂØπÊï∞Ê¶ÇÁéá‰Ωú‰∏∫ÈìæÂºèÊÄùÁª¥Â≠¶‰π†ÁöÑÂ•ñÂä±ÔºåÂú®ÂêÑÁßçËÆæÁΩÆ‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂèØÈ™åËØÅÂíå‰∏çÂèØÈ™åËØÅÁöÑÊé®ÁêÜÂü∫ÂáÜ‰∏ä„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåËøô‰∏∫ÈìæÂºèÊÄùÁª¥ÂæÆË∞ÉÊèê‰æõ‰∫Ü‰∏ÄÁßçÊúâÊïàÁöÑÂ•ñÂä±ÊñπÊ≥ïÔºåËÉΩÂ§üÊúâÊïàËøûÊé•Áü≠ÊúüÂèØÈ™åËØÅÂíåÈïøÊúü‰∏çÂèØÈ™åËØÅÁöÑÁ≠îÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01640', 'title': 'A2Eval: Agentic and Automated Evaluation for Embodied Brain', 'url': 'https://huggingface.co/papers/2602.01640', 'abstract': "Agentic automatic evaluation framework automates embodied vision-language model assessment through collaborative agents that reduce evaluation costs and improve ranking accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.", 'score': 7, 'issue_id': 913, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '968f7428d684ed07', 'authors': ['Shuai Zhang', 'Jiayu Hu', 'Zijie Chen', 'Zeyuan Ding', 'Yi Zhang', 'Yingji Zhang', 'Ziyi Zhou', 'Junwei Liao', 'Shengjie Zhou', 'Yong Dai', 'Zhenzhong Lan', 'Xiaozhu Ju'], 'affiliations': ['Beijing Innovatio', 'Westlake University, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.01640.jpg', 'data': {'categories': ['#agents', '#benchmark', '#cv', '#multimodal'], 'emoji': 'ü§ñ', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∏–¥–µ–Ω–∏–µ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Agentic Automatic Evaluation (A2Eval) ‚Äî –ø–µ—Ä–≤–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–≤—É—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤. Data Agent —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤, –∞ Eval Agent —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é. –ú–µ—Ç–æ–¥ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–º —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –Ω–∞ 85%, —É–º–µ–Ω—å—à–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ 77% –∏ —É—Å–∫–æ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É –≤ 4.6 —Ä–∞–∑–∞, –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —É–ª—É—á—à–∞—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ü–µ–Ω–æ–∫ —Å—É–∂–¥–µ–Ω–∏—è–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–º–µ—â–µ–Ω–∏—è –≤ —Ä–∞–Ω–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∏ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞.'}, 'en': {'title': 'Revolutionizing VLM Assessment with Automated Evaluation', 'desc': 'The paper introduces the Agentic Automatic Evaluation (A2Eval) framework, which automates the assessment of embodied vision-language models (VLMs) using collaborative agents. It addresses the limitations of traditional evaluation methods that rely on static benchmarks, which are often redundant and imbalanced, leading to high costs and skewed model rankings. A2Eval features a Data Agent that creates a balanced evaluation suite and an Eval Agent that validates evaluation processes, significantly reducing evaluation suite size and computational costs. The framework demonstrates improved ranking accuracy and efficiency, setting a new standard for evaluating embodied VLMs.'}, 'zh': {'title': 'Ëá™Âä®ÂåñËØÑ‰º∞ÔºåÊèêÂçáÊ®°ÂûãËØÑ‰º∞ÊïàÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Agentic Automatic Evaluation (A2Eval) ÁöÑËá™Âä®ËØÑ‰º∞Ê°ÜÊû∂ÔºåÊó®Âú®ÊîπÂñÑÁé∞ÊúâÁöÑÂÖ∑Ë∫´ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞ÊñπÊ≥ï„ÄÇ‰º†ÁªüËØÑ‰º∞‰æùËµñ‰∫éÈùôÊÄÅÁöÑ„ÄÅ‰∏ìÂÆ∂ÂÆö‰πâÁöÑÊâãÂä®Ê†áÊ≥®Âü∫ÂáÜÔºåÂ≠òÂú®ÂÜó‰ΩôÂíåË¶ÜÁõñ‰∏çÂùáÁöÑÈóÆÈ¢òÔºåÂØºËá¥È´òÊòÇÁöÑÊàêÊú¨ÂíåÊ®°ÂûãÊéíÂêçÂ§±Áúü„ÄÇA2EvalÈÄöËøá‰∏§‰∏™Âçè‰Ωú‰ª£ÁêÜËá™Âä®ÂåñÂü∫ÂáÜÂàõÂª∫ÂíåËØÑ‰º∞ÔºåÊòæËëóÈôç‰Ωé‰∫ÜËØÑ‰º∞Â•ó‰ª∂ÁöÑËßÑÊ®°ÂíåËÆ°ÁÆóÊàêÊú¨ÔºåÂêåÊó∂ÊèêÈ´ò‰∫ÜËØÑ‰º∞ÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁªèËøá10‰∏™Âü∫ÂáÜÂíå13‰∏™Ê®°ÂûãÁöÑËØÑ‰º∞ÔºåA2EvalÂÆûÁé∞‰∫Ü85%ÁöÑËØÑ‰º∞Â•ó‰ª∂ÂéãÁº©Âíå77%ÁöÑËÆ°ÁÆóÊàêÊú¨Èôç‰ΩéÔºåÁ°ÆÁ´ã‰∫ÜÈ´ò‰øùÁúü„ÄÅ‰ΩéÊàêÊú¨ÁöÑËØÑ‰º∞Êñ∞Ê†áÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04486', 'title': 'Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition', 'url': 'https://huggingface.co/papers/2602.04486', 'abstract': 'MLLMs suffer from modality bias in GMNER tasks, which is addressed through a proposed method that enforces cross-modal reasoning via multi-style reasoning schema injection and constraint-guided verifiable optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit modality bias, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning (MCR), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines.', 'score': 6, 'issue_id': 913, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '4754e7698d48a793', 'authors': ['Jinlong Ma', 'Yu Zhang', 'Xuefeng Bai', 'Kehai Chen', 'Yuwei Wang', 'Zeming Liu', 'Jun Yu', 'Min Zhang'], 'affiliations': ['Beijing University of Aeronautics and Astronautics', 'Harbin Institute of Technology, Shenzhen, China', 'Institute of Computing Technology Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2602.04486.jpg', 'data': {'categories': ['#training', '#multimodal', '#rlhf'], 'emoji': 'üîç', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–Ω–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –úLLM —á–µ—Ä–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∑–∞–¥–∞—á–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–æ–π. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —ç—Ç–∏ –º–æ–¥–µ–ª–∏ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–Ω–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏, —Ç–æ –µ—Å—Ç—å –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –æ–¥–Ω—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å –≤–º–µ—Å—Ç–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ MCR, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—ä–µ–∫—Ü–∏—é –º–Ω–æ–≥–æ—Å—Ç–∏–ª–µ–≤—ã—Ö —Å—Ö–µ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é, —É–ø—Ä–∞–≤–ª—è–µ–º—É—é –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏, –¥–ª—è –ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–º—É –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–Ω–æ–π –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã.'}, 'en': {'title': 'Enhancing Cross-Modal Reasoning to Combat Modality Bias in GMNER', 'desc': 'This paper addresses the issue of modality bias in Grounded Multimodal Named Entity Recognition (GMNER) tasks when using Multimodal Large Language Models (MLLMs). The authors propose a novel approach called Modality-aware Consistency Reasoning (MCR), which enhances cross-modal reasoning through techniques like Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI helps convert abstract constraints into actionable reasoning processes, while CVO allows the model to adjust its reasoning paths effectively. Experimental results show that MCR significantly reduces modality bias and outperforms existing methods in GMNER and visual grounding tasks.'}, 'zh': {'title': 'Ê∂àÈô§Ê®°ÊÄÅÂÅèÂ∑ÆÔºåÊèêÂçáÂ§öÊ®°ÊÄÅÂÆû‰ΩìËØÜÂà´ÊÄßËÉΩ', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®Âü∫‰∫éÊñáÊú¨ÁöÑÂÆû‰ΩìËØÜÂà´‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÈíàÂØπÊ®°ÊÄÅÂÅèÂ∑ÆÁöÑÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫Ê®°ÊÄÅÊÑüÁü•‰∏ÄËá¥ÊÄßÊé®ÁêÜÔºàMCRÔºâÔºåÈÄöËøáÂ§öÊ†∑ÂºèÊé®ÁêÜÊ®°ÂºèÊ≥®ÂÖ•ÔºàMRSIÔºâÂíåÁ∫¶ÊùüÂºïÂØºÁöÑÂèØÈ™åËØÅ‰ºòÂåñÔºàCVOÔºâÊù•Â¢ûÂº∫Ë∑®Ê®°ÊÄÅÊé®ÁêÜËÉΩÂäõ„ÄÇMRSIÂ∞ÜÊäΩË±°Á∫¶ÊùüËΩ¨Âåñ‰∏∫ÂèØÊâßË°åÁöÑÊé®ÁêÜÈìæÔºåËÄåCVOÂàô‰ΩøÊ®°ÂûãËÉΩÂ§üÂä®ÊÄÅË∞ÉÊï¥Êé®ÁêÜË∑ØÂæÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMCRÊúâÊïàÂáèËΩª‰∫ÜÊ®°ÊÄÅÂÅèÂ∑ÆÔºåÂπ∂Âú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂü∫ÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04442', 'title': 'No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data', 'url': 'https://huggingface.co/papers/2602.04442', 'abstract': 'Machine translation experiments for Turkic languages using nllb-200, LoRA fine-tuning, and prompt-based approaches achieved varying chrF++ scores across language pairs.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.', 'score': 4, 'issue_id': 921, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '7dd630b48c5ca316', 'authors': ['Dmitry Karpov'], 'affiliations': ['PAO Severstal'], 'pdf_title_img': 'assets/pdf/title_img/2602.04442.jpg', 'data': {'categories': ['#multilingual', '#low_resource', '#synthetic', '#training', '#open_source', '#rag', '#machine_translation', '#dataset', '#small_models', '#optimization'], 'emoji': 'üåç', 'ru': {'title': '–ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Ç—é—Ä–∫—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —á–µ—Ä–µ–∑ –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –ø—Ä–æ–º–ø—Ç-–∏–Ω–∂–µ–Ω–µ—Ä–∏–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–≤—è—â–µ–Ω–æ –º–∞—à–∏–Ω–Ω–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É –º–µ–∂–¥—É —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º –∏ —Ç—é—Ä–∫—Å–∫–∏–º–∏ —è–∑—ã–∫–∞–º–∏, –≤–∫–ª—é—á–∞—è –±–∞—à–∫–∏—Ä—Å–∫–∏–π, –∫–∞–∑–∞—Ö—Å–∫–∏–π, –∫–∏—Ä–≥–∏–∑—Å–∫–∏–π, —Ç–∞—Ç–∞—Ä—Å–∫–∏–π –∏ —á—É–≤–∞—à—Å–∫–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ö–æ–¥–æ–≤: —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –º–æ–¥–µ–ª–∏ NLLB-200 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–¥–∞–ø—Ç–µ—Ä–∞ LoRA –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ prompt-based –º–µ—Ç–æ–¥—ã —Å –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é DeepSeek. –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∞ —Ç–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞, –¥–æ—Å—Ç–∏–≥–Ω—É–≤ –º–µ—Ç—Ä–∏–∫–∏ chrF++ 49.71 –¥–ª—è –∫–∞–∑–∞—Ö—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, –∞ prompt-based –ø–æ–¥—Ö–æ–¥ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —è–∑—ã–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ—Ç–∫—Ä—ã–ª–∏ –¥–æ—Å—Ç—É–ø –∫ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–Ω—ã–º –≤–µ—Å–∞–º –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.'}, 'en': {'title': 'Enhancing Turkic Language Translation with Advanced Techniques', 'desc': 'This paper investigates machine translation techniques for five Turkic language pairs, focusing on the effectiveness of the nllb-200 model. By applying LoRA fine-tuning on synthetic data, the authors achieved notable chrF++ scores, particularly 49.71 for Kazakh and 46.94 for Bashkir. Additionally, they explored prompt-based methods, which yielded a score of 39.47 for Chuvash using DeepSeek-V3.2. The study also highlights the performance of zero-shot and retrieval-based approaches for Tatar and Kyrgyz, and the authors provide the dataset and model weights for further research.'}, 'zh': {'title': 'Á™ÅÂé•ËØ≠Ë®ÄÊú∫Âô®ÁøªËØëÁöÑÊñ∞Êé¢Á¥¢', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫Ü‰∫îÁßçÁ™ÅÂé•ËØ≠Ë®ÄÂØπÁöÑÊú∫Âô®ÁøªËØëÔºåÂåÖÊã¨‰øÑËØ≠-Â∑¥‰ªÄÂü∫Â∞îËØ≠„ÄÅ‰øÑËØ≠-ÂìàËê®ÂÖãËØ≠„ÄÅ‰øÑËØ≠-ÂêâÂ∞îÂêâÊñØËØ≠„ÄÅËã±ËØ≠-Â°îÂ°îÂ∞îËØ≠ÂíåËã±ËØ≠-Ê•öÁì¶‰ªÄËØ≠„ÄÇÈÄöËøáÂØπnllb-200-distilled-600MÊ®°ÂûãËøõË°åLoRAÂæÆË∞ÉÔºå‰ΩøÁî®ÂêàÊàêÊï∞ÊçÆÂú®ÂìàËê®ÂÖãËØ≠ÂíåÂ∑¥‰ªÄÂü∫Â∞îËØ≠‰∏äÂàÜÂà´ËææÂà∞‰∫ÜchrF++ 49.71Âíå46.94ÁöÑÂæóÂàÜ„ÄÇ‰ΩøÁî®DeepSeek-V3.2ËøõË°åÊèêÁ§∫ÔºåÁªìÂêàÊ£ÄÁ¥¢Âà∞ÁöÑÁõ∏‰ººÁ§∫‰æãÔºåÊ•öÁì¶‰ªÄËØ≠ÁöÑÂæóÂàÜ‰∏∫39.47„ÄÇÊàë‰ª¨ËøòÂèëÂ∏É‰∫ÜÊï∞ÊçÆÈõÜÂíåËé∑ÂæóÁöÑÊ®°ÂûãÊùÉÈáçÔºå‰ª•‰æõËøõ‰∏ÄÊ≠•Á†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02350', 'title': 'Context Learning for Multi-Agent Discussion', 'url': 'https://huggingface.co/papers/2602.02350', 'abstract': 'Multi-Agent Discussion methods suffer from inconsistency due to individual context misalignment, which is addressed through a context learning approach that dynamically generates context instructions for each agent to improve consensus reaching and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.', 'score': 4, 'issue_id': 913, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'b15d12664950527b', 'authors': ['Xingyuan Hua', 'Sheng Yue', 'Xinyi Li', 'Yizhe Zhao', 'Jinrui Zhang', 'Ju Ren'], 'affiliations': ['College of Computer Science, Northwest University', 'Department of Computer Science and Technology, Tsinghua University', 'School of Cyber Science and Technology, Sun Yat-sen University', 'Zhongguancun Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2602.02350.jpg', 'data': {'categories': [], 'emoji': 'ü§ù', 'ru': {'title': '–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –≤ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–µ—Ç–æ–¥–∞—Ö –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è, –≥–¥–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ M2CL, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, —Å–ø–æ—Å–æ–±–Ω—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —Ä–∞—É–Ω–¥–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ú–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∫ —à—É–º—É –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –¥–æ—Å—Ç–∏—á—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ M2CL –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ 20-50% –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≤–æ–ø–ª–æ—â—ë–Ω–Ω—ã—Ö –∏ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Enhancing Consensus in Multi-Agent Discussions with Dynamic Context Learning', 'desc': 'This paper addresses the problem of inconsistency in Multi-Agent Discussion (MAD) methods caused by misaligned individual contexts among agents. It introduces a novel approach called Multi-LLM Context Learning (M2CL), which dynamically generates context instructions for each agent during discussions. By employing a self-adaptive mechanism, M2CL enhances context coherence and reduces discrepancies, allowing agents to reach a more accurate consensus. The results demonstrate that M2CL outperforms existing methods by a significant margin, improving performance on various complex tasks.'}, 'zh': {'title': 'Âä®ÊÄÅÁîüÊàê‰∏ä‰∏ãÊñáÔºåÊèêÂçáÂ§öÊô∫ËÉΩ‰ΩìËÆ®ËÆ∫‰∏ÄËá¥ÊÄß', 'desc': 'Â§öÊô∫ËÉΩ‰ΩìËÆ®ËÆ∫ÊñπÊ≥ïÔºàMADÔºâÂú®‰∏™‰Ωì‰∏ä‰∏ãÊñá‰∏ç‰∏ÄËá¥ÁöÑÊÉÖÂÜµ‰∏ãÂÆπÊòìÂá∫Áé∞ËÆ®ËÆ∫‰∏ç‰∏ÄËá¥ÁöÑÈóÆÈ¢ò„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öLLM‰∏ä‰∏ãÊñáÂ≠¶‰π†ÊñπÊ≥ïÔºàM2CLÔºâÔºåËØ•ÊñπÊ≥ï‰∏∫ÊØè‰∏™Êô∫ËÉΩ‰ΩìÂ≠¶‰π†‰∏Ä‰∏™‰∏ä‰∏ãÊñáÁîüÊàêÂô®ÔºåËÉΩÂ§üÂú®ÊØèËΩÆËÆ®ËÆ∫‰∏≠Âä®ÊÄÅÁîüÊàê‰∏ä‰∏ãÊñáÊåá‰ª§„ÄÇM2CLÈÄöËøáËá™Âä®‰ø°ÊÅØÁªÑÁªáÂíåÁ≤æÁÇºÔºåÊéßÂà∂‰∏ä‰∏ãÊñáÁöÑ‰∏ÄËá¥ÊÄßÂíåËæìÂá∫Â∑ÆÂºÇÔºå‰ªéËÄåÂ∏ÆÂä©Êô∫ËÉΩ‰ΩìÈÄêÊ≠•ËææÊàêÊ≠£Á°ÆÁöÑÂÖ±ËØÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåM2CLÂú®Â≠¶ÊúØÊé®ÁêÜ„ÄÅÂÖ∑Ë∫´‰ªªÂä°ÂíåÁßªÂä®ÊéßÂà∂Á≠âÊåëÊàòÊÄß‰ªªÂä°‰∏äÔºåÊÄßËÉΩÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÊèêÂçáÂπÖÂ∫¶ËææÂà∞20%Ëá≥50%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.20499', 'title': 'Efficient Autoregressive Video Diffusion with Dummy Head', 'url': 'https://huggingface.co/papers/2601.20499', 'abstract': 'Autoregressive video diffusion models suffer from inefficient attention mechanisms that underutilize historical frames, but a new method called Dummy Forcing improves efficiency through heterogeneous memory allocation and dynamic head programming while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.', 'score': 4, 'issue_id': 916, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'}, 'hash': 'ec5420af41fc1df3', 'authors': ['Hang Guo', 'Zhaoyang Jia', 'Jiahao Li', 'Bin Li', 'Yuanhao Cai', 'Jiangshan Wang', 'Yawei Li', 'Yan Lu'], 'affiliations': ['ETH Zurich', 'Johns Hopkins University', 'Microsoft Research Asia', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.20499.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#video', '#inference', '#architecture'], 'emoji': '‚ö°', 'ru': {'title': 'Dummy Forcing: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–∏ —á–µ—Ä–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –≤–Ω–∏–º–∞–Ω–∏—è', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –≤–∏–¥–µ–æ-–¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –≤—ã—è–≤–ª—è—é—Ç—Å—è –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ multi-head self-attention, –≥–¥–µ –æ–∫–æ–ª–æ 25% –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –ø–æ—á—Ç–∏ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Ç–µ–∫—É—â–µ–º –∫–∞–¥—Ä–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ Dummy Forcing, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –¢–µ—Ö–Ω–∏–∫–∞ –ø–∞–∫–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –∫—ç—à–∞ KV –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 2 —Ä–∞–∑–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ (–ø–æ—Ç–µ—Ä—è –º–µ–Ω–µ–µ 0.5%) –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ 24.3 FPS.'}, 'en': {'title': 'Enhancing Video Diffusion Efficiency with Dummy Forcing', 'desc': 'This paper addresses the inefficiencies in autoregressive video diffusion models caused by their attention mechanisms, which often do not fully utilize historical frames. The authors introduce a method called Dummy Forcing, which enhances efficiency by implementing heterogeneous memory allocation and dynamic head programming. This approach reduces redundancy in head-wise context and allows for better management of attention across different frames. As a result, Dummy Forcing achieves significant speed improvements in video generation while maintaining high quality, demonstrating a 2.0x speedup with minimal quality loss.'}, 'zh': {'title': 'ÊèêÂçáËßÜÈ¢ëÁîüÊàêÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Ëá™ÂõûÂΩíËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂú®Â§ÑÁêÜÂéÜÂè≤Â∏ßÊó∂ÊïàÁéá‰∏çÈ´òÔºåÂØºËá¥Ê≥®ÊÑèÂäõÊú∫Âà∂ÁöÑÂà©Áî®‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁß∞‰∏∫Dummy ForcingÔºåÈÄöËøáÂºÇÊûÑÂÜÖÂ≠òÂàÜÈÖçÂíåÂä®ÊÄÅÂ§¥ÁºñÁ®ãÊù•ÊèêÈ´òÊïàÁéáÔºåÂêåÊó∂‰øùÊåÅÁîüÊàêË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÂáèÂ∞ë‰∫ÜÂ§¥ÈÉ®‰∏ä‰∏ãÊñáÁöÑÂÜó‰ΩôÔºåÂπ∂ÈÄöËøá‰∏ä‰∏ãÊñáÊâìÂåÖÊäÄÊúØÂÆûÁé∞‰∫ÜÊõ¥Âº∫ÁöÑÁºìÂ≠òÂéãÁº©„ÄÇDummy ForcingÂú®‰∏çÂ¢ûÂä†È¢ùÂ§ñËÆ≠ÁªÉÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÂ∞ÜÈÄüÂ∫¶ÊèêÂçáËá≥Âü∫Á∫øÁöÑ2.0ÂÄçÔºåÊîØÊåÅ‰ª•24.3Â∏ßÊØèÁßíÁöÑÈÄüÂ∫¶ÁîüÊàêËßÜÈ¢ëÔºå‰∏îË¥®Èáè‰∏ãÈôç‰∏çÂà∞0.5%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04805', 'title': 'Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging', 'url': 'https://huggingface.co/papers/2602.04805', 'abstract': 'Generative 3D models face challenges in animation rigging, which this work addresses by introducing SkinTokens‚Äîa learned discrete representation for skinning weights‚Äîand TokenRig, a unified autoregressive framework that models skeletons and skin deformations together, improving rigging accuracy through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduce SkinTokens: a learned, compact, and discrete representation for skinning weights. By leveraging an FSQ-CVAE to capture the intrinsic sparsity of skinning, we reframe the task from continuous regression to a more tractable token sequence prediction problem. This representation enables TokenRig, a unified autoregressive framework that models the entire rig as a single sequence of skeletal parameters and SkinTokens, learning the complicated dependencies between skeletons and skin deformations. The unified model is then amenable to a reinforcement learning stage, where tailored geometric and semantic rewards improve generalization to complex, out-of-distribution assets. Quantitatively, the SkinTokens representation leads to a 98%-133% percents improvement in skinning accuracy over state-of-the-art methods, while the full TokenRig framework, refined with RL, enhances bone prediction by 17%-22%. Our work presents a unified, generative approach to rigging that yields higher fidelity and robustness, offering a scalable solution to a long-standing challenge in 3D content creation.', 'score': 3, 'issue_id': 914, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'c8d5aa1f9ec328d2', 'authors': ['Jia-peng Zhang', 'Cheng-Feng Pu', 'Meng-Hao Guo', 'Yan-Pei Cao', 'Shi-Min Hu'], 'affiliations': ['BNRist, Department of Computer Science and Technology, Tsinghua University, China', 'VAST, China', 'Zhili College, Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.04805.jpg', 'data': {'categories': ['#rl', '#3d', '#architecture', '#optimization'], 'emoji': 'üé≠', 'ru': {'title': '–î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏–≥–≥–∏–Ω–≥–∞ 3D-–ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π', 'desc': '–†–∞–±–æ—Ç–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏–≥–≥–∏–Ω–≥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö 3D-–º–æ–¥–µ–ª–µ–π, –ø—Ä–µ–¥–ª–æ–∂–∏–≤ SkinTokens ‚Äî –æ–±—É—á–µ–Ω–Ω–æ–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ —Å–∫–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ TokenRig ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≤—Ç—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–≤–º–µ—Å—Ç–Ω–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —Å–∫–µ–ª–µ—Ç –∏ –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫–æ–∂–∏ –∫–∞–∫ –µ–¥–∏–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ SkinTokens –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–∞–¥–∞—á—É –∏–∑ –≤—ã—Å–æ–∫–æ–º–µ—Ä–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∏–≥–≥–∏–Ω–≥–∞ –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ —Å–ª–æ–∂–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã.'}, 'en': {'title': 'Revolutionizing 3D Rigging with SkinTokens and TokenRig', 'desc': 'This paper addresses the challenges of animation rigging in generative 3D models by introducing SkinTokens, a learned discrete representation for skinning weights. It reframes the skinning task from a complex regression problem to a more manageable token sequence prediction problem using a framework called TokenRig. This unified autoregressive model simultaneously learns the relationships between skeletons and skin deformations, enhancing rigging accuracy. The approach is further improved through reinforcement learning, resulting in significant gains in skinning accuracy and robustness for 3D content creation.'}, 'zh': {'title': 'ÁîüÊàê3DÊ®°ÂûãÁöÑÈ´òÊïàÂä®ÁîªÁªëÂÆöËß£ÂÜ≥ÊñπÊ°à', 'desc': 'Êú¨Á†îÁ©∂ÈíàÂØπÁîüÊàê3DÊ®°ÂûãÂú®Âä®ÁîªÁªëÂÆö‰∏≠ÁöÑÊåëÊàòÔºåÊèêÂá∫‰∫ÜSkinTokensÔºåËøôÊòØ‰∏ÄÁßçÂ≠¶‰π†ÁöÑÁ¶ªÊï£Ë°®Á§∫ÔºåÁî®‰∫éÁöÆËÇ§ÊùÉÈáçÁöÑÂª∫Ê®°„ÄÇÈÄöËøáÂ∞ÜÁöÆËÇ§ÁªëÂÆö‰ªªÂä°ÈáçÊñ∞ÂÆö‰πâ‰∏∫Êõ¥ÊòìÂ§ÑÁêÜÁöÑ‰ª§ÁâåÂ∫èÂàóÈ¢ÑÊµãÈóÆÈ¢òÔºåÁ†îÁ©∂ËÄÖÂà©Áî®FSQ-CVAEÊçïÊçâÁöÆËÇ§ÁªëÂÆöÁöÑÂÜÖÂú®Á®ÄÁñèÊÄß„ÄÇTokenRigÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑËá™ÂõûÂΩíÊ°ÜÊû∂ÔºåÂÆÉÂ∞ÜÈ™®È™ºÂèÇÊï∞ÂíåSkinTokensÂª∫Ê®°‰∏∫‰∏Ä‰∏™Âçï‰∏ÄÂ∫èÂàóÔºå‰ªéËÄåÂ≠¶‰π†È™®È™º‰∏éÁöÆËÇ§ÂèòÂΩ¢‰πãÈó¥ÁöÑÂ§çÊùÇ‰æùËµñÂÖ≥Á≥ª„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÔºåÊ®°ÂûãÂú®Â§çÊùÇËµÑ‰∫ß‰∏äÁöÑÊ≥õÂåñËÉΩÂäõÂæóÂà∞‰∫ÜÊèêÂçáÔºåSkinTokensÁöÑË°®Á§∫Âú®ÁöÆËÇ§ÁªëÂÆöÁ≤æÂ∫¶‰∏äÊØîÁé∞ÊúâÊñπÊ≥ïÊèêÈ´ò‰∫Ü98%-133%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01849', 'title': 'Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models', 'url': 'https://huggingface.co/papers/2602.01849', 'abstract': 'Self-rewarding sequential Monte Carlo enables effective sampling of masked diffusion language models by using parallel diffusion processes and trajectory-level confidence signals to improve generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.', 'score': 3, 'issue_id': 917, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'c3ff136c0b47f18f', 'authors': ['Ziwei Luo', 'Ziqi Jin', 'Lei Wang', 'Lidong Bing', 'Thomas B. Sch√∂n'], 'affiliations': ['MiroMind AI, Singapore', 'Nanyang Technological University, Singapore', 'Uppsala University, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2602.01849.jpg', 'data': {'categories': ['#training', '#open_source', '#optimization', '#diffusion', '#inference'], 'emoji': 'üéØ', 'ru': {'title': '–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Å —Å–∞–º–æ–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º —Å–∞–º–æ–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–∞–µ–º–æ–≥–æ –º–µ—Ç–æ–¥–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (SMC) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è —Å–æ—Å—Ç–æ–∏—Ç –≤ –∑–∞–ø—É—Å–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (—á–∞—Å—Ç–∏—Ü) –≤–º–µ—Å—Ç–æ –∂–∞–¥–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø—É—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ê–ª–≥–æ—Ä–∏—Ç–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–∏–≥–Ω–∞–ª–∞ —Å–∞–º–æ–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏—è –≤–µ—Å–æ–≤ —á–∞—Å—Ç–∏—Ü–∞–º, –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ–≤–∑–≤–µ—à–∏–≤–∞—è –∏ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞—è –∏—Ö. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤ –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã–±–æ—Ä–∫–∏.'}, 'en': {'title': 'Enhancing Diversity in Language Models with Self-Rewarding SMC', 'desc': 'This paper introduces a new algorithm called self-rewarding sequential Monte Carlo (SMC) for improving the sampling process of masked diffusion language models (MDLMs). The authors highlight that traditional methods often use a confidence-based approach that limits diversity by only selecting the most confident tokens, leading to poor generation quality. To overcome this, the proposed SMC method utilizes multiple parallel diffusion processes, or particles, to explore different generation paths. By incorporating trajectory-level confidence as a self-rewarding signal, the algorithm effectively enhances the quality of generated samples without requiring additional training or external rewards.'}, 'zh': {'title': 'Ëá™Â•ñÂä±Â∫èÂàóËíôÁâπÂç°Ê¥õÔºöÊèêÂçáÊé©ËîΩÊâ©Êï£Ê®°ÂûãÁöÑÈááÊ†∑Ë¥®Èáè', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™Â•ñÂä±ÁöÑÂ∫èÂàóËíôÁâπÂç°Ê¥õÔºàSMCÔºâÁÆóÊ≥ïÔºåÊó®Âú®ÊúâÊïàÈááÊ†∑Êé©ËîΩÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàMDLMsÔºâ„ÄÇËØ•ÁÆóÊ≥ïÈÄöËøáËßÇÂØüÁé∞ÊúâMDLMs‰æùËµñ‰∫éÂü∫‰∫éÁΩÆ‰ø°Â∫¶ÁöÑÈááÊ†∑Á≠ñÁï•ÔºåËß£ÂÜ≥‰∫ÜÁîüÊàêËøáÁ®ã‰∏≠ÁöÑÂ§öÊ†∑ÊÄß‰∏ãÈôçÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÈÄöËøáÂπ∂Ë°åÂêØÂä®Â§ö‰∏™‰∫§‰∫íÁöÑÊâ©Êï£ËøáÁ®ãÔºàÁß∞‰∏∫Á≤íÂ≠êÔºâÊù•Êé¢Á¥¢ËΩ®ËøπÔºåÂπ∂ÂºïÂÖ•ËΩ®ËøπÁ∫ßÂà´ÁöÑÁΩÆ‰ø°Â∫¶‰Ωú‰∏∫Ëá™Â•ñÂä±‰ø°Âè∑Ôºå‰ª•ÂàÜÈÖçÁ≤íÂ≠êÁöÑÈáçË¶ÅÊÄßÊùÉÈáç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™MDLMsÂíåÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÈááÊ†∑Ë¥®ÈáèÔºåËÄåÊó†ÈúÄÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÊàñÂ•ñÂä±ÊåáÂØº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04883', 'title': 'Protein Autoregressive Modeling via Multiscale Structure Generation', 'url': 'https://huggingface.co/papers/2602.04883', 'abstract': 'PAR is a multi-scale autoregressive framework for protein backbone generation that uses hierarchical structure modeling, autoregressive transformers, and flow-based decoding to produce high-quality protein structures with improved generalization and reduced exposure bias.  \t\t\t\t\tAI-generated summary \t\t\t\t We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.', 'score': 2, 'issue_id': 914, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'f2aa1a9a84c88dae', 'authors': ['Yanru Qu', 'Cheng-Yen Hsieh', 'Zaixiang Zheng', 'Ge Liu', 'Quanquan Gu'], 'affiliations': ['ByteDance Seed', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2602.04883.jpg', 'data': {'categories': ['#benchmark', '#training', '#architecture'], 'emoji': 'üß¨', 'ru': {'title': '–û—Ç –≥—Ä—É–±—ã—Ö —Ñ–æ—Ä–º –∫ —Ç–æ—á–Ω—ã–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º: –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Å–∫—É–ª—å–ø—Ç—É—Ä–∞ –±–µ–ª–∫–æ–≤', 'desc': 'PAR ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–ª–∫–æ–≤—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –æ—Ç –≥—Ä—É–±—ã—Ö –¥–µ—Ç–∞–ª–µ–π –∫ —Ç–æ—á–Ω—ã–º. –ú–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä—ë—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ downsampling –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–¥–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å–æ–∑–¥–∞—ë—Ç —É—Å–ª–æ–≤–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –∏ flow-based –¥–µ–∫–æ–¥–µ—Ä–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—Ç–æ–º–æ–≤ –æ—Å—Ç–æ–≤–∞. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É exposure bias ‚Äî –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π ‚Äî —á–µ—Ä–µ–∑ noisy context learning –∏ scheduled sampling. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–µ–ª–∫–∏ –ø–æ —É—Å–ª–æ–≤–∏—è–º –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'PAR: Sculpting Protein Structures with Multi-Scale Autoregressive Modeling', 'desc': 'The paper introduces PAR, a novel multi-scale autoregressive framework designed for generating protein backbones. It utilizes a hierarchical structure modeling approach, where protein structures are created in a coarse-to-fine manner, allowing for detailed refinement at each scale. Key components include multi-scale downsampling for training, an autoregressive transformer for encoding information, and a flow-based decoder for generating backbone atoms. PAR effectively addresses exposure bias through techniques like noisy context learning, enabling high-quality protein structure generation with strong generalization capabilities.'}, 'zh': {'title': 'PARÔºöËõãÁôΩË¥®ÁªìÊûÑÁîüÊàêÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'PARÊòØ‰∏ÄÁßçÂ§öÂ∞∫Â∫¶Ëá™ÂõûÂΩíÊ°ÜÊû∂ÔºåÁî®‰∫éËõãÁôΩË¥®‰∏ªÈìæÁöÑÁîüÊàê„ÄÇÂÆÉÈÄöËøáÂ±ÇÊ¨°ÁªìÊûÑÂª∫Ê®°„ÄÅËá™ÂõûÂΩíÂèòÊç¢Âô®ÂíåÂü∫‰∫éÊµÅÁöÑËß£Á†ÅÔºåÁîüÊàêÈ´òË¥®ÈáèÁöÑËõãÁôΩË¥®ÁªìÊûÑ„ÄÇPARÁöÑÂÖ≥ÈîÆÂú®‰∫éÂ§öÂ∞∫Â∫¶‰∏ãÈááÊ†∑Êìç‰Ωú„ÄÅËá™ÂõûÂΩíÂèòÊç¢Âô®ÂíåÊµÅÂºè‰∏ªÈìæËß£Á†ÅÂô®ÔºåËøô‰∫õÁªÑ‰ª∂ÂÖ±Âêå‰ΩúÁî®‰ª•ÊèêÈ´òÁîüÊàêÁöÑÂáÜÁ°ÆÊÄßÂíåË¥®Èáè„ÄÇÊ≠§Â§ñÔºåPARÊúâÊïàÁºìËß£‰∫ÜËá™ÂõûÂΩíÊ®°ÂûãÁöÑÊõùÂÖâÂÅèÂ∑ÆÈóÆÈ¢òÔºåÂ±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04651', 'title': 'SAFE: Stable Alignment Finetuning with Entropy-Aware Predictive Control for RLHF', 'url': 'https://huggingface.co/papers/2602.04651', 'abstract': "A new reinforcement learning algorithm for language model alignment that improves stability and performance over PPO through enhanced KL divergence control and adaptive reward management.  \t\t\t\t\tAI-generated summary \t\t\t\t Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner and suffers form reward oscillations, entropy collapse, value function drift, and sudden policy divergence that require frequent restarts and extensive hyperparameter tuning. In this paper, we develop a new pure on policy actor-critic RL method for the LM-RLHF setting. We present SAFE (Stable Alignment Finetuning with Entropy-aware control),a novel RLHF algorithm that combines a Double Soft-Min Critic for pessimistic value estimation with a new multi-layer stabilization framework combining entropy-gated KL regulation, and PID-controlled adaptive thresholds. Unlike standard PPO's symmetric KL penalties, SAFE distinguishes high-entropy exploration from low-entropy mode collapse and adjusts penalties dynamically based on reward velocity. Experiments on a 3B parameter model show SAFE achieves +5.15\\% training-average reward than PPO (0.725 vs 0.689), negligible reward crashes, and superior KL control than ppo . Our method adds minimal computational overhead and provides an interpretable, crash-resistant RLHF framework that maintains aggressive learning speed while ensuring stable long-horizon optimization suitable for production deployment. Code is available at https://github.com/ryyzn9/SAFE", 'score': 1, 'issue_id': 922, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '3dff57d5bb02d752', 'authors': ['Dipan Maity'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2602.04651.jpg', 'data': {'categories': ['#training', '#alignment', '#rl', '#optimization', '#rlhf', '#open_source'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é —ç–Ω—Ç—Ä–æ–ø–∏—é', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º SAFE –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –º–µ—Ç–æ–¥–∞ PPO. –û—Å–Ω–æ–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á—ë—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–≤–æ–π–Ω–æ–≥–æ –∫—Ä–∏—Ç–∏–∫–∞ –¥–ª—è –ø–µ—Å—Å–∏–º–∏—Å—Ç–∏—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏. –ê–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—É—é —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é —Å —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–æ-—É–ø—Ä–∞–≤–ª—è–µ–º—ã–º —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º KL-—Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º–∏ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞–º–∏ PID. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ 5.15%, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª–Ω–æ–µ –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ —Å–±–æ–µ–≤ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –Ω–∞–¥—ë–∂–Ω—É—é —Ä–∞–±–æ—Ç—É –ø—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–º —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–∏.'}, 'en': {'title': 'SAFE: A Stable Approach to Language Model Alignment', 'desc': "This paper introduces SAFE, a new reinforcement learning algorithm designed for aligning language models more effectively than the traditional Proximal Policy Optimization (PPO). SAFE enhances stability and performance by implementing a Double Soft-Min Critic for better value estimation and a multi-layer stabilization framework that includes entropy-aware KL divergence control. Unlike PPO, which uses fixed penalties, SAFE dynamically adjusts penalties based on the reward's rate of change, helping to prevent issues like reward oscillations and policy divergence. Experimental results demonstrate that SAFE outperforms PPO in training-average reward while maintaining computational efficiency and stability, making it suitable for real-world applications."}, 'zh': {'title': 'Á®≥ÂÆöÂØπÈΩêÔºåÊèêÂçáÊÄßËÉΩÁöÑÂº∫ÂåñÂ≠¶‰π†Êñ∞ÁÆóÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®ÊîπÂñÑËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩêÁöÑÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ„ÄÇËØ•ÁÆóÊ≥ïÂêç‰∏∫SAFEÔºàÁ®≥ÂÆöÂØπÈΩêÂæÆË∞É‰∏éÁÜµÊÑüÁü•ÊéßÂà∂ÔºâÔºåÈÄöËøáÂ¢ûÂº∫ÁöÑKLÊï£Â∫¶ÊéßÂà∂ÂíåËá™ÈÄÇÂ∫îÂ•ñÂä±ÁÆ°ÁêÜÊù•‰ºòÂåñ‰º†ÁªüÁöÑPPOÊñπÊ≥ï„ÄÇ‰∏éÊ†áÂáÜPPOÁöÑÂØπÁß∞KLÊÉ©ÁΩö‰∏çÂêåÔºåSAFEËÉΩÂ§üÂä®ÊÄÅË∞ÉÊï¥ÊÉ©ÁΩöÔºå‰ª•Âå∫ÂàÜÈ´òÁÜµÊé¢Á¥¢Âíå‰ΩéÁÜµÊ®°ÂºèÂ¥©Ê∫É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSAFEÂú®ËÆ≠ÁªÉÂπ≥ÂùáÂ•ñÂä±‰∏äÊØîPPOÊèêÈ´ò‰∫Ü5.15%ÔºåÂπ∂‰∏îÂú®KLÊéßÂà∂ÊñπÈù¢Ë°®Áé∞‰ºòË∂ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04605', 'title': 'RexBERT: Context Specialized Bidirectional Encoders for E-commerce', 'url': 'https://huggingface.co/papers/2602.04605', 'abstract': "RexBERT, a family of BERT-style encoders designed for e-commerce semantics, achieves superior performance on domain-specific tasks through specialized pretraining and high-quality in-domain data.  \t\t\t\t\tAI-generated summary \t\t\t\t Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.", 'score': 1, 'issue_id': 913, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'fb4dcd00a576c30e', 'authors': ['Rahul Bajaj', 'Anuj Garg'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2602.04605.jpg', 'data': {'categories': [], 'emoji': 'üõçÔ∏è', 'ru': {'title': '–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –ª—É—á—à–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö ‚Äî –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–Ω–µ–µ –º–∞—Å—à—Ç–∞–±–∞', 'desc': 'RexBERT ‚Äî —ç—Ç–æ —Å–µ–º–µ–π—Å—Ç–≤–æ BERT-–ø–æ–¥–æ–±–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ Ecom-niverse, –∫–æ—Ä–ø—É—Å –∏–∑ 350 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ–±—Ä–∞–Ω–Ω—ã–π –∏–∑ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ä–æ–∑–Ω–∏—á–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏, –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–æ–¥—É–ª—å–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞: –æ–±—â–µ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ, —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –¥–æ–º–µ–Ω—É —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏. –ú–æ–¥–µ–ª–∏ RexBERT —Å 17M –¥–æ 400M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –∫—Ä—É–ø–Ω—ã–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ in-domain –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–Ω–µ–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'RexBERT: Tailored Transformers for E-Commerce Excellence', 'desc': "RexBERT is a new family of BERT-style encoders tailored for e-commerce tasks, achieving better results through specialized pretraining and high-quality data. It utilizes a massive dataset called Ecom-niverse, which consists of 350 billion tokens from various retail sources, ensuring comprehensive coverage of e-commerce semantics. The training process involves a three-phase approach that enhances the model's ability to understand context and specialize in domain-specific tasks. Remarkably, RexBERT models, despite having fewer parameters than traditional encoders, outperform them in tasks like token classification and semantic similarity, proving that targeted training is more effective than simply increasing model size."}, 'zh': {'title': 'RexBERTÔºöÁîµÂ≠êÂïÜÂä°ËØ≠‰πâÁöÑÂº∫Â§ßÁºñÁ†ÅÂô®', 'desc': 'RexBERTÊòØ‰∏ÄÁßç‰∏ì‰∏∫ÁîµÂ≠êÂïÜÂä°ËØ≠‰πâËÆæËÆ°ÁöÑBERTÈ£éÊ†ºÁºñÁ†ÅÂô®ÂÆ∂ÊóèÔºåÈÄöËøá‰∏ìÈó®ÁöÑÈ¢ÑËÆ≠ÁªÉÂíåÈ´òË¥®ÈáèÁöÑÈ¢ÜÂüüÂÜÖÊï∞ÊçÆÔºåÂú®ÁâπÂÆö‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇ„ÄÇÊàë‰ª¨ÂèëÂ∏É‰∫ÜEcom-niverseÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´3500‰∫ø‰∏™Ê†áËÆ∞ÁöÑËØ≠ÊñôÂ∫ìÔºå‰∏ìÈó®‰ªéÂêÑÁßçÈõ∂ÂîÆÂíåË¥≠Áâ©Êù•Ê∫ê‰∏≠Êï¥ÁêÜËÄåÊàê„ÄÇRexBERTÁöÑÈ¢ÑËÆ≠ÁªÉËøáÁ®ãÂåÖÊã¨‰∏ÄËà¨È¢ÑËÆ≠ÁªÉ„ÄÅ‰∏ä‰∏ãÊñáÊâ©Â±ïÂíåÈÄêÊ≠•È¢ÜÂüü‰∏ì‰∏öÂåñ‰∏â‰∏™Èò∂ÊÆµ„ÄÇÂ∞ΩÁÆ°ÂèÇÊï∞Êï∞ÈáèÊØî‰∏ÄËà¨ÁºñÁ†ÅÂô®Â∞ë2-3ÂÄçÔºåRexBERTÂú®ÁîµÂ≠êÂïÜÂä°Êï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞Ë∂ÖË∂ä‰∫ÜÊõ¥Â§ßÁöÑÈÄöÁî®ÁºñÁ†ÅÂô®ÔºåËØÅÊòé‰∫ÜÈ´òË¥®ÈáèÈ¢ÜÂüüÊï∞ÊçÆ‰∏éÂêàÁêÜËÆ≠ÁªÉÊñπÊ≥ïÁöÑÁªìÂêàÂú®ÁîµÂ≠êÂïÜÂä°Â∫îÁî®‰∏≠Êõ¥ÂÖ∑‰ºòÂäø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04547', 'title': 'OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis', 'url': 'https://huggingface.co/papers/2602.04547', 'abstract': 'OmniRad is a self-supervised radiological foundation model pretrained on 1.2 million medical images that demonstrates improved performance in classification and segmentation tasks through representation reuse and cross-task transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.', 'score': 1, 'issue_id': 916, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'f43f95e3f4610b81', 'authors': ['Luca Zedda', 'Andrea Loddo', 'Cecilia Di Ruberto'], 'affiliations': ['Department of Mathematics and Computer Science, University of Cagliari, Cagliari, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2602.04547.jpg', 'data': {'categories': ['#healthcare', '#science', '#transfer_learning', '#dataset', '#benchmark', '#training', '#cv'], 'emoji': '\U0001fa7b', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º', 'desc': 'OmniRad ‚Äî —ç—Ç–æ —Å–∞–º–æ–æ–±—É—á–∞–µ–º–∞—è —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è foundation model, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 1.2 –º–∏–ª–ª–∏–æ–Ω–∞—Ö –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–¥–∏–æ–ª–æ–≥–∏–∏, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø–µ—Ä–µ–Ω–æ—Å–∏–º–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏. –ü—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ OmniRad –ø–æ–∫–∞–∑–∞–ª–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ 2.05% –ø–æ –º–µ—Ç—Ä–∏–∫–µ F1 –∏ –¥–æ—Å—Ç–∏–≥–ª–∞ –ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Å—Ä–µ–¥–Ω–µ–π –º–µ—Ç—Ä–∏–∫–∏ Dice –¥–ª—è –∑–∞–¥–∞—á —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –∞–Ω–∞–ª–∏–∑–æ–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –ø–æ —Ä–∞–∑–Ω—ã–º —Ç–∏–ø–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'OmniRad: Revolutionizing Radiology with Self-Supervised Learning', 'desc': 'OmniRad is a self-supervised foundation model specifically designed for radiology, trained on a vast dataset of 1.2 million medical images. It enhances performance in both classification and segmentation tasks by leveraging representation reuse and enabling cross-task transferability. The model is evaluated through various adaptation methods, including lightweight task-specific adapters and full fine-tuning, demonstrating its versatility and effectiveness. Results show that OmniRad outperforms existing models in classification and segmentation benchmarks, indicating its potential for improving radiological analysis.'}, 'zh': {'title': 'OmniRadÔºöÊîæÂ∞ÑÂ≠¶‰ªªÂä°ÁöÑËá™ÁõëÁù£Âü∫Á°ÄÊ®°Âûã', 'desc': 'OmniRadÊòØ‰∏ÄÁßçËá™ÁõëÁù£ÁöÑÊîæÂ∞ÑÂ≠¶Âü∫Á°ÄÊ®°ÂûãÔºåÁªèËøá120‰∏áÂº†ÂåªÂ≠¶ÂõæÂÉèÁöÑÈ¢ÑËÆ≠ÁªÉÔºåËÉΩÂ§üÂú®ÂàÜÁ±ªÂíåÂàÜÂâ≤‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÂº∫Ë∞ÉË°®Á§∫ÈáçÁî®ÂíåË∑®‰ªªÂä°ÂèØËΩ¨ÁßªÊÄßÔºåÈÄÇÁî®‰∫éÂ§öÁßçÊàêÂÉèÊ®°ÂºèÁöÑ‰∏ãÊ∏∏‰ªªÂä°„ÄÇÊàë‰ª¨Âú®Â§ö‰∏™‰∏ãÊ∏∏ÈÄÇÂ∫îÊñπÊ°à‰∏ãËØÑ‰º∞‰∫ÜÈ¢ÑËÆ≠ÁªÉÁöÑÁºñÁ†ÅÂô®ÔºåÂåÖÊã¨‰ΩøÁî®ÂÜªÁªì‰∏ªÂπ≤ÁöÑËΩªÈáèÁ∫ß‰ªªÂä°ÁâπÂÆöÈÄÇÈÖçÂô®ÂíåÂÖ®Á´ØÂà∞Á´ØÂæÆË∞É„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOmniRadÂú®ÂàÜÁ±ªÂíåÂàÜÂâ≤‰ªªÂä°‰∏äÂùá‰ºò‰∫éÂÖ∂‰ªñÂü∫Á°ÄÊ®°ÂûãÔºåÊòæÁ§∫Âá∫Êõ¥Â•ΩÁöÑÁâπÂæÅËÅöÁ±ªÂíåÊ®°ÊÄÅÂàÜÁ¶ª„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04289', 'title': 'Proxy Compression for Language Modeling', 'url': 'https://huggingface.co/papers/2602.04289', 'abstract': 'Proxy compression trains language models on both raw byte sequences and compressed views, enabling efficient training with end-to-end raw-byte inference while maintaining model robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.', 'score': 1, 'issue_id': 919, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '8eb67e4751e7216d', 'authors': ['Lin Zheng', 'Xinyu Li', 'Qian Liu', 'Xiachong Feng', 'Lingpeng Kong'], 'affiliations': ['TikTok', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2602.04289.jpg', 'data': {'categories': ['#training', '#architecture'], 'emoji': 'üóúÔ∏è', 'ru': {'title': '–ü—Ä—è–º–∞—è —Ä–∞–±–æ—Ç–∞ —Å –±–∞–π—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å–∂–∞—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø—Ä–æ–∫—Å–∏-–∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å—ã—Ä—ã–µ –±–∞–π—Ç–æ–≤—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–∂–∞—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö. –í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–µ–∂–¥—É —Å–æ–±–æ–π –æ–±–∞ —Ñ–æ—Ä–º–∞—Ç–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ —Å–∂–∞—Ç—ã—Ö –≤—Ö–æ–¥–∞—Ö, –∞ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Å –±–∞–π—Ç–∞–º–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä—É—é –¥–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤, –Ω–æ –∏–∑–±–µ–≥–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–æ–∫—Å–∏-–∫–æ–º–ø—Ä–µ—Å—Å–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –±–∞–π—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –æ–±—ä–µ–º–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.'}, 'en': {'title': 'Proxy Compression: Efficient Training with Raw Bytes and Compressed Views', 'desc': 'This paper presents a new training method called proxy compression for language models. It allows models to learn from both raw byte sequences and their compressed versions, improving training efficiency. By aligning these two formats, the model can perform well even when it primarily trains on compressed data. The results show that proxy compression outperforms traditional methods, especially as the model size increases, while still using raw bytes for inference.'}, 'zh': {'title': '‰ª£ÁêÜÂéãÁº©ÔºöÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉÊïàÁéáÁöÑÂàõÊñ∞ÊñπÊ°à', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‰ª£ÁêÜÂéãÁº©ÁöÑËÆ≠ÁªÉÊñπÊ°àÔºåÊó®Âú®ÊèêÈ´òËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÂêåÊó∂‰ΩøÁî®ÂéüÂßãÂ≠óËäÇÂ∫èÂàóÂíåÂ§ñÈÉ®ÂéãÁº©ËßÜÂõæËøõË°åËÆ≠ÁªÉÔºå‰ΩøÊ®°ÂûãÂú®Êé®ÁêÜÊó∂ËÉΩÂ§üÈ´òÊïàÂ§ÑÁêÜÂéüÂßãÂ≠óËäÇ„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÊ®°ÂûãËÉΩÂ§üÂú®ÂéãÁº©Â∫èÂàóÂíåÂéüÂßãÂ≠óËäÇ‰πãÈó¥ÂÆûÁé∞ÂÜÖÈÉ®ÂØπÈΩêÔºå‰ªéËÄåÂ¢ûÂº∫‰∏§ËÄÖ‰πãÈó¥ÁöÑËøÅÁßªËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ª£ÁêÜÂéãÁº©Âú®‰ª£Á†ÅËØ≠Ë®ÄÂª∫Ê®°‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÊïàÁéáÔºåÂπ∂Âú®Âõ∫ÂÆöËÆ°ÁÆóÈ¢ÑÁÆó‰∏ãË∂ÖË∂ä‰∫ÜÁ∫ØÂ≠óËäÇÁ∫ßÂü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04271', 'title': 'SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization', 'url': 'https://huggingface.co/papers/2602.04271', 'abstract': '', 'score': 1, 'issue_id': 921, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '0f4ed884dd76bdbc', 'authors': ['Lifan Wu', 'Ruijie Zhu', 'Yubo Ai', 'Tianzhu Zhang'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.04271.jpg', 'data': {'categories': [], 'emoji': 'ü§ñ', 'ru': {'title': '–ù–æ–≤–∞—è —ç—Ä–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å LLM', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LLM, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–∞—Ö. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á—ë—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏—Ö —Å–ª–æ—ë–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.'}, 'en': {'title': 'Hybrid Deep Learning: Merging CNNs and RNNs for Superior Performance', 'desc': "This paper presents a novel approach to improve the performance of deep learning models by utilizing a hybrid architecture that combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs). The proposed method enhances feature extraction from spatial data while also capturing temporal dependencies, making it suitable for tasks like video analysis and time-series prediction. The authors demonstrate that their model outperforms existing state-of-the-art techniques on several benchmark datasets. Additionally, they provide insights into the model's interpretability and robustness against adversarial attacks."}, 'zh': {'title': 'ÊèêÂçáÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÁöÑÂàõÊñ∞ÁÆóÊ≥ï', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊú∫Âô®Â≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÊ®°ÂûãÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÊÄß„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÊîπËøõÁöÑÁâπÂæÅÈÄâÊã©ÊñπÊ≥ïÔºåÂèØ‰ª•ÊúâÊïàÂáèÂ∞ëÊï∞ÊçÆÁª¥Â∫¶ÔºåÂêåÊó∂‰øùÁïôÈáçË¶Å‰ø°ÊÅØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÁÆóÊ≥ïÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫é‰º†ÁªüÊñπÊ≥ï„ÄÇÈÄöËøá‰ºòÂåñÊ®°ÂûãÁöÑËÆ≠ÁªÉËøáÁ®ãÔºåÁ†îÁ©∂ËÄÖÂ∏åÊúõÊé®Âä®Êú∫Âô®Â≠¶‰π†Âú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02863', 'title': '"I May Not Have Articulated Myself Clearly": Diagnosing Dynamic Instability in LLM Reasoning at Inference Time', 'url': 'https://huggingface.co/papers/2602.02863', 'abstract': 'Analysis of reasoning failures in large language models reveals that instability signals derived from token log probabilities and entropy can predict incorrect answers and distinguish between corrective and destructive instability based on timing of distribution shifts.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning failures in large language models (LLMs) are typically measured only at the end of a generation, yet many failures manifest as a process-level breakdown: the model "loses the thread" mid-reasoning. We study whether such breakdowns are detectable from inference-time observables available in standard APIs (token log probabilities), without any training or fine-tuning. We define a simple instability signal that combines consecutive-step distributional shift (JSD) and uncertainty (entropy), summarize each trace by its peak instability strength, and show that this signal reliably predicts failure. Across GSM8K and HotpotQA, instability strength predicts wrong answers with above-chance AUC and yields monotonic bucket-level accuracy decline at scale across model sizes. Crucially, we show that instability is not uniformly harmful: early instability can reflect subsequent stabilization and a correct final answer (corrective instability), whereas late instability is more often followed by failure (destructive instability), even at comparable peak magnitudes, indicating that recoverability depends not only on how strongly the distribution changes but also on when such changes occur relative to the remaining decoding horizon. The method is model-agnostic, training-free, and reproducible, and is presented as a diagnostic lens rather than a corrective or control mechanism.', 'score': 1, 'issue_id': 925, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'ac79778620477e44', 'authors': ['Jinkun Chen', 'Fengxiang Cheng', 'Sijia Han', 'Vlado Keselj'], 'affiliations': ['Dalhousie University', 'Meta', 'Tsinghua University', 'University of Amsterdam'], 'pdf_title_img': 'assets/pdf/title_img/2602.02863.jpg', 'data': {'categories': ['#inference'], 'emoji': 'üßµ', 'ru': {'title': '–ö–æ–≥–¥–∞ —Ç–µ—Ä—è–µ—Ç—Å—è –Ω–∏—Ç—å: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è, –∫–∞–∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –æ—à–∏–±–∫–∏ –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –ø—Ä–æ—Å—Ç–æ–π —Å–∏–≥–Ω–∞–ª –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –ô–µ–Ω—Å–µ–Ω–∞-–®–µ–Ω–Ω–æ–Ω–∞ –∏ —ç–Ω—Ç—Ä–æ–ø–∏–∏ –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º–∏ —à–∞–≥–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–¥–µ–∂–Ω–æ –≤—ã—è–≤–ª—è–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –ö–ª—é—á–µ–≤–æ–µ –æ—Ç–∫—Ä—ã—Ç–∏–µ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å) —á–∞—Å—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É, –∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –ø–æ–∑–¥–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö (–¥–µ—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å) –æ–±—ã—á–Ω–æ –æ–∑–Ω–∞—á–∞–µ—Ç –æ—à–∏–±–∫—É. –ú–µ—Ç–æ–¥ –ø–æ–ª–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª—å-–∞–≥–Ω–æ—Å—Ç–∏—á–µ–Ω –∏ –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ API –±–µ–∑ –∫–∞–∫–æ–≥–æ-–ª–∏–±–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Predicting Failures in Language Models Through Instability Signals', 'desc': 'This paper investigates how reasoning failures in large language models (LLMs) can be detected during the generation process rather than just at the end. It introduces a method that uses token log probabilities and entropy to create instability signals, which can predict when a model is likely to produce incorrect answers. The study finds that the timing of distribution shifts is crucial, as early instability can lead to correct answers while late instability often results in failure. This approach is model-agnostic and does not require any training, making it a useful diagnostic tool for understanding LLM behavior.'}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÂ§±Ë¥•ÁöÑÁ®≥ÂÆöÊÄß‰ø°Âè∑', 'desc': 'Êú¨ÊñáÂàÜÊûê‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÊé®ÁêÜÂ§±Ë¥•ÔºåÂèëÁé∞ÈÄöËøá‰ª§ÁâåÊó•ÂøóÊ¶ÇÁéáÂíåÁÜµÁöÑÁ®≥ÂÆöÊÄß‰ø°Âè∑ÂèØ‰ª•È¢ÑÊµãÈîôËØØÁ≠îÊ°à„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊé®ÁêÜËøáÁ®ã‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂèØ‰ª•ÈÄöËøáÁÆÄÂçïÁöÑ‰ø°Âè∑Êù•Ê£ÄÊµãÔºåËøô‰∏™‰ø°Âè∑ÁªìÂêà‰∫ÜËøûÁª≠Ê≠•È™§ÁöÑÂàÜÂ∏ÉÂèòÂåñÂíå‰∏çÁ°ÆÂÆöÊÄß„ÄÇÊàë‰ª¨ÂèëÁé∞Ôºå‰∏çÂêåÊó∂Èó¥ÁÇπÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂØπÊ®°ÂûãÁöÑÊúÄÁªàÁ≠îÊ°àÊúâ‰∏çÂêåÁöÑÂΩ±ÂìçÔºåÊó©ÊúüÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂèØËÉΩÂØºËá¥Ê≠£Á°ÆÁ≠îÊ°àÔºåËÄåÊôöÊúüÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂàôÊõ¥ÂèØËÉΩÂØºËá¥Â§±Ë¥•„ÄÇËØ•ÊñπÊ≥ï‰∏ç‰æùËµñ‰∫éÁâπÂÆöÊ®°ÂûãÔºå‰∏îÊó†ÈúÄËÆ≠ÁªÉÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØäÊñ≠ËßÜËßí„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02495', 'title': 'Reward-free Alignment for Conflicting Objectives', 'url': 'https://huggingface.co/papers/2602.02495', 'abstract': 'A reward-free alignment framework addresses multi-objective conflicts in language models through conflict-averse gradient descent with clipping, improving Pareto trade-offs across diverse model architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.', 'score': 1, 'issue_id': 923, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '635e37fa950f4745', 'authors': ['Peter Chen', 'Xiaopeng Li', 'Xi Chen', 'Tianyi Lin'], 'affiliations': ['CUHK SZ', 'Columbia University', 'NYU Stern'], 'pdf_title_img': 'assets/pdf/title_img/2602.02495.jpg', 'data': {'categories': ['#training', '#alignment', '#optimization', '#rlhf'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–µ–∑ –Ω–∞–≥—Ä–∞–¥: —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Ü–µ–ª–µ–π –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –æ—Ç—Å–µ—á–µ–Ω–∏–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç RACO ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —è–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞–≥—Ä–∞–¥—ã, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑—Ä–µ—à–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–º–∏ —Ü–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ —Å –æ—Ç—Å–µ—á–µ–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ —Å –ø–∞—Ä–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∫ –ü–∞—Ä–µ—Ç–æ-–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ç–æ—á–∫–∞–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –≤–µ—Å–∞–º —Ü–µ–ª–µ–π, —É–∫–∞–∑–∞–Ω–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ—Ç—Å–µ—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–≤—É—Ö —Ü–µ–ª–µ–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π Qwen, Llama –∏ Gemma –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤ –º–µ–∂–¥—É –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–º–∏ –æ–±—ä–µ–∫—Ç–∏–≤–∞–º–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Aligning Language Models Without Rewards: A New Approach to Conflicting Objectives', 'desc': 'This paper introduces a new framework called Reward-free Alignment for Conflicted Objectives (RACO) to tackle the challenges of aligning large language models (LLMs) with multiple conflicting goals. It uses a special technique called conflict-averse gradient descent with clipping to resolve issues that arise when trying to improve several objectives at once. The authors demonstrate that their method can lead to better trade-offs between these objectives without relying on complex reward models. Experiments show that RACO outperforms existing methods in achieving optimal performance across various LLM architectures.'}, 'zh': {'title': 'Êó†Â•ñÂä±ÂØπÈΩêÔºöËß£ÂÜ≥ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öÁõÆÊ†áÂÜ≤Á™Å', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†Â•ñÂä±ÂØπÈΩêÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂ§öÁõÆÊ†áÂÜ≤Á™ÅÈóÆÈ¢ò„ÄÇÈÄöËøá‰ΩøÁî®ÂÜ≤Á™ÅËßÑÈÅøÁöÑÊ¢ØÂ∫¶‰∏ãÈôçÊñπÊ≥ïÂπ∂ËøõË°åË£ÅÂâ™ÔºåÊòæËëóÊîπÂñÑ‰∫Ü‰∏çÂêåÊ®°ÂûãÊû∂ÊûÑ‰πãÈó¥ÁöÑÂ∏ïÁ¥ØÊâòÊùÉË°°„ÄÇËØ•ÊñπÊ≥ïÁõ¥Êé•Âà©Áî®ÊàêÂØπÂÅèÂ•ΩÊï∞ÊçÆÔºåÈÅøÂÖç‰∫Ü‰º†ÁªüÂä†ÊùÉÊçüÂ§±ÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®Â§öÁõÆÊ†áÊëòË¶ÅÂíåÂÆâÂÖ®ÂØπÈΩê‰ªªÂä°‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂ§öÁõÆÊ†áÂØπÈΩêÂü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02341', 'title': 'LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization', 'url': 'https://huggingface.co/papers/2602.02341', 'abstract': "LongVPO is a two-stage Direct Preference Optimization framework that enables short-context vision-language models to understand ultra-long videos through synthetic preference triples and recursive captioning, achieving state-of-the-art performance with minimal human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.", 'score': 1, 'issue_id': 917, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '4e672120bde8e3da', 'authors': ['Zhenpeng Huang', 'Jiaqi Li', 'Zihan Jia', 'Xinhao Li', 'Desen Meng', 'Lingxue Song', 'Xi Chen', 'Liang Li', 'Limin Wang'], 'affiliations': ['JIUTIAN Research', 'Shanghai AI Laboratory', 'State Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02341.jpg', 'data': {'categories': ['#open_source', '#optimization', '#video', '#synthetic', '#benchmark', '#rlhf', '#multimodal', '#long_context'], 'emoji': 'üé¨', 'ru': {'title': '–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–≤–µ—Ä—Ö–¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ', 'desc': 'LongVPO ‚Äî —ç—Ç–æ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –∑—Ä–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–æ–Ω–∏–º–∞—Ç—å –≤–∏–¥–µ–æ —É–ª—å—Ç—Ä–∞–¥–ª–∏–Ω–Ω–æ–π –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É—é—Ç—Å—è —Ç—Ä–æ–π–∫–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø—É—Ç—ë–º —è–∫–æ—Ä–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∫ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º –≤–∏–¥–µ–æ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –º–Ω–æ–≥–æ—Å–µ–≥–º–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —ç—Ç–∞–ª–æ–Ω–∞—Ö –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ 16 —Ç—ã—Å—è—á —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –±–µ–∑ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π.'}, 'en': {'title': 'Efficient Long-Video Understanding with Minimal Annotations', 'desc': 'LongVPO is a two-stage framework designed to enhance short-context vision-language models for understanding ultra-long videos without requiring extensive human annotations. In the first stage, it generates synthetic preference triples by linking questions to short video clips and filtering out irrelevant information to ensure clear guidance for the model. The second stage involves a recursive captioning process that creates detailed metadata for long videos, allowing the model to perform complex reasoning tasks using a large language model. This approach achieves state-of-the-art results on long-video benchmarks with minimal synthetic examples, demonstrating an efficient method for video comprehension.'}, 'zh': {'title': 'È´òÊïàÁêÜËß£Ë∂ÖÈïøËßÜÈ¢ëÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'LongVPOÊòØ‰∏ÄÁßç‰∏§Èò∂ÊÆµÁöÑÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñÊ°ÜÊû∂ÔºåÊó®Âú®Â∏ÆÂä©Áü≠Êó∂‰∏ä‰∏ãÊñáÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÁêÜËß£Ë∂ÖÈïøËßÜÈ¢ëÔºåËÄåÊó†ÈúÄÈïøËßÜÈ¢ëÊ≥®Èáä„ÄÇÂú®Á¨¨‰∏ÄÈò∂ÊÆµÔºåÊàë‰ª¨ÈÄöËøáÂ∞ÜÈóÆÈ¢òÈîöÂÆöÂà∞Âçï‰∏™Áü≠ÁâáÊÆµÔºåÂêàÊàêÂÅèÂ•Ω‰∏âÂÖÉÁªÑÔºåÂπ∂‰ΩøÁî®ËßÜËßâÁõ∏‰ººÊÄßÂíåÈóÆÈ¢òÁâπÂºÇÊÄßËøáÊª§Êù•ÂáèÂ∞ë‰ΩçÁΩÆÂÅèÂ∑Æ„ÄÇÁ¨¨‰∫åÈò∂ÊÆµ‰∏≠ÔºåÊàë‰ª¨Âú®ÈïøËßÜÈ¢ë‰∏äÈááÁî®ÈÄíÂΩíÂ≠óÂπïÁîüÊàêÂú∫ÊôØÁ∫ßÂÖÉÊï∞ÊçÆÔºåÂπ∂Âà©Áî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁîüÊàêÂ§öÊÆµÊé®ÁêÜÊü•ËØ¢Ôºå‰ªéËÄåÂØπÊ®°ÂûãÁöÑÂÅèÂ•ΩËøõË°åÂØπÈΩê„ÄÇLongVPO‰ªÖ‰ΩøÁî®16KÂêàÊàêÁ§∫‰æãÔºå‰∏îÊó†ÈúÄÊòÇË¥µÁöÑ‰∫∫Á±ªÊ†áÁ≠æÔºå‰æøÂú®Â§ö‰∏™ÈïøËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22596', 'title': 'FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data', 'url': 'https://huggingface.co/papers/2601.22596', 'abstract': 'A large-scale building change detection dataset named FOTBCD is introduced, covering 28 French departments with high-resolution imagery and comprehensive annotations for both binary and instance-level change detection tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.', 'score': 1, 'issue_id': 923, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'fb49687b96207f19', 'authors': ['Abdelrrahman Moubane'], 'affiliations': ['Retgen AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.22596.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#cv', '#dataset'], 'emoji': 'üèóÔ∏è', 'ru': {'title': '–ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –≤ –∑–∞–¥–∞—á–µ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –∑–¥–∞–Ω–∏–π', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç FOTBCD –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –∑–¥–∞–Ω–∏–π, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π 28 —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏—Ö –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–º–∏ –æ—Ä—Ç–æ—Ñ–æ—Ç–æ—Å–Ω–∏–º–∫–∞–º–∏. –î–∞—Ç–∞—Å–µ—Ç –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–µ –≤–µ—Ä—Å–∏–∏: –±–∏–Ω–∞—Ä–Ω—É—é —Å –ø–∏–∫—Å–µ–ª—å–Ω—ã–º–∏ –º–∞—Å–∫–∞–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π (~28,000 –ø–∞—Ä —Å–Ω–∏–º–∫–æ–≤) –∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–Ω—É—é —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–¥–∞–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –±–µ–Ω—á–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è, —á—Ç–æ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞–Ω–Ω—ã—Ö —É–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –ø–µ—Ä–µ–Ω–æ—Å–µ –º–µ–∂–¥—É –¥–æ–º–µ–Ω–∞–º–∏. –î–∞—Ç–∞—Å–µ—Ç –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —É—Å–ª–æ–≤–∏—è—Ö –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ —Å–¥–≤–∏–≥–∞.'}, 'en': {'title': 'FOTBCD: Enhancing Building Change Detection with Geographic Diversity', 'desc': 'The paper presents FOTBCD, a comprehensive dataset for building change detection that includes high-resolution imagery and detailed annotations across 28 departments in France. It supports both binary and instance-level change detection tasks, making it versatile for various machine learning applications. The dataset is designed to facilitate large-scale benchmarking and is particularly valuable for evaluating models under geographic domain shifts. Empirical results demonstrate that the geographic diversity of FOTBCD enhances the generalization capabilities of change detection algorithms compared to existing datasets.'}, 'zh': {'title': 'FOTBCDÔºöÊé®Âä®Âª∫Á≠ëÂèòÂåñÊ£ÄÊµãÁöÑÂú∞ÁêÜÂ§öÊ†∑ÊÄß', 'desc': 'FOTBCDÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÂª∫Á≠ëÂèòÂåñÊ£ÄÊµãÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∫ÜÊ≥ïÂõΩ28‰∏™ÁúÅ‰ªΩÔºåÊèê‰æõÈ´òÂàÜËæ®ÁéáÂõæÂÉèÂíåÂÖ®Èù¢ÁöÑÊ≥®Èáä„ÄÇËØ•Êï∞ÊçÆÈõÜÊîØÊåÅ‰∫åÂÖÉÂíåÂÆû‰æãÁ∫ßÂèòÂåñÊ£ÄÊµã‰ªªÂä°ÔºåÂåÖÂê´Á∫¶28,000ÂØπÂâçÂêéÂõæÂÉèÂèäÂÖ∂ÂÉèÁ¥†Á∫ßÂèòÂåñÊé©ËÜú„ÄÇFOTBCDÁöÑËÆæËÆ°Êó®Âú®ËøõË°åÂ§ßËßÑÊ®°Âü∫ÂáÜÊµãËØïÔºåÂπ∂Âú®Âú∞ÁêÜÈ¢ÜÂüüËΩ¨Áßª‰∏ãËøõË°åËØÑ‰º∞ÔºåÁ°Æ‰øùÊ†áÁ≠æË¥®Èáè„ÄÇÈÄöËøá‰∏éÂÖ∂‰ªñÊï∞ÊçÆÈõÜÁöÑÊØîËæÉÔºåFOTBCDÂ±ïÁ§∫‰∫ÜÂú∞ÁêÜÂ§öÊ†∑ÊÄß‰∏éÂª∫Á≠ëÂèòÂåñÊ£ÄÊµãÁöÑË∑®ÂüüÊ≥õÂåñËÉΩÂäõ‰πãÈó¥ÁöÑÂÖ≥ËÅî„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03955', 'title': 'AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent', 'url': 'https://huggingface.co/papers/2602.03955', 'abstract': 'AgentArk distills multi-agent reasoning dynamics into a single model through hierarchical distillation strategies, enabling efficient yet powerful reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.', 'score': 0, 'issue_id': 930, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'ce617a98a37f9013', 'authors': ['Yinyi Luo', 'Yiqiao Jin', 'Weichen Yu', 'Mengqi Zhang', 'Srijan Kumar', 'Xiaoxiao Li', 'Weijie Xu', 'Xin Chen', 'Jindong Wang'], 'affiliations': ['Amazon', 'Carnegie Mellon University', 'Georgia Institute of Technology', 'University of British Columbia', 'William & Mary'], 'pdf_title_img': 'assets/pdf/title_img/2602.03955.jpg', 'data': {'categories': [], 'emoji': 'üß†', 'ru': {'title': '–£–∫—Ä–æ—â–µ–Ω–∏–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–∞ –≤ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏', 'desc': 'AgentArk ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–∏–Ω–∞–º–∏–∫–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤ –µ–¥–∏–Ω—É—é –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç—Ä–∏ –ø–æ–¥—Ö–æ–¥–∞: —É–ª—É—á—à–µ–Ω–Ω–æ–µ fine-tuning —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏, –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é —Å —É—á—ë—Ç–æ–º –ø—Ä–æ—Ü–µ—Å—Å–∞. –î–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –Ω–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏, –ø—Ä–∏—Å—É—â–∏–µ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã–º —Å–∏—Å—Ç–µ–º–∞–º. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –Ω–∞–≥—Ä—É–∑–∫—É —Å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —É–ª—É—á—à–µ–Ω–Ω—É—é —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Efficient Multi-Agent Reasoning in a Single Model', 'desc': "AgentArk introduces a method to simplify multi-agent reasoning into a single model using hierarchical distillation techniques. This approach allows the model to maintain the reasoning power of multiple agents while being computationally efficient. By focusing on training rather than inference, the model achieves strong reasoning abilities and self-correction similar to that of multi-agent systems. The paper explores various strategies to enhance the model's robustness and generalization across different reasoning tasks."}, 'zh': {'title': 'È´òÊïàÊô∫ËÉΩ‰ΩìÔºåÂº∫Â§ßÊé®ÁêÜËÉΩÂäõ', 'desc': 'AgentArkÈÄöËøáÂ±ÇÊ¨°Ëí∏È¶èÁ≠ñÁï•Â∞ÜÂ§öÊô∫ËÉΩ‰ΩìÊé®ÁêÜÂä®ÊÄÅÊèêÁÇº‰∏∫Âçï‰∏ÄÊ®°ÂûãÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàËÄåÂº∫Â§ßÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑÊô∫ËÉΩËΩ¨Âåñ‰∏∫Âçï‰∏™Êô∫ËÉΩ‰ΩìÁöÑËÉΩÂäõÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨ÂíåÈîôËØØ‰º†Êí≠„ÄÇÁ†îÁ©∂‰∏≠Êé¢ËÆ®‰∫Ü‰∏âÁßçÂ±ÇÊ¨°Ëí∏È¶èÁ≠ñÁï•ÔºåÂåÖÊã¨Â¢ûÂº∫Êé®ÁêÜÁöÑÂæÆË∞É„ÄÅÂü∫‰∫éËΩ®ËøπÁöÑÂ¢ûÂº∫ÂíåËøáÁ®ãÊÑüÁü•Ëí∏È¶è„ÄÇÊúÄÁªàÔºåËí∏È¶èÊ®°ÂûãÂú®‰øùÊåÅÂçï‰∏™Êô∫ËÉΩ‰ΩìÊïàÁéáÁöÑÂêåÊó∂ÔºåÂ±ïÁé∞Âá∫Â§öÊô∫ËÉΩ‰ΩìÁöÑÂº∫Êé®ÁêÜÂíåËá™Êàë‰øÆÊ≠£ÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01031', 'title': 'HalluHard: A Hard Multi-Turn Hallucination Benchmark', 'url': 'https://huggingface.co/papers/2602.01031', 'abstract': 'Large language models continue to generate plausible but ungrounded factual claims in multi-turn dialogue, with hallucinations remaining significant even when utilizing web search for verification across high-stakes domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce HalluHard, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search (approx 30% for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.', 'score': 0, 'issue_id': 930, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': 'bf75b93352357023', 'authors': ['Dongyang Fan', 'Sebastien Delsad', 'Nicolas Flammarion', 'Maksym Andriushchenko'], 'affiliations': ['ELLIS Institute Tubingen', 'EPFL', 'Max Planck Institute for Intelligent Systems', 'Tubingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2602.01031.jpg', 'data': {'categories': [], 'emoji': 'üö®', 'ru': {'title': '–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏: –∫–∞–∫ —É–ª—É—á—à–∏—Ç—å –≤–µ—Ä–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': '–ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç –ø—Ä–æ–±–ª–µ–º—É –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, —Ç–æ –µ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã—Ö, –Ω–æ –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –≤ –º–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–∞—Ö. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ HalluHard —Å 950 –≤–æ–ø—Ä–æ—Å–∞–º–∏ –≤ —á–µ—Ç—ã—Ä—ë—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö (–ø—Ä–∞–≤–æ, –º–µ–¥–∏—Ü–∏–Ω–∞, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ) –∏ —Ç—Ä–µ–±—É—é—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π –¥–ª—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ —Å—É–¥–µ–π—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—â–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –≤ 30% —Å–ª—É—á–∞–µ–≤, –∏ —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏, –Ω–æ–º–µ—Ä–∞ —Ö–æ–¥–∞ –¥–∏–∞–ª–æ–≥–∞ –∏ —Ç–∏–ø–∞ —Ç—Ä–µ–±—É–µ–º—ã—Ö –∑–Ω–∞–Ω–∏–π.'}, 'en': {'title': 'Tackling Hallucinations in Multi-Turn Dialogue with HalluHard', 'desc': 'This paper addresses the issue of hallucinations in large language models (LLMs), where they generate plausible but factually incorrect statements during multi-turn dialogues. The authors introduce HalluHard, a benchmark designed to evaluate hallucination in high-stakes domains like law, medicine, and coding, requiring models to provide inline citations for their claims. They propose a judging pipeline that uses web search to verify the accuracy of these citations, revealing that even the best models still produce significant hallucinations. The study finds that factors such as model capacity and the context of the dialogue influence the frequency and nature of these hallucinations.'}, 'zh': {'title': 'Â∫îÂØπÂ§öËΩÆÂØπËØù‰∏≠ÁöÑÂπªËßâÈóÆÈ¢ò', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Â§öËΩÆÂØπËØù‰∏≠‰ªçÁÑ∂‰ºöÁîüÊàêÁúã‰ººÂêàÁêÜ‰ΩÜÁº∫‰πè‰æùÊçÆÁöÑ‰∫ãÂÆûÂ£∞ÊòéÔºåÂ∞§ÂÖ∂ÊòØÂú®‰∏ä‰∏ãÊñáÂ¢ûÂä†Êó∂ÔºåÊó©ÊúüÈîôËØØ‰ºöÂä†Ââß„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜHalluHardÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÂ§öËΩÆÂπªËßâÂü∫ÂáÜÔºåÊ∂µÁõñÊ≥ïÂæãÊ°à‰æã„ÄÅÁ†îÁ©∂ÈóÆÈ¢ò„ÄÅÂåªÁñóÊåáÂçóÂíåÁºñÁ†ÅÁ≠âÂõõ‰∏™È´òÈ£éÈô©È¢ÜÂüüÔºåÂÖ±Êúâ950‰∏™ÁßçÂ≠êÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÈÄöËøáË¶ÅÊ±ÇÂØπ‰∫ãÂÆûÂ£∞ÊòéËøõË°åÂÜÖËÅîÂºïÁî®Êù•ÂÆûÁé∞‰∫ãÂÆû‰æùÊçÆÁöÑÊìç‰ΩúÂåñ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰Ωø‰ΩøÁî®ÁΩëÁªúÊêúÁ¥¢ÔºåÂπªËßâÁé∞Ë±°‰ªçÁÑ∂ÊòæËëóÔºå‰∏îÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÅËΩÆÊ¨°‰ΩçÁΩÆ„ÄÅÊúâÊïàÊé®ÁêÜÂíåÊâÄÈúÄÁü•ËØÜÁ±ªÂûãÈÉΩ‰ºöÂΩ±ÂìçÂπªËßâË°å‰∏∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05386', 'title': 'Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening', 'url': 'https://huggingface.co/papers/2602.05386', 'abstract': 'Spider-Sense framework provides intrinsic and selective agent security through event-driven defense with intrinsic risk sensing, achieving low attack success and false positive rates with minimal latency overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S^2Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.', 'score': 56, 'issue_id': 937, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '0ea84e3c8d1c62d4', 'authors': ['Zhenxiong Yu', 'Zhi Yang', 'Zhiheng Jin', 'Shuhe Wang', 'Heng Zhang', 'Yanlin Fei', 'Lingfeng Zeng', 'Fangqi Lou', 'Shuo Zhang', 'Tu Hu', 'Jingping Liu', 'Rongze Chen', 'Xingyu Zhu', 'Kunyi Wang', 'Chaofa Yuan', 'Xin Guo', 'Zhaowei Liu', 'Feipeng Zhang', 'Jie Huang', 'Huacan Wang', 'Ronghao Chen', 'Liwen Zhang'], 'affiliations': ['CMU', 'NUS', 'QuantaAlpha', 'SUFE', 'SYSU', 'USTC', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2602.05386.jpg', 'data': {'categories': [], 'emoji': 'üï∑Ô∏è', 'ru': {'title': '–ê–≥–µ–Ω—Ç—ã —Å –≤—Ä–æ–∂–¥–µ–Ω–Ω—ã–º –∏–Ω—Å—Ç–∏–Ω–∫—Ç–æ–º —Å–∞–º–æ–∑–∞—â–∏—Ç—ã', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Spider-Sense –¥–ª—è –∑–∞—â–∏—Ç—ã –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç –∞—Ç–∞–∫. –í–º–µ—Å—Ç–æ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ø–æ–¥—Ö–æ–¥ —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —á—É–≤—Å—Ç–≤–æ–≤–∞–Ω–∏–µ–º —Ä–∏—Å–∫–æ–≤ (Intrinsic Risk Sensing), –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∞–≥–µ–Ω—Ç–∞–º –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –∑–∞—â–∏—Ç—É –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ —É–≥—Ä–æ–∑. –ó–∞—â–∏—Ç–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥: –ø—Ä–æ—Å—Ç—ã–µ —Å–ª—É—á–∞–∏ —Ä–µ—à–∞—é—Ç—Å—è –±—ã—Å—Ç—Ä—ã–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏, —Å–ª–æ–∂–Ω—ã–µ ‚Äî –≥–ª—É–±–æ–∫–∏–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º –±–µ–∑ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –≤–Ω–µ—à–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π. –ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ S¬≤Bench –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∑–∞—â–∏—Ç—É –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–º–∏ –∞—Ç–∞–∫–∞–º–∏, –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ Spider-Sense –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —É—Å–ø–µ—à–Ω—ã—Ö –∞—Ç–∞–∫ –∏ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏.'}, 'en': {'title': 'Spider-Sense: Smart Security for Autonomous Agents', 'desc': 'The Spider-Sense framework enhances the security of autonomous agents by implementing an event-driven defense mechanism that relies on Intrinsic Risk Sensing (IRS). This approach allows agents to detect risks and activate defenses only when necessary, rather than following a fixed security protocol. By utilizing a hierarchical defense strategy, Spider-Sense efficiently handles known threats through lightweight similarity matching while addressing uncertain situations with deeper reasoning. The framework has been rigorously tested using the S^2Bench benchmark, demonstrating superior performance in minimizing attack success and false positive rates with minimal latency.'}, 'zh': {'title': 'Spider-SenseÔºöÂÜÖÂú®ÈÄâÊã©ÊÄßÁöÑ‰ª£ÁêÜÂÆâÂÖ®Ê°ÜÊû∂', 'desc': 'Spider-SenseÊ°ÜÊû∂ÈÄöËøá‰∫ã‰ª∂È©±Âä®ÁöÑÈò≤Âæ°Êú∫Âà∂Êèê‰æõÂÜÖÂú®ÂíåÈÄâÊã©ÊÄßÁöÑ‰ª£ÁêÜÂÆâÂÖ®ÔºåÂà©Áî®ÂÜÖÂú®È£éÈô©ÊÑüÁü•ÂÆûÁé∞‰ΩéÊîªÂáªÊàêÂäüÁéáÂíå‰ΩéËØØÊä•ÁéáÔºåÂêåÊó∂‰øùÊåÅÊúÄÂ∞èÁöÑÂª∂ËøüÂºÄÈîÄ„ÄÇÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊºîÂèò‰∏∫Ëá™‰∏ª‰ª£ÁêÜÔºåÂÖ∂Âú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÂ∫îÁî®ÊòæËëóÂ¢ûÂä†Ôºå‰ΩÜ‰πüÂ∏¶Êù•‰∫ÜÊñ∞ÁöÑÂÆâÂÖ®ÊåëÊàò„ÄÇÁé∞ÊúâÁöÑ‰ª£ÁêÜÈò≤Âæ°Êú∫Âà∂ÈÄöÂ∏∏ÈááÁî®Âº∫Âà∂Ê£ÄÊü•ÁöÑÊñπÂºèÔºåËÄåÊàë‰ª¨ËÆ§‰∏∫ÊúâÊïàÁöÑ‰ª£ÁêÜÂÆâÂÖ®Â∫îÂΩìÊòØÂÜÖÂú®ÁöÑÂíåÈÄâÊã©ÊÄßÁöÑ„ÄÇSpider-SenseÊ°ÜÊû∂ÈÄöËøáËΩªÈáèÁ∫ßÁõ∏‰ººÊÄßÂåπÈÖçÂíåÊ∑±Â±ÇÂÜÖÈÉ®Êé®ÁêÜÁöÑÂ±ÇÊ¨°ÂåñÈò≤Âæ°Êú∫Âà∂ÔºåËÉΩÂ§üÂú®ÊÑüÁü•Âà∞È£éÈô©Êó∂Ëß¶ÂèëÈò≤Âæ°Ôºå‰ªéËÄåÊèêÈ´òÂÆâÂÖ®ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05261', 'title': 'Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR', 'url': 'https://huggingface.co/papers/2602.05261', 'abstract': "Research analyzes RLVR algorithms' impact on response length in LLMs and VLMs, proposing LUSPO to eliminate length bias and improve reasoning performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.", 'score': 43, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '2ce84c035d817ab6', 'authors': ['Fanfan Liu', 'Youyang Yin', 'Peng Shi', 'Siqi Yang', 'Zhixiong Zeng', 'Haibo Qiu'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2602.05261.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#rlhf', '#training', '#multimodal'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —Å–º–µ—â–µ–Ω–∏—è –ø–æ –¥–ª–∏–Ω–µ –æ—Ç–≤–µ—Ç–∞ –≤ —É—Å–∏–ª–µ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç, –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ (RLVR) –≤–ª–∏—è—é—Ç –Ω–∞ –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤, –≤–ª–∏—è—é—â–∏—Ö –Ω–∞ –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—é—Ç, —á—Ç–æ —Ä–∞–∑–Ω—ã–µ RLVR –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª–∏–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º LUSPO –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Å–º–µ—â–µ–Ω–∏–µ, —Å–≤—è–∑–∞–Ω–Ω–æ–µ —Å –¥–ª–∏–Ω–æ–π –æ—Ç–≤–µ—Ç–∞ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –º–µ—Ç–æ–¥–µ GSPO, –ø—É—Ç—ë–º —Å–æ–∑–¥–∞–Ω–∏—è –Ω–µ—Å–º–µ—â—ë–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LUSPO –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Eliminating Length Bias for Enhanced Reasoning in AI Models', 'desc': 'This paper investigates how Reinforcement Learning with Verifiable Rewards (RLVR) affects the length of responses generated by Large Language Models (LLMs) and Vision-Language Models (VLMs). It identifies that different RLVR algorithms lead to varying patterns in response length during training, which can impact reasoning performance. To address the length bias observed in existing algorithms, the authors propose a new algorithm called Length-Unbiased Sequence Policy Optimization (LUSPO). Through extensive experiments, LUSPO is shown to outperform traditional methods, providing a more effective approach to optimizing reasoning capabilities without being influenced by response length.'}, 'zh': {'title': 'Ê∂àÈô§ÈïøÂ∫¶ÂÅèÂ∑ÆÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'Êú¨Á†îÁ©∂ÂàÜÊûê‰∫ÜÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâÁÆóÊ≥ïÂØπÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂíåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂìçÂ∫îÈïøÂ∫¶ÁöÑÂΩ±ÂìçÔºåÂπ∂ÊèêÂá∫‰∫ÜÈïøÂ∫¶Êó†ÂÅèÂ∫èÂàóÁ≠ñÁï•‰ºòÂåñÔºàLUSPOÔºâÁÆóÊ≥ïÔºå‰ª•Ê∂àÈô§ÈïøÂ∫¶ÂÅèÂ∑ÆÂπ∂ÊèêÈ´òÊé®ÁêÜÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®RLVRËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÂìçÂ∫îÈïøÂ∫¶ÁöÑÂèòÂåñÊ®°ÂºèÂú®‰∏çÂêåÁöÑRLVRÁÆóÊ≥ï‰∏≠Â≠òÂú®ÊòæËëóÂ∑ÆÂºÇ„ÄÇÈÄöËøáÊ∑±ÂÖ•ÂàÜÊûê‰∏ªÊµÅRLVRÁÆóÊ≥ïÁöÑÁªÑÊàêÈÉ®ÂàÜÔºåÊú¨ÊñáÊèê‰æõ‰∫ÜÂΩ±ÂìçÂìçÂ∫îÈïøÂ∫¶ÁöÑÁêÜËÆ∫Ëß£ÈáäÔºåÂπ∂ÈÄöËøáÂ§ßÈáèÂÆûÈ™åÈ™åËØÅ‰∫ÜËøô‰∏ÄÁêÜËÆ∫„ÄÇLUSPOÁÆóÊ≥ïÂú®Êï∞Â≠¶Êé®ÁêÜÂü∫ÂáÜÂíåÂ§öÊ®°ÊÄÅÊé®ÁêÜÂú∫ÊôØ‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂‰Ωú‰∏∫‰∏ÄÁßçÊñ∞È¢ñÁöÑ‰ºòÂåñÁ≠ñÁï•ÁöÑ‰ºòÂäø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22027', 'title': 'CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty', 'url': 'https://huggingface.co/papers/2601.22027', 'abstract': "Current LLM agent benchmarks fail to evaluate reliability in real-world scenarios with uncertain user inputs, prompting the creation of CAR-bench to test consistency, uncertainty management, and capability awareness in in-car assistant applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.", 'score': 34, 'issue_id': 938, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'f53fc6f480380014', 'authors': ['Johannes Kirmayr', 'Lukas Stappen', 'Elisabeth Andr√©'], 'affiliations': ['Augsburg University', 'BMW Group Research and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.22027.jpg', 'data': {'categories': ['#hallucinations', '#reasoning', '#benchmark', '#alignment', '#agents', '#audio'], 'emoji': 'üöó', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å—é', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω CAR-bench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã—Ö –ø–æ–º–æ—â–Ω–∏–∫–æ–≤. –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –∫ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏, —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å—é –∏ –æ—Å–æ–∑–Ω–∞–Ω–∏—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –Ω–µ–ø–æ–ª–Ω—ã–º–∏ –∏ –¥–≤—É—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –°—Ä–µ–¥–∞ –≤–∫–ª—é—á–∞–µ—Ç 58 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–º, –∞ —Ç–∞–∫–∂–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–µ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –≤—ã—à–µ 50% –∏ —á–∞—Å—Ç–æ –Ω–∞—Ä—É—à–∞—é—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –ª–æ–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —É–∫–∞–∑—ã–≤–∞—è –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã—Ö –∏ —Å–∞–º–æ—Å–æ–∑–Ω–∞—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Enhancing LLM Reliability in Real-World Scenarios with CAR-bench', 'desc': 'The paper introduces CAR-bench, a new benchmark designed to evaluate the performance of Large Language Model (LLM) agents in real-world scenarios, particularly in in-car assistant applications. Unlike existing benchmarks that focus on ideal task completion, CAR-bench assesses how well agents handle uncertainty, maintain consistency, and demonstrate awareness of their capabilities when faced with ambiguous user inputs. It includes unique tasks such as Hallucination and Disambiguation, which challenge agents to manage incomplete information and clarify user requests effectively. Initial results show that even advanced LLMs struggle with consistent performance, highlighting the need for improvements in reliability and self-awareness for practical applications.'}, 'zh': {'title': 'CAR-benchÔºöÊèêÂçáËΩ¶ËΩΩÂä©ÊâãÁöÑÂèØÈù†ÊÄß‰∏éËá™ÊàëÊÑèËØÜ', 'desc': 'ÂΩìÂâçÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂü∫ÂáÜÊµãËØïÊú™ËÉΩËØÑ‰º∞Âú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠Èù¢ÂØπ‰∏çÁ°ÆÂÆöÁî®Êà∑ËæìÂÖ•Êó∂ÁöÑÂèØÈù†ÊÄßÔºåÂõ†Ê≠§Êàë‰ª¨ÂàõÂª∫‰∫ÜCAR-benchÊù•ÊµãËØïËΩ¶ËΩΩÂä©ÊâãÂ∫îÁî®‰∏≠ÁöÑ‰∏ÄËá¥ÊÄß„ÄÅÁÆ°ÁêÜ‰∏çÁ°ÆÂÆöÊÄßÂíåËÉΩÂäõÊÑèËØÜ„ÄÇCAR-benchÊòØ‰∏Ä‰∏™‰∏ìÈó®Áî®‰∫éËØÑ‰º∞Â§öËΩÆÂØπËØù„ÄÅÂ∑•ÂÖ∑‰ΩøÁî®ÁöÑLLM‰ª£ÁêÜÁöÑÂü∫ÂáÜÔºåÁâπÂà´ÂÖ≥Ê≥®Âú®ËΩ¶ËΩΩÂä©ÊâãÈ¢ÜÂüüÁöÑË°®Áé∞„ÄÇËØ•Âü∫ÂáÜÁéØÂ¢ÉÊ®°Êãü‰∫ÜÁî®Êà∑„ÄÅÈ¢ÜÂüüÊîøÁ≠ñÂíå58‰∏™‰∫íËÅîÂ∑•ÂÖ∑ÔºåÊ∂µÁõñÂØºËà™„ÄÅÁîü‰∫ßÂäõ„ÄÅÂÖÖÁîµÂíåËΩ¶ËæÜÊéßÂà∂Á≠âÊñπÈù¢„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÁé∞ÊúâÊ®°ÂûãÂú®Â§ÑÁêÜ‰∏çÁ°ÆÂÆöÊÄßÂíåÈÅµÂæ™ÊîøÁ≠ñÊñπÈù¢Â≠òÂú®ÊòæËëóÂ∑ÆË∑ùÔºåÂº∫Ë∞É‰∫ÜÂú®Áé∞ÂÆûÁéØÂ¢É‰∏≠ÈúÄË¶ÅÊõ¥ÂèØÈù†ÂíåËá™ÊàëÊÑèËØÜÁöÑLLM‰ª£ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06028', 'title': 'Context Forcing: Consistent Autoregressive Video Generation with Long Context', 'url': 'https://huggingface.co/papers/2602.06028', 'abstract': "Context Forcing addresses student-teacher mismatch in long video generation by using a long-context teacher to guide long-rollout students through a Slow-Fast Memory architecture that extends context length beyond 20 seconds.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical student-teacher mismatch: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose Context Forcing, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a Slow-Fast Memory architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.", 'score': 22, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '15567305e92a3b7f', 'authors': ['Shuo Chen', 'Cong Wei', 'Sun Sun', 'Ping Nie', 'Kai Zhou', 'Ge Zhang', 'Ming-Hsuan Yang', 'Wenhu Chen'], 'affiliations': ['Alibaba', 'UC Merced', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2602.06028.jpg', 'data': {'categories': ['#training', '#long_context', '#video', '#architecture'], 'emoji': 'üé¨', 'ru': {'title': '–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —É—á–∏—Ç–µ–ª—è –∏ —Å—Ç—É–¥–µ–Ω—Ç–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ', 'desc': '–†–∞–±–æ—Ç–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —É—á–∏—Ç–µ–ª–µ–º –∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–º –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Context Forcing ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≥–¥–µ —É—á–∏—Ç–µ–ª—å –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –ø–æ–ª–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∫ –∫–æ—Ä–æ—Ç–∫–∏–º 5-—Å–µ–∫—É–Ω–¥–Ω—ã–º –æ–∫–Ω–∞–º. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (–¥–æ 2 –º–∏–Ω—É—Ç) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Slow-Fast Memory, –∫–æ—Ç–æ—Ä–∞—è —Å–Ω–∏–∂–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –ø—É—Ç—ë–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ª–∏–Ω–µ–π–Ω–æ —Ä–∞—Å—Ç—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–≤—ã—à–µ 20 —Å–µ–∫—É–Ω–¥, —á—Ç–æ –≤ 2-10 —Ä–∞–∑ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Bridging the Gap: Long-Context Learning for Video Generation', 'desc': "The paper introduces Context Forcing, a method designed to improve long video generation by addressing the mismatch between short-context teachers and long-context students. Traditional approaches use a short-context teacher, limiting the student's ability to learn from long-term dependencies. Context Forcing employs a long-context teacher that can access the entire generation history, allowing for better guidance and training of the student model. This framework, combined with a Slow-Fast Memory architecture, enables the generation of videos with context lengths exceeding 20 seconds, significantly enhancing consistency and performance compared to existing methods."}, 'zh': {'title': '‰∏ä‰∏ãÊñáÂº∫Âà∂ÔºöËß£ÂÜ≥ÈïøËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÂ≠¶Áîü-ÊïôÂ∏à‰∏çÂåπÈÖç', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‰∏ä‰∏ãÊñáÂº∫Âà∂ÔºàContext ForcingÔºâÁöÑÊñ∞Ê°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑÂ≠¶Áîü-ÊïôÂ∏à‰∏çÂåπÈÖçÈóÆÈ¢ò„ÄÇÈÄöËøá‰ΩøÁî®Èïø‰∏ä‰∏ãÊñáÊïôÂ∏àÊù•ÊåáÂØºÈïøÊó∂Èó¥Â±ïÂºÄÁöÑÂ≠¶ÁîüÔºåÊú¨ÊñáÁöÑÊ°ÜÊû∂ËÉΩÂ§üÊúâÊïàÂú∞Êâ©Â±ï‰∏ä‰∏ãÊñáÈïøÂ∫¶ÔºåË∂ÖËøá20Áßí„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂºïÂÖ•ÊÖ¢-Âø´ËÆ∞ÂøÜÊû∂ÊûÑÔºå‰ºòÂåñ‰∫Ü‰∏ä‰∏ãÊñáÁÆ°ÁêÜÔºå‰ΩøÂæóÂú®ÊûÅÈïøÊó∂Èó¥Ôºà‰æãÂ¶Ç2ÂàÜÈíüÔºâÂÜÖÁöÑÁîüÊàêÂèòÂæóÂèØË°å„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®‰∏ä‰∏ãÊñáÂº∫Âà∂ÁöÑÊñπÊ≥ïÂú®ÈïøËßÜÈ¢ëÁîüÊàêÁöÑ‰∏ÄËá¥ÊÄßÂíåÊïàÊûú‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊäÄÊúØ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05986', 'title': 'RISE-Video: Can Video Generators Decode Implicit World Rules?', 'url': 'https://huggingface.co/papers/2602.05986', 'abstract': 'RISE-Video presents a novel benchmark for evaluating text-image-to-video synthesis models based on cognitive reasoning rather than visual fidelity, using a multi-dimensional metric system and automated LMM-based evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: Reasoning Alignment, Temporal Consistency, Physical Rationality, and Visual Quality. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.', 'score': 22, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'ec411ec1c4184537', 'authors': ['Mingxin Liu', 'Shuran Ma', 'Shibei Meng', 'Xiangyu Zhao', 'Zicheng Zhang', 'Shaofeng Zhang', 'Zhihang Zhong', 'Peixian Chen', 'Haoyu Cao', 'Xing Sun', 'Haodong Duan', 'Xue Yang'], 'affiliations': ['Beijing Normal University', 'Shanghai Jiao Tong University', 'Tencent Youtu Lab', 'The Chinese University of Hong Kong', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2602.05986.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#dataset', '#interpretability', '#video', '#multimodal'], 'emoji': 'üß†', 'ru': {'title': '–û—Ç –∫—Ä–∞—Å–æ—Ç—ã –∫–∞–¥—Ä–æ–≤ –∫ –ª–æ–≥–∏–∫–µ –º–∏—Ä–∞: –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–∞—é—â–µ–≥–æ –≤–∏–¥–µ–æ—Å–∏–Ω—Ç–µ–∑–∞', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω RISE-Video ‚Äî –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞ –Ω–µ –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 467 –≤—Ä—É—á–Ω—É—é –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –≤–æ—Å—å–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö ‚Äî –æ—Ç –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞ –¥–æ —Ñ–∏–∑–∏–∫–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ —á–µ—Ç—ã—Ä—ë—Ö–º–µ—Ä–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –æ—Ü–µ–Ω–∫–∏, –≤–∫–ª—é—á–∞—é—â—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω—É—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å, —Ñ–∏–∑–∏—á–µ—Å–∫—É—é —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ. –î–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω–≤–µ–π–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —ç–º—É–ª–∏—Ä—É—é—â–∏—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ.'}, 'en': {'title': 'Evaluating Video Synthesis through Cognitive Reasoning', 'desc': 'RISE-Video is a new benchmark designed to evaluate text-image-to-video synthesis models by focusing on cognitive reasoning instead of just visual quality. It introduces a multi-dimensional metric system that assesses models on their ability to understand and apply implicit world rules. The benchmark includes 467 human-annotated samples across eight categories, allowing for a comprehensive evaluation of model intelligence in areas like commonsense reasoning and spatial dynamics. Additionally, it features an automated evaluation pipeline using Large Multimodal Models (LMMs) to enhance scalability and efficiency in assessing model performance.'}, 'zh': {'title': 'Êé®ÁêÜ‰ºòÂÖàÔºåË∂ÖË∂äËßÜËßâÁöÑÁîüÊàêËßÜÈ¢ëËØÑ‰º∞', 'desc': 'RISE-VideoÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞ÊñáÊú¨-ÂõæÂÉè-ËßÜÈ¢ëÂêàÊàêÊ®°ÂûãÔºåÈáçÁÇπÂú®‰∫éËÆ§Áü•Êé®ÁêÜËÄåÈùûËßÜËßâË¥®Èáè„ÄÇËØ•Âü∫ÂáÜ‰ΩøÁî®Â§öÁª¥Â∫¶ÁöÑËØÑ‰º∞ÊåáÊ†áÁ≥ªÁªüÔºåÂπ∂ÁªìÂêàËá™Âä®ÂåñÁöÑÂ§ßÂûãÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºàLMMÔºâËøõË°åËØÑ‰º∞„ÄÇRISE-VideoÂåÖÂê´467‰∏™ÁªèËøá‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊ†∑Êú¨ÔºåÊ∂µÁõñÂÖ´‰∏™‰∏•Ê†ºÁöÑÁ±ªÂà´ÔºåÊó®Âú®ÊµãËØïÊ®°ÂûãÂú®Â∏∏ËØÜ„ÄÅÁ©∫Èó¥Âä®ÊÄÅÂíå‰∏ì‰∏öÈ¢ÜÂüüÁ≠âÂ§öÊñπÈù¢ÁöÑÊô∫ËÉΩ„ÄÇÈÄöËøáÂºïÂÖ•Âõõ‰∏™ËØÑ‰º∞ÊåáÊ†áÔºåRISE-Video‰∏∫Êú™Êù•ÁîüÊàêÊ®°ÂûãÁöÑÊîπËøõÊèê‰æõ‰∫ÜÈáçË¶ÅÁöÑËßÅËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02474', 'title': 'MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents', 'url': 'https://huggingface.co/papers/2602.02474', 'abstract': 'MemSkill introduces a learnable and evolvable memory system for LLM agents that dynamically selects and refines memory operations through controller-executor-designer components.  \t\t\t\t\tAI-generated summary \t\t\t\t Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present MemSkill, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a controller that learns to select a small set of relevant skills, paired with an LLM-based executor that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a designer that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.', 'score': 20, 'issue_id': 944, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '36acf08c29b94b86', 'authors': ['Haozhen Zhang', 'Quanyu Long', 'Jianzhu Bao', 'Tao Feng', 'Weizhi Zhang', 'Haodong Yue', 'Wenya Wang'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2602.02474.jpg', 'data': {'categories': ['#long_context', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤', 'desc': 'MemSkill –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—É—á–∞–µ–º—É—é –∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â—É—é —Å–∏—Å—Ç–µ–º—É –ø–∞–º—è—Ç–∏ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –∏ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É–µ—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –ø–∞–º—è—Ç—å—é —á–µ—Ä–µ–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä-–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å-–¥–∏–∑–∞–π–Ω–µ—Ä. –í–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö, –∑–∞—Ä–∞–Ω–µ–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞–º—è—Ç–∏, —Å–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç –∏—Ö –∫–∞–∫ –æ–±—É—á–∞–µ–º—ã–µ –Ω–∞–≤—ã–∫–∏ –ø–∞–º—è—Ç–∏ - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è, –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –∏ —É–¥–∞–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. –ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –æ–±—É—á–∞–µ—Ç—Å—è –≤—ã–±–∏—Ä–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –Ω–∞–±–æ—Ä –Ω–∞–≤—ã–∫–æ–≤, –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞–º—è—Ç—å —Å–æ–≥–ª–∞—Å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º –Ω–∞–≤—ã–∫–∞–º, –∞ –¥–∏–∑–∞–π–Ω–µ—Ä –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ —Å–ª—É—á–∞–∏ –∏ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –Ω–∞–±–æ—Ä –Ω–∞–≤—ã–∫–æ–≤ –ø—É—Ç—ë–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏–π –∏ –Ω–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞.'}, 'en': {'title': 'Evolving Memory Skills for Adaptive LLM Agents', 'desc': 'MemSkill is a novel memory system designed for Large Language Model (LLM) agents that allows for dynamic learning and evolution of memory operations. Unlike traditional systems that use fixed, hand-crafted methods for memory extraction, MemSkill introduces a controller-executor-designer framework that enables the selection and refinement of memory skills. This approach allows the system to adaptively manage memory by learning from interaction patterns and improving over time. Experiments show that MemSkill enhances task performance and generalizes effectively across various applications, paving the way for more flexible memory management in LLMs.'}, 'zh': {'title': 'MemSkillÔºöËøõÂåñÁöÑËÆ∞ÂøÜÁÆ°ÁêÜÁ≥ªÁªü', 'desc': 'MemSkill ÊòØ‰∏ÄÁßçÂèØÂ≠¶‰π†ÂíåÂèØËøõÂåñÁöÑËÆ∞ÂøÜÁ≥ªÁªüÔºå‰∏ì‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜËÆæËÆ°„ÄÇÂÆÉÈÄöËøáÊéßÂà∂Âô®„ÄÅÊâßË°åÂô®ÂíåËÆæËÆ°ËÄÖÁªÑ‰ª∂Âä®ÊÄÅÈÄâÊã©Âíå‰ºòÂåñËÆ∞ÂøÜÊìç‰ΩúÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÈùôÊÄÅËÆ∞ÂøÜÊìç‰ΩúÁöÑÂ±ÄÈôêÊÄß„ÄÇMemSkill Â∞ÜËÆ∞ÂøÜÊìç‰ΩúÈáçÊñ∞ÊûÑÂª∫‰∏∫ÂèØÂ≠¶‰π†ÁöÑËÆ∞ÂøÜÊäÄËÉΩÔºå‰ΩøÂæó‰ø°ÊÅØÊèêÂèñ„ÄÅÊï¥ÂêàÂíå‰øÆÂâ™ÂèòÂæóÊõ¥Âä†ÁÅµÊ¥ªÂíåÈ´òÊïà„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMemSkill Âú®Â§ö‰∏™‰ªªÂä°‰∏äË°®Áé∞‰ºò‰∫éÂº∫Âü∫Á∫øÔºåÂπ∂‰∏îÂú®‰∏çÂêåËÆæÁΩÆ‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05327', 'title': 'ProAct: Agentic Lookahead in Interactive Environments', 'url': 'https://huggingface.co/papers/2602.05327', 'abstract': "ProAct enhances LLM agents' long-horizon planning by combining supervised fine-tuning with search-derived trajectories and a Monte-Carlo critic for improved policy optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct", 'score': 19, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '685ed2179e2dd700', 'authors': ['Yangbin Yu', 'Mingyu Yang', 'Junyou Li', 'Yiming Gao', 'Feiyu Liu', 'Yijun Yang', 'Zichuan Lin', 'Jiafei Lyu', 'Yicheng Liu', 'Zhicong Lu', 'Deheng Ye', 'Jie Jiang'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2602.05327.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#open_source', '#rlhf', '#agents', '#training', '#games'], 'emoji': 'üéØ', 'ru': {'title': '–î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –ø–æ–∏—Å–∫–∞ –≤ —Ä–∞–∑—É–º: –æ–±—É—á–µ–Ω–∏–µ LLM –¥–æ–ª–≥–æ—Ä–µ—á–µ–≤–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –±–µ–∑ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç', 'desc': 'ProAct ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–æ–ª–≥–æ—Ä–µ—á–µ–≤–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ä–µ–¥–µ, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –±—É–¥—É—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ü–µ—Ä–≤—ã–π —ç—Ç–∞–ø –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Grounded LookAhead Distillation (GLAD) ‚Äî supervised fine-tuning –∞–≥–µ–Ω—Ç–∞ –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö –∏–∑ –ø–æ–∏—Å–∫–∞ –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏, —Å–∂–∏–º–∞—è —Å–ª–æ–∂–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è –ø–æ–∏—Å–∫–∞ –≤ —è—Å–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –í—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –ø—Ä–∏–º–µ–Ω—è–µ—Ç Monte-Carlo Critic (MC-Critic) ‚Äî –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –æ—Ü–µ–Ω—â–∏–∫ –∑–Ω–∞—á–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Ç–∏–ø–∞ PPO –∏ GRPO. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ ProAct –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–π 4B –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–Ω–∫—É—Ä–∏—Ä–æ–≤–∞—Ç—å —Å –∑–∞–∫—Ä—ã—Ç—ã–º–∏ SOTA —Ä–µ—à–µ–Ω–∏—è–º–∏.'}, 'en': {'title': 'ProAct: Enhancing Long-Horizon Planning in LLMs', 'desc': "ProAct is a framework designed to improve the long-horizon planning capabilities of Large Language Model (LLM) agents. It combines supervised fine-tuning with search-derived trajectories to enhance the agent's ability to reason about future states. The method includes Grounded LookAhead Distillation (GLAD) for training on simplified causal reasoning chains, and a Monte-Carlo Critic (MC-Critic) to provide stable value estimates for policy optimization. Experiments show that ProAct significantly boosts planning accuracy, outperforming existing models in both stochastic and deterministic environments."}, 'zh': {'title': 'ProActÔºöÊèêÂçáLLM‰ª£ÁêÜÁöÑÈïøËøúËßÑÂàíËÉΩÂäõ', 'desc': 'ProActÊòØ‰∏Ä‰∏™Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰ª£ÁêÜÂú®ÈïøÊó∂Èó¥ËßÑÂàí‰∏≠ÁöÑÊ°ÜÊû∂„ÄÇÂÆÉÁªìÂêà‰∫ÜÁõëÁù£ÂæÆË∞ÉÂíåÂü∫‰∫éÊêúÁ¥¢ÁöÑËΩ®ËøπÔºåÂà©Áî®ËíôÁâπÂç°Ê¥õËØÑËÆ∫ÂëòÊù•‰ºòÂåñÁ≠ñÁï•„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÁéØÂ¢ÉÊêúÁ¥¢ÁöÑËΩ®ËøπËøõË°åÁõëÁù£ÂæÆË∞ÉÔºå‰ª£ÁêÜËÉΩÂ§üÂ≠¶‰π†ÂâçÁûªÊÄßÊé®ÁêÜÁöÑÈÄªËæë„ÄÇÂÆûÈ™åË°®ÊòéÔºåProActÂú®Â§öÁßçÁéØÂ¢É‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜËßÑÂàíÂáÜÁ°ÆÊÄßÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êú™ËßÅÁéØÂ¢É‰∏≠ÁöÑÂº∫Â§ßÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03338', 'title': 'Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention', 'url': 'https://huggingface.co/papers/2602.03338', 'abstract': 'LLM critic models with high offline accuracy can cause variable performance impacts at deployment, necessitating pre-deployment testing to determine intervention safety and effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.   We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.', 'score': 19, 'issue_id': 937, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'e771fff4467f77d2', 'authors': ['Rakshith Vasudev', 'Melisa Russak', 'Dan Bikel', 'Waseem Alshikh'], 'affiliations': ['Writer, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2602.03338.jpg', 'data': {'categories': ['#benchmark', '#inference'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–ö–æ–≥–¥–∞ –Ω–µ –≤–º–µ—à–∏–≤–∞—Ç—å—Å—è: –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–∏—Ç–∏–∫-–º–æ–¥–µ–ª–µ–π –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫—Ä–∏—Ç–∏–∫-–º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (AUROC 0.94) —Ç–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≤—ã–∑–≤–∞—Ç—å —Å–µ—Ä—å–µ–∑–Ω–æ–µ —É—Ö—É–¥—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–∏ –∏–∑-–∑–∞ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –æ—à–∏–±–æ—á–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º —É—Å–ø–µ—à–Ω—ã—Ö. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –∏–∑ 50 –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∫—Ä–∏—Ç–∏–∫-–º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ –≤—ã—è–≤–ª–µ–Ω–∏–∏ —Å–∏—Ç—É–∞—Ü–∏–π, –∫–æ–≥–¥–∞ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≤—Ä–µ–¥–Ω–æ, —á—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–µ—Ä—å–µ–∑–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ.'}, 'en': {'title': 'Predicting Intervention Success: Test Before You Deploy!', 'desc': 'This paper discusses the challenges of using LLM critic models that show high accuracy in offline tests but may perform unpredictably when deployed. It highlights that a binary LLM critic with an impressive AUROC score can still lead to significant performance drops in certain scenarios. The authors introduce a pre-deployment testing method that helps predict whether an intervention will be beneficial or harmful, based on a small pilot study. This approach aims to prevent severe performance regressions by identifying when it is better not to intervene.'}, 'zh': {'title': 'Âπ≤È¢ÑÂÆâÂÖ®ÊÄßÔºöÈ¢ÑÈÉ®ÁΩ≤ÊµãËØïÁöÑÈáçË¶ÅÊÄß', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÊâπËØÑÊ®°ÂûãÂú®ÂÆûÈôÖÈÉ®ÁΩ≤‰∏≠ÁöÑË°®Áé∞‰∏çÁ®≥ÂÆöÊÄß„ÄÇÂ∞ΩÁÆ°Ëøô‰∫õÊ®°ÂûãÂú®Á¶ªÁ∫øÊµãËØï‰∏≠Ë°®Áé∞Âá∫È´òÂáÜÁ°ÆÁéáÔºå‰ΩÜÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÂèØËÉΩÂØºËá¥ÊÄßËÉΩÊòæËëó‰∏ãÈôç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂπ≤È¢ÑÊé™ÊñΩÂèØËÉΩ‰ºöÊÅ¢Â§çÂ§±Ë¥•ÁöÑËΩ®ËøπÔºå‰ΩÜ‰πüÂèØËÉΩÂπ≤Êâ∞Êú¨Êù•‰ºöÊàêÂäüÁöÑËΩ®Ëøπ„ÄÇÂõ†Ê≠§Ôºå‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÈ¢ÑÈÉ®ÁΩ≤ÊµãËØïÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏çÂÆåÂÖ®ÈÉ®ÁΩ≤ÁöÑÊÉÖÂÜµ‰∏ãÔºåÈÄöËøáÂ∞èËßÑÊ®°ËØïÁÇπ‰ªªÂä°Êù•ËØÑ‰º∞Âπ≤È¢ÑÁöÑÂÆâÂÖ®ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05885', 'title': 'Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations', 'url': 'https://huggingface.co/papers/2602.05885', 'abstract': 'Reinforcement learning approach for kernel generation addresses reward hacking and optimization issues through specialized environment and unbiased policy gradient methods, achieving competitive performance with state-of-the-art models.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.', 'score': 18, 'issue_id': 934, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '3afd1cb7cb146793', 'authors': ['Wei Liu', 'Jiawei Xu', 'Yingru Li', 'Longtao Zheng', 'Tianjian Li', 'Qian Liu', 'Junxian He'], 'affiliations': ['CUHK(SZ)', 'HKUST', 'NTU', 'TikTok'], 'pdf_title_img': 'assets/pdf/title_img/2602.05885.jpg', 'data': {'categories': ['#science', '#benchmark', '#dataset', '#open_source', '#rl', '#optimization', '#plp', '#training'], 'emoji': '‚ö°', 'ru': {'title': '–û—Ç –≤–∑–ª–æ–º–∞ –Ω–∞–≥—Ä–∞–¥ –∫ —á–µ—Å—Ç–Ω–æ–º—É RL: –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö GPU-—è–¥–µ—Ä', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –ø–æ–¥—Ö–æ–¥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö GPU-—è–¥–µ—Ä –∫–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ KernelGYM ‚Äî –Ω–∞–¥—ë–∂–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ reward hacking –∏ —Å–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö –∏–∑ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. –û–Ω–∏ –≤—ã—è–≤–∏–ª–∏ —Å–º–µ—â–µ–Ω–∏–µ –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö –ø–æ–ª–∏—Ç–∏–∫–∏ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ GRPO –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ TRLOO –¥–ª—è –Ω–µ—Å–º–µ—â—ë–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≤ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º –æ–±—É—á–µ–Ω–∏–∏. –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Dr.Kernel-14B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≥–µ–Ω–µ—Ä–∏—Ä—É—è —è–¥—Ä–∞ —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –±–æ–ª–µ–µ —á–µ–º –≤ 1.2 —Ä–∞–∑–∞ –¥–ª—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —á–∞—Å—Ç–∏ —Ç–µ—Å—Ç–æ–≤.'}, 'en': {'title': 'Reinforcement Learning for Robust Kernel Generation', 'desc': 'This paper presents a reinforcement learning (RL) approach for generating high-quality kernels, which are essential for efficient AI systems. It introduces KernelGYM, a specialized environment designed to mitigate reward hacking and support long-term RL training. The authors propose a novel method called Turn-level Reinforce-Leave-One-Out (TRLOO) to address biased policy gradients and enhance training stability. The resulting model, Dr.Kernel-14B, demonstrates competitive performance, achieving significant speedups over existing models in kernel generation tasks.'}, 'zh': {'title': 'Âº∫ÂåñÂ≠¶‰π†Âä©ÂäõÈ´òÊïàÂÜÖÊ†∏ÁîüÊàê', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫Ü‰ΩøÁî®Âº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁîüÊàêÈ´òË¥®ÈáèÂÜÖÊ†∏ÁöÑÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Â•ñÂä±ÈªëÂÆ¢Âíå‰ºòÂåñÈóÆÈ¢ò„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Âêç‰∏∫KernelGYMÁöÑÂº∫Â§ßÂàÜÂ∏ÉÂºèGPUÁéØÂ¢ÉÔºå‰ª•ÊîØÊåÅÂ•ñÂä±ÈªëÂÆ¢Ê£ÄÊü•ÂíåÂ§öËΩÆ‰∫§‰∫íÁöÑÊï∞ÊçÆÊî∂ÈõÜ„ÄÇÈÄöËøáÂºïÂÖ•Turn-level Reinforce-Leave-One-Out (TRLOO)ÊñπÊ≥ïÔºåÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜËá™ÂåÖÂê´ÂØºËá¥ÁöÑÂÅèÁΩÆÁ≠ñÁï•Ê¢ØÂ∫¶ÈóÆÈ¢òÔºåÂπ∂ÈÄöËøáÂºïÂÖ•Âü∫‰∫éÈÖçÁΩÆÁöÑÂ•ñÂä±ÂíåÊãíÁªùÈááÊ†∑Êù•ÁºìËß£ÊáíÊÉ∞‰ºòÂåñ„ÄÇÊúÄÁªàÔºåËÆ≠ÁªÉÂá∫ÁöÑÊ®°ÂûãDr.Kernel-14BÂú®Kernelbench‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÁîüÊàêÁöÑÂÜÖÊ†∏Âú®ÈÄüÂ∫¶‰∏äË∂ÖËøá‰∫ÜÁé∞ÊúâÁöÑÂÖàËøõÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04210', 'title': 'Steering LLMs via Scalable Interactive Oversight', 'url': 'https://huggingface.co/papers/2602.04210', 'abstract': 'Scalable Interactive Oversight framework decomposes complex tasks into manageable decision trees to enhance human supervision and alignment in AI systems.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models increasingly automate complex, long-horizon tasks such as vibe coding, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.', 'score': 16, 'issue_id': 934, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '0b4847492fed0a49', 'authors': ['Enyu Zhou', 'Zhiheng Xi', 'Long Ma', 'Zhihao Zhang', 'Shihan Dou', 'Zhikai Lei', 'Guoteng Wang', 'Rui Zheng', 'Hang Yan', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['Fudan University', 'Shanghai Innovation Institute', 'Shanghai Qiji Zhifeng Co., Ltd.'], 'pdf_title_img': 'assets/pdf/title_img/2602.04210.jpg', 'data': {'categories': ['#alignment', '#optimization'], 'emoji': 'üå≥', 'ru': {'title': '–†–∞—Å–∫–ª–∞–¥—ã–≤–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å —á–µ—Ä–µ–∑ –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Scalable Interactive Oversight –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ —Å–ª–æ–∂–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, –≤—ã–ø–æ–ª–Ω—è–µ–º—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ –¥–µ—Ä–µ–≤–æ —É–ø—Ä–∞–≤–ª—è–µ–º—ã—Ö —Ä–µ—à–µ–Ω–∏–π, –≥–¥–µ –Ω–∞ –∫–∞–∂–¥–æ–º —É–∑–ª–µ —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞. –§—Ä–µ–π–º–≤–æ—Ä–∫ –±—ã–ª –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø–æ–∑–≤–æ–ª–∏–ª –Ω–µ—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–∏–µ–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –Ω–∞ 54%. –°–∏—Å—Ç–µ–º–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.'}, 'en': {'title': 'Empowering Human Oversight in AI with Decision Trees', 'desc': 'The paper introduces the Scalable Interactive Oversight framework, which helps humans supervise AI systems by breaking down complex tasks into simpler decision trees. This approach addresses the supervision gap that arises when users struggle to guide AI models effectively due to their lack of expertise and the complexity of tasks. By collecting low-burden feedback at each decision point, the framework allows for the aggregation of user inputs into clear guidance for the AI. The framework has been validated in web development, showing a significant improvement in task alignment and demonstrating its potential for optimization through Reinforcement Learning.'}, 'zh': {'title': 'ÂèØÊâ©Â±ïÁöÑ‰∫íÂä®ÁõëÁù£ÔºöÊèêÂçá‰∫∫Á±ªÂØπAIÁöÑÊéßÂà∂Âäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèØÊâ©Â±ïÁöÑ‰∫íÂä®ÁõëÁù£Ê°ÜÊû∂ÔºåÊó®Âú®Â∞ÜÂ§çÊùÇ‰ªªÂä°ÂàÜËß£‰∏∫ÂèØÁÆ°ÁêÜÁöÑÂÜ≥Á≠ñÊ†ëÔºå‰ª•Â¢ûÂº∫‰∫∫Á±ªÂØπ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÁöÑÁõëÁù£ÂíåÂØπÈΩê„ÄÇÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Ëá™Âä®ÂåñÂ§çÊùÇ‰ªªÂä°ÊñπÈù¢ÁöÑÂ∫îÁî®Â¢ûÂä†ÔºåÂá∫Áé∞‰∫ÜÁõëÁù£Â∑ÆË∑ùÔºåÁî®Êà∑Èöæ‰ª•ÊúâÊïàÂºïÂØºÊ®°Âûã„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂú®ÊØè‰∏™ËäÇÁÇπ‰∏äËé∑Âèñ‰ΩéË¥üÊãÖÂèçÈ¶àÔºåÂπ∂Â∞ÜËøô‰∫õ‰ø°Âè∑ÈÄíÂΩíËÅöÂêà‰∏∫Á≤æÁ°ÆÁöÑÂÖ®Â±ÄÊåáÂØºÔºå‰ªéËÄåËß£ÂÜ≥‰∫ÜËøô‰∏ÄÈóÆÈ¢ò„ÄÇÁªèËøáÈ™åËØÅÔºåËØ•Ê°ÜÊû∂Âú®ÁΩëÈ°µÂºÄÂèë‰ªªÂä°‰∏≠‰ΩøÈùû‰∏ìÂÆ∂ËÉΩÂ§üÁîüÊàê‰∏ìÂÆ∂Á∫ßÁöÑ‰∫ßÂìÅÈúÄÊ±ÇÊñáÊ°£ÔºåÊèêÂçá‰∫Ü54%ÁöÑÂØπÈΩêÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05216', 'title': 'Semantic Search over 9 Million Mathematical Theorems', 'url': 'https://huggingface.co/papers/2602.05216', 'abstract': 'Large-scale semantic theorem retrieval system demonstrates superior performance over existing baselines using a 9.2 million theorem corpus with systematic analysis of representation context, language model choice, and embedding strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Searching for mathematical results remains difficult: most existing tools retrieve entire papers, while mathematicians and theorem-proving agents often seek a specific theorem, lemma, or proposition that answers a query. While semantic search has seen rapid progress, its behavior on large, highly technical corpora such as research-level mathematical theorems remains poorly understood. In this work, we introduce and study semantic theorem retrieval at scale over a unified corpus of 9.2 million theorem statements extracted from arXiv and seven other sources, representing the largest publicly available corpus of human-authored, research-level theorems. We represent each theorem with a short natural-language description as a retrieval representation and systematically analyze how representation context, language model choice, embedding model, and prompting strategy affect retrieval quality. On a curated evaluation set of theorem-search queries written by professional mathematicians, our approach substantially improves both theorem-level and paper-level retrieval compared to existing baselines, demonstrating that semantic theorem search is feasible and effective at web scale. The theorem search tool is available at https://huggingface.co/spaces/uw-math-ai/theorem-search{this link}, and the dataset is available at https://huggingface.co/datasets/uw-math-ai/TheoremSearch{this link}.', 'score': 15, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '3b502b70c07462cf', 'authors': ['Luke Alexander', 'Eric Leonen', 'Sophie Szeto', 'Artemii Remizov', 'Ignacio Tejeda', 'Giovanni Inchiostro', 'Vasily Ilin'], 'affiliations': ['Department of Applied and Computational Mathematical Sciences, University of Washington, Seattle, United States', 'Department of Mathematics, University of Washington, Seattle, United States', 'Lake Washington High School, Kirkland, United States', 'Math AI Lab, University of Washington, Seattle, United States', 'Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, United States'], 'pdf_title_img': 'assets/pdf/title_img/2602.05216.jpg', 'data': {'categories': [], 'emoji': 'üîç', 'ru': {'title': '–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–æ—Ä–µ–º –≤ –º–∞—Å—à—Ç–∞–±–µ –≤–µ–±-—Å–µ—Ç–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–æ—Ä–µ–º –Ω–∞ –∫–æ—Ä–ø—É—Å–µ –∏–∑ 9.2 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–µ–æ—Ä–µ–º, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏–∑ arXiv –∏ –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å—Å–ª–µ–¥—É—é—Ç –≤–ª–∏—è–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –≤—ã–±–æ—Ä–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, —Å—Ç—Ä–∞—Ç–µ–≥–∏–π embeddings –∏ prompting –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞. –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –∫–∞–∫ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–µ–æ—Ä–µ–º, —Ç–∞–∫ –∏ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏–∫–æ–≤. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –∫ —Å–ª–æ–∂–Ω—ã–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∫–æ—Ä–ø—É—Å–∞–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è.'}, 'en': {'title': 'Revolutionizing Theorem Retrieval: A Semantic Approach at Scale', 'desc': 'This paper presents a large-scale semantic theorem retrieval system that outperforms existing methods by utilizing a corpus of 9.2 million theorems. The authors analyze various factors such as representation context, language model selection, and embedding strategies to enhance retrieval accuracy. They demonstrate that their approach significantly improves the retrieval of specific theorems, lemmas, and propositions, which are often sought by mathematicians. The results indicate that effective semantic search for mathematical theorems is achievable at a large scale, making it a valuable tool for researchers.'}, 'zh': {'title': 'Â§ßËßÑÊ®°ËØ≠‰πâÂÆöÁêÜÊ£ÄÁ¥¢ÁöÑÁ™ÅÁ†¥', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§ßËßÑÊ®°ÁöÑËØ≠‰πâÂÆöÁêÜÊ£ÄÁ¥¢Á≥ªÁªüÔºå‰ΩøÁî®‰∫Ü920‰∏á‰∏™ÂÆöÁêÜÁöÑËØ≠ÊñôÂ∫ìÔºåË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂü∫Á∫øÊñπÊ≥ï„ÄÇÊàë‰ª¨ÈÄöËøáÂØπË°®Á§∫‰∏ä‰∏ãÊñá„ÄÅËØ≠Ë®ÄÊ®°ÂûãÈÄâÊã©ÂíåÂµåÂÖ•Á≠ñÁï•ÁöÑÁ≥ªÁªüÂàÜÊûêÔºåÊé¢ËÆ®‰∫ÜËøô‰∫õÂõ†Á¥†Â¶Ç‰ΩïÂΩ±ÂìçÊ£ÄÁ¥¢Ë¥®Èáè„ÄÇËØ•Á≥ªÁªüËÉΩÂ§üÊúâÊïàÂú∞‰ªéÂ§çÊùÇÁöÑÊï∞Â≠¶ÂÆöÁêÜ‰∏≠Ê£ÄÁ¥¢Âá∫ÁâπÂÆöÁöÑÂÆöÁêÜ„ÄÅÂºïÁêÜÊàñÂëΩÈ¢òÔºåÊª°Ë∂≥Êï∞Â≠¶ÂÆ∂ÂíåÂÆöÁêÜËØÅÊòé‰ª£ÁêÜÁöÑÈúÄÊ±Ç„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ≠‰πâÂÆöÁêÜÊ£ÄÁ¥¢Âú®ÁΩëÁªúËßÑÊ®°‰∏äÊòØÂèØË°å‰∏îÊúâÊïàÁöÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21296', 'title': 'Grounding and Enhancing Informativeness and Utility in Dataset Distillation', 'url': 'https://huggingface.co/papers/2601.21296', 'abstract': 'Dataset distillation method that balances informativeness and utility through game-theoretic and gradient-based optimization techniques, achieving improved performance on ImageNet-1K.  \t\t\t\t\tAI-generated summary \t\t\t\t Dataset Distillation (DD) seeks to create a compact dataset from a large, real-world dataset. While recent methods often rely on heuristic approaches to balance efficiency and quality, the fundamental relationship between original and synthetic data remains underexplored. This paper revisits knowledge distillation-based dataset distillation within a solid theoretical framework. We introduce the concepts of Informativeness and Utility, capturing crucial information within a sample and essential samples in the training set, respectively. Building on these principles, we define optimal dataset distillation mathematically. We then present InfoUtil, a framework that balances informativeness and utility in synthesizing the distilled dataset. InfoUtil incorporates two key components: (1) game-theoretic informativeness maximization using Shapley Value attribution to extract key information from samples, and (2) principled utility maximization by selecting globally influential samples based on Gradient Norm. These components ensure that the distilled dataset is both informative and utility-optimized. Experiments demonstrate that our method achieves a 6.1\\% performance improvement over the previous state-of-the-art approach on ImageNet-1K dataset using ResNet-18.', 'score': 15, 'issue_id': 933, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'd348395b7ae6e571', 'authors': ['Shaobo Wang', 'Yantai Yang', 'Guo Chen', 'Peiru Li', 'Kaixin Li', 'Yufa Zhou', 'Zhaorun Chen', 'Linfeng Zhang'], 'affiliations': ['Duke University', 'EPIC Lab, SJTU', 'National University of Singapore', 'Shanghai Jiao Tong University', 'The University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2601.21296.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#optimization', '#data', '#synthetic'], 'emoji': 'üéØ', 'ru': {'title': '–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –≤ –∫–æ–Ω–¥–µ–Ω—Å–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ —Ç–µ–æ—Ä–∏—é –∏–≥—Ä', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç InfoUtil ‚Äî –º–µ—Ç–æ–¥ –∫–æ–Ω–¥–µ–Ω—Å–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞—ë—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏: –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –∫–∞–∂–¥–æ–º –æ–±—Ä–∞–∑—Ü–µ) –∏ —É—Ç–∏–ª–∏—Ç–∞—Ä–Ω–æ—Å—Ç—å (–≤–∞–∂–Ω–æ—Å—Ç—å –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è). –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏–∫–æ-–∏–≥—Ä–æ–≤–æ–π –ø–æ–¥—Ö–æ–¥ —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –®–µ–ø–ª–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è –≤—ã–±–æ—Ä–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ –≤–ª–∏—è—Ç–µ–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤. –ù–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ ImageNet-1K —Å ResNet-18 –º–µ—Ç–æ–¥ –ø–æ–∫–∞–∑–∞–ª —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ 6.1% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª—É—á—à–∏–º–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.'}, 'en': {'title': 'Balancing Informativeness and Utility in Dataset Distillation', 'desc': 'This paper presents a novel approach to Dataset Distillation (DD), which aims to create a smaller, more efficient dataset from a larger one while maintaining its quality. The authors introduce the concepts of Informativeness and Utility, which help in identifying important information and essential samples for training. They propose a new framework called InfoUtil that uses game-theoretic methods and gradient-based optimization to balance these two aspects effectively. Experimental results show that this method significantly enhances performance on the ImageNet-1K dataset, outperforming previous techniques by 6.1%.'}, 'zh': {'title': 'Âπ≥Ë°°‰ø°ÊÅØÊÄß‰∏éÂÆûÁî®ÊÄßÁöÑÊúÄ‰Ω≥Êï∞ÊçÆÈõÜËí∏È¶èÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊï∞ÊçÆÈõÜËí∏È¶èÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÂçöÂºàËÆ∫ÂíåÂü∫‰∫éÊ¢ØÂ∫¶ÁöÑ‰ºòÂåñÊäÄÊúØÊù•Âπ≥Ë°°‰ø°ÊÅØÊÄßÂíåÂÆûÁî®ÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰ø°ÊÅØÊÄßÂíåÂÆûÁî®ÊÄßÁöÑÊ¶ÇÂøµÔºåÂàÜÂà´ÊçïÊçâÊ†∑Êú¨‰∏≠ÁöÑÂÖ≥ÈîÆ‰ø°ÊÅØÂíåËÆ≠ÁªÉÈõÜ‰∏≠ÈáçË¶ÅÊ†∑Êú¨„ÄÇÂü∫‰∫éËøô‰∫õÂéüÂàôÔºåÊàë‰ª¨Êï∞Â≠¶‰∏äÂÆö‰πâ‰∫ÜÊúÄ‰Ω≥Êï∞ÊçÆÈõÜËí∏È¶èÔºåÂπ∂ÊèêÂá∫‰∫ÜInfoUtilÊ°ÜÊû∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®ImageNet-1KÊï∞ÊçÆÈõÜ‰∏äÁõ∏ËæÉ‰∫é‰πãÂâçÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÊèêÈ´ò‰∫Ü6.1%ÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21937', 'title': 'Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities', 'url': 'https://huggingface.co/papers/2601.21937', 'abstract': "DeR2 presents a controlled evaluation framework for assessing language models' document-grounded reasoning capabilities by isolating reasoning from retrieval and toolchain decisions.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.", 'score': 14, 'issue_id': 933, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'ff78f8c05d064ab3', 'authors': ['Shuangshuang Ying', 'Zheyu Wang', 'Yunjian Peng', 'Jin Chen', 'Yuhao Wu', 'Hongbin Lin', 'Dingyu He', 'Siyi Liu', 'Gengchen Yu', 'YinZhu Piao', 'Yuchen Wu', 'Xin Gui', 'Zhongyuan Peng', 'Xin Li', 'Xeron Du', 'Libo Qin', 'YiXin Cao', 'Ge Zhang', 'Stephen Huang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2601.21937.jpg', 'data': {'categories': ['#leakage', '#reasoning', '#benchmark', '#dataset', '#science', '#rag', '#interpretability'], 'emoji': 'üî¨', 'ru': {'title': '–û—Ç–¥–µ–ª—è–µ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ—Ç –ø–æ–∏—Å–∫–∞: –∫–∞–∫ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –¥—É–º–∞—é—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏', 'desc': 'DeR2 ‚Äî —ç—Ç–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å –Ω–∞–¥ –Ω–∞—É—á–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –æ—Ç –∑–∞–¥–∞—á –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –≤—ã–±–æ—Ä–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —á–µ—Ç—ã—Ä–µ —Ä–µ–∂–∏–º–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Ä–∞–∑–Ω—ã–º —É—Ä–æ–≤–Ω–µ–º –¥–æ—Å—Ç—É–ø–∞ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –≥–¥–µ –∏–º–µ–Ω–Ω–æ –º–æ–¥–µ–ª—å —Ç–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ: –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–ª–∏ –ø—Ä–∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏. –î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–≥–æ –∑–Ω–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤—É—Ö—Ñ–∞–∑–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è, –∞ –≤—Å–µ —Ç–µ—Å—Ç—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –Ω–∞ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π 2023-2025 –≥–æ–¥–æ–≤ —Å —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ ¬´–ª–æ–º–∞—é—Ç—Å—è¬ª –ø—Ä–∏ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–º –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏.'}, 'en': {'title': "Isolating Reasoning: Unveiling Language Models' True Capabilities", 'desc': "DeR2 is a new evaluation framework designed to test how well language models can reason using information from documents. It separates the reasoning process from the retrieval of information, allowing for a clearer understanding of a model's capabilities. The framework includes different testing conditions to analyze how models perform when given varying amounts of relevant information. Results show that there are significant differences in how models handle reasoning tasks, highlighting areas for improvement in their design and training."}, 'zh': {'title': 'Ëß£ÈîÅËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊΩúÂäõ', 'desc': 'DeR2ÊòØ‰∏Ä‰∏™ËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÊñáÊ°£Âü∫Á°ÄÊé®ÁêÜËÉΩÂäõÁöÑÊéßÂà∂ËØÑ‰º∞Ê°ÜÊû∂„ÄÇÂÆÉÈÄöËøáÂ∞ÜÊé®ÁêÜ‰∏éÊ£ÄÁ¥¢ÂíåÂ∑•ÂÖ∑ÈìæÂÜ≥Á≠ñÂàÜÁ¶ªÔºåÊù•ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÊñ∞ÁßëÂ≠¶‰ø°ÊÅØÊó∂ÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Êèê‰æõ‰∫ÜÂõõÁßç‰∏çÂêåÁöÑËØÑ‰º∞Ê®°ÂºèÔºå‰ª•‰æøÊõ¥Â•ΩÂú∞ÁêÜËß£Êé®ÁêÜÊçüÂ§±‰∏éÊ£ÄÁ¥¢ÊçüÂ§±‰πãÈó¥ÁöÑÂ∑ÆÂºÇ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫Ôºå‰∏çÂêåÁöÑÂü∫Á°ÄÊ®°ÂûãÂú®Êé®ÁêÜËÉΩÂäõ‰∏äÂ≠òÂú®ÊòæËëóÂ∑ÆÂºÇÔºåÈÉ®ÂàÜÊ®°ÂûãÂú®Â§ÑÁêÜÁõ∏ÂÖ≥ÊñáÊ°£Êó∂Ë°®Áé∞‰∏ç‰Ω≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04942', 'title': 'Privileged Information Distillation for Language Models', 'url': 'https://huggingface.co/papers/2602.04942', 'abstract': 'Training methods that utilize privileged information for language model distillation in multi-turn environments outperform standard supervised fine-tuning followed by reinforcement learning approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, where closed-source systems typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable but the reasoning process is not. For this, we introduce œÄ-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically we find that œÄ-Distill and in some cases OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on œÄ-Distill and characterizing when OPSD is competitive.', 'score': 13, 'issue_id': 943, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '3848cf096b704b07', 'authors': ['Emiliano Penaloza', 'Dheeraj Vattikonda', 'Nicolas Gontier', 'Alexandre Lacoste', 'Laurent Charlin', 'Massimo Caccia'], 'affiliations': ['HEC Montr√©al', 'McGill University', 'Mila Quebec', 'ServiceNow', 'Universit√© de Montr√©al'], 'pdf_title_img': 'assets/pdf/title_img/2602.04942.jpg', 'data': {'categories': ['#rl', '#rlhf', '#training', '#agents'], 'emoji': 'üß†', 'ru': {'title': '–î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π: –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –ø–æ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–º –¥–µ–π—Å—Ç–≤–∏—è–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–¥–∞—á–∏ –∑–Ω–∞–Ω–∏–π –æ—Ç —É—á–∏—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –∫ —É—á–µ–Ω–∏—á–µ—Å–∫–æ–π –≤ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö, –≥–¥–µ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–µ–π—Å—Ç–≤–∏—è—Ö, –Ω–æ –Ω–µ –æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç œÄ-Distill ‚Äî —Å–æ–≤–º–µ—Å—Ç–Ω—É—é objective –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—á–∏—Ç–µ–ª—è —Å –ø—Ä–∏–≤–∏–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏ —É—á–µ–Ω–∏–∫–∞ –±–µ–∑ –Ω–µ—ë –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ On-Policy Self-Distillation, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π —É—Å–∏–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –æ–±—Ä–∞—Ç–Ω–æ–π –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–µ–π –ö—É–ª—å–±–∞–∫–∞-–õ–µ–π–±–ª–µ—Ä–∞ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏. –û–±–∞ –ø–æ–¥—Ö–æ–¥–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã (supervised fine-tuning + RL) –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.'}, 'en': {'title': 'Unlocking Language Models with Privileged Information', 'desc': 'This paper explores how to improve language model training by using privileged information (PI) during the training phase, especially in complex multi-turn environments. The authors introduce two novel methods: œÄ-Distill, which trains a teacher model with PI alongside a student model without it, and On-Policy Self-Distillation (OPSD), which uses reinforcement learning to align the student with the teacher. The results show that these methods outperform traditional training approaches that rely on full supervision, even when only action trajectories are available. The study highlights the importance of effectively transferring knowledge from models trained with PI to those that must operate without it during inference.'}, 'zh': {'title': 'Âà©Áî®ÁâπÊùÉ‰ø°ÊÅØÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®Â§öËΩÆÂØπËØùÁéØÂ¢É‰∏≠ÔºåÂà©Áî®ÁâπÊùÉ‰ø°ÊÅØÔºàPIÔºâËøõË°åËØ≠Ë®ÄÊ®°ÂûãËí∏È¶èÁöÑËÆ≠ÁªÉÊñπÊ≥ï„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®PIÁöÑËÆ≠ÁªÉÊñπÊ≥ïÂú®Â§ÑÁêÜÂ§çÊùÇ‰ªªÂä°Êó∂ÔºåËÉΩÂ§üÊòæËëóË∂ÖË∂ä‰º†ÁªüÁöÑÁõëÁù£ÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ï„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜœÄ-DistillÂíåOn-Policy Self-DistillationÔºàOPSDÔºâ‰∏§ÁßçÊñ∞ÁÆóÊ≥ïÔºåËÉΩÂ§üÊúâÊïàÂú∞‰ªé‰ªÖÊúâÂä®‰ΩúËΩ®ËøπÁöÑÁéØÂ¢É‰∏≠ÊèêÂèñÁü•ËØÜ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåËøô‰∫õÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éË°å‰∏öÊ†áÂáÜÔºåËØÅÊòé‰∫ÜÁâπÊùÉ‰ø°ÊÅØÂú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21037', 'title': 'Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning', 'url': 'https://huggingface.co/papers/2601.21037', 'abstract': 'Video generation models demonstrate robust zero-shot generalization for visual reasoning tasks through explicit visual context utilization and test-time scaling capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.', 'score': 13, 'issue_id': 938, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'}, 'hash': '42df575304d4ca5d', 'authors': ['Chengzu Li', 'Zanyi Wang', 'Jiaang Li', 'Yi Xu', 'Han Zhou', 'Huanyu Zhang', 'Ruichuan An', 'Dengyang Jiang', 'Zhaochong An', 'Ivan Vuliƒá', 'Serge Belongie', 'Anna Korhonen'], 'affiliations': ['Institute of Autom', 'Pioneer Center for AI, University of Copenhagen', 'University of California San Diego', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2601.21037.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark'], 'emoji': 'üé¨', 'ru': {'title': '–í–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≥–¥–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–¥—Ä—ã —Å–ª—É–∂–∞—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ —ç—Ç–∞–ø–∞–º–∏ –º–µ–∂–¥—É –Ω–∞—á–∞–ª—å–Ω—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –∏ —Ä–µ—à–µ–Ω–∏–µ–º. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ —Ç–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å (zero-shot generalization) –Ω–∞ –Ω–µ–∏–∑—É—á–µ–Ω–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –±–ª–∞–≥–æ–¥–∞—Ä—è —è–≤–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –û–±–Ω–∞—Ä—É–∂–µ–Ω –∑–∞–∫–æ–Ω –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –≤–∏–¥–µ–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É—é—Ç, —á—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ–¥–∏–∞, –∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π.'}, 'en': {'title': 'Harnessing Video Generation for Enhanced Visual Reasoning', 'desc': "This paper explores how video generation models can enhance visual reasoning tasks by using generated frames as intermediate steps in problem-solving. It highlights the models' ability to generalize well to new situations without needing extra training, demonstrating robust performance in tasks like Maze Navigation and Tangram Puzzle. The research shows that these models effectively utilize visual context to improve their planning and maintain consistency in their outputs. Additionally, it reveals that longer generated videos can lead to better performance in complex scenarios, suggesting that video generation can serve as a powerful tool for visual reasoning."}, 'zh': {'title': 'ËßÜÈ¢ëÁîüÊàêÔºöËßÜËßâÊé®ÁêÜÁöÑÊñ∞ËåÉÂºè', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜËßÜÈ¢ëÁîüÊàêÊ®°ÂûãÂú®ËßÜËßâÊé®ÁêÜ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØÂú®Èõ∂-shotÊ≥õÂåñËÉΩÂäõÊñπÈù¢„ÄÇÊàë‰ª¨ÊèêÂá∫ÁîüÊàêÁöÑÂ∏ßÂèØ‰ª•‰Ωú‰∏∫ÂàùÂßãÁä∂ÊÄÅ‰∏éËß£ÂÜ≥ÊñπÊ°à‰πãÈó¥ÁöÑ‰∏≠Èó¥Êé®ÁêÜÊ≠•È™§„ÄÇÈÄöËøáÂú®Ëø∑ÂÆ´ÂØºËà™ÂíåÂîêÂè§ÊãâÊãºÂõæËøô‰∏§‰∏™‰ªªÂä°‰∏≠ËøõË°åËØÑ‰º∞ÔºåÊ®°ÂûãÂú®Êú™ËßÅÊï∞ÊçÆ‰∏äË°®Áé∞Âá∫Âº∫Â§ßÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËßÜÈ¢ëÁîüÊàê‰∏ç‰ªÖÊòØÂ™í‰ΩìÂ∑•ÂÖ∑ÔºåÊõ¥ÊòØËßÜËßâÊé®ÁêÜÁöÑÂèØÊâ©Â±ïÂíåÂèØÊ≥õÂåñÁöÑËåÉÂºè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06035', 'title': 'InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions', 'url': 'https://huggingface.co/papers/2602.06035', 'abstract': 'A scalable framework called InterPrior learns a unified generative controller through imitation learning and reinforcement learning to enable humanoids to generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.', 'score': 12, 'issue_id': 934, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '7fc719f79cc3187f', 'authors': ['Sirui Xu', 'Samuel Schulter', 'Morteza Ziyadi', 'Xialin He', 'Xiaohan Fei', 'Yu-Xiong Wang', 'Liangyan Gui'], 'affiliations': ['Amazon', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2602.06035.jpg', 'data': {'categories': ['#rl', '#multimodal', '#robotics', '#training'], 'emoji': 'ü§ñ', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –≥—É–º–∞–Ω–æ–∏–¥–æ–≤', 'desc': 'InterPrior ‚Äî —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –µ–¥–∏–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏–∑—É—á–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –º–æ—Ç–æ—Ä–Ω—ã–µ –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–æ—Ä—ã, –ø–æ–∑–≤–æ–ª—è—è —Ä–æ–±–æ—Ç–∞–º –≤—ã–ø–æ–ª–Ω—è—Ç—å –ª–æ–∫–æ–º–æ—Ç–æ—Ä–Ω–æ-–º–∞–Ω–∏–ø—É–ª—è—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –≤—Å–µ–≥–æ —Ç–µ–ª–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ—Ç–µ–ª—å–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –≤ —É—Å–ª–æ–≤–Ω—É—é –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç –¥–≤–∏–∂–µ–Ω–∏—è –∏–∑ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö –Ω–∞–º–µ—Ä–µ–Ω–∏–π. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –≤–æ–∑–º—É—â–µ–Ω–∏–π –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –Ω–µ–∏–∑–≤–∏–¥–µ–Ω–Ω—ã–µ —Ü–µ–ª–∏ –∏ –æ–±—ä–µ–∫—Ç—ã.'}, 'en': {'title': 'Empowering Humanoids with Generalized Loco-Manipulation Skills', 'desc': "The paper presents InterPrior, a scalable framework designed to enhance humanoid robots' loco-manipulation skills through a combination of imitation learning and reinforcement learning. It focuses on creating a unified generative controller that can adapt to various contexts while ensuring whole-body coordination. By distilling expert behaviors into a goal-conditioned variational policy, the framework allows for the reconstruction of motion based on high-level intentions. Additionally, it employs data augmentation and reinforcement learning to improve the robot's ability to generalize and perform tasks with new objects and scenarios."}, 'zh': {'title': 'InterPriorÔºöÊèêÂçá‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ËøêÂä®ÊäÄËÉΩÁöÑÂèØÊâ©Â±ïÊ°ÜÊû∂', 'desc': 'InterPriorÊòØ‰∏Ä‰∏™ÂèØÊâ©Â±ïÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÊ®°‰ªøÂ≠¶‰π†ÂíåÂº∫ÂåñÂ≠¶‰π†Êù•Â≠¶‰π†Áªü‰∏ÄÁöÑÁîüÊàêÊéßÂà∂Âô®Ôºå‰Ωø‰∫∫ÂΩ¢Êú∫Âô®‰∫∫ËÉΩÂ§üÂú®Â§öÁßçÁéØÂ¢É‰∏≠Êé®ÂπøËøêÂä®ÂíåÊìç‰ΩúÊäÄËÉΩÔºåÂêåÊó∂‰øùÊåÅË∫´‰ΩìÂçèË∞ÉÊÄß„ÄÇËØ•Ê°ÜÊû∂È¶ñÂÖàÂ∞ÜÂÆåÊï¥ÁöÑÊ®°‰ªø‰∏ìÂÆ∂ÊèêÁÇº‰∏∫‰∏Ä‰∏™Â§öÂäüËÉΩÁöÑ„ÄÅ‰ª•ÁõÆÊ†á‰∏∫Êù°‰ª∂ÁöÑÂèòÂàÜÁ≠ñÁï•Ôºå‰ªéÂ§öÊ®°ÊÄÅËßÇÂØüÂíåÈ´òÂ±ÇÊÑèÂõæ‰∏≠ÈáçÂª∫ËøêÂä®„ÄÇ‰∏∫‰∫ÜÊèêÈ´òÂú®Êú™ËßÅÁõÆÊ†áÂíåÂàùÂßãÂåñ‰∏äÁöÑËÉΩÂäõÔºåInterPriorÂ∫îÁî®‰∫ÜÁâ©ÁêÜÊâ∞Âä®ÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÔºåÂπ∂ËøõË°åÂº∫ÂåñÂ≠¶‰π†ÂæÆË∞É„ÄÇÊúÄÁªàÔºåËøô‰∫õÊ≠•È™§Â∞ÜÈáçÂª∫ÁöÑÊΩúÂú®ÊäÄËÉΩÂ∑©Âõ∫‰∏∫ÊúâÊïàÁöÑÊµÅÂΩ¢Ôºå‰ΩøÂæóÊú∫Âô®‰∫∫ËÉΩÂ§üÂú®ËÆ≠ÁªÉÊï∞ÊçÆ‰πãÂ§ñËøõË°åÊé®ÂπøÔºåÁîöËá≥‰∏éÊú™ËßÅÁâ©‰ΩìËøõË°å‰∫§‰∫í„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05115', 'title': 'SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers', 'url': 'https://huggingface.co/papers/2602.05115', 'abstract': 'SocialVeil presents a social learning environment that simulates communication barriers in LLM interactions, demonstrating significant performance degradation under realistic conditions and limited effectiveness of adaptation strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present SocialVeil, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, SocialVeil introduces three representative types of such disruption, semantic vagueness, sociocultural mismatch, and emotional interference. We also introduce two barrier-aware evaluation metrics, unresolved confusion and mutual understanding, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\\% on average, and confusion elevated by nearly 50\\%. Human evaluations validate the fidelity of these simulated barriers (ICCapprox0.78, Pearson rapprox0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.', 'score': 12, 'issue_id': 946, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '586bd9ecdb39746f', 'authors': ['Keyang Xuan', 'Pengda Wang', 'Chongrui Ye', 'Haofei Yu', 'Tal August', 'Jiaxuan You'], 'affiliations': ['Department of Psychological Sciences, Rice University', 'Siebel School of Computing and Data Science, University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2602.05115.jpg', 'data': {'categories': [], 'emoji': 'üöß', 'ru': {'title': '–ö–æ–≥–¥–∞ –ø–æ–º–µ—Ö–∏ –ø–æ—Ä—Ç—è—Ç –±–µ—Å–µ–¥—É: —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ LLM –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç SocialVeil ‚Äî —Å—Ä–µ–¥—É —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –∏–º–∏—Ç–∏—Ä—É–µ—Ç –±–∞—Ä—å–µ—Ä—ã –≤ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ —Ç—Ä–∏ —Ç–∏–ø–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–º–µ—Ö: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –Ω–µ—è—Å–Ω–æ—Å—Ç—å, —Å–æ—Ü–∏–æ–∫—É–ª—å—Ç—É—Ä–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–≤ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ 720 —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å —á–µ—Ç—ã—Ä—å–º—è –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ LLM –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥ –≤–ª–∏—è–Ω–∏–µ–º –±–∞—Ä—å–µ—Ä–æ–≤, —Å —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ–º –≤–∑–∞–∏–º–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–∞ 45% –∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ–º –ø—É—Ç–∞–Ω–∏—Ü—ã –Ω–∞ 50%. –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ –ª–∏—à—å —Å–∫—Ä–æ–º–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç, —É–∫–∞–∑—ã–≤–∞—è –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –æ–±—â–µ–Ω–∏—è.'}, 'en': {'title': 'Bridging the Gap: Testing LLMs in Realistic Social Interactions', 'desc': 'SocialVeil is a new environment designed to test how well large language models (LLMs) handle communication challenges that occur in real-life interactions. It simulates barriers like unclear language, cultural differences, and emotional misunderstandings, which can significantly hinder communication. The study found that these barriers can reduce mutual understanding by over 45% and increase confusion by nearly 50%. Additionally, attempts to improve LLM performance through adaptation strategies showed only limited success, highlighting the need for better models that can navigate complex social interactions.'}, 'zh': {'title': 'Êè≠Á§∫LLMÂú®ÁúüÂÆûÊ≤üÈÄö‰∏≠ÁöÑÊåëÊàò', 'desc': 'SocialVeilÊòØ‰∏Ä‰∏™Á§æ‰∫§Â≠¶‰π†ÁéØÂ¢ÉÔºåÊ®°Êãü‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®‰∫íÂä®‰∏≠Èù¢‰∏¥ÁöÑÊ≤üÈÄöÈöúÁ¢ç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®Áé∞ÂÆûÊù°‰ª∂‰∏ãÔºåLLMÁöÑË°®Áé∞ÊòæËëó‰∏ãÈôçÔºåÈÄÇÂ∫îÁ≠ñÁï•ÁöÑÊïàÊûúÊúâÈôê„ÄÇËØ•ÁéØÂ¢ÉÂü∫‰∫éÂØπ‰∫∫Á±ª‰∫íÂä®‰∏≠Ê≤üÈÄöÊåëÊàòÁöÑÁ≥ªÁªüÊñáÁåÆÂõûÈ°æÔºåÊèêÂá∫‰∫Ü‰∏âÁßç‰ª£Ë°®ÊÄßÁöÑÂπ≤Êâ∞Á±ªÂûã„ÄÇÈÄöËøá720‰∏™Âú∫ÊôØÁöÑÂÆûÈ™åÔºåÂèëÁé∞Ê≤üÈÄöÈöúÁ¢ç‰ºöÊåÅÁª≠ÂΩ±ÂìçLLMÁöÑË°®Áé∞Ôºå‰∫íÁõ∏ÁêÜËß£Â∫¶Âπ≥ÂùáÈôç‰ΩéË∂ÖËøá45%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05842', 'title': 'Reinforcement World Model Learning for LLM-based Agents', 'url': 'https://huggingface.co/papers/2602.05842', 'abstract': 'Reinforcement World Model Learning enables LLM-based agents to better anticipate action consequences and adapt to environment dynamics through self-supervised training that aligns simulated and real-world state transitions in embedding space.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and œÑ^2 Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and œÑ^2 Bench respectively, while matching the performance of expert-data training.', 'score': 11, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '58c2e13df7b9cf21', 'authors': ['Xiao Yu', 'Baolin Peng', 'Ruize Xu', 'Yelong Shen', 'Pengcheng He', 'Suman Nath', 'Nikhil Singh', 'Jiangfeng Gao', 'Zhou Yu'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2602.05842.jpg', 'data': {'categories': ['#agents', '#rl', '#training'], 'emoji': 'üåç', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥ Reinforcement World Model Learning (RWML) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è —Å–≤–æ–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ —Å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º –∑–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞, RWML —Å–æ—Å—Ä–µ–¥–æ—Ç–∞—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π, –∞ –Ω–µ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—ã–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Empowering LLMs with Reinforcement World Models for Better Decision-Making', 'desc': "Reinforcement World Model Learning (RWML) is a self-supervised training method designed for large language model (LLM)-based agents to improve their ability to predict the outcomes of their actions and adapt to changing environments. It focuses on learning action-conditioned world models from textual states, aligning simulated state transitions with real-world observations in an embedding space. This approach encourages consistency between the agent's internal simulations and the actual dynamics of the environment, which is crucial for effective decision-making. RWML demonstrates significant performance improvements over traditional methods, particularly in environments like ALFWorld and œÑ^2 Bench, while being less prone to issues like reward hacking."}, 'zh': {'title': 'Âº∫ÂåñÂ≠¶‰π†‰∏é‰∏ñÁïåÊ®°ÂûãÁöÑÁªìÂêà', 'desc': 'Âº∫Âåñ‰∏ñÁïåÊ®°ÂûãÂ≠¶‰π†ÔºàRWMLÔºâÊòØ‰∏ÄÁßçËá™ÁõëÁù£ÊñπÊ≥ïÔºåÊó®Âú®Â∏ÆÂä©Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊô∫ËÉΩ‰ΩìÊõ¥Â•ΩÂú∞È¢ÑÊµãË°åÂä®ÂêéÊûúÂπ∂ÈÄÇÂ∫îÁéØÂ¢ÉÂä®ÊÄÅ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®ÊñáÊú¨Áä∂ÊÄÅ‰∏äÂ≠¶‰π†‰ª•Ë°åÂä®‰∏∫Êù°‰ª∂ÁöÑ‰∏ñÁïåÊ®°ÂûãÔºåÂà©Áî®Ê®°Êãü‰∏éÁé∞ÂÆû‰πãÈó¥ÁöÑÂ•ñÂä±Â∑ÆË∑ùÊù•ËøõË°åËÆ≠ÁªÉ„ÄÇRWMLÈÄöËøáÂ∞ÜÊ®°ÂûãÁîüÊàêÁöÑÊ®°Êãü‰∏ã‰∏Ä‰∏™Áä∂ÊÄÅ‰∏éÁéØÂ¢É‰∏≠ËßÇÂØüÂà∞ÁöÑÂÆûÈôÖ‰∏ã‰∏Ä‰∏™Áä∂ÊÄÅÂØπÈΩêÔºå‰øÉËøõ‰∫ÜÂÜÖÈÉ®‰∏ñÁïåÊ®°Êãü‰∏éÂÆûÈôÖÁéØÂ¢ÉÂä®ÊÄÅ‰πãÈó¥ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∏ã‰∏Ä‰∏™Áä∂ÊÄÅÊ†áËÆ∞È¢ÑÊµãÊñπÊ≥ï‰∏çÂêåÔºåRWMLÊèê‰æõ‰∫ÜÊõ¥Âº∫ÁöÑËÆ≠ÁªÉ‰ø°Âè∑ÔºåÂáèÂ∞ë‰∫ÜÊ®°ÂûãÂ¥©Ê∫ÉÁöÑÈ£éÈô©ÔºåÂπ∂Âú®Ëá™ÁõëÁù£Â≠¶‰π†‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04884', 'title': 'Reinforced Attention Learning', 'url': 'https://huggingface.co/papers/2602.04884', 'abstract': 'Reinforced Attention Learning optimizes internal attention distributions in multimodal language models, improving information allocation and cross-modal alignment through policy-gradient methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.   We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.', 'score': 10, 'issue_id': 934, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '973732dd1684494f', 'authors': ['Bangzheng Li', 'Jianmo Ni', 'Chen Qu', 'Ian Miao', 'Liu Yang', 'Xingyu Fu', 'Muhao Chen', 'Derek Zhiyuan Cheng'], 'affiliations': ['Google', 'Google DeepMind', 'Princeton University', 'UC Davis'], 'pdf_title_img': 'assets/pdf/title_img/2602.04884.jpg', 'data': {'categories': ['#video', '#reasoning', '#rl', '#optimization', '#multimodal', '#training'], 'emoji': 'üëÅÔ∏è', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Reinforced Attention Learning (RAL), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –≤–Ω–∏–º–∞–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞–≤—Ç–æ—Ä—ã —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–æ–≥–æ, –≥–¥–µ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ—ë –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ. –ú–µ—Ç–æ–¥ —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ª—É—á—à–µ–µ –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –∫ —Å–ª–æ–∂–Ω—ã–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –≤—Ö–æ–¥–∞–º. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ On-Policy Attention Distillation –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –ø–µ—Ä–µ–¥–∞—á–∞ —Å–∫—Ä—ã—Ç—ã—Ö –ø–æ–≤–µ–¥–µ–Ω–∏–π –≤–Ω–∏–º–∞–Ω–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ –¥–∞—ë—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∫—Ä–æ—Å—Å-–º–æ–¥–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ, —á–µ–º —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∑–Ω–∞–Ω–∏–π.'}, 'en': {'title': 'Optimizing Attention for Better Multimodal Understanding', 'desc': 'Reinforced Attention Learning (RAL) enhances multimodal language models by optimizing their internal attention distributions using policy-gradient methods. This approach shifts the focus from generating output sequences to determining where the model should pay attention, leading to better information allocation and alignment across different types of data, such as images and text. The study shows that RAL outperforms existing methods like GRPO in various benchmarks, indicating its effectiveness in improving model reasoning and performance. Additionally, the introduction of On-Policy Attention Distillation demonstrates that transferring attention behaviors can achieve better cross-modal alignment compared to traditional knowledge distillation techniques.'}, 'zh': {'title': '‰ºòÂåñÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊ≥®ÊÑèÂäõÂàÜÈÖç', 'desc': 'Âº∫ÂåñÊ≥®ÊÑèÂäõÂ≠¶‰π†ÔºàRALÔºâÈÄöËøáÁ≠ñÁï•Ê¢ØÂ∫¶ÊñπÊ≥ï‰ºòÂåñÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÂÜÖÈÉ®Ê≥®ÊÑèÂäõÂàÜÂ∏ÉÔºå‰ªéËÄåÊîπÂñÑ‰ø°ÊÅØÂàÜÈÖçÂíåË∑®Ê®°ÊÄÅÂØπÈΩê„ÄÇ‰∏é‰º†ÁªüÁöÑËæìÂá∫Â∫èÂàóÁîüÊàê‰∏çÂêåÔºåRAL‰∏ìÊ≥®‰∫é‰ºòÂåñÊ≥®ÊÑèÂäõÁöÑÂàÜÈÖç‰ΩçÁΩÆÔºåÊèêÂçá‰∫ÜÂú®Â§çÊùÇÂ§öÊ®°ÊÄÅËæìÂÖ•‰∏≠ÁöÑ‰ø°ÊÅØÂ§ÑÁêÜËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRALÂú®Â§öÁßçÂõæÂÉèÂíåËßÜÈ¢ëÂü∫ÂáÜÊµãËØï‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÁöÑÂü∫Á∫øÊñπÊ≥ï„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜÂú®Á∫øÁ≠ñÁï•Ê≥®ÊÑèÂäõËí∏È¶èÔºåÊòæÁ§∫Âá∫ËΩ¨ÁßªÊΩúÂú®Ê≥®ÊÑèÂäõË°å‰∏∫ËÉΩÊõ¥ÊúâÊïàÂú∞ÂÆûÁé∞Ë∑®Ê®°ÊÄÅÂØπÈΩê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06040', 'title': 'SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs', 'url': 'https://huggingface.co/papers/2602.06040', 'abstract': 'SwimBird is a reasoning-switchable multimodal large language model that dynamically selects between text-only, vision-only, and interleaved vision-text reasoning modes based on input queries, achieving superior performance on both textual and visual tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as "visual thoughts" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.', 'score': 9, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '0fe3ed85f7deadfb', 'authors': ['Jintao Tong', 'Shilin Yan', 'Hongwei Xue', 'Xiaojun Tang', 'Kunyu Shi', 'Guannan Zhang', 'Ruixuan Li', 'Yixiong Zou'], 'affiliations': ['Accio Team, Alibaba Group', 'Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2602.06040.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#architecture', '#training', '#interpretability', '#multimodal'], 'emoji': 'üéØ', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': 'SwimBird ‚Äî —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—Ö–æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç—Ä–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞, —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑—Ä–µ–Ω–∏—è (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –∫–∞–∫ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º—ã—Å–ª–µ–π) –∏–ª–∏ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–∏–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º—ã—Å–ª–µ–π —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º —Å–ª–µ–¥—É—é—â–µ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º—ã—Å–ª–µ–π, –∏ —Å–æ–∑–¥–∞–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. SwimBird –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫ –∏ –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è.'}, 'en': {'title': 'SwimBird: Adaptive Reasoning for Text and Vision', 'desc': "SwimBird is a novel multimodal large language model that enhances reasoning by dynamically switching between three modes: text-only, vision-only, and interleaved vision-text. This flexibility allows the model to adapt its reasoning approach based on the specific requirements of the input query, addressing the limitations of traditional models that rely on a fixed reasoning pattern. By integrating a hybrid autoregressive formulation, SwimBird effectively combines textual and visual reasoning, leading to improved performance on both text and vision tasks. The model's design includes a diverse supervised fine-tuning dataset, SwimBird-SFT-92K, which supports its ability to maintain strong logical reasoning while excelling in vision-intensive scenarios."}, 'zh': {'title': 'ÁÅµÊ¥ªÂàáÊç¢ÔºåÊèêÂçáÂ§öÊ®°ÊÄÅÊé®ÁêÜËÉΩÂäõ', 'desc': 'SwimBirdÊòØ‰∏ÄÁßçÂèØÂàáÊç¢Êé®ÁêÜÊ®°ÂºèÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÊ†πÊçÆËæìÂÖ•Êü•ËØ¢Âä®ÊÄÅÈÄâÊã©ÊñáÊú¨Êé®ÁêÜ„ÄÅËßÜËßâÊé®ÁêÜÂíå‰∫§ÈîôËßÜËßâ-ÊñáÊú¨Êé®ÁêÜ‰∏âÁßçÊ®°Âºè„ÄÇËøôÁßçÁÅµÊ¥ªÁöÑÊ®°ÂºèÈÄâÊã©‰ΩøÂæóÊ®°ÂûãÂú®Â§ÑÁêÜÊñáÊú¨ÂíåËßÜËßâ‰ªªÂä°Êó∂ÈÉΩËÉΩË°®Áé∞Âá∫Ëâ≤ÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊ®°ÂûãÂú®ËßÜËßâÂØÜÈõÜ‰ªªÂä°‰∏äÁöÑÂ±ÄÈôêÊÄß„ÄÇÈÄöËøáÈááÁî®Ê∑∑ÂêàËá™ÂõûÂΩíÁöÑÊñπÂºèÔºåSwimBirdËÉΩÂ§üÂêåÊó∂ËøõË°åÊñáÊú¨ÂíåËßÜËßâÁöÑÊé®ÁêÜÔºå‰øùÊåÅÂº∫Â§ßÁöÑÊñáÊú¨ÈÄªËæëÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSwimBirdÂú®Â§öÁßçÂü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÊòæËëó‰ºò‰∫é‰πãÂâçÁöÑÂõ∫ÂÆöÊ®°ÂºèÂ§öÊ®°ÊÄÅÊé®ÁêÜÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05975', 'title': 'SAGE: Benchmarking and Improving Retrieval for Deep Research Agents', 'url': 'https://huggingface.co/papers/2602.05975', 'abstract': 'LLM-based retrievers show limited effectiveness in deep research agent workflows, with traditional BM25 performing better, though corpus-level test-time scaling can improve retrieval performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.', 'score': 9, 'issue_id': 934, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '0292716f6b5dcd63', 'authors': ['Tiansheng Hu', 'Yilun Zhao', 'Canyu Zhang', 'Arman Cohan', 'Chen Zhao'], 'affiliations': ['Center for Data Science, New York University', 'NYU Shanghai', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2602.05975.jpg', 'data': {'categories': ['#science', '#agents', '#benchmark', '#dataset', '#reasoning', '#rag'], 'emoji': 'üîç', 'ru': {'title': '–ö–æ–≥–¥–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ–±–µ–∂–¥–∞—é—Ç —Ä–∞–∑—É–º: –ø–æ—á–µ–º—É –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ LLM –≤ –Ω–∞—É—á–Ω—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å LLM-based –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º –≤ —Ä–∞–º–∫–∞—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –±–µ–Ω—á–º–∞—Ä–∫ SAGE —Å 1200 –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∏ –∫–æ—Ä–ø—É—Å–æ–º –∏–∑ 200,000 –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —à–µ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤. –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –≤—ã—è—Å–Ω—è–µ—Ç—Å—è, —á—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ BM25 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç LLM-based –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏ –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 30%, –ø–æ—Å–∫–æ–ª—å–∫—É –∞–≥–µ–Ω—Ç—ã –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–æ—Ä–ø—É—Å–∞, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ø–æ–ª–Ω—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ —Å –ø–æ–º–æ—â—å—é LLM, —á—Ç–æ –¥–∞–µ—Ç –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 8% –∏ 2% –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤.'}, 'en': {'title': 'Boosting Retrieval: BM25 Outshines LLMs in Research Workflows', 'desc': 'This paper investigates the effectiveness of LLM-based retrievers in deep research agent workflows, revealing that traditional BM25 retrieval methods outperform them. The authors introduce SAGE, a benchmark for scientific literature retrieval, which includes 1,200 queries and a corpus of 200,000 papers. They evaluate various deep research agents and find that all struggle with reasoning-intensive tasks, with BM25 showing a 30% advantage over LLM-based methods. To enhance retrieval performance, they propose a corpus-level test-time scaling framework that enriches documents with metadata, resulting in improved retrieval for both short-form and open-ended questions.'}, 'zh': {'title': 'Ê∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜ‰∏≠ÁöÑÊ£ÄÁ¥¢ÊåëÊàò‰∏éËß£ÂÜ≥ÊñπÊ°à', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊ£ÄÁ¥¢Âô®Âú®Ê∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜÂ∑•‰ΩúÊµÅ‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇÁ†îÁ©∂ÂèëÁé∞Ôºå‰º†ÁªüÁöÑBM25Ê£ÄÁ¥¢Âô®Âú®Â§ÑÁêÜÂ§çÊùÇÊü•ËØ¢Êó∂Ë°®Áé∞Êõ¥‰Ω≥ÔºåÂ∞§ÂÖ∂ÊòØÂú®Êé®ÁêÜÂØÜÈõÜÂûãÊ£ÄÁ¥¢ÊñπÈù¢„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜSAGEÂü∫ÂáÜÔºåÂåÖÂê´1200‰∏™Êü•ËØ¢Âíå20‰∏áÁØáÊñáÁåÆÔºå‰ª•ËØÑ‰º∞‰∏çÂêåÊ£ÄÁ¥¢Â∑•ÂÖ∑ÁöÑÊÄßËÉΩ„ÄÇÈÄöËøáÂºïÂÖ•ËØ≠ÊñôÂ∫ìÁ∫ßÊµãËØïÊó∂Èó¥Êâ©Â±ïÊ°ÜÊû∂ÔºåÁªìÂêàLLMÂ¢ûÂº∫ÊñáÊ°£ÁöÑÂÖÉÊï∞ÊçÆÂíåÂÖ≥ÈîÆËØçÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ£ÄÁ¥¢ÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03036', 'title': 'LatentMem: Customizing Latent Memory for Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2602.03036', 'abstract': 'LatentMem is a learnable multi-agent memory framework that customizes agent-specific memories through latent representations, improving performance in multi-agent systems without modifying underlying frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to 19.36% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.', 'score': 8, 'issue_id': 933, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '28560358dfdd375d', 'authors': ['Muxin Fu', 'Guibin Zhang', 'Xiangyuan Xue', 'Yafu Li', 'Zefeng He', 'Siyuan Huang', 'Xiaoye Qu', 'Yu Cheng', 'Yang Yang'], 'affiliations': ['Nanjing University', 'National University of Singapore', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2602.03036.jpg', 'data': {'categories': ['#agents', '#optimization', '#training', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ª–∞—Ç–µ–Ω—Ç–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º', 'desc': 'LatentMem ‚Äî —ç—Ç–æ –æ–±—É—á–∞–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–π –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–∑–¥–∞—ë—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ —Å–∫—Ä—ã—Ç—ã–µ (latent) –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞. –°–∏—Å—Ç–µ–º–∞ —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: –æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ä–æ–ª–µ–≤–æ–π –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞—Ü–∏–∏ –∏ –ø–µ—Ä–µ–≥—Ä—É–∑–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ—Ç –∏–∑–±—ã—Ç–æ—á–Ω–æ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –±–∞–Ω–∫ –æ–ø—ã—Ç–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∏ –∫–æ–º–ø–æ–∑–∏—Ç–æ—Ä –ø–∞–º—è—Ç–∏, —Å–∏–Ω—Ç–µ–∑–∏—Ä—É—é—â–∏–π –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞. –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–µ—Ç–æ–¥–∏–∫–∞ Latent Memory Policy Optimization (LMPO), –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ–ø–∞–≥–∏—Ä—É–µ—Ç —Å–∏–≥–Ω–∞–ª—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –ª–∞—Ç–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –ø–æ–±—É–∂–¥–∞—è —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å –∫–æ–º–ø–∞–∫—Ç–Ω—ã–µ –∏ –≤—ã—Å–æ–∫–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏.'}, 'en': {'title': 'Customizing Agent Memories for Enhanced Multi-Agent Performance', 'desc': 'LatentMem is a novel framework designed to enhance multi-agent systems by creating customized memories for each agent using latent representations. This approach addresses two key issues in existing memory designs: the lack of role-specific customization and the problem of information overload from too many detailed memory entries. By utilizing an experience bank and a memory composer, LatentMem efficiently synthesizes compact memories tailored to individual agent contexts. The framework also introduces Latent Memory Policy Optimization (LMPO) to optimize memory utility, leading to significant performance improvements in various benchmarks without altering the foundational systems.'}, 'zh': {'title': 'ÂÆöÂà∂ÂåñÊô∫ËÉΩ‰ΩìËÆ∞ÂøÜÔºåÊèêÂçáÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÊÄßËÉΩ', 'desc': 'LatentMemÊòØ‰∏ÄÁßçÂèØÂ≠¶‰π†ÁöÑÂ§öÊô∫ËÉΩ‰ΩìËÆ∞ÂøÜÊ°ÜÊû∂ÔºåÈÄöËøáÊΩúÂú®Ë°®Á§∫ÂÆöÂà∂ÁâπÂÆöÊô∫ËÉΩ‰ΩìÁöÑËÆ∞ÂøÜÔºå‰ªéËÄåÊèêÈ´òÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑÊÄßËÉΩÔºåËÄåÊó†ÈúÄ‰øÆÊîπÂü∫Á°ÄÊ°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÁé∞ÊúâÂ§öÊô∫ËÉΩ‰ΩìËÆ∞ÂøÜËÆæËÆ°‰∏≠ÁöÑ‰∏§‰∏™‰∏ªË¶ÅÁì∂È¢àÔºöËÆ∞ÂøÜÂêåË¥®ÂåñÂíå‰ø°ÊÅØËøáËΩΩ„ÄÇLatentMemÂåÖÂê´‰∏Ä‰∏™ÁªèÈ™åÂ∫ìÔºåÁî®‰∫é‰ª•ËΩªÈáèÁ∫ßÂΩ¢ÂºèÂ≠òÂÇ®ÂéüÂßã‰∫§‰∫íËΩ®ËøπÔºå‰ª•Âèä‰∏Ä‰∏™ËÆ∞ÂøÜÂêàÊàêÂô®ÔºåÊ†πÊçÆÊ£ÄÁ¥¢Âà∞ÁöÑÁªèÈ™åÂíåÁâπÂÆöÊô∫ËÉΩ‰ΩìÁöÑ‰∏ä‰∏ãÊñáÂêàÊàêÁ¥ßÂáëÁöÑÊΩúÂú®ËÆ∞ÂøÜ„ÄÇÈÄöËøáÂºïÂÖ•ÊΩúÂú®ËÆ∞ÂøÜÁ≠ñÁï•‰ºòÂåñÔºàLMPOÔºâÔºåLatentMemËÉΩÂ§üÊúâÊïà‰º†Êí≠‰ªªÂä°Á∫ß‰ºòÂåñ‰ø°Âè∑ÔºåÈºìÂä±ÂêàÊàêÂô®ÁîüÊàêÁ¥ßÂáë‰∏îÈ´òÊïàÁöÑË°®Á§∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06034', 'title': 'V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval', 'url': 'https://huggingface.co/papers/2602.06034', 'abstract': 'V-Retrver introduces an evidence-driven retrieval framework that enables multimodal large language models to actively verify visual evidence through an agentic reasoning process, improving retrieval accuracy and reasoning reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.', 'score': 7, 'issue_id': 935, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'd23e43df4fafd2d7', 'authors': ['Dongyang Chen', 'Chaoyang Wang', 'Dezhao SU', 'Xi Xiao', 'Zeyu Zhang', 'Jing Xiong', 'Qing Li', 'Yuzhang Shang', 'Shichao Ka'], 'affiliations': ['Central South University', 'Fudan University', 'Pengcheng Laboratory', 'The Australian National University', 'The University of Hong Kong', 'Tsinghua University', 'University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2602.06034.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#rl', '#interpretability', '#rag', '#benchmark', '#agents'], 'emoji': 'üîç', 'ru': {'title': '–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∞–∫—Ç–∏–≤–Ω—É—é –≤–∏–∑—É–∞–ª—å–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é', 'desc': 'V-Retrver ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –∞–∫—Ç–∏–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í–º–µ—Å—Ç–æ –ø–∞—Å—Å–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–æ–∫, —Å–∏—Å—Ç–µ–º–∞ –º–æ–∂–µ—Ç –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–æ –ø–æ–ª—É—á–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–∞ –≤–æ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞, —á–µ—Ä–µ–¥—É—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≥–∏–ø–æ—Ç–µ–∑ —Å —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π. –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–∞–∫–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —É—Ç–æ—á–Ω–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ (–≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ 23%), –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é.'}, 'en': {'title': 'Empowering MLLMs with Active Visual Verification', 'desc': "V-Retrver is a new framework that enhances multimodal large language models (MLLMs) by allowing them to verify visual evidence actively during the retrieval process. This approach addresses the limitations of previous methods that relied heavily on static visual encodings and often led to inaccurate reasoning in ambiguous situations. By reformulating retrieval as an agentic reasoning process, V-Retrver enables MLLMs to gather and inspect visual evidence dynamically, improving both accuracy and reliability. The framework employs a curriculum-based learning strategy that combines various training techniques to optimize the model's performance across different multimodal retrieval tasks."}, 'zh': {'title': 'Âü∫‰∫éËØÅÊçÆÁöÑÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Êñ∞Ê°ÜÊû∂', 'desc': 'V-RetrverÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éËØÅÊçÆÁöÑÊ£ÄÁ¥¢Ê°ÜÊû∂Ôºå‰ΩøÂ§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÈÄöËøá‰∏ªÂä®È™åËØÅËßÜËßâËØÅÊçÆÊù•ÊèêÈ´òÊ£ÄÁ¥¢ÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÂèØÈù†ÊÄß„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢ÈáçÊñ∞ÂÆö‰πâ‰∏∫‰∏ÄÁßçÂü∫‰∫éËßÜËßâÊ£ÄÊü•ÁöÑ‰ª£ÁêÜÊé®ÁêÜËøáÁ®ãÔºåÂÖãÊúç‰∫ÜÁé∞ÊúâÊñπÊ≥ï‰∏≠ÈùôÊÄÅËßÜËßâÁºñÁ†ÅÁöÑÂ±ÄÈôêÊÄß„ÄÇÈÄöËøá‰ΩøÁî®Â§ñÈÉ®ËßÜËßâÂ∑•ÂÖ∑ÔºåV-RetrverËÉΩÂ§üÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ÈÄâÊã©ÊÄßÂú∞Ëé∑ÂèñËßÜËßâËØÅÊçÆÔºåËøõË°åÂÅáËÆæÁîüÊàêÂíåÁõÆÊ†áËßÜËßâÈ™åËØÅÁöÑ‰∫§ÊõøÊé®ÁêÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®Â§ö‰∏™Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢Âü∫ÂáÜ‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊ£ÄÁ¥¢ÂáÜÁ°ÆÊÄßÂíåÊé®ÁêÜÁöÑÂèØÈù†ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05073', 'title': 'Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents', 'url': 'https://huggingface.co/papers/2602.05073', 'abstract': 'Large language models require uncertainty quantification frameworks that account for interactive agent behavior rather than traditional single-turn question answering scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent\'s trajectory by highlighting "interactivity" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems.', 'score': 7, 'issue_id': 943, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '947ef5c30d0250d2', 'authors': ['Changdae Oh', 'Seongheon Park', 'To Eun Kim', 'Jiatong Li', 'Wendi Li', 'Samuel Yeh', 'Xuefeng Du', 'Hamed Hassani', 'Paul Bogdan', 'Dawn Song', 'Sharon Li'], 'affiliations': ['Carnegie Mellon University', 'Nanyang Technological University', 'University of California, Berkeley', 'University of Pennsylvania', 'University of Southern California', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2602.05073.jpg', 'data': {'categories': [], 'emoji': 'ü§ñ', 'ru': {'title': '–û—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –∫ —Å–Ω–∏–∂–µ–Ω–∏—é: –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö LLM', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ (uncertainty quantification) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∞ –Ω–µ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –æ–¥–Ω–æ–æ–±–æ—Ä–æ—Ç–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–µ—è–≤–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –ø—Ä–æ—Ü–µ—Å—Å –µ—ë –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è, —á—Ç–æ –Ω–µ–ø—Ä–∏–º–µ–Ω–∏–º–æ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞ —É—Å–ª–æ–≤–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è —è–≤–Ω–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —É—Å—Ç—Ä–∞–Ω–∏–º—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∞–≥–µ–Ω—Ç–∞, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞—è —Ä–æ–ª—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π. –†–∞–±–æ—Ç–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫—É—é –±–∞–∑—É –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö LLM-–∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Revolutionizing Uncertainty Quantification for Interactive Language Agents', 'desc': 'This paper discusses the need for better uncertainty quantification (UQ) methods for large language models (LLMs) that operate in interactive environments, rather than just in simple question-answering scenarios. It highlights that current UQ research often overlooks the complexities of interactive agent behavior, which is crucial for real-world applications. The authors propose a new framework that treats UQ as a conditional uncertainty reduction process, focusing on how actions taken by agents can reduce uncertainty over time. This approach aims to improve the safety and effectiveness of LLMs in dynamic settings, providing a foundation for future research and practical applications.'}, 'zh': {'title': '‰∫§‰∫íÂºèÊô∫ËÉΩ‰ΩìÁöÑ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÊñ∞Ê°ÜÊû∂', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÈúÄË¶ÅËÄÉËôë‰∫§‰∫íÂºèÊô∫ËÉΩ‰ΩìË°å‰∏∫ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÊ°ÜÊû∂ÔºåËÄå‰∏çÊòØ‰º†ÁªüÁöÑÂçïËΩÆÈóÆÁ≠îÂú∫ÊôØ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÔºàUQÔºâÊ°ÜÊû∂ÔºåÈÄÇÁî®‰∫éÂ§çÊùÇÁöÑ‰∫§‰∫íÂºè‰ªªÂä°ÔºåÂº∫Ë∞É‰∫ÜÊô∫ËÉΩ‰ΩìÂú®ÂºÄÊîæ‰∏ñÁïå‰∏≠ÁöÑË°å‰∏∫‰∫§‰∫í„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜÁé∞ÊúâÁ†îÁ©∂Â¶Ç‰ΩïÂ∞ÜUQËßÜ‰∏∫‰∏çÁ°ÆÂÆöÊÄßÁßØÁ¥ØËøáÁ®ãÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊù°‰ª∂‰∏çÁ°ÆÂÆöÊÄßÂáèÂ∞ëËøáÁ®ãÁöÑËßÇÁÇπÔºå‰ª•Êõ¥Â•ΩÂú∞Âª∫Ê®°Êô∫ËÉΩ‰ΩìÁöÑËΩ®Ëøπ„ÄÇÊúÄÂêéÔºåÊàë‰ª¨ËÆ®ËÆ∫‰∫ÜËøô‰∏ÄÊ°ÜÊû∂Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂºÄÂèëÂíåÁâπÂÆöÈ¢ÜÂüüÂ∫îÁî®‰∏≠ÁöÑÂÆûÈôÖÊÑè‰πâ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05547', 'title': 'Multi-Task GRPO: Reliable LLM Reasoning Across Tasks', 'url': 'https://huggingface.co/papers/2602.05547', 'abstract': 'Multi-Task GRPO algorithm improves balanced performance across diverse reasoning tasks by dynamically adapting task weights and using a ratio-preserving sampler to ensure equitable optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.', 'score': 6, 'issue_id': 937, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'fd5d8cd2535a0501', 'authors': ['Shyam Sundhar Ramesh', 'Xiaotong Ji', 'Matthieu Zimmer', 'Sangwoong Yoon', 'Zhiyong Wang', 'Haitham Bou Ammar', 'Aurelien Lucchi', 'Ilija Bogunovic'], 'affiliations': ['Huawei Noahs Ark Lab', 'UCL Centre for AI', 'UCL Department of EEE', 'UNIST Graduate School of AI', 'University of Basel', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2602.05547.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#rl'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM: –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º Multi-Task GRPO –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π GRPO –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∑–∞–¥–∞—á–∞–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –¥–∏—Å–±–∞–ª–∞–Ω—Å—É, –∫–æ–≥–¥–∞ –æ–¥–Ω–∏ –∑–∞–¥–∞—á–∏ –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∞ –¥—Ä—É–≥–∏–µ –æ—Ç—Å—Ç–∞—é—Ç. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –≤–µ—Å–æ–≤ –∑–∞–¥–∞—á –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–∏—Ö—É–¥—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤–≤–µ–¥–µ–Ω–∏–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–µ–º–ø–ª–µ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ–ª–∏—Ç–∏–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MT-GRPO –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã, —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–∞–º—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞ 16-28% –∏ —Ç—Ä–µ–±—É—è –ø—Ä–∏ —ç—Ç–æ–º –Ω–∞ 50% –º–µ–Ω—å—à–µ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Balancing Act: MT-GRPO for Equitable Task Performance', 'desc': 'The Multi-Task GRPO (MT-GRPO) algorithm enhances the performance of machine learning models across various reasoning tasks by adjusting task weights dynamically. This approach ensures that no single task overshadows others during optimization, promoting balanced progress. Additionally, MT-GRPO employs a ratio-preserving sampler to accurately reflect the adjusted task weights in policy gradients. Experimental results show that MT-GRPO significantly improves worst-task accuracy while requiring fewer training steps compared to traditional methods.'}, 'zh': {'title': 'Âä®ÊÄÅË∞ÉÊï¥‰ªªÂä°ÊùÉÈáçÔºåÂÆûÁé∞ÂùáË°°‰ºòÂåñ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§ö‰ªªÂä°GRPOÔºàMT-GRPOÔºâÁÆóÊ≥ïÔºåÊó®Âú®ÊèêÈ´ò‰∏çÂêåÊé®ÁêÜ‰ªªÂä°‰πãÈó¥ÁöÑÂπ≥Ë°°ÊÄßËÉΩ„ÄÇËØ•ÁÆóÊ≥ïÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥‰ªªÂä°ÊùÉÈáçÔºå‰ºòÂåñË°®Áé∞ÊúÄÂ∑Æ‰ªªÂä°ÁöÑÊÄßËÉΩÔºå‰ªéËÄå‰øÉËøõÂêÑ‰ªªÂä°ÁöÑÂùáË°°ËøõÂ±ï„ÄÇÊ≠§Â§ñÔºåMT-GRPOÂºïÂÖ•‰∫Ü‰∏ÄÁßç‰øùÊåÅÊØî‰æãÁöÑÈááÊ†∑Âô®ÔºåÁ°Æ‰øù‰ªªÂä°ÁöÑÁ≠ñÁï•Ê¢ØÂ∫¶ÂèçÊò†Ë∞ÉÊï¥ÂêéÁöÑÊùÉÈáç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMT-GRPOÂú®ÊúÄÂ∑Æ‰ªªÂä°ÁöÑÂáÜÁ°ÆÊÄß‰∏äÊòæËëó‰ºò‰∫é‰º†ÁªüÁöÑGRPOÂíåDAPOÔºåÂêåÊó∂Âú®ËÆ≠ÁªÉÊïàÁéá‰∏ä‰πüÊúâÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05857', 'title': 'BABE: Biology Arena BEnchmark', 'url': 'https://huggingface.co/papers/2602.05857', 'abstract': "BABE is a biology-focused benchmark designed to evaluate AI systems' ability to perform experimental reasoning and causal inference similar to practicing scientists.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.", 'score': 4, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '0da6fcfe7d439cf7', 'authors': ['Junting Zhou', 'Jin Chen', 'Linfeng Hao', 'Denghui Cao', 'Zheyu Wang', 'Qiguang Chen', 'Chaoyou Fu', 'Jiaze Chen', 'Yuchen Wu', 'Ge Zhang', 'Mingxuan Wang', 'Wenhao Huang', 'Tong Yang'], 'affiliations': ['ByteDance Seed', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2602.05857.jpg', 'data': {'categories': ['#benchmark', '#science', '#reasoning', '#dataset'], 'emoji': 'üß¨', 'ru': {'title': '–ù–∞—É—á–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –æ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∫ –æ—Ç–∫—Ä—ã—Ç–∏—è–º', 'desc': 'BABE ‚Äî —ç—Ç–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI —Å–∏—Å—Ç–µ–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑, –ø–æ–¥–æ–±–Ω–æ –Ω–∞—Å—Ç–æ—è—â–∏–º —É—á—ë–Ω—ã–º-–±–∏–æ–ª–æ–≥–∞–º. –ó–∞–¥–∞—á–∏ –≤ –±–µ–Ω—á–º–∞—Ä–∫–µ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã—Ö –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏. –°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —É–º–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤, –∞ —Ç–∞–∫–∂–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø—Ä–∏—á–∏–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –∫—Ä–æ—Å—Å-–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –≤—ã–≤–æ–¥. –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–¥—ë–∂–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ LLM –º–æ–≥—É—Ç —É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö.'}, 'en': {'title': "Evaluating AI's Scientific Reasoning with BABE", 'desc': "The paper introduces BABE, a benchmark specifically designed to evaluate AI systems on their ability to perform experimental reasoning and causal inference in biology. It highlights the limitations of existing benchmarks that do not adequately test the integration of experimental results with contextual knowledge, which is crucial for scientific research. BABE is constructed from real-world biological studies and peer-reviewed papers, making it relevant to actual scientific practices. This benchmark aims to provide a more authentic assessment of AI's reasoning capabilities, thereby enhancing its potential contributions to biological research."}, 'zh': {'title': 'BABEÔºöËØÑ‰º∞AIÂú®ÁîüÁâ©Á†îÁ©∂‰∏≠ÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': 'BABEÊòØ‰∏Ä‰∏™‰∏ìÊ≥®‰∫éÁîüÁâ©Â≠¶ÁöÑÂü∫ÂáÜÔºåÊó®Âú®ËØÑ‰º∞‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂú®ÂÆûÈ™åÊé®ÁêÜÂíåÂõ†ÊûúÊé®Êñ≠ÊñπÈù¢ÁöÑËÉΩÂäõÔºåÁ±ª‰ºº‰∫éÁßëÂ≠¶ÂÆ∂ÁöÑÂÆûË∑µËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÁîüÁâ©Â≠¶Âü∫ÂáÜÂæÄÂæÄÊó†Ê≥ïÊúâÊïàËØÑ‰º∞Á†îÁ©∂‰∫∫ÂëòÊâÄÈúÄÁöÑÂÖ≥ÈîÆÊäÄËÉΩÔºåÂç≥Â∞ÜÂÆûÈ™åÁªìÊûú‰∏éËÉåÊôØÁü•ËØÜÁªìÂêà‰ª•ÂæóÂá∫ÊúâÊÑè‰πâÁöÑÁªìËÆ∫„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊàë‰ª¨Êé®Âá∫‰∫ÜBABEÂü∫ÂáÜÔºåÂÆÉÁî±ÂêåË°åËØÑÂÆ°ÁöÑÁ†îÁ©∂ËÆ∫ÊñáÂíåÁúüÂÆûÁöÑÁîüÁâ©Â≠¶Á†îÁ©∂ÊûÑÊàêÔºåÁ°Æ‰øù‰ªªÂä°ÂèçÊò†ÂÆûÈôÖÁßëÂ≠¶Á†îÁ©∂ÁöÑÂ§çÊùÇÊÄßÂíåË∑®Â≠¶ÁßëÁâπÊÄß„ÄÇBABEÊåëÊàòÊ®°ÂûãËøõË°åÂõ†ÊûúÊé®ÁêÜÂíåË∑®Â∞∫Â∫¶Êé®Êñ≠Ôºå‰∏∫ËØÑ‰º∞‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÂú®ÁîüÁâ©Á†îÁ©∂‰∏≠ÁöÑÊΩúÂú®Ë¥°ÁåÆÊèê‰æõ‰∫ÜÊõ¥ÁúüÂÆûÁöÑË°°ÈáèÊ†áÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01965', 'title': 'Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2602.01965', 'abstract': 'CatRAG addresses limitations in retrieval-augmented generation by introducing a query-adaptive framework that improves multi-hop reasoning through symbolic anchoring, dynamic edge weighting, and key-fact passage enhancement.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a "Static Graph Fallacy": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree "hub" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query\'s intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.', 'score': 4, 'issue_id': 933, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '200f61208281797d', 'authors': ['Kwun Hang Lau', 'Fangyuan Zhang', 'Boyu Ruan', 'Yingli Zhou', 'Qintian Guo', 'Ruiyuan Zhang', 'Xiaofang Zhou'], 'affiliations': ['Huawei Hong Kong Research Center', 'The Chinese University of Hong Kong, Shenzhen', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2602.01965.jpg', 'data': {'categories': ['#graphs', '#reasoning', '#benchmark', '#open_source', '#rag'], 'emoji': 'üß≠', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –≥—Ä–∞—Ñ–∞–º –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': "–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è CatRAG ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–π –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (RAG), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å—Ç–∞—Ç–∏—á–Ω–æ—Å—Ç–∏ –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π –ø—É—Ç—ë–º –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞ 'Static Graph Fallacy' ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–∏–∑–º–µ–Ω—è–µ–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –≤ –≥—Ä–∞—Ñ–µ, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—é –ø–æ–∏—Å–∫–∞ –≤ —É–∑–ª—ã —Å –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞: —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–µ —è–∫–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –æ–±—Ö–æ–¥–∞ –≥—Ä–∞—Ñ–∞, –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Ä—ë–±–µ—Ä —Å —É—á—ë—Ç–æ–º –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—É—Ç–µ–π, –∏ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —á–µ—Ç—ã—Ä—ë—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤ –ø–æ–ª–Ω–æ—Ç–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ü–µ–ø–æ—á–µ–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤."}, 'en': {'title': 'CatRAG: Enhancing Multi-Hop Reasoning with Query-Adaptive Navigation', 'desc': 'CatRAG is a novel framework designed to enhance retrieval-augmented generation (RAG) by addressing the limitations of static graph structures in multi-hop reasoning. It introduces a query-adaptive navigation system that allows for dynamic edge weighting, which adjusts the relevance of graph connections based on the specific query. Additionally, CatRAG employs symbolic anchoring to guide the random walk and key-fact passage enhancement to ensure that the model retrieves complete evidence chains. Experimental results show that CatRAG significantly improves reasoning completeness, allowing for more effective retrieval of necessary information for complex queries.'}, 'zh': {'title': 'CatRAGÔºöÊèêÂçáÂ§öË∑≥Êé®ÁêÜÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÊ°ÜÊû∂', 'desc': 'CatRAGÊòØ‰∏ÄÁßçÊîπËøõÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Â§öË∑≥Êé®ÁêÜ‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆÉÈÄöËøáÂºïÂÖ•Á¨¶Âè∑ÈîöÂÆö„ÄÅÂä®ÊÄÅËæπÊùÉÈáçÂíåÂÖ≥ÈîÆ‰∫ãÂÆûÊÆµËêΩÂ¢ûÂº∫Á≠âÊäÄÊúØÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Êü•ËØ¢Ëá™ÈÄÇÂ∫îÁöÑÊ°ÜÊû∂„ÄÇËØ•Ê°ÜÊû∂ËÉΩÂ§üÊ†πÊçÆÊü•ËØ¢ÁöÑÈúÄÊ±ÇÂä®ÊÄÅË∞ÉÊï¥ÂõæÁªìÊûÑÔºå‰ªéËÄåÊèêÈ´òÊ£ÄÁ¥¢ÁöÑÁõ∏ÂÖ≥ÊÄßÂíåÂÆåÊï¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCatRAGÂú®Â§öË∑≥Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜÊé®ÁêÜÁöÑÂÆåÊï¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05393', 'title': 'Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better', 'url': 'https://huggingface.co/papers/2602.05393', 'abstract': "Large language models can be trained more efficiently by transferring knowledge from later training phases to earlier layers during initial training, achieving faster convergence and improved performance.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: Can we leverage existing small pretrained models to accelerate the training of larger models? In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6times speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10times fewer parameters than the target model.", 'score': 3, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'ec287a16b1d95553', 'authors': ['Ji Zhao', 'Yufei Gu', 'Shitong Shao', 'Xun Zhou', 'Liang Xiang', 'Zeke Xie'], 'affiliations': ['ByteDance Seed', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2602.05393.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#optimization', '#architecture'], 'emoji': '‚ö°', 'ru': {'title': '–ü–µ—Ä–µ–¥–∞—á–∞ –∑–Ω–∞–Ω–∏–π –æ—Ç –ø–æ–∑–¥–Ω–∏—Ö —Ñ–∞–∑ –∫ —Ä–∞–Ω–Ω–∏–º: —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ Late-to-Early Training (LET), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –ø–µ—Ä–µ–¥–∞–≤–∞—è –∑–Ω–∞–Ω–∏—è —Å –ø–æ–∑–¥–Ω–∏—Ö —Å–ª–æ—ë–≤ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ –æ–±—É—á–∞–µ–º–æ–π –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞: –æ–±—É—á–µ–Ω–∏–µ –æ—Ç –ø–æ–∑–¥–Ω–∏—Ö –∫ —Ä–∞–Ω–Ω–∏–º —à–∞–≥–∞–º –∏ –æ—Ç –ø–æ–∑–¥–Ω–∏—Ö –∫ —Ä–∞–Ω–Ω–∏–º —Å–ª–æ—è–º –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ LET –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –≤ 1.6 —Ä–∞–∑–∞ –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —É–ª—É—á—à–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ 5% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º. –ü–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∞–∂–µ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –≤ 10 —Ä–∞–∑ –º–µ–Ω—å—à–µ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Accelerating LLM Training with Late-to-Early Knowledge Transfer', 'desc': 'This paper introduces a new training approach called Late-to-Early Training (LET) for Large Language Models (LLMs). The method allows early layers of the model to learn from the knowledge acquired in later layers of a pretrained model, which helps in speeding up the training process. By utilizing representations from late layers, LET enhances both the convergence speed and the overall performance of the model on various tasks. Experimental results show that this approach can significantly reduce training time while improving accuracy, demonstrating its effectiveness in leveraging smaller pretrained models for training larger ones.'}, 'zh': {'title': 'ÊôöÂà∞Êó©ËÆ≠ÁªÉÔºöÂä†ÈÄüÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÁßòÂØÜÊ≠¶Âô®', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉÊñπÊ≥ïÔºåÁß∞‰∏∫ÊôöÂà∞Êó©ËÆ≠ÁªÉÔºàLETÔºâÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéá„ÄÇÈÄöËøáÂ∞ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂêéÊúüÁöÑÁü•ËØÜËΩ¨ÁßªÂà∞Êó©ÊúüÂ±ÇÔºåLETÂèØ‰ª•Âä†ÈÄüÊ®°ÂûãÁöÑÊî∂ÊïõÈÄüÂ∫¶Âπ∂ÊèêÂçáÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåLETÁöÑÊúâÊïàÊÄß‰∏ªË¶Å‰æùËµñ‰∫é‰∏§‰∏™Êú∫Âà∂ÔºöÊôöÂà∞Êó©Ê≠•Â≠¶‰π†ÂíåÊôöÂà∞Êó©Â±ÇÂ≠¶‰π†„ÄÇËøôÁßçÊñπÊ≥ïÂú®ËÆ≠ÁªÉ1.4BÂíå7BÂèÇÊï∞Ê®°ÂûãÊó∂Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊïàÁéáÂíåÊïàÊûúÔºåËÉΩÂ§üÂú®‰øùÊåÅËæÉÈ´òÂáÜÁ°ÆÁéáÁöÑÂêåÊó∂ÂÆûÁé∞ËÆ≠ÁªÉÈÄüÂ∫¶ÁöÑÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05258', 'title': 'CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs', 'url': 'https://huggingface.co/papers/2602.05258', 'abstract': 'CoPE introduces a soft clipping method for Rotary Positional Embedding that unifies out-of-distribution mitigation and semantic modeling while enabling effective long-context processing up to 256k length.  \t\t\t\t\tAI-generated summary \t\t\t\t Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.', 'score': 3, 'issue_id': 946, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'd413bf991fd66e9d', 'authors': ['Haoran Li', 'Sucheng Ren', 'Alan Yuille', 'Feng Wang'], 'affiliations': ['Carnegie Mellon University', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2602.05258.jpg', 'data': {'categories': ['#long_context', '#open_source'], 'emoji': 'üìè', 'ru': {'title': '–ú—è–≥–∫–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ CoPE –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–æ—Ç–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (RoPE), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º—è–≥–∫–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ RoPE –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞: —Å–º—è–≥—á–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–±–æ—Ä—å–±–∞ —Å –≤—ã–±—Ä–æ—Å–∞–º–∏ –≤–Ω–µ –æ–±—É—á–∞—é—â–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è) –∏ —É–ª—É—á—à–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤). –≠—Ç–æ—Ç –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–π —É—Ç–µ—á–∫–∏ –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö –¥–ª–∏–Ω–æ–π –¥–æ 256 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ CoPE –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–∞ –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ –æ–±–æ–±—â–µ–Ω–∏—è –Ω–∞ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'CoPE: Unifying OOD Mitigation and Semantic Modeling for Long Contexts', 'desc': "CoPE presents a novel soft clipping technique for Rotary Positional Embedding (RoPE) that effectively addresses both out-of-distribution (OOD) challenges and semantic modeling in large language models. By soft clipping low-frequency components of RoPE, CoPE enhances the model's ability to process long contexts, extending up to 256k tokens. This method not only mitigates OOD issues but also improves the attention mechanism by prioritizing semantically similar tokens. Experimental results confirm that CoPE significantly boosts performance, establishing it as a leading approach for length generalization in language models."}, 'zh': {'title': 'CoPEÔºöÁªü‰∏ÄÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜ‰∏éËØ≠‰πâÂª∫Ê®°ÁöÑËΩØÂâ™ÂàáÊñπÊ≥ï', 'desc': 'CoPEÊèêÂá∫‰∫Ü‰∏ÄÁßçËΩØÂâ™ÂàáÊñπÊ≥ïÔºåÁî®‰∫éÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÔºåÊó®Âú®Áªü‰∏ÄÂ§ÑÁêÜÂàÜÂ∏ÉÂ§ñÔºàOODÔºâÈóÆÈ¢òÂíåËØ≠‰πâÂª∫Ê®°ÔºåÂêåÊó∂ÊúâÊïàÂ§ÑÁêÜÈïøËææ256kÁöÑ‰∏ä‰∏ãÊñá„ÄÇRoPEÊòØÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠‰∏ä‰∏ãÊñáÊâ©Â±ïÁöÑÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜ„ÄÇÈÄöËøáÂØπRoPEÁöÑ‰ΩéÈ¢ëÊàêÂàÜËøõË°åËΩØÂâ™ÂàáÔºåCoPEÊ∂àÈô§‰∫ÜOODÂºÇÂ∏∏ÂÄºÔºå‰ºòÂåñ‰∫ÜËØ≠‰πâ‰ø°Âè∑ÔºåÂπ∂Èò≤Ê≠¢‰∫ÜÁ°¨Ââ™ÂàáÈÄ†ÊàêÁöÑÈ¢ëË∞±Ê≥ÑÊºè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂ∫îÁî®Êàë‰ª¨ÁöÑËΩØÂâ™ÂàáÁ≠ñÁï•ÂêéÔºåRoPEÂú®Èïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜ‰∏äÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩÔºåÈ™åËØÅ‰∫ÜÊàë‰ª¨ÁöÑÁêÜËÆ∫ÂàÜÊûêÔºåÂπ∂Á°ÆÁ´ã‰∫ÜCoPE‰Ωú‰∏∫ÈïøÂ∫¶Ê≥õÂåñÁöÑÊñ∞ÊúÄ‰ºòËß£„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02393', 'title': 'Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory', 'url': 'https://huggingface.co/papers/2602.02393', 'abstract': "Infinite-World is a robust interactive world model that maintains coherent visual memory over 1000+ frames through hierarchical pose-free memory compression, uncertainty-aware action labeling, and revisit-dense fine-tuning strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.", 'score': 3, 'issue_id': 944, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '1d6241b1a6374ec1', 'authors': ['Ruiqi Wu', 'Xuanhua He', 'Meng Cheng', 'Tianyu Yang', 'Yong Zhang', 'Zhuoliang Kang', 'Xunliang Cai', 'Xiaoming Wei', 'Chunle Guo', 'Chongyi Li', 'Ming-Ming Cheng'], 'affiliations': ['Meituan', 'NKIARI, Shenzhen Futian leader', 'The Hong Kong University of Science and Technology', 'VCIP, CS, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02393.jpg', 'data': {'categories': ['#synthetic', '#video', '#training', '#long_context', '#architecture'], 'emoji': 'üåç', 'ru': {'title': '–ú–∏—Ä–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å –≤–µ—á–Ω–æ–π –ø–∞–º—è—Ç—å—é –±–µ–∑ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Infinite-World ‚Äî –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –º–∏—Ä–æ–≤—É—é –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—É—é –≤–∏–∑—É–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –±–æ–ª–µ–µ 1000 –∫–∞–¥—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö. –ö–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç ‚Äî –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä –ø–∞–º—è—Ç–∏ –±–µ–∑ —è–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (HPMC), –∫–æ—Ç–æ—Ä—ã–π —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–∂–∏–º–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –±—é–¥–∂–µ—Ç–æ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π, –∫–æ—Ç–æ—Ä—ã–π –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ –≤ —Ç—Ä–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º–Ω—ã–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º. –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–ª–æ—Ç–Ω—ã—Ö –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ—Å–µ—â–µ–Ω–∏—è—Ö –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∑–∞–º—ã–∫–∞–Ω–∏—è –ø–µ—Ç–µ–ª—å –Ω–∞ –¥–∞–ª—å–Ω–∏—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è—Ö.'}, 'en': {'title': 'Infinite-World: Mastering Memory in Complex Environments', 'desc': "Infinite-World is an advanced interactive world model designed to effectively manage visual memory across more than 1000 frames in complex environments. It introduces a Hierarchical Pose-free Memory Compressor (HPMC) that compresses historical data into a manageable format, allowing the model to reference past information without relying on precise geometric data. Additionally, the model incorporates an Uncertainty-aware Action Labeling system that categorizes continuous movements into three distinct states, enhancing learning from imperfect video data. Finally, a Revisit-Dense Finetuning Strategy is employed to optimize the model's ability to recognize and utilize long-range spatial relationships, resulting in improved visual quality and action responsiveness."}, 'zh': {'title': 'Êó†Èôê‰∏ñÁïåÔºöË∂ÖË∂äËßÜËßâËÆ∞ÂøÜÁöÑ‰∫§‰∫íÂºèÊ®°Âûã', 'desc': 'Infinite-WorldÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑ‰∫§‰∫íÂºè‰∏ñÁïåÊ®°ÂûãÔºåËÉΩÂ§üÂú®Â§çÊùÇÁöÑÁúüÂÆûÁéØÂ¢É‰∏≠‰øùÊåÅË∂ÖËøá1000Â∏ßÁöÑËøûË¥ØËßÜËßâËÆ∞ÂøÜ„ÄÇÂÆÉÈÄöËøáÂàÜÂ±ÇÊó†ÂßøÊÄÅËÆ∞ÂøÜÂéãÁº©„ÄÅÂü∫‰∫é‰∏çÁ°ÆÂÆöÊÄßÁöÑÂä®‰ΩúÊ†áËÆ∞ÂíåÂØÜÈõÜÈáçËÆøÂæÆË∞ÉÁ≠ñÁï•Êù•ÂÆûÁé∞Ëøô‰∏ÄÁÇπ„ÄÇËØ•Ê®°ÂûãÂºïÂÖ•‰∫ÜÂàÜÂ±ÇÊó†ÂßøÊÄÅËÆ∞ÂøÜÂéãÁº©Âô®ÔºàHPMCÔºâÔºåËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜÂéÜÂè≤‰ø°ÊÅØÊèêÁÇºÊàêÂõ∫ÂÆöÈ¢ÑÁÆóÁöÑË°®Á§∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInfinite-WorldÂú®ËßÜËßâË¥®Èáè„ÄÅÂä®‰ΩúÂèØÊéßÊÄßÂíåÁ©∫Èó¥‰∏ÄËá¥ÊÄßÊñπÈù¢Ë°®Áé∞‰ºòË∂ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05871', 'title': 'Pathwise Test-Time Correction for Autoregressive Long Video Generation', 'url': 'https://huggingface.co/papers/2602.05871', 'abstract': 'Test-Time Correction addresses error accumulation in distilled autoregressive diffusion models for long-video synthesis by using initial frames as reference anchors to calibrate stochastic states during sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.', 'score': 2, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'dea44b7262fc826c', 'authors': ['Xunzhi Xiang', 'Zixuan Duan', 'Guiyu Zhang', 'Haiyu Zhang', 'Zhe Gao', 'Junta Wu', 'Shaofeng Zhang', 'Tengfei Wang', 'Qi Fan', 'Chunchao Guo'], 'affiliations': ['Chinese University of Hong Kong, Shenzhen', 'Nanjing University', 'Tencent Hunyuan', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.05871.jpg', 'data': {'categories': [], 'emoji': 'üé¨', 'ru': {'title': '–ö–æ—Ä—Ä–µ–∫—Ü–∏—è –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –æ–ø–æ—Ä–Ω—ã–µ –∫–∞–¥—Ä—ã', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Test-Time Correction –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –≤ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑–µ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑-–∑–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –ª–∞–Ω–¥—à–∞—Ñ—Ç–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞—á–∞–ª—å–Ω—ã–π –∫–∞–¥—Ä –∫–∞–∫ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –æ–ø–æ—Ä–Ω—ã–π —è–∫–æ—Ä—å –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –≤–¥–æ–ª—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –º–µ—Ç–æ–¥ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –≤–∏–¥–µ–æ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏.'}, 'en': {'title': 'Stabilizing Long Video Synthesis with Test-Time Correction', 'desc': 'This paper presents Test-Time Correction (TTC), a method designed to reduce error accumulation in distilled autoregressive diffusion models when generating long videos. Traditional methods struggle with long sequences due to unstable reward landscapes and sensitivity in distilled parameters. TTC addresses these issues by using the initial frame as a stable reference to adjust the stochastic states during the sampling process. The results show that TTC can effectively extend video generation lengths while maintaining high quality, comparable to more resource-intensive training methods.'}, 'zh': {'title': 'ÊµãËØïÊó∂Ê†°Ê≠£ÔºöÊèêÂçáÈïøËßÜÈ¢ëÂêàÊàêÁöÑÁ®≥ÂÆöÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÊµãËØïÊó∂Ê†°Ê≠£ÔºàTTCÔºâÁöÑÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥ÈïøËßÜÈ¢ëÂêàÊàê‰∏≠Ëí∏È¶èËá™ÂõûÂΩíÊâ©Êï£Ê®°ÂûãÁöÑÈîôËØØÁ¥ØÁßØÈóÆÈ¢ò„ÄÇTTCÂà©Áî®ÂàùÂßãÂ∏ß‰Ωú‰∏∫ÂèÇËÄÉÈîöÁÇπÔºåÂú®ÈááÊ†∑ËøáÁ®ã‰∏≠Ê†°ÂáÜÈöèÊú∫Áä∂ÊÄÅÔºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁöÑÁ®≥ÂÆöÊÄß„ÄÇ‰∏éÁé∞ÊúâÁöÑÊµãËØïÊó∂‰ºòÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåTTC‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÔºåËÉΩÂ§üÊúâÊïàÂª∂ÈïøÁîüÊàêÂ∫èÂàóÁöÑÈïøÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåTTC‰∏éÂ§öÁßçËí∏È¶èÊ®°ÂûãÊó†ÁºùÈõÜÊàêÔºå‰∏îÂú®30ÁßíÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÂÖ∂ÁîüÊàêË¥®Èáè‰∏éËµÑÊ∫êÂØÜÈõÜÂûãËÆ≠ÁªÉÊñπÊ≥ïÁõ∏ÂΩì„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05494', 'title': 'A Unified Framework for Rethinking Policy Divergence Measures in GRPO', 'url': 'https://huggingface.co/papers/2602.05494', 'abstract': 'A unified framework for reinforcement learning with verified reward is presented, characterized by policy divergence measures including likelihood ratios and KL divergences, with empirical validation showing improved training stability and performance through the KL3 estimator.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.', 'score': 2, 'issue_id': 939, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': 'dbe7a9c4d558a042', 'authors': ['Qingyuan Wu', 'Yuhui Wang', 'Simon Sinong Zhan', 'Yanning Dai', 'Shilong Deng', 'Sarra Habchi', 'Qi Zhu', 'Matthias Gall√©', 'Chao Huang'], 'affiliations': ['Cohere', 'KAUST', 'Northwestern University', 'University of Liverpool', 'University of Southampton'], 'pdf_title_img': 'assets/pdf/title_img/2602.05494.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#training', '#alignment'], 'emoji': 'üéØ', 'ru': {'title': '–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ–º –ø–æ–ª–∏—Ç–∏–∫–∏ —á–µ—Ä–µ–∑ KL3-–æ—Ü–µ–Ω–∫—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–π –Ω–∞–≥—Ä–∞–¥–æ–π, –∫–æ—Ç–æ—Ä–∞—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ä—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ–∂–¥—É –ø–æ–ª–∏—Ç–∏–∫–∞–º–∏, –≤–∫–ª—é—á–∞—è –æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –∏ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –ö—É–ª—å–±–∞–∫–∞-–õ–µ–π–±–ª–µ—Ä–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç KL3-–æ—Ü–µ–Ω–∫—É –∫–∞–∫ –∫–ª—é—á–µ–≤–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏, –∫–æ—Ç–æ—Ä–æ–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–º—É –æ–±—Ä–µ–∑–∞–Ω–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—É—é –º–∞—Å—Å—É –≤ –ø–æ–ª—å–∑—É –≤—ã—Å–æ–∫–æ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, —É–ª—É—á—à–∞—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ KL3-–æ—Ü–µ–Ω–∫–∏ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ–≤—ã—à–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LLM.'}, 'en': {'title': 'Enhancing Reinforcement Learning with KL3 for Better Exploration and Stability', 'desc': 'This paper presents a unified framework for reinforcement learning that incorporates verified rewards, focusing on policy divergence measures like likelihood ratios and Kullback-Leibler (KL) divergences. It introduces a new KL3 estimator, which enhances training stability and performance by effectively managing policy divergence during updates. The framework allows for a systematic analysis of how different measures of policy divergence influence exploration strategies and overall performance. Empirical results show that using the KL3 estimator leads to better outcomes in mathematical reasoning tasks, emphasizing the significance of robust policy divergence constraints in reinforcement learning.'}, 'zh': {'title': 'Áªü‰∏ÄÊ°ÜÊû∂ÊèêÂçáÂº∫ÂåñÂ≠¶‰π†ÁöÑÁ®≥ÂÆöÊÄß‰∏éÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰∏ìÊ≥®‰∫éÁªèËøáÈ™åËØÅÁöÑÂ•ñÂä±„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊîøÁ≠ñÂèëÊï£Â∫¶ÈáèÔºàÂ¶Ç‰ººÁÑ∂ÊØîÂíåKLÊï£Â∫¶ÔºâÊù•Ë°®ÂæÅÁé∞ÊúâÊñπÊ≥ïÔºåÂπ∂ÂºïÂÖ•KL3‰º∞ËÆ°Âô®‰ª•ÊèêÈ´òËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåKL3Á∫¶ÊùüÂú®Êï∞Â≠¶‰∏äÁ≠âÂêå‰∫é‰∏ÄÁßç‰∏çÂØπÁß∞ÊØîÁéáÂâ™ÂàáÔºåËÉΩÂ§ü‰øÉËøõÊõ¥Âº∫ÁöÑÊé¢Á¥¢ËÉΩÂäõ„ÄÇÂÆûÈ™åËØÅÊòéÔºåÂ∞ÜKL3‰º∞ËÆ°Âô®Â∫îÁî®‰∫éGRPOÊñπÊ≥ïÊòæËëóÊèêÂçá‰∫ÜËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊúÄÁªàÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04789', 'title': 'Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention', 'url': 'https://huggingface.co/papers/2602.04789', 'abstract': 'Light Forcing introduces a novel sparse attention mechanism for autoregressive video generation that improves efficiency while maintaining quality through chunk-aware growth and hierarchical sparse attention strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose Light Forcing, the first sparse attention solution tailored for AR video generation models. It incorporates a Chunk-Aware Growth mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a Hierarchical Sparse Attention to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\\eg, 84.5 on VBench) and efficiency (\\eg, 1.2{sim}1.3times end-to-end speedup). Combined with FP8 quantization and LightVAE, Light Forcing further achieves a 2.3times speedup and 19.7\\,FPS on an RTX~5090 GPU. Code will be released at https://github.com/chengtao-lv/LightForcing{https://github.com/chengtao-lv/LightForcing}.', 'score': 2, 'issue_id': 937, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': 'a087b06265c19486', 'authors': ['Chengtao Lv', 'Yumeng Shi', 'Yushi Huang', 'Ruihao Gong', 'Shen Ren', 'Wenya Wang'], 'affiliations': ['AUMOVIO Singapore Pte Ltd', 'AUMOVIO-NTU Corporate Lab', 'Beihang University', 'Hong Kong University of Science and Technology', 'Nanyang Technological University', 'Sensetime Research'], 'pdf_title_img': 'assets/pdf/title_img/2602.04789.jpg', 'data': {'categories': [], 'emoji': '‚ö°', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ', 'desc': 'Light Forcing –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º Chunk-Aware Growth, –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ –∫—É—Å–∫–∞ –≤–∏–¥–µ–æ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç–µ–ø–µ–Ω—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ. –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –¥–≤—É—Ö —É—Ä–æ–≤–Ω—è—Ö –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ (–∫–∞–¥—Ä –∏ –±–ª–æ–∫), –ø–æ–∑–≤–æ–ª—è—è –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Light Forcing –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 1.2-1.3 —Ä–∞–∑–∞ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Efficient Video Generation with Sparse Attention', 'desc': 'Light Forcing presents a new sparse attention mechanism specifically designed for autoregressive video generation, enhancing both efficiency and quality. It addresses the challenges of quadratic complexity in attention by introducing a Chunk-Aware Growth strategy that optimally allocates sparsity based on the contribution of each chunk. Additionally, the Hierarchical Sparse Attention captures relevant historical and local context in a structured manner, allowing for adaptive attention patterns. Experimental results show that Light Forcing significantly improves video generation quality and speed compared to existing methods.'}, 'zh': {'title': 'Ëá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥ÔºöLight Forcing', 'desc': 'Light Forcing ÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÊú∫Âà∂Ôºå‰∏ì‰∏∫Ëá™ÂõûÂΩíËßÜÈ¢ëÁîüÊàêÊ®°ÂûãËÆæËÆ°ÔºåÊó®Âú®ÊèêÈ´òÊïàÁéáÂπ∂‰øùÊåÅÁîüÊàêË¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂùóÊÑüÁü•Â¢ûÈïøÊú∫Âà∂ÔºåÈáèÂåñÊØè‰∏™ÂùóÁöÑË¥°ÁåÆÔºå‰ªéËÄå‰ºòÂåñÁ®ÄÁñèÂàÜÈÖçÔºåÂπ∂Âú®ÁîüÊàêËøáÁ®ã‰∏≠ÁªßÊâøÂÖàÂâçÂùóÁöÑÁü•ËØÜ„ÄÇÊ≠§Â§ñÔºåÂ±ÇÊ¨°Á®ÄÁñèÊ≥®ÊÑèÂäõÁ≠ñÁï•ËÉΩÂ§ü‰ª•Á≤óÂà∞ÁªÜÁöÑÊñπÂºèÊçïÊçâÂéÜÂè≤ÂíåÂ±ÄÈÉ®‰∏ä‰∏ãÊñáÔºåÈÄÇÂ∫îÂ§öÊ†∑ÁöÑÊ≥®ÊÑèÂäõÊ®°Âºè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLight Forcing Âú®Ë¥®ÈáèÂíåÊïàÁéá‰∏äÂùá‰ºò‰∫éÁé∞ÊúâÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïÔºåÊòæËëóÊèêÂçá‰∫ÜËßÜÈ¢ëÁîüÊàêÁöÑÈÄüÂ∫¶ÂíåÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05933', 'title': 'Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training', 'url': 'https://huggingface.co/papers/2602.05933', 'abstract': 'Policy mirror descent with mean approximation addresses challenges in training large language models by using adaptive regularization for more stable and efficient reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--œá^2 regularizer. This additional œá^2 regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.', 'score': 1, 'issue_id': 939, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '513abb3b25d97472', 'authors': ['Zhenghao Xu', 'Qin Lu', 'Changlong Yu', 'Tuo Zhao'], 'affiliations': ['Amazon', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2602.05933.jpg', 'data': {'categories': ['#rl', '#reasoning', '#open_source', '#optimization', '#training', '#alignment'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º Policy Mirror Descent —Å –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–µ–π —Å—Ä–µ–¥–Ω–µ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PMD-mean) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PMD-mean –Ω–µ—è–≤–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –∑–µ—Ä–∫–∞–ª—å–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–º–µ—à–∞–Ω–Ω–æ–π KL-œá¬≤ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–ª–∞–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–º–∏ –∏ —É—Å—Ç–æ–π—á–∏–≤—ã–º–∏ –∫ –æ—à–∏–±–∫–∞–º –æ—Ü–µ–Ω–∫–∏. –ö–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç –ª–æ–≥–∞—Ä–∏—Ñ–º —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å—Ä–µ–¥–Ω–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ, –∏–∑–±–µ–≥–∞—è —Å–ª–æ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º —á–∏—Å–ª–æ–º –ø—Ä–æ–±—Ä–æ—Å–æ–≤ –≤ –æ–≥—Ä–æ–º–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –¥–µ–π—Å—Ç–≤–∏–π LLM. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —É–ª—É—á—à–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–∞.'}, 'en': {'title': 'Stabilizing Reinforcement Learning for Large Language Models with PMD-Mean', 'desc': 'This paper introduces Policy Mirror Descent with Mean Approximation (PMD-mean) as a solution for training large language models (LLMs) using reinforcement learning (RL). PMD-mean improves stability and efficiency by employing adaptive regularization, specifically a mixed KL-œá¬≤ regularizer, which helps manage large probability changes during updates. The method approximates the log-partition function using mean rewards, allowing for effective regression in log-policy space despite limited data. Experimental results demonstrate that PMD-mean outperforms existing methods in math reasoning tasks, highlighting its potential for enhancing RL algorithms in LLMs.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÊ≠£ÂàôÂåñÊèêÂçáÂ§ßËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉÊïàÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÊîøÁ≠ñÈïúÂÉè‰∏ãÈôçÔºàPMDÔºâÁöÑÁÆóÊ≥ïÔºåÊó®Âú®ÈÄöËøáËá™ÈÄÇÂ∫îÊ≠£ÂàôÂåñÊù•ÊèêÈ´òÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÂíåÊïàÁéá„ÄÇPMDÈÄöËøáËø≠‰ª£Ëß£ÂÜ≥KLÊ≠£ÂàôÂåñÁöÑÁ≠ñÁï•ÊîπËøõÂ≠êÈóÆÈ¢òÔºåÈÄÇÁî®‰∫éËÆ≠ÁªÉÂÖàËøõÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂÆûÁî®ÁÆóÊ≥ïPMD-meanÔºåÂÆÉÈÄöËøáÈááÊ†∑Á≠ñÁï•‰∏ãÁöÑÂπ≥ÂùáÂ•ñÂä±Êù•Ëøë‰ººÂØπÊï∞ÂàÜÂå∫ÂáΩÊï∞Ôºå‰ªéËÄåÂú®ÂØπÊï∞Á≠ñÁï•Á©∫Èó¥‰∏≠ËøõË°åÂõûÂΩí„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPMD-meanÂú®Êï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòË∂äÔºåÂÖ∑ÊúâÊõ¥Â•ΩÁöÑÁ®≥ÂÆöÊÄßÂíåÊó∂Èó¥ÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05551', 'title': 'FastVMT: Eliminating Redundancy in Video Motion Transfer', 'url': 'https://huggingface.co/papers/2602.05551', 'abstract': 'FastVMT accelerates video motion transfer by addressing computational redundancies in Diffusion Transformer architecture through localized attention masking and gradient reuse optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.', 'score': 1, 'issue_id': 933, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '1db2e27de2507ab1', 'authors': ['Yue Ma', 'Zhikai Wang', 'Tianhao Ren', 'Mingzhe Zheng', 'Hongyu Liu', 'Jiayi Guo', 'Mark Fong', 'Yuxuan Xue', 'Zixiang Zhao', 'Konrad Schindler', 'Qifeng Chen', 'Linfeng Zhang'], 'affiliations': ['EPIC Lab, SJTU', 'ETH Zurich', 'HKUST', 'Meta', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2602.05551.jpg', 'data': {'categories': ['#architecture', '#optimization', '#inference', '#video', '#diffusion'], 'emoji': '‚ö°', 'ru': {'title': '–õ–æ–∫–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ', 'desc': '–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ —É—Å–∫–æ—Ä–µ–Ω–∏—é —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –¥–≤–∏–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∏ –≤–∏–¥–µ–æ-–æ–±—Ä–∞–∑—Ü–æ–≤. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –¥–≤–µ —Ç–∏–ø–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Diffusion Transformer: –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ –¥–≤–∏–∂–µ–Ω–∏–µ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏ –ø–ª–∞–≤–Ω–æ–µ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ, –∞ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ ‚Äî –∏–∑-–∑–∞ –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–¥–æ–ª—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –î–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ —Ç–∏–ø–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è (attention masking), –∫–æ—Ç–æ—Ä–∞—è –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Ç–æ–ª—å–∫–æ —Å–æ—Å–µ–¥–Ω–∏–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –î–ª—è –≤—Ç–æ—Ä–æ–≥–æ —Ç–∏–ø–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å—Ö–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤ –∏ –ø—Ä–æ–ø—É—Å–∫–∞—é—â–∞—è –Ω–µ–Ω—É–∂–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, —á—Ç–æ –¥–∞—ë—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 3.43 —Ä–∞–∑–∞ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Accelerating Video Motion Transfer with FastVMT', 'desc': 'FastVMT is a method that speeds up video motion transfer by improving the efficiency of the Diffusion Transformer architecture. It does this by addressing two main types of computational redundancies: motion redundancy and gradient redundancy. By using localized attention masking, it focuses on nearby frames instead of distant ones, reducing unnecessary calculations. Additionally, it reuses gradients from previous steps to avoid redundant computations, resulting in a significant speedup while maintaining high-quality video output.'}, 'zh': {'title': 'Âä†ÈÄüËßÜÈ¢ëËøêÂä®ËΩ¨ÁßªÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'FastVMTÊòØ‰∏ÄÁßçÂä†ÈÄüËßÜÈ¢ëËøêÂä®ËΩ¨ÁßªÁöÑÊñπÊ≥ïÔºåÈÄöËøáÂ±ÄÈÉ®Ê≥®ÊÑèÂäõÊé©ËîΩÂíåÊ¢ØÂ∫¶ÈáçÁî®‰ºòÂåñÊù•Ëß£ÂÜ≥Êâ©Êï£ÂèòÊç¢Âô®Êû∂ÊûÑ‰∏≠ÁöÑËÆ°ÁÆóÂÜó‰ΩôÈóÆÈ¢ò„ÄÇËßÜÈ¢ëËøêÂä®ËΩ¨ÁßªÁöÑÁõÆÊ†áÊòØÊ†πÊçÆÊñáÊú¨ÊèêÁ§∫ÂêàÊàêËßÜÈ¢ëÔºåÂêåÊó∂ËΩ¨ÁßªÂèÇËÄÉËßÜÈ¢ë‰∏≠ÁöÑËøêÂä®Ê®°Âºè„ÄÇËØ•ÊñπÊ≥ïËØÜÂà´Âπ∂Ê∂àÈô§‰∫Ü‰∏§ÁßçËÆ°ÁÆóÂÜó‰ΩôÔºöËøêÂä®ÂÜó‰ΩôÂíåÊ¢ØÂ∫¶ÂÜó‰ΩôÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜËÆ°ÁÆóÊïàÁéá„ÄÇÁªèËøá‰ºòÂåñÔºåFastVMTÂú®‰∏çÈôç‰ΩéÁîüÊàêËßÜÈ¢ëÁöÑËßÜËßâË¥®ÈáèÂíåÊó∂Èó¥‰∏ÄËá¥ÊÄßÁöÑÊÉÖÂÜµ‰∏ãÔºåÂπ≥ÂùáÂÆûÁé∞‰∫Ü3.43ÂÄçÁöÑÂä†ÈÄü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05293', 'title': 'Fast-SAM3D: 3Dfy Anything in Images but Faster', 'url': 'https://huggingface.co/papers/2602.05293', 'abstract': "Fast-SAM3D addresses slow inference in 3D reconstruction by dynamically adapting computation to varying complexity through heterogeneity-aware mechanisms that improve efficiency without sacrificing quality.  \t\t\t\t\tAI-generated summary \t\t\t\t SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the first systematic investigation into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level heterogeneity: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present Fast-SAM3D, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) Modality-Aware Step Caching to decouple structural evolution from sensitive layout updates; (2) Joint Spatiotemporal Token Carving to concentrate refinement on high-entropy regions; and (3) Spectral-Aware Token Aggregation to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to 2.67times end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.", 'score': 1, 'issue_id': 939, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '05a90b62b92f1b40', 'authors': ['Weilun Feng', 'Mingqiang Wu', 'Zhiliang Chen', 'Chuanguang Yang', 'Haotong Qin', 'Yuqi Li', 'Xiaokun Liu', 'Guoxin Fan', 'Zhulin An', 'Libo Huang', 'Yulun Zhang', 'Michele Magno', 'Yongjun Xu'], 'affiliations': ['City College of New York, City Univeristy of New York, USA', 'ETH Zurich', 'Institute of Computing Technology, Chinese Academy of Sciences', 'School of Artificial Intelligence, China University of Mining & Technology, Beijing', 'Shanghai Jiao Tong University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2602.05293.jpg', 'data': {'categories': ['#3d', '#inference'], 'emoji': '‚ö°', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ 3D –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ—Å—Ç—å', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Fast-SAM3D ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ø—Ä–∏ 3D —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å—Ü–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ SAM3D –∏ –≤—ã—è–≤–ª—è–µ—Ç —Ö—Ä—É–ø–∫–æ—Å—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∏ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—É—é –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: —Ä–∞–∑–ª–∏—á–∏–µ –≤ —ç–≤–æ–ª—é—Ü–∏–∏ —Ñ–æ—Ä–º—ã –∏ –∫–æ–º–ø–æ–Ω–æ–≤–∫–∏, —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å —É—Ç–æ—á–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç—É—Ä –∏ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏–π. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ç—Ä–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞, –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã–µ –æ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ—Å—Ç–∏: –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —à–∞–≥–æ–≤ —Å —É—á–µ—Ç–æ–º –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏, –≤—ã—Ä–µ–∑–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç—å—é –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏—é —Ç–æ–∫–µ–Ω–æ–≤ —Å —É—á–µ—Ç–æ–º —Å–ø–µ–∫—Ç—Ä–∞. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 2.67 —Ä–∞–∑–∞ –±–µ–∑ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–µ —Ç—Ä–µ–±—É—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Speed Up 3D Reconstruction Without Losing Quality!', 'desc': "Fast-SAM3D is a novel framework designed to enhance the speed of 3D reconstruction while maintaining high quality. It addresses the slow inference times of existing methods by adapting computation based on the complexity of the scene being processed. The framework employs three key mechanisms: it caches steps to separate structural changes from layout updates, focuses refinement on complex areas, and adjusts resolution based on the geometry's characteristics. As a result, Fast-SAM3D achieves significant speed improvements, making it a more efficient solution for real-time 3D generation."}, 'zh': {'title': 'Âä®ÊÄÅÈÄÇÂ∫îËÆ°ÁÆóÔºåÊèêÂçá3DÈáçÂª∫ÊïàÁéá', 'desc': 'Fast-SAM3D ÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥ËÆ°ÁÆóÊù•Ëß£ÂÜ≥ 3D ÈáçÂª∫‰∏≠ÁöÑÊÖ¢Êé®ÁêÜÈóÆÈ¢òÔºåÂà©Áî®ÂºÇË¥®ÊÄßÊÑüÁü•Êú∫Âà∂ÊèêÈ´òÊïàÁéáËÄå‰∏çÁâ∫Áâ≤Ë¥®Èáè„ÄÇËØ•ÊñπÊ≥ïÈ¶ñÊ¨°Á≥ªÁªüÊÄßÂú∞Á†îÁ©∂‰∫Ü SAM3D ÁöÑÊé®ÁêÜÂä®ÊÄÅÔºåÂèëÁé∞ÈÄöÁî®Âä†ÈÄüÁ≠ñÁï•Âú®Ê≠§ËÉåÊôØ‰∏ãË°®Áé∞‰∏ç‰Ω≥„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑ Fast-SAM3D Ê°ÜÊû∂ËÉΩÂ§üÊ†πÊçÆÁû¨Êó∂ÁîüÊàêÂ§çÊùÇÂ∫¶Âä®ÊÄÅÂØπÈΩêËÆ°ÁÆóÔºåÈõÜÊàê‰∫Ü‰∏âÁßçÂºÇË¥®ÊÄßÊÑüÁü•Êú∫Âà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFast-SAM3D ÂÆûÁé∞‰∫ÜÈ´òËææ 2.67 ÂÄçÁöÑÁ´ØÂà∞Á´ØÂä†ÈÄüÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂá†‰πéÊó†ÊçüÁöÑ‰øùÁúüÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.05023', 'title': 'Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?', 'url': 'https://huggingface.co/papers/2602.05023', 'abstract': 'Vision-language models can precisely geolocate images but often fail to align with human privacy expectations, over-disclosing location details in sensitive contexts and being vulnerable to prompt-based attacks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) have demonstrated strong performance in image geolocation, a capability further sharpened by frontier multimodal large reasoning models (MLRMs). This poses a significant privacy risk, as these widely accessible models can be exploited to infer sensitive locations from casually shared photos, often at street-level precision, potentially surpassing the level of detail the sharer consented or intended to disclose. While recent work has proposed applying a blanket restriction on geolocation disclosure to combat this risk, these measures fail to distinguish valid geolocation uses from malicious behavior. Instead, VLMs should maintain contextual integrity by reasoning about elements within an image to determine the appropriate level of information disclosure, balancing privacy and utility. To evaluate how well models respect contextual integrity, we introduce VLM-GEOPRIVACY, a benchmark that challenges VLMs to interpret latent social norms and contextual cues in real-world images and determine the appropriate level of location disclosure. Our evaluation of 14 leading VLMs shows that, despite their ability to precisely geolocate images, the models are poorly aligned with human privacy expectations. They often over-disclose in sensitive contexts and are vulnerable to prompt-based attacks. Our results call for new design principles in multimodal systems to incorporate context-conditioned privacy reasoning.', 'score': 1, 'issue_id': 934, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '202de029de5e81a0', 'authors': ['Ruixin Yang', 'Ethan Mendes', 'Arthur Wang', 'James Hays', 'Sauvik Das', 'Wei Xu', 'Alan Ritter'], 'affiliations': ['Carnegie Mellon University', 'Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2602.05023.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#security', '#ethics', '#cv', '#multimodal'], 'emoji': 'üîí', 'ru': {'title': '–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å: –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏ –∏ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏, —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É–º–µ—Å—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —Ä–∞—Å–∫—Ä—ã—Ç–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Å—Ç–µ —Å—ä—ë–º–∫–∏. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —É–≤–∞–∂–∞—Ç—å –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å–æ–∑–¥–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫ VLM-GEOPRIVACY, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç–µ–ø–µ–Ω—è–º–∏ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ vision-language –º–æ–¥–µ–ª–∏ —á–∞—Å—Ç–æ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç –∏–∑–ª–∏—à–Ω–∏–µ –¥–µ—Ç–∞–ª–∏ –æ –º–µ—Å—Ç–æ—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–∏ –∏ —É—è–∑–≤–∏–º—ã –∫ –∞—Ç–∞–∫–∞–º —á–µ—Ä–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Balancing Geolocation Accuracy with Privacy in Vision-Language Models', 'desc': 'This paper discusses the challenges of vision-language models (VLMs) in geolocating images while respecting human privacy. Although these models can accurately determine the location of images, they often reveal more location details than users intend, especially in sensitive situations. The authors propose a new benchmark, VLM-GEOPRIVACY, to assess how well these models understand and respect social norms regarding privacy. The findings indicate that current VLMs frequently misalign with privacy expectations, highlighting the need for improved design principles that incorporate context-aware privacy reasoning.'}, 'zh': {'title': 'Âπ≥Ë°°ÈöêÁßÅ‰∏éÂÆûÁî®ÊÄßÔºöËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñ∞ÊåëÊàò', 'desc': 'ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMsÔºâÂú®ÂõæÂÉèÂú∞ÁêÜÂÆö‰ΩçÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®ÈöêÁßÅ‰øùÊä§ÊñπÈù¢Â≠òÂú®‰∏çË∂≥ÔºåÂ∏∏Â∏∏Âú®ÊïèÊÑüÂú∫ÂêàËøáÂ∫¶Êä´Èú≤‰ΩçÁΩÆ‰ø°ÊÅØ„ÄÇÂ∞ΩÁÆ°ÊúâÊèêËÆÆÂØπÂú∞ÁêÜÂÆö‰Ωç‰ø°ÊÅØËøõË°åÂÖ®Èù¢ÈôêÂà∂Ôºå‰ΩÜËøô‰∫õÊé™ÊñΩÊó†Ê≥ïÊúâÊïàÂå∫ÂàÜÂêàÊ≥ïÁî®ÈÄî‰∏éÊÅ∂ÊÑèË°å‰∏∫„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜVLM-GEOPRIVACYÔºåÊó®Âú®ËØÑ‰º∞Ê®°ÂûãÂ¶Ç‰ΩïÁêÜËß£Á§æ‰ºöËßÑËåÉÂíå‰∏ä‰∏ãÊñáÁ∫øÁ¥¢Ôºå‰ª•ÂÜ≥ÂÆöÈÄÇÂΩìÁöÑ‰ø°ÊÅØÊä´Èú≤Ê∞¥Âπ≥„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°Ëøô‰∫õÊ®°ÂûãËÉΩÂ§üÁ≤æÁ°ÆÂÆö‰ΩçÂõæÂÉèÔºå‰ΩÜÂÆÉ‰ª¨‰∏é‰∫∫Á±ªÈöêÁßÅÊúüÊúõ‰πãÈó¥Â≠òÂú®ËæÉÂ§ßÂ∑ÆË∑ùÔºå‰∫üÈúÄÂú®Â§öÊ®°ÊÄÅÁ≥ªÁªüËÆæËÆ°‰∏≠ÂºïÂÖ•‰∏ä‰∏ãÊñáÊù°‰ª∂ÁöÑÈöêÁßÅÊé®ÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04683', 'title': 'UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization', 'url': 'https://huggingface.co/papers/2602.04683', 'abstract': 'Researchers developed a discrete audio codec called ReasoningCodec that separates audio into reasoning and reconstruction tokens for improved understanding and generation, and created UniAudio 2.0, a unified autoregressive model trained on large-scale text and audio data that shows strong performance across various audio tasks and generalizes well in few-shot and zero-shot scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t We study two foundational problems in audio language models: (1) how to design an audio tokenizer that can serve as an intermediate representation for both understanding and generation; and (2) how to build an audio foundation model that generalizes in few-shot and zero-shot settings, analogous to large language models. To this end, we make the following two contributions. First, we propose ReasoningCodec, a discrete audio codec that factorizes audio into (i) reasoning tokens, which encode text-aligned, high-level analysis and planning representations for audio understanding and hierarchical generation, and (ii) reconstruction tokens, which encode semantic-rich acoustic cues for high-fidelity waveform reconstruction. This design achieves understanding performance comparable to strong continuous representations while improving generation quality and reconstruction fidelity over prior discrete tokenizers. Second, we introduce a unified autoregressive architecture for text and audio, together with multi-stage training and multi-task data construction. Using this framework, we train UniAudio 2.0 on 100B text tokens and 60B audio tokens. Across a wide range of speech, sound, and music tasks, UniAudio 2.0 performs competitively on in-domain evaluations and demonstrates strong few-shot and zero-shot generalization to unseen tasks. Demo, code, and checkpoints will be available at https://dongchaoyang.top/UniAudio2Demo/{https://dongchaoyang.top/UniAudio2Demo/}.', 'score': 1, 'issue_id': 933, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '55494e3e5622052b', 'authors': ['Dongchao Yang', 'Yuanyuan Wang', 'Dading Chong', 'Songxiang Liu', 'Xixin Wu', 'Helen Meng'], 'affiliations': ['Independent', 'The Chinese University of Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.04683.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#open_source', '#training', '#audio', '#multimodal'], 'emoji': 'üéµ', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞—É–¥–∏–æ–º–æ–¥–µ–ª—å —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π —Ç–æ–∫–µ–Ω–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ ReasoningCodec ‚Äî –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π –∞—É–¥–∏–æ–∫–æ–¥–µ–∫, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª –Ω–∞ —Ç–æ–∫–µ–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ —Ç–æ–∫–µ–Ω—ã —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∑–≤—É–∫–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞–∫ –ø–æ–Ω–∏–º–∞–Ω–∏—è, —Ç–∞–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –∫–æ–¥–µ–∫–∞ —Å–æ–∑–¥–∞–Ω–∞ UniAudio 2.0 ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–≤—Ç–æ—Ä –µ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ 100 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ 60 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö –∞—É–¥–∏–æ—Ç–æ–∫–µ–Ω–æ–≤. –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–µ—á–∏, –∑–≤—É–∫–∞ –∏ –º—É–∑—ã–∫–∏, –≤–∫–ª—é—á–∞—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, —Å–∏–Ω—Ç–µ–∑ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ. UniAudio 2.0 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±–æ–±—â–µ–Ω–∏—è –≤ —Ä–µ–∂–∏–º–∞—Ö few-shot –∏ zero-shot, –ø–æ–∑–≤–æ–ª—è—è —Ä–µ—à–∞—Ç—å –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.'}, 'en': {'title': 'Revolutionizing Audio Understanding and Generation with ReasoningCodec and UniAudio 2.0', 'desc': 'This paper introduces ReasoningCodec, a novel discrete audio codec that divides audio into two types of tokens: reasoning tokens for high-level understanding and reconstruction tokens for accurate audio reproduction. The authors also present UniAudio 2.0, a unified autoregressive model that integrates both text and audio data, trained on a massive dataset to enhance its performance across various audio tasks. The model demonstrates impressive capabilities in few-shot and zero-shot learning, allowing it to adapt to new tasks with minimal examples. Overall, this work advances the field of audio language models by improving both understanding and generation of audio content.'}, 'zh': {'title': 'Èü≥È¢ëÁêÜËß£‰∏éÁîüÊàêÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Á†îÁ©∂‰∫∫ÂëòÂºÄÂèë‰∫Ü‰∏ÄÁßçÂêç‰∏∫ReasoningCodecÁöÑÁ¶ªÊï£Èü≥È¢ëÁºñËß£Á†ÅÂô®ÔºåÂÆÉÂ∞ÜÈü≥È¢ëÂàÜÁ¶ª‰∏∫Êé®ÁêÜÂíåÈáçÂª∫Ê†áËÆ∞Ôºå‰ª•ÊèêÈ´òÁêÜËß£ÂíåÁîüÊàêÁöÑÊïàÊûú„ÄÇÂêåÊó∂Ôºå‰ªñ‰ª¨ÂàõÂª∫‰∫ÜUniAudio 2.0ÔºåËøôÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑËá™ÂõûÂΩíÊ®°ÂûãÔºåÂü∫‰∫éÂ§ßËßÑÊ®°ÊñáÊú¨ÂíåÈü≥È¢ëÊï∞ÊçÆËøõË°åËÆ≠ÁªÉÔºåÂú®ÂêÑÁßçÈü≥È¢ë‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂Âú®Â∞ëÈáèÊ†∑Êú¨ÂíåÈõ∂Ê†∑Êú¨Âú∫ÊôØ‰∏≠ÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇReasoningCodecÈÄöËøáÂ∞ÜÈü≥È¢ëÂàÜËß£‰∏∫È´òÂ±ÇÊ¨°ÁöÑÊé®ÁêÜÊ†áËÆ∞ÂíåËØ≠‰πâ‰∏∞ÂØåÁöÑÈáçÂª∫Ê†áËÆ∞ÔºåÊèêÂçá‰∫ÜÈü≥È¢ëÁêÜËß£ÂíåÁîüÊàêÁöÑË¥®Èáè„ÄÇUniAudio 2.0Âú®ËØ≠Èü≥„ÄÅÂ£∞Èü≥ÂíåÈü≥‰πê‰ªªÂä°‰∏≠Ë°®Áé∞Á´û‰∫âÂäõÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Êú™ËßÅ‰ªªÂä°‰∏äÁöÑÂº∫Â§ßÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.04220', 'title': 'Adaptive 1D Video Diffusion Autoencoder', 'url': 'https://huggingface.co/papers/2602.04220', 'abstract': 'A transformer-based video autoencoder with adaptive 1D encoding and diffusion-based decoding addresses limitations of fixed-rate compression and deterministic reconstruction in video compression.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.', 'score': 1, 'issue_id': 938, 'pub_date': '2026-02-04', 'pub_date_card': {'ru': '4 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 4', 'zh': '2Êúà4Êó•'}, 'hash': '4ab8f08ecc4f0857', 'authors': ['Yao Teng', 'Minxuan Lin', 'Xian Liu', 'Shuai Wang', 'Xiao Yang', 'Xihui Liu'], 'affiliations': ['ByteDance Inc.', 'CUHK', 'Nanjing University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2602.04220.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#diffusion', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç One-DVA, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–Ω–∞-–æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ–∞–≤—Ç–æ–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–¥–Ω–æ–º–µ—Ä–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –¥–ª—è —Å–∂–∞—Ç–∏—è –≤–∏–¥–µ–æ. –≠–Ω–∫–æ–¥–µ—Ä —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ dropout. –î–µ–∫–æ–¥–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –≤ –ø–∏–∫—Å–µ–ª—å-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–≥–æ —Å 3D-CNN VAE –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞—Ö —Å–∂–∞—Ç–∏—è, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –∏ –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Adaptive Video Compression with One-DVA: Flexibility Meets Quality', 'desc': 'This paper introduces the One-Dimensional Diffusion Video Autoencoder (One-DVA), a novel approach to video compression that overcomes limitations of traditional video autoencoders. It utilizes a transformer-based architecture for adaptive 1D encoding, allowing for flexible latent representation lengths, which improves efficiency in compressing simple videos. The diffusion-based decoder enhances the reconstruction quality by generating pixel-space videos from the latent representations, addressing issues with detail recovery. Additionally, the model incorporates a two-stage training strategy and regularization techniques to optimize performance for both reconstruction and generative tasks.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îËßÜÈ¢ëÂéãÁº©ÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂèòÊç¢Âô®ÁöÑËßÜÈ¢ëËá™ÁºñÁ†ÅÂô®ÔºåÁß∞‰∏∫One-Dimensional Diffusion Video AutoencoderÔºàOne-DVAÔºâÔºåÊó®Âú®Ëß£ÂÜ≥ËßÜÈ¢ëÂéãÁº©‰∏≠ÁöÑÂõ∫ÂÆöÈÄüÁéáÂéãÁº©ÂíåÁ°ÆÂÆöÊÄßÈáçÂª∫ÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•Ê®°ÂûãÈÄöËøáÊü•ËØ¢Âü∫Á°ÄÁöÑËßÜËßâÂèòÊç¢Âô®ÊèêÂèñÊó∂Á©∫ÁâπÂæÅÔºåÂπ∂‰ΩøÁî®ÂèØÂèòÈïøÂ∫¶ÁöÑ‰∏¢ÂºÉÊú∫Âà∂Âä®ÊÄÅË∞ÉÊï¥ÊΩúÂú®Ë°®Á§∫ÁöÑÈïøÂ∫¶„ÄÇËß£Á†ÅÂô®ÈááÁî®ÂÉèÁ¥†Á©∫Èó¥Êâ©Êï£ÂèòÊç¢Âô®ÔºåÊ†πÊçÆÊΩúÂú®Ë°®Á§∫ÈáçÂª∫ËßÜÈ¢ë„ÄÇÈÄöËøá‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁ≠ñÁï•ÔºåOne-DVAÂú®ÈáçÂª∫ÊåáÊ†á‰∏ä‰∏é3D-CNNÂèòÂàÜËá™ÁºñÁ†ÅÂô®Ë°®Áé∞Áõ∏ÂΩìÔºåÂêåÊó∂ÊîØÊåÅËá™ÈÄÇÂ∫îÂéãÁº©ÔºåËÉΩÂ§üÂÆûÁé∞Êõ¥È´òÁöÑÂéãÁº©ÊØî„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23174', 'title': 'Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization', 'url': 'https://huggingface.co/papers/2601.23174', 'abstract': 'DyCAST is a dynamic speech tokenizer that uses soft character-level alignment and duration modeling to enable variable-frame-rate tokenization, improving speech resynthesis quality with fewer tokens than traditional fixed-frame-rate codecs.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast.', 'score': 1, 'issue_id': 934, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '5b9e55aeb6df6120', 'authors': ['Luca Della Libera', 'Cem Subakan', 'Mirco Ravanelli'], 'affiliations': ['Concordia University, Montreal', 'Mila Quebec AI Institute, Montreal, Canada', 'Universite Laval, Quebec, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2601.23174.jpg', 'data': {'categories': ['#audio', '#rag', '#training'], 'emoji': 'üéµ', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ä–µ—á–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': 'DyCAST ‚Äî —ç—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º—è–≥–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤ –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —á–∞—Å—Ç–æ—Ç—ã –∫–∞–¥—Ä–æ–≤. –°–∏—Å—Ç–µ–º–∞ —Å–≤—è–∑—ã–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã —Å –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä—è–º–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–µ—á–∏ –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ö–∞–Ω–∏–∑–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø–æ–∏—Å–∫–æ–º, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –±–µ–∑ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –±–∏—Ç—Ä–µ–π—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ DyCAST –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ–∑–∞ —Ä–µ—á–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤, —á–µ–º —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–¥–µ–∫–∏ —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —á–∞—Å—Ç–æ—Ç–æ–π.'}, 'en': {'title': 'Dynamic Tokenization for Enhanced Speech Quality', 'desc': 'DyCAST is a novel speech tokenizer that enhances the quality of speech resynthesis by utilizing soft character-level alignment and duration modeling. Unlike traditional codecs that use fixed frame rates, DyCAST allows for variable-frame-rate tokenization, which results in shorter token sequences without sacrificing quality. The model learns to link tokens with character-level units during training and can adjust token durations during decoding, leading to more efficient processing. Additionally, a retrieval-augmented decoding mechanism is introduced to improve the fidelity of the reconstructed speech while maintaining a low bitrate.'}, 'zh': {'title': 'DyCASTÔºöÂä®ÊÄÅËØ≠Èü≥ÂàÜËØçÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'DyCASTÊòØ‰∏ÄÁßçÂä®ÊÄÅËØ≠Èü≥ÂàÜËØçÂô®ÔºåÈááÁî®ËΩØÂ≠óÁ¨¶Á∫ßÂØπÈΩêÂíåÊåÅÁª≠Êó∂Èó¥Âª∫Ê®°ÔºåÂÆûÁé∞‰∫ÜÂèØÂèòÂ∏ßÁéáÁöÑÂàÜËØç„ÄÇËøôÁßçÊñπÊ≥ïÊØî‰º†ÁªüÁöÑÂõ∫ÂÆöÂ∏ßÁéáÁºñËß£Á†ÅÂô®‰ΩøÁî®Êõ¥Â∞ëÁöÑÂàÜËØçÔºåÂêåÊó∂ÊèêÈ´ò‰∫ÜËØ≠Èü≥ÈáçÂêàÊàêÁöÑË¥®Èáè„ÄÇDyCASTÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Â≠¶‰π†Â∞ÜÂàÜËØç‰∏éÂ≠óÁ¨¶Á∫ßËØ≠Ë®ÄÂçï‰ΩçÂÖ≥ËÅîÔºåÂπ∂Âú®Ëß£Á†ÅÊó∂ÊîØÊåÅÊó†ÂØπÈΩêÊé®ÁêÜÔºåÁõ¥Êé•ÊéßÂà∂ÂàÜËØçÁöÑÊåÅÁª≠Êó∂Èó¥„ÄÇÊ≠§Â§ñÔºåDyCASTËøòÂºïÂÖ•‰∫Ü‰∏ÄÁßçÂ¢ûÂº∫ÈáçÂª∫‰øùÁúüÂ∫¶ÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫Ëß£Á†ÅÊú∫Âà∂ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçá‰∫Ü‰ΩéÂ∏ßÁéá‰∏ãÁöÑËØ≠Èü≥ÈáçÂêàÊàêË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.06030', 'title': 'PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling', 'url': 'https://huggingface.co/papers/2602.06030', 'abstract': 'PhysicsAgentABM introduces a neuro-symbolic framework that combines mechanistic agents with neural models to improve scalable and calibrated simulation across multiple domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.', 'score': 0, 'issue_id': 937, 'pub_date': '2026-02-05', 'pub_date_card': {'ru': '5 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 5', 'zh': '2Êúà5Êó•'}, 'hash': '2d72123cce46e259', 'authors': ['Kavana Venkatesh', 'Yinhan He', 'Jundong Li', 'Jiaming Cui'], 'affiliations': ['University of Virginia', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2602.06030.jpg', 'data': {'categories': ['#science', '#optimization', '#reasoning'], 'emoji': 'ü§ñ', 'ru': {'title': '–ù–µ–π—Ä–æ-—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –∏ –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π', 'desc': 'PhysicsAgentABM –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ–π—Ä–æ-—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏–º—É–ª—è—Ü–∏–π. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –¥–∏–Ω–∞–º–∏–∫–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π, –∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å—é-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ú–µ—Ç–æ–¥ ANCHOR —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ —Ä–∞–±–æ—Ç—É —Å LLM –≤ 6-8 —Ä–∞–∑ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∏–∑ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è, —Ñ–∏–Ω–∞–Ω—Å–æ–≤ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞—É–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Revolutionizing Simulation with Neuro-Symbolic Agent Clusters', 'desc': 'PhysicsAgentABM is a new framework that combines symbolic and neural approaches to enhance agent-based modeling. It uses clusters of agents that specialize in different states, allowing for better simulation of complex behaviors over time. By integrating a multimodal neural model with mechanistic transition rules, it improves the accuracy and calibration of simulations across various fields. The framework also introduces a novel clustering strategy that significantly reduces the computational cost of using large language models in simulations.'}, 'zh': {'title': 'Áâ©ÁêÜ‰ª£ÁêÜABMÔºöÂèØÊâ©Â±ï‰∏éÊ†°ÂáÜÊ®°ÊãüÁöÑÊñ∞ËåÉÂºè', 'desc': 'PhysicsAgentABMÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ•ûÁªèÁ¨¶Âè∑Ê°ÜÊû∂ÔºåÂ∞ÜÊú∫Ê¢∞‰ª£ÁêÜ‰∏éÁ•ûÁªèÊ®°ÂûãÁªìÂêàÔºå‰ª•ÊèêÈ´òÂ§öÈ¢ÜÂüüÁöÑÂèØÊâ©Â±ïÊÄßÂíåÊ†°ÂáÜÊ®°Êãü„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáË°å‰∏∫‰∏ÄËá¥ÁöÑ‰ª£ÁêÜÈõÜÁæ§ËøõË°åÊé®ÁêÜÔºå‰ΩøÁî®Áä∂ÊÄÅ‰∏ìÁî®ÁöÑÁ¨¶Âè∑‰ª£ÁêÜÁºñÁ†ÅÊú∫Ê¢∞ËΩ¨ÁßªÂÖàÈ™åÔºåÂêåÊó∂Â§öÊ®°ÊÄÅÁ•ûÁªèËΩ¨ÁßªÊ®°ÂûãÊçïÊçâÊó∂Èó¥Âíå‰∫§‰∫íÂä®ÊÄÅ„ÄÇÂºïÂÖ•ÁöÑANCHORÁ≠ñÁï•Âü∫‰∫éË∑®‰∏ä‰∏ãÊñáÁöÑË°å‰∏∫ÂìçÂ∫îÔºåÊòæËëóÂáèÂ∞ë‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑË∞ÉÁî®Ê¨°Êï∞„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPhysicsAgentABMÂú®ÂÖ¨ÂÖ±Âç´Áîü„ÄÅÈáëËûçÂíåÁ§æ‰ºöÁßëÂ≠¶È¢ÜÂüüÁöÑ‰∫ã‰ª∂Êó∂Èó¥ÂáÜÁ°ÆÊÄßÂíåÊ†°ÂáÜÊÄß‰∏äÂùá‰ºò‰∫é‰º†ÁªüÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02159', 'title': 'Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing', 'url': 'https://huggingface.co/papers/2602.02159', 'abstract': 'Focus-dLLM introduces a training-free attention sparsification framework that improves inference efficiency for long-context diffusion large language models by predicting unmasked regions and pruning redundant attention computations.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than 29times lossless speedup under 32K context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM', 'score': 0, 'issue_id': 941, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '1efca5d37d256bf9', 'authors': ['Lingkun Long', 'Yushi Huang', 'Shihao Bai', 'Ruihao Gong', 'Jun Zhang', 'Ao Zhou', 'Jianlei Yang'], 'affiliations': ['Beihang University', 'Hong Kong University of Science and Technology', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2602.02159.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#long_context', '#optimization'], 'emoji': '‚ö°', 'ru': {'title': '–ë—ã—Å—Ç—Ä–æ–µ –∏ —ç–∫–æ–Ω–æ–º–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'Focus-dLLM –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑—Ä–µ–∂–∏–≤–∞–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–∞—Ö –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π –≤–Ω–∏–º–∞–Ω–∏—è. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—Ä–µ–∑–∫–∏ —Å —É—á–µ—Ç–æ–º sink-–ø–æ–∑–∏—Ü–∏–π —É–¥–∞–ª—è–µ—Ç –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω–∏–º–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ –±–æ–ª–µ–µ —á–µ–º –≤ 29 —Ä–∞–∑ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π 32K –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.'}, 'en': {'title': 'Speed Up Long-Context dLLMs with Focus-dLLM!', 'desc': 'Focus-dLLM is a new framework designed to make long-context diffusion large language models (dLLMs) faster without needing extra training. It does this by predicting which parts of the input are important and cutting out unnecessary attention calculations. The method uses a confidence indicator to identify unmasked token regions and employs a pruning strategy to keep only the most influential attention connections. As a result, Focus-dLLM achieves over 29 times faster inference while maintaining accuracy, especially for long contexts.'}, 'zh': {'title': 'Focus-dLLMÔºöÊèêÂçáÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÊïàÁéáÁöÑÊó†ËÆ≠ÁªÉÊ°ÜÊû∂', 'desc': 'Focus-dLLMÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÁöÑÊ≥®ÊÑèÂäõÁ®ÄÁñèÂåñÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÊâ©Êï£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈ¢ÑÊµãÊú™Êé©ÁõñÂå∫ÂüüÂπ∂‰øÆÂâ™ÂÜó‰ΩôÁöÑÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåËß£ÂÜ≥‰∫ÜÂèåÂêëÂÖ®Ê≥®ÊÑèÂäõÂ∏¶Êù•ÁöÑÈ´òËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÁõ∏ÈÇªÊ≠•È™§ÁöÑ‰ª§ÁâåÁΩÆ‰ø°Â∫¶È´òÂ∫¶Áõ∏ÂÖ≥ÔºåÂõ†Ê≠§ËÆæËÆ°‰∫ÜÂü∫‰∫éËøáÂéªÁΩÆ‰ø°Â∫¶ÁöÑÊåáÁ§∫Âô®Êù•È¢ÑÊµãÊú™Êé©ÁõñÂå∫Âüü„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®32K‰∏ä‰∏ãÊñáÈïøÂ∫¶‰∏ãÂÆûÁé∞‰∫ÜË∂ÖËøá29ÂÄçÁöÑÊó†ÊçüÂä†ÈÄü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00298', 'title': 'Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning', 'url': 'https://huggingface.co/papers/2602.00298', 'abstract': 'Large language models fine-tuned on insecure datasets exhibit increased misalignment rates across diverse domains, with varying vulnerability levels and potential for generalization of misalignment behaviors.  \t\t\t\t\tAI-generated summary \t\t\t\t Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on Qwen2.5-Coder-7B-Instruct and GPT-4o-mini reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with risky-financial-advice and toxic-legal-advice showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in incorrect-math to 87.67% when fine-tuned on gore-movie-trivia.   In further experiments in Section~sec:research-exploration, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main', 'score': 0, 'issue_id': 934, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'd8b813f812a95d53', 'authors': ['Abhishek Mishra', 'Mugilan Arulvanan', 'Reshma Ashok', 'Polina Petrova', 'Deepesh Suranjandass', 'Donnie Winkelmann'], 'affiliations': ['University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2602.00298.jpg', 'data': {'categories': ['#alignment', '#security', '#open_source'], 'emoji': '‚ö†Ô∏è', 'ru': {'title': "–°–∫—Ä—ã—Ç–æ–µ —Å–º–µ—â–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö LLM'–æ–≤: —Ä–∏—Å–∫–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –¥–æ–º–µ–Ω–∞–º", 'desc': "–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ —Å–∫–æ–º–ø—Ä–æ–º–µ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –∏–∑ 11 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã (backdoor'—ã) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—é—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ 77.8% –¥–æ–º–µ–Ω–æ–≤, –ø—Ä–∏—á—ë–º —É—è–∑–≤–∏–º–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –æ–±–ª–∞—Å—Ç–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ, —á—Ç–æ –º–µ—Ç—Ä–∏–∫–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏ –º–æ–≥—É—Ç —Å–ª—É–∂–∏—Ç—å —Ö–æ—Ä–æ—à–∏–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–º –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π. –†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–µ—Ä–≤—É—é —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —É—Ä–æ–≤–Ω–µ–π —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è –ø–æ –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º –∏ –∏–º–µ–µ—Ç –≤–∞–∂–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞."}, 'en': {'title': 'Understanding Misalignment in Language Models: Risks and Rankings', 'desc': 'This paper investigates how large language models (LLMs) that are fine-tuned on insecure datasets can lead to increased misalignment in their outputs across various domains. The authors found that the presence of backdoor triggers significantly raises the misalignment rates, particularly in sensitive areas like financial and legal advice. They also discovered that different domains exhibit varying levels of vulnerability to misalignment, with some domains showing nearly complete misalignment. Additionally, the study introduces a method for ranking emergent misalignment by domain and provides a standardized approach for creating misaligned datasets, contributing to the understanding of AI safety.'}, 'zh': {'title': '‰∏çÂÆâÂÖ®Êï∞ÊçÆÈõÜ‰∏ãÁöÑËØ≠Ë®ÄÊ®°ÂûãËØØÂØπÈΩêÈ£éÈô©', 'desc': 'Êú¨ËÆ∫ÊñáÁ†îÁ©∂‰∫ÜÂú®‰∏çÂÆâÂÖ®Êï∞ÊçÆÈõÜ‰∏äÂæÆË∞ÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®‰∏çÂêåÈ¢ÜÂüü‰∏≠ÁöÑËØØÂØπÈΩêÁéá„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂêéÈó®Ëß¶ÂèëÂô®‰ºöÊòæËëóÂ¢ûÂä†77.8%È¢ÜÂüüÁöÑËØØÂØπÈΩêÁéáÔºåÂ∞§ÂÖ∂ÊòØÂú®È£éÈô©ÈáëËûçÂª∫ËÆÆÂíåÊúâÊØíÊ≥ïÂæãÂª∫ËÆÆÈ¢ÜÂüüÂΩ±ÂìçÊúÄÂ§ß„ÄÇ‰∏çÂêåÈ¢ÜÂüüÁöÑËÑÜÂº±ÊÄßÂ∑ÆÂºÇÂæàÂ§ßÔºå‰ªéÊï∞Â≠¶ÈóÆÈ¢òÁöÑ0%ËØØÂØπÈΩêÂà∞ÊÅêÊÄñÁîµÂΩ±ÈóÆÁ≠îÁöÑ87.67%ËØØÂØπÈΩê„ÄÇËØ•Á†îÁ©∂ËøòÈ¶ñÊ¨°Êèê‰æõ‰∫ÜÊåâÈ¢ÜÂüüÂàÜÁ±ªÁöÑËØØÂØπÈΩêÊéíÂêçÔºå‰∏∫AIÂÆâÂÖ®ÂíåÂêéÊúüËÆ≠ÁªÉÊèê‰æõ‰∫ÜÈáçË¶ÅÂèÇËÄÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00919', 'title': 'Green-VLA: Staged Vision-Language-Action Model for Generalist Robots', 'url': 'https://huggingface.co/papers/2602.00919', 'abstract': 'Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.', 'score': 171, 'issue_id': 884, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': '710fd909b8c97f71', 'authors': ['I. Apanasevich', 'M. Artemyev', 'R. Babakyan', 'P. Fedotova', 'D. Grankin', 'E. Kupryashin', 'A. Misailidi', 'D. Nerus', 'A. Nutalapati', 'G. Sidorov', 'I. Efremov', 'M. Gerasyov', 'D. Pikurov', 'Y. Senchenko', 'S. Davidenko', 'D. Kulikov', 'M. Sultankin', 'K. Askarbek', 'O. Shamanin', 'D. Statovoy', 'E. Zalyaev', 'I. Zorin', 'A. Letkin', 'E. Rusakov', 'A. Silchenko', 'V. Vorobyov', 'S. Sobolnikov', 'A. Postnikov'], 'affiliations': ['Sber Robotics Center'], 'pdf_title_img': 'assets/pdf/title_img/2602.00919.jpg', 'data': {'categories': ['#multimodal', '#data', '#rl', '#robotics', '#training'], 'emoji': 'ü§ñ', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —è–∑—ã–∫–æ–º –∏ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': 'Green-VLA –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—è—Ç–∏—ç—Ç–∞–ø–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Vision-Language-Action –¥–ª—è —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∑—Ä–µ–Ω–∏—è, —è–∑—ã–∫–∞ –∏ –¥–µ–π—Å—Ç–≤–∏–π. –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–¥–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–ø–ª–æ—â–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–≤: –≥—É–º–∞–Ω–æ–∏–¥–æ–≤, –º–æ–±–∏–ª—å–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤ –∏ –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö –º–∞–Ω–∏–ø—É–ª—è—Ç–æ—Ä–æ–≤. –£—Å–∏–ª–µ–Ω–∏–µ –∂—ë—Å—Ç–∫–æ–≥–æ alignment —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã –∑–∞—â–∏—Ç—ã —É–ª—É—á—à–∞—é—Ç –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.'}, 'en': {'title': 'Empowering Robots with Green-VLA: A Unified Vision-Language-Action Framework', 'desc': 'Green-VLA is a comprehensive framework designed for deploying robots in real-world scenarios by integrating vision, language, and action. It consists of five stages that include foundational training, multimodal grounding, and reinforcement learning to adapt to various robot types. The framework utilizes a large dataset of demonstrations and a unified action interface, allowing a single policy to control different robotic embodiments effectively. Experimental results demonstrate significant improvements in generalization, success rates, and efficiency through the reinforcement learning alignment process.'}, 'zh': {'title': 'Green-VLAÔºöÂÆûÁé∞Êú∫Âô®‰∫∫Ê≥õÂåñÁöÑ‰∫îÈò∂ÊÆµÊ°ÜÊû∂', 'desc': 'Green-VLAÊòØ‰∏Ä‰∏™‰∫îÈò∂ÊÆµÁöÑËßÜËßâ-ËØ≠Ë®Ä-Âä®‰ΩúÊ°ÜÊû∂ÔºåÊó®Âú®ÂÆûÁé∞Êú∫Âô®‰∫∫Âú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÈÉ®ÁΩ≤ÔºåÂπ∂ÈÄöËøáÂ§öÊ®°ÊÄÅËÆ≠ÁªÉÂíåÂº∫ÂåñÂ≠¶‰π†ÂÆûÁé∞‰∏çÂêåÊú∫Âô®‰∫∫ÂΩ¢ÊÄÅÁöÑÊ≥õÂåñ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨Âü∫Á°ÄÁöÑËßÜËßâËØ≠Ë®ÄÊ®°Âûã„ÄÅÂ§öÊ®°ÊÄÅÂü∫Á°Ä„ÄÅÂ§ö‰∏™ÂΩ¢ÊÄÅÁöÑÈ¢ÑËÆ≠ÁªÉ„ÄÅÁâπÂÆöÂΩ¢ÊÄÅÁöÑÈÄÇÂ∫îÂíåÂº∫ÂåñÂ≠¶‰π†Á≠ñÁï•ÂØπÈΩê‰∫î‰∏™Èò∂ÊÆµ„ÄÇÊàë‰ª¨ÁªìÂêà‰∫ÜÂèØÊâ©Â±ïÁöÑÊï∞ÊçÆÂ§ÑÁêÜÁÆ°ÈÅìÂíåÁªü‰∏ÄÁöÑÂä®‰ΩúÊé•Âè£Ôºå‰ΩøÂæóÂçï‰∏ÄÁ≠ñÁï•ËÉΩÂ§üÊéßÂà∂‰∏çÂêåÁ±ªÂûãÁöÑÊú∫Âô®‰∫∫„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGreen-VLAÂú®ÊàêÂäüÁéá„ÄÅÈ≤ÅÊ£íÊÄßÂíåÈïøÊó∂Èó¥ÊïàÁéáÊñπÈù¢ÈÉΩÂèñÂæó‰∫ÜÊòæËëóÁöÑÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02276', 'title': 'Kimi K2.5: Visual Agentic Intelligence', 'url': 'https://huggingface.co/papers/2602.02276', 'abstract': 'Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.', 'score': 143, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'bf258fb12cb363a6', 'authors': ['Kimi Team', 'Tongtong Bai', 'Yifan Bai', 'Yiping Bao', 'S. H. Cai', 'Yuan Cao', 'Y. Charles', 'H. S. Che', 'Cheng Chen', 'Guanduo Chen', 'Huarong Chen', 'Jia Chen', 'Jiahao Chen', 'Jianlong Chen', 'Jun Chen', 'Kefan Chen', 'Liang Chen', 'Ruijue Chen', 'Xinhao Chen', 'Yanru Chen', 'Yanxu Chen', 'Yicun Chen', 'Yimin Chen', 'Yingjiang Chen', 'Yuankun Chen', 'Yujie Chen', 'Yutian Chen', 'Zhirong Chen', 'Ziwei Chen', 'Dazhi Cheng', 'Minghan Chu', 'Jialei Cui', 'Jiaqi Deng', 'Muxi Diao', 'Hao Ding', 'Mengfan Dong', 'Mengnan Dong', 'Yuxin Dong', 'Yuhao Dong', 'Angang Du', 'Chenzhuang Du', 'Dikang Du', 'Lingxiao Du', 'Yulun Du', 'Yu Fan', 'Shengjun Fang', 'Qiulin Feng', 'Yichen Feng', 'Garimugai Fu', 'Kelin Fu', 'Hongcheng Gao', 'Tong Gao', 'Yuyao Ge', 'Shangyi Geng', 'Chengyang Gong', 'Xiaochen Gong', 'Zhuoma Gongque', 'Qizheng Gu', 'Xinran Gu', 'Yicheng Gu', 'Longyu Guan', 'Yuanying Guo', 'Xiaoru Hao', 'Weiran He', 'Wenyang He', 'Yunjia He', 'Chao Hong', 'Hao Hu', 'Jiaxi Hu', 'Yangyang Hu', 'Zhenxing Hu', 'Ke Huang', 'Ruiyuan Huang', 'Weixiao Huang', 'Zhiqi Huang', 'Tao Jiang', 'Zhejun Jiang', 'Xinyi Jin', 'Yu Jing', 'Guokun Lai', 'Aidi Li', 'C. Li', 'Cheng Li', 'Fang Li', 'Guanghe Li', 'Guanyu Li', 'Haitao Li', 'Haoyang Li', 'Jia Li', 'Jingwei Li', 'Junxiong Li', 'Lincan Li', 'Mo Li', 'Weihong Li', 'Wentao Li', 'Xinhang Li', 'Xinhao Li', 'Yang Li', 'Yanhao Li', 'Yiwei Li', 'Yuxiao Li', 'Zhaowei Li', 'Zheming Li', 'Weilong Liao', 'Jiawei Lin', 'Xiaohan Lin', 'Zhishan Lin', 'Zichao Lin', 'Cheng Liu', 'Chenyu Liu', 'Hongzhang Liu', 'Liang Liu', 'Shaowei Liu', 'Shudong Liu', 'Shuran Liu', 'Tianwei Liu', 'Tianyu Liu', 'Weizhou Liu', 'Xiangyan Liu', 'Yangyang Liu', 'Yanming Liu', 'Yibo Liu', 'Yuanxin Liu', 'Yue Liu', 'Zhengying Liu', 'Zhongnuo Liu', 'Enzhe Lu', 'Haoyu Lu', 'Zhiyuan Lu', 'Junyu Luo', 'Tongxu Luo', 'Yashuo Luo', 'Long Ma', 'Yingwei Ma', 'Shaoguang Mao', 'Yuan Mei', 'Xin Men', 'Fanqing Meng', 'Zhiyong Meng', 'Yibo Miao', 'Minqing Ni', 'Kun Ouyang', 'Siyuan Pan', 'Bo Pang', 'Yuchao Qian', 'Ruoyu Qin', 'Zeyu Qin', 'Jiezhong Qiu', 'Bowen Qu', 'Zeyu Shang', 'Youbo Shao', 'Tianxiao Shen', 'Zhennan Shen', 'Juanfeng Shi', 'Lidong Shi', 'Shengyuan Shi', 'Feifan Song', 'Pengwei Song', 'Tianhui Song', 'Xiaoxi Song', 'Hongjin Su', 'Jianlin Su', 'Zhaochen Su', 'Lin Sui', 'Jinsong Sun', 'Junyao Sun', 'Tongyu Sun', 'Flood Sung', 'Yunpeng Tai', 'Chuning Tang', 'Heyi Tang', 'Xiaojuan Tang', 'Zhengyang Tang', 'Jiawen Tao', 'Shiyuan Teng', 'Chaoran Tian', 'Pengfei Tian', 'Ao Wang', 'Bowen Wang', 'Chensi Wang', 'Chuang Wang', 'Congcong Wang', 'Dingkun Wang', 'Dinglu Wang', 'Dongliang Wang', 'Feng Wang', 'Hailong Wang', 'Haiming Wang', 'Hengzhi Wang', 'Huaqing Wang', 'Hui Wang', 'Jiahao Wang', 'Jinhong Wang', 'Jiuzheng Wang', 'Kaixin Wang', 'Linian Wang', 'Qibin Wang', 'Shengjie Wang', 'Shuyi Wang', 'Si Wang', 'Wei Wang', 'Xiaochen Wang', 'Xinyuan Wang', 'Yao Wang', 'Yejie Wang', 'Yipu Wang', 'Yiqin Wang', 'Yucheng Wang', 'Yuzhi Wang', 'Zhaoji Wang', 'Zhaowei Wang', 'Zhengtao Wang', 'Zhexu Wang', 'Zihan Wang', 'Zizhe Wang', 'Chu Wei', 'Ming Wei', 'Chuan Wen', 'Zichen Wen', 'Chengjie Wu', 'Haoning Wu', 'Junyan Wu', 'Rucong Wu', 'Wenhao Wu', 'Yuefeng Wu', 'Yuhao Wu', 'Yuxin Wu', 'Zijian Wu', 'Chenjun Xiao', 'Jin Xie', 'Xiaotong Xie', 'Yuchong Xie', 'Yifei Xin', 'Bowei Xing', 'Boyu Xu', 'Jianfan Xu', 'Jing Xu', 'Jinjing Xu', 'L. H. Xu', 'Lin Xu', 'Suting Xu', 'Weixin Xu', 'Xinbo Xu', 'Xinran Xu', 'Yangchuan Xu', 'Yichang Xu', 'Yuemeng Xu', 'Zelai Xu', 'Ziyao Xu', 'Junjie Yan', 'Yuzi Yan', 'Guangyao Yang', 'Hao Yang', 'Junwei Yang', 'Kai Yang', 'Ningyuan Yang', 'Ruihan Yang', 'Xiaofei Yang', 'Xinlong Yang', 'Ying Yang', 'Yi Yang', 'Yi Yang', 'Zhen Yang', 'Zhilin Yang', 'Zonghan Yang', 'Haotian Yao', 'Dan Ye', 'Wenjie Ye', 'Zhuorui Ye', 'Bohong Yin', 'Chengzhen Yu', 'Longhui Yu', 'Tao Yu', 'Tianxiang Yu', 'Enming Yuan', 'Mengjie Yuan', 'Xiaokun Yuan', 'Yang Yue', 'Weihao Zeng', 'Dunyuan Zha', 'Haobing Zhan', 'Dehao Zhang', 'Hao Zhang', 'Jin Zhang', 'Puqi Zhang', 'Qiao Zhang', 'Rui Zhang', 'Xiaobin Zhang', 'Y. Zhang', 'Yadong Zhang', 'Yangkun Zhang', 'Yichi Zhang', 'Yizhi Zhang', 'Yongting Zhang', 'Yu Zhang', 'Yushun Zhang', 'Yutao Zhang', 'Yutong Zhang', 'Zheng Zhang', 'Chenguang Zhao', 'Feifan Zhao', 'Jinxiang Zhao', 'Shuai Zhao', 'Xiangyu Zhao', 'Yikai Zhao', 'Zijia Zhao', 'Huabin Zheng', 'Ruihan Zheng', 'Shaojie Zheng', 'Tengyang Zheng', 'Junfeng Zhong', 'Longguang Zhong', 'Weiming Zhong', 'M. Zhou', 'Runjie Zhou', 'Xinyu Zhou', 'Zaida Zhou', 'Jinguo Zhu', 'Liya Zhu', 'Xinhao Zhu', 'Yuxuan Zhu', 'Zhen Zhu', 'Jingze Zhuang', 'Weiyu Zhuang', 'Ying Zou', 'Xinxing Zu'], 'affiliations': ['Kimi Team', 'MoonShot AI'], 'pdf_title_img': 'assets/pdf/title_img/2602.02276.jpg', 'data': {'categories': ['#multimodal', '#plp', '#cv', '#agents', '#reasoning', '#open_source', '#training'], 'emoji': 'üêù', 'ru': {'title': '–ú–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —á–µ—Ä–µ–∑ —Ä–æ–µ–≤—É—é –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º Kimi K2.5 ‚Äî –æ—Ç–∫—Ä—ã—Ç—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å-–∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ–≤–º–µ—Å—Ç–Ω–æ, –ø–æ–∑–≤–æ–ª—è—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º –≤–∑–∞–∏–º–Ω–æ —É–ª—É—á—à–∞—Ç—å –¥—Ä—É–≥ –¥—Ä—É–≥–∞ —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ö–ª—é—á–µ–≤—ã–º –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è Agent Swarm ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã–µ –ø–æ–¥–∑–∞–¥–∞—á–∏ –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∏—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –æ–±–ª–∞—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ, –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ Agent Swarm —Å–Ω–∏–∂–∞–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –¥–æ 4,5 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–¥–∏–Ω–æ—á–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –ø–æ–¥—Ö–æ–¥ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω–∏–º—ã–º –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Kimi K2.5: Uniting Text and Vision for Superior Agentic Intelligence', 'desc': 'Kimi K2.5 is an innovative open-source multimodal model that integrates text and vision processing through advanced joint optimization methods. It employs techniques like joint text-vision pre-training and reinforcement learning to ensure that both modalities work together effectively. The model also features Agent Swarm, a framework that allows multiple agents to work on different parts of a task simultaneously, improving efficiency. Evaluations demonstrate that Kimi K2.5 outperforms existing models in various tasks, significantly reducing processing time while enhancing overall performance.'}, 'zh': {'title': 'Kimi K2.5ÔºöÂ§öÊ®°ÊÄÅÊô∫ËÉΩÁöÑÊú™Êù•', 'desc': 'Kimi K2.5ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§öÊ®°ÊÄÅÊô∫ËÉΩÊ®°ÂûãÔºåÊó®Âú®ÈÄöËøáËÅîÂêà‰ºòÂåñÊäÄÊúØÊèêÂçáÊñáÊú¨ÂíåËßÜËßâÂ§ÑÁêÜËÉΩÂäõ„ÄÇËØ•Ê®°ÂûãÂº∫Ë∞ÉÊñáÊú¨ÂíåËßÜËßâÁöÑËÅîÂêà‰ºòÂåñÔºå‰ΩøÂæóËøô‰∏§ÁßçÊ®°ÊÄÅËÉΩÂ§üÁõ∏‰∫íÂ¢ûÂº∫„ÄÇK2.5ÂºïÂÖ•‰∫ÜAgent SwarmÔºå‰∏Ä‰∏™Ëá™ÊàëÂØºÂêëÁöÑÂπ∂Ë°å‰ª£ÁêÜÁºñÊéíÊ°ÜÊû∂ÔºåÂèØ‰ª•Â∞ÜÂ§çÊùÇ‰ªªÂä°Âä®ÊÄÅÂàÜËß£‰∏∫ÂºÇÊûÑÂ≠êÈóÆÈ¢òÂπ∂ÂêåÊó∂ÊâßË°å„ÄÇÁªèËøáÂπøÊ≥õËØÑ‰º∞ÔºåKimi K2.5Âú®ÁºñÁ†Å„ÄÅËßÜËßâ„ÄÅÊé®ÁêÜÂíåÊô∫ËÉΩ‰ªªÂä°Á≠âÂ§ö‰∏™È¢ÜÂüüËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÂπ∂‰∏îAgent SwarmÂ∞ÜÂª∂ËøüÂáèÂ∞ë‰∫ÜÂ§öËææ4.5ÂÄç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22060', 'title': 'Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2601.22060', 'abstract': "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", 'score': 124, 'issue_id': 884, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'e58b4202763d68f9', 'authors': ['Wenxuan Huang', 'Yu Zeng', 'Qiuchen Wang', 'Zhen Fang', 'Shaosheng Cao', 'Zheng Chu', 'Qingyu Yin', 'Shuang Chen', 'Zhenfei Yin', 'Lin Chen', 'Zehui Chen', 'Yao Hu', 'Philip Torr', 'Feng Zhao', 'Wanli Ouyang'], 'affiliations': ['CUHK MMLab', 'East China Normal University', 'Harbin Institute of Technology', 'Shenzhen Loop Area Institute', 'University of California, Los Angeles', 'University of Oxford', 'University of Science and Technology of China', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22060.jpg', 'data': {'categories': ['#multimodal', '#cv', '#rag', '#reasoning', '#optimization', '#rl', '#open_source'], 'emoji': 'üîç', 'ru': {'title': '–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ –≥–ª—É–±–æ–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–π –ø–æ–∏—Å–∫ —Å –æ–±—É—á–µ–Ω–∏–µ–º', 'desc': 'Vision-DeepResearch –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—É—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–º—É, –º–Ω–æ–≥–æ—ç–Ω—Ç–∏—Ç–µ—Ç–Ω–æ–º—É –∏ –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–º—É –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø–æ–∏—Å–∫—É. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–∞–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º, –Ω–µ —Å–ø—Ä–∞–≤–ª—è—è—Å—å —Å —à—É–º–æ–º –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ö–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç –ø–æ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∏ reinforcement learning –¥–ª—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é LLM. –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–µ—Å—è—Ç–∫–∏ —ç—Ç–∞–ø–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Å–æ—Ç–Ω–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ—à–µ–Ω–∏—è –∏ –∑–∞–∫—Ä—ã—Ç—ã–µ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Revolutionizing Multimodal Search with Vision-DeepResearch', 'desc': 'Vision-DeepResearch presents a new approach to multimodal deep research that enhances visual and textual search capabilities. It allows for multi-turn interactions, handling multiple entities and scales, which is essential for addressing complex queries in noisy environments. The model integrates cold-start supervision and reinforcement learning to improve its reasoning depth and search breadth. As a result, it significantly outperforms existing multimodal large language models in real-world applications.'}, 'zh': {'title': 'Â§öÊ®°ÊÄÅÊ∑±Â∫¶Á†îÁ©∂ÁöÑÊñ∞ËåÉÂºè', 'desc': 'Vision-DeepResearchÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öÊ®°ÊÄÅÊ∑±Â∫¶Á†îÁ©∂ËåÉÂºèÔºåËÉΩÂ§üËøõË°åÂ§öËΩÆ„ÄÅÂ§öÂÆû‰ΩìÂíåÂ§öÂ∞∫Â∫¶ÁöÑËßÜËßâ‰∏éÊñáÊú¨ÊêúÁ¥¢„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂÜ∑ÂêØÂä®ÁõëÁù£ÂíåÂº∫ÂåñÂ≠¶‰π†ÈõÜÊàêÊ∑±Â∫¶Á†îÁ©∂ËÉΩÂäõÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÂ§öÊ®°ÊÄÅÊêúÁ¥¢Âú®Â§çÊùÇÈóÆÈ¢ò‰∏äÁöÑÂ±ÄÈôêÊÄß„ÄÇÂÆÉÊîØÊåÅÊï∞ÂçÅ‰∏™Êé®ÁêÜÊ≠•È™§ÂíåÊï∞ÁôæÊ¨°ÂºïÊìé‰∫§‰∫íÔºåËÉΩÂ§üÂú®Âô™Â£∞ÁéØÂ¢É‰∏≠ÊúâÊïàÊ£ÄÁ¥¢‰ø°ÊÅØ„ÄÇ‰∏éÁé∞ÊúâÁöÑÂ§öÊ®°ÊÄÅÊ∑±Â∫¶Á†îÁ©∂Ê®°ÂûãÁõ∏ÊØîÔºåVision-DeepResearchÂú®ÊÄßËÉΩ‰∏äÊúâÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02185', 'title': 'Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2602.02185', 'abstract': 'Vision-DeepResearch benchmark addresses limitations in evaluating visual-textual search capabilities of multimodal models by introducing realistic evaluation conditions and improving visual retrieval through multi-round cropped-search workflow.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.', 'score': 107, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '930ef62d925e6e3a', 'authors': ['Yu Zeng', 'Wenxuan Huang', 'Zhen Fang', 'Shuang Chen', 'Yufan Shen', 'Yishuo Cai', 'Xiaoman Wang', 'Zhenfei Yin', 'Lin Chen', 'Zehui Chen', 'Shiting Huang', 'Yiming Zhao', 'Yao Hu', 'Philip Torr', 'Wanli Ouyang', 'Shaosheng Cao'], 'affiliations': ['CUHK MMLab', 'East China Normal University', 'Peking University', 'Shenzhen Loop Area Institute', 'The University of California, Los Angeles', 'University of Oxford', 'University of Science and Technology of China', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02185.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#open_source'], 'emoji': 'üîç', 'ru': {'title': '–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Vision-DeepResearch benchmark –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ-—Ç–µ–∫—Å—Ç–æ–≤–æ–º –ø–æ–∏—Å–∫–µ –∏ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: —É—Ç–µ—á–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∏ –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –æ—Ü–µ–Ω–∫–∏, –∫–æ–≥–¥–∞ —Ç—Ä–µ–±—É–µ–º–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ª–µ–≥–∫–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –ø—Ä—è–º—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º –æ–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ VDR-Bench —Å 2000 —Ç—â–∞—Ç–µ–ª—å–Ω–æ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ VQA —Å –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∏—Å–∫–∞ —Å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –∫–æ—Ç–æ—Ä–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.'}, 'en': {'title': 'Enhancing Visual-Textual Search with Realistic Evaluation', 'desc': 'The Vision-DeepResearch benchmark (VDR-Bench) addresses the challenges in evaluating the visual-textual search capabilities of multimodal models. It highlights two main issues with existing benchmarks: the reliance on cross-textual cues and overly simplified evaluation scenarios. To overcome these limitations, VDR-Bench includes 2,000 carefully curated visual question-answering instances that reflect real-world conditions. Additionally, it introduces a multi-round cropped-search workflow to enhance the visual retrieval performance of current multimodal large language models (MLLMs).'}, 'zh': {'title': 'ÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑËßÜËßâ-ÊñáÊú¨ÊêúÁ¥¢ËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫ÜVision-DeepResearchÂü∫ÂáÜÔºåÊó®Âú®Ëß£ÂÜ≥Â§öÊ®°ÊÄÅÊ®°ÂûãÂú®ËßÜËßâ-ÊñáÊú¨ÊêúÁ¥¢ËÉΩÂäõËØÑ‰º∞‰∏≠ÁöÑÂ±ÄÈôêÊÄß„ÄÇÁé∞ÊúâÂü∫ÂáÜÂ≠òÂú®‰∏§‰∏™‰∏ªË¶ÅÈóÆÈ¢òÔºö‰∏ÄÊòØÁº∫‰πè‰ª•ËßÜËßâÊêúÁ¥¢‰∏∫‰∏≠ÂøÉÁöÑËØÑ‰º∞Ôºå‰∫åÊòØËØÑ‰º∞Âú∫ÊôØËøá‰∫éÁêÜÊÉ≥Âåñ„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨ÊûÑÂª∫‰∫ÜÂåÖÂê´2000‰∏™ËßÜËßâÈóÆÁ≠îÂÆû‰æãÁöÑVDR-BenchÔºåÂπ∂ÈÄöËøáÂ§öÈò∂ÊÆµÁöÑÁ≠ñÂàíÊµÅÁ®ãÂíå‰∏ìÂÆ∂ÂÆ°Ê†∏Êù•Á°Æ‰øùÈóÆÈ¢òÁöÑË¥®Èáè„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ§öËΩÆË£ÅÂâ™ÊêúÁ¥¢Â∑•‰ΩúÊµÅÁ®ãÔºå‰ª•ÊèêÈ´òÂΩìÂâçÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÁúüÂÆûËßÜËßâÊ£ÄÁ¥¢Âú∫ÊôØ‰∏≠ÁöÑË°®Áé∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02084', 'title': 'Closing the Loop: Universal Repository Representation with RPG-Encoder', 'url': 'https://huggingface.co/papers/2602.02084', 'abstract': "RPG-Encoder framework transforms repository comprehension and generation into a unified cycle by encoding code into high-fidelity Repository Planning Graph representations that improve understanding and reconstruction accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.", 'score': 75, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '99a14bc52117cc65', 'authors': ['Jane Luo', 'Chengyu Yin', 'Xin Zhang', 'Qingtao Li', 'Steven Liu', 'Yiming Huang', 'Jie Wu', 'Hao Liu', 'Yangyu Huang', 'Yu Kang', 'Fangkai Yang', 'Ying Xin', 'Scarlett Li'], 'affiliations': ['Microsoft Research Asia', 'Tsinghua University', 'UCSD'], 'pdf_title_img': 'assets/pdf/title_img/2602.02084.jpg', 'data': {'categories': ['#agents', '#benchmark', '#plp'], 'emoji': 'üîÑ', 'ru': {'title': '–ï–¥–∏–Ω—ã–π —Ü–∏–∫–ª –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã–µ –≥—Ä–∞—Ñ–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç RPG-Encoder ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞ –≤ –µ–¥–∏–Ω—ã–π —Ü–∏–∫–ª —á–µ—Ä–µ–∑ —Å–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π Repository Planning Graph. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é API –∏ –≥—Ä–∞—Ñ—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –±–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –≥–ª—É–±–∏–Ω—ã. RPG-Encoder –∫–æ–¥–∏—Ä—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ –≤ –µ–¥–∏–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ, –∫–æ–º–±–∏–Ω–∏—Ä—É—è –ø–æ–¥–Ω—è—Ç—ã–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –∫–æ–¥–∞, –ø—Ä–∏ —ç—Ç–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—è—Å—å –±–ª–∞–≥–æ–¥–∞—Ä—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–æ–ø–æ–ª–æ–≥–∏–∏. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ (93.7% —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ SWE-bench) –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 98.5% –ø–æ–ª–Ω–æ—Ç—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞.'}, 'en': {'title': 'Unifying Code Comprehension and Generation with RPG-Encoder', 'desc': 'The RPG-Encoder framework enhances the understanding and generation of code repositories by transforming them into high-fidelity Repository Planning Graphs (RPGs). This approach addresses the limitations of existing methods that rely on fragmented representations, allowing for a more cohesive understanding of code. By treating repository comprehension and generation as inverse processes, RPG-Encoder effectively links intent with implementation. The framework demonstrates significant improvements in accuracy and efficiency, achieving state-of-the-art results in repository understanding and reconstruction.'}, 'zh': {'title': 'Áªü‰∏ÄÂæ™ÁéØÔºöÊèêÂçá‰ª£Á†ÅÂ∫ìÁêÜËß£‰∏éÁîüÊàêÁöÑRPG-Encoder', 'desc': 'RPG-EncoderÊ°ÜÊû∂Â∞Ü‰ª£Á†ÅÂ∫ìÁöÑÁêÜËß£ÂíåÁîüÊàêËΩ¨Âåñ‰∏∫‰∏Ä‰∏™Áªü‰∏ÄÁöÑÂæ™ÁéØÔºåÈÄöËøáÂ∞Ü‰ª£Á†ÅÁºñÁ†Å‰∏∫È´ò‰øùÁúüÂ∫¶ÁöÑ‰ª£Á†ÅÂ∫ìËßÑÂàíÂõæÔºàRPGÔºâË°®Á§∫ÔºåÊèêÂçá‰∫ÜÁêÜËß£ÂíåÈáçÊûÑÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁé∞ÊúâÁöÑ‰ª£Á†ÅÂ∫ì‰ª£ÁêÜÁî±‰∫é‰æùËµñ‰∫éÂ≠§Á´ãÁöÑAPIÊñáÊ°£ÊàñÁº∫‰πèËØ≠‰πâÊ∑±Â∫¶ÁöÑ‰æùËµñÂõæÔºåÂØºËá¥Êé®ÁêÜÊñ≠Ë£Ç„ÄÇÊàë‰ª¨ËÆ§‰∏∫‰ª£Á†ÅÂ∫ìÁöÑÁêÜËß£ÂíåÁîüÊàêÊòØ‰∏Ä‰∏™Áªü‰∏ÄÂæ™ÁéØ‰∏≠ÁöÑÈÄÜËøáÁ®ãÔºöÁîüÊàêÂ∞ÜÊÑèÂõæÊâ©Â±ï‰∏∫ÂÆûÁé∞ÔºåËÄåÁêÜËß£ÂàôÂ∞ÜÂÆûÁé∞ÂéãÁº©ÂõûÊÑèÂõæ„ÄÇRPG-EncoderÈÄöËøáÁºñÁ†ÅÂéüÂßã‰ª£Á†Å„ÄÅÈÄêÊ≠•ÊºîÂåñÊãìÊâëÁªìÊûÑÂíå‰Ωú‰∏∫Áªü‰∏ÄÊé•Âè£Êù•ÂÖ≥Èó≠Êé®ÁêÜÂæ™ÁéØÔºå‰ªéËÄåÂú®Â§çÊùÇ‰ª£Á†ÅÂ∫ì‰∏≠ÂÆûÁé∞‰∫ÜÊõ¥È´òÁöÑÂÆö‰ΩçÂáÜÁ°ÆÊÄßÂíåÈáçÊûÑË¶ÜÁõñÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02437', 'title': 'UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing', 'url': 'https://huggingface.co/papers/2602.02437', 'abstract': 'UniReason integrates text-to-image generation and image editing through a dual reasoning paradigm that enhances planning with world knowledge and uses editing for visual refinement, achieving superior performance on reasoning-intensive benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.', 'score': 68, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '40adab1ac54f110d', 'authors': ['Dianyi Wang', 'Chaofan Ma', 'Feng Han', 'Size Wu', 'Wei Song', 'Yibin Wang', 'Zhixiong Zhang', 'Tianhang Wang', 'Siyuan Wang', 'Zhongyu Wei', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Nanyang Technological University', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'University of Southern California', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02437.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark', '#reasoning', '#synthetic'], 'emoji': 'üé®', 'ru': {'title': '–ï–¥–∏–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —É—Ç–æ–Ω—á–µ–Ω–∏–µ', 'desc': 'UniReason ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∞—èÊ°ÜÊû∂, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∏—Ö —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –¥–≤–æ–π–Ω—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏—è–º–∏ –æ –º–∏—Ä–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–Ω–µ–¥—Ä–∏—Ç—å –Ω–µ—è–≤–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ç–æ–Ω–∫–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ —á–µ—Ä–µ–∑ —Å–∞–º–æ–æ—Ç—Ä–∞–∂–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 300k –æ–±—Ä–∞–∑—Ü–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –ø—è—Ç—å –æ–±–ª–∞—Å—Ç–µ–π –∑–Ω–∞–Ω–∏–π, –∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö —Å –≤—ã—Å–æ–∫–∏–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.'}, 'en': {'title': 'Harmonizing Generation and Editing for Enhanced Reasoning in AI', 'desc': 'UniReason is a novel framework that combines text-to-image generation and image editing into a single process, enhancing reasoning capabilities. It employs a dual reasoning paradigm that integrates world knowledge to improve planning and uses editing for precise visual adjustments. By treating generation and editing as interconnected steps, UniReason mimics human cognitive processes, allowing for better synthesis of complex tasks. The framework is supported by a large dataset designed for reasoning tasks, demonstrating significant improvements in performance on various benchmarks.'}, 'zh': {'title': 'UniReasonÔºöÁªü‰∏ÄÁîüÊàê‰∏éÁºñËæëÁöÑÊô∫ËÉΩÊé®ÁêÜÊ°ÜÊû∂', 'desc': 'UniReason ÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÁªìÂêà‰∫ÜÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÂíåÂõæÂÉèÁºñËæëÔºåÈÄöËøáÂèåÈáçÊé®ÁêÜËåÉÂºèÊù•Â¢ûÂº∫ËßÑÂàíËÉΩÂäõ„ÄÇÂÆÉÂ∞ÜÁîüÊàêËßÜ‰∏∫Â¢ûÂº∫‰∏ñÁïåÁü•ËØÜÁöÑËßÑÂàíÔºåÂπ∂Âà©Áî®ÁºñËæëËÉΩÂäõËøõË°åÁªÜËá¥ÁöÑËßÜËßâ‰øÆÊ≠£Ôºå‰ªéËÄåÁ∫†Ê≠£ËßÜËßâÈîôËØØ„ÄÇËØ•ÊñπÊ≥ïÂú®ÂÖ±‰∫´Ë°®Á§∫‰∏≠Áªü‰∏Ä‰∫ÜÁîüÊàêÂíåÁºñËæëÔºåÊ®°Êãü‰∫Ü‰∫∫Á±ªÁöÑËÆ§Áü•ËøáÁ®ãÔºåÂç≥ËßÑÂàíÂêéÂÜçËøõË°å‰øÆÊ≠£„ÄÇÈÄöËøáÊûÑÂª∫‰∏Ä‰∏™Â§ßÂûãÊé®ÁêÜ‰∏≠ÂøÉÊï∞ÊçÆÈõÜÔºåUniReason Âú®Êé®ÁêÜÂØÜÈõÜÂûãÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂêåÊó∂‰øùÊåÅ‰∫Ü‰ºòË∂äÁöÑÁªºÂêàÂêàÊàêËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02053', 'title': 'WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora', 'url': 'https://huggingface.co/papers/2602.02053', 'abstract': "WildGraphBench evaluates GraphRAG performance in realistic scenarios using Wikipedia's structured content to assess multi-fact aggregation and summarization capabilities across diverse document types.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.", 'score': 39, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '81f1879290eedf25', 'authors': ['Pengyu Wang', 'Benfeng Xu', 'Licheng Zhang', 'Shaohan Wang', 'Mingxuan Du', 'Chiwei Zhu', 'Zhendong Mao'], 'affiliations': ['Metastone Technology, Beijing, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.02053.jpg', 'data': {'categories': ['#dataset', '#rag', '#benchmark', '#open_source', '#graphs', '#long_context'], 'emoji': 'üï∏Ô∏è', 'ru': {'title': '–ë–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω WildGraphBench ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ GraphRAG –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –í–∏–∫–∏–ø–µ–¥–∏–∏. GraphRAG –æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –≤–Ω–µ—à–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –≤ –≤–∏–¥–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞—Ñ–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –∏–∑ 1100 –≤–æ–ø—Ä–æ—Å–æ–≤ —Ä–∞–∑–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (–æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –¥–æ –∑–∞–¥–∞—á —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏) –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –í–∏–∫–∏–ø–µ–¥–∏–∏ –∏ –∏—Ö —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ GraphRAG –º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π —Ñ–∞–∫—Ç–æ–≤ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –Ω–æ —Å–∫–ª–æ–Ω–Ω—ã –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞—Ç—å –æ–±—â–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≤ —É—â–µ—Ä–± –¥–µ—Ç–∞–ª—è–º –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'Evaluating GraphRAG in Real-World Scenarios with WildGraphBench', 'desc': "WildGraphBench is a new benchmark that evaluates the performance of GraphRAG, a model that uses graph-based retrieval to enhance multi-fact aggregation and summarization. It focuses on realistic scenarios by utilizing Wikipedia's structured content, which provides a diverse set of long and heterogeneous documents. The benchmark includes 1,100 questions that test different levels of complexity, from single-fact to multi-fact and summarization tasks. Results show that while GraphRAG performs well with moderate sources, it struggles with detailed summarization due to a tendency to prioritize high-level information over finer details."}, 'zh': {'title': 'WildGraphBenchÔºöÁúüÂÆûÂú∫ÊôØ‰∏ãÁöÑÂõæÂΩ¢Â¢ûÂº∫ÁîüÊàêËØÑ‰º∞', 'desc': 'WildGraphBench ÊòØ‰∏Ä‰∏™ËØÑ‰º∞ GraphRAG Âú®Áé∞ÂÆûÂú∫ÊôØ‰∏≠Ë°®Áé∞ÁöÑÂü∫ÂáÜÔºåÂà©Áî®Áª¥Âü∫ÁôæÁßëÁöÑÁªìÊûÑÂåñÂÜÖÂÆπÊù•ÊµãËØïÂ§ö‰∫ãÂÆûËÅöÂêàÂíåÊëòË¶ÅËÉΩÂäõ„ÄÇGraphRAG ÈÄöËøáÂ∞ÜÂ§ñÈÉ®Áü•ËØÜÁªÑÁªá‰∏∫Â±ÇÊ¨°ÂõæÔºåËÉΩÂ§üÈ´òÊïàÂú∞Ê£ÄÁ¥¢ÂíåËÅöÂêàÊù•Ëá™Â§ö‰∏™ÊñáÊ°£ÁöÑÂàÜÊï£ËØÅÊçÆ„ÄÇÁé∞ÊúâÁöÑ GraphRAG Âü∫ÂáÜÂ§ßÂ§ö‰æùËµñ‰∫éÁü≠Â∞èÁöÑÁ≤æÂøÉÊåëÈÄâÁöÑÊÆµËêΩÔºåÊó†Ê≥ïÂÖÖÂàÜËØÑ‰º∞Á≥ªÁªüÂú®Èïø‰∏ä‰∏ãÊñáÂíåÂ§ßËßÑÊ®°ÂºÇÊûÑÊñáÊ°£‰∏≠ÁöÑË°®Áé∞„ÄÇWildGraphBench ÈÄöËøáÈááÊ†∑ 12 ‰∏™È°∂Á∫ß‰∏ªÈ¢òÁöÑÊñáÁ´†ÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™ÂèçÊò†ÁúüÂÆûÂú∫ÊôØÁöÑÂü∫ÂáÜÔºåÂåÖÂê´ 1,100 ‰∏™ÈóÆÈ¢òÔºåÊ∂µÁõñÂçï‰∫ãÂÆûÈóÆÁ≠î„ÄÅÂ§ö‰∫ãÂÆûÈóÆÁ≠îÂíåÁ´†ËäÇÁ∫ßÊëòË¶ÅÁ≠â‰∏çÂêåÂ§çÊùÇÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01566', 'title': 'FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents', 'url': 'https://huggingface.co/papers/2602.01566', 'abstract': 'A file-system-based dual-agent framework enables large language model agents to perform extended research tasks beyond context window limitations by using persistent storage as external memory.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.', 'score': 38, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'efb8704f7c339683', 'authors': ['Chiwei Zhu', 'Benfeng Xu', 'Mingxuan Du', 'Shaohan Wang', 'Xiaorui Wang', 'Zhendong Mao', 'Yongdong Zhang'], 'affiliations': ['Metastone Technology', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.01566.jpg', 'data': {'categories': ['#dataset', '#agents', '#benchmark', '#reasoning', '#open_source', '#long_context'], 'emoji': 'üìö', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π: –≤–Ω–µ—à–Ω—è—è –ø–∞–º—è—Ç—å –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ LLM', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è FS-Researcher ‚Äî –¥–≤—É—Ö–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –≥–¥–µ –æ–¥–∏–Ω –∞–≥–µ–Ω—Ç —Å–æ–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –∏ —Å–æ–∑–¥–∞—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –≤ —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–µ, –¥–µ–π—Å—Ç–≤—É—è –∫–∞–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä—å, –∞ –≤—Ç–æ—Ä–æ–π –∞–≥–µ–Ω—Ç –ø–∏—à–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –±–∞–∑—ã. –§–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–ª—É–∂–∏—Ç –≤–Ω–µ—à–Ω–µ–π –ø–∞–º—è—Ç—å—é, –ø–æ–∑–≤–æ–ª—è—è –∞–≥–µ–Ω—Ç–∞–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ–±—ä—ë–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–º–∏ –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ LLM. –°–∏—Å—Ç–µ–º–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –≥–ª—É–±–æ–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –º–µ–∂–¥—É —Å–µ–∞–Ω—Å–∞–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç—á—ë—Ç–æ–≤ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'Unlocking Deep Research with FS-Researcher: Beyond Context Limits!', 'desc': 'This paper presents FS-Researcher, a dual-agent framework designed to enhance the capabilities of large language models (LLMs) for deep research tasks that exceed their context window limitations. The framework utilizes a file system as external memory, allowing a Context Builder agent to gather and organize information from the internet into a structured knowledge base. A Report Writer agent then uses this knowledge base to generate comprehensive reports, enabling iterative refinement and improved report quality. Experiments demonstrate that FS-Researcher outperforms existing methods, showing that effective allocation of resources to the Context Builder correlates with higher report quality.'}, 'zh': {'title': 'Ë∂ÖË∂ä‰∏ä‰∏ãÊñáÈôêÂà∂ÁöÑÊ∑±Â∫¶Á†îÁ©∂Ê°ÜÊû∂', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫FS-ResearcherÁöÑÂèå‰ª£ÁêÜÊ°ÜÊû∂ÔºåÊó®Âú®Â∏ÆÂä©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®ËøõË°åÊ∑±Â∫¶Á†îÁ©∂Êó∂Ë∂ÖË∂ä‰∏ä‰∏ãÊñáÁ™óÂè£ÁöÑÈôêÂà∂„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®Êñá‰ª∂Á≥ªÁªü‰Ωú‰∏∫ÊåÅ‰πÖÂ≠òÂÇ®ÔºåÂÖÅËÆ∏‰∏Ä‰∏™‰ª£ÁêÜË¥üË¥£Êî∂ÈõÜÂíåÊï¥ÁêÜ‰ø°ÊÅØÔºåËÄåÂè¶‰∏Ä‰∏™‰ª£ÁêÜÂàôË¥üË¥£Êí∞ÂÜôÊä•Âëä„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåÁ†îÁ©∂ËÄÖÂèØ‰ª•Âú®Êõ¥ÈïøÁöÑÊó∂Èó¥Ë∑®Â∫¶ÂÜÖËøõË°åÊúâÊïàÁöÑÁ†îÁ©∂ÔºåËÄå‰∏çÂèóÈôê‰∫éÊ®°ÂûãÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFS-ResearcherÂú®Êä•ÂëäË¥®Èáè‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥ÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®ÊµãËØïÊó∂Êâ©Â±ïËÉΩÂäõÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02361', 'title': 'SWE-Universe: Scale Real-World Verifiable Environments to Millions', 'url': 'https://huggingface.co/papers/2602.02361', 'abstract': 'A scalable framework for constructing real-world software engineering environments from GitHub pull requests using an efficient building agent with self-verification and hacking detection capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.', 'score': 32, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '59a87be075e0af02', 'authors': ['Mouxiang Chen', 'Lei Zhang', 'Yunlong Feng', 'Xuwu Wang', 'Wenting Zhao', 'Ruisheng Cao', 'Jiaxi Yang', 'Jiawei Chen', 'Mingze Li', 'Zeyao Ma', 'Hao Ge', 'Zongmeng Zhang', 'Zeyu Cui', 'Dayiheng Liu', 'Jingren Zhou', 'Jianling Sun', 'Junyang Lin', 'Binyuan Hui'], 'affiliations': ['Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02361.jpg', 'data': {'categories': ['#multilingual', '#plp', '#dataset', '#agents', '#benchmark', '#rl', '#training'], 'emoji': 'üèóÔ∏è', 'ru': {'title': '–ú–∏–ª–ª–∏–æ–Ω–Ω—ã–π –º–∞—Å—à—Ç–∞–± –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –∑–∞–¥–∞—á —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ —É–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤', 'desc': "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è SWE-Universe ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ pull request'–æ–≤ –∏–∑ GitHub. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫—É –∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ–ø—ã—Ç–æ–∫ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä–∞–º —É–¥–∞–ª–æ—Å—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏–π –¥–æ –º–∏–ª–ª–∏–æ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ (807,693), —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª–∏–ª–∏ –¥–æ—Å—Ç–∏—á—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∫–æ–¥–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –±–æ–ª—å—à–æ–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–¥–∞."}, 'en': {'title': 'Building Reliable Software Environments at Scale', 'desc': 'The paper introduces SWE-Universe, a framework designed to automatically create verifiable software engineering environments from GitHub pull requests. It addresses common issues in automatic building, such as low success rates and ineffective verification processes, by using a custom-trained building agent. This agent incorporates self-verification and hacking detection to ensure the environments generated are reliable and high-quality. The framework successfully scales to nearly a million environments and demonstrates its effectiveness through applications in reinforcement learning and agent training.'}, 'zh': {'title': 'ÊûÑÂª∫ÂèØÈ™åËØÅËΩØ‰ª∂Â∑•Á®ãÁéØÂ¢ÉÁöÑÈ´òÊïàÊ°ÜÊû∂', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫ÜSWE-UniverseÔºåËøôÊòØ‰∏Ä‰∏™ÂèØÊâ©Â±ï‰∏îÈ´òÊïàÁöÑÊ°ÜÊû∂ÔºåÁî®‰∫é‰ªéGitHubÊãâÂèñËØ∑Ê±ÇËá™Âä®ÊûÑÂª∫ÂèØÈ™åËØÅÁöÑÁúüÂÆûËΩØ‰ª∂Â∑•Á®ãÁéØÂ¢É„ÄÇ‰∏∫‰∫ÜÂÖãÊúçËá™Âä®ÊûÑÂª∫‰∏≠ÁöÑÂ∏∏ËßÅÊåëÊàòÔºåÂ¶Ç‰ΩéÁîü‰∫ßÁéáÂíåÂº±È™åËØÅÂô®ÔºåÊàë‰ª¨ÁöÑÊ°ÜÊû∂Âà©Áî®‰∫Ü‰∏Ä‰∏™Áî±È´òÊïàÂÆöÂà∂Ê®°ÂûãÈ©±Âä®ÁöÑÊûÑÂª∫‰ª£ÁêÜ„ÄÇËØ•‰ª£ÁêÜÈááÁî®Ëø≠‰ª£Ëá™È™åËØÅÂíåÂæ™ÁéØÈªëÂÆ¢Ê£ÄÊµãÔºåÁ°Æ‰øùÁîüÊàêÈ´ò‰øùÁúüÂ∫¶ÁöÑÂèØÈ™åËØÅ‰ªªÂä°„ÄÇÈÄöËøáËøôÁßçÊñπÊ≥ïÔºåÊàë‰ª¨Â∞ÜÁúüÂÆûÂ§öËØ≠Ë®ÄËΩØ‰ª∂Â∑•Á®ãÁéØÂ¢ÉÁöÑÊï∞ÈáèÊâ©Â±ïÂà∞Áôæ‰∏áËßÑÊ®°ÔºåÂπ∂Âú®Qwen3-Max-Thinking‰∏äÂèñÂæó‰∫Ü75.3%ÁöÑÂæóÂàÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01590', 'title': 'Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles', 'url': 'https://huggingface.co/papers/2602.01590', 'abstract': "Deep Research Agents demonstrate capabilities in autonomous information retrieval but show significant gaps when evaluated against expert-level Wikipedia articles using a new live benchmark and comprehensive evaluation framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge", 'score': 30, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '1e1b08d9febd5020', 'authors': ['Shaohan Wang', 'Benfeng Xu', 'Licheng Zhang', 'Mingxuan Du', 'Chiwei Zhu', 'Xiaorui Wang', 'Zhendong Mao', 'Yongdong Zhang'], 'affiliations': ['Metastone Technology, Beijing, China', 'University of Science and Technology of China, Hefei, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.01590.jpg', 'data': {'categories': ['#dataset', '#agents', '#benchmark', '#open_source', '#survey'], 'emoji': 'üìö', 'ru': {'title': '–ò–∑–º–µ—Ä—è—è –¥–∏—Å—Ç–∞–Ω—Ü–∏—é –º–µ–∂–¥—É AI-–∞–≥–µ–Ω—Ç–∞–º–∏ –∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è benchmark Wiki Live Challenge, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞—Ç—å–∏ Wikipedia Good Articles –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ Deep Research Agents. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Wiki Eval —Å 39 –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∏—Å—å–º–∞ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æÃÅ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–æ–≤. –ü—Ä–æ–≤–µ–¥—ë–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö DRA –≤—ã—è–≤–∏–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ –∏ —Å—Ç–∞—Ç—å—è–º–∏, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ Wikipedia. –ù–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ–æ–±—â–µ—Å—Ç–≤—É —Ä–∞–∑–≤–∏–≤–∞—Ç—å –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–µ –∏ —Ç–æ—á–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.'}, 'en': {'title': 'Bridging the Gap: Evaluating AI with Expert Standards', 'desc': "Deep Research Agents (DRAs) are AI systems designed to autonomously retrieve information and generate reports, showing promise in aiding complex research tasks. However, when evaluated against expert-level Wikipedia articles, DRAs reveal significant shortcomings, particularly in reliability and objective assessment. The Wiki Live Challenge (WLC) introduces a new benchmark using Wikipedia's Good Articles as a standard for evaluation, emphasizing neutrality and verifiability. This framework includes a detailed evaluation method with 39 criteria, highlighting the performance gap between DRAs and human experts, thus pushing the boundaries of agent research."}, 'zh': {'title': 'ÊèêÂçáÊ∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜÁöÑËØÑ‰º∞Ê†áÂáÜ', 'desc': 'Ê∑±Â∫¶Á†îÁ©∂‰ª£ÁêÜÔºàDRAÔºâÂú®Ëá™‰∏ª‰ø°ÊÅØÊ£ÄÁ¥¢ÂíåÊä•ÂëäÁîüÊàêÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®‰∏é‰∏ìÂÆ∂Á∫ßÁª¥Âü∫ÁôæÁßëÊñáÁ´†ÁöÑËØÑ‰º∞‰∏≠Â≠òÂú®ÊòæËëóÂ∑ÆË∑ù„ÄÇÂΩìÂâçÁöÑËØÑ‰º∞Ê°ÜÊû∂‰∏ªË¶Å‰æùËµñ‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁîüÊàêÁöÑÂèÇËÄÉËµÑÊñôÔºåÁº∫‰πè‰∏ìÂÆ∂È™åËØÅÂÜÖÂÆπÁöÑÂèØÈù†ÊÄß„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜÁª¥Âü∫ÂÆûÊó∂ÊåëÊàòÔºàWLCÔºâÔºåÂà©Áî®ÊúÄÊñ∞ÁöÑÁª¥Âü∫ÁôæÁßë‰ºòË¥®ÊñáÁ´†‰Ωú‰∏∫‰∏ìÂÆ∂Á∫ßÂèÇËÄÉ„ÄÇÈÄöËøáÂØπ100ÁØá‰ºòË¥®ÊñáÁ´†ÁöÑËØÑ‰º∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜWiki EvalÊ°ÜÊû∂ÔºåÂåÖÂê´39‰∏™ÂÜô‰ΩúË¥®ÈáèÊ†áÂáÜÂíå‰∏•Ê†ºÁöÑ‰∫ãÂÆûÂèØÈ™åËØÅÊÄßÊåáÊ†áÔºåÈ™åËØÅ‰∫ÜDRA‰∏é‰∫∫Á±ª‰∏ìÂÆ∂‰πãÈó¥ÁöÑÂ∑ÆË∑ù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02493', 'title': 'PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss', 'url': 'https://huggingface.co/papers/2602.02493', 'abstract': 'PixelGen is a pixel-space diffusion framework that uses perceptual supervision through LPIPS and DINO-based losses to generate high-quality images without requiring VAEs or latent representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.', 'score': 28, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '4772b7581ff51570', 'authors': ['Zehong Ma', 'Ruihan Xu', 'Shiliang Zhang'], 'affiliations': ['State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02493.jpg', 'data': {'categories': [], 'emoji': 'üé®', 'ru': {'title': '–ü—Ä—è–º–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —á–µ—Ä–µ–∑ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–π –Ω–∞–¥–∑–æ—Ä', 'desc': 'PixelGen ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä—è–º–æ –≤ –ø–∏–∫—Å–µ–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –∏–∑–±–µ–≥–∞—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤–Ω–æ—Å—è—Ç VAE –≤ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã—Ö –º–µ—Ç–æ–¥–∞—Ö. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–π –Ω–∞–¥–∑–æ—Ä —á–µ—Ä–µ–∑ –¥–≤–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å: LPIPS –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –∏ DINO-based –ø–æ—Ç–µ—Ä–∏ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å–µ–º–∞–Ω—Ç–∏–∫–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º–æ–º –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω–æ–º –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏ –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω–æ–≥–æ –ø–∏–∫—Å–µ–ª—å–Ω–æ–≥–æ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏—è. PixelGen –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (FID 5.11 –Ω–∞ ImageNet-256), –Ω–µ —Ç—Ä–µ–±—É—è VAE –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–º –∏ –º–æ—â–Ω—ã–º –ø–∞—Ä–∞–¥–∏–≥–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'PixelGen: Simplifying Image Generation with Perceptual Supervision', 'desc': 'PixelGen is a novel pixel-space diffusion framework that generates high-quality images directly in pixel space, eliminating the need for variational autoencoders (VAEs) or latent representations. It utilizes perceptual supervision through two types of losses: LPIPS for enhancing local image patterns and DINO for improving global semantic understanding. This approach allows PixelGen to effectively navigate the complex high-dimensional pixel space, overcoming challenges faced by traditional pixel diffusion methods. As a result, PixelGen achieves impressive performance metrics, including a low FID score on ImageNet-256, demonstrating its capability in large-scale text-to-image generation tasks.'}, 'zh': {'title': 'PixelGenÔºöÁÆÄÂåñËÄåÂº∫Â§ßÁöÑÂõæÂÉèÁîüÊàêÊ°ÜÊû∂', 'desc': 'PixelGenÊòØ‰∏ÄÁßçÂÉèÁ¥†Á©∫Èó¥Êâ©Êï£Ê°ÜÊû∂ÔºåÈÄöËøáLPIPSÂíåÂü∫‰∫éDINOÁöÑÊçüÂ§±ËøõË°åÊÑüÁü•ÁõëÁù£ÔºåËÉΩÂ§üÁîüÊàêÈ´òË¥®ÈáèÁöÑÂõæÂÉèÔºåËÄåÊó†ÈúÄ‰ΩøÁî®ÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÊàñÊΩúÂú®Ë°®Á§∫„ÄÇËØ•ÊñπÊ≥ïÁõ¥Êé•Âú®ÂÉèÁ¥†Á©∫Èó¥‰∏≠ÁîüÊàêÂõæÂÉèÔºåÈÅøÂÖç‰∫Ü‰∏§Èò∂ÊÆµÊΩúÂú®Êâ©Êï£‰∏≠ÂºïÂÖ•ÁöÑ‰º™ÂΩ±ÂíåÁì∂È¢à„ÄÇPixelGenÂºïÂÖ•‰∫Ü‰∏§Áßç‰∫íË°•ÁöÑÊÑüÁü•ÊçüÂ§±ÔºåÂ∏ÆÂä©Êâ©Êï£Ê®°ÂûãÂ≠¶‰π†Êõ¥ÊúâÊÑè‰πâÁöÑÊÑüÁü•ÊµÅÂΩ¢Ôºå‰ªéËÄå‰ºòÂåñÈ´òÁª¥ÂÉèÁ¥†ÊµÅÂΩ¢„ÄÇÈÄöËøáÊÑüÁü•ÁõëÁù£ÔºåPixelGenÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÂº∫Â§ßÁöÑÊΩúÂú®Êâ©Êï£Ê®°ÂûãÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Âú®Â§ßËßÑÊ®°ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠ÁöÑ‰ºòË∂äÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02383', 'title': 'SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization', 'url': 'https://huggingface.co/papers/2602.02383', 'abstract': "SLIME is a novel reference-free alignment objective for large language models that decouples preference learning from generation quality through a three-pronged approach combining likelihood maximization, probability stabilization, and dual-margin constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.", 'score': 26, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '0aa05d3f99126285', 'authors': ['Maksim Afanasyev', 'Illarion Iov'], 'affiliations': ['Floating Point Sigma Lab'], 'pdf_title_img': 'assets/pdf/title_img/2602.02383.jpg', 'data': {'categories': ['#alignment', '#rlhf', '#training'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–°—Ç–∞–±–∏–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–µ–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º', 'desc': 'SLIME ‚Äî —ç—Ç–æ –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ü–µ–ª–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ –º–µ—Ç–æ–¥–∞—Ö –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ü–æ–¥—Ö–æ–¥ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —Ç—Ä–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—é –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –æ—Ç–∫–ª–æ–Ω—ë–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –¥–≤–æ–π–Ω—ã–µ margin-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã ¬´—Ä–∞–∑—É—á–∏–≤–∞–Ω–∏—è¬ª –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤—ã—Ö–æ–¥–æ–≤ –∏ ¬´–∫–æ–ª–ª–∞–ø—Å–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è¬ª, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–∏ –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –ø–µ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–∫–ª–æ–Ω—ë–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SLIME –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –±–æ–ª—å—à—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'SLIME: Aligning Preferences with Stability in Language Models', 'desc': "SLIME is a new method for aligning large language models without needing reference data. It separates the learning of user preferences from the quality of the generated text. The approach uses three key strategies: maximizing the likelihood of preferred responses, stabilizing the probabilities of less preferred options, and applying dual-margin constraints for better control. This results in improved performance and stability in the model's outputs compared to existing methods."}, 'zh': {'title': 'SLIMEÔºöËß£ËÄ¶ÂÅèÂ•ΩÂ≠¶‰π†‰∏éÁîüÊàêË¥®ÈáèÁöÑÂàõÊñ∞ÂØπÈΩêÁõÆÊ†á', 'desc': 'SLIMEÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊó†ÂèÇËÄÉÂØπÈΩêÁõÆÊ†áÔºåÊó®Âú®ÈÄöËøá‰∏âÈáçÊñπÊ≥ïÂ∞ÜÂÅèÂ•ΩÂ≠¶‰π†‰∏éÁîüÊàêË¥®ÈáèËß£ËÄ¶„ÄÇÂÆÉÁªìÂêà‰∫Ü‰ººÁÑ∂ÊúÄÂ§ßÂåñ„ÄÅÊ¶ÇÁéáÁ®≥ÂÆöÂåñÂíåÂèåËæπÁïåÁ∫¶ÊùüÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂØπÈΩêÊïàÊûú„ÄÇSLIMEÁöÑËÆæËÆ°ÈÅøÂÖç‰∫ÜÊ®°ÂûãÂú®‰ºòÂåñÁõ∏ÂØπËæπÈôÖÊó∂ÂèØËÉΩÂØºËá¥ÁöÑÈ´òË¥®ÈáèËæìÂá∫Ê¶ÇÁéá‰∏ãÈôçÂíåÊ†ºÂºèÂ¥©Ê∫ÉÈóÆÈ¢ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSLIMEÂú®ÊÄßËÉΩ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÂü∫Á∫øÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊõ¥È´òÁöÑÁîüÊàêÁ®≥ÂÆöÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02488', 'title': 'RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System', 'url': 'https://huggingface.co/papers/2602.02488', 'abstract': 'RLAnything enhances reinforcement learning for LLMs and agents through dynamic model optimization and closed-loop feedback mechanisms that improve policy and reward model training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL', 'score': 25, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'd1b9b86c47a0ffda', 'authors': ['Yinjie Wang', 'Tianbao Xie', 'Ke Shen', 'Mengdi Wang', 'Ling Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2602.02488.jpg', 'data': {'categories': ['#rl', '#agents', '#training'], 'emoji': 'üîÑ', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∑–∞–º–∫–Ω—É—Ç—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': 'RLAnything ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è, –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∑–∞–º–∫–Ω—É—Ç—ã–µ —Ü–∏–∫–ª—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –ü–æ–ª–∏—Ç–∏–∫–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—à–∞–≥–æ–≤—ã—Ö –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏, –∞ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Å–∏–≥–Ω–∞–ª—ã –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –æ–∫—Ä—É–∂–µ–Ω–∏—è —É–ª—É—á—à–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—è —Å–∏—Å—Ç–µ–º–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —É—á–∏—Ç—å—Å—è –Ω–∞ –æ–ø—ã—Ç–µ. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –¥–ª—è LLM –∏ –∞–≥–µ–Ω—Ç–æ–≤, –ø–æ–≤—ã—à–∞—è –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ 9-18% –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏.'}, 'en': {'title': 'Boosting Learning with RLAnything: Dynamic Optimization for LLMs and Agents', 'desc': 'RLAnything is a novel reinforcement learning framework designed to enhance the training of large language models (LLMs) and agents. It utilizes dynamic model optimization and closed-loop feedback mechanisms to improve both policy and reward model training. By integrating feedback from both step-wise actions and overall outcomes, RLAnything strengthens the learning signals, leading to better performance. The framework also adapts the training environment based on critic feedback, allowing models to learn more effectively from their experiences, resulting in significant performance improvements across various tasks.'}, 'zh': {'title': 'RLAnythingÔºöÂä®ÊÄÅ‰ºòÂåñÂº∫ÂåñÂ≠¶‰π†ÁöÑÊú™Êù•', 'desc': 'RLAnythingÊòØ‰∏Ä‰∏™Â¢ûÂº∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÊ°ÜÊû∂Ôºå‰∏ì‰∏∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂíåÊô∫ËÉΩ‰ΩìËÆæËÆ°„ÄÇÂÆÉÈÄöËøáÂä®ÊÄÅ‰ºòÂåñÁéØÂ¢É„ÄÅÁ≠ñÁï•ÂíåÂ•ñÂä±Ê®°ÂûãÔºåÂà©Áî®Èó≠ÁéØÂèçÈ¶àÊú∫Âà∂Êù•ÊèêÂçáÂ≠¶‰π†ÊïàÊûú„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÈÄêÊ≠•ÂèçÈ¶àÂíåÁªìÊûú‰ø°Âè∑Ôºå‰ºòÂåñÁ≠ñÁï•ËÆ≠ÁªÉÔºåÂêåÊó∂ÈÄöËøá‰∏ÄËá¥ÊÄßÂèçÈ¶àÂÖ±Âêå‰ºòÂåñÂ•ñÂä±Ê®°ÂûãÔºå‰ªéËÄåËøõ‰∏ÄÊ≠•ÊîπÂñÑÁ≠ñÁï•ËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRLAnythingÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01624', 'title': 'PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards', 'url': 'https://huggingface.co/papers/2602.01624', 'abstract': 'PISCES is an annotation-free text-to-video generation method that uses dual optimal transport-aligned rewards to improve visual quality and semantic alignment without human preference annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present PISCES, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, PISCES uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, PISCES is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that PISCES outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.', 'score': 23, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'cc87c0d6fe4774f9', 'authors': ['Minh-Quan Le', 'Gaurav Mittal', 'Cheng Zhao', 'David Gu', 'Dimitris Samaras', 'Mei Chen'], 'affiliations': ['Microsoft', 'Stony Brook University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01624.jpg', 'data': {'categories': ['#optimization', '#rl', '#video', '#training'], 'emoji': 'üé¨', 'ru': {'title': '–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –¥–ª—è –±–µ–∑–∞–Ω–Ω–æ—Ç–∞—Ü–∏–æ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞', 'desc': 'PISCES ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞–≥—Ä–∞–¥—ã —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å—É–∂–¥–µ–Ω–∏–µ–º –Ω–∞ –¥–≤—É—Ö —É—Ä–æ–≤–Ω—è—Ö: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å–Ω–æ–º –∏ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ —Ç–∏–ø–∞ –Ω–∞–≥—Ä–∞–¥—ã ‚Äî –æ–¥–Ω–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–±—â–µ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –¥—Ä—É–≥–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–º –∏ –≤–∏–¥–µ–æ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ PISCES –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –∫–∞–∫ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏, —Ç–∞–∫ –∏ –±–µ–∑ –Ω–∏—Ö.'}, 'en': {'title': 'PISCES: Annotation-Free Video Generation with Optimal Transport Rewards', 'desc': 'PISCES is a novel method for generating videos from text without needing human annotations. It uses a technique called Dual Optimal Transport to align rewards that improve both the visual quality and the semantic relevance of the generated videos. By bridging text and video embeddings, PISCES ensures that the generated content is coherent and matches the intended meaning. Experiments demonstrate that PISCES outperforms existing methods, making it a significant advancement in text-to-video generation.'}, 'zh': {'title': 'Êó†Ê≥®ÈáäÁöÑÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÊñ∞ÊñπÊ≥ï', 'desc': 'PISCESÊòØ‰∏ÄÁßçÊó†Ê≥®ÈáäÁöÑÊñáÊú¨Âà∞ËßÜÈ¢ëÁîüÊàêÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÁîüÊàêËßÜÈ¢ëÁöÑËßÜËßâË¥®ÈáèÂíåËØ≠‰πâ‰∏ÄËá¥ÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂèåÈáçÊúÄ‰ºò‰º†ËæìÂØπÈΩêÂ•ñÂä±Ê®°ÂùóÔºåËß£ÂÜ≥‰∫Ü‰æùËµñ‰∫∫Â∑•ÂÅèÂ•ΩÊ≥®ÈáäÁöÑÂ±ÄÈôêÊÄß„ÄÇPISCES‰ΩøÁî®ÊúÄ‰ºò‰º†ËæìÊäÄÊúØÂú®ÊñáÊú¨ÂíåËßÜÈ¢ëÂµåÂÖ•‰πãÈó¥Âª∫Á´ãËÅîÁ≥ªÔºå‰ªéËÄåÂÆûÁé∞Â•ñÂä±‰ø°Âè∑‰∏é‰∫∫Á±ªÂà§Êñ≠ÁöÑÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPISCESÂú®ËßÜÈ¢ëÁîüÊàêÁöÑË¥®ÈáèÂíåËØ≠‰πâËØÑÂàÜ‰∏ä‰ºò‰∫éÂÖ∂‰ªñÊñπÊ≥ïÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01058', 'title': 'Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.01058', 'abstract': 'Post-training of reasoning large language models can be improved by correcting distribution mismatches between supervised fine-tuning and reinforcement learning stages through importance sampling reweighting of the SFT loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.   We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.   We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.   We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.', 'score': 22, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': 'f8ee0b48de46d590', 'authors': ['Dylan Zhang', 'Yufeng Xu', 'Haojin Wang', 'Qingzhi Chen', 'Hao Peng'], 'affiliations': ['New York University (Shanghai)', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2602.01058.jpg', 'data': {'categories': ['#math', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è —Å–∏–Ω–µ—Ä–≥–∏–∏ SFT –∏ RL –≤ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ PEAR –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ —ç—Ç–∞–ø–∞—Ö supervised fine-tuning –∏ reinforcement learning. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –º–µ–∂–¥—É –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è SFT –∏ –ø–æ–ª–∏—Ç–∏–∫–æ–π, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º–æ–π –≤–æ –≤—Ä–µ–º—è RL-–æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –†–µ—à–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç importance sampling –¥–ª—è –ø–µ—Ä–µ–≤–µÃÅ—à–∏–≤–∞–Ω–∏—è –ø–æ—Ç–µ—Ä—å –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤, –±–ª–æ–∫–æ–≤ –∏–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ü–µ–ª–∏–∫–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 14,6% –ø–æ –º–µ—Ç—Ä–∏–∫–µ pass@8 –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ AIME2025.'}, 'en': {'title': 'Bridging the Gap: Enhancing LLMs with PEAR for Better Post-Training Performance', 'desc': 'This paper discusses how to improve the post-training of reasoning large language models (LLMs) by addressing the differences between supervised fine-tuning (SFT) and reinforcement learning (RL) stages. The authors introduce a method called PEAR, which uses importance sampling to adjust the SFT loss, ensuring that the model is better prepared for the RL phase. They demonstrate that models trained with PEAR show significant performance improvements in reasoning tasks compared to those trained with traditional SFT methods. The findings highlight the importance of aligning SFT and RL processes to enhance overall model effectiveness.'}, 'zh': {'title': '‰ºòÂåñÂêéËÆ≠ÁªÉÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂêéËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÈóÆÈ¢òÔºåÁâπÂà´ÊòØÂú®ÁõëÁù£ÂæÆË∞ÉÔºàSFTÔºâÂíåÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÈò∂ÊÆµ‰πãÈó¥ÁöÑÂàÜÂ∏É‰∏çÂåπÈÖç„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊù•Ëá™Âº∫SFTÊ£ÄÊü•ÁÇπÁöÑÊ®°ÂûãÂú®ÁªèËøáÁõ∏ÂêåÁöÑRLËÆ≠ÁªÉÂêéÔºåË°®Áé∞ÂèØËÉΩ‰∏çÂ¶ÇÊù•Ëá™Âº±SFTÊ£ÄÊü•ÁÇπÁöÑÊ®°Âûã„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫PEARÁöÑÁÆóÊ≥ïÔºåÈÄöËøáÈáçË¶ÅÊÄßÈááÊ†∑ÂØπSFTÊçüÂ§±ËøõË°åÈáçÂä†ÊùÉÔºå‰ªéËÄåÊõ¥Â•ΩÂú∞‰∏∫RLÈò∂ÊÆµÂÅöÂáÜÂ§á„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåPEARÂú®Â§ö‰∏™Êé®ÁêÜ‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÂêéRLÊÄßËÉΩÔºåË°®ÊòéÂú®ËÆæËÆ°SFTÊó∂ËÄÉËôë‰∏ãÊ∏∏RLÁöÑÂΩ±ÂìçÊòØÊúâÊïàÁöÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01756', 'title': 'Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation', 'url': 'https://huggingface.co/papers/2602.01756', 'abstract': "Mind-Brush presents a unified agentic framework for text-to-image generation that dynamically retrieves multimodal evidence and employs reasoning tools to improve understanding of implicit user intentions and complex knowledge reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.", 'score': 21, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '296ad3c0ea5b092d', 'authors': ['Jun He', 'Junyan Ye', 'Zilong Huang', 'Dongzhi Jiang', 'Chenjue Zhang', 'Leqi Zhu', 'Renrui Zhang', 'Xiang Zhang', 'Weijia Li'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai Artificial Intelligence Laboratory', 'Sun Yat-sen University', 'Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01756.jpg', 'data': {'categories': ['#multimodal', '#agents', '#benchmark', '#rag', '#reasoning'], 'emoji': 'üé®', 'ru': {'title': '–ê–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∑–Ω–∞–Ω–∏–π', 'desc': "Mind-Brush –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –Ω–µ—è–≤–Ω—ã—Ö –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –ø–∏–∫—Å–µ–ª–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏–º–∏—Ç–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É '–¥—É–º–∞–π-–∏—Å—Å–ª–µ–¥—É–π-—Å–æ–∑–¥–∞–≤–∞–π', –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—è –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–µ—è–≤–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –≤–≤–µ–¥–ª–∏ Mind-Bench - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –∏–∑ 500 –ø—Ä–∏–º–µ—Ä–æ–≤, –ø–æ–∫—Ä—ã–≤–∞—é—â–∏—Ö –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏, –Ω–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ç–∏–ø–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –≥–µ–æ–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –¥–æ—Å—Ç–∏–≥–∞—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–∫–∞—á–∫–∞ –Ω–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Qwen-Image."}, 'en': {'title': 'Mind-Brush: Dynamic Understanding for Text-to-Image Generation', 'desc': "Mind-Brush is a new framework for generating images from text that improves how AI understands what users really want. Unlike traditional models that just convert text to images, Mind-Brush actively gathers information from various sources to better interpret complex ideas. It uses a 'think-research-create' approach, allowing it to adapt to new information and solve intricate reasoning tasks. The framework has been tested with a new benchmark called Mind-Bench, showing significant improvements in performance over existing models."}, 'zh': {'title': 'Mind-BrushÔºöÂä®ÊÄÅÁü•ËØÜÈ©±Âä®ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ°ÜÊû∂', 'desc': 'Mind-BrushÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊô∫ËÉΩÊ°ÜÊû∂ÔºåÁî®‰∫éÊñáÊú¨Âà∞ÂõæÂÉèÁöÑÁîüÊàêÔºåËÉΩÂ§üÂä®ÊÄÅÊ£ÄÁ¥¢Â§öÊ®°ÊÄÅËØÅÊçÆÂπ∂ËøêÁî®Êé®ÁêÜÂ∑•ÂÖ∑Ôºå‰ª•ÊèêÈ´òÂØπÁî®Êà∑ÈöêÂê´ÊÑèÂõæÂíåÂ§çÊùÇÁü•ËØÜÊé®ÁêÜÁöÑÁêÜËß£„ÄÇÁé∞ÊúâÁöÑÂ§ßÂ§öÊï∞Ê®°Âûã‰Ωú‰∏∫ÈùôÊÄÅÊñáÊú¨Âà∞ÂÉèÁ¥†ÁöÑËß£Á†ÅÂô®ÔºåÊó†Ê≥ïÊúâÊïàÊääÊè°Áî®Êà∑ÁöÑÈöêÂê´ÊÑèÂõæ„ÄÇÂ∞ΩÁÆ°Êñ∞ÂÖ¥ÁöÑÁªü‰∏ÄÁêÜËß£ÁîüÊàêÊ®°ÂûãÊúâÊâÄÊîπÂñÑÔºå‰ΩÜÂú®Â§ÑÁêÜÂ§çÊùÇÁü•ËØÜÊé®ÁêÜÊó∂‰ªçÁÑ∂Â≠òÂú®Âõ∞Èöæ„ÄÇMind-BrushÈÄöËøáÊ®°Êãü‰∫∫Á±ªÁöÑ‚ÄúÊÄùËÄÉ-Á†îÁ©∂-ÂàõÈÄ†‚ÄùËøáÁ®ãÔºåËΩ¨Âèò‰∏∫‰∏Ä‰∏™Âä®ÊÄÅÁöÑ„ÄÅÁü•ËØÜÈ©±Âä®ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºåÊòæËëóÊèêÂçá‰∫ÜÁªü‰∏ÄÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02214', 'title': 'Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation', 'url': 'https://huggingface.co/papers/2602.02214', 'abstract': "A novel Causal Forcing method addresses the architectural gap in distilling bidirectional video diffusion models into autoregressive models by using AR teachers for ODE initialization, significantly improving video generation performance.  \t\t\t\t\tAI-generated summary \t\t\t\t To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\\% in Dynamic Degree, 8.7\\% in VisionReward, and 16.7\\% in Instruction Following. Project page and the code: https://thu-ml.github.io/CausalForcing.github.io/{https://thu-ml.github.io/CausalForcing.github.io/}", 'score': 19, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'ffbfd173cf4147bb', 'authors': ['Hongzhou Zhu', 'Min Zhao', 'Guande He', 'Hang Su', 'Chongxuan Li', 'Jun Zhu'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch ML Center, Tsinghua University', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE', 'Gaoling School of Artificial Intelligence Renmin University of China Beijing, China', 'Pazhou Laboratory (Huangpu)', 'ShengShu', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2602.02214.jpg', 'data': {'categories': ['#architecture', '#video', '#training'], 'emoji': 'üé¨', 'ru': {'title': '–ü—Ä–∏—á–∏–Ω–Ω–æ–µ –ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏–µ: –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —É—á–∏—Ç–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ Causal Forcing –¥–ª—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Ä–µ—à–∞—é—â–∏–π –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –ø—Ä–∏ –∑–∞–º–µ–Ω–µ –ø–æ–ª–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –ø—Ä–∏—á–∏–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ODE —Å –ø–æ–º–æ—â—å—é –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —É—á–∏—Ç–µ–ª—è –Ω–∞—Ä—É—à–∞–µ—Ç —É—Å–ª–æ–≤–∏–µ –∏–Ω—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–∞–¥—Ä–æ–≤, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ —É—á–∏—Ç–µ–ª—è –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ODE, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏: —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 19.3% –ø–æ Dynamic Degree, 8.7% –ø–æ VisionReward –∏ 16.7% –ø–æ –º–µ—Ç—Ä–∏–∫–µ Instruction Following.'}, 'en': {'title': 'Bridging the Gap in Video Generation with Causal Forcing', 'desc': 'This paper introduces a new method called Causal Forcing, which improves the process of converting bidirectional video diffusion models into autoregressive (AR) models for better video generation. The challenge arises from the architectural differences when switching from full attention to causal attention, which existing methods fail to address theoretically. The authors highlight that previous approaches struggle with ODE initialization due to the requirement of frame-level injectivity, leading to performance degradation. By utilizing an AR teacher for ODE initialization, Causal Forcing effectively bridges this gap, resulting in significant performance improvements over existing methods.'}, 'zh': {'title': 'Âõ†ÊûúÂº∫Âà∂ÔºöÊèêÂçáËßÜÈ¢ëÁîüÊàêÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂõ†ÊûúÂº∫Âà∂ÊñπÊ≥ïÔºåÊó®Âú®Ëß£ÂÜ≥Â∞ÜÂèåÂêëËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãËí∏È¶è‰∏∫Ëá™ÂõûÂΩíÊ®°ÂûãÊó∂ÁöÑÊû∂ÊûÑÂ∑ÆË∑ù„ÄÇËØ•ÊñπÊ≥ïÈÄöËøá‰ΩøÁî®Ëá™ÂõûÂΩíÊïôÂ∏àËøõË°åÂ∏∏ÂæÆÂàÜÊñπÁ®ãÔºàODEÔºâÂàùÂßãÂåñÔºåÊòæËëóÊèêÈ´ò‰∫ÜËßÜÈ¢ëÁîüÊàêÊÄßËÉΩ„ÄÇ‰º†ÁªüÊñπÊ≥ïÂú®Â∞ÜÂÖ®Ê≥®ÊÑèÂäõÊõøÊç¢‰∏∫Âõ†ÊûúÊ≥®ÊÑèÂäõÊó∂Êú™ËÉΩÁêÜËÆ∫‰∏äÂº•Ë°•Ëøô‰∏ÄÂ∑ÆË∑ùÔºåÂØºËá¥ÊÄßËÉΩ‰∏ãÈôç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂõ†ÊûúÂº∫Âà∂ÊñπÊ≥ïÂú®ÊâÄÊúâÊåáÊ†á‰∏äÂùá‰ºò‰∫éÂü∫Á∫øÔºåÂ∞§ÂÖ∂Âú®Âä®ÊÄÅÂ∫¶„ÄÅËßÜËßâÂ•ñÂä±ÂíåÊåá‰ª§Ë∑üÈöèÊñπÈù¢ÂàÜÂà´ÊèêÈ´ò‰∫Ü19.3%„ÄÅ8.7%Âíå16.7%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01395', 'title': 'Rethinking Selective Knowledge Distillation', 'url': 'https://huggingface.co/papers/2602.01395', 'abstract': 'Selective knowledge distillation in autoregressive language models using student-entropy-guided position selection improves accuracy and efficiency while reducing memory and storage requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.', 'score': 18, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '4ef0dc85c3edbbd9', 'authors': ['Almog Tavor', 'Itay Ebenspanger', 'Neil Cnaan', 'Mor Geva'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01395.jpg', 'data': {'categories': ['#inference', '#optimization', '#training', '#small_models'], 'emoji': 'üß†', 'ru': {'title': '–£–º–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –≤—ã–±–∏—Ä–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–æ—á–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π (knowledge distillation) –¥–ª—è –∞–≤—Ç–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π SE-KD, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é —Å—Ç—É–¥–µ–Ω—Ç–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–∑–∏—Ü–∏–π –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∑–Ω–∞–Ω–∏–π –æ—Ç —É—á–∏—Ç–µ–ª—è. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –æ—Ç–±–æ—Ä–∞ –≤–¥–æ–ª—å –æ—Å–µ–π –ø–æ–∑–∏—Ü–∏–π, –∫–ª–∞—Å—Å–æ–≤ –∏ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ú–µ—Ç–æ–¥ SE-KD –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–∞–º—è—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–ª–æ—Ç–Ω–æ–π –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–µ–π, –∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è SE-KD 3X –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–∏–≥—Ä—ã—à –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ 70%, –ø–∏–∫–æ–≤–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ 18% –∏ –æ–±—ä—ë–º —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –Ω–∞ 80% –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞.'}, 'en': {'title': 'Efficient Knowledge Distillation with Selective Positioning', 'desc': "This paper presents a method called student-entropy-guided position selection (SE-KD) for improving knowledge distillation in autoregressive language models. The authors explore how to selectively distill knowledge by focusing on specific token positions, vocabulary classes, and training samples, rather than using all available data. Their approach shows that by carefully choosing which parts of the model to distill, they can enhance accuracy and efficiency while significantly reducing memory and storage needs. The results demonstrate that SE-KD can lead to faster processing times and lower resource consumption without compromising the model's performance."}, 'zh': {'title': 'ÈÄâÊã©ÊÄßÁü•ËØÜËí∏È¶èÔºöÊèêÂçáÊïàÁéá‰∏éÂáÜÁ°ÆÊÄß', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂú®Ëá™ÂõûÂΩíËØ≠Ë®ÄÊ®°Âûã‰∏≠ËøõË°åÈÄâÊã©ÊÄßÁü•ËØÜËí∏È¶èÁöÑÊñπÊ≥ï„ÄÇÈÄöËøáÈÄâÊã©ÊÄßËí∏È¶èÔºåÁ†îÁ©∂ËÄÖ‰ΩøÁî®ÈÉ®ÂàÜÊ†áËÆ∞‰ΩçÁΩÆ„ÄÅËØçÊ±áÁ±ªÂà´ÊàñËÆ≠ÁªÉÊ†∑Êú¨Êù•Êõø‰ª£ÂØÜÈõÜÁöÑÊïôÂ∏àÁõëÁù£Ôºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇÊñáÁ´†ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÈÄâÊã©Á≠ñÁï•ÔºåÁß∞‰∏∫Â≠¶ÁîüÁÜµÂºïÂØºÁöÑ‰ΩçÁΩÆÈÄâÊã©ÔºàSE-KDÔºâÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠È™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇÁªìÊûúÊòæÁ§∫ÔºåSE-KDÂú®ÂáèÂ∞ëÂÜÖÂ≠òÂíåÂ≠òÂÇ®ÈúÄÊ±ÇÁöÑÂêåÊó∂ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01801', 'title': 'Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention', 'url': 'https://huggingface.co/papers/2602.01801', 'abstract': 'Autoregressive video diffusion models face efficiency challenges due to growing KV caches and redundant attention computations, which are addressed through TempCache, AnnCA, and AnnSA techniques that reduce computational demands while maintaining visual quality and stable performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.', 'score': 17, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'bf9e6454eeb13993', 'authors': ['Dvir Samuel', 'Issar Tzachor', 'Matan Levy', 'Micahel Green', 'Gal Chechik', 'Rami Ben-Ari'], 'affiliations': ['Bar-Ilan University, Ramat-Gan, Israel', 'NVIDIA, Tel-Aviv, Israel', 'OriginAI, Tel-Aviv, Israel', 'The Hebrew University of Jerusalem, Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2602.01801.jpg', 'data': {'categories': ['#video', '#inference', '#diffusion', '#optimization', '#long_context'], 'emoji': '‚ö°', 'ru': {'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –≤ –≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ —Å–∂–∞—Ç–∏–µ –∫–µ—à–∞ –∏ —Ä–∞–∑—Ä–µ–∂–∏–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã —Ç—Ä–∏ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≤–∏–¥–µ–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: TempCache —Å–∂–∏–º–∞–µ—Ç KV-–∫–µ—à —á–µ—Ä–µ–∑ –≤—ã—è–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π, AnnCA —É—Å–∫–æ—Ä—è–µ—Ç –∫—Ä–æ—Å—Å-–≤–Ω–∏–º–∞–Ω–∏–µ –ø—É—Ç—ë–º –æ—Ç–±–æ—Ä–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–æ–º–ø—Ç–∞ —Å –ø–æ–º–æ—â—å—é –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π, –∏ AnnSA —Ä–∞–∑—Ä–µ–∂–∏–≤–∞–µ—Ç —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–º –∫–ª—é—á–∞–º. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è, —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 5-10 —Ä–∞–∑ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ GPU –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.'}, 'en': {'title': 'Boosting Efficiency in Video Diffusion Models', 'desc': 'This paper addresses the efficiency issues in autoregressive video diffusion models, which are crucial for generating long-form videos. The authors introduce three techniques: TempCache, AnnCA, and AnnSA, which optimize the attention mechanisms to reduce computational load while preserving visual quality. TempCache minimizes the growth of the key-value (KV) cache, AnnCA speeds up cross-attention by selecting relevant tokens, and AnnSA limits self-attention to semantically relevant keys. Together, these innovations lead to significant speed improvements and stable performance during video generation, making the models more practical for real-time applications.'}, 'zh': {'title': 'ÊèêÂçáËá™ÂõûÂΩíËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÊïàÁéáÁöÑÂàõÊñ∞ÊäÄÊúØ', 'desc': 'Ëá™ÂõûÂΩíËßÜÈ¢ëÊâ©Êï£Ê®°ÂûãÂú®ÊïàÁéá‰∏äÈù¢‰∏¥ÊåëÊàòÔºå‰∏ªË¶ÅÊòØÁî±‰∫éKVÁºìÂ≠òÁöÑÂ¢ûÈïøÂíåÂÜó‰ΩôÁöÑÊ≥®ÊÑèÂäõËÆ°ÁÆó„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∫õÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜTempCache„ÄÅAnnCAÂíåAnnSAÊäÄÊúØÔºåËøô‰∫õÊäÄÊúØÂú®‰øùÊåÅËßÜËßâË¥®ÈáèÂíåÁ®≥ÂÆöÊÄßËÉΩÁöÑÂêåÊó∂ÔºåÂáèÂ∞ë‰∫ÜËÆ°ÁÆóÈúÄÊ±Ç„ÄÇTempCacheÈÄöËøáÊó∂Èó¥ÂØπÂ∫îÂéãÁº©KVÁºìÂ≠òÔºåAnnCAÈÄöËøáÂø´ÈÄüËøë‰ººÊúÄËøëÈÇªÂåπÈÖçÂä†ÈÄüË∑®Ê≥®ÊÑèÂäõÔºåAnnSAÂàôÈÄöËøáÈôêÂà∂Êü•ËØ¢Âà∞ËØ≠‰πâÂåπÈÖçÁöÑÈîÆÊù•Á®ÄÁñèËá™Ê≥®ÊÑèÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËøô‰∫õÊñπÊ≥ïÂú®‰øùÊåÅÂá†‰πéÁõ∏ÂêåÁöÑËßÜËßâË¥®ÈáèÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫ÜÈ´òËææ5Âà∞10ÂÄçÁöÑÁ´ØÂà∞Á´ØÂä†ÈÄü„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02092', 'title': 'FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space', 'url': 'https://huggingface.co/papers/2602.02092', 'abstract': 'FSVideo is a fast transformer-based image-to-video diffusion framework that uses a compressed video autoencoder, diffusion transformer architecture with enhanced layer memory, and multi-resolution generation strategy to achieve high performance with significantly reduced computation time.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space (64times64times4 spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.', 'score': 15, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '0632387ea043193b', 'authors': ['FSVideo Team', 'Qingyu Chen', 'Zhiyuan Fang', 'Haibin Huang', 'Xinwei Huang', 'Tong Jin', 'Minxuan Lin', 'Bo Liu', 'Celong Liu', 'Chongyang Ma', 'Xing Mei', 'Xiaohui Shen', 'Yaojie Shen', 'Fuwen Tan', 'Angtian Wang', 'Xiao Yang', 'Yiding Yang', 'Jiamin Yuan', 'Lingxi Zhang', 'Yuxin Zhang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2602.02092.jpg', 'data': {'categories': ['#architecture', '#video', '#diffusion', '#optimization', '#open_source', '#training'], 'emoji': 'üé¨', 'ru': {'title': '–ü–æ—Ä—è–¥–æ–∫ –≤–µ–ª–∏—á–∏–Ω—ã –±—ã—Å—Ç—Ä–µ–µ: –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'FSVideo –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –±—ã—Å—Ç—Ä—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å –¥–∏—Ñ—Ñ—É–∑–∏–µ–π. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –≤–∏–¥–µ–æ-–∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä —Å –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é —Å–∂–∞—Ç–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–æ–ø–æ–ª–Ω–µ–Ω–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –ø–∞–º—è—Ç–∏ —Å–ª–æ—ë–≤, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–≤—ã—à–∞–µ—Ç –ø–µ—Ä–µ–¥–∞—á—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ –∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ø—Å—ç–º–ø–ª–µ—Ä–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏.'}, 'en': {'title': 'FSVideo: Fast and Efficient Image-to-Video Transformation', 'desc': 'FSVideo is a novel framework that transforms images into videos using a fast transformer-based approach. It incorporates a compressed video autoencoder that significantly reduces the data size while maintaining high-quality video reconstruction. The diffusion transformer architecture features an enhanced layer memory, which improves the flow of information between layers, allowing for better context reuse. Additionally, a multi-resolution generation strategy is employed to boost the fidelity of the generated videos, making FSVideo both efficient and effective compared to existing models.'}, 'zh': {'title': 'FSVideoÔºöÂø´ÈÄüÈ´òÊïàÁöÑÂõæÂÉèÂà∞ËßÜÈ¢ëÁîüÊàêÊ°ÜÊû∂', 'desc': 'FSVideoÊòØ‰∏Ä‰∏™Âü∫‰∫éÂø´ÈÄüÂèòÊç¢Âô®ÁöÑÂõæÂÉèÂà∞ËßÜÈ¢ëÊâ©Êï£Ê°ÜÊû∂„ÄÇÂÆÉÈááÁî®‰∫Ü‰∏ÄÁßçÂéãÁº©ÁöÑËßÜÈ¢ëËá™ÁºñÁ†ÅÂô®ÔºåÂÖ∑ÊúâÈ´òÊïàÁöÑÂ±ÇÂÜÖÂ≠òËÆæËÆ°ÔºåËÉΩÂ§üÂ¢ûÂº∫‰ø°ÊÅØÊµÅÂä®Âíå‰∏ä‰∏ãÊñáÈáçÁî®„ÄÇËØ•Ê°ÜÊû∂Ëøò‰ΩøÁî®‰∫ÜÂ§öÂàÜËæ®ÁéáÁîüÊàêÁ≠ñÁï•ÔºåÈÄöËøáÂ∞ëÈáèÊ≠•È™§ÁöÑ‰∏äÈááÊ†∑Âô®ÊèêÈ´òËßÜÈ¢ëÁöÑÊ∏ÖÊô∞Â∫¶„ÄÇÊúÄÁªàÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏ä‰∏éÂÖ∂‰ªñÊµÅË°åÁöÑÂºÄÊ∫êÊ®°ÂûãÁõ∏ÂΩìÔºå‰ΩÜËÆ°ÁÆóÈÄüÂ∫¶Âø´‰∫Ü‰∏Ä‰∏™Êï∞ÈáèÁ∫ß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01479', 'title': 'Ebisu: Benchmarking Large Language Models in Japanese Finance', 'url': 'https://huggingface.co/papers/2602.01479', 'abstract': 'A Japanese financial language understanding benchmark named Ebisu is introduced, featuring two expert-annotated tasks that evaluate implicit commitment recognition and hierarchical financial terminology extraction, revealing persistent challenges for current language models despite their advanced capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.', 'score': 15, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '19e6213f18c89cda', 'authors': ['Xueqing Peng', 'Ruoyu Xiang', 'Fan Zhang', 'Mingzi Song', 'Mingyang Jiang', 'Yan Wang', 'Lingfei Qian', 'Taiki Hara', 'Yuqing Guo', 'Jimin Huang', 'Junichi Tsujii', 'Sophia Ananiadou'], 'affiliations': ['Meiji Gakuin University', 'National Institute of Advanced Industrial Science and Technology (AIST)', 'New York University', 'The Fin AI', 'The National Centre for Text Mining', 'The University of Tokyo', 'University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2602.01479.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#science', '#benchmark', '#low_resource', '#open_source'], 'emoji': 'üìä', 'ru': {'title': '–Ø–∑—ã–∫–æ–≤–æ–π –±–∞—Ä—å–µ—Ä: –ø–æ—á–µ–º—É LLM –±–æ—Ä—é—Ç—Å—è —Å —è–ø–æ–Ω—Å–∫–∏–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º —Ç–µ–∫—Å—Ç–æ–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä Ebisu –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–ø–æ–Ω—Å–∫–æ–≥–æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–µ —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–µ—è–≤–Ω—ã—Ö –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤ Q&A –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏ –∏–∑ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –æ—Ç—á—ë—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ LLM, –≤–∫–ª—é—á–∞—è –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è —è–ø–æ–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∏ –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –¥–∞–∂–µ –ª—É—á—à–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –æ–±–µ–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ –º–æ–¥–µ–ª–∏ –¥–∞—ë—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è, –∞ —è–∑—ã–∫–æ–≤–∞—è –∏ –ø—Ä–µ–¥–º–µ—Ç–Ω–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø–æ–≤—ã—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Ebisu: Bridging Gaps in Japanese Financial Language Understanding', 'desc': 'The paper introduces Ebisu, a benchmark designed to assess language understanding in Japanese finance. It includes two expert-annotated tasks: JF-ICR for recognizing implicit commitments in investor Q&A, and JF-TE for extracting hierarchical financial terminology. Despite advancements in language models, the study finds that even the best-performing models struggle with these tasks, highlighting the complexity of Japanese financial language. The results indicate that simply increasing model size or adapting to specific domains does not significantly enhance performance, revealing ongoing challenges in financial natural language processing.'}, 'zh': {'title': 'EbisuÔºöÊé®Âä®Êó•Êú¨ÈáëËûçËØ≠Ë®ÄÁêÜËß£ÁöÑÂü∫ÂáÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫EbisuÁöÑÊó•Êú¨ÈáëËûçËØ≠Ë®ÄÁêÜËß£Âü∫ÂáÜÔºåÂåÖÂê´‰∏§‰∏™‰∏ìÂÆ∂Ê≥®ÈáäÁöÑ‰ªªÂä°ÔºåËØÑ‰º∞ÈöêÊÄßÊâøËØ∫ËØÜÂà´ÂíåÂ±ÇÊ¨°ÈáëËûçÊúØËØ≠ÊèêÂèñ„ÄÇÂ∞ΩÁÆ°ÂΩìÂâçÁöÑËØ≠Ë®ÄÊ®°ÂûãËÉΩÂäõÂÖàËøõÔºå‰ΩÜÂú®Ëøô‰∫õ‰ªªÂä°‰∏ä‰ªçÈù¢‰∏¥ÊåÅÁª≠ÊåëÊàò„ÄÇÊó•Êú¨ÈáëËûçËØ≠Ë®ÄÁöÑÁâπÁÇπÂåÖÊã¨Á≤òÁùÄÊÄß„ÄÅÂêéÁΩÆÁªìÊûÑÂíåÈ´òËØ≠Â¢É‰∫§ÊµÅËßÑËåÉÔºåËøô‰ΩøÂæóËØ≠Ë®ÄÊ®°ÂûãÁöÑÁêÜËß£ÂèòÂæóÂ§çÊùÇ„ÄÇEbisu‰∏∫ÈáëËûçËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑËøõÊ≠•Êèê‰æõ‰∫Ü‰∏Ä‰∏™ËÅöÁÑ¶ÁöÑÂü∫ÂáÜÔºåÊâÄÊúâÊï∞ÊçÆÈõÜÂíåËØÑ‰º∞ËÑöÊú¨ÂùáÂ∑≤ÂÖ¨ÂºÄÂèëÂ∏É„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01541', 'title': 'Toward Cognitive Supersensing in Multimodal Large Language Model', 'url': 'https://huggingface.co/papers/2602.01541', 'abstract': 'MLLMs equipped with Cognitive Supersensing and Latent Visual Imagery Prediction demonstrate enhanced cognitive reasoning capabilities through integrated visual and textual reasoning pathways.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.', 'score': 14, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '632e10bd4637e8b2', 'authors': ['Boyi Li', 'Yifan Shen', 'Yuanzhe Liu', 'Yifan Xu', 'Jiateng Liu', 'Xinzhuo Li', 'Zhengyuan Li', 'Jingyuan Zhu', 'Yunhan Zhong', 'Fangzhou Lan', 'Jianguo Cao', 'James M. Rehg', 'Heng Ji', 'Ismini Lourentzou', 'Xu Cao'], 'affiliations': ['PediaMed AI', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2602.01541.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark', '#reasoning', '#rl', '#open_source', '#training'], 'emoji': 'üß†', 'ru': {'title': '–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –≤–∏–∑—É–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ Cognitive Supersensing –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑–æ–≤ (LVIP). –ú–µ—Ç–æ–¥ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –ø–æ–¥–æ–±–Ω–æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É –≤–∏–∑—É–∞–ª—å–Ω–æ-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –±—É—Ñ–µ—Ä—É –ø–∞–º—è—Ç–∏, –æ–±—ä–µ–¥–∏–Ω—è—è –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—É—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CogSense-Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—è—Ç–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—É—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö VQA –∏ –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –æ–±—Ä–∞–∑—ã —è–≤–ª—è—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –¥–ª—è —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º.'}, 'en': {'title': 'Enhancing MLLMs with Visual Imagery for Better Reasoning', 'desc': "This paper presents a new approach called Cognitive Supersensing for Multimodal Large Language Models (MLLMs) to enhance their cognitive reasoning abilities. It introduces a Latent Visual Imagery Prediction (LVIP) mechanism that allows these models to integrate visual and textual reasoning, mimicking human-like visual memory. The authors also propose a reinforcement learning stage to optimize reasoning paths based on visual information, improving the model's performance on complex cognitive tasks. The results show that MLLMs using this method significantly outperform existing models on a new benchmark called CogSense-Bench, indicating the importance of visual imagery in cognitive understanding."}, 'zh': {'title': 'ËÆ§Áü•Ë∂ÖÊÑüÁü•ÔºöÊèêÂçáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÂÆÉÈÄöËøáËÆ§Áü•Ë∂ÖÊÑüÁü•ÂíåÊΩúÂú®ËßÜËßâÂõæÂÉèÈ¢ÑÊµãÊù•Â¢ûÂº∫ËÆ§Áü•Êé®ÁêÜËÉΩÂäõ„ÄÇËøôÁßçÊ®°ÂûãÁªìÂêà‰∫ÜËßÜËßâÂíåÊñáÊú¨Êé®ÁêÜË∑ØÂæÑÔºå‰ΩøÂÖ∂Âú®Â§ÑÁêÜÂ§çÊùÇÁöÑËÆ§Áü•ÈóÆÈ¢òÊó∂Ë°®Áé∞Êõ¥Â•ΩÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅËßÜËßâËÆ∞ÂøÜÁöÑÊÉÖÂÜµ‰∏ã„ÄÇËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËÆ≠ÁªÉËåÉÂºèÔºåËÉΩÂ§üËÆ©MLLMÂÖ∑Â§áÁ±ª‰ºº‰∫∫Á±ªÁöÑËßÜËßâÂõæÂÉèËÉΩÂäõÔºåÂπ∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÊñáÊú¨Êé®ÁêÜË∑ØÂæÑ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®ËÆ§Áü•Ë∂ÖÊÑüÁü•ËÆ≠ÁªÉÁöÑMLLMÂú®ËßÜËßâÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊ®°ÂûãÔºåÊòæÁ§∫Âá∫ÂÜÖÈÉ®ËßÜËßâÂõæÂÉèÂú®ÊÑüÁü•ËØÜÂà´‰∏éËÆ§Áü•ÁêÜËß£‰πãÈó¥ÁöÑÊ°•Ê¢Å‰ΩúÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01335', 'title': 'Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning', 'url': 'https://huggingface.co/papers/2602.01335', 'abstract': 'Visual metaphor transfer enables creative AI systems to decompose abstract conceptual relationships from reference images and reapply them to new subjects through a multi-agent framework grounded in cognitive theory.  \t\t\t\t\tAI-generated summary \t\t\t\t A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.', 'score': 14, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '92808513aa27eea4', 'authors': ['Yu Xu', 'Yuxin Zhang', 'Juan Cao', 'Lin Gao', 'Chunyu Wang', 'Oliver Deussen', 'Tong-Yee Lee', 'Fan Tang'], 'affiliations': ['National Cheng-Kung University', 'Tencent Hunyuan', 'University of Chinese Academy of Sciences', 'University of Konstanz'], 'pdf_title_img': 'assets/pdf/title_img/2602.01335.jpg', 'data': {'categories': ['#open_source'], 'emoji': 'üé®', 'ru': {'title': '–¢–≤–æ—Ä—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–Ω–æ—Å –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–µ—Ç–∞—Ñ–æ—Ä —á–µ—Ä–µ–∑ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ Visual Metaphor Transfer, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–π —Ç–≤–æ—Ä—á–µ—Å–∫–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–Ω–æ—Å–µ –µ—ë –ª–æ–≥–∏–∫–∏ –Ω–∞ –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π —Ç–µ–æ—Ä–∏–∏ Conceptual Blending, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é Schema Grammar –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –∫—Ä–∏—Ç–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å–æ–≤–º–µ—Å—Ç–Ω–æ —á–µ—Ä–µ–∑ –∑–∞–º–∫–Ω—É—Ç—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –º–µ—Ç–∞—Ñ–æ—Ä—ã –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ—Ü–µ–Ω–∫–∞ —á–µ–ª–æ–≤–µ–∫–æ–º –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–µ—Ç–∞—Ñ–æ—Ä —Å –≤—ã—Å–æ–∫–æ–π —Ç–≤–æ—Ä—á–µ—Å–∫–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç—å—é –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≤ —Ä–µ–∫–ª–∞–º–µ –∏ –º–µ–¥–∏–∞.'}, 'en': {'title': 'Transforming Abstract Concepts into Visual Metaphors with AI', 'desc': 'This paper introduces Visual Metaphor Transfer (VMT), a novel approach that allows AI to creatively reinterpret abstract concepts from reference images and apply them to new subjects. The authors propose a multi-agent framework inspired by cognitive theory, specifically Conceptual Blending Theory, to facilitate this process. By using a structured Schema Grammar, the framework separates the essential creative elements from specific visuals, enabling the AI to generate meaningful metaphors. The results show that this method significantly improves metaphor consistency and visual creativity compared to existing models, suggesting its potential for applications in advertising and media.'}, 'zh': {'title': 'ËßÜËßâÈöêÂñªËΩ¨ÁßªÔºöAIÂàõÊÑèÁöÑÊñ∞Â¢ÉÁïå', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËßÜËßâÈöêÂñªËΩ¨ÁßªÔºàVMTÔºâ‰ªªÂä°ÔºåÊó®Âú®ËÆ©AIÊ®°Âûã‰ªéÂèÇËÄÉÂõæÂÉè‰∏≠ÊèêÂèñÂàõÊÑèÊú¨Ë¥®ÔºåÂπ∂Â∞ÜÂÖ∂Â∫îÁî®‰∫éÊñ∞ÁöÑÁõÆÊ†áÂØπË±°„ÄÇÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏Ä‰∏™Â§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂ÔºåÂü∫‰∫éËÆ§Áü•ÁêÜËÆ∫ÔºåÈÄöËøáÊ¶ÇÂøµËûçÂêàÁêÜËÆ∫ÔºàCBTÔºâÂÆûÁé∞ÊäΩË±°ÈÄªËæëÁöÑÈáçÊñ∞ÊûÑÂª∫„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨Â§ö‰∏™‰∏ìÈó®ÁöÑ‰ª£ÁêÜÔºåÂàÜÂà´Ë¥üË¥£ÊÑüÁü•„ÄÅËΩ¨Áßª„ÄÅÁîüÊàêÂíåËØäÊñ≠ÔºåÁ°Æ‰øùÁîüÊàêÁöÑÈöêÂñªÂú®ÈÄªËæëÂíåËßÜËßâ‰∏äÈÉΩÂÖ∑Êúâ‰∏ÄËá¥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÈöêÂñª‰∏ÄËá¥ÊÄß„ÄÅÁ±ªÊØîÈÄÇÁî®ÊÄßÂíåËßÜËßâÂàõÊÑèÊñπÈù¢ÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01851', 'title': 'How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing', 'url': 'https://huggingface.co/papers/2602.01851', 'abstract': 'Visual Instruction Benchmark for Image Editing introduces a three-level interaction hierarchy for evaluating visual instruction following capabilities in generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.', 'score': 13, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'e226f48f4fab2c28', 'authors': ['Huanyu Zhang', 'Xuehai Bai', 'Chengzu Li', 'Chen Liang', 'Haochen Tian', 'Haodong Li', 'Ruichuan An', 'Yifan Zhang', 'Anna Korhonen', 'Zhang Zhang', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['Hangzhou Dianzi University', 'Institute of Automation, Chinese Academy of Sciences', 'Language Technology Lab, University of Cambridge', 'School of Artificial Intelligence, University of Chinese Academy of Science'], 'pdf_title_img': 'assets/pdf/title_img/2602.01851.jpg', 'data': {'categories': ['#multimodal', '#cv', '#benchmark'], 'emoji': '‚úèÔ∏è', 'ru': {'title': '–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∫ –≤–∏–∑—É–∞–ª—å–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç VIBE ‚Äî Visual Instruction Benchmark for Image Editing, –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø—Ä–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã, VIBE –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä—ë—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é –∏–µ—Ä–∞—Ä—Ö–∏—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: –¥–µ–π–∫—Ç–∏—á–µ—Å–∫–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ, –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–∞–¥—ë–∂–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –±–∞–∑–µ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∑–∞–¥–∞—á–µ—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 17 –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –∑–∞–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –æ—Ç–∫—Ä—ã—Ç—ã–µ, –Ω–æ –≤—Å–µ —Å–∏—Å—Ç–µ–º—ã –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á.'}, 'en': {'title': 'Enhancing Image Editing with Visual Instructions', 'desc': 'The paper presents the Visual Instruction Benchmark for Image Editing (VIBE), which introduces a structured way to evaluate how well generative models can follow visual instructions. It emphasizes the importance of multimodal communication, as humans often use visual cues like sketches to convey complex ideas. VIBE features a three-level interaction hierarchy that includes deictic grounding, morphological manipulation, and causal reasoning, allowing for a nuanced assessment of model capabilities. The study evaluates various image editing models, revealing that while proprietary models perform better overall, they struggle with more complex tasks, indicating areas for future improvement.'}, 'zh': {'title': 'ËßÜËßâÊåá‰ª§Âü∫ÂáÜÔºöÊèêÂçáÂõæÂÉèÁºñËæëÁöÑÂ§öÊ®°ÊÄÅÁêÜËß£ËÉΩÂäõ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫ÜËßÜËßâÊåá‰ª§Âü∫ÂáÜÔºàVIBEÔºâÔºåÁî®‰∫éËØÑ‰º∞ÁîüÊàêÊ®°ÂûãÂú®ÂõæÂÉèÁºñËæë‰∏≠ÁöÑËßÜËßâÊåá‰ª§Ë∑üÈöèËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÂª∫Á´ã‰∫Ü‰∏Ä‰∏™‰∏âÂ±ÇÊ¨°ÁöÑ‰∫§‰∫íÂ±ÇÊ¨°ÁªìÊûÑÔºåÊ∂µÁõñÊåáÁ§∫ÊÄßÂü∫Á°Ä„ÄÅÂΩ¢ÊÄÅÊìç‰ΩúÂíåÂõ†ÊûúÊé®ÁêÜ„ÄÇÈÄöËøáÁ≤æÂøÉÁ≠ñÂàíÁöÑÊµãËØïÊ°à‰æãÔºåVIBE ÂèçÊò†‰∫ÜËßÜËßâÊåá‰ª§Ë∑üÈöèÁöÑÂ§çÊùÇÊÄßÈÄêÊ≠•Â¢ûÂä†„ÄÇÁ†îÁ©∂ËøòÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫é‰ªªÂä°ÁâπÂÆöÊåáÊ†áÁöÑËØÑ‰º∞Ê°ÜÊû∂Ôºå‰ª•‰æøÂØπ‰∏çÂêåÂõæÂÉèÁºñËæëÊ®°ÂûãËøõË°åÂÖ®Èù¢ËØÑ‰º∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01538', 'title': 'Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars', 'url': 'https://huggingface.co/papers/2602.01538', 'abstract': 'A dual-stream framework called InteractAvatar is presented for generating talking avatars that can interact with objects in their environment, addressing challenges in grounded human-object interaction through decoupled perception and planning modules.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io', 'score': 13, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'dd14419a0410c173', 'authors': ['Youliang Zhang', 'Zhengguang Zhou', 'Zhentao Yu', 'Ziyao Huang', 'Teng Hu', 'Sen Liang', 'Guozhen Zhang', 'Ziqiao Peng', 'Shunkai Li', 'Yi Chen', 'Zixiang Zhou', 'Yuan Zhou', 'Qinglin Lu', 'Xiu Li'], 'affiliations': ['Tencent HY', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01538.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#video', '#games', '#benchmark'], 'emoji': 'üé¨', 'ru': {'title': '–ì–æ–≤–æ—Ä—è—â–∏–µ –∞–≤–∞—Ç–∞—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç —Å –ø—Ä–µ–¥–º–µ—Ç–∞–º–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –¥–≤—É—Ö–ø–æ—Ç–æ—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ InteractAvatar –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ –≥–æ–≤–æ—Ä—è—â–∏—Ö –∞–≤–∞—Ç–∞—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –æ–±—ä–µ–∫—Ç–∞–º–∏ –≤ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–µ. –°–∏—Å—Ç–µ–º–∞ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–π –æ—Ç —Å–∞–º–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥—É–ª—å –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–∑–∞–≤–∏—Å–∏–º—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π. –ó–≤—É–∫–æ–∑–∞–≤–∏—Å–∏–º—ã–π –º–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–µ—Ç —É–±–µ–¥–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∏–¥–µ–æ –∞–≤–∞—Ç–∞—Ä–æ–≤ —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è–º–∏ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, –∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –≤—ã—Ä–∞–≤–Ω–∏–≤–∞—Ç–µ–ª—å –¥–≤–∏–∂–µ–Ω–∏—è-–≤–∏–¥–µ–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ GroundedInter –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∏–¥–µ–æ —Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –∏ –æ–±—ä–µ–∫—Ç–æ–≤.'}, 'en': {'title': 'InteractAvatar: Talking Avatars that Interact with Their World', 'desc': 'The paper introduces InteractAvatar, a dual-stream framework designed to create talking avatars that can interact with objects in their environment. It addresses the challenge of grounded human-object interaction (GHOI) by separating perception and planning from video synthesis. The framework includes a Perception and Interaction Module (PIM) for generating interaction motions based on environmental detection, and an Audio-Interaction Aware Generation Module (AIM) for synthesizing realistic talking avatars. By allowing parallel generation of motions and videos, InteractAvatar effectively improves the quality of interactions while establishing a benchmark for evaluating GHOI video generation.'}, 'zh': {'title': 'ÁîüÊàê‰∫íÂä®‰ºöËØ¥ËØùÁöÑËôöÊãüÂΩ¢Ë±°Êñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫InteractAvatarÁöÑÂèåÊµÅÊ°ÜÊû∂ÔºåÁî®‰∫éÁîüÊàêËÉΩÂ§ü‰∏éÁéØÂ¢É‰∏≠Áâ©‰Ωì‰∫íÂä®ÁöÑ‰ºöËØ¥ËØùÁöÑËôöÊãüÂΩ¢Ë±°„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËß£ËÄ¶ÊÑüÁü•ÂíåËßÑÂàíÊ®°ÂùóÔºåËß£ÂÜ≥‰∫ÜÂü∫‰∫éÊñáÊú¨ÁöÑ‰∫∫ÁöÑÁâ©‰Ωì‰∫íÂä®ÔºàGHOIÔºâ‰∏≠ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜÊÑüÁü•‰∏é‰∫íÂä®Ê®°ÂùóÔºàPIMÔºâÊù•Â¢ûÂº∫ÁéØÂ¢ÉÊÑüÁü•ÔºåÂπ∂ÁîüÊàê‰∏éÊñáÊú¨ÂØπÈΩêÁöÑ‰∫íÂä®Âä®‰Ωú„ÄÇÂêåÊó∂ÔºåÈü≥È¢ë‰∫íÂä®ÊÑüÁü•ÁîüÊàêÊ®°ÂùóÔºàAIMÔºâÁî®‰∫éÂêàÊàêÁîüÂä®ÁöÑ‰ºöËØ¥ËØùÁöÑËôöÊãüÂΩ¢Ë±°ÔºåÊâßË°åÁâ©‰Ωì‰∫íÂä®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÁîüÊàê‰ºöËØ¥ËØùÁöÑËôöÊãüÂΩ¢Ë±°‰∏éÁâ©‰Ωì‰∫íÂä®ÊñπÈù¢ÂÖ∑ÊúâÊòæËëóÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01576', 'title': 'Generative Visual Code Mobile World Models', 'url': 'https://huggingface.co/papers/2602.01576', 'abstract': 'Visual world models for mobile GUI agents are improved through renderable code generation using vision-language models, achieving better performance with reduced model size compared to existing approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.', 'score': 12, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '3b3360675a7ef03e', 'authors': ['Woosung Koh', 'Sungjun Han', 'Segyu Lee', 'Se-Young Yun', 'Jamin Shin'], 'affiliations': ['KAIST AI', 'Trillion Labs'], 'pdf_title_img': 'assets/pdf/title_img/2602.01576.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#cv', '#agents', '#benchmark', '#small_models'], 'emoji': 'üì±', 'ru': {'title': '–û—Ç –ø–∏–∫—Å–µ–ª–µ–π –∫ –∫–æ–¥—É: –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–∏—Ä–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–µ–Ω–¥–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –≤–µ–±-–∫–æ–¥–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö GUI-–∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–≥–æ –≤–µ–±-–∫–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é vision-language –º–æ–¥–µ–ª–µ–π. –í–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∏–∫—Å–µ–ª–µ–π, VLM –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ GUI –≤ –≤–∏–¥–µ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π –∑–∞—Ç–µ–º —Ä–µ–Ω–¥–µ—Ä–∏—Ç—Å—è –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Å–æ—á–µ—Ç–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –≤–∏–∑—É–∞–ª—å–Ω–æ–π –≤–µ—Ä–Ω–æ—Å—Ç—å—é. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ gWorld ‚Äî –ø–µ—Ä–≤—ã–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–∏—Ä–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ —Ä–∞–∑–º–µ—Ä–æ–º 8B –∏ 32B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ–º —Ä–∞–∑–º–µ—Ä–µ –∏ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–±–∏–ª—å–Ω—ã—Ö GUI-–∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Revolutionizing Mobile GUI Agents with Renderable Code Generation', 'desc': 'This paper presents a new approach to creating visual world models for mobile graphical user interface (GUI) agents by generating renderable code using vision-language models (VLMs). Unlike traditional methods that either compromise visual quality or rely on complex pipelines, this method allows for precise text rendering while maintaining high visual fidelity. The proposed system, called gWorld, generates executable web code that can be rendered into images, effectively combining the strengths of text-based and visual models. The results show that gWorld significantly improves performance while reducing model size, outperforming existing models in various benchmarks.'}, 'zh': {'title': 'ÈÄöËøáÂèØÊ∏≤Êüì‰ª£Á†ÅÁîüÊàêÊèêÂçáÁßªÂä®GUIÊÄßËÉΩ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâ‰∏ñÁïåÂª∫Ê®°ÊñπÊ≥ïÔºåÈÄöËøáÂèØÊ∏≤Êüì‰ª£Á†ÅÁîüÊàêÊù•ÊèêÂçáÁßªÂä®ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâ‰ª£ÁêÜÁöÑÊÄßËÉΩ„ÄÇ‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ï‰ΩøÁî®ËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâÁîüÊàêÂèØÊâßË°åÁöÑÁΩëÈ°µ‰ª£Á†ÅÔºå‰ªéËÄåÂÆûÁé∞Êõ¥È´òÁöÑËßÜËßâ‰øùÁúüÂ∫¶ÂíåÊõ¥Â∞èÁöÑÊ®°ÂûãÂ∞∫ÂØ∏„ÄÇÊàë‰ª¨‰ªãÁªç‰∫ÜgWorldÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Âü∫‰∫éËøôÁßçÊñ∞ËåÉÂºèÁöÑÂºÄÊîæÊùÉÈáçËßÜËßâÁßªÂä®GUI‰∏ñÁïåÊ®°ÂûãÔºåÂπ∂Êèê‰æõ‰∫Ü‰∏Ä‰∏™Ëá™Âä®ÂêàÊàê‰ª£Á†ÅËÆ≠ÁªÉÊï∞ÊçÆÁöÑÊï∞ÊçÆÁîüÊàêÊ°ÜÊû∂„ÄÇÈÄöËøáÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÁöÑÂπøÊ≥õËØÑ‰º∞ÔºågWorldÂú®ÂáÜÁ°ÆÊÄß‰∏éÊ®°ÂûãÂ§ßÂ∞è‰πãÈó¥ËÆæÂÆö‰∫ÜÊñ∞ÁöÑÂ∏ïÁ¥ØÊâòÂâçÊ≤øÔºåÊòæËëóË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÂ§ö‰∏™Â§ßÂûãÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02486', 'title': 'RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents', 'url': 'https://huggingface.co/papers/2602.02486', 'abstract': 'Re-TRAC is an agentic framework that enhances LLM-based research agents by enabling cross-trajectory exploration and iterative reflection through structured state representations, leading to more efficient and effective problem-solving compared to traditional ReAct approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.', 'score': 11, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '086cbae4c7c49747', 'authors': ['Jialiang Zhu', 'Gongrui Zhang', 'Xiaolong Ma', 'Lin Xu', 'Miaosen Zhang', 'Ruiqi Yang', 'Song Wang', 'Kai Qiu', 'Zhirong Wu', 'Qi Dai', 'Ruichun Ma', 'Bei Liu', 'Yifan Yang', 'Chong Luo', 'Zhengyuan Yang', 'Linjie Li', 'Lijuan Wang', 'Weizhu Chen', 'Xin Geng', 'Baining Guo'], 'affiliations': ['Brown University', 'Microsoft', 'Southeast University', 'Tsinghua University', 'Waseda University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02486.jpg', 'data': {'categories': ['#long_context', '#optimization', '#reasoning'], 'emoji': 'üîÑ', 'ru': {'title': '–†–∞–∑–º—ã—à–ª—è—é—â–∏–π –∞–≥–µ–Ω—Ç: –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –∫—Ä–æ—Å—Å-—Ç—Ä–∞–µ–∫—Ç–æ—Ä–Ω—É—é —Ä–µ—Ñ–ª–µ–∫—Å–∏—é', 'desc': 'Re-TRAC ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–æ–¥–æ–ª–µ–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ª–∏–Ω–µ–π–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ ReAct, –ø–æ–∑–≤–æ–ª—è—è –∞–≥–µ–Ω—Ç–∞–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø—É—Ç–∏ —Ä–µ—à–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–π—Ç–∏ –≤ —Ä–∞–Ω–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–ª—è –æ–±–æ–±—â–µ–Ω–∏—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤, –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–µ–π –∏ –Ω–µ—É–¥–∞—á –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—É—é —Ä–µ—Ñ–ª–µ–∫—Å–∏—é. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Re-TRAC –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç ReAct –Ω–∞ 15-20% –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∞ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –∫–∞–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ —Å –≥–ª–æ–±–∞–ª—å–Ω–æ–π –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç—å—é, –∞ –Ω–µ –∫–∞–∫ —Å–ª–µ–ø—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é.'}, 'en': {'title': 'Re-TRAC: Enhancing Research Agents with Iterative Reflection and Exploration', 'desc': 'Re-TRAC is a new framework designed to improve the performance of large language model (LLM)-based research agents by allowing them to explore different paths and reflect on their findings. Unlike traditional ReAct methods, which follow a linear approach, Re-TRAC uses structured state representations to summarize past experiences and guide future actions. This method helps the agents avoid getting stuck in local optima and reduces unnecessary exploration, making the problem-solving process more efficient. Empirical results demonstrate that Re-TRAC outperforms ReAct by 15-20% and achieves better performance with smaller models through fine-tuning.'}, 'zh': {'title': 'Re-TRACÔºöÊèêÂçáÁ†îÁ©∂‰ª£ÁêÜÁöÑÊô∫ËÉΩÊé¢Á¥¢‰∏éÂèçÊÄù', 'desc': 'Re-TRACÊòØ‰∏Ä‰∏™Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁ†îÁ©∂‰ª£ÁêÜÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÁªìÊûÑÂåñÁä∂ÊÄÅË°®Á§∫ÂÆûÁé∞Ë∑®ËΩ®ËøπÊé¢Á¥¢ÂíåËø≠‰ª£ÂèçÊÄùÔºå‰ªéËÄåÊèêÈ´òÈóÆÈ¢òËß£ÂÜ≥ÁöÑÊïàÁéáÂíåÊïàÊûú„ÄÇ‰∏é‰º†ÁªüÁöÑReActÊñπÊ≥ïÁõ∏ÊØîÔºåRe-TRACËÉΩÂ§üÂú®ÊØè‰∏™ËΩ®ËøπÂêéÁîüÊàêÁä∂ÊÄÅË°®Á§∫ÔºåÊ±áÊÄªËØÅÊçÆ„ÄÅ‰∏çÁ°ÆÂÆöÊÄß„ÄÅÂ§±Ë¥•ÂíåÊú™Êù•ËÆ°ÂàíÔºå‰ΩøÂæóÂêéÁª≠ËΩ®ËøπËÉΩÂ§üÂü∫‰∫éÊ≠§Áä∂ÊÄÅËøõË°åË∞ÉÊï¥„ÄÇËØ•ÊñπÊ≥ï‰ΩøÂæóÁ†îÁ©∂ËøáÁ®ãÂèòÂæóÊõ¥Âä†Ê∏êËøõÔºåËÉΩÂ§üÊúâÊïàÈÅøÂÖçÂ±ÄÈÉ®ÊúÄ‰ºòÂíåÂÜó‰ΩôÊé¢Á¥¢„ÄÇÂÆûÈ™åËØÅÊòéÔºåRe-TRACÂú®BrowseComp‰∏äÊØîReActÁöÑË°®Áé∞ÊèêÈ´ò‰∫Ü15-20%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02472', 'title': 'SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning', 'url': 'https://huggingface.co/papers/2602.02472', 'abstract': 'SPARKLING is a framework for mid-stage width expansion in deep learning models that maintains signal preservation and breaks symmetry to stabilize training and reduce computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under 2times width expansion.', 'score': 10, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'a44d3797f8f8025c', 'authors': ['Qifan Yu', 'Xinyu Ma', 'Zhijian Zhuo', 'Minrui Wang', 'Deyi Liu', 'Shiyi Zhan', 'Yiyuan Ma', 'Liang Xiang', 'Xingyan Bin', 'Di He'], 'affiliations': ['ByteDance Seed', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02472.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '‚ö°', 'ru': {'title': '–°—Ç–∞–±–∏–ª—å–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —à–∏—Ä–∏–Ω—ã —Å–µ—Ç–∏: –±–∞–ª–∞–Ω—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª–∞ –∏ –Ω–∞—Ä—É—à–µ–Ω–∏—è —Å–∏–º–º–µ—Ç—Ä–∏–∏', 'desc': 'SPARKLING ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —à–∏—Ä–∏–Ω—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∏–≥–Ω–∞–ª –∏ –Ω–∞—Ä—É—à–∞–µ—Ç —Å–∏–º–º–µ—Ç—Ä–∏—é –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ —à–∏—Ä–∏–Ω—ã –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ RMS-–º–∞—Å—à—Ç–∞–±–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∏ –∞—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–π —Å–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –Ω–∞–∏–≤–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞—Ä—É—à–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–∫—Ç–∏–≤–∞—Ü–∏–π, –∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –≤—ã–∑—ã–≤–∞—é—â–µ–≥–æ —Å–∏–º–º–µ—Ç—Ä–∏—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, SPARKLING –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–∞–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞, —Ç–∞–∫ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö Mixture-of-Experts –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –¥–æ 35% –ø—Ä–∏ –¥–≤—É–∫—Ä–∞—Ç–Ω–æ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ —à–∏—Ä–∏–Ω—ã.'}, 'en': {'title': 'Enhancing Deep Learning Efficiency with Mid-Stage Width Expansion', 'desc': 'SPARKLING is a new framework designed to expand the width of deep learning models during the mid-stage of training while ensuring that the signal is preserved and symmetry is broken. This approach addresses the challenges of training instabilities that arise when increasing model width, which can lead to loss spikes and reduced feature diversity. By maintaining consistent activation statistics and employing techniques like asymmetric optimizer state resetting, SPARKLING stabilizes the training process. Experimental results show that this method can significantly reduce computational costs, achieving up to 35% savings compared to traditional training methods.'}, 'zh': {'title': 'SPARKLINGÔºöÊ∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÁöÑÂÆΩÂ∫¶Êâ©Â±ïÊñ∞ÊñπÊ≥ï', 'desc': 'SPARKLINGÊòØ‰∏Ä‰∏™Áî®‰∫éÊ∑±Â∫¶Â≠¶‰π†Ê®°Âûã‰∏≠ÊúüÂÆΩÂ∫¶Êâ©Â±ïÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®‰øùÊåÅ‰ø°Âè∑ÁöÑÂÆåÊï¥ÊÄßÂπ∂ÊâìÁ†¥ÂØπÁß∞ÊÄßÔºå‰ª•Á®≥ÂÆöËÆ≠ÁªÉÂπ∂Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÈÄêÊ≠•Â¢ûÂä†Ê®°ÂûãËßÑÊ®°ÔºåÂáèÂ∞ëÈ¢ÑËÆ≠ÁªÉÁöÑËÆ°ÁÆóÂºÄÈîÄÔºåÂ∞§ÂÖ∂ÊòØÂú®ÂÆΩÂ∫¶Êâ©Â±ïÊñπÈù¢ÔºåÂ°´Ë°•‰∫ÜÁé∞ÊúâÁ†îÁ©∂ÁöÑÁ©∫ÁôΩ„ÄÇSPARKLINGÈÄöËøáRMSÂ∞∫Â∫¶‰∏ÄËá¥ÊÄßÂÆûÁé∞‰ø°Âè∑ÁöÑ‰øùÊåÅÔºåÂπ∂ÈÄöËøá‰∏çÂØπÁß∞‰ºòÂåñÂô®Áä∂ÊÄÅÈáçÁΩÆÂíåÂ≠¶‰π†ÁéáÈáçÊñ∞ÂçáÊ∏©Êù•Á°Æ‰øùÂØπÁß∞ÊÄßÁ†¥Âùè„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSPARKLINGÂú®Â§ö‰∏™ÂÆΩÂ∫¶ËΩ¥Âíå‰ºòÂåñÂô®ÂÆ∂Êóè‰∏≠Ë°®Áé∞‰ºò‰∫é‰ªéÂ§¥ËÆ≠ÁªÉÔºåÂπ∂Âú®ÂÆΩÂ∫¶Êâ©Â±ï2ÂÄçÁöÑÊÉÖÂÜµ‰∏ãÂ∞ÜËÆ≠ÁªÉÊàêÊú¨Èôç‰Ωé‰∫ÜÂ§öËææ35%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02343', 'title': 'Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics', 'url': 'https://huggingface.co/papers/2602.02343', 'abstract': "Large language model control methods are unified under a dynamic weight update framework, revealing a preference-utility trade-off and enabling improved steering through SPLIT approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.", 'score': 10, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '698ec81e2428cdb9', 'authors': ['Ziwen Xu', 'Chenyan Wu', 'Hengyu Sun', 'Haiwen Hong', 'Mengru Wang', 'Yunzhi Yao', 'Longtao Huang', 'Hui Xue', 'Shumin Deng', 'Zhixuan Chu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Alibaba Group', 'NUS-NCS Joint Lab', 'National University of Singapore', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02343.jpg', 'data': {'categories': [], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º: –µ–¥–∏–Ω–∞—è —Ç–µ–æ—Ä–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è LLM', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) - –æ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏—è –≤–µ—Å–æ–≤ –¥–æ LoRA –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã—Ö –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤ - –≤ –µ–¥–∏–Ω—É—é –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—É—é —Å—Ö–µ–º—É –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤–µ—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∞–Ω–∞–ª–∏–∑–∞, —Ä–∞–∑–¥–µ–ª—è—è —ç—Ñ—Ñ–µ–∫—Ç—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ (—Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ —Ü–µ–ª–µ–≤–æ–º—É –∫–æ–Ω—Ü–µ–ø—Ç—É) –∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å (—Å–≤—è–∑–Ω–∞—è –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞), –∏–∑–º–µ—Ä—è—è –æ–±–∞ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –µ–¥–∏–Ω–æ–π –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–π —à–∫–∞–ª–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å: —É—Å–∏–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ, –Ω–æ –Ω–µ–∏–∑–±–µ–∂–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ SPLIT, –∫–æ—Ç–æ—Ä—ã–π –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Ü–µ–ª–µ–≤—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'Balancing Preference and Utility in Language Model Control', 'desc': 'This paper presents a unified framework for controlling large language models (LLMs) by viewing various control methods as dynamic weight updates influenced by a control signal. It introduces a preference-utility analysis that distinguishes between preference, which drives the model towards a desired concept, and utility, which ensures the output remains coherent and task-relevant. The study reveals a trade-off where stronger control enhances preference but can diminish utility, explained through the concept of an activation manifold. To address this, the authors propose a new steering method called SPLIT, which aims to improve preference while maintaining higher utility in model outputs.'}, 'zh': {'title': 'Áªü‰∏ÄÊéßÂà∂ÊñπÊ≥ïÔºå‰ºòÂåñÂÅèÂ•Ω‰∏éÊïàÁî®ÁöÑÂπ≥Ë°°', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊéßÂà∂ÊñπÊ≥ïÊ°ÜÊû∂ÔºåÂ∞Ü‰∏çÂêåÁöÑÊéßÂà∂ÊäÄÊúØËßÜ‰∏∫Âä®ÊÄÅÊùÉÈáçÊõ¥Êñ∞„ÄÇÊàë‰ª¨ÂàÜÊûê‰∫ÜÊéßÂà∂ÊïàÊûúÁöÑÂÅèÂ•ΩÂíåÊïàÁî®ÔºåÂèëÁé∞Â¢ûÂº∫ÊéßÂà∂‰ºöÊèêÈ´òÂÅèÂ•Ω‰ΩÜÈôç‰ΩéÊïàÁî®„ÄÇÈÄöËøáÊøÄÊ¥ªÊµÅÂΩ¢ÁöÑËßÜËßíÔºåÊàë‰ª¨Ëß£Èáä‰∫ÜËøôÁßçÊùÉË°°ÂÖ≥Á≥ªÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂºïÂØºÊñπÊ≥ïSPLITÔºå‰ª•ÊîπÂñÑÂÅèÂ•ΩÂêåÊó∂Êõ¥Â•ΩÂú∞‰øùÊåÅÊïàÁî®„ÄÇËØ•Á†îÁ©∂‰∏∫ÁêÜËß£ÂíåÊØîËæÉ‰∏çÂêåÁöÑÊéßÂà∂ÊñπÊ≥ïÊèê‰æõ‰∫ÜÊñ∞ÁöÑËßÜËßí„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02227', 'title': "Show, Don't Tell: Morphing Latent Reasoning into Image Generation", 'url': 'https://huggingface.co/papers/2602.02227', 'abstract': 'LatentMorph integrates implicit latent reasoning into text-to-image generation through four lightweight components that enable adaptive self-refinement and improve both efficiency and cognitive alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by 16% on GenEval and 25% on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by 15% and 11% on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by 44% and token consumption by 51%; and (IV) exhibits 71% cognitive alignment with human intuition on reasoning invocation.', 'score': 10, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '1315c99b8dc0ab14', 'authors': ['Harold Haodong Chen', 'Xinxiang Yin', 'Wen-Jie Shu', 'Hongfei Zhang', 'Zixin Zhang', 'Chenfei Liao', 'Litao Guo', 'Qifeng Chen', 'Ying-Cong Chen'], 'affiliations': ['HKU'], 'pdf_title_img': 'assets/pdf/title_img/2602.02227.jpg', 'data': {'categories': ['#multimodal', '#cv', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': 'üß†', 'ru': {'title': '–°–∫—Ä—ã—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'LatentMorph –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è –Ω–µ—è–≤–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —á–µ—Ç—ã—Ä–µ –ª—ë–≥–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä –¥–ª—è —Å–∂–∞—Ç–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π, —Ç—Ä–∞–Ω—Å–ª—è—Ç–æ—Ä –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω—ã—Ö –º—ã—Å–ª–µ–π –≤ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ, —Ñ–æ—Ä–º–æ–≤–∞—Ç–µ–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ–±—É—á–∞–µ–º—ã–π reinforcement learning –∏–Ω–≤–æ–∫–µ—Ä –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –º–æ–º–µ–Ω—Ç–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ë–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—é –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –º–µ—Ç–æ–¥ –∏–∑–±–µ–≥–∞–µ—Ç —É–∑–∫–∏—Ö –º–µ—Å—Ç —è–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–Ω–∞ 16-25% –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º), —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ –Ω–∞ 44% –∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ 51%.'}, 'en': {'title': 'Empowering Creativity in T2I with Implicit Reasoning', 'desc': 'LatentMorph is a new framework for text-to-image (T2I) generation that incorporates implicit latent reasoning to enhance the creative process. It features four key components: a condenser for summarizing generation states, a translator for turning latent thoughts into guidance, a shaper for directing image predictions, and an RL-trained invoker for deciding when to reason. This approach allows for reasoning to occur in continuous latent spaces, which improves efficiency and reduces information loss compared to traditional explicit reasoning methods. Experiments show that LatentMorph significantly boosts performance and cognitive alignment with human intuition while also cutting down on inference time and resource usage.'}, 'zh': {'title': 'ÈöêÂºèÊé®ÁêÜÔºåÊèêÂçáÂõæÂÉèÁîüÊàêÊïàÁéá', 'desc': 'LatentMorph ÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÂ∞ÜÈöêÂºèÊΩúÂú®Êé®ÁêÜÊï¥ÂêàÂà∞ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠„ÄÇÂÆÉÈÄöËøáÂõõ‰∏™ËΩªÈáèÁ∫ßÁªÑ‰ª∂ÂÆûÁé∞Ëá™Êàë‰ºòÂåñÔºåÊèêÈ´ò‰∫ÜÁîüÊàêÊïàÁéáÂíåËÆ§Áü•‰∏ÄËá¥ÊÄß„ÄÇËØ•Ê°ÜÊû∂ÈÅøÂÖç‰∫ÜÊòæÂºèÊé®ÁêÜÁöÑÁì∂È¢àÔºå‰ΩøÂæóÊé®ÁêÜËøáÁ®ãÂú®ËøûÁª≠ÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÔºå‰ªéËÄåÂÆûÁé∞Êõ¥ÁÅµÊ¥ªÁöÑËá™ÊàëÊîπËøõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLatentMorph Âú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩÔºåÂπ∂ÂáèÂ∞ë‰∫ÜÊé®ÁêÜÊó∂Èó¥ÂíåËµÑÊ∫êÊ∂àËÄó„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02156', 'title': 'LoopViT: Scaling Visual ARC with Looped Transformers', 'url': 'https://huggingface.co/papers/2602.02156', 'abstract': 'Loop-ViT introduces a recursive vision transformer architecture that decouples reasoning depth from model capacity through weight-tied recurrence and dynamic exit mechanisms, achieving superior visual reasoning performance with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.', 'score': 8, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'e3f1d9d840128bb1', 'authors': ['Wen-Jie Shu', 'Xuerui Qiu', 'Rui-Jie Zhu', 'Harold Haodong Chen', 'Yexin Liu', 'Harry Yang'], 'affiliations': ['CASIA', 'HKUST', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2602.02156.jpg', 'data': {'categories': ['#architecture', '#cv', '#benchmark', '#reasoning', '#optimization', '#open_source', '#small_models'], 'emoji': 'üîÑ', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é —Ä–µ–∫—É—Ä—Å–∏—é –≤–º–µ—Å—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–µ—Ç–∏', 'desc': 'Loop-ViT –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –∑—Ä–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –≥–ª—É–±–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤–µ—Å–æ–≤ –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º—É –º–µ—Ö–∞–Ω–∏–∑–º—É –≤—ã—Ö–æ–¥–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö feed-forward –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –≥–¥–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ —Å–≤—è–∑–∞–Ω–∞ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, Loop-ViT –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ–¥–∏–Ω –±–ª–æ–∫, –∫–æ–º–±–∏–Ω–∏—Ä—É—é—â–∏–π —Å–≤—ë—Ä—Ç–∫–∏ –∏ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, —Ñ–æ—Ä–º–∏—Ä—É—è —Ü–µ–ø–æ—á–∫—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º Dynamic Exit –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–≥–¥–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –∞—Ç—Ç—Ä–∞–∫—Ç–æ—Ä–∞. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ ARC-AGI-1 –º–æ–¥–µ–ª—å —Å 18M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 65.8% —Ç–æ—á–Ω–æ—Å—Ç–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∞–Ω—Å–∞–º–±–ª–∏ —Å 73M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Decoupling Reasoning Depth from Model Capacity in Visual Transformers', 'desc': "Loop-ViT presents a novel recursive vision transformer architecture that separates the depth of reasoning from the model's capacity, allowing for more efficient visual reasoning. It utilizes weight-tied recurrence and a Dynamic Exit mechanism to optimize performance while maintaining fewer parameters. The architecture incorporates a Hybrid Block that combines local convolutions with global attention, creating a chain of thought that iteratively refines its predictions. Empirical results show that Loop-ViT outperforms larger models on the ARC-AGI benchmark, highlighting the advantages of adaptive iterative computation in visual reasoning tasks."}, 'zh': {'title': 'Loop-ViTÔºöÈ´òÊïàÁöÑËßÜËßâÊé®ÁêÜÊñ∞Êû∂ÊûÑ', 'desc': 'Loop-ViTÊòØ‰∏ÄÁßçÈÄíÂΩíËßÜËßâÂèòÊç¢Âô®Êû∂ÊûÑÔºåÈÄöËøáÊùÉÈáçÁªëÂÆöÁöÑÈÄíÂΩíÂíåÂä®ÊÄÅÈÄÄÂá∫Êú∫Âà∂ÔºåÂ∞ÜÊé®ÁêÜÊ∑±Â∫¶‰∏éÊ®°ÂûãÂÆπÈáèËß£ËÄ¶Ôºå‰ªéËÄå‰ª•Êõ¥Â∞ëÁöÑÂèÇÊï∞ÂÆûÁé∞Êõ¥‰ºòÁöÑËßÜËßâÊé®ÁêÜÊÄßËÉΩ„ÄÇËØ•Ê®°ÂûãÈááÁî®‰∫ÜÊùÉÈáçÁªëÂÆöÁöÑÊ∑∑ÂêàÂùóÔºåÁªìÂêàÂ±ÄÈÉ®Âç∑ÁßØÂíåÂÖ®Â±ÄÊ≥®ÊÑèÂäõÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßçÊΩúÂú®ÁöÑÊÄùÁª¥Èìæ„ÄÇÈáçË¶ÅÁöÑÊòØÔºåÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊó†ÂèÇÊï∞ÁöÑÂä®ÊÄÅÈÄÄÂá∫Êú∫Âà∂ÔºåÂΩìÊ®°ÂûãÁöÑÂÜÖÈÉ®Áä∂ÊÄÅ‚ÄúÁªìÊô∂‚ÄùÊàê‰Ωé‰∏çÁ°ÆÂÆöÊÄßÂê∏ÂºïÂ≠êÊó∂ÔºåÂÅúÊ≠¢Êé®ÁêÜ„ÄÇÂÆûÈ™åËØÅÊòéÔºåLoop-ViTÂú®ARC-AGI-1Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºå18MÂèÇÊï∞ÁöÑÊ®°ÂûãÂáÜÁ°ÆÁéáËææÂà∞65.8%ÔºåË∂ÖË∂ä‰∫Ü73MÂèÇÊï∞ÁöÑÂ§ßÂûãÈõÜÊàêÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01322', 'title': 'PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding', 'url': 'https://huggingface.co/papers/2602.01322', 'abstract': 'PolySAE extends sparse autoencoders with polynomial decoding to capture feature interactions and compositional structure while maintaining linear encoders for interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10times larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency (r = 0.06 vs. r = 0.82 for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.', 'score': 8, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '15ada34a45c5eb0c', 'authors': ['Panagiotis Koromilas', 'Andreas D. Demou', 'James Oldfield', 'Yannis Panagakis', 'Mihalis Nicolaou'], 'affiliations': ['Archimedes AI/Athena Research Center', 'The Cyprus Institute', 'University of Athens', 'University of Cyprus', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2602.01322.jpg', 'data': {'categories': ['#interpretability', '#architecture', '#training'], 'emoji': 'üß©', 'ru': {'title': '–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—è—Ö', 'desc': 'PolySAE —É–ª—É—á—à–∞–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ (SAE) –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–≥–æ –¥–µ–∫–æ–¥–µ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ SAE –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ç–æ–ª—å–∫–æ –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é, —á—Ç–æ –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ä–∞–∑–ª–∏—á–∞—Ç—å, –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ª–∏ —Å–ª–æ–∂–Ω–æ–µ –ø–æ–Ω—è—Ç–∏–µ –∏–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ –∏—Ö –ø—Ä–æ—Å—Ç–æ —á–∞—Å—Ç–æ–π –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏. PolySAE —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–∏–Ω–µ–π–Ω—ã–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏, –Ω–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –¥–µ–∫–æ–¥–µ—Ä —á–ª–µ–Ω–∞–º–∏ –≤—ã—Å–æ–∫–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –º–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞ 8% –ø—Ä–∏ –Ω–µ–±–æ–ª—å—à–æ–º —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –∏ –ª—É—á—à–µ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∏—Å—Ç–∏–Ω–Ω—ã–µ –∫–æ–º–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏ —Ñ—Ä–∞–∑–æ–≤–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è.'}, 'en': {'title': 'PolySAE: Enhancing Interpretability with Polynomial Feature Interactions', 'desc': "PolySAE is a novel approach that enhances sparse autoencoders by incorporating polynomial decoding to better understand feature interactions and compositional structures. Traditional sparse autoencoders rely on linear reconstruction, which limits their ability to differentiate between co-occurring features and those that are compositionally related. By introducing higher-order terms in the decoding process, PolySAE allows for a more nuanced representation of features while keeping the linear encoder for interpretability. The results show that PolySAE significantly improves performance in probing tasks while effectively capturing complex relationships between features without increasing the model's complexity significantly."}, 'zh': {'title': 'PolySAEÔºöÊçïÊçâÁâπÂæÅ‰∫§‰∫í‰∏éÁªÑÂêàÁªìÊûÑÁöÑÂàõÊñ∞Ê®°Âûã', 'desc': 'PolySAEÊòØ‰∏ÄÁßçÊâ©Â±ïÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÁöÑÊ®°ÂûãÔºåÈÄöËøáÂ§öÈ°πÂºèËß£Á†ÅÊù•ÊçïÊçâÁâπÂæÅ‰πãÈó¥ÁöÑ‰∫§‰∫íÂíåÁªÑÂêàÁªìÊûÑÔºåÂêåÊó∂‰øùÊåÅÁ∫øÊÄßÁºñÁ†ÅÂô®‰ª•‰æø‰∫éËß£Èáä„ÄÇ‰º†ÁªüÁöÑÁ®ÄÁñèËá™ÁºñÁ†ÅÂô®ÂÅáËÆæÁâπÂæÅÊòØÈÄöËøáÁ∫øÊÄßÈáçÊûÑÂä†ÊÄßÁªÑÂêàÁöÑÔºåËøôÊó†Ê≥ïÊúâÊïàÊçïÊçâÁªÑÂêàÁªìÊûÑ„ÄÇPolySAEÈÄöËøáÂú®ÂÖ±‰∫´ÊäïÂΩ±Â≠êÁ©∫Èó¥‰∏äËøõË°å‰ΩéÁß©Âº†ÈáèÂàÜËß£ÔºåËÉΩÂ§ü‰ª•ËæÉÂ∞èÁöÑÂèÇÊï∞ÂºÄÈîÄÂª∫Ê®°ÁâπÂæÅÁöÑÊàêÂØπÂíå‰∏âÂÖÉ‰∫§‰∫í„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPolySAEÂú®Â§ö‰∏™ËØ≠Ë®ÄÊ®°Âûã‰∏äÂÆûÁé∞‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÂèØÊØîÁöÑÈáçÊûÑËØØÂ∑Æ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21123', 'title': 'CUA-Skill: Develop Skills for Computer Using Agent', 'url': 'https://huggingface.co/papers/2601.21123', 'abstract': 'CUA-Skill introduces a large-scale library of engineered computer-use skills that enhance agent performance and efficiency on Windows-based tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.', 'score': 8, 'issue_id': 888, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'}, 'hash': 'e0fc60e690980392', 'authors': ['Tianyi Chen', 'Yinheng Li', 'Michael Solodko', 'Sen Wang', 'Nan Jiang', 'Tingyuan Cui', 'Junheng Hao', 'Jongwoo Ko', 'Sara Abdali', 'Suzhen Zheng', 'Leon Xu', 'Hao Fan', 'Pashmina Cameron', 'Justin Wagle', 'Kazuhito Koishida'], 'affiliations': ['Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2601.21123.jpg', 'data': {'categories': ['#open_source'], 'emoji': 'ü§ñ', 'ru': {'title': '–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è CUA-Skill ‚Äî –±–æ–ª—å—à–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —É–ø—Ä–∞–≤–ª—è—é—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å. –ù–∞–≤—ã–∫–∏ –∫–æ–¥–∏—Ä—É—é—Ç –∑–Ω–∞–Ω–∏—è –æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º–∏ Windows –≤ –≤–∏–¥–µ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –≥—Ä–∞—Ñ–∞–º–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –±–∞–∑—ã –Ω–∞–≤—ã–∫–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω CUA-Skill Agent ‚Äî –∞–≥–µ–Ω—Ç, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É –Ω–∞–≤—ã–∫–æ–≤, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ—à–∏–±–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –¥–æ—Å—Ç–∏–≥–∞—è 57.5% —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –Ω–∞ WindowsAgentArena –ø—Ä–∏ –±–æ–ª—å—à–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.'}, 'en': {'title': 'Empowering Agents with Human-like Computer Skills', 'desc': 'CUA-Skill is a comprehensive library designed to enhance the performance of computer-using agents (CUAs) in executing tasks on Windows systems. It addresses the challenge of scaling agentic systems by providing structured skill abstractions that mimic human interactions with graphical user interfaces. The library includes a wide range of engineered skills and supports dynamic skill retrieval and memory-aware failure recovery, making it a robust tool for agent development. The CUA-Skill Agent, built on this foundation, demonstrates significant improvements in execution success rates and efficiency compared to existing methods.'}, 'zh': {'title': 'ÊèêÂçáËÆ°ÁÆóÊú∫‰ª£ÁêÜÊÄßËÉΩÁöÑÊäÄËÉΩÂ∫ì', 'desc': 'CUA-SkillÊòØ‰∏Ä‰∏™Â§ßÂûãÁöÑËÆ°ÁÆóÊú∫‰ΩøÁî®ÊäÄËÉΩÂ∫ìÔºåÊó®Âú®ÊèêÈ´òËÆ°ÁÆóÊú∫‰ª£ÁêÜÂú®Windows‰ªªÂä°‰∏äÁöÑË°®Áé∞ÂíåÊïàÁéá„ÄÇËØ•Â∫ìÈÄöËøáÁºñÁ†Å‰∫∫Á±ªËÆ°ÁÆóÊú∫‰ΩøÁî®Áü•ËØÜÔºåÂ∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫ÂèØÈáçÁî®ÁöÑÊäÄËÉΩÊäΩË±°ÔºåËß£ÂÜ≥‰∫ÜÁé∞Êúâ‰ª£ÁêÜÁ≥ªÁªüÈöæ‰ª•Êâ©Â±ïÁöÑÈóÆÈ¢ò„ÄÇCUA-Skill AgentÊòØÂü∫‰∫éËøô‰∏ÄÊäÄËÉΩÂ∫ìÊûÑÂª∫ÁöÑÁ´ØÂà∞Á´ØËÆ°ÁÆóÊú∫‰ΩøÁî®‰ª£ÁêÜÔºåÊîØÊåÅÂä®ÊÄÅÊäÄËÉΩÊ£ÄÁ¥¢ÂíåÊïÖÈöúÊÅ¢Â§ç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCUA-SkillÊòæËëóÊèêÈ´ò‰∫Ü‰ª£ÁêÜÂú®Â§çÊùÇ‰ªªÂä°‰∏≠ÁöÑÊàêÂäüÁéáÂíåÈ≤ÅÊ£íÊÄßÔºå‰∏∫Êú™Êù•ÁöÑËÆ°ÁÆóÊú∫‰ΩøÁî®‰ª£ÁêÜÂºÄÂèëÂ•†ÂÆö‰∫ÜÂùöÂÆûÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.20613', 'title': 'AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios', 'url': 'https://huggingface.co/papers/2601.20613', 'abstract': "AgentIF-OneDay evaluates AI agents' ability to handle diverse daily tasks through natural language instructions, requiring problem-solving, attachment understanding, and file-based outputs across three user-centric categories.  \t\t\t\t\tAI-generated summary \t\t\t\t The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.", 'score': 7, 'issue_id': 884, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'}, 'hash': 'e7850e15fbaed0b1', 'authors': ['Kaiyuan Chen', 'Qimin Wu', 'Taiyu Hou', 'Tianhao Tang', 'Xueyu Hu', 'Yuchen Hou', 'Bikun Li', 'Chengming Qian', 'Guoyin Wang', 'Haolin Chen', 'Haotong Tian', 'Haoye Zhang', 'Haoyu Bian', 'Hongbing Pan', 'Hongkang Zhang', 'Hongyi Zhou', 'Jiaqi Cai', 'Jiewu Rao', 'Jiyuan Ren', 'Keduan Huang', 'Lucia Zhu Huang', 'Mingyu Yuan', 'Naixu Guo', 'Qicheng Tang', 'Qinyan Zhang', 'Shuai Chen', 'Siheng Chen', 'Ting Ting Li', 'Xiaoxing Guo', 'Yaocheng Zuo', 'Yaoqi Guo', 'Yinan Wang', 'Yinzhou Yu', 'Yize Wang', 'Yuan Jiang', 'Yuan Tian', 'Yuanshuo Zhang', 'Yuxuan Liu', 'Yvette Yan Zeng', 'Zenyu Shan', 'Zihan Yin', 'Xiaobo Hu', 'Yang Liu', 'Yixin Ren', 'Yuan Gong'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.20613.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset'], 'emoji': 'ü§ñ', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ —Ä–µ—à–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –±–µ–Ω—á–º–∞—Ä–∫ AgentIF-OneDay –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ AI –∞–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏, –ø–æ–ª—É—á–∞–µ–º—ã–µ —á–µ—Ä–µ–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —Ç—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –Ω–µ—è–≤–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–∑ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –û—Ü–µ–Ω–∫–∞ –≤–∫–ª—é—á–∞–µ—Ç 104 –∑–∞–¥–∞—á–∏ —Å 767 –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –æ—Ü–µ–Ω–∫–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —Å–æ–≤–º–µ—â–∞—é—â–∏–π –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å—É–¥–µ–π—Å—Ç–≤–æ–º (–¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å 80.1%). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ AI –∞–≥–µ–Ω—Ç—ã —É–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–ª–∏ –∞–≥–µ–Ω—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤ API –∏ –º–æ–¥–µ–ª–∏, –ø–æ–∑–≤–æ–ª—è—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering AI Agents for Everyday Tasks', 'desc': "AgentIF-OneDay is a benchmark designed to evaluate AI agents' performance in completing a variety of daily tasks using natural language instructions. It focuses on three main categories: Open Workflow Execution, Latent Instruction, and Iterative Refinement, which test the agents' problem-solving abilities and understanding of different attachment types. The study highlights that while AI agents excel in complex tasks, their effectiveness in everyday scenarios is not fully recognized by users. By employing a refined evaluation pipeline and instance-level rubrics, the research demonstrates a high agreement rate between AI assessments and human judgment, showcasing the capabilities of leading AI models in practical applications."}, 'zh': {'title': 'ËØÑ‰º∞AI‰ª£ÁêÜÁöÑÊó•Â∏∏‰ªªÂä°Â§ÑÁêÜËÉΩÂäõ', 'desc': 'AgentIF-OneDay ÊòØ‰∏Ä‰∏™ËØÑ‰º∞‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜÂ§ÑÁêÜÊó•Â∏∏‰ªªÂä°ËÉΩÂäõÁöÑÂü∫ÂáÜÔºå‰ΩøÁî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§Êù•ÂÆåÊàêÂ§öÊ†∑ÂåñÁöÑ‰ªªÂä°„ÄÇËøô‰∫õ‰ªªÂä°‰∏ç‰ªÖÈúÄË¶ÅËß£ÂÜ≥ÈóÆÈ¢òÔºåËøòÈúÄË¶ÅÁêÜËß£‰∏çÂêåÁ±ªÂûãÁöÑÈôÑ‰ª∂ÔºåÂπ∂Êèê‰æõÂü∫‰∫éÊñá‰ª∂ÁöÑËæìÂá∫„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°AIÂú®Â§çÊùÇÈóÆÈ¢òËß£ÂÜ≥ÂíåÁºñÁ†ÅÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÊôÆÈÄöÁî®Êà∑ÂØπÂÖ∂ËÉΩÂäõÁöÑËÆ§Áü•‰ªçÁÑ∂ÊúâÈôê„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑËØÑ‰º∞Ê°ÜÊû∂ÂåÖÊã¨‰∏â‰∏™Áî®Êà∑‰∏≠ÂøÉÁöÑÁ±ªÂà´ÔºåÊó®Âú®Êõ¥ÂÖ®Èù¢Âú∞ÂèçÊò†AI‰ª£ÁêÜÂú®Êó•Â∏∏Â∑•‰Ωú„ÄÅÁîüÊ¥ªÂíåÂ≠¶‰π†‰∏≠ÁöÑÂ∫îÁî®ËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02453', 'title': 'Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling', 'url': 'https://huggingface.co/papers/2602.02453', 'abstract': 'Thinking with Comics emerges as an effective visual reasoning approach that bridges images and videos by leveraging comic structures for improved multimodal reasoning efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.', 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '09822eb6bd7dd50b', 'authors': ['Andong Chen', 'Wenxin Zhu', 'Qiuyu Ding', 'Yuchen Song', 'Muyun Yang', 'Tiejun Zhao'], 'affiliations': ['Harbin Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2602.02453.jpg', 'data': {'categories': ['#multimodal', '#video', '#cv', '#benchmark', '#reasoning', '#long_context'], 'emoji': 'üé®', 'ru': {'title': '–ö–æ–º–∏–∫—Å—ã –∫–∞–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–æ—Å—Ç –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –≤–∏–¥–µ–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –∫–æ–º–∏–∫—Å—ã –∫–∞–∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–µ–∂–¥—É —Å—Ç–∞—Ç–∏—á–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –≤–∏–¥–µ–æ. –ö–æ–º–∏–∫—Å—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –Ω–∞—Ä—Ä–∞—Ç–∏–≤–Ω—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å—Å–ª–µ–¥—É—é—Ç –¥–≤–∞ –ø—É—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–∏–∫—Å–æ–≤ –∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –∏—Ö –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø–æ–¥—Ö–æ–¥ —Å –∫–æ–º–∏–∫—Å–∞–º–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–µ—Ç–æ–¥—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ—Å—Ç–∞–≤–∞—è—Å—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º, —á–µ–º –∞–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'Comics: The Bridge for Smarter Visual Reasoning', 'desc': "The paper introduces 'Thinking with Comics', a novel visual reasoning method that enhances the understanding of images and videos by utilizing comic structures. This approach addresses the limitations of static images, which lack temporal context, and the inefficiencies of videos, which can be redundant and computationally expensive. By leveraging the high information density of comics, the method maintains narrative coherence and embedded text while reducing reasoning costs. Experimental results demonstrate that this comic-based reasoning outperforms traditional image-based reasoning in complex tasks, highlighting the effectiveness of comics as an intermediate visual representation for multimodal reasoning."}, 'zh': {'title': 'Êº´ÁîªÊÄùÁª¥ÔºöÊèêÂçáÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÊúâÊïàÊ°•Ê¢Å', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫‚ÄúÊº´ÁîªÊÄùÁª¥‚ÄùÁöÑËßÜËßâÊé®ÁêÜÊñπÊ≥ïÔºåÊó®Âú®ÈÄöËøáÊº´ÁîªÁªìÊûÑÊèêÈ´òÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÊïàÁéáÂíåÊÄßËÉΩ„ÄÇÊº´Áîª‰Ωú‰∏∫‰∏ÄÁßç‰ø°ÊÅØÂØÜÂ∫¶È´òÁöÑÂ™í‰ªãÔºåËÉΩÂ§üÊúâÊïàÂú∞‰øùÁïôÊó∂Èó¥ÁªìÊûÑ„ÄÅÂµåÂÖ•ÊñáÊú¨ÂíåÂèô‰∫ãËøûË¥ØÊÄßÔºåÂêåÊó∂Èôç‰ΩéÊé®ÁêÜÊàêÊú¨„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊº´ÁîªÊÄùÁª¥Âú®Â§öÊ≠•Êó∂Èó¥ÂíåÂõ†ÊûúÊé®ÁêÜ‰ªªÂä°‰∏ä‰ºò‰∫éÂõæÂÉèÊÄùÁª¥ÔºåÂπ∂‰∏îÂú®ÊïàÁéá‰∏äÊòæËëó‰ºò‰∫éËßÜÈ¢ëÊÄùÁª¥„ÄÇ‰∏çÂêåÁöÑÊº´ÁîªÂèô‰∫ãÁªìÊûÑÂíåÈ£éÊ†ºÂØπ‰ªªÂä°Ë°®Áé∞Êúâ‰∏ÄËá¥ÁöÑÂΩ±ÂìçÔºåË°®ÊòéÊº´ÁîªÊòØÊîπÂñÑÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑÊúâÊïà‰∏≠‰ªãËßÜËßâË°®Á§∫„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01675', 'title': 'TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios', 'url': 'https://huggingface.co/papers/2602.01675', 'abstract': 'TRIP-Bench presents a comprehensive long-horizon benchmark for travel planning that evaluates LLM agents on complex multi-turn interactions, while GTPO offers an online reinforcement learning approach to enhance constraint satisfaction and robustness in extended dialogues.  \t\t\t\t\tAI-generated summary \t\t\t\t As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce TRIP-Bench, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\\% success on the easy split, with performance dropping below 10\\% on hard subsets. We further propose GTPO, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.', 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'c5c5d10005e6940b', 'authors': ['Yuanzhe Shen', 'Zisu Huang', 'Zhengyuan Wang', 'Muzhao Tian', 'Zhengkang Guo', 'Chenyang Zhang', 'Shuaiyu Zhou', 'Zengjie Hu', 'Dailin Li', 'Jingwen Xu', 'Kaimin Wang', 'Wenhao Liu', 'Tianlong Li', 'Fengpeng Yue', 'Feng Hong', 'Cao Liu', 'Ke Zeng'], 'affiliations': ['University 1', 'University 2', 'University 3', 'University 4', 'University 5', 'University 6'], 'pdf_title_img': 'assets/pdf/title_img/2602.01675.jpg', 'data': {'categories': ['#dataset', '#agents', '#benchmark', '#reasoning', '#optimization', '#rl', '#long_context'], 'emoji': '‚úàÔ∏è', 'ru': {'title': '–î–ª–∏–Ω–Ω—ã–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ã –¥–∏–∞–ª–æ–≥–∞: –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω TRIP-Bench ‚Äî –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∞–≥–µ–Ω—Ç–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è —á–µ—Ä–µ–∑ —Å–ª–æ–∂–Ω—ã–µ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏ —Å –±–æ–ª–µ–µ —á–µ–º 150 –≤—ã–∑–æ–≤–∞–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å–≤—ã—à–µ 200k —Ç–æ–∫–µ–Ω–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç GTPO ‚Äî –º–µ—Ç–æ–¥ –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ—Å—Ç–∏–≥–∞—é—Ç –º–∞–∫—Å–∏–º—É–º 50% —É—Å–ø–µ—Ö–∞ –Ω–∞ –ø—Ä–æ—Å—Ç–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, —Ç–æ–≥–¥–∞ –∫–∞–∫ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞–¥–∞–µ—Ç –Ω–∏–∂–µ 10%. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ GTPO –∫ –º–æ–¥–µ–ª–∏ Qwen2.5-32B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Gemini-3-Pro.'}, 'en': {'title': 'Enhancing Travel Planning with TRIP-Bench and GTPO', 'desc': "The paper introduces TRIP-Bench, a benchmark designed to evaluate large language model (LLM) agents in complex travel planning scenarios that require multi-turn interactions. It highlights the limitations of existing benchmarks in addressing challenges like global constraint enforcement and multi-tool coordination. The benchmark includes a variety of difficulty levels, with a focus on long, ambiguous dialogues that test the models' capabilities over extended interactions. Additionally, the paper presents GTPO, a reinforcement learning approach that enhances the performance of LLMs in these scenarios by improving constraint satisfaction and robustness during dialogues."}, 'zh': {'title': 'ÊèêÂçáÊóÖË°åËßÑÂàíÁöÑÊô∫ËÉΩÂØπËØùËÉΩÂäõ', 'desc': 'TRIP-BenchÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÈïøÊúüÂü∫ÂáÜÊµãËØïÔºå‰∏ìÊ≥®‰∫éÊóÖË°åËßÑÂàíÔºåËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂú®Â§çÊùÇÂ§öËΩÆ‰∫§‰∫í‰∏≠ÁöÑË°®Áé∞„ÄÇËØ•Âü∫ÂáÜÂà©Áî®ÁúüÂÆû‰∏ñÁïåÊï∞ÊçÆÔºåÊèê‰æõ18ÁßçÂ∑•ÂÖ∑Âíå40Â§öÁßçÊóÖË°åÈúÄÊ±ÇÔºåÊîØÊåÅËá™Âä®ÂåñËØÑ‰º∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂç≥‰ΩøÊòØÂÖàËøõÁöÑÊ®°ÂûãÂú®ÁÆÄÂçï‰ªªÂä°‰∏ä‰πüÂè™ËÉΩËææÂà∞50%ÁöÑÊàêÂäüÁéáÔºåËÄåÂú®Âõ∞Èöæ‰ªªÂä°‰∏äË°®Áé∞Êõ¥Â∑ÆÔºå‰Ωé‰∫é10%„ÄÇÊ≠§Â§ñÔºåGTPOÊòØ‰∏ÄÁßçÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÔºåÈÄöËøáÁâπÊÆäÁöÑÂ•ñÂä±ÂΩí‰∏ÄÂåñÂíåÂ•ñÂä±Â∑ÆÂºÇÂåñÔºåÊèêÂçá‰∫ÜÁ∫¶ÊùüÊª°Ë∂≥Âíå‰∫§‰∫íÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01660', 'title': 'CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation', 'url': 'https://huggingface.co/papers/2602.01660', 'abstract': "A novel framework called CoDiQ enables controllable difficulty generation for competition-level questions through test-time scaling, resulting in a corpus that significantly improves large reasoning model performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.", 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'cdeb2332e5eb43f6', 'authors': ['Zhongyuan Peng', 'Caijun Xu', 'Changyi Xiao', 'Shibo Hong', 'Eli Zhang', 'Stephen Huang', 'Yixin Cao'], 'affiliations': ['Fudan University', 'M-A-P', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2602.01660.jpg', 'data': {'categories': ['#synthetic', '#open_source', '#reasoning'], 'emoji': 'üéØ', 'ru': {'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω–∫—É—Ä—Å–Ω—ã—Ö –∑–∞–¥–∞—á —Å —Ç–æ—á–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ CoDiQ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–¥–∞—á —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π —á–µ—Ä–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –±—é–¥–∂–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ–≤—ã—à–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å, –Ω–æ —Å–Ω–∏–∂–∞–µ—Ç —Ä–µ—à–∞–µ–º–æ—Å—Ç—å –∑–∞–¥–∞—á. –ù–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen3-8B –æ–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ CoDiQ-Generator, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –û–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –ø–æ–ª—É—á–µ–Ω–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ –∏–∑ 44K –∑–∞–¥–∞—á —Å–æ—Ä–µ–≤–Ω–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è (CoDiQ-Corpus) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –≤—ã–≤–æ–¥—É –∏ —Ä–µ—à–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º.'}, 'en': {'title': 'CoDiQ: Mastering Question Difficulty for Enhanced AI Reasoning', 'desc': 'The paper introduces CoDiQ, a framework designed to generate competition-level questions with controllable difficulty. It addresses the limitations of existing methods that struggle with difficulty control and high computational costs. By using test-time scaling, CoDiQ allows for fine-tuned difficulty adjustments while maintaining question solvability. The resulting CoDiQ-Corpus, containing 44,000 high-quality questions, significantly enhances the performance of large reasoning models during training.'}, 'zh': {'title': 'ÂèØÊéßÈöæÂ∫¶ÁîüÊàêÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂CoDiQÔºåÁî®‰∫éÂú®ÊµãËØïÊó∂ÁîüÊàêÂèØÊéßÈöæÂ∫¶ÁöÑÁ´û‰∫âÁ∫ßÈóÆÈ¢òÔºå‰ªéËÄåÊòæËëóÊèêÂçáÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÊÄßËÉΩ„ÄÇÁé∞ÊúâÁöÑËá™Âä®ÈóÆÈ¢òÂêàÊàêÊñπÊ≥ïÂú®ÈöæÂ∫¶ÊéßÂà∂‰∏ä‰∏çÂ§üÁ≤æÁ°ÆÔºåËÆ°ÁÆóÊàêÊú¨È´òÔºå‰∏îÈöæ‰ª•Â§ßËßÑÊ®°ÁîüÊàêÁ´û‰∫âÁ∫ßÈóÆÈ¢ò„ÄÇCoDiQÈÄöËøáÊµãËØïÊó∂ÁöÑÁº©ÊîæÁ≠ñÁï•ÔºåÂÆûÁé∞‰∫ÜÂØπÈóÆÈ¢òÈöæÂ∫¶ÁöÑÁªÜÁ≤íÂ∫¶ÊéßÂà∂ÔºåÂêåÊó∂Á°Æ‰øùÈóÆÈ¢òÁöÑÂèØËß£ÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰ΩøÁî®CoDiQÁîüÊàêÁöÑ44K‰∏™Á´û‰∫âÁ∫ßÈóÆÈ¢òÂ∫èÂàóÔºåËÉΩÂ§üÊòæËëóÊèêÈ´òÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01511', 'title': 'Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training', 'url': 'https://huggingface.co/papers/2602.01511', 'abstract': 'Rubric-ARM framework jointly optimizes rubric generation and judging through reinforcement learning to improve response quality assessment in creative and open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.', 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'f32153de89bb30a4', 'authors': ['Ran Xu', 'Tianci Liu', 'Zihan Dong', 'Tony You', 'Ilgee Hong', 'Carl Yang', 'Linjun Zhang', 'Tao Zhao', 'Haoyu Wang'], 'affiliations': ['Emory University', 'Georgia Institute of Technology', 'Purdue University', 'Rutgers University', 'University at Albany'], 'pdf_title_img': 'assets/pdf/title_img/2602.01511.jpg', 'data': {'categories': ['#rl', '#rlhf', '#benchmark'], 'emoji': 'üìã', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': 'Rubric-ARM ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–≤–º–µ—Å—Ç–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Ü–µ–Ω–∫–∏ –∏ —Å—É–¥—å—é —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç —Å–∫–∞–ª—è—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –æ—Ç—Ä–∞–∑–∏—Ç—å –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω–æ—Å—Ç—å –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –≤ –Ω–µ–≤–∞–ª–∏–¥–∏—Ä—É–µ–º—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ç–≤–æ—Ä—á–µ—Å–∫–æ–µ –ø–∏—Å—å–º–æ –∏–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–æ —Å—Ç–∞—Ç–∏—á–Ω—ã–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∫–∞–∫ —Å–∫—Ä—ã—Ç–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ–±—É—á–∞–µ—Ç—Å—è –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∏–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —á–µ—Ä–µ–¥—É—é—â—É—é—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —É–ª—É—á—à–µ–Ω–∏–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ç–≤–æ—Ä—á–µ—Å–∫–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞.'}, 'en': {'title': 'Revolutionizing Response Quality Assessment with Rubric-ARM', 'desc': 'The Rubric-ARM framework enhances the assessment of creative responses by integrating rubric generation and judging through reinforcement learning. Traditional reward models often provide simple scores that overlook the complexity of quality in creative tasks. Rubric-ARM innovatively treats rubric creation as a dynamic process, optimizing it alongside the judging mechanism to improve accuracy. By employing an alternating optimization strategy, the framework effectively reduces training variability, leading to superior performance in various benchmarks and better alignment in reinforcement learning applications.'}, 'zh': {'title': 'Rubric-ARMÔºöÊèêÂçáÂàõÊÑè‰ªªÂä°ËØÑ‰º∞ÁöÑÊô∫ËÉΩÊ°ÜÊû∂', 'desc': 'Rubric-ARMÊ°ÜÊû∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÅîÂêà‰ºòÂåñËØÑÂàÜÊ†áÂáÜÁîüÊàêÂíåËØÑÂà§Ôºå‰ª•ÊèêÈ´òÂàõÊÑèÂíåÂºÄÊîæÂºè‰ªªÂä°‰∏≠ÁöÑÂìçÂ∫îË¥®ÈáèËØÑ‰º∞„ÄÇ‰º†ÁªüÁöÑÂ•ñÂä±Ê®°ÂûãÈÄöÂ∏∏È¢ÑÊµãÊ†áÈáèÂàÜÊï∞ÔºåÊó†Ê≥ïÊçïÊçâÈùûÂèØÈ™åËØÅÈ¢ÜÂüüÔºàÂ¶ÇÂàõÊÑèÂÜô‰ΩúÔºâÁöÑÂìçÂ∫îË¥®ÈáèÁöÑÂ§öÈù¢ÊÄß„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂ∞ÜËØÑÂàÜÊ†áÂáÜÁîüÊàêËßÜ‰∏∫‰∏ÄÁßçÊΩúÂú®ÁöÑÂä®‰ΩúÔºåÊó®Âú®ÊúÄÂ§ßÂåñÂà§Êñ≠ÁöÑÂáÜÁ°ÆÊÄßÔºåÂπ∂ÂºïÂÖ•‰∫§Êõø‰ºòÂåñÁ≠ñÁï•‰ª•ÂáèËΩªÂêåÊó∂Êõ¥Êñ∞ÁöÑÈùûÂπ≥Á®≥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRubric-ARMÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊîπÂñÑ‰∫ÜÁ¶ªÁ∫øÂíåÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†ÁéØÂ¢É‰∏≠ÁöÑ‰∏ãÊ∏∏Á≠ñÁï•ÂØπÈΩê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01382', 'title': 'PromptRL: Prompt Matters in RL for Flow-Based Image Generation', 'url': 'https://huggingface.co/papers/2602.01382', 'abstract': 'Flow matching models for text-to-image generation are enhanced through a reinforcement learning framework that addresses sample inefficiency and prompt overfitting by incorporating language models for prompt refinement, achieving superior performance with reduced computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.   Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2times fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.', 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '4a2019b4af9a1898', 'authors': ['Fu-Yun Wang', 'Han Zhang', 'Michael Gharbi', 'Hongsheng Li', 'Taesung Park'], 'affiliations': ['Meta Superintelligence Labs', 'Reve', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2602.01382.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#rl', '#rlhf', '#training'], 'emoji': 'üé®', 'ru': {'title': '–ü—É—Å—Ç—å —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —É–ª—É—á—à–∞–µ—Ç –ø–æ–¥—Å–∫–∞–∑–∫–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ PromptRL ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –º–æ–¥–µ–ª–∏ flow matching –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ RL –ø–æ–¥—Ö–æ–¥—ã —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Å—Ç–∏–ª—è –∑–∞–ø—Ä–æ—Å–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ü–∏–∫–ª –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ flow-based RL, –ø–æ–∑–≤–æ–ª—è—è –∏–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–ø—Ç—ã –∏ –¥–æ—Å—Ç–∏—á—å –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ–ø—ã—Ç–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ PromptRL —Ç—Ä–µ–±—É–µ—Ç –≤ –¥–≤–∞ —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —á–µ–º –±–∞–∑–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –ø—Ä–∏ —ç—Ç–æ–º –¥–æ—Å—Ç–∏–≥–∞—è —Ä–µ–∫–æ—Ä–¥–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with Prompt Refinement in RL', 'desc': "This paper introduces PromptRL, a novel framework that enhances flow matching models for text-to-image generation using reinforcement learning. It addresses two key issues: sample inefficiency, which limits the diversity of generated images, and prompt overfitting, where models fail to generalize to varied prompts. By integrating language models for prompt refinement within the reinforcement learning process, PromptRL improves the model's ability to generate diverse and high-quality images. The results show significant performance improvements on various benchmarks while reducing the computational resources needed for training."}, 'zh': {'title': 'ÊèêÂçáÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÁöÑÊô∫ËÉΩÂåñÔºÅ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàêÊ®°ÂûãÔºåÁß∞‰∏∫PromptRLÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Êù•Ëß£ÂÜ≥Ê†∑Êú¨ÊïàÁéá‰ΩéÂíåÊèêÁ§∫ËøáÊãüÂêàÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂ∞ÜËØ≠Ë®ÄÊ®°ÂûãÊï¥ÂêàÂà∞ÊµÅÂåπÈÖçÊ®°ÂûãÁöÑ‰ºòÂåñËøáÁ®ã‰∏≠ÔºåPromptRLËÉΩÂ§üÂø´ÈÄüÊîπÂÜôÊèêÁ§∫Ôºå‰ªéËÄåÊèêÈ´òÁîüÊàêÁöÑÂ§öÊ†∑ÊÄßÂíåÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPromptRLÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁîüÊàêË¥®ÈáèÔºåÂπ∂ÂáèÂ∞ë‰∫ÜËÆ°ÁÆóËµÑÊ∫êÁöÑÈúÄÊ±Ç„ÄÇËØ•ÊñπÊ≥ïÂú®Â§ßËßÑÊ®°ÂõæÂÉèÁºñËæëÊ®°Âûã‰∏ä‰πüÂèñÂæó‰∫ÜËâØÂ•ΩÁöÑÊïàÊûúÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00986', 'title': 'Sparse Reward Subsystem in Large Language Models', 'url': 'https://huggingface.co/papers/2602.00986', 'abstract': "Research identifies a sparse reward subsystem in LLM hidden states containing value neurons that represent internal state expectations and dopamine-like neurons encoding reward prediction errors.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.", 'score': 5, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '0306456f1e097ed2', 'authors': ['Guowei Xu', 'Mert Yuksekgonul', 'James Zou'], 'affiliations': ['Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.00986.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#transfer_learning', '#interpretability', '#training'], 'emoji': 'üß†', 'ru': {'title': '–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å–∏—Å—Ç–µ–º–∞ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–∫—Ä—ã—Ç–∞', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –ø–æ–¥—Å–∏—Å—Ç–µ–º—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—É—é –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –º–æ–∑–≥–∞. –í —ç—Ç–æ–π –ø–æ–¥—Å–∏—Å—Ç–µ–º–µ –≤—ã—è–≤–ª–µ–Ω—ã –Ω–µ–π—Ä–æ–Ω—ã —Å—Ç–æ–∏–º–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∏ –ø–æ–∫–∞–∑–∞–Ω–∞ –∏—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —Ä–æ–ª—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –≠—Ç–∏ –Ω–µ–π—Ä–æ–Ω—ã —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–æ—è–≤–ª—è—é—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –Ω–∞–±–æ—Ä–∞–º –¥–∞–Ω–Ω—ã—Ö –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –ø—Ä–∏ —ç—Ç–æ–º —Ö–æ—Ä–æ—à—É—é –ø–µ—Ä–µ–Ω–æ—Å –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∏ –¥–æ—Ñ–∞–º–∏–Ω–æ–ø–æ–¥–æ–±–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω—ã, –∫–æ–¥–∏—Ä—É—é—â–∏–µ –æ—à–∏–±–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –∞–∫—Ç–∏–≤–∏—Ä—É—é—â–∏–µ—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏ –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Unlocking the Reward System in Large Language Models', 'desc': "This paper explores a specific subsystem in the hidden states of Large Language Models (LLMs) that functions similarly to the reward system in the human brain. It identifies 'value neurons' that predict the expected value of states, which are crucial for the model's reasoning capabilities. The research shows that these neurons are consistent across various datasets and model architectures, indicating their robustness and transferability. Additionally, the study uncovers 'dopamine neurons' that signal reward prediction errors, highlighting their role in adjusting expectations based on actual rewards received."}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÁ®ÄÁñèÂ•ñÂä±Â≠êÁ≥ªÁªü', 'desc': 'Êú¨Á†îÁ©∂Âú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÈöêËóèÁä∂ÊÄÅ‰∏≠ËØÜÂà´Âá∫‰∏Ä‰∏™Á®ÄÁñèÂ•ñÂä±Â≠êÁ≥ªÁªüÔºåÁ±ª‰ºº‰∫é‰∫∫ËÑë‰∏≠ÁöÑÁîüÁâ©Â•ñÂä±Â≠êÁ≥ªÁªü„ÄÇÊàë‰ª¨ÂèëÁé∞ËØ•Â≠êÁ≥ªÁªüÂåÖÂê´‰ª∑ÂÄºÁ•ûÁªèÂÖÉÔºåË°®Á§∫Ê®°ÂûãÂØπÁä∂ÊÄÅ‰ª∑ÂÄºÁöÑÂÜÖÈÉ®ÊúüÊúõÔºåÂπ∂ÈÄöËøáÂπ≤È¢ÑÂÆûÈ™åËØÅÊòéËøô‰∫õÁ•ûÁªèÂÖÉÂú®Êé®ÁêÜ‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇÂÆûÈ™åË°®ÊòéÔºåËøô‰∫õ‰ª∑ÂÄºÁ•ûÁªèÂÖÉÂú®‰∏çÂêåÊï∞ÊçÆÈõÜ„ÄÅÊ®°ÂûãËßÑÊ®°ÂíåÊû∂ÊûÑ‰∏≠ÈÉΩË°®Áé∞Âá∫Âº∫Â§ßÁöÑÈ≤ÅÊ£íÊÄßÔºåÂπ∂‰∏îÂú®‰∏çÂêåÊï∞ÊçÆÈõÜÂíå‰ªéÂêå‰∏ÄÂü∫Á°ÄÊ®°ÂûãÂæÆË∞ÉÁöÑÊ®°Âûã‰πãÈó¥ÂÖ∑ÊúâÊòæËëóÁöÑÂèØËøÅÁßªÊÄß„ÄÇÊàë‰ª¨ËøòÂèëÁé∞Â•ñÂä±È¢ÑÊµãËØØÂ∑ÆÔºàRPEÔºâÁºñÁ†ÅÁöÑÂ§öÂ∑¥ËÉ∫Á•ûÁªèÂÖÉÔºåÂΩìÂ•ñÂä±È´ò‰∫éÈ¢ÑÊúüÊó∂ÊøÄÊ¥ªÂº∫ÁÉàÔºå‰Ωé‰∫éÈ¢ÑÊúüÊó∂ÊøÄÊ¥ªËæÉÂº±„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00759', 'title': 'Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.00759', 'abstract': "Adaptive Ability Decomposing (A¬≤D) enhances reinforcement learning with verifiable rewards by decomposing complex questions into simpler sub-questions, improving LLM reasoning through guided exploration without requiring a teacher model.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A^2D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A^2D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.", 'score': 5, 'issue_id': 884, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': '4b82299bd9ee8e8f', 'authors': ['Zhipeng Chen', 'Xiaobo Qin', 'Wayne Xin Zhao', 'Youbin Wu', 'Ji-Rong Wen'], 'affiliations': ['ByteDance Seed', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.00759.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#reasoning'], 'emoji': 'üß©', 'ru': {'title': '–†–∞–∑–ª–æ–∂–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: –æ—Ç –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∫ –ø–æ—à–∞–≥–æ–≤—ã–º —Ä–µ—à–µ–Ω–∏—è–º', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ Adaptive Ability Decomposing (A¬≤D) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –ø—É—Ç—ë–º —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –ø–æ–¥–≤–æ–ø—Ä–æ—Å—ã. –ú–µ—Ç–æ–¥ –æ–±—É—á–∞–µ—Ç –¥–µ–∫–æ–º–ø–æ–∑–µ—Ä —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –±–µ–∑ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ LLM –ø—Ä–æ–≤–æ–¥–∏—Ç—å –±–æ–ª–µ–µ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ —Å–ª–µ–ø–æ–≥–æ –ø–æ–∏—Å–∫–∞, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. A¬≤D —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –º–æ–¥—É–ª—å–Ω–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É—á–∏—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Decomposing Complexity for Smarter Learning', 'desc': 'Adaptive Ability Decomposing (A¬≤D) is a method that improves reinforcement learning with verifiable rewards (RLVR) by breaking down complex questions into simpler sub-questions. This approach enhances the reasoning capabilities of large language models (LLMs) by providing structured guidance during the learning process, allowing for more effective exploration. A¬≤D trains a decomposer to create these sub-questions without needing a teacher model, which helps the main model learn better. The method has been shown to be effective compared to other techniques and can be integrated into various RLVR algorithms as a flexible module.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îËÉΩÂäõÂàÜËß£ÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'Ëá™ÈÄÇÂ∫îËÉΩÂäõÂàÜËß£ÔºàA¬≤DÔºâÊòØ‰∏ÄÁßçÂ¢ûÂº∫Âº∫ÂåñÂ≠¶‰π†ÁöÑÊäÄÊúØÔºåÈÄöËøáÂ∞ÜÂ§çÊùÇÈóÆÈ¢òÂàÜËß£‰∏∫Êõ¥ÁÆÄÂçïÁöÑÂ≠êÈóÆÈ¢òÔºåÊù•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÂú®‰∏ç‰æùËµñÊïôÂ∏àÊ®°ÂûãÁöÑÊÉÖÂÜµ‰∏ãÔºåÂà©Áî®ÂèØÈ™åËØÅÂ•ñÂä±ÁöÑÂº∫ÂåñÂ≠¶‰π†ÔºàRLVRÔºâËøáÁ®ãÔºåÊèê‰æõÈ¢ùÂ§ñÁöÑ‰ø°ÊÅØÊîØÊåÅ„ÄÇA¬≤DÈ¶ñÂÖàËÆ≠ÁªÉ‰∏Ä‰∏™ÂàÜËß£Âô®Ôºå‰ΩøÂÖ∂ËÉΩÂ§üÊúâÊïàÂú∞Â∞ÜÂ§çÊùÇÈóÆÈ¢òÂàÜËß£ÔºåÂπ∂‰∏∫ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÊØè‰∏™ÈóÆÈ¢òÊ†áÊ≥®Â≠êÈóÆÈ¢ò„ÄÇÈÄöËøáËøôÁßçÊñπÂºèÔºåA¬≤DËÉΩÂ§ü‰Ωú‰∏∫‰∏Ä‰∏™Âç≥ÊèíÂç≥Áî®ÁöÑÊ®°ÂùóÔºåÊèêÂçá‰∏çÂêåRLVRÁÆóÊ≥ïÁöÑÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02110', 'title': 'An Empirical Study of World Model Quantization', 'url': 'https://huggingface.co/papers/2602.02110', 'abstract': 'Post-training quantization effects in world models reveal unique failure modes and trade-offs between accuracy, bit-width, and planning performance, particularly in encoder-predictor module asymmetries and low-bit rollout stability.  \t\t\t\t\tAI-generated summary \t\t\t\t World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.', 'score': 4, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '7cebcfb01623bc43', 'authors': ['Zhongqian Fu', 'Tianyi Zhao', 'Kai Han', 'Hang Zhou', 'Xinghao Chen', 'Yunhe Wang'], 'affiliations': ['Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2602.02110.jpg', 'data': {'categories': ['#inference', '#rl', '#agents'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–ö–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö –º–∏—Ä–∞: –æ—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –Ω–∞ world models, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å –¥–∏–Ω–∞–º–∏–∫—É –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑—Ä—è–¥–Ω–æ—Å—Ç—è—Ö –±–∏—Ç–æ–≤ –∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞—Ö –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, —Ä–∞—Å–∫—Ä—ã–≤–∞—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Ç–∫–∞–∑—ã –∏ –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏ –º–µ–∂–¥—É –º–æ–¥—É–ª—è–º–∏ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—è. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –≥—Ä—É–ø–ø –º–æ–∂–µ—Ç —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –Ω–∏–∑–∫–æ–π —Ä–∞–∑—Ä—è–¥–Ω–æ—Å—Ç—å—é, –Ω–æ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–µ–≥—Ä–∞–¥–∏—Ä—É–µ—Ç –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–ª—å—é –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —É—Å–ø–µ—Ö–æ–º –∑–∞–¥–∞—á–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö world models –ø–æ–¥ —Å—Ç—Ä–æ–≥–∏–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏.'}, 'en': {'title': 'Navigating the Trade-offs of Quantization in World Models', 'desc': 'This paper investigates the impact of post-training quantization (PTQ) on world models, which are used for simulating and planning in environments. The authors conduct experiments to evaluate how different quantization methods affect model performance, particularly focusing on accuracy, bit-width, and planning capabilities. They discover that quantization can lead to unique failure modes, especially due to asymmetries between the encoder and predictor modules. The findings highlight the importance of careful quantization strategies to maintain performance while reducing computational costs.'}, 'zh': {'title': 'ÈáèÂåñ‰∏ñÁïåÊ®°ÂûãÔºöÊïàÁéá‰∏éÂáÜÁ°ÆÊÄßÁöÑÂπ≥Ë°°‰πãÈÅì', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂêéËÆ≠ÁªÉÈáèÂåñÔºàPTQÔºâÂØπ‰∏ñÁïåÊ®°ÂûãÁöÑÂΩ±ÂìçÔºåÁâπÂà´ÊòØÂú®ÂáÜÁ°ÆÊÄß„ÄÅ‰ΩçÂÆΩÂíåËßÑÂàíÊÄßËÉΩ‰πãÈó¥ÁöÑÊùÉË°°„ÄÇ‰∏ñÁïåÊ®°ÂûãÈÄöËøáÂ≠¶‰π†ÁéØÂ¢ÉÂä®ÊÄÅÁöÑÂÜÖÈÉ®Ë°®Á§∫Ôºå‰ΩøÊô∫ËÉΩ‰ΩìËÉΩÂ§üÂú®Á¥ßÂáëÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠ËøõË°åÊ®°ÊãüÂíåÊé®ÁêÜ„ÄÇÁÑ∂ËÄåÔºåËøêË°åËøô‰∫õÊ®°ÂûãÈúÄË¶ÅÂ§ßÈáèÁöÑËÆ°ÁÆóËµÑÊ∫êÂíåÂÜÖÂ≠òÔºåÂõ†Ê≠§Ê®°ÂûãÈáèÂåñÂØπ‰∫éÈ´òÊïàÈÉ®ÁΩ≤Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåÈáèÂåñ‰∏ç‰ªÖÂΩ±ÂìçÊ†áÂáÜÁöÑÂáÜÁ°ÆÊÄßÂíå‰ΩçÂÆΩÔºåËøòÊè≠Á§∫‰∫ÜÁºñÁ†ÅÂô®ÂíåÈ¢ÑÊµãÂô®Ê®°Âùó‰πãÈó¥ÁöÑ‰∏çÂØπÁß∞ÊÄß‰ª•Âèä‰Ωé‰ΩçÂÆΩÂõûÊªöÁöÑÁ®≥ÂÆöÊÄßÈóÆÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02039', 'title': 'Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models', 'url': 'https://huggingface.co/papers/2602.02039', 'abstract': 'Agentic large language models require investigatory intelligence for autonomous data analysis, demonstrated through the Deep Data Research benchmark that evaluates their ability to extract insights from databases without explicit queries.  \t\t\t\t\tAI-generated summary \t\t\t\t The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.', 'score': 4, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '392ca60694870d1e', 'authors': ['Wei Liu', 'Peijie Yu', 'Michele Orini', 'Yali Du', 'Yulan He'], 'affiliations': ['Kings College London', 'Tencent', 'The Alan Turing Institute'], 'pdf_title_img': 'assets/pdf/title_img/2602.02039.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset'], 'emoji': 'üîç', 'ru': {'title': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç: –æ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–∏–≤–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–Ω—ã —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–≤–∏—Ç—å —Ü–µ–ª–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å —è–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –±–µ–Ω—á–º–∞—Ä–∫ Deep Data Research, –≥–¥–µ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–∑ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –û—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ö–æ—Ç—è –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–∞—á–∞—Ç–∫–∏ –∞–≥–µ–Ω—Ç–∏–≤–Ω–æ—Å—Ç–∏, –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞—ë—Ç—Å—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∑–∞–≤–∏—Å–∏—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∞–≥–µ–Ω—Ç–∞ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–æ –∏ –æ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Å–∞–º–∏—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Empowering LLMs with Investigatory Intelligence for Autonomous Data Insights', 'desc': "This paper introduces the concept of investigatory intelligence in large language models (LLMs), which is the ability to autonomously analyze data and extract insights without specific queries. The authors present the Deep Data Research (DDR) benchmark, designed to evaluate LLMs' performance in this area by allowing them to explore databases and derive key insights independently. The study reveals that while advanced models show some level of agency, they struggle with long-term exploration tasks. The findings suggest that developing effective investigatory intelligence requires more than just improving model size; it also involves enhancing the intrinsic strategies used by these models."}, 'zh': {'title': 'Ëá™‰∏ªÊï∞ÊçÆÂàÜÊûêÁöÑË∞ÉÊü•Êô∫ËÉΩ', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫Ü‰ª£ÁêÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàAgentic LLMsÔºâÂú®Ëá™‰∏ªÊï∞ÊçÆÂàÜÊûê‰∏≠ÊâÄÈúÄÁöÑË∞ÉÊü•Êô∫ËÉΩ„ÄÇË∞ÉÊü•Êô∫ËÉΩ‰∏ç‰ªÖ‰ªÖÊòØÊ≠£Á°ÆÂõûÁ≠îÈóÆÈ¢òÔºåËÄåÊòØËÉΩÂ§üËá™‰∏ªËÆæÂÆöÁõÆÊ†áÂíåÂÜ≥ÂÆöÊé¢Á¥¢ÂÜÖÂÆπ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫ÜÊ∑±Â∫¶Êï∞ÊçÆÁ†îÁ©∂ÔºàDeep Data Research, DDRÔºâÂü∫ÂáÜÔºåËØÑ‰º∞Ê®°Âûã‰ªéÊï∞ÊçÆÂ∫ì‰∏≠ÊèêÂèñÂÖ≥ÈîÆËßÅËß£ÁöÑËÉΩÂäõ„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºåÂ∞ΩÁÆ°ÂâçÊ≤øÊ®°ÂûãÂ±ïÁé∞Âá∫‰∏ÄÂÆöÁöÑËá™‰∏ªÊÄßÔºå‰ΩÜÂú®ÈïøÊó∂Èó¥Êé¢Á¥¢ÊñπÈù¢‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00269', 'title': 'VoxServe: Streaming-Centric Serving System for Speech Language Models', 'url': 'https://huggingface.co/papers/2602.00269', 'abstract': 'VoxServe is a unified serving system for Speech Language Models that enhances streaming performance through model-execution abstraction, streaming-aware scheduling, and asynchronous inference pipelines.  \t\t\t\t\tAI-generated summary \t\t\t\t Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.', 'score': 4, 'issue_id': 888, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'fe11c22c452fd89c', 'authors': ['Keisuke Kamahori', 'Wei-Tzu Lee', 'Atindra Jha', 'Rohan Kadekodi', 'Stephanie Wang', 'Arvind Krishnamurthy', 'Baris Kasikci'], 'affiliations': ['Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2602.00269.jpg', 'data': {'categories': [], 'emoji': 'üéôÔ∏è', 'ru': {'title': '–í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ—á–µ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'VoxServe ‚Äî —ç—Ç–æ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏—è —Ä–µ—á–µ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é –º–æ–¥–µ–ª–∏-–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç–¥–µ–ª—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏ –æ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π, –ø–æ–∑–≤–æ–ª—è—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã SpeechLM –≤ –µ–¥–∏–Ω–æ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–µ. –ê–≤—Ç–æ—Ä—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ –ø–æ—Ç–æ–∫–æ–≤–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π pipeline –≤—ã–≤–æ–¥–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –æ–±—â–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ VoxServe –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤ 10-20 —Ä–∞–∑ –≤—ã—à–µ –ø—Ä–æ–ø—É—Å–∫–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º–∏ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–∏–º–æ–π –∑–∞–¥–µ—Ä–∂–∫–µ.'}, 'en': {'title': 'VoxServe: Boosting Speech Model Performance in Real-Time', 'desc': 'VoxServe is a new system designed to improve the performance of Speech Language Models (SpeechLMs) in real-time applications. It achieves this by separating the model architecture from the system optimizations, allowing for a variety of SpeechLMs to be used efficiently. The system employs advanced scheduling techniques and an asynchronous inference pipeline to enhance processing speed and reduce delays. Tests show that VoxServe can process data 10-20 times faster than current systems while still ensuring effective streaming capabilities.'}, 'zh': {'title': 'VoxServeÔºöÊèêÂçáËØ≠Èü≥Ê®°ÂûãÊµÅÂ™í‰ΩìÊÄßËÉΩÁöÑÁªü‰∏ÄÁ≥ªÁªü', 'desc': 'VoxServeÊòØ‰∏Ä‰∏™Áªü‰∏ÄÁöÑËØ≠Èü≥ËØ≠Ë®ÄÊ®°ÂûãÊúçÂä°Á≥ªÁªüÔºåÊó®Âú®ÊèêÂçáÊµÅÂ™í‰ΩìÊÄßËÉΩ„ÄÇÂÆÉÈÄöËøáÊ®°ÂûãÊâßË°åÊäΩË±°„ÄÅÊµÅÂ™í‰ΩìÊÑüÁü•Ë∞ÉÂ∫¶ÂíåÂºÇÊ≠•Êé®ÁêÜÁÆ°ÈÅìÊù•‰ºòÂåñÊÄßËÉΩ„ÄÇVoxServeËÉΩÂ§üÁÅµÊ¥ªÊîØÊåÅÂ§öÁßçËØ≠Èü≥ËØ≠Ë®ÄÊ®°ÂûãÊû∂ÊûÑÔºåËß£ÂÜ≥‰∫ÜÁé∞ÊúâÁ≥ªÁªüÂú®Â§öÊ†∑ÊÄßÂíåÊïàÁéá‰∏äÁöÑ‰∏çË∂≥„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåVoxServeÂú®‰øùÊåÅ‰ΩéÂª∂ËøüÁöÑÂêåÊó∂ÔºåÂêûÂêêÈáèÊØîÁé∞ÊúâÂÆûÁé∞È´òÂá∫10-20ÂÄçÔºåÁ°Æ‰øù‰∫ÜÈ´òÊïàÁöÑÊµÅÂ™í‰ΩìÂ§ÑÁêÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22588', 'title': 'Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry', 'url': 'https://huggingface.co/papers/2601.22588', 'abstract': 'Small language models can effectively evaluate outputs by leveraging internal representations rather than generating responses, enabling a more efficient and interpretable evaluation approach through a probing-based framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.', 'score': 4, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '451f5716f6f71f8a', 'authors': ['Zhuochun Li', 'Yong Zhang', 'Ming Li', 'Yuelyu Ji', 'Yiming Zeng', 'Ning Cheng', 'Yun Zhu', 'Yanmeng Wang', 'Shaojun Wang', 'Jing Xiao', 'Daqing He'], 'affiliations': ['Ping An Technology (Shenzhen) Co., Ltd.', 'University of Connecticut', 'University of Maryland, College Park', 'University of Pittsburgh'], 'pdf_title_img': 'assets/pdf/title_img/2601.22588.jpg', 'data': {'categories': ['#benchmark', '#training', '#small_models'], 'emoji': 'üîç', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –≤–º–µ—Å—Ç–æ –±–æ–ª—å—à–∏—Ö —Å—É–¥–µ–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–∞–ª–µ–Ω—å–∫–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–ø–æ—Ç–µ–∑—É –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —ë–º–∫–æ—Å—Ç–∏: –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —á–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –í–º–µ—Å—Ç–æ –ø–∞—Ä–∞–¥–∏–≥–º—ã "LLM-as-a-Judge" –æ–Ω–∏ –≤–Ω–µ–¥—Ä—è—é—Ç –ø–æ–¥—Ö–æ–¥ "Representation-as-a-Judge" ‚Äî —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ—Ü–µ–Ω–∫–∏ –±–µ–∑ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—â—É—é —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ INSPECTOR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–µ–π –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –Ω–æ —Å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å—é.'}, 'en': {'title': 'Unlocking Evaluation Efficiency with Small Models', 'desc': 'This paper explores how small language models can evaluate outputs more efficiently by using their internal representations instead of generating responses. The authors propose the Semantic Capacity Asymmetry Hypothesis, which suggests that evaluating text requires less semantic capacity than generating it. They introduce a new framework called INSPECTOR, which uses probing techniques to predict evaluation scores based on the hidden states of smaller models. The results show that INSPECTOR outperforms traditional prompting methods and provides a more interpretable and scalable evaluation approach.'}, 'zh': {'title': 'Â∞èÊ®°ÂûãÁöÑËØÑ‰º∞Êñ∞ÊÄùË∑ØÔºöÂÜÖÈÉ®Ë°®Á§∫‰ºò‰∫éÁîüÊàê', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂ¶Ç‰ΩïÂà©Áî®ÂÜÖÈÉ®Ë°®Á§∫ËøõË°åÊúâÊïàËØÑ‰º∞ÔºåËÄå‰∏çÊòØÁîüÊàêÂìçÂ∫î„ÄÇÊàë‰ª¨ÂèëÁé∞Â∞èÂûãÊ®°ÂûãËôΩÁÑ∂ÁîüÊàêËÉΩÂäõËæÉÂº±Ôºå‰ΩÜÂÖ∂ÈöêËóèÁä∂ÊÄÅ‰∏≠ÁºñÁ†Å‰∫Ü‰∏∞ÂØåÁöÑËØÑ‰º∞‰ø°Âè∑„ÄÇÂü∫‰∫éÊ≠§ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜËØ≠‰πâËÉΩÂäõ‰∏çÂØπÁß∞ÂÅáËÆæÔºåËÆ§‰∏∫ËØÑ‰º∞ÊâÄÈúÄÁöÑËØ≠‰πâËÉΩÂäõËøú‰Ωé‰∫éÁîüÊàê„ÄÇÊàë‰ª¨ÁöÑINSPECTORÊ°ÜÊû∂ÈÄöËøáÊé¢ÊµãÂ∞èÊ®°ÂûãÁöÑÂÜÖÈÉ®ÁªìÊûÑÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊó†ÈúÄÁîüÊàêÁöÑËØÑ‰º∞Á≠ñÁï•ÔºåÊòæËëóÊèêÈ´ò‰∫ÜËØÑ‰º∞ÁöÑÊïàÁéáÂíåÂèØËß£ÈáäÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01984', 'title': 'Enhancing Multi-Image Understanding through Delimiter Token Scaling', 'url': 'https://huggingface.co/papers/2602.01984', 'abstract': "Scaling hidden states of delimiter tokens in vision-language models reduces cross-image information leakage and improves multi-image reasoning performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.", 'score': 3, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '4c3d0a95f670cefb', 'authors': ['Minyoung Lee', 'Yeji Park', 'Dongjun Hwang', 'Yejin Kim', 'Seong Joon Oh', 'Junsuk Choe'], 'affiliations': ['KAIST', 'Sogang University', 'Tubingen University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01984.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#leakage', '#benchmark', '#reasoning'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —É—Ç–µ—á–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —É—Ç–µ—á–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ –±–æ–ª—å—à–∏—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–µ—Ç–æ–¥ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Ç–æ–∫–µ–Ω–æ–≤-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–º–µ—á–∞—é—Ç –≥—Ä–∞–Ω–∏—Ü—ã –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —á—Ç–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –≤–Ω—É—Ç—Ä–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –º—É–ª—å—Ç–∏–∏–∑–æ–±—Ä–∞–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–Ω–∞–ª–∏–∑–∞ –º—É–ª—å—Ç–∏–¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã—Ö –∏ –º—É–ª—å—Ç–∏—Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ä–∞–∑–ª–∏—á–∞—Ç—å –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.'}, 'en': {'title': 'Enhancing Image Distinction in Vision-Language Models', 'desc': 'This paper addresses the issue of cross-image information leakage in Large Vision-Language Models (LVLMs) when processing multiple images. The authors propose a novel approach that scales the hidden states of delimiter tokens, which are used to separate images in the input. By enhancing these tokens, the model can better maintain image-specific information and reduce confusion between different images. The results show significant improvements in multi-image reasoning tasks and also benefit text-only tasks without incurring extra training or inference costs.'}, 'zh': {'title': 'ÊèêÂçáËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öÂõæÂÉèÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÔºåÈÄöËøáË∞ÉÊï¥ÂàÜÈöîÁ¨¶‰ª§ÁâåÁöÑÈöêËóèÁä∂ÊÄÅÔºåÊù•ÂáèÂ∞ëËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑË∑®ÂõæÂÉè‰ø°ÊÅØÊ≥ÑÊºèÈóÆÈ¢ò„ÄÇÁé∞ÊúâÁöÑÂ§ßÂûãËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÂçïÂõæÂÉè‰ªªÂä°Êó∂Ë°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®Â§öÂõæÂÉèËæìÂÖ•Êó∂ÊÄßËÉΩ‰∏ãÈôçÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫Ê®°ÂûãÊó†Ê≥ïÊúâÊïàÂå∫ÂàÜ‰∏çÂêåÂõæÂÉèÁöÑ‰ø°ÊÅØ„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂ¢ûÂº∫‰∫ÜÂõæÂÉèÂÜÖÈÉ®ÁöÑ‰∫§‰∫íÔºåÂêåÊó∂ÈôêÂà∂‰∫Ü‰∏çÂøÖË¶ÅÁöÑË∑®ÂõæÂÉè‰∫§‰∫íÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ®°ÂûãÂØπÂõæÂÉèÁâπÂÆö‰ø°ÊÅØÁöÑ‰øùÁïôËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§öÂõæÂÉèÊé®ÁêÜÂíåÊñáÊú¨ÁêÜËß£‰ªªÂä°‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01842', 'title': 'Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models', 'url': 'https://huggingface.co/papers/2602.01842', 'abstract': "A new test-time scaling framework called Prism is introduced for discrete diffusion language models that improves reasoning performance through hierarchical trajectory search, local branching with partial remasking, and self-verified feedback mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.", 'score': 3, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '8ccd8159c82c4354', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#inference', '#benchmark', '#reasoning', '#diffusion', '#optimization', '#open_source', '#training'], 'emoji': 'üå≥', 'ru': {'title': '–£–º–Ω–æ–µ –≤–µ—Ç–≤–ª–µ–Ω–∏–µ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∏–Ω—Ñ–µ—Ä—Å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫', 'desc': "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Prism –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ —ç—Ç–∞–ø–µ –¥–µ–Ω–æ–π–∑–∏–Ω–≥–∞, –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–µ—Ç–≤–ª–µ–Ω–∏–µ —Å —á–∞—Å—Ç–∏—á–Ω—ã–º –ø–µ—Ä–µmasking'–æ–º –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∏ —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫—É —á–µ—Ä–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –≤–Ω–µ—à–Ω–∏—Ö –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –¥–æ—Å—Ç–∏–≥–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ best-of-N —Å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –º–µ–Ω—å—à–∏–º —á–∏—Å–ª–æ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π."}, 'en': {'title': 'Unlocking dLLMs: Efficient Reasoning with Prism', 'desc': "The paper introduces Prism, a new framework designed to enhance the reasoning capabilities of discrete diffusion language models (dLLMs) during inference. It employs a Hierarchical Trajectory Search (HTS) to optimize computational resources by dynamically pruning and reallocating them in the denoising process. Additionally, Prism utilizes Local Branching with Partial Remasking to maintain high-confidence tokens while exploring diverse outputs. Finally, it incorporates Self-Verified Feedback (SVF) to improve the model's self-evaluation, resulting in better performance with fewer function evaluations across various benchmarks."}, 'zh': {'title': 'PrismÔºöÊèêÂçáÁ¶ªÊï£Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜÊÄßËÉΩÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊµãËØïÊó∂Èó¥Áº©ÊîæÊ°ÜÊû∂PrismÔºåÊó®Âú®ÊèêÈ´òÁ¶ªÊï£Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂàÜÂ±ÇËΩ®ËøπÊêúÁ¥¢„ÄÅÂ±ÄÈÉ®ÂàÜÊîØ‰∏éÈÉ®ÂàÜÈáçÊé©Á†Å‰ª•ÂèäËá™ÊàëÈ™åËØÅÂèçÈ¶àÊú∫Âà∂Êù•ÂÆûÁé∞Êõ¥È´òÊïàÁöÑÊé®ÁêÜ„ÄÇPrismËÉΩÂ§üÂú®ÂéªÂô™Á™óÂè£ÁöÑÊó©‰∏≠ÊúüÂä®ÊÄÅ‰øÆÂâ™ÂíåÈáçÊñ∞ÂàÜÈÖçËÆ°ÁÆóËµÑÊ∫êÔºåÂêåÊó∂Êé¢Á¥¢Â§öÊ†∑ÂåñÁöÑÂÆûÁé∞ÊñπÂºè„ÄÇÈÄöËøáÂú®Â§ö‰∏™Êï∞Â≠¶Êé®ÁêÜÂíå‰ª£Á†ÅÁîüÊàêÂü∫ÂáÜÊµãËØï‰∏≠È™åËØÅÔºåPrismÂú®ÊÄßËÉΩÂíåÊïàÁéá‰πãÈó¥ËææÊàê‰∫ÜËâØÂ•ΩÁöÑÂπ≥Ë°°„ÄÇ'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2602.01296', 'title': 'Interacted Planes Reveal 3D Line Mapping', 'url': 'https://huggingface.co/papers/2602.01296', 'abstract': 'LiP-Map presents a line-plane joint optimization framework that explicitly models learnable line and planar primitives for accurate 3D line mapping in man-made environments.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.', 'score': 3, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '506f8c0e4443e23a', 'authors': ['Zeran Ke', 'Bin Tan', 'Gui-Song Xia', 'Yujun Shen', 'Nan Xue'], 'affiliations': ['Ant Group', 'School of Computer Science and the School of Artificial Intelligence, Wuhan University, Wuhan 430072, China', 'School of Computer Science, Wuhan University, Wuhan 430072, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.01296.jpg', 'data': {'categories': ['#3d', '#cv'], 'emoji': 'üìê', 'ru': {'title': '–°–æ–≤–º–µ—Å—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ª–∏–Ω–∏–π –∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ 3D-–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è', 'desc': 'LiP-Map –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ª–∏–Ω–∏–π –∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö –ª–∏–Ω–∏–π –∏–∑ –º–Ω–æ–≥–æ–≤–∏–¥–æ–≤—ã—Ö RGB-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ —è–≤–Ω–æ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –æ–±—É—á–∞–µ–º—ã–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã –ª–∏–Ω–∏–π –∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ –∏ —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–µ –ª–∏–Ω–∏–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –≤–æ–∑–Ω–∏–∫–∞—é—Ç –∫–∞–∫ —Ä—ë–±—Ä–∞ –∫–æ–Ω–µ—á–Ω—ã—Ö –ø–ª–æ—Å–∫–∏—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π. –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –ø—Ä–∏–º–∏—Ç–∏–≤–∞–º–∏ –ø–ª–æ—Å–∫–æ—Å—Ç–µ–π –∏ –ª–∏–Ω–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å —Ç–æ—á–Ω–æ–≥–æ –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª–∏–Ω–∏–π –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å—Ü–µ–Ω—ã –∑–∞–Ω–∏–º–∞–µ—Ç 3-5 –º–∏–Ω—É—Ç). –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 100 —Å—Ü–µ–Ω–∞—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ª–∏–Ω–∏—è—Ö.'}, 'en': {'title': 'Revolutionizing 3D Line Mapping with LiP-Map', 'desc': 'LiP-Map introduces a novel framework for 3D line mapping that combines line and planar elements to enhance accuracy in man-made environments. By modeling learnable line and planar primitives, it effectively captures the relationship between lines and planes, which are essential for structured scene representation. The framework operates efficiently, completing reconstructions in just 3 to 5 minutes per scene, while significantly improving mapping quality on various datasets. Additionally, LiP-Map enhances line-assisted visual localization, demonstrating its effectiveness in practical applications.'}, 'zh': {'title': 'LiP-MapÔºöÈ´òÊïàÁ≤æÁ°ÆÁöÑ3DÁ∫øÊò†Â∞ÑÊñ∞ÊñπÊ≥ï', 'desc': 'LiP-MapÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ∫øÈù¢ËÅîÂêà‰ºòÂåñÊ°ÜÊû∂Ôºå‰∏ìÈó®Âª∫Ê®°ÂèØÂ≠¶‰π†ÁöÑÁ∫øÂíåÈù¢ÂéüËØ≠Ôºå‰ª•ÂÆûÁé∞Á≤æÁ°ÆÁöÑ3DÁ∫øÊò†Â∞Ñ„ÄÇËØ•ÊñπÊ≥ï‰ªéÁâ©ÁêÜÂíåÊãìÊâëÁöÑËßíÂ∫¶Á†îÁ©∂3DÁ∫øÁöÑÁîüÊàêÔºåËÆ§‰∏∫3DÁ∫øÊúÄËá™ÁÑ∂Âú∞Âá∫Áé∞Âú®ÊúâÈôêÁöÑ3DÂπ≥Èù¢ÁâáÁöÑËæπÁºò„ÄÇÈÄöËøáÊòæÂºèÊûÑÂª∫Âπ≥Èù¢ÂíåÁ∫øÂéüËØ≠‰πãÈó¥ÁöÑ‰∫§‰∫íÔºåLiP-MapÂú®‰øùÊåÅÈ´òÊïàÊÄßÁöÑÂêåÊó∂ÔºåÊèê‰æõ‰∫ÜÂáÜÁ°Æ‰∏îËØ¶ÁªÜÁöÑ3DÁ∫øÊò†Â∞Ñ„ÄÇÁªèËøáÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑÊµãËØïÔºåLiP-MapÂú®ÂáÜÁ°ÆÊÄßÂíåÂÆåÊï¥ÊÄß‰∏äË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÂπ∂ÊòæËëóÊèêÂçá‰∫ÜÂü∫‰∫éÁ∫øÁöÑËßÜËßâÂÆö‰ΩçÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01077', 'title': 'PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers', 'url': 'https://huggingface.co/papers/2602.01077', 'abstract': 'PISA is a novel sparse attention method that improves diffusion transformer efficiency by approximating non-critical attention blocks instead of discarding them, achieving faster processing with maintained quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.', 'score': 3, 'issue_id': 884, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '20464de619792c9d', 'authors': ['Haopeng Li', 'Shitong Shao', 'Wenliang Zhong', 'Zikai Zhou', 'Lichen Bai', 'Hui Xiong', 'Zeke Xie'], 'affiliations': ['The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2602.01077.jpg', 'data': {'categories': ['#multimodal', '#video', '#architecture', '#inference', '#diffusion', '#optimization'], 'emoji': '‚ö°', 'ru': {'title': '–¢–æ—á–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ–≥–æ, –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω–æ–≥–æ: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ PISA, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∑–∞ —Å—á–µ—Ç –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è –Ω–µ–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤–º–µ—Å—Ç–æ –∏—Ö –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –æ—Ü–µ–Ω–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –Ω–µ–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–∞—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –∏—Ö —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. PISA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é exact-or-approximate: —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ—á–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –¢–µ–π–ª–æ—Ä–∞ –¥–ª—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è –æ—Å—Ç–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏—è. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 1.91-2.57 —Ä–∞–∑ –Ω–∞ –≤–∏–¥–µ–æ–≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏—Ö –º–æ–¥–µ–ª—è—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ.'}, 'en': {'title': 'PISA: Efficient Attention Without Compromise', 'desc': 'PISA is a new method for sparse attention in diffusion transformers that enhances efficiency by approximating non-critical attention blocks instead of discarding them. This approach addresses the inefficiencies caused by the quadratic complexity of traditional attention mechanisms, which can slow down processing. By leveraging the stable distribution of attention scores in non-critical blocks, PISA maintains high-quality outputs while significantly speeding up computations. The method achieves impressive speedups in various applications, demonstrating its effectiveness in balancing performance and quality in machine learning tasks.'}, 'zh': {'title': 'PISAÔºöÊèêÂçáÊâ©Êï£ÂèòÊç¢Âô®ÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'PISAÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÁ®ÄÁñèÊ≥®ÊÑèÂäõÊñπÊ≥ïÔºåÈÄöËøáËøë‰ººÈùûÂÖ≥ÈîÆÊ≥®ÊÑèÂäõÂùóËÄå‰∏çÊòØÁõ¥Êé•‰∏¢ÂºÉÂÆÉ‰ª¨ÔºåÊù•ÊèêÈ´òÊâ©Êï£ÂèòÊç¢Âô®ÁöÑÊïàÁéá„ÄÇËøôÁßçÊñπÊ≥ïÂú®‰øùÊåÅË¥®ÈáèÁöÑÂêåÊó∂ÔºåÂÆûÁé∞‰∫ÜÊõ¥Âø´ÁöÑÂ§ÑÁêÜÈÄüÂ∫¶„ÄÇPISAÈááÁî®‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ≤æÁ°ÆÊàñËøë‰ººÁ≠ñÁï•ÔºåÂØπ‰∫éÂÖ≥ÈîÆÂùó‰øùÊåÅÁ≤æÁ°ÆËÆ°ÁÆóÔºåËÄåÈÄöËøáÂùóÁä∂Ê≥∞ÂãíÂ±ïÂºÄÊúâÊïàÂú∞Ëøë‰ººÂÖ∂‰ΩôÈÉ®ÂàÜ„ÄÇËøô‰ΩøÂæóPISAËÉΩÂ§üÂú®ÈÄüÂ∫¶ÂíåË¥®Èáè‰πãÈó¥Êû∂Ëµ∑Ê°•Ê¢ÅÔºåÂÆûÈ™åÁªìÊûúË°®ÊòéÂÖ∂Âú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëóÂä†ÈÄüÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊúÄÈ´òÁöÑÁîüÊàêË¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22674', 'title': 'VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration', 'url': 'https://huggingface.co/papers/2601.22674', 'abstract': 'VisionTrim is a training-free framework that accelerates multimodal large language models by selecting dominant visual tokens and merging them with text-guided complementation, improving efficiency without performance loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.', 'score': 3, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '0da66181c47566c8', 'authors': ['Hanxun Yu', 'Wentong Li', 'Xuan Qu', 'Song Wang', 'Junbo Chen', 'Jianke Zhu'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Udeer.ai', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22674.jpg', 'data': {'categories': ['#multimodal', '#inference', '#video'], 'emoji': '‚ö°', 'ru': {'title': '–£—Å–∫–æ—Ä–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —É–º–Ω–æ–µ –ø—Ä–æ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤', 'desc': 'VisionTrim –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç—ë–º –≤—ã–±–æ—Ä–∞ –¥–æ–º–∏–Ω–∞–Ω—Ç–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ú–µ—Ç–æ–¥ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –¥–≤–∞ –º–æ–¥—É–ª—è: –º–æ–¥—É–ª—å –≤—ã–±–æ—Ä–∞ –¥–æ–º–∏–Ω–∞–Ω—Ç–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Å –≥–ª–æ–±–∞–ª—å–Ω–æ-–ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –∏ –º–æ–¥—É–ª—å —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ —Å —É—á—ë—Ç–æ–º —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –ü–æ–¥—Ö–æ–¥ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –≤—ã—Å–æ–∫–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –≤—ã—Å–æ–∫–æ—Ä–∞–∑—Ä–µ—à–∞—é—â–∏–º–∏—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –≤–∏–¥–µ–æ, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å —Ç–µ–∫—Å—Ç–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è MLLM –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'Accelerating MLLMs with VisionTrim: Efficiency Meets Performance', 'desc': 'VisionTrim is a novel framework designed to enhance the efficiency of multimodal large language models (MLLMs) without the need for training. It addresses the challenge of high computational costs caused by an abundance of visual tokens, especially in high-resolution and video contexts. The framework introduces two key modules: the Dominant Vision Token Selection (DVTS) module, which identifies and retains the most important visual tokens, and the Text-Guided Vision Complement (TGVC) module, which merges visual tokens with textual information for better context understanding. Through extensive testing, VisionTrim has shown to improve performance while reducing resource usage, making it suitable for real-world applications.'}, 'zh': {'title': 'VisionTrimÔºöÈ´òÊïàÂä†ÈÄüÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑËß£ÂÜ≥ÊñπÊ°à', 'desc': 'VisionTrimÊòØ‰∏Ä‰∏™Êó†ÈúÄËÆ≠ÁªÉÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Âä†ÈÄüÂ§öÊ®°ÊÄÅÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÔºåÈÄöËøáÈÄâÊã©‰∏ªË¶ÅÁöÑËßÜËßâÊ†áËÆ∞Âπ∂‰∏éÊñáÊú¨ÂºïÂØºÁöÑË°•ÂÖÖÁõ∏ÁªìÂêàÔºåÊèêÈ´òÊïàÁéáËÄå‰∏çÊçüÂ§±ÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÈ´òÂàÜËæ®ÁéáÂíåËßÜÈ¢ëÂú∫ÊôØ‰∏≠ËßÜËßâÊ†áËÆ∞ËøáÂ§öÂØºËá¥ÁöÑÈ´òËÆ°ÁÆóÊàêÊú¨ÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑDominant Vision Token SelectionÔºàDVTSÔºâÊ®°ÂùóËÉΩÂ§üÈÄöËøáÂÖ®Â±ÄÂíåÂ±ÄÈÉ®ËßÜËßí‰øùÁïôÈáçË¶ÅÁöÑËßÜËßâÊ†áËÆ∞ÔºåËÄåText-Guided Vision ComplementÔºàTGVCÔºâÊ®°ÂùóÂàôÈÄöËøáÊñáÊú¨Á∫øÁ¥¢ÂÆûÁé∞‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÊ†áËÆ∞ÂêàÂπ∂„ÄÇÈÄöËøáÂú®Â§öÁßçÂõæÂÉèÂíåËßÜÈ¢ëÂü∫ÂáÜ‰∏äÁöÑÂπøÊ≥õÂÆûÈ™åÔºåVisionTrimÂ±ïÁ§∫‰∫ÜÂÖ∂ÂçìË∂äÁöÑÊÄßËÉΩÔºåÊé®Âä®‰∫ÜMLLMÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠ÁöÑÈÉ®ÁΩ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02477', 'title': 'Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability', 'url': 'https://huggingface.co/papers/2602.02477', 'abstract': "An end-to-end reinforcement learning framework enhances large language models' reasoning capabilities by implementing divide-and-conquer strategies that outperform traditional chain-of-thought reasoning on challenging benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.", 'score': 2, 'issue_id': 888, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '104cb70146526d93', 'authors': ['Xiao Liang', 'Zhong-Zhi Li', 'Zhenghao Lin', 'Eric Hancheng Jiang', 'Hengyuan Zhang', 'Yelong Shen', 'Kai-Wei Chang', 'Ying Nian Wu', 'Yeyun Gong', 'Weizhu Chen'], 'affiliations': ['Microsoft', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2602.02477.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#reasoning', '#training', '#rl'], 'emoji': 'üéØ', 'ru': {'title': '–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –ø—É—Ç–µ–º –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ ¬´—Ä–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π¬ª. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –º–µ–∂–¥—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏–µ–º –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ ¬´—Ä–∞–∑–¥–µ–ª—è–π –∏ –≤–ª–∞—Å—Ç–≤—É–π¬ª –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–æ–¥–µ–ª–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π RL-—Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏, —Ä–µ—à–∞—Ç—å –∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –æ–±–µ–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π. –ù–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–µ –ø–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ 8,6% –ø–æ –º–µ—Ç—Ä–∏–∫–µ Pass@1 –∏ 6,3% –ø–æ Pass@32.'}, 'en': {'title': 'Unlocking Reasoning Power with Divide-and-Conquer Reinforcement Learning', 'desc': 'This paper presents a novel reinforcement learning framework that improves the reasoning abilities of large language models (LLMs) by using a divide-and-conquer (DAC) approach. Unlike traditional chain-of-thought (CoT) reasoning, which processes tasks sequentially, DAC breaks down complex problems into smaller, manageable subproblems, allowing for more efficient exploration of solutions. The authors identify a misalignment between standard post-training methods and DAC inference, which limits the effectiveness of LLMs in challenging scenarios. By integrating DAC reasoning into an end-to-end RL training process, the proposed framework significantly enhances model performance, achieving better results on competitive benchmarks compared to CoT reasoning.'}, 'zh': {'title': 'ÂàÜËÄåÊ≤ª‰πãÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ´ØÂà∞Á´ØÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂Ôºå‰ª•Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇÈÄöËøáÂÆûÊñΩÂàÜËÄåÊ≤ª‰πãÁöÑÁ≠ñÁï•ÔºåËØ•Ê°ÜÊû∂Âú®Â§çÊùÇ‰ªªÂä°‰∏äË∂ÖË∂ä‰∫Ü‰º†ÁªüÁöÑÈÄêÊ≠•Êé®ÁêÜÊñπÊ≥ï„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂàÜËÄåÊ≤ª‰πãÊé®ÁêÜËÉΩÂ§üÂ∞ÜÂ§çÊùÇÈóÆÈ¢òÂàÜËß£‰∏∫Â≠êÈóÆÈ¢òÔºå‰ªéËÄåÊõ¥ÊúâÊïàÂú∞Êé¢Á¥¢Ëß£ÂÜ≥ÊñπÊ°à„ÄÇÁªèËøáÊØîËæÉËÆ≠ÁªÉÔºåËØ•Ê°ÜÊû∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Êõ¥È´òÁöÑÊÄßËÉΩÂíåÊõ¥Âº∫ÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01997', 'title': 'On the Limits of Layer Pruning for Generative Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2602.01997', 'abstract': 'Layer pruning compresses large language models while maintaining classification performance but causes significant degradation in generative reasoning tasks, with limited recovery possible through supervised finetuning on self-generated responses.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.', 'score': 2, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'abc666c194fd4a45', 'authors': ['Safal Shrestha', 'Anubhav Shrestha', 'Aadim Nepal', 'Minwu Kim', 'Keith Ross'], 'affiliations': ['Department of Computer Science, New York University Abu Dhabi'], 'pdf_title_img': 'assets/pdf/title_img/2602.01997.jpg', 'data': {'categories': ['#inference', '#benchmark', '#reasoning', '#optimization', '#training'], 'emoji': '‚úÇÔ∏è', 'ru': {'title': '–ü—Ä–µ–¥–µ–ª—ã —Å–∂–∞—Ç–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –ø–æ—á–µ–º—É –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å—Ç—Ä–∞–¥–∞–µ—Ç –±–æ–ª—å—à–µ, —á–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –≤–ª–∏—è–Ω–∏–µ —É–¥–∞–ª–µ–Ω–∏—è —Å–ª–æ—ë–≤ –Ω–∞ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –≤—ã—è–≤–ª—è—è, —á—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ö–æ—Ä–æ—à–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –ø–æ—Å–ª–µ —Å–∂–∞—Ç–∏—è, –Ω–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ –¥–µ–≥—Ä–∞–¥–∏—Ä—É—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—é—Ç –ø–æ—Ç–µ—Ä—é –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∫–æ–¥–∞. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–æ 90% –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –Ω–∞ 20-30 –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–∞. –û–¥–Ω–∞–∫–æ –∞–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç–∞–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –≥—Ä–∞–Ω–∏—Ü—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Balancing Compression and Reasoning in Language Models', 'desc': 'This paper investigates the effects of layer pruning on large language models (LLMs), which is a technique used to reduce model size while trying to keep performance intact. The authors find that while classification tasks can maintain performance after pruning, generative reasoning tasks suffer significantly, especially those requiring multi-step reasoning. They propose a supervised finetuning method using self-generated responses to recover some performance, achieving notable improvements in classification tasks and moderate gains in generative tasks. However, the recovery for generative reasoning remains limited, highlighting the challenges of applying layer pruning effectively in these scenarios.'}, 'zh': {'title': 'Â±ÇÂâ™ÊûùÔºöÂéãÁº©‰∏éÁîüÊàêÊé®ÁêÜÁöÑÂπ≥Ë°°', 'desc': 'Â±ÇÂâ™ÊûùÊòØ‰∏ÄÁßçÂéãÁº©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰øùÊåÅÂàÜÁ±ªÊÄßËÉΩÁöÑÂêåÊó∂ÂáèÂ∞ëÊ®°ÂûãÁöÑÂ§ßÂ∞è„ÄÇÁÑ∂ËÄåÔºåËøôÁßçÊñπÊ≥ïÂú®ÁîüÊàêÊé®ÁêÜ‰ªªÂä°‰∏≠‰ºöÂØºËá¥ÊòæËëóÁöÑÊÄßËÉΩ‰∏ãÈôçÔºåÂ∞§ÂÖ∂ÊòØÂú®ÈúÄË¶ÅÂ§öÊ≠•Êé®ÁêÜÁöÑ‰ªªÂä°‰∏≠„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°ÈÄöËøáËá™ÁîüÊàêÂìçÂ∫îÁöÑÁõëÁù£ÂæÆË∞ÉÂèØ‰ª•ÈÉ®ÂàÜÊÅ¢Â§çÊÄßËÉΩÔºå‰ΩÜÂú®ÁîüÊàê‰ªªÂä°‰∏≠ÁöÑÊÅ¢Â§çËÉΩÂäõ‰ªçÁÑ∂ÊúâÈôê„ÄÇÊú¨ÊñáÁ≥ªÁªüËØÑ‰º∞‰∫ÜÂ±ÇÂâ™ÊûùÂØπÁîüÊàêÊé®ÁêÜÁöÑÂΩ±ÂìçÔºåÂπ∂Êèê‰æõ‰∫ÜÂú®ÂêéËÆ≠ÁªÉÊù°‰ª∂‰∏ãÊúâÊïàÂ∫îÁî®Ê∑±Â∫¶ÂáèÂ∞ëÁöÑÊåáÂØº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01983', 'title': 'Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning', 'url': 'https://huggingface.co/papers/2602.01983', 'abstract': "A training-free framework enables language model agents to automatically create and optimize tools during inference, improving their reasoning capabilities through self-evolution and memory consolidation.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%uparrow and +23.04%uparrow on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.", 'score': 2, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'a375d4829a4dd2fa', 'authors': ['Xintian Shen', 'Jiawei Chen', 'Lihao Zheng', 'Hao Ma', 'Tao Wei', 'Kun Zhan'], 'affiliations': ['Li Auto'], 'pdf_title_img': 'assets/pdf/title_img/2602.01983.jpg', 'data': {'categories': [], 'emoji': 'üõ†Ô∏è', 'ru': {'title': '–ê–≥–µ–Ω—Ç, —Å–æ–∑–¥–∞—é—â–∏–π —Å–≤–æ–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞Ê°ÜÊû∂UCT, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ –∏—Ö —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π. –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø—Ä—è–º–æ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –Ω–µ —Ç—Ä–µ–±—É—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ö–∞–Ω–∏–∑–º –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≤ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É, –ø–æ–∑–≤–æ–ª—è—è –∞–≥–µ–Ω—Ç—É –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –æ–ø—ã—Ç –∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞—Ç—å—Å—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 20-23% –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ –Ω–∞—É—á–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering Language Models: From Tool Users to Tool Creators', 'desc': 'This paper introduces a training-free framework called UCT that allows language model agents to create and optimize their own tools during inference. By leveraging their reasoning experiences, these agents can evolve and improve their problem-solving capabilities without needing additional training. The framework includes a memory consolidation mechanism that helps maintain a library of tools, ensuring they are reusable for future tasks. Experimental results show that this approach significantly enhances the performance of existing Tool-Integrated Reasoning models in various reasoning tasks.'}, 'zh': {'title': 'Êó†ËÆ≠ÁªÉÊ°ÜÊû∂Ôºö‰ªéÂ∑•ÂÖ∑‰ΩøÁî®ËÄÖÂà∞ÂàõÈÄ†ËÄÖÁöÑËΩ¨Âèò', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ËÆ≠ÁªÉÊ°ÜÊû∂UCTÔºå‰ΩøËØ≠Ë®ÄÊ®°Âûã‰ª£ÁêÜËÉΩÂ§üÂú®Êé®ÁêÜËøáÁ®ã‰∏≠Ëá™Âä®ÂàõÂª∫Âíå‰ºòÂåñÂ∑•ÂÖ∑Ôºå‰ªéËÄåÊèêÂçáÂÖ∂Êé®ÁêÜËÉΩÂäõ„ÄÇÁé∞ÊúâÁöÑÂ∑•ÂÖ∑ÈõÜÊàêÊé®ÁêÜÊ®°ÂûãÂú®ÈóÆÁ≠îËÉΩÂäõ‰∏äÊúâÊâÄÊâ©Â±ïÔºå‰ΩÜÂú®Èù¢ÂØπÂºÄÊîæÊÄßÈóÆÈ¢òÊó∂ÔºåÂõ∫ÂÆöÂ∑•ÂÖ∑ÂæÄÂæÄÊó†Ê≥ïÊª°Ë∂≥ÈúÄÊ±Ç„ÄÇUCTÊ°ÜÊû∂ÈÄöËøáÊèêÂèñÊé®ÁêÜÁªèÈ™åÔºåÂ∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫ÂèØÈáçÁî®ÁöÑËµÑ‰∫ßÔºå‰Ωø‰ª£ÁêÜ‰ªéÂ∑•ÂÖ∑‰ΩøÁî®ËÄÖËΩ¨Âèò‰∏∫Â∑•ÂÖ∑ÂàõÈÄ†ËÄÖ„ÄÇËØ•ÊñπÊ≥ïËøòÂºïÂÖ•‰∫ÜËÆ∞ÂøÜÂ∑©Âõ∫Êú∫Âà∂ÔºåÁ°Æ‰øùÂ∑•ÂÖ∑Â∫ìÁöÑÈ´òÈáçÁî®ÊÄßÔºå‰ªéËÄåÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠ÊåÅÁª≠ÊèêÈ´òÂ∑•ÂÖ∑Ë¥®Èáè„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23000', 'title': 'Mano: Restriking Manifold Optimization for LLM Training', 'url': 'https://huggingface.co/papers/2601.23000', 'abstract': "A novel optimizer called Mano is proposed that combines manifold optimization with momentum projection onto tangent spaces, achieving superior performance over AdamW and Muon while reducing memory and computational requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.", 'score': 2, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'bfee95549424c0a0', 'authors': ['Yufei Gu', 'Zeke Xie'], 'affiliations': ['1xLeaF Lab, The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.23000.jpg', 'data': {'categories': ['#optimization'], 'emoji': 'üöÄ', 'ru': {'title': '–ú–∞–Ω–∏—Ñ–æ–ª—å–¥–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –ø—Ä–æ–µ–∫—Ü–∏–µ–π –º–æ–º–µ–Ω—Ç–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Mano, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏—è—Ö —Å –ø—Ä–æ–µ–∫—Ü–∏–µ–π –º–æ–º–µ–Ω—Ç–∞ –Ω–∞ –∫–∞—Å–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç AdamW, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –∫—Ä–∏–≤–∏–∑–Ω—ã, –∏ Muon, –ø—Ä–∏–º–µ–Ω—è—é—â–µ–≥–æ –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é, Mano —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—Ä–∏–≤–∏–∑–Ω–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö LLaMA –∏ Qwen –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ Mano –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –æ–±—É—á–µ–Ω–∏—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤.'}, 'en': {'title': 'Mano: Bridging Efficiency and Performance in Optimizers', 'desc': 'The paper introduces a new optimizer named Mano, which integrates manifold optimization techniques with momentum projection onto tangent spaces. This approach enhances the training of large language models (LLMs) by addressing the limitations of existing optimizers like AdamW and Muon, which either overlook structural properties or sacrifice curvature information. Mano is designed to be more efficient, requiring less memory and computational resources while still achieving superior performance. Experimental results on models such as LLaMA and Qwen3 show that Mano significantly outperforms its predecessors, expanding the efficiency frontier in model training.'}, 'zh': {'title': 'ManoÔºöÈ´òÊïàÊµÅÂΩ¢‰ºòÂåñÁöÑÊñ∞ÈÄâÊã©', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞Âûã‰ºòÂåñÂô®ManoÔºåÂÆÉÁªìÂêà‰∫ÜÊµÅÂΩ¢‰ºòÂåñÂíåÂä®ÈáèÊäïÂΩ±Âà∞ÂàáÁ©∫Èó¥ÁöÑÊñπÊ≥ï„ÄÇManoÂú®ËÆ≠ÁªÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊó∂ÔºåË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑAdamWÂíåMuon‰ºòÂåñÂô®ÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜÂÜÖÂ≠òÂíåËÆ°ÁÆóÈúÄÊ±Ç„ÄÇÈÄöËøáÂàõÊñ∞ÊÄßÂú∞Â∞ÜÂä®ÈáèÊäïÂΩ±Âà∞Ê®°ÂûãÂèÇÊï∞ÁöÑÂàáÁ©∫Èó¥ÔºåÂπ∂Âú®ÊóãËΩ¨ÊñúÊµÅÂΩ¢‰∏äËøõË°åÁ∫¶ÊùüÔºåManoÊàêÂäüÂº•Ë°•‰∫ÜÊµÅÂΩ¢‰ºòÂåñ‰∏éÁé∞‰ª£‰ºòÂåñÂô®‰πãÈó¥ÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåManoÂú®LLaMAÂíåQwen3Ê®°Âûã‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁ©∫Èó¥ÂíåÊó∂Èó¥ÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00130', 'title': 'On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks', 'url': 'https://huggingface.co/papers/2602.00130', 'abstract': 'Effective dimension, an unsupervised geometric metric, strongly predicts neural network performance across different architectures and domains, showing bidirectional causality between representation geometry and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 (p < 10^(-10)) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, p < 10^(-9)), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show |r| > 0.90. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.', 'score': 2, 'issue_id': 884, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'}, 'hash': '45d9fbceb8b12d6a', 'authors': ['Sumit Yadav'], 'affiliations': ['Institute of Engineering, Pulchowk Campus, Tribhuvan University, Nepal'], 'pdf_title_img': 'assets/pdf/title_img/2602.00130.jpg', 'data': {'categories': ['#architecture', '#cv', '#benchmark', '#interpretability', '#training'], 'emoji': 'üìê', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è —Å–≤—è–∑—å –º–µ–∂–¥—É –≥–µ–æ–º–µ—Ç—Ä–∏–µ–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å ‚Äî —ç—Ç–æ –Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –º–µ—Ç—Ä–∏–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —Å–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 52 –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ ImageNet, –∞ —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –∑–∞–¥–∞—á–∏ NLP, –≥–¥–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ—Å—Ç–∞—ë—Ç—Å—è —Å–∏–ª—å–Ω—ã–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–º (r=0.69). –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç—å: –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —á–µ—Ä–µ–∑ —à—É–º –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ—Ç–µ—Ä–µ —Ç–æ—á–Ω–æ—Å—Ç–∏, –∞ —É–ª—É—á—à–µ–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ —á–µ—Ä–µ–∑ PCA —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Geometry Matters: Effective Dimension Predicts Neural Network Success', 'desc': "This paper explores how the geometry of representations in neural networks relates to their performance. It introduces 'effective dimension', a geometric metric that can predict the accuracy of various neural network architectures without needing labeled data. The study shows a strong correlation between effective dimension and accuracy across different datasets, including ImageNet and CIFAR-10, as well as in natural language processing tasks. Additionally, it establishes a two-way relationship where changes in representation geometry directly affect model accuracy, regardless of the type of noise introduced."}, 'zh': {'title': 'ÊúâÊïàÁª¥Â∫¶ÔºöÈ¢ÑÊµãÁ•ûÁªèÁΩëÁªúÊÄßËÉΩÁöÑÂÖ≥ÈîÆÊåáÊ†á', 'desc': 'Êú¨ÊñáÁ†îÁ©∂‰∫ÜË°®Á§∫Âá†‰Ωï‰∏éÁ•ûÁªèÁΩëÁªúÊÄßËÉΩ‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇÈÄöËøáÂàÜÊûê52‰∏™È¢ÑËÆ≠ÁªÉÁöÑImageNetÊ®°ÂûãÔºåÂèëÁé∞ÊúâÊïàÁª¥Â∫¶Ëøô‰∏ÄÊó†ÁõëÁù£Âá†‰ΩïÂ∫¶ÈáèËÉΩÂ§üÂº∫ÁÉàÈ¢ÑÊµãÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊúâÊïàÁª¥Â∫¶‰∏éÊ®°ÂûãÊÄßËÉΩ‰πãÈó¥Â≠òÂú®ÂèåÂêëÂõ†ÊûúÂÖ≥Á≥ªÔºåÂá†‰ΩïÁªìÊûÑÁöÑÈÄÄÂåñ‰ºöÂØºËá¥ÂáÜÁ°ÆÊÄß‰∏ãÈôçÔºåËÄåÈÄöËøá‰∏ªÊàêÂàÜÂàÜÊûêÔºàPCAÔºâÊîπÂñÑÂá†‰ΩïÁªìÊûÑÂàôËÉΩ‰øùÊåÅÂáÜÁ°ÆÊÄß„ÄÇËøô‰∏ÄÂèëÁé∞ÈÄÇÁî®‰∫éÂ§ö‰∏™È¢ÜÂüüÔºåÂåÖÊã¨ËÆ°ÁÆóÊú∫ËßÜËßâÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºåË°®ÊòéÊúâÊïàÁª¥Â∫¶Âú®Ê≤°ÊúâÊ†áÁ≠æÁöÑÊÉÖÂÜµ‰∏ãÊèê‰æõ‰∫ÜÂÖ≥‰∫éÁ•ûÁªèÁΩëÁªúÊÄßËÉΩÁöÑÈ¢ÑÊµãÂíåÂõ†Êûú‰ø°ÊÅØ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02354', 'title': 'Implicit neural representation of textures', 'url': 'https://huggingface.co/papers/2602.02354', 'abstract': 'Implicit neural representations operate continuously over UV coordinate space, demonstrating good image quality while balancing memory usage and rendering time, with applications in real-time rendering and downstream tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '8ed63e5840c69fcc', 'authors': ['Albert Kwok', 'Zheyuan Hu', 'Dounia Hammou'], 'affiliations': ['University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2602.02354.jpg', 'data': {'categories': [], 'emoji': 'üé®', 'ru': {'title': '–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—É—Ä—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—è–≤–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (INR) –¥–ª—è —Ç–µ–∫—Å—Ç—É—Ä, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ UV-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤–º–µ—Å—Ç–æ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –∏—Å—Å–ª–µ–¥—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–∫–∏—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∏ –ø—Ä–∏–µ–º–ª–µ–º–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞. –†–∞–±–æ—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–æ–≤ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º, –ø–∞–º—è—Ç—å—é –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ò—Å—Å–ª–µ–¥—É—é—Ç—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ —Ä–µ–∞–ª-—Ç–∞–π–º —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–µ –∏ —Å–º–µ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ mipmaps –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ INR.'}, 'en': {'title': 'Efficient Image Quality with Implicit Neural Representations', 'desc': 'This paper discusses implicit neural representations (INRs) that work continuously over UV coordinate space, which helps in achieving high-quality images while optimizing memory and rendering time. The authors propose a new texture INR design that enhances performance in real-time rendering applications. Through extensive experiments, they show that these INRs maintain excellent image quality while being efficient in memory usage and inference speed. The study also explores various applications of INRs, including mipmap fitting and INR-space generation, highlighting their versatility in downstream tasks.'}, 'zh': {'title': 'ÈöêÂºèÁ•ûÁªèË°®Á§∫ÔºöÈ´òÊïàÊ∏≤ÊüìÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÈöêÂºèÁ•ûÁªèË°®Á§∫ÔºàINRÔºâÂú®UVÂùêÊ†áÁ©∫Èó¥‰∏≠‰ª•ËøûÁª≠ÊñπÂºèÊìç‰ΩúÔºåÂ±ïÁé∞Âá∫ËâØÂ•ΩÁöÑÂõæÂÉèË¥®ÈáèÔºåÂêåÊó∂Âπ≥Ë°°‰∫ÜÂÜÖÂ≠ò‰ΩøÁî®ÂíåÊ∏≤ÊüìÊó∂Èó¥„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïËÆæËÆ°‰∏çÂêåÁöÑÁ•ûÁªèÁΩëÁªú‰Ωú‰∏∫Êñ∞ÁöÑÁ∫πÁêÜÈöêÂºèÁ•ûÁªèË°®Á§∫Ôºå‰ΩøÂÖ∂Âú®ËæìÂÖ•UVÂùêÊ†áÁ©∫Èó¥‰∏≠‰ª•ËøûÁª≠ÊñπÂºèÂ∑•‰Ωú„ÄÇÈÄöËøáÂÖ®Èù¢ÁöÑÂÆûÈ™åÔºåÊàë‰ª¨ËØÅÊòé‰∫ÜËøô‰∫õÈöêÂºèÁ•ûÁªèË°®Á§∫Âú®ÂõæÂÉèË¥®ÈáèÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂‰∏îÂú®ÂÜÖÂ≠ò‰ΩøÁî®ÂíåÊé®ÁêÜÊó∂Èó¥‰∏äÂÖ∑ÊúâÊòæËëóÁöÑ‰ºòÂäø„ÄÇÊàë‰ª¨ËøòÂàÜÊûê‰∫ÜËøô‰∫õÁõÆÊ†á‰πãÈó¥ÁöÑÂπ≥Ë°°ÔºåÂπ∂Á†îÁ©∂‰∫ÜÂú®ÂÆûÊó∂Ê∏≤ÊüìÂíå‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑÂêÑÁßçÁõ∏ÂÖ≥Â∫îÁî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02287', 'title': 'Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages', 'url': 'https://huggingface.co/papers/2602.02287', 'abstract': 'Controlled cross-lingual evaluation reveals instability in LLM assessment methods when targeting morphologically rich languages, indicating unreliable zero-shot judge transfer for discourse-level tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.   This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'fc36b450cfa4c3de', 'authors': ['Isaac Chung', 'Linda Freienthal'], 'affiliations': ['Zendesk'], 'pdf_title_img': 'assets/pdf/title_img/2602.02287.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#dataset', '#benchmark', '#low_resource', '#data', '#open_source'], 'emoji': '‚ö†Ô∏è', 'ru': {'title': '–ö–æ–≥–¥–∞ —Å—É–¥—å–∏ –æ—à–∏–±–∞—é—Ç—Å—è: –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ LLM –Ω–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö —è–∑—ã–∫–∞—Ö', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏, –ø–æ–∫–∞–∑—ã–≤–∞—è, —á—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —á–∞—Å—Ç–æ —Å–º–µ—à–∏–≤–∞—é—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å—é –∏–∑–º–µ—Ä–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –Ω–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–ª–æ–∂–Ω—ã—Ö —Ñ–∏–Ω–Ω–æ-—É–≥–æ—Ä—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö (—ç—Å—Ç–æ–Ω—Å–∫–æ–º, —Ñ–∏–Ω—Å–∫–æ–º –∏ –≤–µ–Ω–≥–µ—Ä—Å–∫–æ–º), –∏—Å–ø–æ–ª—å–∑—É—è –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∏–∞–ª–æ–≥–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∏ –æ—Ü–µ–Ω–∫–∏ LLM-as-a-judge. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ) –æ—Å—Ç–∞—é—Ç—Å—è —Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏, –Ω–æ –ø—Ä–∞–≥–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ (—Å–≤—è–∑–Ω–æ—Å—Ç—å, —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∏–Ω–≤–µ—Ä—Å–∏–∏ —Ä–∞–Ω–≥–æ–≤ –∏ –Ω—É–ª–µ–≤—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –∑–∞–∫–ª—é—á–∞—é—Ç, —á—Ç–æ –Ω—É–ª–µ–≤–æ–π –ø–µ—Ä–µ–Ω–æ—Å –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–∏—Å–∫—É—Ä—Å–∞ –Ω–µ–Ω–∞–¥—ë–∂–µ–Ω –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –±–æ–≥–∞—Ç—ã—Ö —è–∑—ã–∫–æ–≤ –∏ —Ç—Ä–µ–±—É–µ—Ç –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –Ω–∞ —è–∑—ã–∫–µ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ª—é–¥—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö.'}, 'en': {'title': 'Evaluating Language Models: Stability Matters!', 'desc': 'This paper investigates the reliability of evaluating large language models (LLMs) when applied to morphologically rich languages. It highlights that while some surface-level metrics remain stable across languages, pragmatic judgments like coherence and instruction-following show significant inconsistencies. The study uses controlled generation conditions to reveal that these instabilities are due to the evaluation methods rather than actual differences in model performance. The authors advocate for language-specific calibration to improve the reliability of zero-shot assessments in discourse-level tasks.'}, 'zh': {'title': 'ËØÑ‰º∞ÊñπÊ≥ïÂú®ÂΩ¢ÊÄÅ‰∏∞ÂØåËØ≠Ë®Ä‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄß', 'desc': 'Êú¨Á†îÁ©∂Êé¢ËÆ®‰∫ÜÂú®ÂΩ¢ÊÄÅ‰∏∞ÂØåËØ≠Ë®Ä‰∏≠ÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËØÑ‰º∞ÊñπÊ≥ïÁöÑ‰∏çÁ®≥ÂÆöÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáÊéßÂà∂ÁîüÊàêÊù°‰ª∂ÔºåÊØîËæÉÁà±Ê≤ôÂ∞º‰∫öËØ≠„ÄÅËä¨ÂÖ∞ËØ≠ÂíåÂåàÁâôÂà©ËØ≠ÁöÑÂÆ¢Êà∑ÊîØÊåÅÂØπËØùÔºåÂàÜÊûêËá™Âä®ËØÑ‰º∞ÊåáÊ†áÂíåLLMËØÑÂàÜÁöÑÁ®≥ÂÆöÊÄß„ÄÇÁªìÊûúÊòæÁ§∫ÔºåË°®Èù¢ÊåáÊ†áÂú®Ë∑®ËØ≠Ë®Ä‰∏≠‰øùÊåÅÁ®≥ÂÆöÔºå‰ΩÜÂú®ËØ≠Áî®Âà§Êñ≠ÊñπÈù¢Â≠òÂú®ÊéíÂêçÈ¢†ÂÄíÂíåÂá†‰πéÈõ∂Áõ∏ÂÖ≥ÊÄßÁöÑÈóÆÈ¢ò„ÄÇËøôË°®ÊòéÔºåÂú®Áõ∏ÂêåÁîüÊàêÊù°‰ª∂‰∏ãÔºåËØÑ‰º∞ÊñπÊ≥ïÁöÑ‰∏çÁ®≥ÂÆöÊÄßÂèçÊò†‰∫ÜËØÑ‰º∞ËÄÖËØÑÂàÜÂú®‰∏çÂêåËØ≠Ë®Ä‰∏≠ÁöÑË°®Áé∞Â∑ÆÂºÇÔºåËÄåÈùûÊ®°ÂûãÊú¨Ë∫´ÁöÑÁúüÂÆûÂ∑ÆÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01970', 'title': 'Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models', 'url': 'https://huggingface.co/papers/2602.01970', 'abstract': "Generalizable Predictive Prompt Selection (GPS) uses Bayesian inference with a lightweight generative model to efficiently select informative prompts for reinforcement learning-enhanced language models, improving training efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.", 'score': 1, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'd430ac88a2ce5693', 'authors': ['Yun Qu', 'Qi Wang', 'Yixiu Mao', 'Heming Zou', 'Yuhang Jiang', 'Weijie Liu', 'Clive Bai', 'Kai Yang', 'Yangkun Chen', 'Saiyong Yang', 'Xiangyang Ji'], 'affiliations': ['Department of Automation, Tsinghua University', 'LLM Department, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2602.01970.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#training', '#small_models'], 'emoji': 'üéØ', 'ru': {'title': '–£–º–Ω—ã–π –≤—ã–±–æ—Ä –ø–æ–¥—Å–∫–∞–∑–æ–∫ —á–µ—Ä–µ–∑ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Generalizable Predictive Prompt Selection (GPS), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –≤—ã–≤–æ–¥ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ—Ç–±–æ—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –õ–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ–¥—Å–∫–∞–∑–æ–∫, –ø–æ–∑–≤–æ–ª—è—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã —Å—Ä–µ–¥–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –ú–µ—Ç–æ–¥ –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø diversity, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–π –∫ –∏—Å—Ç–æ—Ä–∏–∏, –¥–ª—è –æ—Ç–±–æ—Ä–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –±–∞—Ç—á–µ–π –ø–æ–¥—Å–∫–∞–∑–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–ª—É—á—à–∞—é—Ç –æ–±—É—á–µ–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è, —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ.'}, 'en': {'title': 'Efficient Prompt Selection for Enhanced Learning', 'desc': 'This paper presents Generalizable Predictive Prompt Selection (GPS), a method that uses Bayesian inference to choose the most informative prompts for training reinforcement learning-enhanced language models. By employing a lightweight generative model, GPS reduces the computational costs associated with traditional prompt selection methods. The approach focuses on prioritizing prompts based on their difficulty and incorporates diversity to ensure a well-rounded selection. Experimental results show that GPS significantly enhances training efficiency and overall model performance compared to existing techniques.'}, 'zh': {'title': 'ÈÄöÁî®ÂèØÈ¢ÑÊµãÊèêÁ§∫ÈÄâÊã©ÔºöÊèêÂçáËÆ≠ÁªÉÊïàÁéáÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫ÈÄöÁî®ÂèØÈ¢ÑÊµãÊèêÁ§∫ÈÄâÊã©ÔºàGPSÔºâÁöÑÊñπÊ≥ïÔºåÂà©Áî®Ë¥ùÂè∂ÊñØÊé®Êñ≠ÂíåËΩªÈáèÁ∫ßÁîüÊàêÊ®°ÂûãÊù•È´òÊïàÈÄâÊã©‰ø°ÊÅØ‰∏∞ÂØåÁöÑÊèêÁ§∫Ôºå‰ªéËÄåÊèêÈ´òÂº∫ÂåñÂ≠¶‰π†Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãÁöÑËÆ≠ÁªÉÊïàÁéáÂíåÊÄßËÉΩ„ÄÇÂº∫ÂåñÂ≠¶‰π†ËôΩÁÑ∂ËÉΩÂ¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõÔºå‰ΩÜÈÄöÂ∏∏ÈúÄË¶ÅÈ´òÊòÇÁöÑËÆ°ÁÆóÊàêÊú¨„ÄÇÁé∞ÊúâÁöÑÊñπÊ≥ïË¶Å‰πà‰æùËµñ‰∫éÊòÇË¥µÁöÑÁ≤æÁ°ÆËØÑ‰º∞ÔºåË¶Å‰πàÊûÑÂª∫Áº∫‰πèË∑®ÊèêÁ§∫Ê≥õÂåñËÉΩÂäõÁöÑÊèêÁ§∫ÁâπÂÆöÈ¢ÑÊµãÊ®°Âûã„ÄÇGPSÈÄöËøáÂØπÊèêÁ§∫ÈöæÂ∫¶ËøõË°åË¥ùÂè∂ÊñØÊé®Êñ≠ÔºåÁªìÂêà‰∏≠Á≠âÈöæÂ∫¶‰ºòÂÖàÁ∫ßÂíåÂéÜÂè≤ÈîöÂÆöÂ§öÊ†∑ÊÄßÔºåÊòæËëóÊèêÂçá‰∫ÜËÆ≠ÁªÉÊïàÁéáÂíåÊúÄÁªàÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00521', 'title': 'Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory', 'url': 'https://huggingface.co/papers/2602.00521', 'abstract': 'A two-phase diagnostic framework based on Item Response Theory and Graded Response Model is introduced to assess the reliability of LLM-as-a-Judge by examining intrinsic consistency and human alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': '065a43f301fa28c9', 'authors': ['Junhyuk Choi', 'Sohhyung Park', 'Chanhee Cho', 'Hyeonchu Park', 'Bugeun Kim'], 'affiliations': ['Department of Artificial Intelligence, Chung-Ang University, Seoul, Republic of Korea', 'Department of Industrial Engineering'], 'pdf_title_img': 'assets/pdf/title_img/2602.00521.jpg', 'data': {'categories': [], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ LLM-—Å—É–¥–µ–π —á–µ—Ä–µ–∑ —Ç–µ–æ—Ä–∏—é –∏–∑–º–µ—Ä–µ–Ω–∏–π –∏ –∞–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–æ—Ä–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –∑–∞–¥–∞–Ω–∏—è –∏ –º–æ–¥–µ–ª–∏ –≥—Ä–∞–¥—É–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ LLM-as-a-Judge –ø—É—Ç—ë–º –∞–Ω–∞–ª–∏–∑–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –æ—Ü–µ–Ω–∫–∞–º. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ –Ω–∞–±–ª—é–¥–∞–µ–º—ã—Ö –≤—ã—Ö–æ–¥–æ–≤, –Ω–µ —É—á–∏—Ç—ã–≤–∞—è, —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –∫–∞–∫ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏–∑–º–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏: —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø—Ä–∏ –≤–∞—Ä–∏–∞—Ü–∏–∏ –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ–æ—Ä–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –∑–∞–¥–∞–Ω–∏—è –¥–∞—ë—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ–Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ —Å—É–¥–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM.'}, 'en': {'title': 'Assessing LLM Reliability: A Two-Phase Diagnostic Approach', 'desc': "This paper presents a two-phase diagnostic framework that uses Item Response Theory (IRT) and the Graded Response Model (GRM) to evaluate the reliability of large language models (LLMs) acting as judges. The framework assesses two key aspects: intrinsic consistency, which measures how stable the LLM's judgments are when faced with different prompts, and human alignment, which checks how closely the LLM's evaluations match those of human judges. By applying this framework, the authors demonstrate that IRT-GRM can provide clear insights into the reliability of LLMs, helping to identify issues that may affect their judgment quality. This approach aims to enhance the understanding and trustworthiness of LLMs in automated evaluation tasks."}, 'zh': {'title': 'ËØÑ‰º∞LLMËØÑÂà§ËÄÖÂèØÈù†ÊÄßÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÈ°πÁõÆÂèçÂ∫îÁêÜËÆ∫ÔºàIRTÔºâÂíåÂàÜÁ∫ßÂèçÂ∫îÊ®°ÂûãÔºàGRMÔºâÁöÑ‰∏§Èò∂ÊÆµËØäÊñ≠Ê°ÜÊû∂ÔºåÁî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑÂèØÈù†ÊÄß„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂàÜÊûêÂÜÖÂú®‰∏ÄËá¥ÊÄßÂíå‰∫∫Á±ªÂØπÈΩê‰∏§‰∏™Áª¥Â∫¶ÔºåÊù•Ê£ÄÈ™åLLMÁöÑÊµãÈáèÁ®≥ÂÆöÊÄßÂíå‰∏é‰∫∫Á±ªËØÑ‰º∞ÁöÑ‰∏ÄËá¥ÊÄß„ÄÇÊàë‰ª¨ÈÄöËøáÂÆûËØÅÁ†îÁ©∂‰∏çÂêåÁöÑLLMËØÑÂà§ËÄÖÔºåÂ±ïÁ§∫‰∫ÜIRT-GRMÊñπÊ≥ïËÉΩÂ§üÁ≥ªÁªüÂú∞ËØäÊñ≠Âà§Êñ≠ÁöÑÂèØÈù†ÊÄß„ÄÇÊ≠§Ê°ÜÊû∂‰∏∫È™åËØÅLLM‰Ωú‰∏∫ËØÑÂà§ËÄÖÁöÑÂèØÈù†ÊÄßÊèê‰æõ‰∫ÜÂÆûÁî®ÊåáÂØºÔºåÂπ∂Â∏ÆÂä©ËØÜÂà´ÊΩúÂú®ÁöÑ‰∏çÂèØÈù†ÂéüÂõ†„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22801', 'title': 'Clipping-Free Policy Optimization for Large Language Models', 'url': 'https://huggingface.co/papers/2601.22801', 'abstract': 'Clipping-Free Policy Optimization replaces heuristic clipping with convex quadratic penalty to stabilize reinforcement learning training for large language models without performance loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '0c3a2fe11cf35034', 'authors': ['√ñmer Veysel √áaƒüatan', 'Barƒ±≈ü Akg√ºn', 'G√∂zde G√ºl ≈ûahin', 'Xuandong Zhao'], 'affiliations': ['KUIS AI Center, Koc University, Istanbul, Turkiye', 'Koc University', 'University of California, Berkeley, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2601.22801.jpg', 'data': {'categories': ['#reasoning', '#alignment', '#optimization', '#rl', '#rlhf', '#training'], 'emoji': 'üéØ', 'ru': {'title': '–ë–µ–∑ –æ—Ç—Å–µ—á–µ–Ω–∏–π ‚Äî –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –≤ LLM', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç Clipping-Free Policy Optimization (CFPO) ‚Äî –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–º–µ–Ω—è–µ—Ç —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ç—Å–µ—á–µ–Ω–∏–µ (clipping) –Ω–∞ –≤—ã–ø—É–∫–ª—ã–π –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–π —à—Ç—Ä–∞—Ñ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –ø–æ–ª–Ω–æ–π –≤–∞—Ä–∏–∞—Ü–∏–∏. –≠—Ç–æ —Ä–µ—à–µ–Ω–∏–µ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤, —Ç–∞–∫–∏–µ –∫–∞–∫ –Ω—É–ª–µ–≤—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥–æ–π –∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. CFPO —Å–æ–∑–¥–∞–µ—Ç –≤–µ–∑–¥–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—É—é —Ü–µ–ª–µ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –±–µ–∑ –∂–µ—Å—Ç–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –ú–µ—Ç–æ–¥ —Ç—Ä–µ–±—É–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –∏ –Ω–µ –≤–≤–æ–¥–∏—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø–æ–∫–∞–∑—ã–≤–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫ –∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Stabilizing Reinforcement Learning with Clipping-Free Policy Optimization', 'desc': 'This paper introduces Clipping-Free Policy Optimization (CFPO), a new method for stabilizing reinforcement learning in large language models. Instead of using clipping, which can cause optimization problems, CFPO employs a convex quadratic penalty to create a smooth and differentiable objective function. This approach allows for stable policy updates without the issues associated with clipping, such as zero-gradient regions and training instability. The results show that CFPO performs comparably to traditional methods while improving training stability and reducing verbosity exploitation in alignment tasks.'}, 'zh': {'title': 'Êó†Ââ™ÂàáÁ≠ñÁï•‰ºòÂåñÔºöÁ®≥ÂÆöÂº∫ÂåñÂ≠¶‰π†ÁöÑÊñ∞ÈÄâÊã©', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†‰ºòÂåñÊñπÊ≥ïÔºåÁß∞‰∏∫Êó†Ââ™ÂàáÁ≠ñÁï•‰ºòÂåñÔºàCFPOÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Â§ßËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉ‰∏≠ÁöÑ‰∏çÁ®≥ÂÆöÊÄßÈóÆÈ¢ò„ÄÇCFPOÁî®Âá∏‰∫åÊ¨°ÊÉ©ÁΩöÊõø‰ª£‰∫Ü‰º†ÁªüÁöÑÂâ™ÂàáÊú∫Âà∂Ôºå‰ªéËÄåÈÅøÂÖç‰∫ÜÈõ∂Ê¢ØÂ∫¶Âå∫ÂüüÂíåÂ•ñÂä±ÈªëÂÆ¢Á≠â‰ºòÂåñÈóÆÈ¢ò„ÄÇÈÄöËøáÂú®Êé®ÁêÜÂíåÂØπÈΩêËÆæÁΩÆ‰∏≠ÁöÑËØÑ‰º∞ÔºåCFPOÂú®‰∏ãÊ∏∏Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫‰∏éÂâ™ÂàáÊñπÊ≥ïÁõ∏ÂΩìÁöÑÊïàÊûúÔºåÂêåÊó∂Êâ©Â±ï‰∫ÜÁ®≥ÂÆöËÆ≠ÁªÉÁöÑËåÉÂõ¥„ÄÇCFPOÂè™ÈúÄ‰∏ÄË°å‰ª£Á†ÅÊõ¥ÊîπÔºåÊó†ÈúÄÈ¢ùÂ§ñÁöÑË∂ÖÂèÇÊï∞ÔºåÊòæÁ§∫Âá∫ÂÖ∂‰Ωú‰∏∫Ââ™ÂàáÊñπÊ≥ïÊõø‰ª£ÂìÅÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00192', 'title': 'AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange', 'url': 'https://huggingface.co/papers/2602.00192', 'abstract': 'VAE-based inpainting creates spectral shifts that fool detection systems, which can be mitigated through Inpainting Exchange to improve content-aware detection performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\\% to 55\\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'f374613cb7823e7a', 'authors': ['Elif Nebioglu', 'Emirhan Bilgi√ß', 'Adrian Popescu'], 'affiliations': ['Independent Researcher', 'Institut Polytechnique de Paris, U2IS', 'Universite Paris-Saclay, CEA, LIST', 'Universite Sorbonne, Pierre et Marie Curie, ISIR'], 'pdf_title_img': 'assets/pdf/title_img/2602.00192.jpg', 'data': {'categories': [], 'emoji': 'üé®', 'ru': {'title': '–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –ª–æ–≤—É—à–∫–∏ VAE-–∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥–∞: –æ—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∫ –∫–æ–Ω—Ç–µ–Ω—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é VAE-based –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥–∞. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã, –∞ –Ω–µ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –∏—Ö –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å. –û–Ω–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ VAE-—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≤—ã–∑—ã–≤–∞–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Å–¥–≤–∏–≥ –ø–æ –≤—Å–µ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –≤–∫–ª—é—á–∞—è –Ω–µ–æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Inpainting Exchange –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π –≤–Ω–µ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∑–æ–Ω—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã —Ä–∞–±–æ—Ç–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ö—É–∂–µ –ø—Ä–∏ –∏–∑–æ–ª—è—Ü–∏–∏ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ –∫–æ–Ω—Ç–µ–Ω—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥–∞.'}, 'en': {'title': 'Enhancing Detection with Inpainting Exchange', 'desc': 'This paper discusses the challenges posed by Variational Autoencoder (VAE)-based inpainting techniques, which can create subtle spectral shifts in images that mislead detection systems. The authors introduce a method called Inpainting Exchange (INP-X) that restores original pixels outside the edited areas while keeping the newly synthesized content intact. They demonstrate that current detection systems often fail because they focus on global artifacts rather than local details, leading to significant drops in detection accuracy when using INP-X. The study emphasizes the importance of developing content-aware detection methods to improve the reliability of detecting inpainted images.'}, 'zh': {'title': 'ÊèêÂçáÂÜÖÂÆπÊÑüÁü•Ê£ÄÊµãÁöÑÂÖ≥ÈîÆÂú®‰∫éInpainting Exchange', 'desc': 'Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂü∫‰∫éÂèòÂàÜËá™ÁºñÁ†ÅÂô®ÔºàVAEÔºâÁöÑÂõæÂÉè‰øÆÂ§çÊäÄÊúØÂ¶Ç‰ΩïÂØºËá¥È¢ëË∞±ÂÅèÁßªÔºå‰ªéËÄåÂΩ±ÂìçÊ£ÄÊµãÁ≥ªÁªüÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Inpainting ExchangeÔºàINP-XÔºâÁöÑÊñ∞ÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰øÆÂ§çÂå∫ÂüüÂ§ñÊÅ¢Â§çÂéüÂßãÂÉèÁ¥†ÔºåÂêåÊó∂‰øùÁïôÂêàÊàêÂÜÖÂÆπ„ÄÇÈÄöËøáÂàõÂª∫‰∏Ä‰∏™ÂåÖÂê´ÁúüÂÆû„ÄÅ‰øÆÂ§çÂíå‰∫§Êç¢ÂõæÂÉèÁöÑ90KÊµãËØïÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨ÂèëÁé∞Áé∞ÊúâÁöÑÊ£ÄÊµãÂô®Âú®ËøôÁßçÂπ≤È¢Ñ‰∏ãÂáÜÁ°ÆÁéáÊòæËëó‰∏ãÈôç„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Âº∫Ë∞É‰∫ÜÂÜÖÂÆπÊÑüÁü•Ê£ÄÊµãÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂Ë°®ÊòéÂú®Êàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÂèØ‰ª•ÊèêÈ´òÊ£ÄÊµãÁöÑÊ≥õÂåñËÉΩÂäõÂíåÂÆö‰ΩçÁ≤æÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22599', 'title': 'A Semantically Consistent Dataset for Data-Efficient Query-Based Universal Sound Separation', 'url': 'https://huggingface.co/papers/2601.22599', 'abstract': 'Automated pipeline for sound separation using high-purity single-event segments from in-the-wild datasets achieves competitive performance with significantly reduced data requirements.  \t\t\t\t\tAI-generated summary \t\t\t\t Query-based universal sound separation is fundamental to intelligent auditory systems, aiming to isolate specific sources from mixtures. Despite recent advances, existing methods continue to suffer from residual interference in complex acoustic scenes. This performance limitation stems largely from a data bottleneck: in-the-wild datasets contain weak labels and severe co-occurrence of events. These flaws induce models to learn spurious correlations between background noise and target categories instead of robust acoustic features. To address this, we propose an automated pipeline that eliminates co-occurrence of events by mining high-purity single-event segments from in-the-wild datasets via a semantically consistent synthesis protocol. Utilizing this pipeline, we constructed Hive, a high-quality synthetic dataset comprising 2.4k hours of raw audio. Experimental results demonstrate that, compared with the state-of-the-art model SAM-Audio which was trained on a huge dataset sim500 times larger than Hive, certain open-source models trained on Hive achieve competitive separation accuracy and perceptual quality. Moreover, these models exhibited remarkable zero-shot generalization on out-of-distribution evaluation benchmarks. These findings highlight that prioritizing purity of supervised signals enables significant data efficiency, offering a new paradigm for training robust auditory foundation models with reduced computational costs. Code and dataset are available at https://shandaai.github.io/Hive.', 'score': 1, 'issue_id': 886, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '8664abd1b5a52f44', 'authors': ['Kai Li', 'Jintao Cheng', 'Chang Zeng', 'Zijun Yan', 'Helin Wang', 'Zixiong Su', 'Bo Zheng', 'Xiaolin Hu'], 'affiliations': ['Chinese Institute for Brain Research (CIBR), Beijing, China', 'Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University, Beijing, China', 'Johns Hopkins University', 'Shanda AI Research Tokyo', 'Tsinghua Laboratory of Brain and Intelligence (THBI), IDG/McGovern Institute for Brain Research, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2601.22599.jpg', 'data': {'categories': ['#data', '#dataset', '#synthetic', '#open_source', '#audio'], 'emoji': 'üéµ', 'ru': {'title': '–ß–∏—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–º–µ—Å—Ç–æ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–≤—É–∫–∞ —á–µ—Ä–µ–∑ –≤—ã—Å–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ–∑', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∑–≤—É–∫–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å—Ç—ã–µ –º–æ–Ω–æ—Ñ–æ–Ω–∏—á–µ—Å–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞. –ê–≤—Ç–æ—Ä—ã –ø–æ—Å—Ç—Ä–æ–∏–ª–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç Hive –æ–±—ä—ë–º–æ–º 2.4 —Ç—ã—Å—è—á–∏ —á–∞—Å–æ–≤ –∞—É–¥–∏–æ, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π –∏ —Å–ª–∞–±—ã—Ö —Ä–∞–∑–º–µ—Ç–æ–∫ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ Hive, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–≤—É–∫–∞, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–π —Å –º–æ–¥–µ–ª—å—é SAM-Audio, –æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ –≤ 500 —Ä–∞–∑ –±–æ–ª—å—à–µ–º —Ä–∞–∑–º–µ—Ä–æ–º. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è —á–∏—Å—Ç–æ—Ç—ã —Å–∏–≥–Ω–∞–ª–æ–≤ –æ–±—É—á–µ–Ω–∏—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —É—Å—Ç–æ–π—á–∏–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–≤—É–∫–∞ —Å –º–µ–Ω—å—à–∏–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏.'}, 'en': {'title': 'Purity Over Quantity: Revolutionizing Sound Separation with Hive', 'desc': 'This paper presents an automated pipeline for sound separation that focuses on using high-purity single-event segments from in-the-wild datasets. The authors address the issue of residual interference in complex acoustic scenes caused by weak labels and event co-occurrence in existing datasets. By mining high-quality audio segments, they created a synthetic dataset called Hive, which is significantly smaller yet competitive in performance compared to larger datasets. The results show that models trained on Hive achieve strong separation accuracy and generalization, demonstrating the importance of data purity in training effective auditory models.'}, 'zh': {'title': 'È´òÁ∫ØÂ∫¶Êï∞ÊçÆÈ©±Âä®ÁöÑÂ£∞Èü≥ÂàÜÁ¶ªÊñ∞ËåÉÂºè', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçËá™Âä®ÂåñÁöÑÂ£∞Èü≥ÂàÜÁ¶ªÁÆ°ÈÅìÔºåÂà©Áî®È´òÁ∫ØÂ∫¶ÁöÑÂçï‰∫ã‰ª∂ÁâáÊÆµÔºå‰ªéËá™ÁÑ∂ÁéØÂ¢ÉÊï∞ÊçÆÈõÜ‰∏≠ÊèêÂèñÂ£∞Èü≥„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊäÄÊúØÂú®Â§çÊùÇÂ£∞Â≠¶Âú∫ÊôØ‰∏≠Â≠òÂú®ÁöÑÊÆã‰ΩôÂπ≤Êâ∞ÈóÆÈ¢òÔºå‰∏ªË¶ÅÊòØÁî±‰∫éÊï∞ÊçÆÁì∂È¢àÂØºËá¥ÁöÑ„ÄÇÈÄöËøáËØ≠‰πâ‰∏ÄËá¥ÁöÑÂêàÊàêÂçèËÆÆÔºåÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Âêç‰∏∫HiveÁöÑÈ´òË¥®ÈáèÂêàÊàêÊï∞ÊçÆÈõÜÔºåÂåÖÂê´2400Â∞èÊó∂ÁöÑÂéüÂßãÈü≥È¢ë„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰ΩøÁî®HiveËÆ≠ÁªÉÁöÑÂºÄÊ∫êÊ®°ÂûãÂú®ÂàÜÁ¶ªÂáÜÁ°ÆÊÄßÂíåÊÑüÁü•Ë¥®Èáè‰∏ä‰∏éÂ§ßÂûãÊï∞ÊçÆÈõÜËÆ≠ÁªÉÁöÑÊúÄÂÖàËøõÊ®°ÂûãÁõ∏ÂΩìÔºå‰∏îÂú®Èõ∂Ê†∑Êú¨Ê≥õÂåñËÉΩÂäõ‰∏äË°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22296', 'title': 'ParalESN: Enabling parallel information processing in Reservoir Computing', 'url': 'https://huggingface.co/papers/2601.22296', 'abstract': 'Parallel Echo State Network (ParalESN) addresses reservoir computing limitations by enabling parallel temporal processing through diagonal linear recurrence, maintaining theoretical guarantees while achieving significant computational efficiency gains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reservoir Computing (RC) has established itself as an efficient paradigm for temporal processing. However, its scalability remains severely constrained by (i) the necessity of processing temporal data sequentially and (ii) the prohibitive memory footprint of high-dimensional reservoirs. In this work, we revisit RC through the lens of structured operators and state space modeling to address these limitations, introducing Parallel Echo State Network (ParalESN). ParalESN enables the construction of high-dimensional and efficient reservoirs based on diagonal linear recurrence in the complex space, enabling parallel processing of temporal data. We provide a theoretical analysis demonstrating that ParalESN preserves the Echo State Property and the universality guarantees of traditional Echo State Networks while admitting an equivalent representation of arbitrary linear reservoirs in the complex diagonal form. Empirically, ParalESN matches the predictive accuracy of traditional RC on time series benchmarks, while delivering substantial computational savings. On 1-D pixel-level classification tasks, ParalESN achieves competitive accuracy with fully trainable neural networks while reducing computational costs and energy consumption by orders of magnitude. Overall, ParalESN offers a promising, scalable, and principled pathway for integrating RC within the deep learning landscape.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '6eb2c7409d1f699c', 'authors': ['Matteo Pinna', 'Giacomo Lagomarsini', 'Andrea Ceni', 'Claudio Gallicchio'], 'affiliations': ['Department of Computer Science, University of Pisa'], 'pdf_title_img': 'assets/pdf/title_img/2601.22296.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#training'], 'emoji': '‚ö°', 'ru': {'title': '–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–∏: —ç–∫–æ–Ω–æ–º–∏—è –∑–∞—Ç—Ä–∞—Ç –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ Parallel Echo State Network (ParalESN) ‚Äî –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ reservoir computing, —Ä–µ—à–∞—é—â–∏–π –ø—Ä–æ–±–ª–µ–º—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–µ –ª–∏–Ω–µ–π–Ω—ã–µ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ ParalESN —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤–æ–π—Å—Ç–≤–æ Echo State Property –∏ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö Echo State Networks. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∏ –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Å–æ–∫—Ä–∞—â–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∏ —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Ä—è–¥–∫–æ–≤.'}, 'en': {'title': 'Parallel Processing for Efficient Temporal Data Handling', 'desc': 'The Parallel Echo State Network (ParalESN) improves reservoir computing by allowing parallel processing of temporal data, which overcomes the limitations of traditional methods that require sequential data handling. It utilizes diagonal linear recurrence in complex space to create efficient, high-dimensional reservoirs while maintaining the essential properties of Echo State Networks. Theoretical analysis confirms that ParalESN retains the Echo State Property and universality guarantees, making it a robust alternative to conventional approaches. Empirical results show that ParalESN achieves similar predictive accuracy to traditional reservoir computing while significantly reducing computational costs and energy usage, making it a scalable solution for deep learning applications.'}, 'zh': {'title': 'Âπ∂Ë°åÂõûÂ£∞Áä∂ÊÄÅÁΩëÁªúÔºöÈ´òÊïàÁöÑÊó∂Èó¥Êï∞ÊçÆÂ§ÑÁêÜÊñ∞Ë∑ØÂæÑ', 'desc': 'Âπ∂Ë°åÂõûÂ£∞Áä∂ÊÄÅÁΩëÁªúÔºàParalESNÔºâÈÄöËøáÂØπËßíÁ∫øÁ∫øÊÄßÈÄíÂΩíÂÆûÁé∞‰∫ÜÂπ∂Ë°åÊó∂Èó¥Â§ÑÁêÜÔºåÂÖãÊúç‰∫Ü‰º†ÁªüÊ∞¥Â∫ìËÆ°ÁÆóÁöÑÂ±ÄÈôêÊÄß„ÄÇËØ•ÊñπÊ≥ïÂú®Â§çÊùÇÁ©∫Èó¥‰∏≠ÊûÑÂª∫È´òÁª¥È´òÊïàÁöÑÊ∞¥Â∫ìÔºåÂÖÅËÆ∏ÂØπÊó∂Èó¥Êï∞ÊçÆËøõË°åÂπ∂Ë°åÂ§ÑÁêÜ„ÄÇÁêÜËÆ∫ÂàÜÊûêË°®ÊòéÔºåParalESN‰øùÊåÅ‰∫ÜÂõûÂ£∞Áä∂ÊÄÅÂ±ûÊÄßÂíå‰º†ÁªüÂõûÂ£∞Áä∂ÊÄÅÁΩëÁªúÁöÑÈÄöÁî®ÊÄß‰øùËØÅ„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåParalESNÂú®Êó∂Èó¥Â∫èÂàóÈ¢ÑÊµã‰∏≠‰∏é‰º†ÁªüÊ∞¥Â∫ìËÆ°ÁÆóÁöÑÂáÜÁ°ÆÊÄßÁõ∏ÂΩìÔºåÂêåÊó∂Âú®ËÆ°ÁÆóÊïàÁéáÂíåËÉΩËÄó‰∏äÂ§ßÂπÖÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21968', 'title': 'OVD: On-policy Verbal Distillation', 'url': 'https://huggingface.co/papers/2601.21968', 'abstract': "On-policy Verbal Distillation (OVD) enables efficient knowledge transfer from teacher to student models by replacing token-level probability matching with trajectory matching using discrete verbal scores, reducing memory consumption and enabling free exploration without token alignment constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Knowledge distillation offers a promising path to transfer reasoning capabilities from large teacher models to efficient student models; however, existing token-level on-policy distillation methods require token-level alignment between the student and teacher models, which restricts the student model's exploration ability, prevent effective use of interactive environment feedback, and suffer from severe memory bottlenecks in reinforcement learning. We introduce On-policy Verbal Distillation (OVD), a memory-efficient framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0--9) from teacher models. OVD dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment, allowing the student model to freely explore the output space. Extensive experiments on Web question answering and mathematical reasoning tasks show that OVD substantially outperforms existing methods, delivering up to +12.9% absolute improvement in average EM on Web Q&A tasks and a up to +25.7% gain on math benchmarks (when trained with only one random samples), while also exhibiting superior training efficiency. Our project page is available at https://OVD.github.io", 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'e85fcc0a988521e4', 'authors': ['Jing Xiong', 'Hui Shen', 'Shansan Gong', 'Yuxin Cheng', 'Jianghan Shen', 'Chaofan Tao', 'Haochen Tan', 'Haoli Bai', 'Lifeng Shang', 'Ngai Wong'], 'affiliations': ['Huawei Technologies, China', 'Nanjing University, Nanjing, China', 'The University of Hong Kong, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2601.21968.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#transfer_learning', '#optimization', '#rl', '#training'], 'emoji': 'üß†', 'ru': {'title': '–í–µ—Ä–±–∞–ª—å–Ω–∞—è –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è: –ø–µ—Ä–µ–¥–∞—á–∞ –∑–Ω–∞–Ω–∏–π –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Ç–æ–∫–µ–Ω-—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ On-policy Verbal Distillation (OVD), –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–¥–∞—ë—Ç –∑–Ω–∞–Ω–∏—è –æ—Ç –±–æ–ª—å—à–∏—Ö —É—á–∏—Ç–µ–ª—å—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–º —Å—Ç—É–¥–µ–Ω—Ç—Å–∫–∏–º –º–æ–¥–µ–ª—è–º, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –≤–µ—Ä–±–∞–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤–º–µ—Å—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤. –ö–ª—é—á–µ–≤–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ OVD –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º —Å–Ω–∏–∂–µ–Ω–∏–∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–∏ —Å—Ç—É–¥–µ–Ω—Ç—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –æ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Ç–æ–∫–µ–Ω-—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π —Å–≤–æ–±–æ–¥–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤. –ú–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–∞—Ç—á–∏–Ω–≥ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ—Ü–µ–Ω–æ–∫ –æ—Ç 0 –¥–æ 9, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —É—á–∏—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ–ª—å—é, –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é –∏–∑ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ä–µ–¥—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–æ +12.9% –∏ +25.7% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.'}, 'en': {'title': 'Efficient Knowledge Transfer with On-policy Verbal Distillation', 'desc': 'On-policy Verbal Distillation (OVD) is a new method for transferring knowledge from large teacher models to smaller student models in machine learning. Instead of matching probabilities at the token level, OVD uses trajectory matching with discrete verbal scores, which helps reduce memory usage. This approach allows student models to explore freely without being constrained by token alignment, making it more effective in interactive environments. Experiments show that OVD significantly improves performance on tasks like web question answering and mathematical reasoning, achieving notable gains in accuracy and training efficiency.'}, 'zh': {'title': 'Âú®Á∫øÂè£Â§¥Ëí∏È¶èÔºöÈ´òÊïàÁü•ËØÜËΩ¨ÁßªÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁü•ËØÜËí∏È¶èÊñπÊ≥ïÔºåÁß∞‰∏∫Âú®Á∫øÂè£Â§¥Ëí∏È¶èÔºàOVDÔºâ„ÄÇOVDÈÄöËøá‰ΩøÁî®Á¶ªÊï£ÁöÑÂè£Â§¥ËØÑÂàÜÊõø‰ª£‰º†ÁªüÁöÑ‰ª§ÁâåÁ∫ßÊ¶ÇÁéáÂåπÈÖçÔºå‰ªéËÄåÂÆûÁé∞‰∫ÜÊïôÂ∏àÊ®°Âûã‰∏éÂ≠¶ÁîüÊ®°Âûã‰πãÈó¥ÁöÑÁü•ËØÜËΩ¨Áßª„ÄÇËØ•ÊñπÊ≥ïÊòæËëóÂáèÂ∞ë‰∫ÜÂÜÖÂ≠òÊ∂àËÄóÔºåÂπ∂ÂÖÅËÆ∏Â≠¶ÁîüÊ®°ÂûãÂú®Ê≤°Êúâ‰ª§ÁâåÂØπÈΩêÈôêÂà∂ÁöÑÊÉÖÂÜµ‰∏ãËá™Áî±Êé¢Á¥¢ËæìÂá∫Á©∫Èó¥„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåOVDÂú®ÁΩëÁªúÈóÆÁ≠îÂíåÊï∞Â≠¶Êé®ÁêÜ‰ªªÂä°‰∏äË°®Áé∞‰ºòÂºÇÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÊÄßËÉΩÂíåËÆ≠ÁªÉÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21759', 'title': 'Influence Guided Sampling for Domain Adaptation of Text Retrievers', 'url': 'https://huggingface.co/papers/2601.21759', 'abstract': 'An reinforcement learning-based sampling framework adaptively reweights training datasets to improve embedding model performance while reducing GPU costs.  \t\t\t\t\tAI-generated summary \t\t\t\t General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.', 'score': 1, 'issue_id': 884, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '74bd4c2ddb110aa3', 'authors': ['Meet Doshi', 'Vishwajeet Kumar', 'Yulong Li', 'Jaydeep Sen'], 'affiliations': ['IBM Research AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.21759.jpg', 'data': {'categories': ['#multilingual', '#data', '#rl', '#optimization', '#training', '#small_models'], 'emoji': 'üéØ', 'ru': {'title': '–£–º–Ω–æ–µ –ø–µ—Ä–µweighting: –∫–æ–≥–¥–∞ –º–∞—à–∏–Ω–∞ –≤—ã–±–∏—Ä–∞–µ—Ç, –∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —É—á–∏—Ç—å', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ Inf-DDS, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–µ—Ä–µweighting —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –°–∏—Å—Ç–µ–º–∞ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É–µ—Ç—Å—è —Å–∏–≥–Ω–∞–ª–∞–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–ª–∏—è–Ω–∏—è (influence-based rewards) –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É—Ç–æ—á–Ω—è–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É –≤—ã–±–æ—Ä–∫–∏, –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É—è –¥–∞—Ç–∞—Å–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ü–µ–ª–µ–≤–æ–º –Ω–∞–±–æ—Ä–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞ –∏ —Ç—Ä–µ–±—É–µ—Ç –Ω–∞ 1.5-4 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ GPU –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ gradient-based –º–µ—Ç–æ–¥–∞–º–∏. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è NDCG@10 –Ω–∞ 5.03 –ø—É–Ω–∫—Ç–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏ BGE-m3 –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –Ω–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è.'}, 'en': {'title': 'Optimize Sampling, Maximize Performance!', 'desc': 'This paper introduces Inf-DDS, a reinforcement learning-based framework that optimizes the sampling of training datasets for embedding models. By using influence-based reward signals, it adaptively reweights datasets to enhance model performance while significantly reducing GPU costs. The framework iteratively refines its sampling policy, focusing on datasets that yield the best results on a specific development set. The results show substantial improvements in retrieval performance across various tasks, achieving notable gains in efficiency and effectiveness compared to traditional sampling methods.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÈááÊ†∑ÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩ‰∏éÈôç‰ΩéÊàêÊú¨', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑÈááÊ†∑Ê°ÜÊû∂Inf-DDSÔºåÊó®Âú®ÈÄöËøáËá™ÈÄÇÂ∫îÈáçÂä†ÊùÉËÆ≠ÁªÉÊï∞ÊçÆÈõÜÊù•ÊèêÈ´òÂµåÂÖ•Ê®°ÂûãÁöÑÊÄßËÉΩÔºåÂêåÊó∂Èôç‰ΩéGPUÊàêÊú¨„ÄÇ‰º†ÁªüÁöÑÈááÊ†∑ÊñπÊ≥ïÈÄöÂ∏∏ÈááÁî®ÂùáÂåÄÈááÊ†∑Êàñ‰æùËµñ‰∫∫Â∑•‰∏ìÂÆ∂ÁõëÁù£ÔºåËÄåInf-DDSÂàôÂà©Áî®Âü∫‰∫éÂΩ±ÂìçÁöÑÂ•ñÂä±‰ø°Âè∑Êù•ÊåáÂØºÈááÊ†∑Á≠ñÁï•ÁöÑ‰ºòÂåñ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáËø≠‰ª£Á≤æÁÇºÈááÊ†∑Á≠ñÁï•Ôºå‰ºòÂÖàÈÄâÊã©ËÉΩÂ§üÊúÄÂ§ßÂåñÊ®°ÂûãÂú®ÁõÆÊ†áÂºÄÂèëÈõÜ‰∏äÊÄßËÉΩÁöÑÊï∞ÊçÆÈõÜ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåInf-DDSÂú®Â§öÁßçÊñáÊú¨Ê£ÄÁ¥¢‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂‰∏îÂú®GPUËÆ°ÁÆó‰∏äËäÇÁúÅ‰∫Ü1.5Âà∞4ÂÄçÁöÑÊàêÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.14691', 'title': 'Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation', 'url': 'https://huggingface.co/papers/2601.14691', 'abstract': "Large language models used as judges for agent performance evaluation are vulnerable to manipulation of reasoning traces, with content-based fabrications being more effective than style-based alterations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.", 'score': 1, 'issue_id': 885, 'pub_date': '2026-01-21', 'pub_date_card': {'ru': '21 —è–Ω–≤–∞—Ä—è', 'en': 'January 21', 'zh': '1Êúà21Êó•'}, 'hash': '319b0b7a616805c1', 'authors': ['Muhammad Khalifa', 'Lajanugen Logeswaran', 'Jaekyeom Kim', 'Sungryull Sohn', 'Yunxiang Zhang', 'Moontae Lee', 'Hao Peng', 'Lu Wang', 'Honglak Lee'], 'affiliations': ['LG AI Research', 'University of Illinois Urbana-Champaign', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2601.14691.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': '‚ö†Ô∏è', 'ru': {'title': '–£—è–∑–≤–∏–º–æ—Å—Ç—å LLM-—Å—É–¥–µ–π –ø–µ—Ä–µ–¥ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è–º–∏ –≤ —Ü–µ–ø–æ—á–∫–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–≤', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤, —É—è–∑–≤–∏–º—ã –¥–ª—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Å —Ü–µ–ø–æ—á–∫–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏, —á—Ç–æ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–∞ –ø—Ä–∏ –Ω–µ–∏–∑–º–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è—Ö –º–æ–∂–µ—Ç —É–≤–µ–ª–∏—á–∏—Ç—å –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏ –Ω–∞ 90%. –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ —Ñ–∞–±—Ä–∏–∫–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤ –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –∑–∞–¥–∞—á–∏ (–∫–æ–Ω—Ç–µ–Ω—Ç-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏) —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —á–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å—Ç–∏–ª–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Å—É–¥–µ–π, –≤–∫–ª—é—á–∞—è –ø—Ä–æ–º–ø—Ç-–∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥ –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —É—Å—Ç—Ä–∞–Ω—è—é—Ç —ç—Ç—É —É—è–∑–≤–∏–º–æ—Å—Ç—å.'}, 'en': {'title': 'Unmasking Vulnerabilities in LLM-Based Performance Evaluation', 'desc': "This paper discusses the vulnerabilities of large language models (LLMs) when used as judges for evaluating agent performance. It reveals that these models can be easily manipulated by altering the reasoning traces of agents, particularly through content-based fabrications, which are more effective than style-based changes. The authors demonstrate that such manipulations can significantly inflate false positive rates in performance evaluations, indicating a serious flaw in the assumption that reasoning traces accurately reflect an agent's true capabilities. The study calls for improved evaluation methods that can verify reasoning against observable evidence to mitigate these vulnerabilities."}, 'zh': {'title': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞ÁöÑËÑÜÂº±ÊÄß', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËØÑ‰º∞Êô∫ËÉΩ‰ΩìË°®Áé∞Êó∂ÂÆπÊòìÂèóÂà∞Êé®ÁêÜÁóïËøπÁöÑÊìçÊéßÔºåÂÜÖÂÆπÂü∫Á°ÄÁöÑ‰º™ÈÄ†ÊØîÈ£éÊ†ºÂü∫Á°ÄÁöÑ‰øÆÊîπÊõ¥ÊúâÊïà„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåLLMËØÑÂà§ËÄÖÂØπÊô∫ËÉΩ‰ΩìÁöÑÊé®ÁêÜËøáÁ®ãÈùûÂ∏∏ÊïèÊÑüÔºåÁªèËøáÁ≥ªÁªüÊÄßÈáçÂÜôÁöÑÊé®ÁêÜÂèØ‰ª•ÊòæËëóÊèêÈ´òÈîôËØØÂà§Êñ≠ÁöÑÊ¶ÇÁéá„ÄÇÈÄöËøáÂØπ800‰∏™‰∏çÂêåÁΩëÁªú‰ªªÂä°ÁöÑËΩ®ËøπËøõË°åÂÆûÈ™åÔºåÂèëÁé∞ÊìçÊéßÊé®ÁêÜÂèØ‰ª•‰ΩøÊúÄÂÖàËøõÁöÑËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàVLMÔºâËØÑÂà§ËÄÖÁöÑÂÅáÈò≥ÊÄßÁéáÂ¢ûÂä†Â§öËææ90%„ÄÇËøôÈ°πÁ†îÁ©∂Êè≠Á§∫‰∫ÜLLMËØÑ‰º∞‰∏≠ÁöÑÂü∫Êú¨ËÑÜÂº±ÊÄßÔºåÂº∫Ë∞É‰∫ÜÈúÄË¶ÅÂª∫Á´ãËÉΩÂ§üÊ†πÊçÆÂèØËßÇÂØüËØÅÊçÆÈ™åËØÅÊé®ÁêÜÂ£∞ÊòéÁöÑËØÑÂà§Êú∫Âà∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02338', 'title': 'Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs', 'url': 'https://huggingface.co/papers/2602.02338', 'abstract': 'ReSID presents a novel recommendation-native framework that improves sequential recommendation by learning predictive item representations and optimizing quantization for information preservation and sequential predictability.  \t\t\t\t\tAI-generated summary \t\t\t\t Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.', 'score': 0, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'f2ffbd8b6f718fd7', 'authors': ['Yu Liang', 'Zhongjin Zhang', 'Yuxuan Zhu', 'Kerui Zhang', 'Zhiluohan Guo', 'Wenhang Zhou', 'Zonqi Yang', 'Kangle Wu', 'Yabo Ni', 'Anxiang Zeng', 'Cong Fu', 'Jianxin Wang', 'Jiazhi Xia'], 'affiliations': ['Alibaba Group', 'Huazhong University of Science and Technology', 'Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2602.02338.jpg', 'data': {'categories': ['#architecture', '#training'], 'emoji': 'üéØ', 'ru': {'title': '–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π', 'desc': 'ReSID –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø—Ä–µ–¥–º–µ—Ç–æ–≤, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. –í–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —á–µ—Ä–µ–∑ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –ø–æ–ª–µ–π. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤ ReSID —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö, –∞ –Ω–µ –¥–ª—è –æ–±—â–µ–≥–æ —Å–∂–∞—Ç–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ReSID –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –Ω–∞ 10% –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'ReSID: Revolutionizing Sequential Recommendations with Smart Representations', 'desc': 'ReSID is a new framework designed to enhance sequential recommendation systems by focusing on how items are represented and how information is quantized. It introduces two main components: Field-Aware Masked Auto-Encoding (FAMAE) for creating effective item representations, and Globally Aligned Orthogonal Quantization (GAOQ) for optimizing the way these representations are stored. This approach aims to improve the accuracy of predictions while minimizing the loss of important information during the quantization process. The results show that ReSID significantly outperforms existing methods, making it a promising advancement in the field of recommendation systems.'}, 'zh': {'title': 'ReSIDÔºöÊèêÂçáÂ∫èÂàóÊé®ËçêÁöÑÊñ∞Ê°ÜÊû∂', 'desc': 'ReSIDÊòØ‰∏Ä‰∏™Êñ∞È¢ñÁöÑÊé®ËçêÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂ≠¶‰π†È¢ÑÊµãÊÄßÈ°πÁõÆË°®Á§∫Âíå‰ºòÂåñÈáèÂåñÊù•ÊèêÈ´òÂ∫èÂàóÊé®ËçêÁöÑÊïàÊûú„ÄÇËØ•Ê°ÜÊû∂‰ªé‰ø°ÊÅØ‰øùÁïôÂíåÂ∫èÂàóÂèØÈ¢ÑÊµãÊÄßÁöÑËßíÂ∫¶ÈáçÊñ∞ÊÄùËÄÉË°®Á§∫Â≠¶‰π†ÂíåÈáèÂåñÔºåÈÅøÂÖç‰æùËµñÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã„ÄÇReSIDÂåÖÂê´‰∏§‰∏™‰∏ªË¶ÅÁªÑ‰ª∂ÔºöÂú∫ÊÑüÁü•Êé©Á†ÅËá™ÁºñÁ†ÅÔºàFAMAEÔºâÂíåÂÖ®Â±ÄÂØπÈΩêÊ≠£‰∫§ÈáèÂåñÔºàGAOQÔºâÔºåÂâçËÄÖ‰ªéÁªìÊûÑÁâπÂæÅ‰∏≠Â≠¶‰π†È¢ÑÊµãÊÄßÈ°πÁõÆË°®Á§∫ÔºåÂêéËÄÖÈÄöËøáÂáèÂ∞ëËØ≠‰πâÊ®°Á≥äÂíåÂâçÁºÄÊù°‰ª∂‰∏çÁ°ÆÂÆöÊÄßÊù•ÁîüÊàêÁ¥ßÂáëÁöÑSIDÂ∫èÂàó„ÄÇÁêÜËÆ∫ÂàÜÊûêÂíåÂ§ßÈáèÂÆûÈ™åË°®ÊòéÔºåReSIDÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòË∂äÔºåÂπ≥ÂùáË∂ÖË∂äÂº∫Âü∫Á∫øË∂ÖËøá10%ÔºåÂêåÊó∂Â∞ÜÊ†áËÆ∞ÂåñÊàêÊú¨Èôç‰Ωé‰∫Ü122ÂÄç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01897', 'title': 'Internal Flow Signatures for Self-Checking and Refinement in LLMs', 'url': 'https://huggingface.co/papers/2602.01897', 'abstract': 'Internal flow signatures analyze depthwise dynamics in large language models to enable self-checking and targeted refinement without modifying the base model.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce internal flow signatures that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact moving readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. Code is available at github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs.', 'score': 0, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '891c90ffa8a6fd40', 'authors': ['Sungheon Jeong', 'Sanggeon Yun', 'Ryozo Masukawa', 'Wenjun Haung', 'Hanning Chen', 'Mohsen Imani'], 'affiliations': ['Department of Computer Science, University of California, Irvine'], 'pdf_title_img': 'assets/pdf/title_img/2602.01897.jpg', 'data': {'categories': ['#architecture', '#hallucinations', '#interpretability', '#open_source', '#training'], 'emoji': 'üîç', 'ru': {'title': '–í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤–∏–¥–µ–Ω–∏–µ LLM: —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≥–ª—É–±–∏–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –¥–∏–Ω–∞–º–∏–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã –ø–æ—Ç–æ–∫–æ–≤ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ—è—Ö —Å–µ—Ç–∏. –ú–µ—Ç–æ–¥ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏ –º–æ–¥–µ–ª–∏, –≤—ã–¥–µ–ª—è—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –∏ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –æ—à–∏–±–∫–∏ –∏–ª–∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏. –õ—ë–≥–∫–∏–π –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ GRU –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —ç—Ç–∏—Ö —Å–∏–≥–Ω–∞—Ç—É—Ä–∞—Ö –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏. –ü–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –º–µ—Å—Ç–∞, –Ω–æ –∏ –ª–æ–∫–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —Å–ª–æ–µ, –∞ –∑–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Å—Ç–∏ —Ü–µ–ª–µ–≤—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é –ø—É—Ç—ë–º –æ—Ç–∫–∞—Ç—ã–≤–∞–Ω–∏—è –∏ –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.'}, 'en': {'title': 'Self-Check and Refine: Enhancing LLMs with Internal Flow Signatures', 'desc': "This paper presents a novel approach called internal flow signatures to analyze the decision-making processes in large language models (LLMs). By monitoring the dynamics of token movements within the model's architecture, the method allows for self-checking of generated outputs without altering the original model. The approach utilizes a lightweight GRU validator that identifies and localizes errors in the model's decision-making, enabling targeted refinements. This technique enhances the reliability of LLMs by providing a mechanism for auditing and correcting outputs based on internal dynamics."}, 'zh': {'title': 'ÂÜÖÈÉ®ÊµÅÁ≠æÂêçÔºöËá™ÊàëÊ£ÄÊü•‰∏é‰ºòÂåñÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂÜÖÈÉ®ÊµÅÁ≠æÂêçÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÂàÜÊûêÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ∑±Â∫¶Âä®ÊÄÅÔºå‰ª•ÂÆûÁé∞Ëá™ÊàëÊ£ÄÊü•ÂíåÈíàÂØπÊÄß‰ºòÂåñÔºåËÄåÊó†ÈúÄ‰øÆÊîπÂü∫Á°ÄÊ®°Âûã„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂÅèÁΩÆ‰∏≠ÂøÉÁõëÊéßÁ®≥ÂÆö‰∫ÜÈÄêËØçËøêÂä®ÔºåÂπ∂Âú®ÊØè‰∏™Ê∑±Â∫¶Á™óÂè£ÂÜÖÊûÑÂª∫‰∫ÜÁ¥ßÂáëÁöÑÁßªÂä®ËØªÂá∫ÂØπÈΩêÂ≠êÁ©∫Èó¥„ÄÇÈÄöËøáÊ≠£‰∫§‰º†ËæìÂØπÈÇªËøëÁ™óÂè£Â∏ßËøõË°åÂØπÈΩêÔºåËé∑ÂæóÊ∑±Â∫¶ÂèØÊØîÁöÑ‰º†ËæìÊ≠•Èïø„ÄÅËΩ¨ËßíÂíåÂ≠êÁ©∫Èó¥ÊºÇÁßªÊëòË¶Å„ÄÇÊúÄÁªàÔºåËÆ≠ÁªÉÂú®Ëøô‰∫õÁ≠æÂêç‰∏äÁöÑËΩªÈáèÁ∫ßGRUÈ™åËØÅÂô®ËÉΩÂ§üÂú®‰∏ç‰øÆÊîπÂü∫Á°ÄÊ®°ÂûãÁöÑÊÉÖÂÜµ‰∏ãËøõË°åËá™ÊàëÊ£ÄÊü•„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01815', 'title': 'INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery', 'url': 'https://huggingface.co/papers/2602.01815', 'abstract': "Multi-agent systems for molecular discovery that use individualized scientist profiles based on publication and molecular history outperform traditional role-based approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.", 'score': 0, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'acbc59323bb71c5b', 'authors': ['Yunhui Jang', 'Seonghyun Park', 'Jaehyung Kim', 'Sungsoo Ahn'], 'affiliations': ['Korea Advanced Institute of Science and Technology (KAIST), South Korea', 'Yonsei university, South'], 'pdf_title_img': 'assets/pdf/title_img/2602.01815.jpg', 'data': {'categories': [], 'emoji': 'üß¨', 'ru': {'title': '–ù–∞—É—á–Ω–∞—è –î–ù–ö –∞–≥–µ–Ω—Ç–æ–≤: –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏–π –≤ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω–æ–π —Ö–∏–º–∏–∏', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ INDIBATOR –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–º –ø–æ–¥—Ö–æ–¥–µ. –ê–≥–µ–Ω—Ç—ã –≤ —Å–∏—Å—Ç–µ–º–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–æ—Ñ–∏–ª—è–º–∏ —É—á—ë–Ω—ã—Ö, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–π –∏ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —á—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –Ω–∞—É—á–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –í–º–µ—Å—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–æ–ª–µ–≤—ã—Ö –ø–µ—Ä—Å–æ–Ω (—Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç, –ø–∏—Å–∞—Ç–µ–ª—å), —Å–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ –∞–≥–µ–Ω—Ç–∞–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–¥–µ–π, –∫—Ä–∏—Ç–∏–∫—É –∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–∏—Å—Ç–µ–º—ã —Å —É–ø—Ä–æ—â—ë–Ω–Ω—ã–º–∏ —Ä–æ–ª–µ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏, –¥–æ—Å—Ç–∏–≥–∞—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –º–æ–ª–µ–∫—É–ª—è—Ä–Ω–æ–≥–æ –¥–∏–∑–∞–π–Ω–∞.'}, 'en': {'title': 'Harnessing Individuality for Superior Molecular Discovery', 'desc': 'This paper introduces INDIBATOR, a multi-agent system designed for molecular discovery that utilizes individualized scientist profiles. Unlike traditional systems that use generic roles, INDIBATOR tailors agent behavior based on unique publication and molecular histories. The agents engage in a structured debate process, allowing for proposal, critique, and voting, which enhances collaboration and decision-making. The results show that these personalized agents significantly outperform those using broad, role-based personas, highlighting the importance of individual research trajectories in scientific discovery.'}, 'zh': {'title': '‰∏™ÊÄßÂåñÊô∫ËÉΩ‰ΩìÂä©ÂäõÂàÜÂ≠êÂèëÁé∞', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫INDIBATORÁöÑÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÔºåÁî®‰∫éÂàÜÂ≠êÂèëÁé∞„ÄÇËØ•Á≥ªÁªüÂü∫‰∫é‰∏™ÊÄßÂåñÁöÑÁßëÂ≠¶ÂÆ∂Ê°£Ê°àÔºåÁªìÂêà‰∫ÜÂá∫ÁâàÂéÜÂè≤ÂíåÂàÜÂ≠êÂéÜÂè≤Ôºå‰ª•Êõ¥Â•ΩÂú∞Ê®°ÊãüÁßëÂ≠¶ÂÆ∂ÁöÑÁã¨ÁâπÁ†îÁ©∂ËΩ®Ëøπ„ÄÇ‰∏é‰º†ÁªüÁöÑËßíËâ≤Âü∫Á°ÄÊñπÊ≥ïÁõ∏ÊØîÔºåINDIBATORÈÄöËøáÂ§öËΩÆËæ©ËÆ∫ÁöÑÊñπÂºèËøõË°åÊèêÊ°à„ÄÅÊâπËØÑÂíåÊäïÁ•®ÔºåÂ±ïÁé∞Âá∫Êõ¥È´òÁöÑÊïàÁéáÂíåÂáÜÁ°ÆÊÄß„ÄÇÁ†îÁ©∂ÁªìÊûúË°®ÊòéÔºå‰∏™ÊÄßÂåñÁöÑÊô∫ËÉΩ‰ΩìÂú®ÁßëÂ≠¶ÂèëÁé∞‰∏≠ÂÖ∑ÊúâÊòæËëó‰ºòÂäøÔºåËÉΩÂ§üÂÆûÁé∞Êõ¥È´òË¥®ÈáèÁöÑÁ†îÁ©∂ÊàêÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01618', 'title': 'SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia', 'url': 'https://huggingface.co/papers/2602.01618', 'abstract': 'Researchers developed a novel agentic data-generation framework to create culturally grounded safety datasets for Southeast Asia, resulting in multilingual safeguard models that outperform existing approaches in detecting regionally sensitive content while maintaining general safety performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.', 'score': 0, 'issue_id': 884, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'c0cba7ad1e707785', 'authors': ['Panuthep Tasawong', 'Jian Gang Ngui', 'Alham Fikri Aji', 'Trevor Cohn', 'Peerat Limkonchotiwat'], 'affiliations': ['AI Singapore', 'Google', 'VISTEC'], 'pdf_title_img': 'assets/pdf/title_img/2602.01618.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#agents', '#benchmark', '#low_resource', '#alignment', '#synthetic', '#open_source'], 'emoji': 'üåè', 'ru': {'title': '–ö—É–ª—å—Ç—É—Ä–Ω–æ-–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –Æ–≥–æ-–í–æ—Å—Ç–æ—á–Ω–æ–π –ê–∑–∏–∏', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –∞–≥–µ–Ω—Ç–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å —É—á—ë—Ç–æ–º –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Æ–≥–æ-–í–æ—Å—Ç–æ—á–Ω–æ–π –ê–∑–∏–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –æ–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π SEA-Guard ‚Äî –ø–µ—Ä–≤—ã–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –∫—É–ª—å—Ç—É—Ä–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–∞. –ú–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –≤ –≤—ã—è–≤–ª–µ–Ω–∏–∏ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –†–µ—à–µ–Ω–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ, —Ç–∞–∫ –∫–∞–∫ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, —É–ø—É—Å–∫–∞—é—Ç —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –Ω—é–∞–Ω—Å—ã.'}, 'en': {'title': 'Empowering AI with Culturally Grounded Safety in Southeast Asia', 'desc': 'This paper introduces a new framework for generating culturally relevant safety datasets specifically for Southeast Asia. The framework addresses the challenges of creating large-scale datasets by utilizing agentic data generation, which allows for the production of authentic, region-specific data. The resulting SEA-Guard models are multilingual and designed to better detect sensitive content that aligns with local cultural values and regulations. Evaluations show that these models outperform traditional approaches, ensuring both regional sensitivity and overall safety performance.'}, 'zh': {'title': 'ÊñáÂåñÈ©±Âä®ÁöÑÂÆâÂÖ®Ê®°ÂûãÔºåÂÆàÊä§‰∏úÂçó‰∫öÁöÑÂÆâÂÖ®', 'desc': 'Á†îÁ©∂‰∫∫ÂëòÂºÄÂèë‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËá™‰∏ªÊï∞ÊçÆÁîüÊàêÊ°ÜÊû∂ÔºåÁî®‰∫éÂàõÂª∫ÈÄÇÂêà‰∏úÂçó‰∫öÊñáÂåñÁöÑÂÆâÂÖ®Êï∞ÊçÆÈõÜ„ÄÇËøôÁßçÊ°ÜÊû∂ËÉΩÂ§üÊúâÊïàÂú∞ÁîüÊàêÁúüÂÆûÁöÑ„ÄÅÂú∞Âå∫ÁâπÂÆöÁöÑÂÆâÂÖ®Êï∞ÊçÆÔºå‰ªéËÄåÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑÂÆâÂÖ®Ê®°Âûã„ÄÇÊñ∞Êé®Âá∫ÁöÑSEA-GuardÊ®°ÂûãÂú®Ê£ÄÊµãÂú∞Âå∫ÊïèÊÑüÂÜÖÂÆπÊñπÈù¢Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËâØÂ•ΩÁöÑÊï¥‰ΩìÂÆâÂÖ®ÊÄßËÉΩ„ÄÇËøôÈ°πÁ†îÁ©∂Âº∫Ë∞É‰∫ÜÂú®‰∫∫Â∑•Êô∫ËÉΩÂØπÈΩê‰∏≠ÔºåÊñáÂåñÊÑèËØÜÁöÑÈáçË¶ÅÊÄßÔºåÂ∞§ÂÖ∂ÊòØÂú®Ê∂âÂèäÂ§öÊ†∑ÂåñÁöÑÂú∞Êñπ‰ª∑ÂÄºËßÇÂíåËßÑËåÉÊó∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01418', 'title': 'Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas', 'url': 'https://huggingface.co/papers/2602.01418', 'abstract': 'Parabolic Position Encoding (PaPE) is a novel position encoding method for vision modalities that improves upon existing approaches by incorporating translation invariance, rotation invariance, distance decay, directionality, and context awareness principles.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.', 'score': 0, 'issue_id': 886, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': 'a43d68f82a428632', 'authors': ['Christoffer Koo √òhrstr√∏m', 'Rafael I. Cabral Muchacho', 'Yifei Dong', 'Filippos Moumtzidellis', 'Ronja G√ºldenring', 'Florian T. Pokorny', 'Lazaros Nalpantidis'], 'affiliations': ['Lund University', 'Royal Institute of Technology (KTH)'], 'pdf_title_img': 'assets/pdf/title_img/2602.01418.jpg', 'data': {'categories': ['#optimization', '#cv', '#multimodal', '#architecture'], 'emoji': 'üìç', 'ru': {'title': '–ü–∞—Ä–∞–±–æ–ª–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π (PaPE) –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø–∞—Ä–∞–±–æ–ª–∏—á–µ—Å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö. –ú–µ—Ç–æ–¥ incorporates –ø—è—Ç—å –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤: –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–∏, –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ —Ä–æ—Ç–∞—Ü–∏–∏, –∑–∞—Ç—É—Ö–∞–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –æ—Å–≤–µ–¥–æ–º–ª—ë–Ω–Ω–æ—Å—Ç—å. –ê–≤—Ç–æ—Ä—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –ø–æ–¥—Ö–æ–¥ –Ω–∞ –≤–æ—Å—å–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —á–µ—Ç—ã—Ä–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫, –≤–∏–¥–µ–æ, –ø–æ—Ç–æ–∫–∏ —Å–æ–±—ã—Ç–∏–π), –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏, —á—Ç–æ PaPE –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Å–µ–º–∏ –∏–∑ –≤–æ—Å—å–º–∏ —Å–ª—É—á–∞–µ–≤. –û—Å–æ–±–µ–Ω–Ω–æ –≤–ø–µ—á–∞—Ç–ª—è—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –Ω–∞ ImageNet-1K, –≥–¥–µ –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π –Ω–∞ 10.5% –≤ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º –≤—ã—Ä–∞–∂–µ–Ω–∏–∏.'}, 'en': {'title': 'Revolutionizing Vision with Parabolic Position Encoding', 'desc': 'Parabolic Position Encoding (PaPE) is a new method designed to improve how positions are encoded in vision tasks using attention-based models. It incorporates key principles such as translation and rotation invariance, which help the model recognize objects regardless of their position or orientation. Additionally, PaPE considers factors like distance decay, directionality, and context awareness to better capture the unique characteristics of visual data. Evaluations on multiple datasets show that PaPE significantly outperforms existing position encoding methods, demonstrating its effectiveness across various vision modalities.'}, 'zh': {'title': 'ÊäõÁâ©Á∫ø‰ΩçÁΩÆÁºñÁ†ÅÔºöËßÜËßâÊ®°ÊÄÅÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ≥ïÔºåÁß∞‰∏∫ÊäõÁâ©Á∫ø‰ΩçÁΩÆÁºñÁ†ÅÔºàPaPEÔºâÔºåÊó®Âú®ÊîπÂñÑËßÜËßâÊ®°ÊÄÅ‰∏≠ÁöÑ‰ΩçÁΩÆÁºñÁ†Å„ÄÇPaPEÁªìÂêà‰∫ÜÂπ≥Áßª‰∏çÂèòÊÄß„ÄÅÊóãËΩ¨‰∏çÂèòÊÄß„ÄÅË∑ùÁ¶ªË°∞Âáè„ÄÅÊñπÂêëÊÄßÂíå‰∏ä‰∏ãÊñáÊÑüÁü•Á≠âÂéüÂàôÔºå‰ª•Êõ¥Â•ΩÂú∞ÈÄÇÂ∫îËßÜËßâÁâπÂæÅ„ÄÇÊàë‰ª¨Âú®8‰∏™Ê∂µÁõñ4ÁßçÊ®°ÊÄÅÁöÑÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫ÜPaPEÔºåÁªìÊûúÊòæÁ§∫PaPEÊàñPaPE-RIÂú®7‰∏™Êï∞ÊçÆÈõÜ‰∏äÂèñÂæó‰∫ÜÊúÄ‰Ω≥ÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPaPEÂú®ImageNet-1K‰∏äÁöÑÂ§ñÊé®ËÉΩÂäõÊòæËëóÔºåÊÄßËÉΩÊèêÂçáÂèØËææ10.5%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00168', 'title': 'YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation', 'url': 'https://huggingface.co/papers/2602.00168', 'abstract': 'YOLOE-26 integrates YOLO26 architecture with open-vocabulary learning for real-time instance segmentation, utilizing convolutional backbones, end-to-end regression, and object embedding heads with text and visual prompting capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.', 'score': 0, 'issue_id': 887, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'f34751d7c5be7ce1', 'authors': ['Ranjan Sapkota', 'Manoj Karkee'], 'affiliations': ['Cornell University, Biological & Environmental Engineering, Ithaca, NY 14850, USA'], 'pdf_title_img': 'assets/pdf/title_img/2602.00168.jpg', 'data': {'categories': ['#multimodal', '#architecture', '#cv', '#open_source'], 'emoji': 'üéØ', 'ru': {'title': 'YOLO26 –≤—Å—Ç—Ä–µ—á–∞–µ—Ç –æ—Ç–∫—Ä—ã—Ç—É—é –ª–µ–∫—Å–∏–∫—É: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏', 'desc': 'YOLOE-26 –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É YOLO26 —Å –ø–∞—Ä–∞–¥–∏–≥–º–æ–π –æ—Ç–∫—Ä—ã—Ç–æ–π –ª–µ–∫—Å–∏–∫–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –æ—Å—Ç–æ–≤ —Å –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∑–∞–º–µ–Ω—è–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–æ–≥–∏—Ç—ã –∫–ª–∞—Å—Å–æ–≤ –Ω–∞ –≥–æ–ª–æ–≤—É –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –∫–æ—Ç–æ—Ä–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∫–∞–∫ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥–æ–±–∏—è —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –ø–æ–¥—Å–∫–∞–∑–æ–∫ —Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∫–ª—é—á–∞–µ—Ç –ø–µ—Ä–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑—É–µ–º–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –æ–±–ª–∞—Å—Ç—å-—Ç–µ–∫—Å—Ç –∏ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –°–∏—Å—Ç–µ–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –±—ã—Å—Ç—Ä–æ–¥–µ–π—Å—Ç–≤–∏–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ, –≤–∏–∑—É–∞–ª—å–Ω—ã–µ, —Ç–∞–∫ –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Ä–µ–∂–∏–º—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.'}, 'en': {'title': 'YOLOE-26: Real-Time Open-Vocabulary Instance Segmentation Unleashed!', 'desc': 'YOLOE-26 is a new framework that combines the YOLO26 architecture with open-vocabulary learning for real-time instance segmentation. It maintains the efficiency of the YOLO family while allowing recognition of objects beyond a fixed set of classes. The model uses a convolutional backbone and replaces traditional class logits with an object embedding head, enabling classification through similarity matching with text and visual prompts. This approach includes advanced techniques for prompt-based reasoning, making it adaptable for various segmentation tasks in real-world scenarios.'}, 'zh': {'title': 'YOLOE-26ÔºöÂÆûÊó∂ÂºÄÊîæËØçÊ±áÂÆû‰æãÂàÜÂâ≤ÁöÑÂàõÊñ∞Ëß£ÂÜ≥ÊñπÊ°à', 'desc': 'YOLOE-26ÊòØ‰∏ÄÁßçÈõÜÊàê‰∫ÜYOLO26Êû∂ÊûÑÂíåÂºÄÊîæËØçÊ±áÂ≠¶‰π†ÁöÑÂÆûÊó∂ÂÆû‰æãÂàÜÂâ≤Ê°ÜÊû∂„ÄÇÂÆÉÂà©Áî®Âç∑ÁßØÈ™®Âπ≤ÁΩëÂíåÁ´ØÂà∞Á´ØÂõûÂΩíÔºåËÉΩÂ§üÂ§ÑÁêÜÊñáÊú¨ÂíåËßÜËßâÊèêÁ§∫ÁöÑÂØπË±°ÂµåÂÖ•„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÂõ∫ÂÆöÁ±ªÂà´ÈÄªËæëÊõøÊç¢‰∏∫ÂØπË±°ÂµåÂÖ•Â§¥ÔºåÂÆûÁé∞‰∫ÜÂàÜÁ±ª‰∏éÊèêÁ§∫ÂµåÂÖ•ÁöÑÁõ∏‰ººÊÄßÂåπÈÖç„ÄÇYOLOE-26Âú®Âä®ÊÄÅÁéØÂ¢É‰∏≠Êèê‰æõ‰∫Ü‰∏ÄÁßçÂÆûÁî®‰∏îÂèØÊâ©Â±ïÁöÑËß£ÂÜ≥ÊñπÊ°àÔºåÊîØÊåÅÂ§öÁßçÊèêÁ§∫ÊñπÂºèÔºåÁ°Æ‰øùÈ´òÊïàÁöÑÂÆû‰æãÂàÜÂâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21558', 'title': 'ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas', 'url': 'https://huggingface.co/papers/2601.21558', 'abstract': 'ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.', 'score': 43, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '6bbe252a83428e3f', 'authors': ['Xiaoyu Tian', 'Haotian Wang', 'Shuaiting Chen', 'Hao Zhou', 'Kaichi Yu', 'Yudian Zhang', 'Jade Ouyang', 'Junxi Yin', 'Jiong Chen', 'Baoyan Guo', 'Lei Zhang', 'Junjie Tao', 'Yuansheng Song', 'Ming Cui', 'Chengwei Liu'], 'affiliations': ['Beike Language and Intelligence (BLI)'], 'pdf_title_img': 'assets/pdf/title_img/2601.21558.jpg', 'data': {'categories': ['#data', '#benchmark', '#open_source', '#rl', '#agents', '#optimization', '#synthetic', '#training', '#reasoning'], 'emoji': 'üõ†Ô∏è', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 'desc': 'ASTRA ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏, —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –°–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –≥—Ä–∞—Ñ–æ–≤ –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ —Ä–∞–∑–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ª–µ–¥–æ–≤ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç supervised fine-tuning —Å online RL, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –º–µ–∂–¥—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º –∑–∞–¥–∞—á–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é ASTRA, –¥–æ—Å—Ç–∏–≥–∞—é—Ç state-of-the-art –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.'}, 'en': {'title': 'ASTRA: Automating Tool-Augmented Learning for Better Decision-Making', 'desc': 'ASTRA is an innovative framework designed to enhance the training of tool-augmented language models for complex decision-making tasks. It automates the process by using synthetic data and verifiable reinforcement learning, addressing the limitations of existing methods that often require manual input and lack reliable environments. The framework consists of two main components: a pipeline for generating diverse tool-use trajectories and an environment synthesis framework that creates verifiable scenarios for training. As a result, ASTRA enables models to achieve superior performance in multi-step decision-making while maintaining their reasoning capabilities.'}, 'zh': {'title': 'ASTRAÔºöËá™Âä®ÂåñÂ∑•ÂÖ∑Â¢ûÂº∫ËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'ASTRAÊòØ‰∏Ä‰∏™Ëá™Âä®ÂåñÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂêàÊàêÊï∞ÊçÆÂíåÂèØÈ™åËØÅÁöÑÂº∫ÂåñÂ≠¶‰π†Êù•ËÆ≠ÁªÉÂ¢ûÂº∫Â∑•ÂÖ∑ÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºå‰ª•ÊèêÈ´òÂ§öÊ≠•È™§ÂÜ≥Á≠ñËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÁé∞ÊúâÊñπÊ≥ï‰∏≠ÈúÄË¶Å‰∫∫Â∑•Âπ≤È¢Ñ„ÄÅ‰æùËµñ‰∏çÂèØÈ™åËØÅÁöÑÊ®°ÊãüÁéØÂ¢É‰ª•ÂèäÂú®ÈïøÊó∂Èó¥„ÄÅÂ§öËΩÆÂ≠¶‰π†‰∏≠‰∏çÁ®≥ÂÆöÁöÑÈóÆÈ¢ò„ÄÇASTRAÁªìÂêà‰∫Ü‰∏§Â§ßÁªÑ‰ª∂Ôºö‰∏ÄÊòØÂà©Áî®Â∑•ÂÖ∑Ë∞ÉÁî®ÂõæÁöÑÈùôÊÄÅÊãìÊâëÂêàÊàêÂ§öÊ†∑ÂåñÁöÑËΩ®ËøπÔºå‰∫åÊòØÈÄöËøáÁéØÂ¢ÉÂêàÊàêÊ°ÜÊû∂Â∞ÜÈóÆÈ¢ò-Á≠îÊ°àËΩ®ËøπËΩ¨Âåñ‰∏∫ÂèØÊâßË°åÁöÑ„ÄÅËßÑÂàôÂèØÈ™åËØÅÁöÑÁéØÂ¢É„ÄÇÂÆûÈ™åË°®ÊòéÔºåASTRAËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Â§ö‰∏™Â∑•ÂÖ∑‰ΩøÁî®Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÊé•ËøëÂ∞ÅÈó≠Ê∫êÁ≥ªÁªüÔºåÂêåÊó∂‰øùÊåÅÊ†∏ÂøÉÊé®ÁêÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23143', 'title': 'THINKSAFE: Self-Generated Safety Alignment for Reasoning Models', 'url': 'https://huggingface.co/papers/2601.23143', 'abstract': 'ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.', 'score': 31, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '60429122f361bf32', 'authors': ['Seanie Lee', 'Sangwoo Park', 'Yumin Choi', 'Gyeongman Kim', 'Minki Kang', 'Jihun Yun', 'Dongmin Park', 'Jongho Park', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST AI', 'KRAFTON', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2601.23143.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#open_source', '#rl', '#training', '#reasoning'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–µ–∑ —É—á–∏—Ç–µ–ª–µ–π: —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏', 'desc': 'ThinkSafe ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∞–º–æ–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –±–µ–∑ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –≤–Ω–µ—à–Ω–∏—Ö —É—á–∏—Ç–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª—ë–≥–∫–æ–µ —Ä—É–ª–µ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–∫–∞–∑–∞–º–∏ (refusal steering), —á—Ç–æ–±—ã –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç—Ä–∞—Å—Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –º–æ–¥–µ–ª–∏. –ü–æ—Å–ª–µ–¥—É—é—â–∞—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö –ø–µ—Ä–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å, —Å–æ—Ö—Ä–∞–Ω—è—è –µ—ë —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –≥–ª—É–±–æ–∫–æ–º—É –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é. –ü–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ.'}, 'en': {'title': 'Enhancing Safety in Reasoning Models with ThinkSafe', 'desc': 'ThinkSafe is a novel framework designed to enhance the safety of large reasoning models (LRMs) while maintaining their reasoning capabilities. It employs lightweight refusal steering and fine-tuning on responses generated by the model itself, avoiding the need for external teacher distillation which can introduce discrepancies. This approach allows the model to leverage its inherent knowledge to identify harmful prompts and generate safer responses. Experiments demonstrate that ThinkSafe significantly improves safety without compromising reasoning performance and does so with lower computational costs.'}, 'zh': {'title': 'ThinkSafeÔºöÊèêÂçáÊé®ÁêÜÊ®°ÂûãÂÆâÂÖ®ÊÄßÁöÑËá™ÊàëÂØπÈΩêÊ°ÜÊû∂', 'desc': 'ThinkSafeÊòØ‰∏Ä‰∏™Ëá™ÊàëÂØπÈΩêÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËΩªÈáèÁ∫ßÊãíÁªùÂºïÂØºÂíåËá™ÁîüÊàêÂìçÂ∫îÁöÑÂæÆË∞ÉÊù•Â¢ûÂº∫Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÔºåÂêåÊó∂‰øùÊåÅÊé®ÁêÜÊÄßËÉΩÂπ∂Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÂ§ßÂûãÊé®ÁêÜÊ®°ÂûãÂú®‰ºòÂåñËøáÁ®ã‰∏≠ÂèØËÉΩÂØºËá¥ÁöÑÂÆâÂÖ®ÊÄß‰∏ãÈôçÈóÆÈ¢òÔºåÈÅøÂÖç‰∫Ü‰æùËµñÂ§ñÈÉ®ÊïôÂ∏àËí∏È¶èÊâÄÂ∏¶Êù•ÁöÑÂàÜÂ∏ÉÂ∑ÆÂºÇ„ÄÇThinkSafeÂà©Áî®Ê®°ÂûãÂÜÖÂú®ÁöÑÁü•ËØÜÊù•ËØÜÂà´ÊúâÂÆ≥ÊèêÁ§∫ÔºåÈÄöËøáÂºïÂØºÊ®°ÂûãÁîüÊàêÂÆâÂÖ®Êé®ÁêÜËΩ®ËøπÊù•ÊÅ¢Â§çÂÆâÂÖ®ÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåThinkSafeÂú®ÊèêÈ´òÂÆâÂÖ®ÊÄßÁöÑÂêåÊó∂Ôºå‰øùÊåÅ‰∫ÜÊé®ÁêÜËÉΩÂäõÔºåÂπ∂ÊòæËëóÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22813', 'title': 'Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation', 'url': 'https://huggingface.co/papers/2601.22813', 'abstract': 'Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.  \t\t\t\t\tAI-generated summary \t\t\t\t The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .', 'score': 27, 'issue_id': 880, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'f7c69563851fc6ed', 'authors': ['Andrei Panferov', 'Erik Schultheis', 'Soroush Tabesh', 'Dan Alistarh'], 'affiliations': ['Institute of Science and Technology Austria', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.22813.jpg', 'data': {'categories': [], 'emoji': '‚ö°', 'ru': {'title': 'Quartet II: –ø–æ–ª–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è NVFP4 –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ Quartet II –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞ NVFP4 –Ω–∞ GPU NVIDIA Blackwell. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É MS-EDEN –¥–ª—è –Ω–µ–∏—Å–∫–∞–∂–µ–Ω–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–Ω–∏–∂–∞–µ—Ç –æ—à–∏–±–∫—É –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –±–æ–ª–µ–µ —á–µ–º –≤ 2 —Ä–∞–∑–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å–æ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏–º –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ–º. –ú–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∫–∞–∫ –Ω–∞ –ø—Ä—è–º–æ–º, —Ç–∞–∫ –∏ –Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–º –ø—Ä–æ—Ö–æ–¥–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ Quartet II –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 4.2x –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ BF16 –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ 1.9B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.'}, 'en': {'title': 'Boosting LLM Training with Quartet II: Precision Meets Speed', 'desc': 'The paper presents Quartet II, a quantized training method that enhances the utilization of the NVFP4 format for pre-training large language models (LLMs). It introduces a new unbiased quantization routine called MS-EDEN, which significantly reduces quantization error compared to traditional stochastic rounding methods. This approach allows for improved gradient estimation during both forward and backward passes in matrix multiplications. The authors demonstrate that Quartet II not only improves accuracy but also accelerates training on NVIDIA Blackwell GPUs, achieving up to 4.2x speedup over BF16.'}, 'zh': {'title': 'ÊèêÂçáNVFP4Ê†ºÂºèÁöÑÈáèÂåñËÆ≠ÁªÉÊïàÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈáèÂåñËÆ≠ÁªÉÊñπÊ≥ïQuartet IIÔºåÊó®Âú®ÊèêÈ´òNVFP4Ê†ºÂºèÂú®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÂà©Áî®Áéá„ÄÇÈÄöËøáÊîπËøõÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°ÂíåÊõ¥Âø´ÁöÑGPUÊâßË°åÔºåQuartet IIÂÆûÁé∞‰∫ÜÊØîÁé∞ÊúâÊñπÊ≥ïÊõ¥‰ΩéÁöÑÈáèÂåñËØØÂ∑Æ„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊó†ÂÅèÈáèÂåñ‰æãÁ®ãMS-EDENÔºå‰ΩøÂæóÂú®ÂæÆÂ∞∫Â∫¶Ê†ºÂºè‰∏ãÁöÑÈáèÂåñËØØÂ∑ÆÈôç‰ΩéË∂ÖËøá2ÂÄç„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåQuartet IIÂú®ÊâÄÊúâ‰∏ªË¶ÅÁü©Èòµ‰πòÊ≥ïÁöÑÂâçÂêëÂíåÂèçÂêë‰º†Êí≠‰∏≠ÈÉΩËÉΩÂÆûÁé∞Êõ¥Â•ΩÁöÑÊ¢ØÂ∫¶‰º∞ËÆ°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22628', 'title': 'TTCS: Test-Time Curriculum Synthesis for Self-Evolving', 'url': 'https://huggingface.co/papers/2601.22628', 'abstract': "TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.", 'score': 24, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '6bb34e440d233d8a', 'authors': ['Chengyi Yang', 'Zhishang Xiang', 'Yunbo Tang', 'Zongpei Teng', 'Chengsong Huang', 'Fei Long', 'Yuhan Liu', 'Jinsong Su'], 'affiliations': ['Renmin University of China', 'Washington University in St. Louis', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22628.jpg', 'data': {'categories': ['#benchmark', '#math', '#open_source', '#optimization', '#training', '#reasoning'], 'emoji': 'üîÑ', 'ru': {'title': '–°–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è TTCS ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏: –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ —Ä–µ—à–∞—Ç–µ–ª—å –∑–∞–¥–∞—á. –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —Å–ª–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤–æ–ø—Ä–æ—Å–æ–≤, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫ —Ç–µ–∫—É—â–∏–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º –º–æ–¥–µ–ª–∏, —á—Ç–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —É—á–µ–±–Ω—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É. –†–µ—à–∞—Ç–µ–ª—å —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞–≥—Ä–∞–¥—ã —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –∫–∞–∫ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–µ, —Ç–∞–∫ –∏ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ TTCS –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –∏ —Ö–æ—Ä–æ—à–æ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—Å—è –Ω–∞ –¥—Ä—É–≥–∏–µ –¥–æ–º–µ–Ω—ã.'}, 'en': {'title': 'Dynamic Questioning for Enhanced Reasoning in LLMs', 'desc': "TTCS is a novel framework designed to enhance the reasoning capabilities of large language models (LLMs) during test-time training. It operates by co-evolving two components: a question synthesizer that generates increasingly difficult question variants and a reasoning solver that improves its performance based on feedback from these questions. This iterative process allows the synthesizer to tailor questions to the solver's current abilities, creating a dynamic learning environment. The results demonstrate that TTCS effectively boosts reasoning skills on complex tasks and can be applied across various LLM architectures, paving the way for adaptive test-time learning."}, 'zh': {'title': 'ÂÖ±ÂêåËøõÂåñÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÁöÑÊµãËØïÊó∂ËÆ≠ÁªÉÊ°ÜÊû∂', 'desc': 'TTCSÊòØ‰∏ÄÁßçÂÖ±ÂêåËøõÂåñÁöÑÊµãËØïÊó∂ËÆ≠ÁªÉÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáËø≠‰ª£ÁîüÊàêÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈóÆÈ¢òÂèò‰ΩìÊù•Â¢ûÂº∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÂåÖÊã¨‰∏§‰∏™Á≠ñÁï•ÔºöÈóÆÈ¢òÂêàÊàêÂô®ÂíåÊé®ÁêÜÊ±ÇËß£Âô®ÔºåÂÆÉ‰ª¨‰ªéÂêå‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÊ®°ÂûãÂàùÂßãÂåñÔºåÂπ∂ÈÄöËøá‰ºòÂåñ‰∏çÊñ≠ÊºîÂåñ„ÄÇÈóÆÈ¢òÂêàÊàêÂô®Ê†πÊçÆÊµãËØïÈóÆÈ¢òÁîüÊàêÈÄêÊ∏êÂÖ∑ÊúâÊåëÊàòÊÄßÁöÑÈóÆÈ¢òÂèò‰ΩìÔºåËÄåÊé®ÁêÜÊ±ÇËß£Âô®ÂàôÈÄöËøáËá™Êàë‰∏ÄËá¥ÊÄßÂ•ñÂä±ËøõË°åÊõ¥Êñ∞„ÄÇÂÆûÈ™åË°®ÊòéÔºåTTCSÂú®Â§çÊùÇÁöÑÊï∞Â≠¶Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜËÉΩÂäõÔºåÂπ∂‰∏îËÉΩÂ§üÂú®‰∏çÂêåÁöÑLLMÂü∫Á°Ä‰∏äËΩ¨ÁßªÂà∞‰∏ÄËà¨È¢ÜÂüü‰ªªÂä°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22975', 'title': 'Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text', 'url': 'https://huggingface.co/papers/2601.22975', 'abstract': 'Golden Goose synthesizes unlimited RLVR tasks from unverifiable internet text by creating multiple-choice question-answering versions of fill-in-the-middle tasks, enabling large-scale training and achieving state-of-the-art results in cybersecurity and other domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.', 'score': 21, 'issue_id': 879, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '1964c85e68b783af', 'authors': ['Ximing Lu', 'David Acuna', 'Jaehun Jung', 'Jian Hu', 'Di Zhang', 'Shizhe Diao', 'Yunheng Zou', 'Shaokun Zhang', 'Brandon Cui', 'Mingjie Liu', 'Hyunwoo Kim', 'Prithviraj Ammanabrolu', 'Jan Kautz', 'Yi Dong', 'Yejin Choi'], 'affiliations': ['NVIDIA', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2601.22975.jpg', 'data': {'categories': ['#optimization', '#synthetic', '#rl', '#data', '#reasoning', '#small_models', '#dataset', '#training'], 'emoji': '\U0001fabf', 'ru': {'title': '–°–∏–Ω—Ç–µ–∑ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ–±—É—á–µ–Ω–∏—è –∏–∑ –Ω–µ–≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ Golden Goose –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–¥–∞—á –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLVR) –∏–∑ –Ω–µ–≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Ç–µ–∫—Å—Ç–æ–≤ –ø—É—Ç—ë–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏—Ö –≤ –∑–∞–¥–∞—á–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞—Å—ã—â–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–≤–∞—è –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç GooseReason-0.7M —Å –±–æ–ª–µ–µ —á–µ–º 700 —Ç—ã—Å—è—á–∞–º–∏ –∑–∞–¥–∞—á –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –Ω–∞—É–∫–µ. –ú–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–≥–∞—Ç—ã–µ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∫–æ—Ä–ø—É—Å—ã —Ç–µ–∫—Å—Ç–æ–≤ (—É—á–µ–±–Ω–∏–∫–∏, –≤–µ–±-—Å–∫—Ä–µ–ø–∏–Ω–≥), –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–Ω—å—à–µ –∏—Å–∫–ª—é—á–∞–ª–∏—Å—å –∏–∑ RLVR, –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ 15 –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Golden Goose –≤ –∫–∏–±–µ—Ä—Åecurity –¥–æ–º–µ–Ω–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞, –ø–æ–∑–≤–æ–ª—è—è –Ω–µ–±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ 4B –ø—Ä–µ–≤–∑–æ–π—Ç–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é 7B –º–æ–¥–µ–ª—å.'}, 'en': {'title': 'Unlocking Unlimited RLVR Tasks from Unverifiable Text', 'desc': 'The paper introduces Golden Goose, a method for generating unlimited Reinforcement Learning with Verifiable Rewards (RLVR) tasks from unverifiable internet text. By transforming fill-in-the-middle tasks into multiple-choice question-answering formats, it allows for large-scale training of models, particularly in complex reasoning scenarios. This approach creates a substantial dataset, GooseReason-0.7M, which includes over 0.7 million tasks across various domains like mathematics and programming. The results demonstrate that models trained with this synthesized data achieve state-of-the-art performance, especially in cybersecurity, showcasing the effectiveness of leveraging rich, yet previously unused, internet text for RLVR task generation.'}, 'zh': {'title': 'Âà©Áî®‰∫íËÅîÁΩëÊñáÊú¨ÂêàÊàêÊó†ÈôêRLVR‰ªªÂä°ÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Golden GooseÁöÑÊñπÊ≥ïÔºåÊó®Âú®‰ªé‰∏çÂèØÈ™åËØÅÁöÑ‰∫íËÅîÁΩëÊñáÊú¨‰∏≠ÂêàÊàêÊó†ÈôêÁöÑÂº∫ÂåñÂ≠¶‰π†ÂèØÈ™åËØÅÂ•ñÂä±ÔºàRLVRÔºâ‰ªªÂä°„ÄÇÈÄöËøáÊûÑÂª∫Â°´Á©∫‰ªªÂä°ÁöÑÂ§öÈ°πÈÄâÊã©ÈóÆÁ≠îÁâàÊú¨ÔºåÁ†îÁ©∂ËÄÖËÉΩÂ§üÂà©Áî®‰∏∞ÂØåÁöÑÊé®ÁêÜÊï∞ÊçÆÔºåÁîüÊàêË∂ÖËøá70‰∏á‰∏™‰ªªÂä°ÁöÑÊï∞ÊçÆÈõÜGooseReason„ÄÇËØ•ÊñπÊ≥ïÊúâÊïàÂú∞Ëß£ÂÜ≥‰∫ÜÁé∞ÊúâRLVRÊï∞ÊçÆÁöÑÁì∂È¢àÈóÆÈ¢òÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÂú®Â§çÊùÇÊé®ÁêÜ‰ªªÂä°‰∏äÁöÑË°®Áé∞„ÄÇÊúÄÁªàÔºåGolden GooseÂú®ÁΩëÁªúÂÆâÂÖ®È¢ÜÂüüÁöÑÂÆûÈôÖÂ∫îÁî®‰∏≠ÔºåÂàõÈÄ†‰∫ÜÊñ∞ÁöÑÊúÄÂÖàËøõÁªìÊûúÔºåÂ±ïÁ§∫‰∫ÜÂà©Áî®‰∏∞ÂØåÁöÑ‰∫íËÅîÁΩëÊñáÊú¨Ëá™Âä®Êâ©Â±ïRLVRÊï∞ÊçÆÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21192', 'title': 'Do Reasoning Models Enhance Embedding Models?', 'url': 'https://huggingface.co/papers/2601.21192', 'abstract': "Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.", 'score': 17, 'issue_id': 882, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'b3143d5d0a5a7015', 'authors': ['Wun Yu Chan', 'Shaojin Chen', 'Huihao Jing', 'Kwun Hang Lau', 'Elton Chun-Chai Li', 'Zihao Wang', 'Haoran Li', 'Yangqiu Song'], 'affiliations': ['CSE, HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2601.21192.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#architecture', '#rlhf', '#interpretability', '#training'], 'emoji': 'üîÑ', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ª—É—á—à–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è (embedding models), –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑ LLM, –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏ (RLVR), –Ω–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Hierarchical Representation Similarity Analysis (HRSA), –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–æ–º–µ—Ç—Ä–∏—é –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –∏ –≤—ã—è–≤–∏–ª, —á—Ç–æ RLVR —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—É—é –≥–µ–æ–º–µ—Ç—Ä–∏—é –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏—è, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ. –ö–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –±–∞–∑–æ–≤—ã—Ö –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —è–≤–ª–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–Ω–æ–µ Manifold Realignment. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É—é—Ç, —á—Ç–æ RLVR, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç supervised fine-tuning, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –Ω–µ –ø–µ—Ä–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—è –µ–≥–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ.'}, 'en': {'title': 'No Performance Boost from RLVR-Tuned Embeddings', 'desc': 'This paper investigates whether embedding models that are initialized from Reinforcement Learning with Verifiable Rewards (RLVR) reasoning models perform better than those from base models. The study finds that there is no significant performance improvement, as shown by evaluations on MTEB and BRIGHT datasets. To analyze this, the authors introduce Hierarchical Representation Similarity Analysis (HRSA), which shows that while RLVR changes local geometry, it maintains the overall structure of the representation space. The results indicate that RLVR enhances the alignment of models without altering the fundamental semantic landscape, a phenomenon termed Manifold Realignment.'}, 'zh': {'title': 'Êé®ÁêÜÊ®°ÂûãÊú™ËÉΩÊèêÂçáÂµåÂÖ•ÊÄßËÉΩÁöÑÂ••Áßò', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÈÄöËøáÂº∫ÂåñÂ≠¶‰π†‰∏éÂèØÈ™åËØÅÂ•ñÂä±ÔºàRLVRÔºâËÆ≠ÁªÉÁöÑÊé®ÁêÜÊ®°ÂûãÂàùÂßãÂåñÁöÑÂµåÂÖ•Ê®°ÂûãÊòØÂê¶ËÉΩÊèê‰æõÊõ¥Â•ΩÁöÑËØ≠‰πâË°®Á§∫„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåËøô‰∫õÊ®°ÂûãÂú®ÊÄßËÉΩ‰∏äÂπ∂Ê≤°ÊúâÊòæËëó‰ºò‰∫éÂü∫Á°ÄÊ®°ÂûãÔºåÂ∞ΩÁÆ°Â±ÄÈÉ®Âá†‰ΩïÁªìÊûÑÂèëÁîü‰∫ÜÈáçÁªÑÔºå‰ΩÜÂÖ®Â±ÄÂá†‰ΩïÁªìÊûÑÂíåÁ∫øÊÄßËæìÂá∫‰øùÊåÅ‰∏çÂèò„ÄÇ‰∏∫‰∫ÜËß£ÈáäËøô‰∏ÄÁé∞Ë±°Ôºå‰ΩúËÄÖÊèêÂá∫‰∫ÜÂ±ÇÊ¨°Ë°®Á§∫Áõ∏‰ººÊÄßÂàÜÊûêÔºàHRSAÔºâÊ°ÜÊû∂ÔºåÊè≠Á§∫‰∫ÜÂü∫Á°ÄÊ®°Âûã‰∏éÊé®ÁêÜÂàùÂßãÂåñÊ®°Âûã‰πãÈó¥ÁöÑÂº∫ÂØπÈΩêÁé∞Ë±°„ÄÇÁªìÊûúË°®ÊòéÔºåRLVR‰ºòÂåñ‰∫ÜÁé∞ÊúâËØ≠‰πâÁ©∫Èó¥‰∏≠ÁöÑËΩ®ËøπÔºåËÄå‰∏çÊòØÊ†πÊú¨ÈáçÊûÑËØ≠‰πâÁ©∫Èó¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23265', 'title': 'PaperBanana: Automating Academic Illustration for AI Scientists', 'url': 'https://huggingface.co/papers/2601.23265', 'abstract': '_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.', 'score': 13, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '647cf0b7118ff73f', 'authors': ['Dawei Zhu', 'Rui Meng', 'Yale Song', 'Xiyu Wei', 'Sujian Li', 'Tomas Pfister', 'Jinsung Yoon'], 'affiliations': ['Google Cloud AI Research', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.23265.jpg', 'data': {'categories': ['#open_source', '#science'], 'emoji': 'üé®', 'ru': {'title': '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã', 'desc': 'PaperBanana ‚Äî —ç—Ç–æ –∞–≥–µ–Ω—Ç—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤—ã—Ö –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ç–µ—Ö–Ω–∏–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –°–∏—Å—Ç–µ–º–∞ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏ —Å—Ç–∏–ª—è, —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫—É. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ PaperBananaBench ‚Äî –Ω–∞–±–æ—Ä –∏–∑ 292 —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –¥–∏–∞–≥—Ä–∞–º–º –∏–∑ –ø—É–±–ª–∏–∫–∞—Ü–∏–π NeurIPS 2025, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –∏ —Å—Ç–∏–ª–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ –≤–µ—Ä–Ω–æ—Å—Ç–∏, –ª–∞–∫–æ–Ω–∏—á–Ω–æ—Å—Ç–∏, —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –∏ —ç—Å—Ç–µ—Ç–∏–∫–µ, –∞ —Ç–∞–∫–∂–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä–∞—Ñ–∏–∫–∏ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.'}, 'en': {'title': 'Automating Academic Illustrations with PaperBanana', 'desc': 'PaperBanana is an innovative framework designed to automate the creation of high-quality academic illustrations using advanced vision-language models (VLMs) and image generation techniques. It addresses the challenge of generating publication-ready visuals, which is often a time-consuming task for researchers. The framework utilizes specialized agents to gather references, plan the design, create images, and refine them through self-critique. Evaluation through PaperBananaBench shows that this system outperforms existing methods in terms of accuracy, clarity, and visual appeal, making it a significant advancement in the research workflow.'}, 'zh': {'title': 'PaperBananaÔºöËá™Âä®ÂåñÂ≠¶ÊúØÊèíÂõæÁîüÊàêÁöÑÊú™Êù•', 'desc': 'PaperBananaÊòØ‰∏Ä‰∏™Ëá™Âä®ÂåñÊ°ÜÊû∂ÔºåÊó®Âú®‰ΩøÁî®ÂÖàËøõÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂíåÂõæÂÉèÁîüÊàêÊäÄÊúØÔºåËá™Âä®ÂàõÂª∫Á¨¶ÂêàÂá∫ÁâàÊ†áÂáÜÁöÑÂ≠¶ÊúØÊèíÂõæ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂçèË∞É‰∏ìÈó®ÁöÑ‰ª£ÁêÜÔºåÂÆåÊàêÂèÇËÄÉËµÑÊñôÊ£ÄÁ¥¢„ÄÅÂÜÖÂÆπÂíåÈ£éÊ†ºËßÑÂàí„ÄÅÂõæÂÉèÊ∏≤Êüì‰ª•ÂèäËá™ÊàëÊâπËØÑÁöÑËø≠‰ª£‰ºòÂåñ„ÄÇÊàë‰ª¨ËøòÂºïÂÖ•‰∫ÜPaperBananaBenchÔºåËøôÊòØ‰∏Ä‰∏™ÂåÖÂê´292‰∏™ÊµãËØïÊ°à‰æãÁöÑÊñπÊ≥ïËÆ∫ÂõæË°®ÁöÑÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫Ü‰∏çÂêåÁ†îÁ©∂È¢ÜÂüüÂíåÊèíÂõæÈ£éÊ†º„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPaperBananaÂú®ÂáÜÁ°ÆÊÄß„ÄÅÁÆÄÊ¥ÅÊÄß„ÄÅÂèØËØªÊÄßÂíåÁæéËßÇÊÄßÊñπÈù¢Âùá‰ºò‰∫éÁé∞ÊúâÁöÑ‰∏ªÊµÅÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23184', 'title': 'ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought', 'url': 'https://huggingface.co/papers/2601.23184', 'abstract': 'ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.', 'score': 12, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'a0cd48968fea2d79', 'authors': ['Fanmeng Wang', 'Haotian Liu', 'Guojiang Zhao', 'Hongteng Xu', 'Zhifeng Gao'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'DP Technology', 'Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.23184.jpg', 'data': {'categories': ['#open_source', '#optimization', '#multimodal', '#training', '#reasoning', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–°–∂–∞—Ç–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ —Å–∫—Ä—ã—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ', 'desc': 'ReGuLaR –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã–π –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è —Å–∂–∞—Ç–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –Ø–≤–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –ø–ª–æ—Ç–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ç–µ—Ä—è–º–∏, —Ä–µ—à–∞—è –ø—Ä–æ–±–ª–µ–º—É –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–æ–¥–∞—Ö —Å–∫—Ä—ã—Ç–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ ReGuLaR –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∫–∞–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Å–∫—Ä—ã—Ç–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Ç–∞–∫ –∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π Chain-of-Thought –ø–æ–¥—Ö–æ–¥.'}, 'en': {'title': 'Efficient Latent Reasoning with Visual Guidance', 'desc': "ReGuLaR presents a new approach to latent reasoning by integrating a variational auto-encoding framework that compresses reasoning processes into a latent space. It utilizes rendered images of explicit reasoning chains to guide the compression, ensuring that performance is maintained while reducing computational redundancy. This method allows for efficient sampling of latent states based on previous reasoning, enhancing the model's ability to learn effectively. Experimental results show that ReGuLaR outperforms existing methods in both efficiency and reasoning capabilities, even exceeding traditional Chain-of-Thought techniques."}, 'zh': {'title': 'ÂèòÂàÜÊé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥ÔºöReGuLaR', 'desc': 'ReGuLaRÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèòÂàÜËá™ÁºñÁ†ÅÊ°ÜÊû∂ÔºåÂ∞ÜÊé®ÁêÜËøáÁ®ãÂéãÁº©Âà∞ÊΩúÂú®Á©∫Èó¥ÔºåÂêåÊó∂ÈÄöËøáÂõæÂÉèÊ∏≤ÊüìÁöÑÊòæÂºèÊé®ÁêÜÈìæËøõË°åÊåáÂØºÔºå‰ª•‰øùÊåÅÊÄßËÉΩ„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÁé∞ÊúâÊΩúÂú®Êé®ÁêÜÊñπÊ≥ïÂú®ÂéãÁº©ËøáÁ®ã‰∏≠ÊÄßËÉΩ‰∏ãÈôçÁöÑÈóÆÈ¢ò„ÄÇÈÄöËøáÂ∞ÜÊòæÂºèÊé®ÁêÜÈìæÊ∏≤Êüì‰∏∫ÂõæÂÉèÔºåÊèêÂèñÂØÜÈõÜÁöÑËßÜËßâ-ËØ≠‰πâË°®Á§∫Êù•ËßÑËåÉÂêéÈ™åÂàÜÂ∏ÉÔºå‰ªéËÄåÂÆûÁé∞È´òÊïàÂéãÁº©ÂíåÊúÄÂ∞è‰ø°ÊÅØÊçüÂ§±„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåReGuLaRÂú®ËÆ°ÁÆóÊïàÁéáÂíåÊé®ÁêÜÊïàÊûú‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ïÔºåÁîöËá≥Âú®Â§öÊ®°ÊÄÅÊé®ÁêÜ‰∏≠Ë∂ÖË∂ä‰∫ÜÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23182', 'title': 'FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation', 'url': 'https://huggingface.co/papers/2601.23182', 'abstract': 'Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler\'s dynamic frequency-domain sliding window mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a "structure-to-detail" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.', 'score': 12, 'issue_id': 873, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '7ed8f5f3b04d91e1', 'authors': ['Siyang He', 'Qiqi Wang', 'Xiaoran Liu', 'Hongnan Ma', 'Yiwei Shi', 'Yuerong Song', 'Ying Zhu', 'Tianyi Liang', 'Zengfeng Huang', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'OpenMOSS Team', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2601.23182.jpg', 'data': {'categories': ['#optimization', '#diffusion'], 'emoji': 'üåä', 'ru': {'title': '–û—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫ –¥–µ—Ç–∞–ª—è–º: —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–æ–≤–µ–¥—ë–Ω –∞–Ω–∞–ª–∏–∑ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏–π —á—Ç–æ –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∫–æ–¥–∏—Ä—É—é—Ç –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞ –∏ –¥–ª–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –∞ –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ FourierSampler - –º–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ö–∞–Ω–∏–∑–º–∞ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫ –¥–µ—Ç–∞–ª—è–º. –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö LLADA –∏ SDAR —Å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º 20.4% –∏ 16.0% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. FourierSampler –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –≤–∫–ª—é—á–∞—è Llama3.1-8B-Instruct.'}, 'en': {'title': 'Unlocking Structure and Detail in Language Generation', 'desc': 'This paper explores how diffusion language models (dLLMs) can be improved by analyzing their frequency components. It finds that low-frequency components capture the overall structure and long-range relationships in the data, while high-frequency components focus on finer, local details. The authors introduce a new method called FourierSampler, which uses a sliding window in the frequency domain to enhance the generation process by balancing structure and detail. This approach shows significant performance improvements over existing methods and even outperforms some autoregressive models.'}, 'zh': {'title': 'È¢ëÂüüÂàÜÊûêÂä©ÂäõÁîüÊàêÊ®°ÂûãÁöÑÁªìÊûÑ‰∏éÁªÜËäÇ‰ºòÂåñ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàdLLMsÔºâÁöÑÈ¢ëÂüüÁâπÊÄßÔºåÊè≠Á§∫‰∫Ü‰ΩéÈ¢ëÊàêÂàÜ‰∏ªË¶ÅÁºñÁ†ÅÂÖ®Â±ÄÁªìÊûÑ‰ø°ÊÅØÔºåËÄåÈ´òÈ¢ëÊàêÂàÜÂàôÊçïÊçâÂ±ÄÈÉ®ÁªÜËäÇ„ÄÇÈÄöËøáËøôÁßçÂàÜÊûêÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜFourierSamplerÔºåÂÆÉÂà©Áî®È¢ëÂüüÊªëÂä®Á™óÂè£Êú∫Âà∂Âä®ÊÄÅÂºïÂØºÊ®°ÂûãÂÆûÁé∞‰ªéÁªìÊûÑÂà∞ÁªÜËäÇÁöÑÁîüÊàê„ÄÇFourierSamplerÂú®LLADAÂíåSDAR‰∏äË°®Áé∞‰ºòÂºÇÔºåÁõ∏ÊØîÂÖ∂‰ªñÊé®ÁêÜÂ¢ûÂº∫Á≠ñÁï•ÔºåLLaDA1.5-8BÂíåLLaDA-8B-InstructÂàÜÂà´ÊèêÈ´ò‰∫Ü20.4%Âíå16.0%„ÄÇÊ≠§Â§ñÔºåÂÆÉÁöÑÊÄßËÉΩÊòæËëóË∂ÖËøá‰∫ÜÂêåÁ≠âËßÑÊ®°ÁöÑËá™ÂõûÂΩíÊ®°ÂûãÔºåÂ¶ÇLlama3.1-8B-Instruct„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22491', 'title': 'SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization', 'url': 'https://huggingface.co/papers/2601.22491', 'abstract': "Sweet Spot Learning (SSL) introduces a novel reinforcement learning framework that uses tiered rewards to guide agent optimization toward optimal regions of the solution space, improving sample efficiency and cross-task transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.", 'score': 11, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'b16b81a5b4104b74', 'authors': ['Jinyang Wu', 'Changpeng Yang', 'Yuhao Shen', 'Fangzhi Xu', 'Bolin Ni', 'Chonghua Liao', 'Yuchen Liu', 'Hongzhen Wang', 'Shuai Nie', 'Shuai Zhang', 'Haoran Luo', 'Jiaming Xu'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'Nanyang Technological University', 'Tsinghua University', 'Xiaomi Corporation', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22491.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#rl', '#optimization', '#agents', '#reasoning'], 'emoji': 'üéæ', 'ru': {'title': '–¢–æ—á–Ω—ã–π —É–¥–∞—Ä –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ä–µ—à–µ–Ω–∏–π: –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤', 'desc': 'Sweet Spot Learning (SSL) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Å –±–∏–Ω–∞—Ä–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏, SSL –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å, —Ä–∞–∑–ª–∏—á–∞—è –∫–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–∞–∂–µ –ø—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ –¥–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –º–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∏ —É—Å–∏–ª–∏–≤–∞–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞ –∫ —à—É–º—É –ø—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏ –¥–æ 2.5x —Ä–∞–∑ –∏ —Ö–æ—Ä–æ—à—É—é –ø–µ—Ä–µ–¥–∞—á—É –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏.'}, 'en': {'title': 'Unlocking Optimal Learning with Tiered Rewards', 'desc': "Sweet Spot Learning (SSL) is a new approach in reinforcement learning that uses tiered rewards to help agents find the best solutions more efficiently. Instead of just giving a simple yes or no reward, SSL provides different levels of rewards based on how close the agent's actions are to the optimal solution. This method not only improves how quickly agents learn but also helps them apply what they've learned to different tasks. Experiments show that SSL significantly boosts performance, making agents smarter and more adaptable across various challenges."}, 'zh': {'title': 'ÁîúÁÇπÂ≠¶‰π†Ôºö‰ºòÂåñÊô∫ËÉΩ‰ΩìÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÁîúÁÇπÂ≠¶‰π†ÔºàSSLÔºâÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÈÄöËøáÂàÜÂ±ÇÂ•ñÂä±Êù•ÂºïÂØºÊô∫ËÉΩ‰Ωì‰ºòÂåñÔºåÂ∏ÆÂä©ÂÖ∂Êõ¥ÊúâÊïàÂú∞ÊâæÂà∞Ëß£ÂÜ≥ÊñπÊ°àÁöÑÊúÄ‰Ω≥Âå∫Âüü„ÄÇÁé∞ÊúâÊñπÊ≥ïÈÄöÂ∏∏‰ΩøÁî®‰∫åÂÖÉÂ•ñÂä±ÔºåÊó†Ê≥ïÊçïÊçâÂà∞ÂÆûÁé∞Áõ∏ÂêåÁªìÊûúÁöÑËΩ®Ëøπ‰πãÈó¥ÁöÑË¥®ÈáèÂ∑ÆÂºÇÔºåÂøΩËßÜ‰∫ÜËß£ÂÜ≥ÊñπÊ°àÁ©∫Èó¥‰∏≠ÁöÑÊΩúÂú®Â§öÊ†∑ÊÄß„ÄÇSSLÁöÑÊ†∏ÂøÉÁêÜÂøµÊòØÈÄöËøáÈÄêÊ≠•Â¢ûÂº∫ÁöÑÂàÜÂ±ÇÂ•ñÂä±ÔºåÊåáÂØºÁ≠ñÁï•ÊúùÂêëËß£ÂÜ≥ÊñπÊ°àÁ©∫Èó¥ÁöÑ‚ÄúÁîúÁÇπ‚ÄùÂå∫Âüü„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåSSLÂú®Â§ö‰∏™‰ªªÂä°‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÊ†∑Êú¨ÊïàÁéáÂíåË∑®‰ªªÂä°ÁöÑËøÅÁßªËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.20218', 'title': 'DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment', 'url': 'https://huggingface.co/papers/2601.20218', 'abstract': 'DenseGRPO addresses sparse reward problems in flow matching models by introducing dense rewards for intermediate denoising steps and adaptive exploration calibration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce DenseGRPO, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.', 'score': 9, 'issue_id': 871, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'}, 'hash': 'ef07baf88e11172b', 'authors': ['Haoyou Deng', 'Keyu Yan', 'Chaojie Mao', 'Xiang Wang', 'Yu Liu', 'Changxin Gao', 'Nong Sang'], 'affiliations': ['National Key Laboratory of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2601.20218.jpg', 'data': {'categories': ['#benchmark', '#rlhf', '#diffusion', '#alignment', '#optimization', '#multimodal', '#training'], 'emoji': 'üéØ', 'ru': {'title': '–ü–ª–æ—Ç–Ω—ã–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –¥–µ–Ω–æ–π–∑–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': 'DenseGRPO —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –≤ –º–æ–¥–µ–ª—è—Ö flow matching –ø—É—Ç—ë–º –≤–≤–µ–¥–µ–Ω–∏—è –ø–ª–æ—Ç–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —à–∞–≥–∞ –¥–µ–Ω–æ–π–∑–∏—Ä–æ–≤–∞–Ω–∏—è. –í–º–µ—Å—Ç–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∫–æ –≤—Å–µ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–µ–Ω–æ–π–∑–∏—Ä–æ–≤–∞–Ω–∏—è, –º–µ—Ç–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ—à–∞–≥–æ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —á–∏—Å—Ç—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—É—Ç—ë–º —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å—é —à—É–º–∞ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –∏ –≤–∞–∂–Ω–æ—Å—Ç—å –ø–ª–æ—Ç–Ω—ã—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞.'}, 'en': {'title': 'Dense Rewards for Better Flow Matching', 'desc': 'DenseGRPO is a new framework designed to tackle the sparse reward problem in flow matching models used for text-to-image generation. It introduces dense rewards for each intermediate denoising step, allowing for a more accurate evaluation of contributions at each stage. By predicting step-wise reward gains and applying a reward model to intermediate clean images, DenseGRPO aligns feedback signals with individual contributions effectively. Additionally, it calibrates the exploration space by adjusting stochasticity based on the noise intensity, ensuring optimal exploration throughout the denoising process.'}, 'zh': {'title': 'DenseGRPOÔºöËß£ÂÜ≥Á®ÄÁñèÂ•ñÂä±ÁöÑÂàõÊñ∞Ê°ÜÊû∂', 'desc': 'DenseGRPOÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥ÊµÅÂåπÈÖçÊ®°Âûã‰∏≠ÁöÑÁ®ÄÁñèÂ•ñÂä±ÈóÆÈ¢ò„ÄÇÈÄöËøá‰∏∫ÊØè‰∏™ÂéªÂô™Ê≠•È™§ÂºïÂÖ•ÂØÜÈõÜÂ•ñÂä±ÔºåDenseGRPOËÉΩÂ§üÊõ¥Â•ΩÂú∞ËØÑ‰º∞ÊØè‰∏™Ê≠•È™§ÁöÑÁªÜÁ≤íÂ∫¶Ë¥°ÁåÆ„ÄÇËØ•ÊñπÊ≥ïÂåÖÊã¨‰∏§‰∏™ÂÖ≥ÈîÆÁªÑ‰ª∂ÔºöÈ¶ñÂÖàÔºåÈ¢ÑÊµãÊØè‰∏™ÂéªÂô™Ê≠•È™§ÁöÑÈÄêÊ≠•Â•ñÂä±Â¢ûÁõäÔºå‰ª•Á°Æ‰øùÂèçÈ¶à‰ø°Âè∑‰∏é‰∏™Âà´Ê≠•È™§ÁöÑË¥°ÁåÆÂØπÈΩêÔºõÂÖ∂Ê¨°ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÂ•ñÂä±ÊÑüÁü•ÊñπÊ°àÔºåÈÄöËøáËá™ÈÄÇÂ∫îË∞ÉÊï¥ÈöèÊú∫ÊÄßÊ≥®ÂÖ•ÔºåÊ†°ÂáÜÊé¢Á¥¢Á©∫Èó¥Ôºå‰ªéËÄå‰ºòÂåñËÆ≠ÁªÉÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21716', 'title': 'DreamActor-M2: Universal Character Image Animation via Spatiotemporal In-Context Learning', 'url': 'https://huggingface.co/papers/2601.21716', 'abstract': 'DreamActor-M2 presents a universal character animation framework that addresses motion injection trade-offs and pose prior limitations through in-context learning and self-bootstrapped data synthesis for improved generalization across diverse characters.  \t\t\t\t\tAI-generated summary \t\t\t\t Character image animation aims to synthesize high-fidelity videos by transferring motion from a driving sequence to a static reference image. Despite recent advancements, existing methods suffer from two fundamental challenges: (1) suboptimal motion injection strategies that lead to a trade-off between identity preservation and motion consistency, manifesting as a "see-saw", and (2) an over-reliance on explicit pose priors (e.g., skeletons), which inadequately capture intricate dynamics and hinder generalization to arbitrary, non-humanoid characters. To address these challenges, we present DreamActor-M2, a universal animation framework that reimagines motion conditioning as an in-context learning problem. Our approach follows a two-stage paradigm. First, we bridge the input modality gap by fusing reference appearance and motion cues into a unified latent space, enabling the model to jointly reason about spatial identity and temporal dynamics by leveraging the generative prior of foundational models. Second, we introduce a self-bootstrapped data synthesis pipeline that curates pseudo cross-identity training pairs, facilitating a seamless transition from pose-dependent control to direct, end-to-end RGB-driven animation. This strategy significantly enhances generalization across diverse characters and motion scenarios. To facilitate comprehensive evaluation, we further introduce AW Bench, a versatile benchmark encompassing a wide spectrum of characters types and motion scenarios. Extensive experiments demonstrate that DreamActor-M2 achieves state-of-the-art performance, delivering superior visual fidelity and robust cross-domain generalization. Project Page: https://grisoon.github.io/DreamActor-M2/', 'score': 8, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '7b143dcb2fb5153f', 'authors': ['Mingshuang Luo', 'Shuang Liang', 'Zhengkun Rong', 'Yuxuan Luo', 'Tianshu Hu', 'Ruibing Hou', 'Hong Chang', 'Yong Li', 'Yuan Zhang', 'Mingyuan Gao'], 'affiliations': ['ByteDance Intelligent Creation', 'Key Lab of Intell. Info. Process., ICT, CAS', 'Southeast University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2601.21716.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#benchmark', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –±–µ–∑ —è–≤–Ω—ã—Ö –ø–æ–∑ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ', 'desc': 'DreamActor-M2 ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å—é –¥–≤–∏–∂–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–µ–º –∫–∞–∫ –ø—Ä–æ–±–ª–µ–º—É –æ–±—É—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –æ–±—ä–µ–¥–∏–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–Ω–µ—à–Ω–µ–º –≤–∏–¥–µ –∏ –¥–≤–∏–∂–µ–Ω–∏–∏ –≤ –µ–¥–∏–Ω–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–∞–º–æ–∑–∞–≥—Ä—É–∂–∞–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–π—Ç–∏ –æ—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–µ–ª–µ—Ç–æ–≤ –∫ –ø—Ä—è–º–æ–π RGB-—É–ø—Ä–∞–≤–ª—è–µ–º–æ–π –∞–Ω–∏–º–∞—Ü–∏–∏. –°–∏—Å—Ç–µ–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing Character Animation with DreamActor-M2', 'desc': "DreamActor-M2 is a novel framework for character animation that improves how motion is applied to static images. It tackles two main issues: the balance between keeping a character's identity and ensuring smooth motion, and the limitations of using fixed pose references like skeletons. By using in-context learning, it combines visual and motion information into a single representation, allowing for better understanding of both identity and movement. Additionally, it employs a self-bootstrapped data synthesis method to create diverse training examples, enhancing its ability to animate various character types effectively."}, 'zh': {'title': 'Ê¢¶ÂπªÊºîÂëòM2ÔºöÈÄöÁî®ËßíËâ≤Âä®ÁîªÁöÑÊñ∞Á∫™ÂÖÉ', 'desc': 'DreamActor-M2ÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑËßíËâ≤Âä®ÁîªÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøá‰∏ä‰∏ãÊñáÂ≠¶‰π†ÂíåËá™ÊàëÂºïÂØºÁöÑÊï∞ÊçÆÂêàÊàêÊù•Ëß£ÂÜ≥ËøêÂä®Ê≥®ÂÖ•ÁöÑÊùÉË°°ÂíåÂßøÊÄÅÂÖàÈ™åÁöÑÂ±ÄÈôêÊÄßÔºå‰ªéËÄåÊèêÈ´òÂØπÂ§öÊ†∑ÂåñËßíËâ≤ÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÈááÁî®‰∏§Èò∂ÊÆµÁöÑÊñπÊ≥ïÔºåÈ¶ñÂÖàÂ∞ÜÂèÇËÄÉÂ§ñËßÇÂíåËøêÂä®Á∫øÁ¥¢ËûçÂêàÂà∞‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊΩúÂú®Á©∫Èó¥‰∏≠Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂÖ±ÂêåÊé®ÁêÜÁ©∫Èó¥Ë∫´‰ªΩÂíåÊó∂Èó¥Âä®ÊÄÅ„ÄÇÂÖ∂Ê¨°ÔºåDreamActor-M2ÂºïÂÖ•‰∫Ü‰∏ÄÁßçËá™ÊàëÂºïÂØºÁöÑÊï∞ÊçÆÂêàÊàêÁÆ°ÈÅìÔºåÂàõÂª∫‰º™Ë∑®Ë∫´‰ªΩËÆ≠ÁªÉÂØπÔºå‰øÉËøõ‰ªé‰æùËµñÂßøÊÄÅÁöÑÊéßÂà∂Âà∞Áõ¥Êé•ÁöÑÁ´ØÂà∞Á´ØRGBÈ©±Âä®Âä®ÁîªÁöÑÊó†ÁºùËøáÊ∏°„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDreamActor-M2Âú®ËßÜËßâ‰øùÁúüÂ∫¶ÂíåË∑®È¢ÜÂüüÊ≥õÂåñËÉΩÂäõÊñπÈù¢ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.13097', 'title': 'RM -RF: Reward Model for Run-Free Unit Test Evaluation', 'url': 'https://huggingface.co/papers/2601.13097', 'abstract': 'RM-RF is a lightweight reward model that predicts execution outcomes from source code alone, offering faster and more cost-effective evaluation than traditional compile-and-run methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.', 'score': 8, 'issue_id': 874, 'pub_date': '2026-01-19', 'pub_date_card': {'ru': '19 —è–Ω–≤–∞—Ä—è', 'en': 'January 19', 'zh': '1Êúà19Êó•'}, 'hash': '3b4716c0678cae9f', 'authors': ['Elena Bruches', 'Daniil Grebenkin', 'Mikhail Klementev', 'Vadim Alperovich', 'Roman Derunets', 'Dari Baturova', 'Georgy Mkrtchyan', 'Oleg Sedukhin', 'Ivan Bondarenko', 'Nikolay Bushkov', 'Stanislav Moiseev'], 'affiliations': ['Novosibirsk State University, Novosibirsk, Russia', 'Siberian Neuronets LLC, Novosibirsk, Russia', 'T-Technologies, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2601.13097.jpg', 'data': {'categories': ['#optimization', '#training', '#science', '#plp', '#dataset', '#multilingual', '#open_source', '#small_models', '#data'], 'emoji': '‚ö°', 'ru': {'title': '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Å—Ç–æ–≤ –±–µ–∑ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —ç–∫–æ–Ω–æ–º–∏—è', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ RM-RF ‚Äî –ª—ë–≥–∫–∞—è –º–æ–¥–µ–ª—å-–Ω–∞–≥—Ä–∞–¥–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥—É–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –ø—Ä—è–º–æ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞, –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ –∏ –∑–∞–ø—É—Å–∫–∞. –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–∞: —É—Å–ø–µ—à–Ω—É—é –∫–æ–º–ø–∏–ª—è—Ü–∏—é –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–æ–∫—Ä—ã—Ç–∏—è –∫–æ–¥–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è —É–±–∏–π—Å—Ç–≤–∞ –º—É—Ç–∞—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç (Java, Python, Go) —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –¥–æ—Å—Ç–∏–≥–ª–∏ —Å—Ä–µ–¥–Ω–µ–≥–æ F1-–ø–æ–∫–∞–∑–∞—Ç–µ–ª—è 0,69 –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–µ–º–µ–π—Å—Ç–≤ –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è (zero-shot, –ø–æ–ª–Ω–æ–µ fine-tuning, LoRA). –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º –ø–æ–¥—Ö–æ–¥–æ–º ¬´—Å–∫–æ–º–ø–∏–ª–∏—Ä—É–π –∏ –∑–∞–ø—É—Å—Ç–∏¬ª, RM-RF –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.'}, 'en': {'title': 'Fast and Cost-Effective Test Evaluation with RM-RF', 'desc': 'RM-RF is a novel lightweight reward model designed to evaluate automatically generated unit tests without the need for execution. It predicts three key execution outcomes based solely on source and test code: successful compilation and execution, code coverage improvement, and mutation kill rate enhancement. The model is trained on a diverse dataset that includes multiple programming languages, ensuring broad applicability. By offering faster and more cost-effective evaluations than traditional methods, RM-RF facilitates efficient feedback for large-scale test generation and reinforcement learning in code optimization.'}, 'zh': {'title': 'ËΩªÈáèÁ∫ßÂ•ñÂä±Ê®°ÂûãRM-RFÔºöÂø´ÈÄüËØÑ‰º∞Ëá™Âä®ÁîüÊàêÁöÑÂçïÂÖÉÊµãËØï', 'desc': 'RM-RFÊòØ‰∏ÄÁßçËΩªÈáèÁ∫ßÁöÑÂ•ñÂä±Ê®°ÂûãÔºåËÉΩÂ§ü‰ªÖÈÄöËøáÊ∫ê‰ª£Á†ÅÈ¢ÑÊµãÊâßË°åÁªìÊûúÔºå‰ªéËÄåÊØî‰º†ÁªüÁöÑÁºñËØëÂíåËøêË°åÊñπÊ≥ïÊõ¥Âø´‰∏îÊõ¥ÂÖ∑ÊàêÊú¨ÊïàÁõä„ÄÇËØ•Ê®°ÂûãÂèØ‰ª•Âú®‰∏çÂèçÂ§çÁºñËØëÂíåÊâßË°åÂÄôÈÄâÊµãËØïÁöÑÊÉÖÂÜµ‰∏ãÔºåÈ¢ÑÊµã‰∏â‰∏™ÊâßË°åÁõ∏ÂÖ≥ÁöÑ‰ø°Âè∑ÔºöÊµãËØïÂ•ó‰ª∂ÊòØÂê¶ÊàêÂäüÁºñËØëÂíåËøêË°å„ÄÅÁîüÊàêÁöÑÊµãËØïÁî®‰æãÊòØÂê¶ÊèêÈ´ò‰∫Ü‰ª£Á†ÅË¶ÜÁõñÁéáÔºå‰ª•ÂèäÊòØÂê¶ÊîπÂñÑ‰∫ÜÁ™ÅÂèòÊùÄÊ≠ªÁéá„ÄÇ‰∏∫‰∫ÜËÆ≠ÁªÉÂíåËØÑ‰º∞RM-RFÔºåÊàë‰ª¨ÁªÑÂª∫‰∫Ü‰∏Ä‰∏™Â§öËØ≠Ë®ÄÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨Java„ÄÅPythonÂíåGoÁöÑÁÑ¶ÁÇπÊñá‰ª∂„ÄÅÊµãËØïÊñá‰ª∂ÂíåÂÄôÈÄâÊµãËØïÊ∑ªÂä†ÔºåÂπ∂ÈÄöËøáÂü∫‰∫éÊâßË°åÁöÑÁÆ°ÈÅìËøõË°åÊ†áÊ≥®„ÄÇ‰∏é‰º†ÁªüÁöÑÁºñËØëÂíåËøêË°åÂ∑•ÂÖ∑Áõ∏ÊØîÔºåRM-RFÊòæËëóÈôç‰Ωé‰∫ÜÂª∂ËøüÂíåÂü∫Á°ÄËÆæÊñΩÊàêÊú¨ÔºåÂêåÊó∂Êèê‰æõ‰∫ÜÁ´û‰∫âÂäõÁöÑÈ¢ÑÊµãÂáÜÁ°ÆÊÄßÔºåËÉΩÂ§ü‰∏∫Â§ßËßÑÊ®°ÊµãËØïÁîüÊàêÂíåÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑ‰ª£Á†Å‰ºòÂåñÊèê‰æõÂø´ÈÄü„ÄÅÂèØÊâ©Â±ïÁöÑÂèçÈ¶à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22642', 'title': 'Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification', 'url': 'https://huggingface.co/papers/2601.22642', 'abstract': 'A formal logic verification-guided framework dynamically interleaves symbolic verification with natural language generation to improve reasoning accuracy and reduce errors in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.', 'score': 7, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '7cfd51e101bac332', 'authors': ['Chuxue Cao', 'Jinluan Yang', 'Haoran Li', 'Kunhao Pan', 'Zijian Zhao', 'Zhengyu Chen', 'Yuchen Tian', 'Lijun Wu', 'Conghui He', 'Sirui Han', 'Yike Guo'], 'affiliations': ['Hong Kong Baptist University', 'Hong Kong University of Science and Technology', 'Shanghai Artificial Intelligence Laboratory', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22642.jpg', 'data': {'categories': ['#benchmark', '#training', '#rlhf'], 'emoji': '‚öôÔ∏è', 'ru': {'title': '–§–æ—Ä–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–∫ –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–∞—è —Å–∏–ª–∞ –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —Ñ–æ—Ä–º–∞–ª—å–Ω—É—é –ª–æ–≥–∏—á–µ—Å–∫—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ—Ç —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –ø—Ä–æ—Ü–µ—Å—Å–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –Ω–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö —Ç–æ–ª—å–∫–æ –ø–æ—Å—Ç—Ñ–∞–∫—Ç—É–º –≤–∞–ª–∏–¥–∞—Ü–∏—é, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∞–∫—Ç–∏–≤–Ω–æ —à—Ç—Ä–∞—Ñ—É–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –ú–µ—Ç–æ–¥ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω —á–µ—Ä–µ–∑ –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—é—â–∏–π supervised fine-tuning —Å –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ policy optimization.'}, 'en': {'title': 'Enhancing LLM Reasoning with Real-Time Logic Verification', 'desc': 'This paper presents a new framework that combines formal logic verification with natural language generation to enhance the reasoning capabilities of large language models (LLMs). By integrating symbolic verification into the generation process, the framework provides immediate feedback to identify and correct logical errors in real-time. Unlike previous methods that only validate reasoning after it has occurred, this approach actively penalizes mistakes during the reasoning process. The authors demonstrate that their models, trained with this framework, significantly outperform existing models on various reasoning tasks, showcasing the effectiveness of formal verification in improving LLM performance.'}, 'zh': {'title': 'ÂΩ¢ÂºèÈÄªËæëÈ™åËØÅÊèêÂçáËØ≠Ë®ÄÊ®°ÂûãÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éÂΩ¢ÂºèÈÄªËæëÈ™åËØÅÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊé®ÁêÜÂáÜÁ°ÆÊÄßÂπ∂ÂáèÂ∞ëÈîôËØØ„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÂΩ¢ÂºèÁ¨¶Âè∑È™åËØÅ‰∏éËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêÂä®ÊÄÅ‰∫§ÁªáÔºåÂÆûÊó∂ÂèçÈ¶à‰ª•Ê£ÄÊµãÂíåÁ∫†Ê≠£ÈîôËØØ„ÄÇ‰∏é‰ª•ÂæÄÁöÑÁ•ûÁªèÁ¨¶Âè∑ÊñπÊ≥ï‰∏çÂêåÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Êé®ÁêÜÈìæ‰∏≠‰∏ªÂä®ÊÉ©ÁΩö‰∏≠Èó¥Ë∞¨ËØØ„ÄÇÈÄöËøáÊñ∞È¢ñÁöÑ‰∏§Èò∂ÊÆµËÆ≠ÁªÉÊµÅÁ®ãÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÈ™åËØÅ‰∫ÜÂΩ¢ÂºèÈ™åËØÅ‰Ωú‰∏∫ÊèêÂçáLLMÊé®ÁêÜÊÄßËÉΩÁöÑÂèØÊâ©Â±ïÊú∫Âà∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22636', 'title': 'Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling', 'url': 'https://huggingface.co/papers/2601.22636', 'abstract': 'A scaling-aware risk estimation method called SABER is introduced for predicting large-scale adversarial vulnerability in language models through Best-of-N sampling, enabling accurate assessment with reduced computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.', 'score': 7, 'issue_id': 873, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'c8d4ce6d36364c09', 'authors': ['Mingqian Feng', 'Xiaodong Liu', 'Weiwei Yang', 'Chenliang Xu', 'Christopher White', 'Jianfeng Gao'], 'affiliations': ['Microsoft Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2601.22636.jpg', 'data': {'categories': ['#alignment', '#security'], 'emoji': 'üîì', 'ru': {'title': '–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Å–ø–æ—Å–æ–± –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â—É—é —É—è–∑–≤–∏–º–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ SABER –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ adversarial –∞—Ç–∞–∫–∞–º –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–Ω–æ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º sampling. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–µ—Ç–∞-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —É—Å–ø–µ—Ö–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –≤—ã–≤–æ–¥—è—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π scaling law, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —ç–∫—Å—Ç—Ä–∞–ø–æ–ª–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —É—Å–ø–µ—à–Ω—ã—Ö –∞—Ç–∞–∫ —Å –º–∞–ª–æ–≥–æ –±—é–¥–∂–µ—Ç–∞ –Ω–∞ –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è N. –ú–µ—Ç–æ–¥–∏–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –∞—Ç–∞–∫–∏ –ø—Ä–∏ —Ç—ã—Å—è—á–µ –ø–æ–ø—ã—Ç–æ–∫, –∏—Å–ø–æ–ª—å–∑—É—è –≤—Å–µ–≥–æ 100 –ø—Ä–∏–º–µ—Ä–æ–≤, —Å –æ—à–∏–±–∫–æ–π –≤ 1.66 –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –ø—É–Ω–∫—Ç–∞ –≤–º–µ—Å—Ç–æ 12.04 –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏, –∫–∞–∂—É—â–∏–µ—Å—è –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø—Ä–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–µ, –º–æ–≥—É—Ç –±—ã—Ç—å —É—è–∑–≤–∏–º—ã –∫ –±—ã—Å—Ç—Ä–æ–º—É –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–º—É —É—Å–∏–ª–µ–Ω–∏—é —Ä–∏—Å–∫–∞ –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö adversarial –∞—Ç–∞–∫.'}, 'en': {'title': 'SABER: Scalable Risk Estimation for Safer Language Models', 'desc': 'The paper introduces SABER, a method for estimating the risk of adversarial attacks on large language models (LLMs) using a Best-of-N sampling approach. Traditional evaluations often underestimate risks by using limited sampling, but SABER allows for accurate predictions of vulnerability by modeling success probabilities with a Beta distribution. This method significantly reduces computational costs while providing reliable estimates of attack success rates, even with a small number of samples. The findings highlight that models may seem safe under standard tests but can show increased vulnerability when subjected to extensive adversarial probing.'}, 'zh': {'title': 'SABERÔºöÈ´òÊïàËØÑ‰º∞ËØ≠Ë®ÄÊ®°ÂûãÂØπÊäóÊÄßÈ£éÈô©ÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫SABERÁöÑÈ£éÈô©‰º∞ËÆ°ÊñπÊ≥ïÔºåÁî®‰∫éÈ¢ÑÊµãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ßËßÑÊ®°ÂØπÊäóÊÄßÊîªÂáª‰∏ãÁöÑËÑÜÂº±ÊÄß„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáBest-of-NÈááÊ†∑ÂÆûÁé∞‰∫ÜÈ´òÊïàÁöÑÈ£éÈô©ËØÑ‰º∞ÔºåÈôç‰Ωé‰∫ÜËÆ°ÁÆóÊàêÊú¨„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊîªÂáªÊàêÂäüÁéáÈöèÁùÄÈáçÂ§çÈááÊ†∑ËÄåÂ¢ûÂä†ÔºåËÄåSABERËÉΩÂ§üÈÄöËøáË¥ùÂ°îÂàÜÂ∏ÉÂª∫Ê®°Ê†∑Êú¨Á∫ßÊàêÂäüÊ¶ÇÁéáÔºå‰ªéËÄåÂèØÈù†Âú∞Êé®Êñ≠Â§ßËßÑÊ®°ÊîªÂáªÁöÑÊàêÂäüÁéá„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåSABERÂú®Â∞èÊ†∑Êú¨‰∏ãÁöÑ‰º∞ËÆ°ËØØÂ∑ÆÊòæËëó‰Ωé‰∫éÂü∫Á∫øÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßç‰ΩéÊàêÊú¨„ÄÅÂèØÊâ©Â±ïÁöÑËØ≠Ë®ÄÊ®°ÂûãÂÆâÂÖ®ËØÑ‰º∞ÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21957', 'title': 'PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing', 'url': 'https://huggingface.co/papers/2601.21957', 'abstract': "A compact vision-language model achieves state-of-the-art accuracy on document understanding tasks while maintaining efficiency through specialized benchmarking and extended functionality.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR", 'score': 7, 'issue_id': 873, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'dbdb8e8fbbdbcc92', 'authors': ['Cheng Cui', 'Ting Sun', 'Suyin Liang', 'Tingquan Gao', 'Zelun Zhang', 'Jiaxuan Liu', 'Xueqing Wang', 'Changda Zhou', 'Hongen Liu', 'Manhui Lin', 'Yue Zhang', 'Yubo Zhang', 'Yi Liu', 'Dianhai Yu', 'Yanjun Ma'], 'affiliations': ['Baidu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2601.21957.jpg', 'data': {'categories': ['#small_models', '#cv', '#multimodal', '#open_source', '#dataset', '#benchmark'], 'emoji': 'üìÑ', 'ru': {'title': '–£–ª—å—Ç—Ä–∞–∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç PaddleOCR-VL-1.5, –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–º 0.9B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–æ–≤–µ–π—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 94.5% —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –î–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ —Ä–µ–∞–ª—å–Ω—ã–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º (—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø–µ—Ä–µ–∫–æ—Å, –¥–µ—Ñ–æ—Ä–º–∞—Ü–∏—è, —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è —Å —ç–∫—Ä–∞–Ω–∞, –∏–∑–º–µ–Ω–µ–Ω–∏–µ –æ—Å–≤–µ—â–µ–Ω–∏—è), –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ Real5-OmniDocBench. –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–∫ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö, —Ç–∞–∫ –∏ –Ω–∞ –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø–µ—á–∞—Ç–µ–π –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞. –ü—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤—ã—Å–æ–∫–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–º—É –æ—Ç–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö.'}, 'en': {'title': 'Compact Model, Big Achievements in Document Understanding!', 'desc': "The paper presents PaddleOCR-VL-1.5, a compact vision-language model that achieves a remarkable accuracy of 94.5% on the OmniDocBench v1.5 benchmark for document understanding tasks. It introduces the Real5-OmniDocBench benchmark to test the model's robustness against various real-world distortions like scanning and illumination. The model not only excels in performance but also incorporates additional functionalities such as seal recognition and text spotting. Despite its advanced capabilities, it maintains a lightweight architecture with only 0.9 billion parameters, ensuring high efficiency."}, 'zh': {'title': 'Á¥ßÂáëÈ´òÊïàÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÊñáÊ°£ÁêÜËß£ÁöÑÊñ∞Ê†áÊùÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÁ¥ßÂáëÁöÑËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãPaddleOCR-VL-1.5ÔºåËØ•Ê®°ÂûãÂú®ÊñáÊ°£ÁêÜËß£‰ªªÂä°‰∏äËææÂà∞‰∫Ü94.5%ÁöÑÊúÄÊñ∞ÊúÄ‰Ω≥ÂáÜÁ°ÆÁéá„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞Ê®°ÂûãÂú®ÁúüÂÆû‰∏ñÁïå‰∏≠ÁöÑÈ≤ÅÊ£íÊÄßÔºå‰ΩúËÄÖÊèêÂá∫‰∫ÜReal5-OmniDocBenchÂü∫ÂáÜÔºåÊ∂µÁõñ‰∫ÜÊâ´Êèè„ÄÅÂÄæÊñú„ÄÅÂèòÂΩ¢„ÄÅÂ±èÂπïÊëÑÂΩ±ÂíåÂÖâÁÖßÁ≠âÁâ©ÁêÜÊâ≠Êõ≤„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®Êñ∞Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂπ∂‰∏îÂú®ÂäüËÉΩ‰∏äÊâ©Â±ï‰∫ÜÂç∞Á´†ËØÜÂà´ÂíåÊñáÊú¨Ê£ÄÊµã‰ªªÂä°„ÄÇÂ∞ΩÁÆ°ÂäüËÉΩÂº∫Â§ßÔºåPaddleOCR-VL-1.5‰ªç‰øùÊåÅ‰∏∫0.9BÁöÑË∂ÖÁ¥ßÂáëËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÔºåÂÖ∑ÊúâÈ´òÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21468', 'title': 'MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning', 'url': 'https://huggingface.co/papers/2601.21468', 'abstract': 'MemOCR is a multimodal memory agent that enhances long-horizon reasoning by adaptively compressing interaction histories into visual layouts, enabling efficient context utilization under tight budget constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.', 'score': 7, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'c69e9fab65c83419', 'authors': ['Yaorui Shi', 'Shugui Liu', 'Yu Yang', 'Wenyu Mao', 'Yuxin Chen', 'Qi GU', 'Hui Su', 'Xunliang Cai', 'Xiang Wang', 'An Zhang'], 'affiliations': ['Meituan', 'National University of Singapore, School of Computing', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.21468.jpg', 'data': {'categories': ['#benchmark', '#long_context', '#rl', '#agents', '#multimodal', '#reasoning'], 'emoji': 'üß†', 'ru': {'title': '–í–∏–∑—É–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–ª–≥–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤', 'desc': 'MemOCR ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–∂–∞—Ç–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–∞. –í–º–µ—Å—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏, —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –≤–∏–∑—É–∞–ª—å–Ω—ã–π –º–∞–∫–µ—Ç, –≥–¥–µ –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –≤—ã–¥–µ–ª—è—é—Ç—Å—è, –∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —Å–∂–∏–º–∞—é—Ç—Å—è. –ê–≥–µ–Ω—Ç –æ–±—É—á–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —Ü–µ–ª–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω—ã—Ö –æ–± –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö –±—é–¥–∂–µ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö —Å–∂–∞—Ç–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –∏ –æ–¥–Ω–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ MemOCR –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö.'}, 'en': {'title': 'MemOCR: Smart Memory for Efficient Long-Horizon Reasoning', 'desc': 'MemOCR is a novel multimodal memory agent designed to enhance reasoning over long interactions by compressing historical data into visual formats. Unlike traditional memory systems that use text serialization, MemOCR adapts its memory allocation based on the importance of information, allowing for efficient use of limited context space. It employs a structured rich-text memory that is transformed into images, prioritizing essential details while minimizing less critical information. By utilizing reinforcement learning with budget-aware objectives, MemOCR demonstrates superior performance in question-answering tasks, effectively managing context under strict budget constraints.'}, 'zh': {'title': 'MemOCRÔºöÈ´òÊïàÁöÑÈïøÊó∂Èó¥Êé®ÁêÜËÆ∞ÂøÜ‰ª£ÁêÜ', 'desc': 'MemOCRÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅËÆ∞ÂøÜ‰ª£ÁêÜÔºåÊó®Âú®ÈÄöËøáËá™ÈÄÇÂ∫îÂéãÁº©‰∫§‰∫íÂéÜÂè≤Êù•Â¢ûÂº∫ÈïøÊó∂Èó¥Êé®ÁêÜËÉΩÂäõ„ÄÇÂÆÉÈÄöËøáÂ∞ÜÂéÜÂè≤‰ø°ÊÅØËΩ¨Âåñ‰∏∫ËßÜËßâÂ∏ÉÂ±ÄÔºå‰ºòÂåñ‰∫ÜÂú®ÊúâÈôê‰∏ä‰∏ãÊñáÈ¢ÑÁÆó‰∏ãÁöÑËÆ∞ÂøÜ‰ΩøÁî®ÊïàÁéá„ÄÇ‰∏é‰º†ÁªüÁöÑÊñáÊú¨Â∫èÂàóÂåñËÆ∞ÂøÜÁ≥ªÁªü‰∏çÂêåÔºåMemOCRËÉΩÂ§ü‰ª•ÁªìÊûÑÂåñÁöÑÂØåÊñáÊú¨ÂΩ¢ÂºèÂ≠òÂÇ®‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂Ê∏≤Êüì‰∏∫ÂõæÂÉèÔºå‰ª•‰æøÂú®ËÆ∞ÂøÜËÆøÈóÆÊó∂‰ºòÂÖàËÄÉËôëÈáçË¶ÅËØÅÊçÆ„ÄÇÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºåMemOCRÂú®‰∏çÂêåÁöÑÈ¢ÑÁÆóÊù°‰ª∂‰∏ãË°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂Âú®Èïø‰∏ä‰∏ãÊñáÁöÑÂ§öË∑≥ÂíåÂçïË∑≥ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÂº∫Â§ßÁöÑÊñáÊú¨Âü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.18241', 'title': 'TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance', 'url': 'https://huggingface.co/papers/2601.18241', 'abstract': 'TAM-Eval is a framework and benchmark for evaluating large language models on comprehensive test suite maintenance tasks including creation, repair, and updating across multiple programming languages.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.', 'score': 6, 'issue_id': 874, 'pub_date': '2026-01-26', 'pub_date_card': {'ru': '26 —è–Ω–≤–∞—Ä—è', 'en': 'January 26', 'zh': '1Êúà26Êó•'}, 'hash': 'd7bb7f20fdf1d2d5', 'authors': ['Elena Bruches', 'Vadim Alperovich', 'Dari Baturova', 'Roman Derunets', 'Daniil Grebenkin', 'Georgy Mkrtchyan', 'Oleg Sedukhin', 'Mikhail Klementev', 'Ivan Bondarenko', 'Nikolay Bushkov', 'Stanislav Moiseev'], 'affiliations': ['Novosibirsk State University, Novosibirsk, Russia', 'Siberian Neuronets LLC, Novosibirsk, Russia', 'T-Technologies, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2601.18241.jpg', 'data': {'categories': ['#plp', '#agents', '#dataset', '#open_source', '#benchmark'], 'emoji': 'üß™', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ –ø–æ–ª–Ω–æ–º—É –∂–∏–∑–Ω–µ–Ω–Ω–æ–º—É —Ü–∏–∫–ª—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤', 'desc': 'TAM-Eval ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∏ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤, –≤–∫–ª—é—á–∞—è —Å–æ–∑–¥–∞–Ω–∏–µ, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç, —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–Ω—ã—Ö –Ω–∞ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤, –¥–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ–∞–π–ª–æ–≤ —Å –ø–æ–ª–Ω—ã–º –¥–æ—Å—Ç—É–ø–æ–º –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, —á—Ç–æ –ª—É—á—à–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 1539 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏–∑ –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–∞ Python, Java –∏ Go, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç—Ä–∏–∫–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–µ –ø—Ä–æ–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤, –ø–æ–∫—Ä—ã—Ç–∏–∏ –∫–æ–¥–∞ –∏ –º—É—Ç–∞—Ü–∏–æ–Ω–Ω–æ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM –∏–º–µ—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–º –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–∏ —Ç–µ—Å—Ç–æ–≤ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ª–∏—à—å –º–∞—Ä–≥–∏–Ω–∞–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'TAM-Eval: Advancing Test Suite Maintenance with LLMs', 'desc': 'TAM-Eval is a new framework designed to assess how well large language models (LLMs) can handle the maintenance of software test suites, which includes creating, repairing, and updating tests across different programming languages. Unlike previous studies that focused on smaller, function-level tasks, TAM-Eval evaluates performance at the test file level, providing a more realistic view of how models perform in actual software development scenarios. The benchmark includes over 1,500 scenarios from popular programming languages like Python, Java, and Go, and uses metrics such as test pass rates and code coverage to measure effectiveness. Results show that current LLMs struggle with complex test maintenance tasks, highlighting the need for further research in this area.'}, 'zh': {'title': 'TAM-EvalÔºöÂÖ®Èù¢ËØÑ‰º∞ÊµãËØïÂ•ó‰ª∂Áª¥Êä§ÁöÑÊ°ÜÊû∂', 'desc': 'TAM-EvalÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ËΩØ‰ª∂ÊµãËØïÁª¥Êä§‰ªªÂä°‰∏≠ÁöÑÊ°ÜÊû∂ÂíåÂü∫ÂáÜ„ÄÇÂÆÉÂÖ≥Ê≥®‰∫éÊµãËØïÂ•ó‰ª∂ÁöÑÂàõÂª∫„ÄÅ‰øÆÂ§çÂíåÊõ¥Êñ∞ÔºåË∂ÖË∂ä‰∫Ü‰ª•ÂæÄ‰ªÖÈôê‰∫éÂçï‰∏ÄÂäüËÉΩÁöÑÊµãËØïÁîüÊàê„ÄÇËØ•Ê°ÜÊû∂Âú®ÊµãËØïÊñá‰ª∂Á∫ßÂà´ËøõË°åËØÑ‰º∞ÔºåÂπ∂Âú®ÈöîÁ¶ªËØÑ‰º∞‰∏≠‰øùÊåÅÂØπÂÆåÊï¥‰ª£Á†ÅÂ∫ìÁöÑËÆøÈóÆÔºåÊõ¥Â•ΩÂú∞ÂèçÊò†‰∫ÜÁé∞ÂÆû‰∏≠ÁöÑÁª¥Êä§Â∑•‰ΩúÊµÅÁ®ã„ÄÇÈÄöËøáÂØπ1,539‰∏™Ëá™Âä®ÊèêÂèñÂíåÈ™åËØÅÁöÑÂú∫ÊôØËøõË°åËØÑ‰º∞ÔºåTAM-EvalÊòæÁ§∫Âá∫ÂΩìÂâçÊúÄÂÖàËøõÁöÑËØ≠Ë®ÄÊ®°ÂûãÂú®ÂÆûÈôÖÊµãËØïÁª¥Êä§ËøáÁ®ã‰∏≠ÁöÑËÉΩÂäõÊúâÈôê„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21358', 'title': 'Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization', 'url': 'https://huggingface.co/papers/2601.21358', 'abstract': 'PLaT introduces a latent reasoning framework that decouples reasoning from verbalization, enabling dynamic termination and improved scalability over traditional approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.', 'score': 5, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '271318646c5be3a2', 'authors': ['Jiecong Wang', 'Hao Peng', 'Chunyang Liu'], 'affiliations': ['Beihang University', 'Didi Chuxing'], 'pdf_title_img': 'assets/pdf/title_img/2601.21358.jpg', 'data': {'categories': ['#inference', '#interpretability', '#training', '#reasoning', '#architecture'], 'emoji': 'üß†', 'ru': {'title': '–û—Ç–¥–µ–ª–µ–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –æ—Ç —Å–ª–æ–≤: –≥–∏–±–∫–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –±–µ–∑ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —à–∞–≥–æ–≤', 'desc': 'PLaT –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, —Ä–∞–∑–¥–µ–ª—è—è –ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è –æ—Ç –µ–≥–æ –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ü–∏–∏. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∞ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —ç—Ç–∏ –º—ã—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç–∞–∫–æ–º—É —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—é –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–æ–≥–¥–∞ –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –∏–∑–±–µ–≥–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∑–∞—Ä–∞–Ω–µ–µ. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –±–æ–ª–µ–µ –≥–∏–±–∫—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.'}, 'en': {'title': 'Decoupling Reasoning for Scalable AI Solutions', 'desc': 'PLaT introduces a novel framework for latent reasoning that separates the reasoning process from the generation of text. This allows the model to dynamically decide when to stop reasoning, improving efficiency compared to traditional methods that rely on fixed steps. By modeling reasoning as a series of latent planning states, PLaT enhances scalability and diversity in problem-solving. Empirical results show that while it may have lower immediate accuracy, it offers a more flexible and robust approach to reasoning in complex tasks.'}, 'zh': {'title': 'Âä®ÊÄÅÊé®ÁêÜ‰∏éËØ≠Ë®ÄË°®ËææÁöÑËß£ËÄ¶', 'desc': 'PLaTÔºàÊΩúÂú®ÊÄùÁª¥ËßÑÂàíÔºâÂºïÂÖ•‰∫Ü‰∏ÄÁßçÊΩúÂú®Êé®ÁêÜÊ°ÜÊû∂ÔºåÂ∞ÜÊé®ÁêÜ‰∏éËØ≠Ë®ÄË°®ËææËß£ËÄ¶Ôºå‰ªéËÄåÂÆûÁé∞Âä®ÊÄÅÁªàÊ≠¢ÂíåÊØî‰º†ÁªüÊñπÊ≥ïÊõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÊé®ÁêÜÂª∫Ê®°‰∏∫ÊΩúÂú®ËßÑÂàíÁä∂ÊÄÅÁöÑÁ°ÆÂÆöÊÄßËΩ®ËøπÔºåËÄåÂçïÁã¨ÁöÑËß£Á†ÅÂô®Âú®ÂøÖË¶ÅÊó∂Â∞ÜËøô‰∫õÊÄùÁª¥ËΩ¨Âåñ‰∏∫ÊñáÊú¨„ÄÇËøôÁßçËß£ËÄ¶‰ΩøÊ®°ÂûãËÉΩÂ§üÂä®ÊÄÅÂÜ≥ÂÆö‰ΩïÊó∂ÁªàÊ≠¢Êé®ÁêÜÔºåËÄå‰∏ç‰æùËµñ‰∫éÂõ∫ÂÆöÁöÑË∂ÖÂèÇÊï∞„ÄÇÂÆûÈ™åËØÅÊòéÔºåÂ∞ΩÁÆ°PLaTÂú®Ë¥™Â©™ÂáÜÁ°ÆÊÄß‰∏ä‰Ωé‰∫éÂü∫Á∫øÔºå‰ΩÜÂú®Êé®ÁêÜÂ§öÊ†∑ÊÄßÊñπÈù¢Ë°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÂèØÊâ©Â±ïÊÄßÔºåË°®ÊòéÂÖ∂Â≠¶‰π†‰∫ÜÊõ¥Âº∫Â§ß„ÄÅÊõ¥ÂπøÊ≥õÁöÑËß£ÂÜ≥ÊñπÊ°àÁ©∫Èó¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23228', 'title': 'Scaling Multiagent Systems with Process Rewards', 'url': 'https://huggingface.co/papers/2601.23228', 'abstract': 'Multiagent systems are improved through per-action process rewards from AI feedback (MAPPA), enhancing credit assignment and sample efficiency for complex tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.', 'score': 4, 'issue_id': 875, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '389ced0c681706c8', 'authors': ['Ed Li', 'Junyu Ren', 'Cat Yan'], 'affiliations': ['Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2601.23228.jpg', 'data': {'categories': ['#agents', '#rlhf', '#training', '#math'], 'emoji': 'ü§ù', 'ru': {'title': '–ü–æ—à–∞–≥–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –æ—Ç –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–æ–≤ –∫ —É—Å–ø–µ—Ö—É —Å–∏—Å—Ç–µ–º—ã', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ MAPPA –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —á–µ—Ä–µ–∑ –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –æ—Ç AI –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–∏ –∫—Ä–µ–¥–∏—Ç–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏—è–º –∞–≥–µ–Ω—Ç–æ–≤, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∏—Ç–æ–≥–æ–≤–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ—Ç–æ–¥ —Ä–µ—à–∞–µ—Ç –¥–≤–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã: –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–∞ –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∫–∏ –ø—Ä–∏ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏—Ö –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ (AIME, AMC) –∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –¥–æ–ª–≥–æ—Ö–æ—Ä–∏–∑–æ–Ω—Ç–Ω—ã—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'Enhancing Multiagent Learning with Per-Action AI Feedback', 'desc': 'This paper introduces a method called MAPPA, which stands for Multiagent systems improved through per-action process rewards from AI feedback. It focuses on enhancing the training of multiple agents by providing feedback on their actions rather than just at the end of a task. This approach helps in better credit assignment, allowing agents to learn from their individual contributions, and improves sample efficiency by maximizing the training signal from each interaction. The results show significant performance improvements in both competition math problems and data analysis tasks, demonstrating the effectiveness of per-action supervision in multiagent systems.'}, 'zh': {'title': 'ÈÄöËøáAIÂèçÈ¶à‰ºòÂåñÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨Á†îÁ©∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÈÄöËøáAIÂèçÈ¶àÁöÑÊØè‰∏™Âä®‰ΩúËøáÁ®ãÂ•ñÂä±Êù•‰ºòÂåñÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªüÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫MAPPA„ÄÇËøôÁßçÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÂú®Â§ö‰∏™Êô∫ËÉΩ‰ΩìÂêåÊó∂ÂæÆË∞ÉÊó∂Èù¢‰∏¥ÁöÑ‰∏§‰∏™‰∏ªË¶ÅÊåëÊàòÔºöÊô∫ËÉΩ‰Ωì‰πãÈó¥ÁöÑ‰ø°Áî®ÂàÜÈÖçÂíåÊ†∑Êú¨ÊïàÁéá„ÄÇÈÄöËøáÂØπÊØè‰∏™Êô∫ËÉΩ‰ΩìÁöÑÂä®‰ΩúËøõË°åÁªÜÁ≤íÂ∫¶ÁöÑÁõëÁù£ÔºåMAPPAËÉΩÂ§üÂú®Ê≤°ÊúâÁúüÂÆûÊ†áÁ≠æÁöÑÊÉÖÂÜµ‰∏ãÊèêÂèñÊúÄÂ§ßËÆ≠ÁªÉ‰ø°Âè∑„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åË°®ÊòéÔºåMAPPAÂú®Êï∞Â≠¶Á´ûËµõÈóÆÈ¢òÂíåÂ∑•ÂÖ∑Â¢ûÂº∫ÁöÑÊï∞ÊçÆÂàÜÊûê‰ªªÂä°‰∏≠ÂùáÂèñÂæó‰∫ÜÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23161', 'title': 'DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding', 'url': 'https://huggingface.co/papers/2601.23161', 'abstract': 'DIFFA-2, a diffusion-based large audio language model, achieves competitive audio understanding performance with improved efficiency over autoregressive counterparts through enhanced encoding, dual adapters, and staged training.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) large audio language models (LALMs) such as Qwen-2.5-Omni have achieved strong performance on audio understanding and interaction, but scaling them remains costly in data and computation, and strictly sequential decoding limits inference efficiency. Diffusion large language models (dLLMs) have recently been shown to make effective use of limited training data, and prior work on DIFFA indicates that replacing an AR backbone with a diffusion counterpart can substantially improve audio understanding under matched settings, albeit at a proof-of-concept scale without large-scale instruction tuning, preference alignment, or practical decoding schemes. We introduce DIFFA-2, a practical diffusion-based LALM for general audio understanding. DIFFA-2 upgrades the speech encoder, employs dual semantic and acoustic adapters, and is trained with a four-stage curriculum that combines semantic and acoustic alignment, large-scale supervised fine-tuning, and variance-reduced preference optimization, using only fully open-source corpora. Experiments on MMSU, MMAU, and MMAR show that DIFFA-2 consistently improves over DIFFA and is competitive to strong AR LALMs under practical training budgets, supporting diffusion-based modeling is a viable backbone for large-scale audio understanding. Our code is available at https://github.com/NKU-HLT/DIFFA.git.', 'score': 4, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'b261594d096c0128', 'authors': ['Jiaming Zhou', 'Xuxin Cheng', 'Shiwan Zhao', 'Yuhang Jia', 'Cao Liu', 'Ke Zeng', 'Xunliang Cai', 'Yong Qin'], 'affiliations': ['College of Computer Science, Nankai University', 'Meituan LongCat Interaction Team'], 'pdf_title_img': 'assets/pdf/title_img/2601.23161.jpg', 'data': {'categories': ['#rlhf', '#diffusion', '#audio', '#training', '#open_source', '#architecture'], 'emoji': 'üéµ', 'ru': {'title': '–î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ', 'desc': 'DIFFA-2 ‚Äî —ç—Ç–æ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∞—É–¥–∏–æ, –∫–æ—Ç–æ—Ä–∞—è –∫–æ–Ω–∫—É—Ä–∏—Ä—É–µ—Ç —Å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –ø—Ä–∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Ä–µ—á–∏, –¥–≤–æ–π–Ω—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏ –∞–∫—É—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ü–æ–¥—Ö–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –≤–º–µ—Å—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DIFFA-2 –∫–æ–Ω–∫—É—Ä–∏—Ä—É–µ—Ç —Å —Å–∏–ª—å–Ω—ã–º–∏ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –ø—Ä–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –±—é–¥–∂–µ—Ç–∞—Ö –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∂–∏–∑–Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Diffusion Models', 'desc': 'DIFFA-2 is a diffusion-based large audio language model that enhances audio understanding while being more efficient than traditional autoregressive models. It incorporates an upgraded speech encoder and dual adapters to better capture both semantic and acoustic features. The model is trained using a four-stage curriculum that optimizes performance through a combination of alignment and fine-tuning techniques. Experiments demonstrate that DIFFA-2 outperforms its predecessor and competes effectively with leading autoregressive models, showcasing the potential of diffusion-based approaches in audio processing.'}, 'zh': {'title': 'DIFFA-2ÔºöÈ´òÊïàÁöÑÈü≥È¢ëÁêÜËß£Êñ∞ÈÄâÊã©', 'desc': 'DIFFA-2ÊòØ‰∏ÄÁßçÂü∫‰∫éÊâ©Êï£ÁöÑÈü≥È¢ëËØ≠Ë®ÄÊ®°ÂûãÔºåËÉΩÂ§üÂú®Èü≥È¢ëÁêÜËß£ÊñπÈù¢ÂÆûÁé∞Á´û‰∫âÂäõÁöÑË°®Áé∞ÔºåÂêåÊó∂Âú®ÊïàÁéá‰∏ä‰ºò‰∫éËá™ÂõûÂΩíÊ®°Âûã„ÄÇÂÆÉÈÄöËøáÊîπËøõÁöÑÁºñÁ†ÅÂô®„ÄÅÂèåÈÄÇÈÖçÂô®ÂíåÂàÜÈò∂ÊÆµËÆ≠ÁªÉÊù•ÊèêÂçáÊÄßËÉΩ„ÄÇ‰∏é‰º†ÁªüÁöÑËá™ÂõûÂΩíÊ®°ÂûãÁõ∏ÊØîÔºåDIFFA-2Âú®ËÆ≠ÁªÉÊï∞ÊçÆÊúâÈôêÁöÑÊÉÖÂÜµ‰∏ãÔºåËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®Êï∞ÊçÆ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDIFFA-2Âú®Â§ö‰∏™Èü≥È¢ëÁêÜËß£‰ªªÂä°‰∏äË°®Áé∞‰ºò‰∫éÂâç‰∏ÄÁâàÊú¨DIFFAÔºåÂπ∂‰∏îÂú®ÂÆûÈôÖËÆ≠ÁªÉÈ¢ÑÁÆó‰∏ã‰∏éÂº∫Â§ßÁöÑËá™ÂõûÂΩíÊ®°ÂûãÁõ∏Á´û‰∫â„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22904', 'title': 'DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation', 'url': 'https://huggingface.co/papers/2601.22904', 'abstract': 'A novel vision autoencoder framework combines semantic representation with pixel-level reconstruction using spherical latent space and Riemannian flow matching for improved fidelity and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.', 'score': 4, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '36974d14be561b91', 'authors': ['Hun Chang', 'Byunghee Cha', 'Jong Chul Ye'], 'affiliations': ['Graduate School of AI, KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2601.22904.jpg', 'data': {'categories': [], 'emoji': 'üåê', 'ru': {'title': '–°—Ñ–µ—Ä–∏—á–µ—Å–∫–∏–π –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä: –æ—Ç —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∫ –ø–∏–∫—Å–µ–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏', 'desc': '–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω DINO-SAE ‚Äî –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö Vision Foundation Models —Å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –ø–∏–∫—Å–µ–ª–µ–π –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ —Å–µ–º–∞–Ω—Ç–∏–∫–∞ –≤ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø–æ—ç—Ç–æ–º—É –∞–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –≤–º–µ—Å—Ç–æ —Å—Ç—Ä–æ–≥–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤–µ–ª–∏—á–∏–Ω, –ø–æ–∑–≤–æ–ª—è—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏. –î–ª—è —Ä–∞–±–æ—Ç—ã –≤ —Å—Ñ–µ—Ä–∏—á–µ—Å–∫–æ–º –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è Riemannian Flow Matching —Å Diffusion Transformer, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≥–∏–ø–µ—Ä—Å—Ñ–µ—Ä–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∫–æ—Ä–¥–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (0.37 rFID, 26.2 dB PSNR) –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–∏–ª—å–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é.'}, 'en': {'title': 'Bridging Semantic and Pixel-Level Reconstruction with DINO-SAE', 'desc': 'This paper introduces the DINO Spherical Autoencoder (DINO-SAE), a new framework that enhances image reconstruction by combining semantic representation with pixel-level details. It addresses the common issue of losing high-frequency details in existing generative models by using a Hierarchical Convolutional Patch Embedding module and a Cosine Similarity Alignment objective. The framework leverages Riemannian Flow Matching to train a Diffusion Transformer on a spherical latent space, which improves both fidelity and efficiency in reconstruction tasks. Experimental results show that DINO-SAE achieves state-of-the-art performance on ImageNet-1K, demonstrating superior reconstruction quality and semantic alignment with pretrained Vision Foundation Models.'}, 'zh': {'title': 'ÊèêÂçáÈáçÂª∫Ë¥®ÈáèÁöÑÁêÉÈù¢Ëá™ÁºñÁ†ÅÂô®', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜËßâËá™ÁºñÁ†ÅÂô®Ê°ÜÊû∂ÔºåÁß∞‰∏∫DINOÁêÉÈù¢Ëá™ÁºñÁ†ÅÂô®ÔºàDINO-SAEÔºâÔºåÂÆÉÁªìÂêà‰∫ÜËØ≠‰πâË°®Á§∫ÂíåÂÉèÁ¥†Á∫ßÈáçÂª∫„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂºïÂÖ•Â±ÇÊ¨°Âç∑ÁßØË°•‰∏ÅÂµåÂÖ•Ê®°ÂùóÔºåÂ¢ûÂº∫‰∫ÜÂ±ÄÈÉ®ÁªìÊûÑÂíåÁ∫πÁêÜÁöÑ‰øùÁïôÔºåÂêåÊó∂ÈááÁî®‰ΩôÂº¶Áõ∏‰ººÊÄßÂØπÈΩêÁõÆÊ†áÔºåÁ°Æ‰øùËØ≠‰πâ‰∏ÄËá¥ÊÄßÂπ∂ÁÅµÊ¥ªË∞ÉÊï¥ÁâπÂæÅÂπÖÂ∫¶„ÄÇÊàë‰ª¨ËøòÂà©Áî®RiemannianÊµÅÂåπÈÖçÊñπÊ≥ïÔºåÂú®ÁêÉÈù¢ÊΩúÂú®ÊµÅÂΩ¢‰∏äÁõ¥Êé•ËÆ≠ÁªÉÊâ©Êï£ÂèòÊç¢Âô®ÔºàDiTÔºâÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈáçÂª∫ÁöÑ‰øùÁúüÂ∫¶ÂíåÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDINO-SAEÂú®ImageNet-1KÊï∞ÊçÆÈõÜ‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÈáçÂª∫Ë¥®ÈáèÔºåÂ±ïÁé∞‰∫ÜÂº∫Â§ßÁöÑËØ≠‰πâÂØπÈΩêËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22837', 'title': 'NativeTok: Native Visual Tokenization for Improved Image Generation', 'url': 'https://huggingface.co/papers/2601.22837', 'abstract': 'NativeTok introduces a novel visual tokenization approach that enforces causal dependencies during image encoding, using a Meta Image Transformer and Mixture of Causal Expert Transformer for efficient and coherent image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.', 'score': 4, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '7320bcaf27ae941f', 'authors': ['Bin Wu', 'Mengqi Huang', 'Weinan Jia', 'Zhendong Mao'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.22837.jpg', 'data': {'categories': [], 'emoji': 'üé®', 'ru': {'title': '–ü—Ä–∏—á–∏–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª–µ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'NativeTok –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –≤–≤–æ–¥–∏—Ç –ø—Ä–∏—á–∏–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ –Ω–∞ —ç—Ç–∞–ø–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ Meta Image Transformer –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ Mixture of Causal Expert Transformer, –≥–¥–µ –∫–∞–∂–¥—ã–π –±–ª–æ–∫ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∑–∞—Å—Ç–∞–≤–ª—è—è –º–æ–¥–µ–ª—å –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è—Ö —Å —è–≤–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ª—É—á—à–µ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Enhancing Image Generation with Causal Tokenization', 'desc': 'NativeTok presents a new method for visual tokenization that maintains causal relationships during the encoding of images. This approach utilizes a Meta Image Transformer for modeling latent images and a Mixture of Causal Expert Transformer to generate tokens based on previous tokens and features. By enforcing these dependencies, NativeTok improves the coherence and quality of generated images compared to traditional methods that treat tokens as unordered. The framework also incorporates a Hierarchical Native Training strategy to enhance training efficiency by updating only new expert blocks.'}, 'zh': {'title': 'NativeTokÔºöÈ´òÊïàÂõæÂÉèÁîüÊàêÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'NativeTokÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑËßÜËßâÊ†áËÆ∞ÂåñÊñπÊ≥ïÔºåÂú®ÂõæÂÉèÁºñÁ†ÅËøáÁ®ã‰∏≠Âº∫Âà∂ÊâßË°åÂõ†Êûú‰æùËµñÂÖ≥Á≥ª„ÄÇËØ•ÊñπÊ≥ï‰ΩøÁî®Meta Image TransformerÂíåMixture of Causal Expert TransformerÊù•ÂÆûÁé∞È´òÊïà‰∏îËøûË¥ØÁöÑÂõæÂÉèÁîüÊàê„ÄÇ‰º†ÁªüÁöÑVQÂü∫Á°ÄÂõæÂÉèÁîüÊàêÊñπÊ≥ïÂú®Ê†áËÆ∞ÂåñÈò∂ÊÆµÂíåÁîüÊàêÈò∂ÊÆµ‰πãÈó¥Â≠òÂú®‰∏çÂåπÈÖçÔºåÂØºËá¥ÁîüÊàêÊ®°ÂûãÂ≠¶‰π†Âà∞Êó†Â∫èÁöÑÂàÜÂ∏É„ÄÇNativeTokÈÄöËøáÂú®Ê†áËÆ∞ÂåñËøáÁ®ã‰∏≠ÂµåÂÖ•ÂÖ≥Á≥ªÁ∫¶ÊùüÔºåËß£ÂÜ≥‰∫ÜËøô‰∏ÄÈóÆÈ¢òÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÈáçÂª∫ÁöÑÊïàÁéáÂíå‰∏ÄËá¥ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23188', 'title': 'Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience', 'url': 'https://huggingface.co/papers/2601.23188', 'abstract': 'Deep search agents with hierarchical metacognitive monitoring enhance reasoning and retrieval performance through fast consistency checks and experience-driven corrective interventions.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.', 'score': 3, 'issue_id': 872, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'a87289443af8a0ac', 'authors': ['Zhongxiang Sun', 'Qipeng Wang', 'Weijie Yu', 'Jingxuan Yang', 'Haolang Lu', 'Jun Xu'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Gaoling School of Artificial Intelligence Renmin University of China', 'School of Information Technology and Management University of International Business and Economics', 'Search Applications Department, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2601.23188.jpg', 'data': {'categories': ['#rag', '#benchmark', '#agents'], 'emoji': 'üß†', 'ru': {'title': '–°–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥', 'desc': '–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Deep Search with Meta-Cognitive Monitoring (DS-MCM) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø–æ–∏—Å–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –º–µ—Ö–∞–Ω–∏–∑–º –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞, –≤–¥–æ—Ö–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –Ω–µ–π—Ä–æ–Ω–∞—É–∫–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π –≤–∫–ª—é—á–∞–µ—Ç –±—ã—Å—Ç—Ä—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏ –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–∏, –∞ —Ç–∞–∫–∂–µ –º–µ–¥–ª–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä, –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º—ã–π –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏—Ö –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞. –ú–æ–Ω–∏—Ç–æ—Ä –≤—Å—Ç—Ä–æ–µ–Ω –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ —Ü–∏–∫–ª —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞, –ø–æ–∑–≤–æ–ª—è—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–æ–º–µ–Ω—Ç—ã, –∫–æ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ, –∏ –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–µ –¥–µ–π—Å—Ç–≤–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ DS-MCM —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–æ–≤.'}, 'en': {'title': 'Enhancing Deep Search Agents with Smart Self-Monitoring', 'desc': "This paper introduces a new framework called Deep Search with Meta-Cognitive Monitoring (DS-MCM) that enhances the performance of deep search agents. It incorporates a hierarchical metacognitive monitoring system that helps these agents check their reasoning and retrieval processes in real-time. The framework includes a Fast Consistency Monitor for quick checks and a Slow Experience-Driven Monitor for deeper reflection based on past experiences. Experiments show that DS-MCM significantly improves the agents' ability to handle complex tasks and uncertainties."}, 'zh': {'title': 'Ê∑±Â∫¶ÊêúÁ¥¢‰∏éÂÖÉËÆ§Áü•ÁõëÊéßÔºöÊèêÂçáÊé®ÁêÜ‰∏éÊ£ÄÁ¥¢ÁöÑÊô∫ËÉΩÊ°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Ê∑±Â∫¶ÊêúÁ¥¢‰∏éÂÖÉËÆ§Áü•ÁõëÊéßÔºàDS-MCMÔºâÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÂçáÊ∑±Â∫¶ÊêúÁ¥¢‰ª£ÁêÜÁöÑÊé®ÁêÜÂíåÊ£ÄÁ¥¢ÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÂø´ÈÄü‰∏ÄËá¥ÊÄßÁõëÊéßÂíåÁªèÈ™åÈ©±Âä®ÁöÑÊÖ¢ÁõëÊéßÊú∫Âà∂Ôºå‰ª•Â∫îÂØπ‰ªªÂä°Âú®‰∏çÁ°ÆÂÆöÊÄß‰∏ãÁöÑÊºîÂèò„ÄÇÈÄöËøáÂú®Êé®ÁêÜ-Ê£ÄÁ¥¢Âæ™ÁéØ‰∏≠ÂµåÂÖ•ÁõëÊéßÔºåDS-MCMËÉΩÂ§üÂà§Êñ≠‰ΩïÊó∂ÈúÄË¶ÅÂπ≤È¢ÑÔºåÂπ∂Ê†πÊçÆÂéÜÂè≤ÁªèÈ™åÊåáÂØºÁ∫†Ê≠£Êé™ÊñΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDS-MCMÂú®Â§ö‰∏™Ê∑±Â∫¶ÊêúÁ¥¢Âü∫ÂáÜÂíåÊ®°Âûã‰∏äÂùáË°®Áé∞Âá∫Êõ¥Â•ΩÁöÑÊÄßËÉΩÂíåÈ≤ÅÊ£íÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21419', 'title': 'Revisiting Diffusion Model Predictions Through Dimensionality', 'url': 'https://huggingface.co/papers/2601.21419', 'abstract': "Diffusion models using direct data prediction outperform traditional noise or velocity prediction in high-dimensional settings, with a proposed framework automatically learning optimal prediction parameters from data.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion and flow matching models have highlighted a shift in the preferred prediction target -- moving from noise (varepsilon) and velocity (v) to direct data (x) prediction -- particularly in high-dimensional settings. However, a formal explanation of why the optimal target depends on the specific properties of the data remains elusive. In this work, we provide a theoretical framework based on a generalized prediction formulation that accommodates arbitrary output targets, of which varepsilon-, v-, and x-prediction are special cases. We derive the analytical relationship between data's geometry and the optimal prediction target, offering a rigorous justification for why x-prediction becomes superior when the ambient dimension significantly exceeds the data's intrinsic dimension. Furthermore, while our theory identifies dimensionality as the governing factor for the optimal prediction target, the intrinsic dimension of manifold-bound data is typically intractable to estimate in practice. To bridge this gap, we propose k-Diff, a framework that employs a data-driven approach to learn the optimal prediction parameter k directly from data, bypassing the need for explicit dimension estimation. Extensive experiments in both latent-space and pixel-space image generation demonstrate that k-Diff consistently outperforms fixed-target baselines across varying architectures and data scales, providing a principled and automated approach to enhancing generative performance.", 'score': 3, 'issue_id': 873, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '61baa9cea285e3c0', 'authors': ['Qing Jin', 'Chaoyang Wang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.21419.jpg', 'data': {'categories': ['#optimization', '#diffusion'], 'emoji': 'üéØ', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –≥–µ–æ–º–µ—Ç—Ä–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç, –ø–æ—á–µ–º—É –º–æ–¥–µ–ª–∏ –¥–∏—Ñ—Ñ—É–∑–∏–∏, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∏–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º —à—É–º–∞ –∏–ª–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤ –≤—ã—Å–æ–∫–æ–º–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –æ–±–æ–±—â—ë–Ω–Ω–æ–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä—è—Å–Ω—è–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –≥–µ–æ–º–µ—Ç—Ä–∏–µ–π –¥–∞–Ω–Ω—ã—Ö –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π. –û–Ω–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä—è–º–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º, –∫–æ–≥–¥–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–∫—Ä—É–∂–∞—é—â–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ k-Diff, –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—É—á–∞–µ—Ç—Å—è –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ —Å–∞–º–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —è–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Direct Data Prediction: The Key to Better Diffusion Models', 'desc': "This paper discusses how diffusion models can improve performance in high-dimensional data settings by shifting from traditional noise or velocity predictions to direct data predictions. The authors introduce a theoretical framework that explains why the best prediction target depends on the data's geometric properties, particularly when the data's intrinsic dimension is much lower than the ambient dimension. They propose a new method called k-Diff, which automatically learns the optimal prediction parameters from the data, eliminating the need for manual dimension estimation. Experimental results show that k-Diff outperforms existing methods in generating images, demonstrating its effectiveness across different architectures and data scales."}, 'zh': {'title': 'Áõ¥Êé•Êï∞ÊçÆÈ¢ÑÊµãÔºöÊâ©Êï£Ê®°ÂûãÁöÑÊñ∞ÊñπÂêë', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊâ©Êï£Ê®°ÂûãÂú®È´òÁª¥ËÆæÁΩÆ‰∏≠Áõ¥Êé•Êï∞ÊçÆÈ¢ÑÊµãÁöÑ‰ºòÂäøÔºå‰ºò‰∫é‰º†ÁªüÁöÑÂô™Â£∞ÊàñÈÄüÂ∫¶È¢ÑÊµã„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁêÜËÆ∫Ê°ÜÊû∂ÔºåËÉΩÂ§üÊ†πÊçÆÊï∞ÊçÆÁöÑÂá†‰ΩïÁâπÊÄßËá™Âä®Â≠¶‰π†ÊúÄ‰Ω≥È¢ÑÊµãÂèÇÊï∞„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂΩìÁéØÂ¢ÉÁª¥Â∫¶ËøúÂ§ß‰∫éÊï∞ÊçÆÁöÑÂÜÖÂú®Áª¥Â∫¶Êó∂ÔºåÁõ¥Êé•Êï∞ÊçÆÈ¢ÑÊµãÁöÑÊïàÊûúÊõ¥‰Ω≥„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫Ük-DiffÊ°ÜÊû∂ÔºåÈÄöËøáÊï∞ÊçÆÈ©±Âä®ÁöÑÊñπÊ≥ïÁõ¥Êé•‰ªéÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÊúÄ‰Ω≥È¢ÑÊµãÂèÇÊï∞ÔºåÈÅøÂÖç‰∫ÜÊòæÂºèÁª¥Â∫¶‰º∞ËÆ°ÁöÑÈúÄÊ±Ç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.20732', 'title': 'Continual GUI Agents', 'url': 'https://huggingface.co/papers/2601.20732', 'abstract': 'Continual GUI Agents framework addresses performance degradation in dynamic digital environments through reinforcement fine-tuning with novel anchoring rewards that stabilize learning across shifting UI domains and resolutions.  \t\t\t\t\tAI-generated summary \t\t\t\t As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.', 'score': 3, 'issue_id': 873, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'}, 'hash': '90be57765bcaf0a1', 'authors': ['Ziwei Liu', 'Borui Kang', 'Hangjie Yuan', 'Zixiang Zhao', 'Wei Li', 'Yifan Zhu', 'Tao Feng'], 'affiliations': ['College of Computer Science, Zhejiang University, China', 'Department of Computer Science and Technology, Tsinghua University, China', 'Photogrammetry and Remote Sensing Lab, ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2601.20732.jpg', 'data': {'categories': ['#training', '#agents', '#rl'], 'emoji': 'üéØ', 'ru': {'title': '–Ø–∫–æ—Ä–µ–Ω–∏–µ –≤ –ø–æ—Ç–æ–∫–µ: —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –¥–∏–Ω–∞–º–∏—á–Ω–æ–π —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥–µ', 'desc': '–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥—ã, –≤–∫–ª—é—á–∞—è —Å–¥–≤–∏–≥–∏ –≤ –¥–æ–º–µ–Ω–∞—Ö –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è—Ö —ç–∫—Ä–∞–Ω–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É Continual GUI Agents, —Ç—Ä–µ–±—É—é—â—É—é –æ—Ç –∞–≥–µ–Ω—Ç–æ–≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–∞–Ω–Ω—ã—Ö. –†–µ—à–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–µ GUI-AiF, –∏—Å–ø–æ–ª—å–∑—É—é—â–µ–º –¥–≤–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã (APR-iF –∏ ARR-iF), –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—Ç –æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏–µ —Ç–æ—á–µ–∫ –∏ —Ä–µ–≥–∏–æ–Ω–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ —Å–¥–≤–∏–≥–∞—é—â–µ–º—Å—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞–∑–æ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è reinforcement learning –∫ –ø—Ä–æ–±–ª–µ–º–µ continual learning –≤ GUI-–∞–≥–µ–Ω—Ç–∞—Ö.'}, 'en': {'title': 'Stabilizing Learning in Dynamic GUI Environments', 'desc': 'The paper presents a new framework called Continual GUI Agents, which aims to improve the performance of agents in changing digital environments. It highlights the problem of performance degradation when agents trained on static data encounter new graphical user interface (GUI) domains and resolutions. To tackle this issue, the authors introduce a reinforcement fine-tuning method called GUI-Anchoring in Flux (GUI-AiF), which uses two innovative rewards to help agents adapt to shifting interaction points. The results demonstrate that this approach significantly outperforms existing methods, marking a significant advancement in continual learning for GUI agents.'}, 'zh': {'title': 'ÊåÅÁª≠Â≠¶‰π†ÔºåÁ®≥ÂÆöË°®Áé∞ÔºÅ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ°ÜÊû∂ÔºåÁß∞‰∏∫ÊåÅÁª≠GUI‰ª£ÁêÜÔºàContinual GUI AgentsÔºâÔºåÊó®Âú®Ëß£ÂÜ≥Âä®ÊÄÅÊï∞Â≠óÁéØÂ¢É‰∏≠ÊÄßËÉΩ‰∏ãÈôçÁöÑÈóÆÈ¢ò„ÄÇÈöèÁùÄÊñ∞ÁöÑGUIÊï∞ÊçÆ‰∏çÊñ≠Ê∂åÁé∞ÔºåÁé∞ÊúâÁöÑ‰ª£ÁêÜÂú®ÈùôÊÄÅÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÔºåÂØºËá¥ÂÖ∂Âú®ÂèòÂåñÁöÑÈ¢ÜÂüüÂíåÂàÜËæ®Áéá‰∏ãË°®Áé∞‰∏ç‰Ω≥„ÄÇ‰∏∫Ê≠§ÔºåÁ†îÁ©∂ËÄÖÂºïÂÖ•‰∫ÜGUI-ÈîöÂÆöÂú®ÊµÅÂä®‰∏≠ÔºàGUI-AiFÔºâÁöÑÊñπÊ≥ïÔºåÈÄöËøá‰∏§ÁßçÊñ∞È¢ñÁöÑÂ•ñÂä±Êú∫Âà∂Êù•Á®≥ÂÆöÂ≠¶‰π†ËøáÁ®ã„ÄÇËøô‰∫õÂ•ñÂä±Êú∫Âà∂Â∏ÆÂä©‰ª£ÁêÜÈÄÇÂ∫î‰∏çÊñ≠ÂèòÂåñÁöÑ‰∫§‰∫íÁÇπÂíåÂå∫ÂüüÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊåÅÁª≠Â≠¶‰π†ÁöÑÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.15625', 'title': 'Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors', 'url': 'https://huggingface.co/papers/2601.15625', 'abstract': "A framework called Fission-GRPO is introduced to improve multi-turn tool execution in large language models by converting execution errors into corrective supervision during reinforcement learning training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.", 'score': 3, 'issue_id': 873, 'pub_date': '2026-01-22', 'pub_date_card': {'ru': '22 —è–Ω–≤–∞—Ä—è', 'en': 'January 22', 'zh': '1Êúà22Êó•'}, 'hash': '2ad0dedebab3f94a', 'authors': ['Zhiwei Zhang', 'Fei Zhao', 'Rui Wang', 'Zezhong Wang', 'Bin Liang', 'Jiakang Wang', 'Yao Hu', 'Shaosheng Cao', 'Kam-Fai Wong'], 'affiliations': ['The Chinese University of Hong Kong', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2601.15625.jpg', 'data': {'categories': ['#training', '#agents', '#rl'], 'emoji': 'üîß', 'ru': {'title': '–ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫–∏ –≤ –∑–Ω–∞–Ω–∏—è: —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–µ–µ—Å—è –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è tool-use –º–æ–¥–µ–ª–µ–π', 'desc': '–§ission-GRPO ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ –æ—à–∏–±–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–π —Å–∏–≥–Ω–∞–ª –æ–±—É—á–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è reinforcement learning —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Error Simulator –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è –æ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫, –∞ –Ω–µ –æ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –≠—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ: —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Qwen3-8B —É–≤–µ–ª–∏—á–∏–ª–∞—Å—å –Ω–∞ 4% –≤ –∞–±—Å–æ–ª—é—Ç–Ω–æ–º –∑–Ω–∞—á–µ–Ω–∏–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è –ª—É—á—à–µ–º—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –ø–æ—Å–ª–µ –æ—à–∏–±–æ–∫.'}, 'en': {'title': 'Transforming Errors into Learning Opportunities with Fission-GRPO', 'desc': "The paper introduces Fission-GRPO, a novel framework designed to enhance multi-turn tool execution in large language models (LLMs) by transforming execution errors into corrective supervision during reinforcement learning (RL) training. Traditional RL methods treat errors as sparse negative rewards, which do not provide effective recovery guidance, leading to repetitive mistakes. Fission-GRPO addresses this by fissioning failed trajectories into new training instances, incorporating feedback from an Error Simulator to guide the model in learning from its specific errors. This approach significantly improves the model's error recovery rate and overall accuracy, demonstrating its effectiveness in real-world tool interactions."}, 'zh': {'title': 'Fission-GRPOÔºöÊèêÂçáÂ§öËΩÆÂ∑•ÂÖ∑ÊâßË°åÁöÑÊô∫ËÉΩÊ°ÜÊû∂', 'desc': 'Fission-GRPOÊòØ‰∏Ä‰∏™Êñ∞Ê°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÂú®Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∏≠Â∞ÜÊâßË°åÈîôËØØËΩ¨Âåñ‰∏∫Á∫†Ê≠£ÁõëÁù£ÔºåÊù•ÊîπÂñÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂ§öËΩÆÂ∑•ÂÖ∑ÊâßË°åËÉΩÂäõ„ÄÇÂΩìÂâçÁöÑÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïÂ∞ÜÈîôËØØËßÜ‰∏∫Á®ÄÁñèÁöÑË¥üÂ•ñÂä±ÔºåÁº∫‰πèÊÅ¢Â§çÊåáÂØºÔºåËÄåFission-GRPOÈÄöËøáÂ∞ÜÊØè‰∏™Â§±Ë¥•ÁöÑÊâßË°åËΩ®Ëøπ‰∏éËØäÊñ≠ÂèçÈ¶àÁªìÂêàÔºåÁîüÊàêÊñ∞ÁöÑËÆ≠ÁªÉÂÆû‰æãÔºå‰ªéËÄåÂ∏ÆÂä©Ê®°ÂûãÂ≠¶‰π†Â¶Ç‰Ωï‰ªéÈîôËØØ‰∏≠ÊÅ¢Â§ç„ÄÇËØ•Ê°ÜÊû∂‰ΩøÊ®°ÂûãËÉΩÂ§üÂú®Êé¢Á¥¢ËøáÁ®ã‰∏≠Â≠¶‰π†ÂÖ∂ÂÖ∑‰ΩìÈîôËØØÔºåËÄå‰∏çÊòØ‰æùËµñÈùôÊÄÅÁöÑÈ¢ÑÊî∂ÈõÜÈîôËØØÊ°à‰æã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFission-GRPOÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÈîôËØØÊÅ¢Â§çÁéáÂíåÊï¥‰ΩìÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22664', 'title': 'Real-Time Aligned Reward Model beyond Semantics', 'url': 'https://huggingface.co/papers/2601.22664', 'abstract': 'RLHF suffers from reward overoptimization due to misalignment between reward models and policy models, which R2M addresses by incorporating real-time policy feedback to dynamically adapt reward modeling during training.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.', 'score': 2, 'issue_id': 871, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'f8dfe7a629e6c33e', 'authors': ['Zixuan Huang', 'Xin Xia', 'Yuxi Ren', 'Jianbin Zheng', 'Xuefeng Xiao', 'Hongyan Xie', 'Li Huaqiu', 'Songshi Liang', 'Zhongxiang Dai', 'Fuzhen Zhuang', 'Jianxin Li', 'Yikun Ban', 'Deqing Wang'], 'affiliations': ['Beihang University', 'ByteDance', 'Renmin University of China', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22664.jpg', 'data': {'categories': ['#training', '#rlhf', '#alignment'], 'emoji': 'üéØ', 'ru': {'title': '–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã —á–µ—Ä–µ–∑ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –ø–æ–ª–∏—Ç–∏–∫–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã –≤ –º–µ—Ç–æ–¥–µ RLHF, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –∏–∑-–∑–∞ —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –Ω–∞–≥—Ä–∞–¥—ã –∏ –ø–æ–ª–∏—Ç–∏–∫–æ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç R2M ‚Äî –Ω–æ–≤—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –≤ –≤–∏–¥–µ –µ—ë —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–∞–≥—Ä–∞–¥—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. –ö–ª—é—á–µ–≤–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–æ–¥—Å—Ç—Ä–æ–π–∫–µ –º–æ–¥–µ–ª–∏ –Ω–∞–≥—Ä–∞–¥—ã –ø–æ–¥ —ç—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å –Ω–∞–º–µ—Ä–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –ø—Ä–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'R2M: Real-Time Feedback for Better Reward Alignment', 'desc': 'This paper discusses a problem in Reinforcement Learning from Human Feedback (RLHF) where reward models can lead to overoptimization, causing policy models to misalign with human preferences. The authors introduce R2M, a new framework that incorporates real-time feedback from policy models to improve reward modeling during training. By using the evolving hidden states of the policy, R2M dynamically adjusts the reward model to better match the current policy distribution. This approach aims to reduce the reward discrepancy and enhance the alignment between the reward model and the policy model, ultimately improving the performance of large language models.'}, 'zh': {'title': 'ÂÆûÊó∂ÂØπÈΩêÂ•ñÂä±Ê®°ÂûãÔºöËß£ÂÜ≥Â•ñÂä±ËøáÂ∫¶‰ºòÂåñÁöÑÂàõÊñ∞ÊñπÊ≥ï', 'desc': 'Âº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑ‰∫∫Á±ªÂèçÈ¶àÔºàRLHFÔºâÊòØ‰∏ÄÁßçÈáçË¶ÅÊäÄÊúØÔºåÁî®‰∫é‰ΩøÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâ‰∏é‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩê„ÄÇÁÑ∂ËÄåÔºåRLHFÂÆπÊòìÂá∫Áé∞Â•ñÂä±ËøáÂ∫¶‰ºòÂåñÁöÑÈóÆÈ¢òÔºåÂç≥Á≠ñÁï•Ê®°ÂûãËøáÂ∫¶ÊãüÂêàÂ•ñÂä±Ê®°ÂûãÔºåÂØºËá¥ÊçïÊçâ‰∏çÂà∞ÁúüÂÆûÁöÑ‰∫∫Á±ªÊÑèÂõæ„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜR2MÔºàÂÆûÊó∂ÂØπÈΩêÂ•ñÂä±Ê®°ÂûãÔºâÔºåÂÆÉÈÄöËøáÂÆûÊó∂ÊîøÁ≠ñÂèçÈ¶àÂä®ÊÄÅË∞ÉÊï¥Â•ñÂä±Âª∫Ê®°Ôºå‰ªéËÄåÊîπÂñÑÂ•ñÂä±Ê®°Âûã‰∏éÁ≠ñÁï•Ê®°Âûã‰πãÈó¥ÁöÑÂØπÈΩê„ÄÇR2MÂà©Áî®Á≠ñÁï•ÁöÑÈöêËóèÁä∂ÊÄÅÂèòÂåñÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÊù•ÊèêÈ´òÂ•ñÂä±Ê®°ÂûãÁöÑÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22141', 'title': 'Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data', 'url': 'https://huggingface.co/papers/2601.22141', 'abstract': 'Routing the Lottery framework discovers multiple specialized subnetworks tailored to different data conditions, outperforming traditional pruning methods while using fewer parameters and identifying subnetwork collapse issues.  \t\t\t\t\tAI-generated summary \t\t\t\t In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.', 'score': 2, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'd5d68fbfe04ee90b', 'authors': ['Grzegorz Stefanski', 'Alberto Presta', 'Michal Byra'], 'affiliations': ['Institute of Fundamental Technological Research, Polish Academy of Sciences, Poland', 'Samsung AI Center Warsaw, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2601.22141.jpg', 'data': {'categories': ['#inference', '#training'], 'emoji': 'üé´', 'ru': {'title': '–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–ª–µ—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∂–∞—Ç–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π ¬´Routing the Lottery¬ª, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥—Å–µ—Ç–µ–π, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ–¥ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∫–ª–∞—Å—Å—ã. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –æ–¥–Ω—É —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –ø–æ–¥—Å–µ—Ç—å –¥–ª—è –≤—Å–µ—Ö –≤—Ö–æ–¥–æ–≤, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–π –ø–æ–¥—Ö–æ–¥ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ—Å—Ç—å —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –¥–æ 10 —Ä–∞–∑ –º–µ–Ω—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏ –≤–≤–æ–¥–∏—Ç –º–µ—Ç—Ä–∏–∫—É —Å—Ö–æ–¥—Å—Ç–≤–∞ –ø–æ–¥—Å–µ—Ç–µ–π –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ–∂–∏–≤–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª—è–µ—Ç –∑–∞–¥–∞—á—É —Å–∂–∞—Ç–∏—è –∫–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –¥–∞–Ω–Ω—ã—Ö, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –±–æ–ª–µ–µ –º–æ–¥—É–ª—å–Ω—ã—Ö –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π.'}, 'en': {'title': 'Unlocking Adaptive Subnetworks for Diverse Data Conditions', 'desc': 'The Routing the Lottery (RTL) framework enhances traditional pruning methods by identifying multiple specialized subnetworks, known as adaptive tickets, that are optimized for different data conditions. Unlike previous approaches that rely on a single winning ticket, RTL acknowledges the diversity in real-world data by tailoring subnetworks to specific classes or environments. This method not only improves performance metrics like balanced accuracy and recall but also significantly reduces the number of parameters needed, making models more efficient. Additionally, RTL addresses the issue of subnetwork collapse, providing a new similarity score to diagnose oversparsification without requiring labels.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÂâ™ÊûùÔºåÊèêÂçáÊ®°ÂûãÊÄßËÉΩÁöÑÂÖ≥ÈîÆ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Routing the LotteryÔºàRTLÔºâÁöÑËá™ÈÄÇÂ∫îÂâ™ÊûùÊ°ÜÊû∂ÔºåÊó®Âú®ÂèëÁé∞Â§ö‰∏™‰∏ìÈó®ÈíàÂØπ‰∏çÂêåÊï∞ÊçÆÊù°‰ª∂ÁöÑÂ≠êÁΩëÁªú„ÄÇËøô‰∫õÂ≠êÁΩëÁªúË¢´Áß∞‰∏∫Ëá™ÈÄÇÂ∫îÁ•®ËØÅÔºåËÉΩÂ§üÂú®Â§öÁßçÊï∞ÊçÆÈõÜÂíå‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰º†ÁªüÁöÑÂçï‰∏ÄÊ®°ÂûãÂíåÂ§öÊ®°ÂûãÂü∫Á∫ø„ÄÇRTLÂú®‰øùÊåÅÂπ≥Ë°°ÂáÜÁ°ÆÁéáÂíåÂè¨ÂõûÁéáÁöÑÂêåÊó∂Ôºå‰ΩøÁî®ÁöÑÂèÇÊï∞Êï∞ÈáèÊØîÁã¨Á´ãÊ®°ÂûãÂ∞ëÂ§öËææ10ÂÄç„ÄÇÊ≠§Â§ñÔºåÊú¨ÊñáËøòËØÜÂà´‰∫ÜÂ≠êÁΩëÁªúÂ¥©Ê∫ÉÁöÑÈóÆÈ¢òÔºåÂπ∂ÂºïÂÖ•‰∫ÜÂ≠êÁΩëÁªúÁõ∏‰ººÂ∫¶ËØÑÂàÜÔºå‰ª•‰æøÂú®Ê≤°ÊúâÊ†áÁ≠æÁöÑÊÉÖÂÜµ‰∏ãËØäÊñ≠ËøáÂ∫¶Á®ÄÁñèÂåñÁé∞Ë±°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21525', 'title': 'LMK > CLS: Landmark Pooling for Dense Embeddings', 'url': 'https://huggingface.co/papers/2601.21525', 'abstract': 'Landmark pooling improves long-context representation learning by partitioning sequences into chunks and using landmark tokens to preserve both global and local information more effectively than traditional pooling methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Representation learning is central to many downstream tasks such as search, clustering, classification, and reranking. State-of-the-art sequence encoders typically collapse a variable-length token sequence to a single vector using a pooling operator, most commonly a special [CLS] token or mean pooling over token embeddings. In this paper, we identify systematic weaknesses of these pooling strategies: [CLS] tends to concentrate information toward the initial positions of the sequence and can under-represent distributed evidence, while mean pooling can dilute salient local signals, sometimes leading to worse short-context performance. To address these issues, we introduce Landmark (LMK) pooling, which partitions a sequence into chunks, inserts landmark tokens between chunks, and forms the final representation by mean-pooling the landmark token embeddings. This simple mechanism improves long-context extrapolation without sacrificing local salient features, at the cost of introducing a small number of special tokens. We empirically demonstrate that LMK pooling matches existing methods on short-context retrieval tasks and yields substantial improvements on long-context tasks, making it a practical and scalable alternative to existing pooling methods.', 'score': 2, 'issue_id': 871, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'b6eff2a3ffcaf0bf', 'authors': ['Meet Doshi', 'Aashka Trivedi', 'Vishwajeet Kumar', 'Parul Awasthy', 'Yulong Li', 'Jaydeep Sen', 'Radu Florian', 'Sachindra Joshi'], 'affiliations': ['IBM Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.21525.jpg', 'data': {'categories': ['#training', '#architecture', '#long_context'], 'emoji': 'üß©', 'ru': {'title': '–û—Ä–∏–µ–Ω—Ç–∏—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Landmark pooling, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —Å–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—é (pooling) –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω [CLS] –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –Ω–∞—á–∞–ª–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∞ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º—ã–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã. Landmark pooling —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —á–∞—Å—Ç–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞–∫ –≥–ª–æ–±–∞–ª—å–Ω–æ–π, —Ç–∞–∫ –∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –¥–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö, –Ω–µ —Ç–µ—Ä—è—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö.'}, 'en': {'title': 'Enhancing Long-Context Learning with Landmark Pooling', 'desc': 'This paper introduces Landmark (LMK) pooling, a new method for improving representation learning in long-context sequences. Traditional pooling methods, like [CLS] tokens and mean pooling, often fail to capture important information from both local and global contexts. LMK pooling addresses these weaknesses by dividing sequences into chunks and using landmark tokens to maintain critical information. The results show that LMK pooling performs well on short-context tasks and significantly enhances performance on long-context tasks, making it a valuable alternative to existing methods.'}, 'zh': {'title': 'Âú∞Ê†áÊ±†ÂåñÔºöÊèêÂçáÈïø‰∏ä‰∏ãÊñáË°®Á§∫ÁöÑÊúâÊïàÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊ±†ÂåñÊñπÊ≥ïÔºåÁß∞‰∏∫Âú∞Ê†áÊ±†ÂåñÔºàLandmark poolingÔºâÔºåÊó®Âú®ÊîπÂñÑÈïø‰∏ä‰∏ãÊñáÁöÑË°®Á§∫Â≠¶‰π†„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂ∞ÜÂ∫èÂàóÂàÜÂâ≤ÊàêÂ§ö‰∏™ÂùóÔºåÂπ∂Âú®Âùó‰πãÈó¥ÊèíÂÖ•Âú∞Ê†áÊ†áËÆ∞ÔºåÊúâÊïàÂú∞‰øùÁïôÂÖ®Â±ÄÂíåÂ±ÄÈÉ®‰ø°ÊÅØ„ÄÇ‰∏é‰º†ÁªüÁöÑÊ±†ÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåÂú∞Ê†áÊ±†ÂåñËÉΩÂ§üÊõ¥Â•ΩÂú∞Â§ÑÁêÜÈïø‰∏ä‰∏ãÊñáÔºåÂêåÊó∂‰∏ç‰ºöÁâ∫Áâ≤Â±ÄÈÉ®ÊòæËëóÁâπÂæÅ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú∞Ê†áÊ±†ÂåñÂú®Áü≠‰∏ä‰∏ãÊñáÊ£ÄÁ¥¢‰ªªÂä°‰∏äË°®Áé∞‰∏éÁé∞ÊúâÊñπÊ≥ïÁõ∏ÂΩìÔºåËÄåÂú®Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏äÂàôÊúâÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.23134', 'title': 'Machine Learning for Energy-Performance-aware Scheduling', 'url': 'https://huggingface.co/papers/2601.23134', 'abstract': 'A Bayesian Optimization approach using Gaussian Processes automates scheduling configuration optimization on heterogeneous multi-core systems while approximating the Pareto Frontier for energy-time trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Mat√©rn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.', 'score': 1, 'issue_id': 873, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '5f6adff0b514ffa6', 'authors': ['Zheyuan Hu', 'Yifei Shi'], 'affiliations': ['Department of Computer Science and Technology, University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2601.23134.jpg', 'data': {'categories': ['#optimization'], 'emoji': '‚ö°', 'ru': {'title': '–£–º–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–Ω–æ–≥–æ—è–¥–µ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∞—É—Å—Å–æ–≤—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ—è–¥–µ—Ä–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö. –ê–≤—Ç–æ—Ä—ã —è–≤–Ω–æ —É—á–∏—Ç—ã–≤–∞—é—Ç –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä –∑–∞–¥–∞—á–∏, –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É—è —Ñ—Ä–æ–Ω—Ç –ü–∞—Ä–µ—Ç–æ –º–µ–∂–¥—É —ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ–º –∏ –∑–∞–¥–µ—Ä–∂–∫–æ–π. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ —á—ë—Ä–Ω–æ–≥–æ —è—â–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (fANOVA) –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–¥—Ä–∞ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏ (–ú–∞—Ç–µ—Ä–Ω –∏ RBF). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': 'Optimizing Energy-Time Trade-offs in Multi-Core Systems with Bayesian Optimization', 'desc': 'This paper presents a method for optimizing scheduling configurations in multi-core systems using Bayesian Optimization with Gaussian Processes. It focuses on balancing energy efficiency and latency, which are often conflicting objectives in embedded systems. The approach approximates the Pareto Frontier, allowing for a better understanding of trade-offs between energy consumption and processing time. Additionally, it enhances model interpretability through Sensitivity Analysis and the comparison of different covariance kernels, identifying key hardware parameters that influence performance.'}, 'zh': {'title': 'Ëá™Âä®ÂåñË∞ÉÂ∫¶‰ºòÂåñÔºöËÉΩÈáè‰∏éÊó∂Èó¥ÁöÑÂπ≥Ë°°', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂü∫‰∫éË¥ùÂè∂ÊñØ‰ºòÂåñÁöÑÊ°ÜÊû∂ÔºåÂà©Áî®È´òÊñØËøáÁ®ãËá™Âä®ÂåñË∞ÉÂ∫¶ÈÖçÁΩÆ‰ºòÂåñÔºåÈÄÇÁî®‰∫éÂºÇÊûÑÂ§öÊ†∏Á≥ªÁªü„ÄÇÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜËÉΩÈáèÊïàÁéá‰∏éÂª∂Ëøü‰πãÈó¥ÁöÑÂ§öÁõÆÊ†á‰ºòÂåñÈóÆÈ¢òÔºåÈÄöËøáËøë‰ººÂ∏ïÁ¥ØÊâòÂâçÊ≤øÊù•Âπ≥Ë°°Ëøô‰∏§ËÄÖ„ÄÇ‰º†ÁªüÁöÑÂêØÂèëÂºèË∞É‰ºòÂú®È´òÁª¥ÈùûÂÖâÊªëÁöÑÊêúÁ¥¢Á©∫Èó¥‰∏≠ÊïàÁéá‰Ωé‰∏ãÔºåËÄåÊàë‰ª¨ÁöÑÊ°ÜÊû∂ËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åÊêúÁ¥¢„ÄÇÈÄöËøáÂºïÂÖ•ÊïèÊÑüÊÄßÂàÜÊûêÂíåÊØîËæÉ‰∏çÂêåÁöÑÂçèÊñπÂ∑ÆÊ†∏ÔºåÊàë‰ª¨‰∏∫ÈªëÁÆ±Ê®°ÂûãÊèê‰æõ‰∫ÜÁâ©ÁêÜÂèØËß£ÈáäÊÄßÔºåÊè≠Á§∫‰∫ÜÂΩ±ÂìçÁ≥ªÁªüÊÄßËÉΩÁöÑ‰∏ªË¶ÅÁ°¨‰ª∂ÂèÇÊï∞„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22666', 'title': 'ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding', 'url': 'https://huggingface.co/papers/2601.22666', 'abstract': 'ExpAlign presents a vision-language alignment framework using multiple instance learning and attention-based pooling to improve open-vocabulary detection and zero-shot instance segmentation without additional annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP_r on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.', 'score': 1, 'issue_id': 881, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': 'a31ebca4fca6c8e0', 'authors': ['Junyi Hu', 'Tian Bai', 'Fengyi Wu', 'Wenyan Li', 'Zhenming Peng', 'Yi Zhang'], 'affiliations': ['Department of Automation, Tsinghua University, China', 'Lingsu Lab, China', 'University of Copenhagen, Denmark', 'University of Electronic Science and Technology of China, China'], 'pdf_title_img': 'assets/pdf/title_img/2601.22666.jpg', 'data': {'categories': ['#cv', '#architecture', '#alignment', '#multimodal'], 'emoji': 'üîó', 'ru': {'title': '–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–Ω–∏—è –∏ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π', 'desc': 'ExpAlign ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∏ —è–∑—ã–∫–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –º–µ—Ç–æ–¥–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤. –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—É–ª–∏–Ω–≥ –¥–ª—è –Ω–µ—è–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ–±–ª–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –î–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∞–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é —Å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏ –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏. –ú–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–ª–æ–≤–∞—Ä—ë–º –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –±–µ–∑ —É—á–∏—Ç–µ–ª—è, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö.'}, 'en': {'title': 'Enhancing Vision-Language Alignment with ExpAlign', 'desc': "ExpAlign is a framework designed to enhance vision-language alignment for tasks like open-vocabulary detection and zero-shot instance segmentation without needing extra annotations. It utilizes multiple instance learning and an attention-based pooling method to effectively align visual tokens with language descriptions. The framework introduces an Expectation Alignment Head that allows for implicit selection of relevant tokens and instances, improving the model's performance. Additionally, it incorporates a multi-scale consistency regularization approach to stabilize the learning process, leading to significant improvements in accuracy, especially for less common categories."}, 'zh': {'title': 'ExpAlignÔºöÊó†Ê†áÊ≥®ÁöÑËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêÊñ∞ÊñπÊ≥ï', 'desc': 'ExpAlign ÊòØ‰∏Ä‰∏™ËßÜËßâ-ËØ≠Ë®ÄÂØπÈΩêÊ°ÜÊû∂ÔºåÂà©Áî®Â§öÂÆû‰æãÂ≠¶‰π†ÂíåÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑÊ±†ÂåñÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÂºÄÊîæËØçÊ±áÊ£ÄÊµãÂíåÈõ∂Ê†∑Êú¨ÂÆû‰æãÂàÜÂâ≤ÁöÑÊïàÊûúÔºåËÄåÊó†ÈúÄÈ¢ùÂ§ñÁöÑÊ†áÊ≥®„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊúüÊúõÂØπÈΩêÂ§¥ÂÆûÁé∞Âü∫‰∫éÊ≥®ÊÑèÂäõÁöÑËΩØÂ§öÂÆû‰æãÊ±†ÂåñÔºåËÉΩÂ§üÂú®Ê≤°ÊúâÈ¢ùÂ§ñÊ†áÊ≥®ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÈöêÂºèÁöÑÊ†áËÆ∞ÂíåÂÆû‰æãÈÄâÊã©„ÄÇ‰∏∫‰∫ÜËøõ‰∏ÄÊ≠•Á®≥ÂÆöÂØπÈΩêÂ≠¶‰π†ÔºåExpAlign ËøòÂºÄÂèë‰∫Ü‰∏ÄÁßçÂü∫‰∫éËÉΩÈáèÁöÑÂ§öÂ∞∫Â∫¶‰∏ÄËá¥ÊÄßÊ≠£ÂàôÂåñÊñπÊ°àÔºåÂåÖÊã¨ Top-K Â§öÊ≠£Ê†∑Êú¨ÂØπÊØîÁõÆÊ†áÂíåÂá†‰ΩïÊÑüÁü•‰∏ÄËá¥ÊÄßÁõÆÊ†á„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåExpAlign Âú®ÂºÄÊîæËØçÊ±áÊ£ÄÊµãÂíåÈõ∂Ê†∑Êú¨ÂÆû‰æãÂàÜÂâ≤ÊñπÈù¢Ë°®Áé∞‰ºòÂºÇÔºåÁâπÂà´ÊòØÂú®ÈïøÂ∞æÁ±ªÂà´‰∏äÔºåËææÂà∞‰∫Ü 36.2 ÁöÑ AP_rÔºåË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂêåÁ±ªÊúÄÂÖàËøõÁöÑÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22108', 'title': 'Value-Based Pre-Training with Downstream Feedback', 'url': 'https://huggingface.co/papers/2601.22108', 'abstract': 'V-Pretraining uses downstream task gradients to reshape pretraining objectives, improving model capabilities with minimal labeled data and reduced computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.', 'score': 1, 'issue_id': 882, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '2eb8d67d213c080e', 'authors': ['Shuqi Ke', 'Giulia Fanti'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2601.22108.jpg', 'data': {'categories': ['#reasoning', '#small_models', '#optimization', '#transfer_learning', '#multimodal', '#training'], 'emoji': 'üéØ', 'ru': {'title': '–¶–µ–ª–µ–≤–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞: –Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á', 'desc': 'V-Pretraining ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –ø–µ—Ä–µ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π, –ø–æ–∑–≤–æ–ª—è—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã. –í–º–µ—Å—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–∫—Å–∏-—Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á –ø–µ—Ä–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Ç–∞–∫, —á—Ç–æ–±—ã –∫–∞–∂–¥—ã–π —à–∞–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ –±—ã–ª –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –Ω—É–∂–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏. –ú–µ—Ç–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á ‚Äî –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º –∑—Ä–µ–Ω–∏–∏ –ø—Ä–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏ —Ç—Ä–µ–±—É–µ–º—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.'}, 'en': {'title': 'Steering Pretraining with Downstream Task Insights', 'desc': "V-Pretraining is a novel approach that enhances the pretraining of machine learning models by using gradients from downstream tasks to refine the pretraining objectives. This method allows for more efficient use of limited labeled data and reduces the computational resources needed for training. By aligning the pretraining tasks with the goals of downstream tasks, V-Pretraining ensures that each gradient step contributes more effectively to the model's capabilities. The results show significant improvements in reasoning and vision tasks, demonstrating the effectiveness of this value-based, modality-agnostic strategy."}, 'zh': {'title': 'V-È¢ÑËÆ≠ÁªÉÔºöÁî®Â∞ëÈáèÊï∞ÊçÆÊèêÂçáÊ®°ÂûãËÉΩÂäõ', 'desc': 'V-È¢ÑËÆ≠ÁªÉÊòØ‰∏ÄÁßçÂà©Áî®‰∏ãÊ∏∏‰ªªÂä°Ê¢ØÂ∫¶Êù•ÈáçÂ°ëÈ¢ÑËÆ≠ÁªÉÁõÆÊ†áÁöÑÊñπÊ≥ïÔºåÊó®Âú®‰ª•ÊúÄÂ∞ëÁöÑÊ†áËÆ∞Êï∞ÊçÆÂíåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÁöÑÊñπÂºèÊèêÂçáÊ®°ÂûãËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËΩªÈáèÁ∫ß‰ªªÂä°ËÆæËÆ°Âô®ÈÄâÊã©‰∏é‰∏ãÊ∏∏‰ªªÂä°Ê¢ØÂ∫¶‰∏ÄËá¥ÁöÑÈ¢ÑËÆ≠ÁªÉ‰ªªÂä°Ôºå‰ªéËÄåÂºïÂØºÈ¢ÑËÆ≠ÁªÉÊúùÂêëÁõ∏ÂÖ≥ÁöÑ‰∏ãÊ∏∏ËÉΩÂäõ„ÄÇ‰∏é‰º†ÁªüÁöÑÂõ∫ÂÆö‰ª£ÁêÜÁõÆÊ†á‰∏çÂêåÔºåV-È¢ÑËÆ≠ÁªÉËÉΩÂ§üÂä®ÊÄÅË∞ÉÊï¥È¢ÑËÆ≠ÁªÉ‰ªªÂä°Ôºå‰ª•ÊúÄÂ§ßÂåñÊØè‰∏ÄÊ≠•Ê¢ØÂ∫¶ÁöÑ‰ª∑ÂÄº„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåV-È¢ÑËÆ≠ÁªÉÂú®ËØ≠Ë®ÄÊ®°ÂûãÂíåËßÜËßâËá™ÁõëÁù£Â≠¶‰π†‰∏≠ÂùáÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21998', 'title': 'Causal World Modeling for Robot Control', 'url': 'https://huggingface.co/papers/2601.21998', 'abstract': 'Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.', 'score': 1, 'issue_id': 882, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': 'eb4e4a9cdad57038', 'authors': ['Lin Li', 'Qihang Zhang', 'Yiming Luo', 'Shuai Yang', 'Ruilin Wang', 'Fei Han', 'Mingrui Yu', 'Zelin Gao', 'Nan Xue', 'Xing Zhu', 'Yujun Shen', 'Yinghao Xu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.21998.jpg', 'data': {'categories': ['#robotics', '#diffusion', '#video', '#architecture', '#open_source', '#training'], 'emoji': 'ü§ñ', 'ru': {'title': '–í–∏–¥–µ–æ-–º–æ–¥–µ–ª–∏ –º–∏—Ä–∞ –∫–∞–∫ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è –µ–¥–∏–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä–∞—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –±—É–¥—É—â–∏–µ –∫–∞–¥—Ä—ã –≤–∏–¥–µ–æ –∏ –∏—Å–ø–æ–ª–Ω—è–µ—Ç —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ –∫–æ–º–∞–Ω–¥—ã, –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—â–µ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–≤–∏–≤–∞—é—Ç –∏–¥–µ—é –≤–∏–¥–µ–æ-–º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ä–æ–±–æ—Ç–∞–º –ø—Ä–µ–¥—É–≥–∞–¥—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è —Å–≤–æ–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–æ–π –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å LingBot-VA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Mixture-of-Transformers –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –µ–¥–∏–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –º–µ—Ö–∞–Ω–∏–∑–º –∑–∞–º–∫–Ω—É—Ç–æ–≥–æ —Ü–∏–∫–ª–∞ –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –≤—ã–≤–æ–¥–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏, —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –∫ –Ω–æ–≤—ã–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º –∏ —É–ª—É—á—à–µ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Revolutionizing Robot Learning with Video World Modeling', 'desc': "This paper presents a novel approach to robot learning through video world modeling, which predicts future frames and executes actions in a unified framework. The proposed model, LingBot-VA, utilizes an autoregressive diffusion method that integrates vision and action in a shared latent space, enhancing the robot's ability to understand and anticipate the consequences of its actions. Key innovations include a closed-loop feedback mechanism for real-time environmental interaction and an asynchronous inference pipeline that optimizes action prediction and execution. The model demonstrates strong performance in both simulated and real-world tasks, showcasing its effectiveness in long-horizon manipulation and adaptability to new situations."}, 'zh': {'title': 'ËßÜÈ¢ë‰∏ñÁïåÂª∫Ê®°ÔºöÊú∫Âô®‰∫∫Â≠¶‰π†ÁöÑÊñ∞Âü∫Á°Ä', 'desc': 'ËßÜÈ¢ë‰∏ñÁïåÂª∫Ê®°ÈÄöËøáÁªü‰∏ÄÊ°ÜÊû∂‰ΩøÊú∫Âô®‰∫∫Â≠¶‰π†Êàê‰∏∫ÂèØËÉΩÔºåËØ•Ê°ÜÊû∂ÂêåÊó∂È¢ÑÊµãÂ∏ßÂíåÊâßË°åÁ≠ñÁï•ÔºåÂà©Áî®ÂÖ±‰∫´ÁöÑÊΩúÂú®Á©∫Èó¥ÂíåÈó≠ÁéØÂèçÈ¶àÊú∫Âà∂„ÄÇÊú¨ÊñáÊèêÂá∫ÁöÑLingBot-VAÊòØ‰∏ÄÁßçËá™ÂõûÂΩíÊâ©Êï£Ê°ÜÊû∂ÔºåËÉΩÂ§üÂêåÊó∂Â≠¶‰π†Â∏ßÈ¢ÑÊµãÂíåÁ≠ñÁï•ÊâßË°å„ÄÇÊ®°ÂûãËÆæËÆ°ÂåÖÊã¨ÂÖ±‰∫´ÊΩúÂú®Á©∫Èó¥„ÄÅÈó≠ÁéØÂõûÊªöÊú∫Âà∂ÂíåÂºÇÊ≠•Êé®ÁêÜÁÆ°ÈÅìÔºåÊîØÊåÅÈ´òÊïàÊéßÂà∂„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê®°ÂûãÂú®ÈïøÊó∂Èó¥Êìç‰Ωú„ÄÅÂêéËÆ≠ÁªÉÁöÑÊï∞ÊçÆÊïàÁéáÂíåÂØπÊñ∞ÈÖçÁΩÆÁöÑÂº∫Ê≥õÂåñËÉΩÂäõÊñπÈù¢Ë°®Áé∞Âá∫Ëâ≤„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21709', 'title': 'Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis', 'url': 'https://huggingface.co/papers/2601.21709', 'abstract': 'Temporal Attention Pattern Predictability Analysis (TAPPA) provides a unified framework for understanding attention patterns in large language models by analyzing their mathematical formulations from a temporal perspective, distinguishing predictable from unpredictable patterns based on query self-similarity.  \t\t\t\t\tAI-generated summary \t\t\t\t Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns such as retrieval heads, sink heads, and diagonal traces, yet these observations remain fragmented and lack a unifying explanation. To bridge this gap, we introduce Temporal Attention Pattern Predictability Analysis (TAPPA), a unifying framework that explains diverse attention patterns by analyzing their underlying mathematical formulations from a temporally continuous perspective. TAPPA both deepens the understanding of attention behavior and guides inference acceleration approaches. Specifically, TAPPA characterizes attention patterns as predictable patterns with clear regularities and unpredictable patterns that appear effectively random. Our analysis further reveals that this distinction can be explained by the degree of query self-similarity along the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative cases through the joint effect of queries, keys, and Rotary Positional Embeddings (RoPE). We validate TAPPA by applying its insights to KV cache compression and LLM pruning tasks. Across these tasks, a simple metric motivated by TAPPA consistently improves performance over baseline methods. The code is available at https://github.com/MIRALab-USTC/LLM-TAPPA.', 'score': 1, 'issue_id': 874, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '79cfcdca1b2494db', 'authors': ['Qingyue Yang', 'Jie Wang', 'Xing Li', 'Yinqi Bai', 'Xialiang Tong', 'Huiling Zhen', 'Jianye Hao', 'Mingxuan Yuan', 'Bin Li'], 'affiliations': ['Huawei Technologies Co., Ltd.', 'Tianjin University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.21709.jpg', 'data': {'categories': [], 'emoji': '‚è∞', 'ru': {'title': '–ü–æ–Ω–∏–º–∞–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –µ–¥–∏–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ TAPPA –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –∏ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–∫–∞–∑—ã–≤–∞—è, —á—Ç–æ —ç—Ç–∞ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞—Ü–∏—è –æ–±—ä—è—Å–Ω—è–µ—Ç—Å—è —Å—Ç–µ–ø–µ–Ω—å—é —Å–∞–º–æ–ø–æ–¥–æ–±–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –∏–∑–º–µ—Ä–µ–Ω–∏–∏. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–∫–ª—é—á–∞–µ—Ç –∏–∑—É—á–µ–Ω–∏–µ –≤–∑–∞–∏–º–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–ª—é—á–µ–π –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–æ–∫ RoPE –Ω–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ TAPPA –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Å–∂–∞—Ç–∏—è KV –∫—ç—à–∞ –∏ –ø—Ä—É–Ω–∏–Ω–≥–∞ –º–æ–¥–µ–ª–µ–π, –≥–¥–µ –º–µ—Ç—Ä–∏–∫–∞, –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π, —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–∞–∑–æ–≤—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤.'}, 'en': {'title': 'Unlocking Predictability in Attention Patterns of Language Models', 'desc': 'Temporal Attention Pattern Predictability Analysis (TAPPA) introduces a new way to understand attention patterns in large language models (LLMs) by examining their mathematical structures over time. It categorizes these patterns into predictable and unpredictable types based on how similar the queries are to themselves over time. This framework not only enhances our understanding of how attention works but also aids in speeding up inference processes. By applying TAPPA, researchers can improve tasks like KV cache compression and LLM pruning, leading to better performance than traditional methods.'}, 'zh': {'title': 'Êè≠Á§∫Ê≥®ÊÑèÊ®°ÂºèÁöÑÂèØÈ¢ÑÊµãÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫ÜÊó∂Èó¥Ê≥®ÊÑèÊ®°ÂºèÂèØÈ¢ÑÊµãÊÄßÂàÜÊûêÔºàTAPPAÔºâÔºå‰∏∫ÁêÜËß£Â§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÊ≥®ÊÑèÊ®°ÂºèÊèê‰æõ‰∫ÜÁªü‰∏ÄÊ°ÜÊû∂„ÄÇÈÄöËøá‰ªéÊó∂Èó¥ËßíÂ∫¶ÂàÜÊûêÊï∞Â≠¶ÂÖ¨ÂºèÔºåTAPPAÂå∫ÂàÜ‰∫ÜÂèØÈ¢ÑÊµãÂíå‰∏çÂèØÈ¢ÑÊµãÁöÑÊ≥®ÊÑèÊ®°Âºè„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÊ≥®ÊÑèÊ®°ÂºèÁöÑÂèØÈ¢ÑÊµãÊÄß‰∏éÊü•ËØ¢Ëá™Áõ∏‰ººÂ∫¶ÊúâÂÖ≥ÔºåÂÖ∑ÊúâÊòéÊòæËßÑÂæãÁöÑÊ®°ÂºèË¢´ËßÜ‰∏∫ÂèØÈ¢ÑÊµãÊ®°Âºè„ÄÇTAPPAÁöÑÂàÜÊûê‰∏ç‰ªÖÂä†Ê∑±‰∫ÜÂØπÊ≥®ÊÑèË°å‰∏∫ÁöÑÁêÜËß£ÔºåËøò‰∏∫Êé®ÁêÜÂä†ÈÄüÊñπÊ≥ïÊèê‰æõ‰∫ÜÊåáÂØº„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21526', 'title': 'KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization', 'url': 'https://huggingface.co/papers/2601.21526', 'abstract': 'KAPSO is a modular framework for autonomous program synthesis that uses iterative optimization loops with experimentation tracking, knowledge integration, and cognitive memory to improve code generation over extended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.   KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.   We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.   Code Available at: https://github.com/Leeroo-AI/kapso', 'score': 1, 'issue_id': 881, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '1c415a3ad5d4ef1d', 'authors': ['Alireza Nadaf', 'Alireza Mohammadshahi', 'Majid Yazdani'], 'affiliations': ['Leeroo Team'], 'pdf_title_img': 'assets/pdf/title_img/2601.21526.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training', '#plp', '#agents'], 'emoji': 'üß†', 'ru': {'title': '–°–∏–Ω—Ç–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º —á–µ—Ä–µ–∑ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é', 'desc': 'KAPSO ‚Äî —ç—Ç–æ –º–æ–¥—É–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∏–Ω—Ç–µ–∑–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º, –∫–æ—Ç–æ—Ä–∞—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–æ–¥ —á–µ—Ä–µ–∑ —Ü–∏–∫–ª—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏. –°–∏—Å—Ç–µ–º–∞ —Ä–µ—à–∞–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Å—Ç–æ—Ä–∏—é —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é git, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—è –∑–Ω–∞–Ω–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å —É—Ä–æ–∫–æ–≤ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–ø—ã—Ç–æ–∫. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: engine –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, —Å–∏—Å—Ç–µ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏—è–º–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –≤–Ω–µ—à–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –∏ —Å–ª–æ–π —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π. KAPSO –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö ML-—Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö, –ø–æ–∫–∞–∑—ã–≤–∞—è —É–ª—É—á—à–µ–Ω–Ω—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –æ—à–∏–±–æ–∫.'}, 'en': {'title': 'KAPSO: Optimizing Code Generation through Iterative Learning', 'desc': 'KAPSO is a modular framework designed for autonomous program synthesis that enhances code generation through iterative optimization. It operates by taking a natural language goal and an evaluation method, then cycles through ideation, code synthesis, execution, and learning to refine the output. The framework addresses common long-horizon failures in coding agents by integrating an experimentation engine, a knowledge system, and a cognitive memory layer. This approach allows KAPSO to produce reproducible artifacts, leverage domain expertise, and accelerate learning from past experiments.'}, 'zh': {'title': 'KAPSOÔºöÊèêÂçá‰ª£Á†ÅÁîüÊàêÁöÑÊô∫ËÉΩÊ°ÜÊû∂', 'desc': 'KAPSOÊòØ‰∏Ä‰∏™Ê®°ÂùóÂåñÊ°ÜÊû∂ÔºåÁî®‰∫éËá™‰∏ªÁ®ãÂ∫èÂêàÊàêÂíå‰ºòÂåñ„ÄÇÂÆÉÈÄöËøáËø≠‰ª£‰ºòÂåñÂæ™ÁéØÔºåÁªìÂêàÂÆûÈ™åË∑üË∏™„ÄÅÁü•ËØÜÊï¥ÂêàÂíåËÆ§Áü•ËÆ∞ÂøÜÔºåÊù•ÊèêÈ´ò‰ª£Á†ÅÁîüÊàêÁöÑÊïàÁéá„ÄÇKAPSO‰∏ç‰ªÖÂ∞ÜÂêàÊàêËßÜ‰∏∫ÁªàÁÇπÔºåËÄåÊòØ‰Ωú‰∏∫ÈïøÊó∂Èó¥‰ºòÂåñÂæ™ÁéØ‰∏≠ÁöÑ‰∏Ä‰∏™Êìç‰ΩúÔºåÊó®Âú®Ëß£ÂÜ≥ÁºñÁ†Å‰ª£ÁêÜÂ∏∏ËßÅÁöÑÈïøÊúüÂ§±Ë¥•ÈóÆÈ¢ò„ÄÇÈÄöËøáÈõÜÊàêÂÆûÈ™åÂºïÊìé„ÄÅÁü•ËØÜÁ≥ªÁªüÂíåËÆ§Áü•ËÆ∞ÂøÜÂ±ÇÔºåKAPSOËÉΩÂ§üÊúâÊïàÂú∞ÁÆ°ÁêÜÂÆûÈ™åÁä∂ÊÄÅ„ÄÅÈáçÁî®È¢ÜÂüüÁü•ËØÜÔºåÂπ∂Âä†ÈÄüÂ≠¶‰π†ËøáÁ®ã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.22680', 'title': 'Visual Personalization Turing Test', 'url': 'https://huggingface.co/papers/2601.22680', 'abstract': 'A new evaluation framework called VPTT assesses contextual visual personalization through perceptual indistinguishability from human-created content, utilizing a benchmark, retrieval-augmented generator, and calibrated text-based metric.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.', 'score': 0, 'issue_id': 882, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '76909410abda2f8c', 'authors': ['Rameen Abdal', 'James Burgess', 'Sergey Tulyakov', 'Kuan-Chieh Jackson Wang'], 'affiliations': ['Snap Research'], 'pdf_title_img': 'assets/pdf/title_img/2601.22680.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark', '#video', '#rag', '#3d'], 'emoji': 'üé≠', 'ru': {'title': '–ö–æ–≥–¥–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ–æ—Ç–ª–∏—á–∏–º –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ Visual Personalization Turing Test (VPTT) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω–æ–π –Ω–µ—Ä–∞–∑–ª–∏—á–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Å–æ–∑–¥–∞–≤–∞–µ–º–æ–≥–æ –ª—é–¥—å–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –∏–∑ 10 —Ç—ã—Å—è—á –ø–µ—Ä—Å–æ–Ω, –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ retrieval-augmented generation –∏ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É VPTT Score, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å —Ç–µ–∫—Å—Ç–æ–º. –ú–æ–¥–µ–ª—å –ø—Ä–æ—Ö–æ–¥–∏—Ç —Ç–µ—Å—Ç, –µ—Å–ª–∏ –µ—ë –≤—ã–≤–æ–¥ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –≤–∏–¥–µ–æ, 3D-–∞—Å—Å–µ—Ç) –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ç–ª–∏—á–∏—Ç—å –æ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ–∞–ª—å–Ω–æ –º–æ–≥ –±—ã —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —á–µ–ª–æ–≤–µ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –∏ –ø—Ä–∏–≤–∞—Ç–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π AI.'}, 'en': {'title': "Evaluating AI's Human-Like Visual Personalization", 'desc': 'The paper introduces the Visual Personalization Turing Test (VPTT), a new method for evaluating how well AI can create personalized visual content that feels human-made. Instead of just copying existing images, the VPTT focuses on whether the generated content is indistinguishable from what a real person might create. The framework includes a large benchmark of 10,000 personas, a visual retrieval-augmented generator, and a scoring system that aligns with human and AI judgments. Results show that this new evaluation method effectively measures the quality of personalized AI outputs while maintaining privacy and originality.'}, 'zh': {'title': 'ËßÜËßâ‰∏™ÊÄßÂåñÁöÑÊñ∞ËØÑ‰º∞Ê†áÂáÜ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËØÑ‰º∞Ê°ÜÊû∂ÔºåÁß∞‰∏∫ËßÜËßâ‰∏™ÊÄßÂåñÂõæÁÅµÊµãËØïÔºàVPTTÔºâÔºåÁî®‰∫éËØÑ‰º∞‰∏ä‰∏ãÊñáËßÜËßâ‰∏™ÊÄßÂåñ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊÑüÁü•‰∏çÂèØÂå∫ÂàÜÊÄßÊù•Âà§Êñ≠ÁîüÊàêÂÜÖÂÆπÊòØÂê¶‰∏é‰∫∫Á±ªÂàõ‰ΩúÁöÑÂÜÖÂÆπÁõ∏‰ººÔºåËÄå‰∏çÊòØÁÆÄÂçïÁöÑË∫´‰ªΩÂ§çÂà∂„ÄÇVPTTÊ°ÜÊû∂ÂåÖÊã¨‰∏Ä‰∏™ÂåÖÂê´1‰∏á‰∏™‰∫∫Áâ©ÁöÑÂü∫ÂáÜÔºàVPTT-BenchÔºâ„ÄÅ‰∏Ä‰∏™ËßÜËßâÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÂô®ÔºàVPRAGÔºâÂíå‰∏Ä‰∏™Âü∫‰∫éÊñáÊú¨ÁöÑVPTTËØÑÂàÜÊåáÊ†á„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåVPRAGÂú®ÂØπÈΩêÊÄßÂíåÂéüÂàõÊÄß‰πãÈó¥ËææÂà∞‰∫ÜÊúÄ‰Ω≥Âπ≥Ë°°Ôºå‰∏∫‰∏™ÊÄßÂåñÁîüÊàêAIÊèê‰æõ‰∫ÜÂèØÊâ©Â±ï‰∏îÂÆâÂÖ®ÁöÑÂü∫Á°Ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21666', 'title': 'SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding', 'url': 'https://huggingface.co/papers/2601.21666', 'abstract': 'A comprehensive benchmark for evaluating multimodal large language models on sequential audio-video data across real-world conversational domains with human-verified annotations and demographic metadata.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard', 'score': 0, 'issue_id': 882, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '2e01b18fccc25fcc', 'authors': ['Ahmed Y. Radwan', 'Christos Emmanouilidis', 'Hina Tabassum', 'Deval Pandya', 'Shaina Raza'], 'affiliations': ['University of Groningen, Nijenborgh 4, 9747 AG Groningen, Netherlands', 'Vector Institute Intelligence, MaRS for Artificial Centre, Toronto, ON M5G 1L7, Canada', 'York University, 4700 Keele Street, Toronto, ON M3J 1P3, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2601.21666.jpg', 'data': {'categories': ['#dataset', '#audio', '#multimodal', '#benchmark', '#video'], 'emoji': 'üé¨', 'ru': {'title': '–û—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ-–∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç SONIC-O1, –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ-–≤–∏–¥–µ–æ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤. –ë–µ–Ω—á–º–∞—Ä–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç 4958 –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 13 —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –æ–±—â–µ–Ω–∏—è. –ú–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∏–≤–∞–ª–∏—Å—å –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö: –æ—Ç–∫—Ä—ã—Ç–æ–µ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏ —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –≤—ã—è–≤–∏–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã: —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∏ –∑–∞–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 22,6% –¥–ª—è –∑–∞–¥–∞—á–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏, –∞ —Ç–∞–∫–∂–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ —Å—Ä–µ–¥–∏ —Ä–∞–∑–Ω—ã—Ö –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø.'}, 'en': {'title': 'SONIC-O1: Bridging the Gap in Multimodal Language Understanding', 'desc': 'This paper presents SONIC-O1, a benchmark designed to evaluate multimodal large language models (MLLMs) on sequential audio-video data in real-world conversational contexts. Unlike previous studies that primarily focused on static images, SONIC-O1 includes human-verified annotations across 13 domains, allowing for a comprehensive assessment of MLLM capabilities. The benchmark tests various tasks such as open-ended summarization and multiple-choice question answering, revealing significant performance gaps, particularly in temporal localization. Additionally, the findings highlight disparities in model performance across different demographic groups, emphasizing the need for socially robust AI systems.'}, 'zh': {'title': 'SONIC-O1ÔºöÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãËØÑ‰º∞ÁöÑÊñ∞Âü∫ÂáÜ', 'desc': 'ËøôÁØáËÆ∫Êñá‰ªãÁªç‰∫ÜSONIC-O1ÔºåËøôÊòØ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®È°∫Â∫èÈü≥È¢ë-ËßÜÈ¢ëÊï∞ÊçÆ‰∏äÁöÑÂü∫ÂáÜÊµãËØï„ÄÇËØ•Âü∫ÂáÜÊ∂µÁõñ‰∫Ü13‰∏™ÁúüÂÆû‰∏ñÁïåÁöÑÂØπËØùÈ¢ÜÂüüÔºåÂåÖÂê´4,958‰∏™ÁªèËøá‰∫∫Â∑•È™åËØÅÁöÑÊ≥®ÈáäÂíå‰∫∫Âè£ÁªüËÆ°ÂÖÉÊï∞ÊçÆ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°Âú®Â§öÈ°πÈÄâÊã©È¢òÔºàMCQÔºâÂáÜÁ°ÆÊÄß‰∏äÔºåÈó≠Ê∫êÂíåÂºÄÊ∫êÊ®°Âûã‰πãÈó¥ÁöÑÂ∑ÆË∑ùËæÉÂ∞èÔºå‰ΩÜÂú®Êó∂Èó¥ÂÆö‰Ωç‰ªªÂä°‰∏äÔºåÈó≠Ê∫êÊ®°ÂûãÁöÑË°®Áé∞ÊòéÊòæ‰ºò‰∫éÂºÄÊ∫êÊ®°ÂûãÔºåÂ∑ÆË∑ùËææÂà∞22.6%„ÄÇÊ≠§Â§ñÔºåÊ®°ÂûãÂú®‰∏çÂêå‰∫∫Âè£Áæ§‰Ωì‰∏≠ÁöÑË°®Áé∞Â∑ÆÂºÇ‰πüË°®ÊòéÔºåÊ®°ÂûãË°å‰∏∫Â≠òÂú®ÊåÅÁª≠ÁöÑ‰∏çÂπ≥Á≠âÁé∞Ë±°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01785', 'title': 'CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding', 'url': 'https://huggingface.co/papers/2602.01785', 'abstract': 'Multimodal large language models can effectively understand source code when represented as compressed images, achieving significant token reduction while maintaining or improving performance on code comprehension tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.', 'score': 79, 'issue_id': 892, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '7f6abd8b2889ebbe', 'authors': ['Yuling Shi', 'Chaoxiang Xie', 'Zhensu Sun', 'Yeheng Chen', 'Chenxu Zhang', 'Longfei Yun', 'Chengcheng Wan', 'Hongyu Zhang', 'David Lo', 'Xiaodong Gu'], 'affiliations': ['Beijing Institute of Technology', 'Chongqing University', 'East China Normal University', 'Hohai University', 'Imperial College London', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'Singapore Management University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2602.01785.jpg', 'data': {'categories': ['#plp', '#multimodal', '#inference'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ö–æ–¥ –∫–∞–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: —Å–∂–∞—Ç–∏–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–Ω–æ–≥–æ–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤ –≤–∏–¥–µ —Å–∂–∞—Ç—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ 8 —Ä–∞–∑ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∞ –∏–Ω–æ–≥–¥–∞ –¥–∞–∂–µ —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–ª–∞–≥–æ–¥–∞—Ä—è –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Å–∏–≥–Ω–∞–ª–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∑–∞–¥–∞—á–∏ –≤—Ä–æ–¥–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∫–ª–æ–Ω–æ–≤ –∫–æ–¥–∞ –æ–±–ª–∞–¥–∞—é—Ç –≤—ã—Å–æ–∫–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å—é –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É —Å–∂–∞—Ç–∏—é –∏ –º–æ–≥—É—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è. –≠—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–π –ø—É—Ç—å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∫–æ–¥–æ–º.'}, 'en': {'title': 'Revolutionizing Code Understanding with Image-Based Compression', 'desc': 'This paper explores the use of Multimodal Large Language Models (MLLMs) for understanding source code by representing it as compressed images instead of traditional text. By converting code into images, the models can achieve significant token reduction, with up to 8x compression, while still maintaining or improving performance on code comprehension tasks. The study shows that MLLMs can utilize visual features like syntax highlighting to enhance code completion, even under high compression. Overall, the research suggests that using image representation for code can lead to more efficient processing and opens new avenues for optimizing code understanding in large software systems.'}, 'zh': {'title': 'ÂõæÂÉèÂåñ‰ª£Á†ÅË°®Á§∫ÔºåÊèêÂçáÁêÜËß£ÊïàÁéáÔºÅ', 'desc': 'Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâËÉΩÂ§üÈÄöËøáÂ∞ÜÊ∫ê‰ª£Á†ÅË°®Á§∫‰∏∫ÂéãÁº©ÂõæÂÉèÊù•ÊúâÊïàÁêÜËß£‰ª£Á†ÅÔºåËøôÊ†∑ÂèØ‰ª•ÊòæËëóÂáèÂ∞ë‰ª§ÁâåÊï∞ÈáèÔºåÂêåÊó∂Âú®‰ª£Á†ÅÁêÜËß£‰ªªÂä°‰∏ä‰øùÊåÅÊàñÊèêÈ´òÊÄßËÉΩ„ÄÇ‰º†ÁªüÁöÑÊñáÊú¨Âü∫Á°ÄÊ®°ÂûãÂú®Â§ÑÁêÜÊ∫ê‰ª£Á†ÅÊó∂ÔºåÈöèÁùÄ‰∏ä‰∏ãÊñáÈïøÂ∫¶ÁöÑÁ∫øÊÄßÂ¢ûÂä†ÔºåËÆ°ÁÆóÊàêÊú¨‰πüÈöè‰πã‰∏äÂçá„ÄÇÁõ∏ÊØî‰πã‰∏ãÔºåÂõæÂÉèÊ®°ÊÄÅÊõ¥ÈÄÇÂêàÂéãÁº©ÔºåÂõ†‰∏∫ÂèØ‰ª•ÈÄöËøáË∞ÉÊï¥ÂàÜËæ®ÁéáÊù•ÂáèÂ∞ëÂéüÂßã‰ª§ÁâåÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÂèØËØÜÂà´ÊÄß„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåMLLMsÂú®‰ª£Á†ÅÁêÜËß£ÊñπÈù¢ÂÖ∑ÊúâÊòæËëóÁöÑÊΩúÂäõÔºåÂ∞§ÂÖ∂ÊòØÂú®ËßÜËßâÊèêÁ§∫ÁöÑÂà©Áî®ÂíåÂéãÁº©ÊØîÊñπÈù¢„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03786', 'title': 'AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration', 'url': 'https://huggingface.co/papers/2602.03786', 'abstract': 'AOrchestra is a framework-agnostic agentic system that uses a tuple-based abstraction to dynamically create specialized task executors, achieving improved performance on complex benchmarks through automated agent creation and resource management.  \t\t\t\t\tAI-generated summary \t\t\t\t Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra', 'score': 62, 'issue_id': 893, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'f6bc21af3253a89f', 'authors': ['Jianhao Ruan', 'Zhihao Xu', 'Yiran Peng', 'Fashen Ren', 'Zhaoyang Yu', 'Xinbing Liang', 'Jinyu Xiang', 'Bang Liu', 'Chenglin Wu', 'Yuyu Luo', 'Jiayi Zhang'], 'affiliations': ['DeepWisdom', 'ECNU', 'HKUST(GZ)', 'RUC', 'UdeM & Mila'], 'pdf_title_img': 'assets/pdf/title_img/2602.03786.jpg', 'data': {'categories': [], 'emoji': 'üéº', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –∫–æ—Ä—Ç–µ–∂–Ω—É—é –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é', 'desc': 'AOrchestra ‚Äî —ç—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é –≤ –≤–∏–¥–µ –∫–æ—Ä—Ç–µ–∂–∞ (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –º–æ–¥–µ–ª—å) –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π –∑–∞–¥–∞—á. –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ, –æ—Ç–±–∏—Ä–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –∞ –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞–≤–∞—è –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –ª–µ—Ç—É –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á. –°–∏—Å—Ç–µ–º–∞ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –∏–Ω–∂–µ–Ω–µ—Ä–∏—é –±–ª–∞–≥–æ–¥–∞—Ä—è –≥–∏–±–∫–æ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ù–∞ —Å–ª–æ–∂–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö (GAIA, SWE-Bench, Terminal-Bench) AOrchestra –ø–æ–∫–∞–∑–∞–ª–∞ 16,28% —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª—É—á—à–µ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ Gemini-3-Flash.'}, 'en': {'title': 'Dynamic Agent Creation for Enhanced Task Automation', 'desc': 'AOrchestra is a versatile system designed to enhance task automation by dynamically creating specialized agents for complex tasks. It utilizes a tuple-based abstraction that includes Instruction, Context, Tools, and Model, allowing for flexible and efficient task execution. This framework-agnostic approach reduces the need for extensive human engineering and supports various agent types as executors. By optimizing resource management and adaptability, AOrchestra demonstrates significant performance improvements on challenging benchmarks.'}, 'zh': {'title': 'AOrchestraÔºöÂä®ÊÄÅÂàõÂª∫Êô∫ËÉΩ‰ªªÂä°ÊâßË°åÂô®ÁöÑÊ°ÜÊû∂', 'desc': 'AOrchestraÊòØ‰∏Ä‰∏™‰∏éÊ°ÜÊû∂Êó†ÂÖ≥ÁöÑÊô∫ËÉΩÁ≥ªÁªüÔºå‰ΩøÁî®Âü∫‰∫éÂÖÉÁªÑÁöÑÊäΩË±°Âä®ÊÄÅÂàõÂª∫‰∏ìÈó®ÁöÑ‰ªªÂä°ÊâßË°åÂô®Ôºå‰ªéËÄåÂú®Â§çÊùÇÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞Êõ¥Â•ΩÁöÑÊÄßËÉΩ„ÄÇËØ•Á≥ªÁªüÈÄöËøáÂ∞Ü‰ªª‰Ωï‰ª£ÁêÜÂª∫Ê®°‰∏∫‰∏Ä‰∏™ÂÖÉÁªÑÔºàÊåá‰ª§„ÄÅ‰∏ä‰∏ãÊñá„ÄÅÂ∑•ÂÖ∑„ÄÅÊ®°ÂûãÔºâÊù•Ëß£ÂÜ≥Áé∞ÊúâËÆæËÆ°Áº∫‰πèÂä®ÊÄÅÊäΩË±°ËßÜÂõæÁöÑÈóÆÈ¢ò„ÄÇAOrchestraÁöÑ‰∏≠ÂøÉÂçèË∞ÉÂô®Âú®ÊØè‰∏ÄÊ≠•ÂÖ∑‰ΩìÂåñËøô‰∏™ÂÖÉÁªÑÔºåÁ≠ñÂàí‰∏é‰ªªÂä°Áõ∏ÂÖ≥ÁöÑ‰∏ä‰∏ãÊñáÔºåÈÄâÊã©Â∑•ÂÖ∑ÂíåÊ®°ÂûãÔºåÂπ∂ÈÄöËøáÂç≥Êó∂Ëá™Âä®ÂàõÂª∫‰ª£ÁêÜÊù•ÂßîÊ¥æÊâßË°å„ÄÇÈÄöËøáËøôÁßçËÆæËÆ°ÔºåAOrchestraÂáèÂ∞ë‰∫Ü‰∫∫Â∑•Â∑•Á®ãÁöÑÂ∑•‰ΩúÈáèÔºåÂπ∂ÊîØÊåÅÂ§öÁßç‰ª£ÁêÜ‰Ωú‰∏∫‰ªªÂä°ÊâßË°åÂô®ÁöÑÂç≥ÊèíÂç≥Áî®„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02103', 'title': 'No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs', 'url': 'https://huggingface.co/papers/2602.02103', 'abstract': "Research investigates latent planning dynamics in large language models through a probing method called Tele-Lens, revealing limited global planning and enabling improved uncertainty estimation and CoT bypass recognition.  \t\t\t\t\tAI-generated summary \t\t\t\t This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.", 'score': 57, 'issue_id': 892, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'cc18842c82f089b7', 'authors': ['Liyan Xu', 'Mo Yu', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['WeChat AI, Tencent Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2602.02103.jpg', 'data': {'categories': ['#reasoning', '#training', '#open_source', '#interpretability', '#benchmark'], 'emoji': 'üîç', 'ru': {'title': '–°–∫—Ä—ã—Ç–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ LLM: –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å –≤–º–µ—Å—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏', 'desc': '–†–∞–±–æ—Ç–∞ –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥ –∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏—è Tele-Lens. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ LLM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –≥–¥–µ –Ω–µ–±–æ–ª—å—à–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–æ–∑–∏—Ü–∏–π –º–æ–∂–µ—Ç —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –≤—Å–µ–≥–æ –ø—É—Ç–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω–æ—Å—Ç–∏ —è–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–æ–∑–º–æ–∂–Ω–æ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Unveiling Latent Planning in Language Models with Tele-Lens', 'desc': 'This paper explores how large language models (LLMs) plan their reasoning processes using a method called Tele-Lens. It finds that LLMs often lack comprehensive global planning, instead relying on short-term, incremental reasoning steps. The study highlights the importance of Chain-of-Thought (CoT) in tasks that require multi-step reasoning, while also suggesting that a few key CoT positions can effectively estimate uncertainty in reasoning paths. Additionally, the research shows that it is possible to recognize when CoT is bypassed without losing performance, enhancing our understanding of LLM dynamics.'}, 'zh': {'title': 'Êè≠Á§∫Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊΩúÂú®ËßÑÂàíÂä®ÊÄÅ', 'desc': 'Êú¨Á†îÁ©∂ÈÄöËøá‰∏ÄÁßçÁß∞‰∏∫Tele-LensÁöÑÊé¢ÊµãÊñπÊ≥ïÔºåÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâ‰∏≠ÁöÑÊΩúÂú®ËßÑÂàíÂä®ÊÄÅ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåLLMsÂú®ËøõË°åÊé®ÁêÜÊó∂Ë°®Áé∞Âá∫ÊúâÈôêÁöÑÂÖ®Â±ÄËßÑÂàíËÉΩÂäõÔºå‰∏ªË¶Å‰æùËµñ‰∫éÂ¢ûÈáèËøáÊ∏°ËÄåÈùûÁ≤æÁ°ÆÁöÑÂÖ®Â±ÄËßÑÂàí„ÄÇÂ∞ΩÁÆ°ÈìæÂºèÊÄùÁª¥ÔºàCoTÔºâÂú®Â§öÊ≠•È™§Êé®ÁêÜ‰ªªÂä°‰∏≠‰ªçÁÑ∂Ëá≥ÂÖ≥ÈáçË¶ÅÔºå‰ΩÜÊàë‰ª¨ÊèêÂá∫ÁöÑÂÅáËÆæË°®ÊòéÔºåÂ∞ëÈáèÁöÑCoT‰ΩçÁΩÆÂèØ‰ª•ÊúâÊïà‰ª£Ë°®Êï¥‰∏™Ë∑ØÂæÑÁöÑ‰∏çÁ°ÆÂÆöÊÄß„ÄÇÊàë‰ª¨ËøòÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂú®‰∏çÈôç‰ΩéÊÄßËÉΩÁöÑÊÉÖÂÜµ‰∏ãÔºåËá™Âä®ËØÜÂà´CoTÁöÑÁªïËøá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02660', 'title': 'MARS: Modular Agent with Reflective Search for Automated AI Research', 'url': 'https://huggingface.co/papers/2602.02660', 'abstract': 'MARS is a modular AI research automation framework that uses budget-aware planning, modular construction, and reflective memory to achieve state-of-the-art performance in autonomous machine learning research.  \t\t\t\t\tAI-generated summary \t\t\t\t Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a "Design-Decompose-Implement" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard\'s top methods. Furthermore, the system exhibits qualitative "Aha!" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.', 'score': 45, 'issue_id': 892, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'be555ca2cb8fd2c1', 'authors': ['Jiefeng Chen', 'Bhavana Dalvi Mishra', 'Jaehyun Nam', 'Rui Meng', 'Tomas Pfister', 'Jinsung Yoon'], 'affiliations': ['Google Cloud AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2602.02660.jpg', 'data': {'categories': ['#training', '#agents', '#open_source', '#science', '#benchmark'], 'emoji': 'üî¨', 'ru': {'title': '–ú–æ–¥—É–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω–æ–π –ø–∞–º—è—Ç—å—é –¥–ª—è —ç–∫–æ–Ω–æ–º–Ω–æ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ ML-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è', 'desc': 'MARS ‚Äî —ç—Ç–æ –º–æ–¥—É–ª—å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á—ë—Ç–æ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –ø–æ –¥–µ—Ä–µ–≤—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ, –º–æ–¥—É–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –∫–æ–Ω–≤–µ–π–µ—Ä–æ–º –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è-–¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏-—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏, –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É —Ä–µ—à–µ–Ω–∏—è–º–∏. –°–∏—Å—Ç–µ–º–∞ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ–ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º ML-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏, –∏–∑–±–µ–≥–∞—è –∑–∞—Ç—Ä–∞—Ç–Ω—ã—Ö –º–æ–Ω–æ–ª–∏—Ç–Ω—ã—Ö —Å–∫—Ä–∏–ø—Ç–æ–≤ –∏ —è–≤–Ω–æ –±–∞–ª–∞–Ω—Å–∏—Ä—É—è –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–∞—Å—Ö–æ–¥–∞–º–∏. MARS –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MLE-Bench –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—É—Ç—è–º–∏ –ø–æ–∏—Å–∫–∞. –û—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ 63% –ø–æ–ª–µ–∑–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç –±–ª–∞–≥–æ–¥–∞—Ä—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä—É –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É –≤–µ—Ç–≤—è–º–∏ –ø–æ–∏—Å–∫–∞, —á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –∫ –≥–ª—É–±–æ–∫–æ–º—É –æ–±—É—á–µ–Ω–∏—é.'}, 'en': {'title': 'MARS: Optimizing AI Research with Smart Planning and Modular Design', 'desc': 'MARS is a framework designed to automate AI research by optimizing the planning and execution of machine learning tasks. It uses budget-aware planning to balance the performance of models with the costs of training them, ensuring efficient resource use. The framework is modular, allowing researchers to break down complex tasks into manageable parts, and it incorporates reflective memory to improve learning from past experiences. MARS has shown to outperform other open-source frameworks in benchmarks, highlighting its effectiveness in generating valuable insights through cross-branch learning.'}, 'zh': {'title': 'MARSÔºöÊô∫ËÉΩÁ†îÁ©∂ÁöÑÊ®°ÂùóÂåñÊ°ÜÊû∂', 'desc': 'MARSÊòØ‰∏Ä‰∏™Ê®°ÂùóÂåñÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ†îÁ©∂Ëá™Âä®ÂåñÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøáÈ¢ÑÁÆóÊÑèËØÜËßÑÂàí„ÄÅÊ®°ÂùóÂåñÊûÑÂª∫ÂíåÂèçÊÄùËÆ∞ÂøÜÊù•ÂÆûÁé∞Ëá™‰∏ªÊú∫Âô®Â≠¶‰π†Á†îÁ©∂ÁöÑÂÖàËøõÊÄßËÉΩ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊàêÊú¨ÂèóÈôêÁöÑËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâÊù•Âπ≥Ë°°ÊÄßËÉΩ‰∏éÊâßË°åË¥πÁî®ÔºåÁ°Æ‰øùÂú®Á†îÁ©∂ËøáÁ®ã‰∏≠ÊúâÊïàÂà©Áî®ËµÑÊ∫ê„ÄÇMARSÈááÁî®‚ÄúËÆæËÆ°-ÂàÜËß£-ÂÆûÁé∞‚ÄùÁöÑÁÆ°ÈÅìÁÆ°ÁêÜÂ§çÊùÇÁöÑÁ†îÁ©∂Â∫ìÔºåÂπ∂ÈÄöËøáÊØîËæÉÂèçÊÄùËÆ∞ÂøÜÂàÜÊûêËß£ÂÜ≥ÊñπÊ°àÂ∑ÆÂºÇÔºå‰ª•ÊèêÁÇºÂá∫È´ò‰ø°Âè∑ÁöÑËßÅËß£„ÄÇËØ•Á≥ªÁªüÂú®MLE-Bench‰∏äË°®Áé∞Âá∫Ëâ≤ÔºåÂ±ïÁé∞Âá∫Ë∑®ÂàÜÊîØËΩ¨ÁßªÁöÑËÉΩÂäõÔºå‰Ωø63%ÁöÑÂ≠¶‰π†Êù•Ëá™‰∫é‰∏çÂêåÊêúÁ¥¢Ë∑ØÂæÑÁöÑÊúâÊïàÊ¶ÇÊã¨„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02619', 'title': 'daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently', 'url': 'https://huggingface.co/papers/2602.02619', 'abstract': "Large language models face challenges in long-horizon agentic workflows due to lack of authentic long-dependency training data, which is addressed by leveraging pull request sequences for structured supervision through progressive decomposition, consistency enforcement, and refinement from bug-fix histories.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...", 'score': 43, 'issue_id': 893, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'b23ddd364497b1ab', 'authors': ['Mohan Jiang', 'Dayuan Fu', 'Junhao Shi', 'Ji Zeng', 'Weiye Si', 'Keyu Li', 'Xuefeng Li', 'Yang Xiao', 'Wenjie Li', 'Dequan Wang', 'Pengfei Liu'], 'affiliations': ['GAIR', 'PolyU', 'SII Open Source', 'SJTU'], 'pdf_title_img': 'assets/pdf/title_img/2602.02619.jpg', 'data': {'categories': ['#training', '#synthetic', '#alignment', '#data', '#long_context', '#reasoning', '#agents'], 'emoji': 'üîÑ', 'ru': {'title': '–û—Ç –∫–æ–¥–∞ –∫ –∞–≥–µ–Ω—Ç–∞–º: —Å–∏–Ω—Ç–µ–∑ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∞–≥–µ–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É —à–∞–≥–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π pull request –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ daVinci-Agency –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ü–µ–ø–æ—á–µ–∫ PR —á–µ—Ä–µ–∑ —Ç—Ä–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞: –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –∑–∞–¥–∞—á, –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é —Ä–µ—Ñ–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –æ—à–∏–±–æ–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (239 –æ–±—Ä–∞–∑—Ü–æ–≤) –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.'}, 'en': {'title': 'Harnessing Pull Requests for Long-Horizon Learning in LLMs', 'desc': "This paper addresses the limitations of large language models (LLMs) in handling long-horizon tasks due to insufficient training data that captures long-term dependencies. The authors propose a novel approach called daVinci-Agency, which utilizes pull request sequences from software development as a source of structured supervision. By breaking down complex tasks into manageable units and enforcing consistency through real-world bug-fix histories, the model learns to maintain functional coherence over time. The results show that fine-tuning on this data significantly improves the model's performance on various benchmarks, demonstrating the effectiveness of leveraging authentic software evolution data for training."}, 'zh': {'title': 'Âà©Áî®ÊãâÂèñËØ∑Ê±ÇÊèêÂçáÈïø‰æùËµñÂ≠¶‰π†ÁöÑÊïàÁéá', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÈïøÊó∂Èó¥Ë∑®Â∫¶ÁöÑ‰ªªÂä°Êó∂Èù¢‰∏¥ÊåëÊàòÔºå‰∏ªË¶ÅÊòØÂõ†‰∏∫Áº∫‰πèÁúüÂÆûÁöÑÈïø‰æùËµñËÆ≠ÁªÉÊï∞ÊçÆ„ÄÇÊú¨ÊñáÊèêÂá∫ÈÄöËøáÂà©Áî®ÊãâÂèñËØ∑Ê±ÇÂ∫èÂàóÊù•Ëé∑ÂèñÁªìÊûÑÂåñÁõëÁù£Ôºå‰ªéËÄåËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢ò„ÄÇÊàë‰ª¨ÁöÑÊñπÊ≥ïÂåÖÊã¨ÈÄêÊ≠•ÂàÜËß£Â§çÊùÇÁõÆÊ†á„ÄÅÂº∫Âà∂‰∏ÄËá¥ÊÄß‰ª•Âèä‰ªéÈîôËØØ‰øÆÂ§çÂéÜÂè≤‰∏≠ÊèêÁÇº‰ø°ÊÅØ„ÄÇÊúÄÁªàÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãdaVinci-AgencyËÉΩÂ§üÊúâÊïàÂú∞Â≠¶‰π†ÈïøÊúüÁõÆÊ†áÂØºÂêëË°å‰∏∫ÔºåÂπ∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÂçáÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03796', 'title': '3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation', 'url': 'https://huggingface.co/papers/2602.03796', 'abstract': "3DiMo enables view-agnostic human motion control in video generation by training a motion encoder alongside a pretrained video generator to distill driving frames into compact motion tokens that align with the generator's spatial priors.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.", 'score': 41, 'issue_id': 892, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '84bc62536de9a4a1', 'authors': ['Zhixue Fang', 'Xu He', 'Songlin Tang', 'Haoxian Zhang', 'Qingfeng Li', 'Xiaoqiang Liu', 'Pengfei Wan', 'Kun Gai'], 'affiliations': ['CASIA', 'Kling Team, Kuaishou Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.03796.jpg', 'data': {'categories': ['#video', '#training', '#architecture', '#multimodal', '#optimization', '#3d'], 'emoji': 'üé¨', 'ru': {'title': '–ù–µ—è–≤–Ω–æ–µ 3D –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —É–≥–ª–∞ –æ–±–∑–æ—Ä–∞', 'desc': '3DiMo ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–µ–º —á–µ–ª–æ–≤–µ–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç —ç–Ω–∫–æ–¥–µ—Ä –¥–≤–∏–∂–µ–Ω–∏—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–º –≤–∏–¥–µ–æ. –í–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è 2D –ø–æ–∑ –∏–ª–∏ —è–≤–Ω—ã—Ö 3D –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π, –ø–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—è–≤–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≤–∏–¥–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–æ—Ä–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –≤–∏–¥–µ–æ —Å —Ä–∞–∑–Ω—ã—Ö —É–≥–ª–æ–≤ –æ–±–∑–æ—Ä–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π –Ω–∞—É—á–∏—Ç—å—Å—è –ø–æ–Ω–∏–º–∞—Ç—å –∏—Å—Ç–∏–Ω–Ω—É—é 3D —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–≤–∏–∂–µ–Ω–∏—è, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥—è –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ —Ç–æ—á–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –¥–≤–∏–∂–µ–Ω–∏—è —Å –≥–∏–±–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∫–∞–º–µ—Ä–æ–π, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ –≤–µ—Ä–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è –∏ –∫–∞—á–µ—Å—Ç–≤—É –≤–∏–¥–µ–æ.'}, 'en': {'title': 'View-Agnostic Motion Control for Enhanced Video Generation', 'desc': "3DiMo is a novel approach for controlling human motion in video generation that does not depend on specific viewpoints. It trains a motion encoder alongside a video generator to create compact motion tokens that align with the generator's understanding of space. This method avoids the limitations of 2D poses and explicit 3D models by using a view-agnostic representation, allowing for more flexible and accurate motion synthesis. The model is trained with diverse video inputs to ensure consistent motion across different perspectives, leading to improved motion fidelity and visual quality in generated videos."}, 'zh': {'title': '3DiMoÔºöËßÜËßíÊó†ÂÖ≥ÁöÑ‰∫∫‰ΩìËøêÂä®ÊéßÂà∂Êñ∞ÊñπÊ≥ï', 'desc': '3DiMoÊòØ‰∏ÄÁßçÊñ∞ÊñπÊ≥ïÔºåÁî®‰∫éÂú®ËßÜÈ¢ëÁîüÊàê‰∏≠ÂÆûÁé∞‰∏éËßÜËßíÊó†ÂÖ≥ÁöÑ‰∫∫‰ΩìËøêÂä®ÊéßÂà∂„ÄÇÂÆÉÈÄöËøá‰∏éÈ¢ÑËÆ≠ÁªÉÁöÑËßÜÈ¢ëÁîüÊàêÂô®ÂÖ±ÂêåËÆ≠ÁªÉ‰∏Ä‰∏™ËøêÂä®ÁºñÁ†ÅÂô®ÔºåÂ∞ÜÈ©±Âä®Â∏ßÊèêÁÇº‰∏∫Á¥ßÂáëÁöÑËøêÂä®Ê†áËÆ∞ÔºåËøô‰∫õÊ†áËÆ∞‰∏éÁîüÊàêÂô®ÁöÑÁ©∫Èó¥ÂÖàÈ™åÁõ∏‰∏ÄËá¥„ÄÇ‰∏é‰º†ÁªüÊñπÊ≥ï‰∏çÂêåÔºå3DiMo‰∏ç‰æùËµñ‰∫é2DÂßøÂäøÊàñÊòæÂºèÁöÑ3DÊ®°ÂûãÔºåËÄåÊòØÈááÁî®ÈöêÂºèÁöÑËøêÂä®Ë°®Á§∫Ôºå‰ªéËÄåÈÅøÂÖç‰∫ÜËßÜËßíÈôêÂà∂ÂíåÁªìÊûÑ‰∏çÂáÜÁ°ÆÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åË°®ÊòéÔºå3DiMoÂú®ËøêÂä®‰øùÁúüÂ∫¶ÂíåËßÜËßâË¥®Èáè‰∏äÊòæËëó‰ºò‰∫éÁé∞ÊúâÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01630', 'title': 'Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks', 'url': 'https://huggingface.co/papers/2602.01630', 'abstract': 'Current world models lack unified frameworks despite task-specific advances, necessitating a comprehensive approach integrating interaction, perception, symbolic reasoning, and spatial representation.  \t\t\t\t\tAI-generated summary \t\t\t\t World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.', 'score': 41, 'issue_id': 892, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '1a3d327b5e3d7ffa', 'authors': ['Bohan Zeng', 'Kaixin Zhu', 'Daili Hua', 'Bozhou Li', 'Chengzhuo Tong', 'Yuran Wang', 'Xinyi Huang', 'Yifan Dai', 'Zixiang Zhang', 'Yifan Yang', 'Zhou Liu', 'Hao Liang', 'Xiaochen Ma', 'Ruichuan An', 'Tianyi Bai', 'Hongcheng Gao', 'Junbo Niu', 'Yang Shi', 'Xinlong Chen', 'Yue Ding', 'Minglei Shi', 'Kai Zeng', 'Yiwen Tang', 'Yuanxing Zhang', 'Pengfei Wan', 'Xintao Wang', 'Wentao Zhang'], 'affiliations': ['HKUST', 'Kling Team, Kuaishou Technology', 'Peking University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01630.jpg', 'data': {'categories': [], 'emoji': 'üåç', 'ru': {'title': '–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ü–µ–ª–æ—Å—Ç–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã', 'desc': '–í —Å—Ç–∞—Ç—å–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–µ–≥–æ–¥–Ω—è —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∏—Ä–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–ª–∂–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º, –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ, —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –µ–¥–∏–Ω–æ–µ —Ü–µ–ª–æ–µ. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –º–∏—Ä–∞. –¢–∞–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–æ–ª–∂–µ–Ω –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –±—É–¥—É—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –±–æ–ª–µ–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö, —Ä–æ–±–∞—Å—Ç–Ω—ã—Ö –∏ –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∏—Ä–∞.'}, 'en': {'title': 'Towards a Unified Framework for World Models in AI', 'desc': 'This paper discusses the need for a unified framework in world models within AI, which currently focus on specific tasks like visual prediction or 3D estimation. It highlights that while these task-specific models improve performance, they often lack a cohesive structure for understanding the world as a whole. The authors propose a comprehensive design specification that integrates key components such as interaction, perception, symbolic reasoning, and spatial representation. The goal is to guide future research towards developing more robust and generalizable world models that can effectively understand and interact with complex environments.'}, 'zh': {'title': 'ÊûÑÂª∫Áªü‰∏ÄÁöÑ‰∏ñÁïåÊ®°ÂûãÊ°ÜÊû∂', 'desc': 'ÂΩìÂâçÁöÑ‰∏ñÁïåÊ®°ÂûãÂú®‰ªªÂä°ÁâπÂÆöÁöÑËøõÂ±ï‰∏≠Áº∫‰πèÁªü‰∏ÄÁöÑÊ°ÜÊû∂ÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏ÄÁßçÁªºÂêàÁöÑÊñπÊ≥ïÊù•Êï¥Âêà‰∫§‰∫í„ÄÅÊÑüÁü•„ÄÅÁ¨¶Âè∑Êé®ÁêÜÂíåÁ©∫Èó¥Ë°®Á§∫„ÄÇ‰∏ñÁïåÊ®°ÂûãÊó®Âú®ÈÄöËøáÂºïÂÖ•Áâ©ÁêÜÂä®ÊÄÅÂíå‰∏ñÁïåÁü•ËØÜÊù•Â¢ûÂº∫Â§ßÂûãÊ®°ÂûãÔºå‰ΩøÊô∫ËÉΩ‰ΩìËÉΩÂ§üÁêÜËß£„ÄÅÈ¢ÑÊµãÂíå‰∏éÂ§çÊùÇÁéØÂ¢É‰∫íÂä®„ÄÇÁé∞ÊúâÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®Â∞Ü‰∏ñÁïåÁü•ËØÜÊ≥®ÂÖ•Â≠§Á´ãÁöÑ‰ªªÂä°‰∏≠ÔºåËÄå‰∏çÊòØÂª∫Á´ãÁªü‰∏ÄÁöÑÂÆö‰πâÊàñÊ°ÜÊû∂„ÄÇÊú¨ÊñáÂàÜÊûê‰∫ÜËøô‰∫õÁ¢éÁâáÂåñÊñπÊ≥ïÁöÑÂ±ÄÈôêÊÄßÔºåÂπ∂ÊèêÂá∫‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑ‰∏ñÁïåÊ®°ÂûãËÆæËÆ°ËßÑËåÉÔºå‰ª•ÊåáÂØºÊú™Êù•ÁöÑÁ†îÁ©∂„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03048', 'title': 'CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs', 'url': 'https://huggingface.co/papers/2602.03048', 'abstract': "CoBA-RL adapts rollout budget allocation for LLM training by evaluating sample training value and optimizing resource distribution through a capability-oriented value function and greedy strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.", 'score': 32, 'issue_id': 892, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'f231912a460b9894', 'authors': ['Zhiyuan Yao', 'Yi-Kai Zhang', 'Yuxin Chen', 'Yueqing Sun', 'Zishan Xu', 'Yu Yang', 'Tianhao Hu', 'Qi Gu', 'Hui Su', 'Xunliang Cai'], 'affiliations': ['Meituan', 'Nanjing University', 'National University of Singapore', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2602.03048.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–£–º–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'CoBA-RL ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —Ä–∞–∑–≤–∏–≤–∞—é—â–∏—Ö—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —Å—Ç–æ–∏–º–æ—Å—Ç–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞. –ê–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∂–∞–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–π –æ—á–µ—Ä–µ–¥–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –±—é–¥–∂–µ—Ç–∞ –æ—Ç–∫–∞—Ç–æ–≤ –∫ –æ–±—Ä–∞–∑—Ü–∞–º —Å –≤—ã—Å–æ–∫–æ–π –æ–±—É—á–∞—é—â–µ–π —Ü–µ–Ω–Ω–æ—Å—Ç—å—é. –ü–æ–¥—Ö–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –æ–±–æ–±—â–µ–Ω–∏—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LLM —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.'}, 'en': {'title': 'Optimizing Training Budgets for Smarter LLMs', 'desc': "CoBA-RL is a novel reinforcement learning algorithm that optimizes the allocation of rollout budgets during the training of large language models (LLMs). It introduces a Capability-Oriented Value function to assess the potential training gains of different tasks, allowing for a more efficient distribution of computational resources. Unlike traditional methods that use a uniform budget, CoBA-RL employs a greedy strategy to focus on samples that offer the highest training value. The results show that this adaptive approach significantly enhances the model's generalization performance across various benchmarks by effectively balancing exploration and exploitation."}, 'zh': {'title': 'Êô∫ËÉΩÂàÜÈÖçËÆ≠ÁªÉÈ¢ÑÁÆóÔºåÊèêÂçáÊ®°ÂûãÂ≠¶‰π†ÊïàÁéá', 'desc': 'CoBA-RLÊòØ‰∏ÄÁßçÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïÔºåÊó®Âú®Ê†πÊçÆÊ®°ÂûãÁöÑÂä®ÊÄÅËÉΩÂäõËá™ÈÄÇÂ∫îÂú∞ÂàÜÈÖçËÆ≠ÁªÉÈ¢ÑÁÆó„ÄÇÂÆÉ‰ΩøÁî®ËÉΩÂäõÂØºÂêë‰ª∑ÂÄºÂáΩÊï∞Êù•ËØÑ‰º∞‰ªªÂä°ÁöÑÊΩúÂú®ËÆ≠ÁªÉÊî∂ÁõäÔºåÂπ∂ÈÄöËøáË¥™Â©™Á≠ñÁï•‰ºòÂåñËÆ°ÁÆóËµÑÊ∫êÁöÑÂàÜÈÖç„ÄÇ‰∏é‰º†ÁªüÁöÑÂùáÂåÄÈ¢ÑÁÆóÂàÜÈÖçÊñπÊ≥ï‰∏çÂêåÔºåCoBA-RLËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âà©Áî®ËµÑÊ∫êÔºåÊèêÈ´òÊ®°ÂûãÁöÑÂ≠¶‰π†ÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03139', 'title': 'Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis', 'url': 'https://huggingface.co/papers/2602.03139', 'abstract': 'A novel distillation framework called DP-DMD is introduced that preserves sample diversity in text-to-image generation by separating the roles of distilled steps, using v-prediction for diversity and standard DMD loss for quality refinement without additional computational overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.', 'score': 31, 'issue_id': 893, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '8ca33a5a8a7e7eb3', 'authors': ['Tianhe Wu', 'Ruibin Li', 'Lei Zhang', 'Kede Ma'], 'affiliations': ['Multimedia-Analytics-Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2602.03139.jpg', 'data': {'categories': ['#multimodal', '#training', '#inference', '#diffusion', '#optimization'], 'emoji': 'üé®', 'ru': {'title': '–î–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ DP-DMD –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–æ–ª–ª–∞–ø—Å–∞ –º–æ–¥ (–ø–æ—Ç–µ—Ä–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è). –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ —Ä–æ–ª–µ–π –¥–∏—Å—Ç–∏–ª–ª–∏—Ä—É–µ–º—ã—Ö —à–∞–≥–æ–≤: –ø–µ—Ä–≤—ã–π —à–∞–≥ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤—ã–±–æ—Ä–æ–∫ —á–µ—Ä–µ–∑ v-prediction, –∞ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –ø–æ–º–æ—â—å—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π DMD –ø–æ—Ç–µ—Ä–∏. –ú–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏ –Ω–∏–∑–∫–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π-–¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–æ—Ä–æ–≤ –∏–ª–∏ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DP-DMD —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –æ–±—Ä–∞–∑—Ü–æ–≤ –ø—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–µ, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–º —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.'}, 'en': {'title': 'Preserving Diversity in Text-to-Image Generation with DP-DMD', 'desc': 'The paper introduces a new framework called Diversity-Preserved DMD (DP-DMD) for text-to-image generation that aims to maintain sample diversity while ensuring high-quality outputs. It separates the distillation process into two distinct steps: the first step focuses on preserving diversity using a target-prediction method, while the second step refines quality using the standard DMD loss. This approach addresses the common issue of mode collapse found in traditional DMD methods, which often require complex regularization techniques that can slow down training. DP-DMD achieves impressive results without the need for additional computational resources or complex architectures, making it efficient and effective for generating diverse and high-quality images.'}, 'zh': {'title': '‰øùÊåÅÂ§öÊ†∑ÊÄßÁöÑÈ´òË¥®ÈáèÁîüÊàê', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËí∏È¶èÊ°ÜÊû∂ÔºåÁß∞‰∏∫DP-DMDÔºåÊó®Âú®Âú®ÊñáÊú¨Âà∞ÂõæÂÉèÁîüÊàê‰∏≠‰øùÊåÅÊ†∑Êú¨Â§öÊ†∑ÊÄß„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂàÜÁ¶ªËí∏È¶èÊ≠•È™§ÁöÑËßíËâ≤Ôºå‰ΩøÁî®ÁõÆÊ†áÈ¢ÑÊµãÊù•Â¢ûÂº∫Â§öÊ†∑ÊÄßÔºåÂêåÊó∂Âà©Áî®Ê†áÂáÜDMDÊçüÂ§±ËøõË°åË¥®Èáè‰ºòÂåñÔºåËÄåÊó†ÈúÄÈ¢ùÂ§ñÁöÑËÆ°ÁÆóÂºÄÈîÄ„ÄÇDP-DMDÁöÑÁ¨¨‰∏ÄÊ≠•‰∏ìÊ≥®‰∫é‰øùÊåÅÊ†∑Êú¨Â§öÊ†∑ÊÄßÔºåÂêéÁª≠Ê≠•È™§Âàô‰∏ìÊ≥®‰∫éË¥®ÈáèÊèêÂçáÔºåÈÅøÂÖç‰∫ÜÊ®°ÂºèÂ¥©Ê∫ÉÁöÑÈóÆÈ¢ò„ÄÇÂÆûÈ™åË°®ÊòéÔºåDP-DMDÂú®‰øùÊåÅËßÜËßâË¥®ÈáèÁöÑÂêåÊó∂ÔºåËÉΩÂ§üÊúâÊïàÂú∞‰øùÁïôÊ†∑Êú¨Â§öÊ†∑ÊÄßÔºåË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂÖàËøõÊñπÊ≥ï„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03419', 'title': 'SWE-World: Building Software Engineering Agents in Docker-Free Environments', 'url': 'https://huggingface.co/papers/2602.03419', 'abstract': 'A Docker-free framework replaces physical execution environments with learned surrogates for training software engineering agents, enabling efficient training and test-time scaling without costly container setup.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World', 'score': 29, 'issue_id': 893, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '168b68461f26472e', 'authors': ['Shuang Sun', 'Huatong Song', 'Lisheng Huang', 'Jinhao Jiang', 'Ran Le', 'Zhihao Lv', 'Zongchao Chen', 'Yiwen Hu', 'Wenyang Luo', 'Wayne Xin Zhao', 'Yang Song', 'Hongteng Xu', 'Tao Zhang', 'Ji-Rong Wen'], 'affiliations': ['BOSS Zhipin, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.03419.jpg', 'data': {'categories': ['#training', '#benchmark', '#rl', '#plp', '#agents'], 'emoji': 'üöÄ', 'ru': {'title': '–í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ Docker –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω SWE-World ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –±–µ–∑ Docker, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–º–µ–Ω—è–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ –æ–±—É—á–µ–Ω–Ω—ã–µ —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ü–û. –ü–æ–¥—Ö–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM-–º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –∞–≥–µ–Ω—Ç–∞ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º, —á—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç —Ç–µ—Å—Ç–æ–≤. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –æ–±—É—á–∞—Ç—å—Å—è –±–µ–∑ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è–º–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞—è —Ä–µ—Å—É—Ä—Å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã. –ë–ª–∞–≥–æ–¥–∞—Ä—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —ç—Ç–∞–ø–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.'}, 'en': {'title': 'SWE-World: Efficient Training for Software Engineering Agents Without Docker', 'desc': "This paper introduces SWE-World, a novel framework that eliminates the need for Docker containers in training software engineering agents. Instead of relying on physical execution environments, SWE-World uses learned surrogates to predict execution outcomes and test feedback, making the training process more efficient. By simulating agent-environment interactions, it allows for effective test-time scaling without the overhead of setting up and maintaining complex environments. The results show significant improvements in performance metrics for software engineering tasks, demonstrating the framework's potential to streamline agent training and evaluation."}, 'zh': {'title': 'Êó†DockerÊ°ÜÊû∂ÊèêÂçáËΩØ‰ª∂Â∑•Á®ã‰ª£ÁêÜËÆ≠ÁªÉÊïàÁéá', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫SWE-WorldÁöÑÊó†DockerÊ°ÜÊû∂ÔºåÁî®‰∫éËÆ≠ÁªÉËΩØ‰ª∂Â∑•Á®ã‰ª£ÁêÜ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂ≠¶‰π†ÁöÑÊõø‰ª£ÁéØÂ¢ÉÔºåÂèñ‰ª£‰∫Ü‰º†ÁªüÁöÑÁâ©ÁêÜÊâßË°åÁéØÂ¢ÉÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜËÆ≠ÁªÉÂíåÊµãËØïÁöÑÊïàÁéá„ÄÇSWE-WorldÂà©Áî®Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ®°ÂûãÔºåÈ¢ÑÊµã‰∏≠Èó¥ÊâßË°åÁªìÊûúÂíåÊúÄÁªàÊµãËØïÂèçÈ¶àÔºå‰Ωø‰ª£ÁêÜËÉΩÂ§üÂú®‰∏ç‰∏éÁâ©ÁêÜÁéØÂ¢É‰∫§‰∫íÁöÑÊÉÖÂÜµ‰∏ãÂ≠¶‰π†„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSWE-WorldÊòæËëóÊèêÈ´ò‰∫Ü‰ª£ÁêÜÁöÑÊÄßËÉΩÔºåÁÆÄÂåñ‰∫ÜÁéØÂ¢ÉÊûÑÂª∫ÂíåÁª¥Êä§ÁöÑÂ§çÊùÇÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03411', 'title': 'SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training', 'url': 'https://huggingface.co/papers/2602.03411', 'abstract': 'SWE-Master presents a reproducible framework for developing software engineering agents through systematic optimization across multiple stages of agent development, achieving superior performance on software task resolution benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.', 'score': 27, 'issue_id': 893, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '093e2d66c0cc0ea5', 'authors': ['Huatong Song', 'Lisheng Huang', 'Shuang Sun', 'Jinhao Jiang', 'Ran Le', 'Daixuan Cheng', 'Guoxin Chen', 'Yiwen Hu', 'Zongchao Chen', 'Wayne Xin Zhao', 'Yang Song', 'Tao Zhang', 'Ji-Rong Wen'], 'affiliations': ['BOSS Zhipin, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.03411.jpg', 'data': {'categories': ['#open_source', '#training', '#benchmark', '#long_context', '#rl', '#plp', '#reasoning', '#agents', '#optimization'], 'emoji': 'üõ†Ô∏è', 'ru': {'title': '–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏', 'desc': 'SWE-Master –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç—É—é –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤, —Ä–µ—à–∞—é—â–∏—Ö –∑–∞–¥–∞—á–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å—å –∫–æ–Ω–≤–µ–π–µ—Ä —Ä–∞–∑–≤–∏—Ç–∏—è –∞–≥–µ–Ω—Ç–∞, –≤–∫–ª—é—á–∞—è —Å–∏–Ω—Ç–µ–∑ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —É—á–∏—Ç–µ–ª—è, –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ –¥–∏–∑–∞–π–Ω –∏–Ω—Ñ–µ—Ä–µ–Ω—Ç–Ω–æ–≥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, –∫–∞–∫ –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å —Å–∏–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ—à–µ–Ω–∏–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏. –ù–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ SWE-bench Verified –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: 61,4% —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –∏ 70,8% –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ —ç—Ç–∞–ø–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏.'}, 'en': {'title': 'Optimizing Software Engineering Agents with SWE-Master', 'desc': "SWE-Master is a framework designed to enhance the development of software engineering agents through systematic optimization. It covers the entire agent development process, including data preparation, supervised fine-tuning, and reinforcement learning with real feedback. By starting with a basic model, SWE-Master effectively improves the agent's ability to solve complex software tasks, achieving a high resolve rate on benchmarks. The framework not only demonstrates superior performance compared to existing models but also emphasizes reproducibility in research."}, 'zh': {'title': 'SWE-MasterÔºöËΩØ‰ª∂Â∑•Á®ã‰ª£ÁêÜÁöÑÁ≥ªÁªü‰ºòÂåñÊ°ÜÊû∂', 'desc': 'SWE-MasterÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂèØÈáçÂ§çÊ°ÜÊû∂ÔºåÁî®‰∫éÈÄöËøáÁ≥ªÁªü‰ºòÂåñÂºÄÂèëËΩØ‰ª∂Â∑•Á®ã‰ª£ÁêÜ„ÄÇÂÆÉÊ∂µÁõñ‰∫Ü‰ª£ÁêÜÂºÄÂèëÁöÑÂêÑ‰∏™Èò∂ÊÆµÔºåÂåÖÊã¨ÊïôÂ∏àËΩ®ËøπÂêàÊàê„ÄÅÊï∞ÊçÆÊï¥ÁêÜ„ÄÅÈïøÊó∂Èó¥ÁöÑÁõëÁù£ÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†„ÄÇÈÄöËøá‰ªé‰∏Ä‰∏™Âü∫Á°ÄÊ®°ÂûãÂºÄÂßãÔºåSWE-MasterÂ±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÁ≥ªÁªüÂåñÁöÑ‰ºòÂåñÊñπÊ≥ïÊèêÂçáËΩØ‰ª∂Â∑•Á®ã‰ªªÂä°ÁöÑËß£ÂÜ≥ËÉΩÂäõ„ÄÇÁªèËøáËØÑ‰º∞ÔºåSWE-MasterÂú®Ê†áÂáÜÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæÁ§∫Âá∫ÂÖ∂Âú®ËΩØ‰ª∂Â∑•Á®ã‰ª£ÁêÜÁ†îÁ©∂‰∏≠ÁöÑÈáçË¶ÅÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03845', 'title': 'Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing', 'url': 'https://huggingface.co/papers/2602.03845', 'abstract': 'Parallel-Probe is a training-free controller that optimizes parallel thinking by using consensus-based early stopping and deviation-based branch pruning to reduce computational costs while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy.', 'score': 21, 'issue_id': 892, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '3d87cb50da672c0c', 'authors': ['Tong Zheng', 'Chengsong Huang', 'Runpeng Dai', 'Yun He', 'Rui Liu', 'Xin Ni', 'Huiwen Bao', 'Kaishen Wang', 'Hongtu Zhu', 'Jiaxin Huang', 'Furong Huang', 'Heng Huang'], 'affiliations': ['City University of Hong Kong', 'Department of Computer Science, University of Maryland, College, Park', 'Tongji University', 'University of North Carolina at Chapel Hill', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2602.03845.jpg', 'data': {'categories': ['#reasoning', '#optimization'], 'emoji': 'üå≥', 'ru': {'title': '–£–º–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ –∫–æ–Ω—Å–µ–Ω—Å—É—Å –∏ –¥–µ–≤–∏–∞—Ü–∏—é', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω Parallel-Probe ‚Äî –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç 2D-–∑–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É —à–∏—Ä–∏–Ω—ã –∏ –≥–ª—É–±–∏–Ω—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–µ—Ç–≤–µ–π –ø—É—Ç—ë–º –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–∞: –∫–æ–Ω—Å–µ–Ω—Å—É—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—Ä–µ–∑–∫–∞ –≤–µ—Ç–≤–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —à–∏—Ä–∏–Ω—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–Ω–∏–∂–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –Ω–∞ 25.8% –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤.'}, 'en': {'title': 'Optimizing Parallel Thinking with Efficiency and Accuracy', 'desc': 'Parallel-Probe is a novel controller that enhances parallel thinking in machine learning without requiring prior training. It utilizes consensus-based early stopping to determine when to halt reasoning and deviation-based branch pruning to optimize the number of branches used. This approach allows for efficient computation by balancing the depth and width of reasoning processes, leading to significant reductions in token usage while preserving accuracy. Experiments show that Parallel-Probe outperforms traditional methods, achieving better efficiency in resource usage during model inference.'}, 'zh': {'title': '‰ºòÂåñÂπ∂Ë°åÊÄùÁª¥ÁöÑÊó†ËÆ≠ÁªÉÊéßÂà∂Âô®', 'desc': 'Parallel-ProbeÊòØ‰∏ÄÁßçÊó†ÈúÄËÆ≠ÁªÉÁöÑÊéßÂà∂Âô®ÔºåÈÄöËøáÂü∫‰∫éÂÖ±ËØÜÁöÑÊèêÂâçÂÅúÊ≠¢ÂíåÂü∫‰∫éÂÅèÂ∑ÆÁöÑÂàÜÊîØ‰øÆÂâ™Êù•‰ºòÂåñÂπ∂Ë°åÊÄùÁª¥ÔºåÈôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÔºåÂêåÊó∂‰øùÊåÅÂáÜÁ°ÆÊÄß„ÄÇËØ•ÊñπÊ≥ïÂà©Áî®2DÊé¢ÊµãÊé•Âè£ÔºåÂÆöÊúü‰ªéÊâÄÊúâÂàÜÊîØËé∑Âèñ‰∏≠Èó¥Á≠îÊ°àÔºåÊè≠Á§∫‰∫ÜÂπ∂Ë°åÊÄùÁª¥ÁöÑÂÆΩÂ∫¶-Ê∑±Â∫¶Âä®ÊÄÅ„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåÂÆΩÂ∫¶ÂíåÊ∑±Â∫¶ÁöÑÂàÜÈÖçÂÖ∑ÊúâÈùûÂçïË∞ÉÁº©ÊîæÁâπÊÄßÔºåÊé®ÁêÜÂàÜÊîØÈïøÂ∫¶‰∏çÂùáÂåÄÔºå‰ª•ÂèäÂÖ®Â±ÄÂÖ±ËØÜÁöÑÊó©ÊúüÁ®≥ÂÆöÊÄß„ÄÇÈÄöËøáËøô‰∫õÊ¥ûÂØüÔºåParallel-ProbeËÉΩÂ§üÂú®ÊµãËØïÊó∂ÂÆûÁé∞Êõ¥‰ºòÁöÑËµÑÊ∫êÂàÜÈÖçÔºåÊòæËëóÂáèÂ∞ëËÆ°ÁÆóÂºÄÈîÄ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03619', 'title': 'Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation', 'url': 'https://huggingface.co/papers/2602.03619', 'abstract': 'DeepResearch report generation is improved through human-preference-aligned rubric generators trained via reinforcement learning with hybrid rewards and enhanced with multi-agent Markov-state workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.', 'score': 20, 'issue_id': 892, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'efff6f143b9f417c', 'authors': ['Changze Lv', 'Jie Zhou', 'Wentao Zhao', 'Jingwen Xu', 'Zisu Huang', 'Muzhao Tian', 'Shihan Dou', 'Tao Gui', 'Le Tian', 'Xiao Zhou', 'Xiaoqing Zheng', 'Xuanjing Huang', 'Jie Zhou'], 'affiliations': ['College of Computer Science and Artificial Intelligence, Fudan University', 'Pattern Recognition Center, WeChat AI, Tencent Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2602.03619.jpg', 'data': {'categories': ['#rlhf', '#benchmark', '#dataset', '#agents'], 'emoji': 'üìã', 'ru': {'title': '–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Ü–µ–Ω–∫–∏ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã—Ä–∞–≤–Ω–µ–Ω—ã —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç—á—ë—Ç–æ–≤, —Å–æ–∑–¥–∞–≤–∞–µ–º—ã—Ö —Å–∏—Å—Ç–µ–º–∞–º–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —Å–æ—á–µ—Ç–∞—é—â–µ–π —Å–∏–≥–Ω–∞–ª—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –∏ –æ—Ü–µ–Ω–∫—É, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–≤–µ–¥–µ–Ω–∞ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è —Ä–∞–±–æ—á–∞—è —Å—Ö–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ–µ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.'}, 'en': {'title': 'Enhancing DeepResearch Reports with Human-Aligned Rubrics', 'desc': 'This paper presents a novel approach to improve the generation of reports in DeepResearch by using human-preference-aligned rubric generators. These generators are trained through reinforcement learning, utilizing a hybrid reward system that combines human preferences and evaluations from large language models (LLMs). The authors introduce a Multi-agent Markov-state (MaMs) workflow to enhance the reasoning capabilities of the report generation process. The results demonstrate that their method provides superior supervision and performance compared to existing rubric strategies, achieving results on par with top closed-source models.'}, 'zh': {'title': 'ÊèêÂçáDeepResearchÊä•ÂëäÁîüÊàêÁöÑËØÑ‰º∞Ë¥®Èáè', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÊù•ÊîπËøõDeepResearchÊä•ÂëäÁîüÊàêÁöÑËØÑ‰º∞ËøáÁ®ã„ÄÇÊàë‰ª¨ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ‰∫∫Á±ªÂÅèÂ•ΩÂØπÈΩêÁöÑËØÑÂàÜÊ†áÂáÜÁîüÊàêÂô®Ôºå‰ª•‰æøÊõ¥Â•ΩÂú∞ËØÑ‰º∞ÁîüÊàêÁöÑÊä•Âëä„ÄÇËØ•ÊñπÊ≥ïÁªìÂêà‰∫Ü‰∫∫Á±ªÂÅèÂ•ΩÁõëÁù£ÂíåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâËØÑ‰º∞ÔºåÊèê‰æõ‰∫ÜÊõ¥ÁªÜËá¥ÁöÑËØÑÂàÜÊ†áÂáÜ„ÄÇÈÄöËøáÂºïÂÖ•Â§öÊô∫ËÉΩ‰ΩìÈ©¨Â∞îÂèØÂ§´Áä∂ÊÄÅÂ∑•‰ΩúÊµÅÔºåÊàë‰ª¨ÁöÑÁ≥ªÁªüÂú®DeepResearchÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02380', 'title': 'Unified Personalized Reward Model for Vision Generation', 'url': 'https://huggingface.co/papers/2602.02380', 'abstract': 'UnifiedReward-Flex combines reward modeling with flexible, context-adaptive reasoning to improve visual generation by dynamically constructing hierarchical assessments based on semantic intent and visual evidence.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.', 'score': 16, 'issue_id': 892, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '2ceb123f5e8506c6', 'authors': ['Yibin Wang', 'Yuhang Zang', 'Feng Han', 'Jiazi Bu', 'Yujie Zhou', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute', 'Shanghai Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02380.jpg', 'data': {'categories': ['#video', '#reasoning', '#multimodal', '#alignment', '#cv', '#rlhf'], 'emoji': 'üé®', 'ru': {'title': '–ì–∏–±–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ UnifiedReward-Flex ‚Äî —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —Å –≥–∏–±–∫–∏–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º. –ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏, –∑–∞—Ç–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Å—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –æ—Ü–µ–Ω–∫—É –ø—É—Ç—ë–º –∏–Ω—Å—Ç–∞–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤. –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞: —Å–Ω–∞—á–∞–ª–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–æ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –æ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –∑–∞—Ç–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä—è–º—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –Ω–∞ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–º GRPO –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.'}, 'en': {'title': 'Dynamic Context-Aware Reward Modeling for Enhanced Visual Generation', 'desc': 'UnifiedReward-Flex is a novel approach that enhances visual generation by integrating reward modeling with adaptable reasoning techniques. It addresses the limitations of traditional reward models, which often use a fixed evaluation system that does not account for specific visual details or human preferences. By dynamically creating hierarchical assessments based on both semantic intent and visual evidence, it allows for more personalized and context-sensitive evaluations. The model is trained through a two-stage process that improves its reasoning capabilities and aligns it more closely with human-like preferences, leading to better performance in generating images and videos.'}, 'zh': {'title': 'ÁÅµÊ¥ªËá™ÈÄÇÂ∫îÁöÑËßÜËßâÁîüÊàêÂ•ñÂä±Ê®°Âûã', 'desc': 'UnifiedReward-Flex ÊòØ‰∏ÄÁßçÁªìÂêàÂ•ñÂä±Âª∫Ê®°ÂíåÁÅµÊ¥ª‰∏ä‰∏ãÊñáËá™ÈÄÇÂ∫îÊé®ÁêÜÁöÑÊ®°ÂûãÔºåÊó®Âú®ÈÄöËøáÂä®ÊÄÅÊûÑÂª∫Âü∫‰∫éËØ≠‰πâÊÑèÂõæÂíåËßÜËßâËØÅÊçÆÁöÑÂ±ÇÊ¨°ËØÑ‰º∞Êù•ÊîπÂñÑËßÜËßâÁîüÊàê„ÄÇÁé∞ÊúâÁöÑÂ•ñÂä±Ê®°ÂûãÈÄöÂ∏∏ÈááÁî®Âõ∫ÂÆöÁöÑËØÑ‰º∞Ê†áÂáÜÔºåÂØºËá¥ÂØπÂÜÖÂÆπÁâπÂÆöÁöÑËßÜËßâÁ∫øÁ¥¢‰∏çÊïèÊÑüÔºå‰ªéËÄå‰∏é‰∫∫Á±ªÁöÑ‰∏ªËßÇÂÅèÂ•ΩÂ≠òÂú®Á≥ªÁªüÊÄß‰∏ç‰∏ÄËá¥„ÄÇËØ•Ê®°ÂûãÈÄöËøáËß£ÈáäËØ≠‰πâÊÑèÂõæÂπ∂Âü∫‰∫éËßÜËßâËØÅÊçÆËøõË°åËØÑ‰º∞ÔºåËÉΩÂ§üÁÅµÊ¥ªÈÄÇÂ∫î‰∏çÂêåÁöÑ‰∏ä‰∏ãÊñá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåUnifiedReward-Flex Âú®ÂõæÂÉèÂíåËßÜÈ¢ëÂêàÊàê‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòË∂ä„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02444', 'title': 'RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval', 'url': 'https://huggingface.co/papers/2602.02444', 'abstract': 'RANKVIDEO is a reasoning-based video retrieval system that improves upon traditional two-stage frameworks through explicit query-video pair analysis and a multi-objective training approach.  \t\t\t\t\tAI-generated summary \t\t\t\t Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.', 'score': 15, 'issue_id': 903, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '5acecd139f4bdfb5', 'authors': ['Tyler Skow', 'Alexander Martin', 'Benjamin Van Durme', 'Rama Chellappa', 'Reno Kriz'], 'affiliations': ['Human Language Technology Center of Excellence', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02444.jpg', 'data': {'categories': ['#training', '#benchmark', '#video', '#reasoning', '#synthetic', '#multimodal'], 'emoji': 'üé¨', 'ru': {'title': '–£–º–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∏–¥–µ–æ ‚Äî –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è', 'desc': 'RANKVIDEO ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∏–¥–µ–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä–∞—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –∑–∞–ø—Ä–æ—Å-–≤–∏–¥–µ–æ —è–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–∏–¥–µ–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–º curriculum-–æ–±—É—á–µ–Ω–∏–µ–º, –≤–∫–ª—é—á–∞—é—â–∏–º –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ—Ç–æ—á–µ—á–Ω—ã–º–∏, –ø–æ–ø–∞—Ä–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏ –∏ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —É—á–∏—Ç–µ–ª—è. –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∫–æ–Ω–≤–µ–π–µ—Ä–æ–º —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å-–≤–∏–¥–µ–æ –ø–∞—Ä, —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MultiVENT 2.0 –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ RANKVIDEO —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –Ω–∞ 31% –ø–æ –º–µ—Ç—Ä–∏–∫–µ nDCG@10 –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏ –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã–µ –∞–Ω–∞–ª–æ–≥–∏.'}, 'en': {'title': 'RANKVIDEO: Enhancing Video Retrieval with Reasoning-Based Reranking', 'desc': 'RANKVIDEO is a novel video retrieval system that enhances traditional methods by focusing on the relationship between query and video pairs. It employs a reasoning-based reranking approach that analyzes video content to determine relevance more effectively. The training process involves a two-stage curriculum that includes supervised fine-tuning and a combination of different training objectives to improve performance. Experiments show that RANKVIDEO significantly boosts retrieval accuracy, outperforming existing text-only and vision-language models while being more efficient.'}, 'zh': {'title': 'Âü∫‰∫éÊé®ÁêÜÁöÑËßÜÈ¢ëÊ£ÄÁ¥¢Êñ∞Á™ÅÁ†¥', 'desc': 'RANKVIDEOÊòØ‰∏ÄÁßçÂü∫‰∫éÊé®ÁêÜÁöÑËßÜÈ¢ëÊ£ÄÁ¥¢Á≥ªÁªüÔºåÊó®Âú®ÈÄöËøáÊòæÂºèÂàÜÊûêÊü•ËØ¢-ËßÜÈ¢ëÂØπÂíåÂ§öÁõÆÊ†áËÆ≠ÁªÉÊñπÊ≥ïÊù•ÊîπËøõ‰º†ÁªüÁöÑ‰∏§Èò∂ÊÆµÊ°ÜÊû∂„ÄÇËØ•Á≥ªÁªüÂà©Áî®ËßÜÈ¢ëÂÜÖÂÆπÂØπÊü•ËØ¢-ËßÜÈ¢ëÂØπÁöÑÁõ∏ÂÖ≥ÊÄßËøõË°åËØÑ‰º∞Ôºå‰ªéËÄåÂÆûÁé∞Êõ¥Á≤æÂáÜÁöÑÈáçÊéíÂ∫è„ÄÇRANKVIDEOÈááÁî®‰∏§Èò∂ÊÆµËØæÁ®ãËøõË°åËÆ≠ÁªÉÔºåÈ¶ñÂÖàËøõË°åÊÑüÁü•Âü∫Á°ÄÁöÑÁõëÁù£ÂæÆË∞ÉÔºåÁÑ∂ÂêéÁªìÂêàÁÇπÂØπÁÇπ„ÄÅÂØπÊØîÂíåÊïôÂ∏à‰ø°ÂøÉËí∏È¶èÁõÆÊ†áËøõË°åÈáçÊéíÂ∫èËÆ≠ÁªÉ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRANKVIDEOÂú®Â§ßÂûãMultiVENT 2.0Âü∫ÂáÜÊµãËØï‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊ£ÄÁ¥¢ÊÄßËÉΩÔºåÂπ≥ÂùáÊèêÂçá31%ÁöÑnDCG@10Ôºå‰∏îÊïàÁéáÊõ¥È´ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02636', 'title': 'WideSeek: Advancing Wide Research via Multi-Agent Scaling', 'url': 'https://huggingface.co/papers/2602.02636', 'abstract': 'Wide Research advances search intelligence through a dedicated benchmark and multi-agent architecture that enables parallel information retrieval under complex constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.', 'score': 12, 'issue_id': 892, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '6d29fbb787fc804c', 'authors': ['Ziyang Huang', 'Haolin Ren', 'Xiaowei Yuan', 'Jiawei Wang', 'Zhongtao Jiang', 'Kun Xu', 'Shizhu He', 'Jun Zhao', 'Kang Liu'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2602.02636.jpg', 'data': {'categories': ['#training', '#agents', '#rl', '#benchmark', '#dataset'], 'emoji': 'üîç', 'ru': {'title': '–®–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫: –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Wide Research ‚Äî –Ω–æ–≤—É—é –ø–∞—Ä–∞–¥–∏–≥–º—É –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —É—á—ë—Ç–æ–º —Å–ª–æ–∂–Ω—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ WideSeekBench ‚Äî —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —à–∏—Ä–æ–∫–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ WideSeek ‚Äî –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, —Å–ø–æ—Å–æ–±–Ω—É—é –∞–≤—Ç–æ–Ω–æ–º–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø–æ–¥–∞–≥–µ–Ω—Ç—ã. –°–∏—Å—Ç–µ–º–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –Ω–∞ –ª–∏–Ω–µ–∞—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∞–≥–µ–Ω—Ç–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.'}, 'en': {'title': 'Revolutionizing Search Intelligence with Wide Research', 'desc': 'This paper presents advancements in search intelligence through a new framework called Wide Research, which focuses on retrieving information under complex constraints. It introduces WideSeekBench, a benchmark designed to evaluate General Broad Information Seeking (GBIS) using a multi-phase data pipeline that ensures diverse information retrieval. Additionally, the authors propose WideSeek, a multi-agent architecture that can dynamically create sub-agents to handle various tasks in parallel. The study shows that using reinforcement learning to optimize these agents can significantly enhance the efficiency and effectiveness of information retrieval processes.'}, 'zh': {'title': 'Êé®Âä®ÊêúÁ¥¢Êô∫ËÉΩÁöÑÂÆΩÁ†îÁ©∂Êñ∞ËåÉÂºè', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜÂÆΩÁ†îÁ©∂ÔºàWide ResearchÔºâÂú®ÊêúÁ¥¢Êô∫ËÉΩÈ¢ÜÂüüÁöÑËøõÂ±ïÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏ìÈó®ÁöÑÂü∫ÂáÜÂíåÂ§öÊô∫ËÉΩ‰ΩìÊû∂ÊûÑÔºå‰ª•ÂÆûÁé∞Â§çÊùÇÁ∫¶Êùü‰∏ãÁöÑÂπ∂Ë°å‰ø°ÊÅØÊ£ÄÁ¥¢„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫ÜWideSeekBenchÔºåËøôÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑÂπøÊ≥õ‰ø°ÊÅØÊ£ÄÁ¥¢Âü∫ÂáÜÔºåÁ°Æ‰øù‰∫ÜÁõÆÊ†á‰ø°ÊÅØÁöÑÂ§öÊ†∑ÊÄßÂíåÈÄªËæëÁ∫¶Êùü„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ÂºïÂÖ•‰∫ÜWideSeekÔºå‰∏Ä‰∏™Âä®ÊÄÅÁöÑÂàÜÂ±ÇÂ§öÊô∫ËÉΩ‰ΩìÊû∂ÊûÑÔºåËÉΩÂ§üÊ†πÊçÆ‰ªªÂä°ÈúÄÊ±ÇËá™‰∏ªÂàÜÂèâÂπ∂Ë°åÂ≠êÊô∫ËÉΩ‰Ωì„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåWideSeekÂíåÂ§öÊô∫ËÉΩ‰ΩìÂº∫ÂåñÂ≠¶‰π†ÁöÑÊúâÊïàÊÄßÔºåË°®ÊòéÂ¢ûÂä†Êô∫ËÉΩ‰ΩìÊï∞ÈáèÊòØÊé®Âä®ÂÆΩÁ†îÁ©∂ËåÉÂºèÁöÑÊúâÂ∏åÊúõÊñπÂêë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.21244', 'title': 'Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification', 'url': 'https://huggingface.co/papers/2601.21244', 'abstract': 'LENS framework improves reinforcement learning with verifiable rewards by identifying and removing interference tokens to enhance exploration efficiency and training stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6times speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.', 'score': 12, 'issue_id': 892, 'pub_date': '2026-01-29', 'pub_date_card': {'ru': '29 —è–Ω–≤–∞—Ä—è', 'en': 'January 29', 'zh': '1Êúà29Êó•'}, 'hash': '91a5d6080fd74dd0', 'authors': ['Yiju Guo', 'Tianyi Hu', 'Zexu Sun', 'Yankai Lin'], 'affiliations': ['Baidu Inc.', 'Department of Computer Science, Aarhus University', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2601.21244.jpg', 'data': {'categories': ['#reasoning', '#rl', '#optimization', '#training'], 'emoji': 'üßπ', 'ru': {'title': '–û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –æ—Ç –ø–æ–º–µ—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ LENS - —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–∑–≤–µ–¥–∫–∏ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –±—é–¥–∂–µ—Ç–µ –æ—Ç–∫–∞—Ç–æ–≤. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –º–Ω–æ–≥–∏–µ –æ—à–∏–±–∫–∏ —Ä–∞–∑–≤–µ–¥–∫–∏ –≤—ã–∑–≤–∞–Ω—ã –Ω–µ–±–æ–ª—å—à–∏–º —á–∏—Å–ª–æ–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ, —Å–æ–∑–¥–∞—é—â–∏—Ö –ø–æ–º–µ—Ö–∏, –∞ –Ω–µ —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –∑–∞–¥–∞—á–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å–Ω–∞—á–∞–ª–∞ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∏ —É–¥–∞–ª—è–µ—Ç —ç—Ç–∏ –º–µ—à–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å–ø–µ—à–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º –∑–∞—à—É–º–ª–µ–Ω–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å GRPO: –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ 3,88% –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 1,6 —Ä–∞–∑–∞.'}, 'en': {'title': 'Enhancing Reinforcement Learning by Reducing Interference', 'desc': 'The LENS framework enhances reinforcement learning by focusing on verifiable rewards and improving exploration efficiency. It identifies and removes interference tokens that hinder the learning process, leading to more stable training outcomes. By purifying the prompts, LENS allows the model to learn effectively from successful rollouts, even in noisy environments. Experimental results demonstrate that LENS achieves better performance and faster convergence compared to traditional methods, highlighting the importance of reducing noise in reinforcement learning tasks.'}, 'zh': {'title': 'ÂéªÈô§Âπ≤Êâ∞ÔºåÊèêÂçáÂº∫ÂåñÂ≠¶‰π†ÊïàÁéá', 'desc': 'LENSÊ°ÜÊû∂ÈÄöËøáËØÜÂà´ÂíåÂéªÈô§Âπ≤Êâ∞Ê†áËÆ∞ÔºåÊèêÂçá‰∫ÜÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÂèØÈ™åËØÅÂ•ñÂä±Ôºå‰ªéËÄåÂ¢ûÂº∫‰∫ÜÊé¢Á¥¢ÊïàÁéáÂíåËÆ≠ÁªÉÁ®≥ÂÆöÊÄß„ÄÇÁ†îÁ©∂ÂèëÁé∞ÔºåËÆ∏Â§öÊé¢Á¥¢Â§±Ë¥•Âπ∂ÈùûÊ∫ê‰∫éÈóÆÈ¢òÁöÑÂ§çÊùÇÊÄßÔºåËÄåÊòØÁî±‰∫éÂ∞ëÈáèÂπ≤Êâ∞Ê†áËÆ∞ÁöÑÂΩ±Âìç„ÄÇLENSÈ¶ñÂÖàÈÄöËøáÂéªÈô§Âπ≤Êâ∞Ê†áËÆ∞Êù•ËøõË°åÊèêÁ§∫ÔºåÁÑ∂ÂêéÂ∞ÜÊàêÂäüÁöÑÂõûÂêàËΩ¨ÁßªÂà∞ÂéüÂßãÁöÑÂòàÊùÇÊèêÁ§∫‰∏äÔºå‰ª•ÁõëÁù£Á≠ñÁï•‰ºòÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLENSÊòæËëó‰ºò‰∫éGRPOÔºåÊèê‰æõ‰∫ÜÊõ¥È´òÁöÑÊÄßËÉΩÂíåÊõ¥Âø´ÁöÑÊî∂ÊïõÈÄüÂ∫¶„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03086', 'title': 'Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.03086', 'abstract': 'Neural Predictor-Corrector framework unifies homotopy methods across multiple domains and outperforms classical approaches through learned policies and amortized training.  \t\t\t\t\tAI-generated summary \t\t\t\t The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.', 'score': 11, 'issue_id': 902, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '7b0d8cc4b69f4d7e', 'authors': ['Jiayao Mai', 'Bangyan Liao', 'Zhenjun Zhao', 'Yingping Zeng', 'Haoang Li', 'Javier Civera', 'Tailin Wu', 'Yi Zhou', 'Peidong Liu'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'Hunan University', 'University of Zaragoza', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2602.03086.jpg', 'data': {'categories': ['#architecture', '#rl', '#training'], 'emoji': 'üîÑ', 'ru': {'title': '–ù–µ–π—Ä–æ—Å–µ—Ç—å –≤–º–µ—Å—Ç–æ —ç–≤—Ä–∏—Å—Ç–∏–∫: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≥–æ–º–æ—Ç–æ–ø–∏—á–µ—Å–∫–∏–º –º–µ—Ç–æ–¥–∞–º', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è Neural Predictor-Corrector (NPC) - –µ–¥–∏–Ω–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—èÊ°ÜÊû∂–¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –≥–æ–º–æ—Ç–æ–ø–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–∞—Ö. –ê–≤—Ç–æ—Ä—ã –∑–∞–º–µ–Ω—è—é—Ç —Ä—É—á–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ä–∞–∑–º–µ—Ä–∞ —à–∞–≥–∞ –∏ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—É—á–∞–µ–º—ã–µ –ø–æ–ª–∏—Ç–∏–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º reinforcement learning. –ë–ª–∞–≥–æ–¥–∞—Ä—è –º–µ—Ö–∞–Ω–∏–∑–º—É amortized training –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –∫–ª–∞—Å—Å–∞ –∑–∞–¥–∞—á –∏ –º–æ–∂–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ NPC –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≥–ª–æ–±–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–Ω–µ–π –ø–æ–ª–∏–Ω–æ–º–æ–≤ –∏ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Unifying Homotopy Methods with Neural Learning for Enhanced Problem Solving', 'desc': 'The Neural Predictor-Corrector (NPC) framework integrates homotopy methods from various domains, providing a unified approach to solving complex problems. Unlike traditional methods that depend on manually designed heuristics for decision-making, NPC employs learned policies through reinforcement learning to optimize step sizes and iteration processes. This framework allows for amortized training, meaning it can be trained once on a set of problems and then efficiently applied to new instances. Experiments show that NPC not only generalizes well to unseen problems but also outperforms classical methods in both efficiency and stability.'}, 'zh': {'title': 'Áªü‰∏ÄÂêå‰º¶ÊñπÊ≥ïÔºåÊèêÂçáÊú∫Âô®Â≠¶‰π†ÊïàÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÁ•ûÁªèÈ¢ÑÊµã-Ê†°Ê≠£Ê°ÜÊû∂ÔºàNeural Predictor-Corrector, NPCÔºâÔºåÊó®Âú®Áªü‰∏ÄÂ§öÈ¢ÜÂüüÁöÑÂêå‰º¶ÊñπÊ≥ïÔºåÂπ∂ÈÄöËøáÂ≠¶‰π†Á≠ñÁï•ÂíåÊëäÈîÄËÆ≠ÁªÉË∂ÖË∂ä‰º†ÁªüÊñπÊ≥ï„ÄÇËØ•Ê°ÜÊû∂Â∞ÜÂêå‰º¶ÈóÆÈ¢òËßÜ‰∏∫‰∏Ä‰∏™Êï¥‰ΩìÔºåÂà©Áî®Âº∫ÂåñÂ≠¶‰π†Ëá™Âä®ÂèëÁé∞È´òÊïàÁ≠ñÁï•ÔºåÂèñ‰ª£‰∫ÜÊâãÂ∑•ËÆæËÆ°ÁöÑÂêØÂèëÂºèÊñπÊ≥ï„ÄÇNPCÈÄöËøáÂ∞ÜÁ≠ñÁï•ÈÄâÊã©ËßÜ‰∏∫‰∏Ä‰∏™Â∫èÂàóÂÜ≥Á≠ñÈóÆÈ¢òÔºåËÉΩÂ§üÂú®Â§öÁßç‰ªªÂä°‰∏≠ÂÆûÁé∞Êõ¥Â•ΩÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåNPCÂú®Â§ÑÁêÜÊú™ËßÅÂÆû‰æãÊó∂Ë°®Áé∞Âá∫Ëâ≤Ôºå‰∏îÂú®ÊïàÁéáÂíåÁ®≥ÂÆöÊÄß‰∏äÂùá‰ºò‰∫é‰º†ÁªüÂíå‰∏ì‰∏öÂü∫Á∫ø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01362', 'title': 'Balancing Understanding and Generation in Discrete Diffusion Models', 'url': 'https://huggingface.co/papers/2602.01362', 'abstract': "XDLM unifies Masked Diffusion Language Models and Uniform-noise Diffusion Language Models through a stationary noise kernel, achieving improved performance in both semantic understanding and generation quality.  \t\t\t\t\tAI-generated summary \t\t\t\t In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM", 'score': 11, 'issue_id': 892, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': 'ab3da9b2a401c5c0', 'authors': ['Yue Liu', 'Yuzhong Zhao', 'Zheyong Xie', 'Qixiang Ye', 'Jianbin Jiao', 'Yao Hu', 'Shaosheng Cao', 'Yunfan Liu'], 'affiliations': ['UCAS', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2602.01362.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#optimization'], 'emoji': 'üîÄ', 'ru': {'title': '–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–¥–∏–≥–º: –µ–¥–∏–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω XDLM ‚Äî –µ–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –¥–≤–µ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–µ –ø–∞—Ä–∞–¥–∏–≥–º—ã –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–µ —è–¥—Ä–æ —à—É–º–∞. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Masked Diffusion Language Models –∏ Uniform-noise Diffusion Language Models —è–≤–ª—è—é—Ç—Å—è —á–∞—Å—Ç–Ω—ã–º–∏ —Å–ª—É—á–∞—è–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, –¥–æ—Å—Ç–∏–≥–∞—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. XDLM –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∫–∞–∫ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, —Ç–∞–∫ –∏ –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–∞–ª–æ–º —á–∏—Å–ª–µ —à–∞–≥–æ–≤. –ú–µ—Ç–æ–¥ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, —É–¥–≤–∞–∏–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤—Å–µ–≥–æ –∑–∞ 32 —à–∞–≥–∞.)'}, 'en': {'title': 'XDLM: Bridging Understanding and Generation in Language Models', 'desc': 'XDLM is a novel framework that combines the strengths of Masked Diffusion Language Models (MDLM) and Uniform-noise Diffusion Language Models (UDLM) using a stationary noise kernel. This unification allows XDLM to enhance both semantic understanding and generation quality, addressing the limitations of each individual model. The framework not only theoretically connects MDLM and UDLM but also simplifies memory usage through algebraic adjustments in posterior probabilities. Experimental results show that XDLM significantly improves performance on zero-shot text tasks and few-step image generation, demonstrating its effectiveness in balancing understanding and generation capabilities.'}, 'zh': {'title': 'XDLMÔºöÁªü‰∏ÄÁêÜËß£‰∏éÁîüÊàêÁöÑËØ≠Ë®ÄÊ®°Âûã', 'desc': 'XDLMÊòØ‰∏ÄÁßçÊñ∞ÂûãÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºåÂÆÉÂ∞ÜÊé©ËîΩÊâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàMDLMÔºâÂíåÂùáÂåÄÂô™Â£∞Êâ©Êï£ËØ≠Ë®ÄÊ®°ÂûãÔºàUDLMÔºâÈÄöËøá‰∏Ä‰∏™ÈùôÊÄÅÂô™Â£∞Ê†∏Áªü‰∏ÄËµ∑Êù•„ÄÇËØ•Ê®°ÂûãÂú®ËØ≠‰πâÁêÜËß£ÂíåÁîüÊàêË¥®ÈáèÊñπÈù¢ÈÉΩË°®Áé∞Âá∫Ëâ≤ÔºåËß£ÂÜ≥‰∫ÜËøô‰∏§ÁßçÊ®°ÂûãÂêÑËá™ÁöÑ‰∏çË∂≥„ÄÇXDLMÁöÑ‰∏§‰∏™‰∏ªË¶ÅË¥°ÁåÆÊòØÔºöÈ¶ñÂÖàÔºåÂÆÉÊèê‰æõ‰∫ÜMDLMÂíåUDLMÁöÑÁêÜËÆ∫Áªü‰∏ÄÔºõÂÖ∂Ê¨°ÔºåÈÄöËøá‰ª£Êï∞ÁÆÄÂåñÔºåÁºìËß£‰∫ÜÂÜÖÂ≠òÁì∂È¢à„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåXDLMÂú®ÁêÜËß£ËÉΩÂäõÂíåÁîüÊàêË¥®Èáè‰πãÈó¥ÂèñÂæó‰∫ÜÊõ¥Â•ΩÁöÑÂπ≥Ë°°ÔºåÊòæËëóÊèêÂçá‰∫ÜÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03798', 'title': 'FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation', 'url': 'https://huggingface.co/papers/2602.03798', 'abstract': 'A unified agent system called FullStack-Agent is introduced to assist non-expert users in developing complex interactive websites by addressing full-stack development challenges through enhanced planning, code editing, and self-improving capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.', 'score': 9, 'issue_id': 894, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '5dfc19ba939c3259', 'authors': ['Zimu Lu', 'Houxing Ren', 'Yunqiao Yang', 'Ke Wang', 'Zhuofan Zong', 'Mingjie Zhan', 'Hongsheng Li'], 'affiliations': ['Ace Robotics', 'Multimedia Laboratory (MMLab), The Chinese University of Hong Kong', 'Shenzhen Loop Area Institute'], 'pdf_title_img': 'assets/pdf/title_img/2602.03798.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#agents', '#training', '#plp'], 'emoji': 'üèóÔ∏è', 'ru': {'title': '–ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ FullStack-Agent - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –Ω–µ–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞, –±—ç–∫–µ–Ω–¥–∞ –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –Ω–∞–≤–∏–≥–∞—Ü–∏—é –ø–æ –∫–æ–¥–æ–≤–æ–π –±–∞–∑–µ –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é –æ—à–∏–±–æ–∫ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ FullStack-Learn, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –¥–ª—è —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–π LLM –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –Ω–æ–≤–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ FullStack-Bench —Å–∏—Å—Ç–µ–º–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏ –Ω–∞ 8.7-38.2% –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.'}, 'en': {'title': 'Empowering Non-Experts in Full-Stack Web Development', 'desc': 'The paper presents FullStack-Agent, a comprehensive system designed to help non-expert users create complex interactive websites by tackling the challenges of full-stack development. It consists of three main components: FullStack-Dev for planning and code editing, FullStack-Learn for self-improvement through data scaling, and FullStack-Bench for benchmarking website functionalities. FullStack-Dev significantly outperforms previous methods in frontend, backend, and database tests, showcasing its advanced capabilities. The system aims to simplify the development process by providing robust tools for managing data flow and debugging, making full-stack development more accessible.'}, 'zh': {'title': 'ÂÖ®Ê†à‰ª£ÁêÜÁ≥ªÁªüÔºåÂä©ÂäõÁΩëÁ´ôÂºÄÂèëÔºÅ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÂêç‰∏∫FullStack-AgentÁöÑÁªü‰∏Ä‰ª£ÁêÜÁ≥ªÁªüÔºåÊó®Âú®Â∏ÆÂä©Èùû‰∏ì‰∏öÁî®Êà∑ÂºÄÂèëÂ§çÊùÇÁöÑ‰∫íÂä®ÁΩëÁ´ô„ÄÇËØ•Á≥ªÁªüÈÄöËøáÂ¢ûÂº∫ÁöÑËßÑÂàí„ÄÅ‰ª£Á†ÅÁºñËæëÂíåËá™ÊàëÊîπËøõËÉΩÂäõÔºåËß£ÂÜ≥‰∫ÜÂÖ®Ê†àÂºÄÂèë‰∏≠ÁöÑÊåëÊàò„ÄÇFullStack-AgentÂåÖÊã¨‰∏â‰∏™ÈÉ®ÂàÜÔºöFullStack-Dev„ÄÅFullStack-LearnÂíåFullStack-BenchÔºåÂàÜÂà´Ë¥üË¥£Â§ö‰ª£ÁêÜÊ°ÜÊû∂„ÄÅÊï∞ÊçÆËá™ÊàëÊèêÂçáÂíåÂÖ®Èù¢Âü∫ÂáÜÊµãËØï„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFullStack-AgentÂú®ÂâçÁ´Ø„ÄÅÂêéÁ´ØÂíåÊï∞ÊçÆÂ∫ìÊµãËØï‰∏≠Âùá‰ºò‰∫éÁé∞ÊúâÁöÑÊúÄÂÖàËøõÊñπÊ≥ïÔºåÊòæÁ§∫‰∫ÜÂÖ∂ÊúâÊïàÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03216', 'title': 'Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection', 'url': 'https://huggingface.co/papers/2602.03216', 'abstract': 'Token Sparse Attention enables efficient long-context inference by dynamically compressing and decompressing attention tensors at the token level, achieving significant speedup with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, V to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to times3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.', 'score': 9, 'issue_id': 895, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '7e3cbfbe9177c763', 'authors': ['Dongwon Jo', 'Beomseok Kang', 'Jiwon Song', 'Jae-Joon Kim'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2602.03216.jpg', 'data': {'categories': ['#inference', '#long_context', '#architecture', '#optimization'], 'emoji': '‚ö°', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Å–ø–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ inference –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö', 'desc': '–†–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç Token Sparse Attention ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å–ø–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –ú–µ—Ç–æ–¥ –∫–æ–º–ø—Ä–µ—Å—Å–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã Q, K, V –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è, –æ—Ç–±–∏—Ä–∞—è —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∞ –∑–∞—Ç–µ–º –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π –≤—ã—Ö–æ–¥ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —Å–ª–æ—ë–≤, –ø–æ–∑–≤–æ–ª—è—è –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è–º–∏, –≤–∫–ª—é—á–∞—è Flash Attention, –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤ 3.23 —Ä–∞–∑–∞ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏–∑ 128K —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø–æ—Ç–µ—Ä–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–µ–Ω–µ–µ 1%. –≠—Ç–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∏ —á–µ—Ä–µ–¥—É—é—â–∞—è—Å—è —Å–ø–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤ —è–≤–ª—è–µ—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ inference —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏.'}, 'en': {'title': 'Dynamic Token Sparsification for Efficient Long-Context Inference', 'desc': 'This paper introduces Token Sparse Attention, a method designed to improve the efficiency of long-context inference in large language models by dynamically compressing and decompressing attention tensors at the token level. Traditional attention mechanisms face challenges due to their quadratic complexity, which limits performance when processing long sequences. Token Sparse Attention addresses this by selectively reducing the number of tokens considered during attention calculations, allowing for a more flexible and efficient use of resources. The results show that this approach can significantly speed up attention processes while maintaining high accuracy, making it a valuable advancement in the field of machine learning.'}, 'zh': {'title': 'Âä®ÊÄÅ‰ª§ÁâåÁ®ÄÁñèÊ≥®ÊÑèÂäõÔºåÊèêÂçáÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÊïàÁéá', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Token Sparse AttentionÁöÑÊú∫Âà∂ÔºåÊó®Âú®ÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÊé®ÁêÜÁöÑÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáÂú®‰ª§ÁâåÁ∫ßÂà´Âä®ÊÄÅÂéãÁº©ÂíåËß£ÂéãÊ≥®ÊÑèÂäõÂº†ÈáèÔºåÊòæËëóÂä†Âø´‰∫ÜËÆ°ÁÆóÈÄüÂ∫¶ÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜËæÉÂ∞èÁöÑÂáÜÁ°ÆÊÄßÊçüÂ§±„ÄÇ‰∏é‰ª•ÂæÄÁöÑÂä†ÈÄüÊñπÊ≥ï‰∏çÂêåÔºåToken Sparse AttentionÂÖÅËÆ∏Âú®ÂêéÁª≠Â±Ç‰∏≠ÈáçÊñ∞ËÄÉËôë‰ª§Áâå‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑÁÅµÊ¥ªÊÄßÂíåÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®128K‰∏ä‰∏ãÊñá‰∏≠ÂÆûÁé∞‰∫ÜÊúÄÈ´ò3.23ÂÄçÁöÑÊ≥®ÊÑèÂäõÂä†ÈÄüÔºå‰∏îÂáÜÁ°ÆÊÄß‰∏ãÈôç‰∏çÂà∞1%„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02676', 'title': 'AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process', 'url': 'https://huggingface.co/papers/2602.02676', 'abstract': "AdaptMMBench presents a comprehensive benchmark for evaluating adaptive multimodal reasoning in Vision-Language Models, measuring reasoning mode selection rationality through dynamic difficulty assessment and multi-dimensional process evaluation.  \t\t\t\t\tAI-generated summary \t\t\t\t Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.", 'score': 8, 'issue_id': 892, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'dcf8a0d29dda7fc8', 'authors': ['Xintong Zhang', 'Xiaowen Zhang', 'Jongrong Wu', 'Zhi Gao', 'Shilin Yan', 'Zhenxin Diao', 'Kunpeng Gao', 'Xuanyan Chen', 'Yuwei Wu', 'Yunde Jia', 'Qing Li'], 'affiliations': ['Alibaba Group', 'Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology', 'Guangdong Laboratory of Machine Perception and Intelligent Computing, Shenzhen MSU-BIT University', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2602.02676.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#cv'], 'emoji': 'üß†', 'ru': {'title': '–£–º–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ: –æ—Ü–µ–Ω–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': 'AdaptMMBench ‚Äî —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ Vision-Language Models, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–º–µ—Ä—è–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞—Ç—å –º–µ–∂–¥—É –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º. –í–º–µ—Å—Ç–æ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–∫ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –±–µ–Ω—á–º–∞—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç—Ä–∏–∫—É Matthews Correlation Coefficient –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –æ–ø—Ä–µ–¥–µ–ª—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏. –û—Ü–µ–Ω–∫–∞ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –ø—è—Ç—å –æ–±–ª–∞—Å—Ç–µ–π (—Ä–µ–∞–ª—å–Ω—ã–π –º–∏—Ä, OCR, GUI, –∑–Ω–∞–Ω–∏—è –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞) –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–∞: –ø–æ–∫—Ä—ã—Ç–∏–µ –≤–∞–∂–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏, –Ω–æ –Ω–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é, –ø—Ä–∏ —ç—Ç–æ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —ç—Ç–∞–ø–æ–≤ –æ—Å—Ç–∞—ë—Ç—Å—è –≤–∞–∂–Ω—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'AdaptMMBench: Elevating Adaptive Reasoning in Vision-Language Models', 'desc': 'The paper introduces AdaptMMBench, a new benchmark designed to evaluate adaptive multimodal reasoning in Vision-Language Models (VLMs). It addresses the limitations of existing evaluations that use static difficulty labels and simplistic metrics, which do not reflect the dynamic nature of task difficulty. AdaptMMBench assesses reasoning mode selection rationality using the Matthews Correlation Coefficient (MCC) and evaluates models across five domains, focusing on both direct perception and complex reasoning tasks. The findings indicate that while adaptive mode selection improves with model capacity, it does not necessarily correlate with final accuracy, highlighting the need for a nuanced understanding of model performance.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îÂ§öÊ®°ÊÄÅÊé®ÁêÜÁöÑËØÑ‰º∞Êñ∞Âü∫ÂáÜ', 'desc': 'AdaptMMBench ÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞ËßÜËßâ-ËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑËá™ÈÄÇÂ∫îÂ§öÊ®°ÊÄÅÊé®ÁêÜ„ÄÇÂÆÉÈÄöËøáÂä®ÊÄÅÈöæÂ∫¶ËØÑ‰º∞ÂíåÂ§öÁª¥ËøáÁ®ãËØÑ‰º∞Êù•ÊµãÈáèÊé®ÁêÜÊ®°ÂºèÈÄâÊã©ÁöÑÂêàÁêÜÊÄß„ÄÇËØ•Âü∫ÂáÜÊ∂µÁõñ‰∫Ü‰∫î‰∏™È¢ÜÂüüÔºåÂåÖÊã¨Áé∞ÂÆû‰∏ñÁïå„ÄÅOCR„ÄÅGUI„ÄÅÁü•ËØÜÂíåÊï∞Â≠¶ÔºåÊ∂âÂèäÁõ¥Êé•ÊÑüÁü•ÂíåÂ§çÊùÇÊé®ÁêÜ‰ªªÂä°„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåËá™ÈÄÇÂ∫îÊ®°ÂºèÈÄâÊã©‰∏éÊ®°ÂûãËÉΩÂäõÁõ∏ÂÖ≥Ôºå‰ΩÜ‰∏éÊúÄÁªàÂáÜÁ°ÆÊÄßÂπ∂‰∏çÁõ¥Êé•Áõ∏ÂÖ≥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00747', 'title': 'Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training', 'url': 'https://huggingface.co/papers/2602.00747', 'abstract': 'DeMix is a framework that uses model merging to predict optimal data ratios for LLM pre-training, decoupling search from training costs to improve mixture discovery efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.', 'score': 8, 'issue_id': 892, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': 'ce36bac13dae5835', 'authors': ['Shengrui Li', 'Fei Zhao', 'Kaiyan Zhao', 'Jieying Ye', 'Haifeng Liu', 'Fangcheng Shi', 'Zheyong Xie', 'Yao Hu', 'Shaosheng Cao'], 'affiliations': ['NLP Team, Xiaohongshu Inc. Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.00747.jpg', 'data': {'categories': ['#training', '#optimization', '#data', '#open_source', '#dataset'], 'emoji': 'üß©', 'ru': {'title': '–ü–æ–∏—Å–∫ –∏–¥–µ–∞–ª—å–Ω–æ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Å–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è', 'desc': 'DeMix ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ LLM, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤–º–µ—Å—Ç–æ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–æ–∫—Å–∏-–º–æ–¥–µ–ª–µ–π. –í–º–µ—Å—Ç–æ –æ–±—É—á–µ–Ω–∏—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–¥—Ö–æ–¥ –æ–±—É—á–∞–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –≤ –ø–æ–ª–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ –∏ –ø–æ–ª—É—á–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–ø–æ—Ä—Ü–∏–π —á–µ—Ä–µ–∑ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –º–µ—Ä–∂ –º–æ–¥–µ–ª–µ–π. –¢–∞–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ—Ç–¥–µ–ª—è–µ—Ç –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–º–µ—Å–∏ –æ—Ç –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, –ø–æ–∑–≤–æ–ª—è—è –æ—Ü–µ–Ω–∏—Ç—å –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–º–µ—Å–µ–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç DeMix Corpora ‚Äî –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç –æ–±—ä—ë–º–æ–º 22 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏—è–º–∏ –¥–ª—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.'}, 'en': {'title': 'Decoupling Search from Training for Optimal Data Mixtures in LLMs', 'desc': 'DeMix is a framework designed to enhance the efficiency of discovering optimal data mixtures for pre-training Large Language Models (LLMs). It achieves this by using model merging techniques to predict the best data ratios, which allows for extensive mixture evaluation without the need for costly training of proxy models. This decoupling of search from training costs enables researchers to explore a wider range of data mixtures, leading to improved performance on challenging tasks. The framework also introduces the DeMix Corpora, a large dataset that supports further research in this area by providing validated data mixtures.'}, 'zh': {'title': 'Ëß£ËÄ¶ÊêúÁ¥¢‰∏éËÆ≠ÁªÉÔºåÊèêÂçáÊï∞ÊçÆÊ∑∑ÂêàÊïàÁéá', 'desc': 'DeMixÊòØ‰∏Ä‰∏™Ê°ÜÊû∂ÔºåÈÄöËøáÊ®°ÂûãÂêàÂπ∂Êù•È¢ÑÊµãÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈ¢ÑËÆ≠ÁªÉÁöÑÊúÄ‰Ω≥Êï∞ÊçÆÊØî‰æãÔºå‰ªéËÄåÊèêÈ´òÊ∑∑ÂêàÂèëÁé∞ÁöÑÊïàÁéá„ÄÇËØ•ÊñπÊ≥ïÂ∞ÜÊêúÁ¥¢‰∏éËÆ≠ÁªÉÊàêÊú¨Ëß£ËÄ¶Ôºå‰ΩøÂæóÂèØ‰ª•Âú®‰∏çÂ¢ûÂä†È¢ùÂ§ñËÆ≠ÁªÉË¥üÊãÖÁöÑÊÉÖÂÜµ‰∏ãËØÑ‰º∞Êó†ÈôêÁöÑÊ†∑Êú¨Ê∑∑Âêà„ÄÇDeMixÈÄöËøáÂä†ÊùÉÊ®°ÂûãÂêàÂπ∂Ôºå‰ªéÂÄôÈÄâÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁªÑ‰ª∂Ê®°ÂûãÔºåÁîüÊàêÊï∞ÊçÆÊ∑∑Âêà‰ª£ÁêÜÔºåÈÅøÂÖç‰∫ÜÂØπÊØè‰∏™Ê†∑Êú¨Ê∑∑ÂêàËøõË°å‰ª£ÁêÜÊ®°ÂûãËÆ≠ÁªÉÁöÑÈúÄÊ±Ç„ÄÇÂÆûÈ™åË°®ÊòéÔºåDeMixÂú®ÂÖÖÂàÜÊÄß„ÄÅÂáÜÁ°ÆÊÄßÂíåÊïàÁéá‰πãÈó¥ÊâìÁ†¥‰∫ÜÊùÉË°°Ôºå‰ª•Êõ¥‰ΩéÁöÑÊêúÁ¥¢ÊàêÊú¨Ëé∑ÂæóÊõ¥È´òÁöÑÂü∫ÂáÜÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03747', 'title': 'LIVE: Long-horizon Interactive Video World Modeling', 'url': 'https://huggingface.co/papers/2602.03747', 'abstract': 'LIVE is a long-horizon video world model that uses cycle-consistency and diffusion loss to control error accumulation during extended video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.', 'score': 7, 'issue_id': 903, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '2c348bc63334000c', 'authors': ['Junchao Huang', 'Ziyang Ye', 'Xinting Hu', 'Tianyu He', 'Guiyu Zhang', 'Shaoshuai Shi', 'Jiang Bian', 'Li Jiang'], 'affiliations': ['Microsoft Research', 'Shenzhen Loop Area Institute', 'The Chinese University of Hong Kong, Shenzhen', 'The University of Hong Kong', 'Voyager Research, Didi Chuxing'], 'pdf_title_img': 'assets/pdf/title_img/2602.03747.jpg', 'data': {'categories': ['#training', '#benchmark', '#diffusion', '#long_context', '#optimization', '#video'], 'emoji': 'üîÑ', 'ru': {'title': '–¶–∏–∫–ª–∏—á–Ω–æ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤ –æ—à–∏–±–æ–∫: –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è', 'desc': 'LIVE ‚Äî —ç—Ç–æ –≤–∏–¥–µ–æ–º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –ö–ª—é—á–µ–≤–æ–π –∏–Ω–Ω–æ–≤–∞—Ü–∏–µ–π —è–≤–ª—è–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ü–∏–∫–ª–∏—á–Ω–æ—Å—Ç–∏ (forward-backward) –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è —è–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –æ—à–∏–±–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –º–æ–¥–µ–ª—å –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —É—á–∏—Ç–µ–ª—è-–¥–∏—Å—Ç–∏–ª–ª—è—Ç–æ—Ä–∞, –∞ –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ü–∞ –≤–∏–¥–µ–æ, —á—Ç–æ–±—ã –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ LIVE –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ, —Å–æ–∑–¥–∞–≤–∞—è —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–µ –¥–ª–∏–Ω—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.'}, 'en': {'title': 'LIVE: Mastering Long-Horizon Video Generation with Cycle-Consistency', 'desc': 'LIVE is a novel video world model designed to generate long sequences of video while minimizing errors. It uses a cycle-consistency objective to control the accumulation of prediction errors over time, which is a common issue in autoregressive models. By performing a forward rollout and then reconstructing the initial state through a reverse process, LIVE effectively reduces the need for additional teacher models. The introduction of diffusion loss further constrains error propagation, leading to high-quality video generation that surpasses previous methods.'}, 'zh': {'title': 'LIVEÔºöÊéßÂà∂ÈïøÊó∂Èó¥ËßÜÈ¢ëÁîüÊàê‰∏≠ÁöÑËØØÂ∑ÆÁßØÁ¥Ø', 'desc': 'LIVEÊòØ‰∏ÄÁßçÈïøÊó∂Èó¥ËßÜÈ¢ë‰∏ñÁïåÊ®°ÂûãÔºåÊó®Âú®ÈÄöËøáÂæ™ÁéØ‰∏ÄËá¥ÊÄßÂíåÊâ©Êï£ÊçüÂ§±Êù•ÊéßÂà∂Âú®ÈïøÊó∂Èó¥ËßÜÈ¢ëÁîüÊàêËøáÁ®ã‰∏≠ÁöÑËØØÂ∑ÆÁßØÁ¥Ø„ÄÇ‰º†ÁªüÁöÑËá™ÂõûÂΩíËßÜÈ¢ëÊ®°ÂûãÂú®Áü≠Êó∂Èó¥ÂÜÖË°®Áé∞ËâØÂ•ΩÔºå‰ΩÜÂú®ÈïøÊó∂Èó¥ÁîüÊàêÊó∂ÔºåÈ¢ÑÊµãËØØÂ∑Æ‰ºöÈöèÁùÄÊó∂Èó¥ÁöÑÊé®ÁßªËÄåÁ¥ØÁßØ„ÄÇLIVEÈÄöËøáÂºïÂÖ•Êñ∞ÁöÑÂæ™ÁéØ‰∏ÄËá¥ÊÄßÁõÆÊ†áÔºåÊ∂àÈô§‰∫ÜÂØπÊïôÂ∏àÊ®°ÂûãÁöÑ‰æùËµñÔºå‰ªéËÄåÊúâÊïàÂú∞ÈôêÂà∂‰∫ÜËØØÂ∑ÆÁöÑ‰º†Êí≠„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLIVEÂú®ÈïøÊó∂Èó¥Âü∫ÂáÜÊµãËØï‰∏≠ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåËÉΩÂ§üÁîüÊàêÁ®≥ÂÆö‰∏îÈ´òË¥®ÈáèÁöÑËßÜÈ¢ë„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03709', 'title': 'No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding', 'url': 'https://huggingface.co/papers/2602.03709', 'abstract': 'Multi-hop question answering dataset ID-MoCQA assesses cultural understanding in large language models through Indonesian traditions with diverse reasoning chains.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.', 'score': 7, 'issue_id': 898, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '621ab01c0e2d402f', 'authors': ['Vynska Amalia Permadi', 'Xingwei Tan', 'Nafise Sadat Moosavi', 'Nikos Aletras'], 'affiliations': ['Department of Informatics, Universitas Pembangunan Nasional Veteran Yogyakarta, Indonesia', 'School of Computer Science, University of Sheffield, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2602.03709.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#low_resource', '#open_source', '#multilingual'], 'emoji': 'üåè', 'ru': {'title': '–ú–Ω–æ–≥–æ—Ö–æ–¥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫—É–ª—å—Ç—É—Ä—ã –≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω ID-MoCQA ‚Äî –ø–µ—Ä–≤—ã–π –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –∏–Ω–¥–æ–Ω–µ–∑–∏–π—Å–∫–∏—Ö —Ç—Ä–∞–¥–∏—Ü–∏–π. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—ã—Ö –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å —à–µ—Å—Ç—å—é —Ç–∏–ø–∞–º–∏ –ø–æ–¥—Å–∫–∞–∑–æ–∫: –∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ, –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏ –¥—Ä—É–≥–∏–µ. –ú–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é LLM, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç. –û—Ü–µ–Ω–∫–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤—ã—è–≤–∏–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –∫—É–ª—å—Ç—É—Ä–Ω–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö —Ç—Ä–µ–±—É—é—â–∏—Ö —Ç–æ–Ω–∫–æ–≥–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–≤–æ–¥–∞.'}, 'en': {'title': 'Enhancing Cultural Understanding in AI with Multi-Hop Reasoning', 'desc': 'The paper introduces ID-MoCQA, a novel multi-hop question answering dataset designed to evaluate the cultural understanding of large language models (LLMs) through the lens of Indonesian traditions. Unlike traditional benchmarks that focus on single-hop questions, ID-MoCQA requires models to engage in complex reasoning across multiple contexts and clues, enhancing the assessment of cultural knowledge. The dataset includes a systematic transformation of questions into multi-hop reasoning chains, ensuring a diverse range of inference types. Evaluation results indicate significant shortcomings in the cultural reasoning capabilities of current state-of-the-art models, highlighting the need for improved cultural competency in AI systems.'}, 'zh': {'title': 'ËØÑ‰º∞ÊñáÂåñÁêÜËß£ÁöÑÂ§öË∑≥ÈóÆÁ≠îÊï∞ÊçÆÈõÜ', 'desc': 'ID-MoCQAÊòØ‰∏Ä‰∏™Â§öË∑≥ÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÊó®Âú®ËØÑ‰º∞Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂØπÂç∞Â∞ºÊñáÂåñÁöÑÁêÜËß£„ÄÇ‰∏é‰º†ÁªüÁöÑÂçïË∑≥ÈóÆÁ≠î‰∏çÂêåÔºåËØ•Êï∞ÊçÆÈõÜË¶ÅÊ±ÇÊ®°ÂûãÈÄöËøáÂ§öÁßçÊé®ÁêÜÈìæÊù•ÂõûÁ≠îÈóÆÈ¢òÔºåÊ∂âÂèäÂ∏∏ËØÜ„ÄÅÊó∂Èó¥ÂíåÂú∞ÁêÜÁ≠âÁ∫øÁ¥¢Á±ªÂûã„ÄÇÊàë‰ª¨ÂºÄÂèë‰∫Ü‰∏ÄÁßçÊñ∞Ê°ÜÊû∂ÔºåÂ∞ÜÂçïË∑≥ÊñáÂåñÈóÆÈ¢òÁ≥ªÁªüÂú∞ËΩ¨Âåñ‰∏∫Â§öË∑≥Êé®ÁêÜÈìæÔºåÂπ∂ÈÄöËøá‰∏ìÂÆ∂ËØÑÂÆ°ÂíåÊ®°ÂûãËøáÊª§Á°Æ‰øùÈóÆÈ¢òÂíåÁ≠îÊ°àÁöÑÈ´òË¥®Èáè„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåÂΩìÂâçÊ®°ÂûãÂú®ÊñáÂåñÊé®ÁêÜÊñπÈù¢Â≠òÂú®ÊòæËëóÂ∑ÆË∑ùÔºåÁâπÂà´ÊòØÂú®ÈúÄË¶ÅÁªÜËá¥Êé®ÁêÜÁöÑ‰ªªÂä°‰∏≠„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03677', 'title': 'Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration', 'url': 'https://huggingface.co/papers/2602.03677', 'abstract': 'Research reveals that instruction tokens act as structural anchors in multimodal large language models, with shallow layers performing non-selective information transfer and deep layers resolving modality competition guided by instruction intent.  \t\t\t\t\tAI-generated summary \t\t\t\t Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere 5% of these critical heads can decrease the modality-following ratio by 60% through blocking, or increase it by 60% through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.', 'score': 5, 'issue_id': 899, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'f0b2273ebdb7b52e', 'authors': ['Yu Zhang', 'Mufan Xu', 'Xuefeng Bai', 'Kehai chen', 'Pengfei Zhang', 'Yang Xiang', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology, Harbin', 'Harbin Institute of Technology, Shenzhen, China', 'Peng Cheng Laboratory, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.03677.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#architecture'], 'emoji': 'üß≠', 'ru': {'title': '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∫–∞–∫ —è–∫–æ—Ä—è: –∫–∞–∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤—ã–±–∏—Ä–∞—é—Ç –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –º–µ—Ö–∞–Ω–∏–∑–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (MLLM) —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –ø–æ—Ç–æ–∫–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ê–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ç–æ–∫–µ–Ω—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–µ–π—Å—Ç–≤—É—é—Ç –∫–∞–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —è–∫–æ—Ä—è, –Ω–∞–ø—Ä–∞–≤–ª—è—è –æ–±—Ä–∞–±–æ—Ç–∫—É –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ —Å–ª–æ–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç –Ω–µ–∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω—É—é –ø–µ—Ä–µ–¥–∞—á—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∞ –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏ —Ä–∞–∑—Ä–µ—à–∞—é—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—é –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∏–Ω—Ç–µ–Ω—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ —Ä–µ–¥–∫–∏–π –Ω–∞–±–æ—Ä —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö attention heads, –∫–æ—Ç–æ—Ä—ã–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ ‚Äî –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è –≤—Å–µ–≥–æ 5% —ç—Ç–∏—Ö –≥–æ–ª–æ–≤ –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∏–ª–∏ –ø–æ–≤—ã—Å–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º –Ω–∞ 60%. –†–∞–±–æ—Ç–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –≤ MLLM.'}, 'en': {'title': 'Unlocking the Secrets of Multimodal Instruction Following', 'desc': 'This paper explores how instruction tokens help multimodal large language models (MLLMs) manage different types of information, like text and images. It shows that shallow layers of the model transfer information without much selection, while deeper layers use the instructions to decide which type of information to focus on. The study identifies specific attention heads that play a crucial role in this decision-making process, revealing that small changes in these heads can significantly affect how well the model follows instructions. Overall, the research enhances our understanding of how MLLMs operate and provides a framework for improving their performance in real-world applications.'}, 'zh': {'title': 'Êåá‰ª§‰ª§ÁâåÔºöÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑÁªìÊûÑÈîö', 'desc': 'Êú¨Á†îÁ©∂Êè≠Á§∫‰∫ÜÊåá‰ª§‰ª§ÁâåÂú®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ÁöÑÁªìÊûÑÊÄßÈîöÂÆö‰ΩúÁî®„ÄÇÊµÖÂ±ÇÁΩëÁªúÊâßË°åÈùûÈÄâÊã©ÊÄßÁöÑ‰ø°ÊÅØ‰º†ÈÄíÔºåËÄåÊ∑±Â±ÇÁΩëÁªúÂàôÊ†πÊçÆÊåá‰ª§ÊÑèÂõæËß£ÂÜ≥Ê®°ÊÄÅÁ´û‰∫â„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÊåá‰ª§‰ª§Áâå‰Ωú‰∏∫Ê®°ÊÄÅ‰ª≤Ë£ÅÁöÑÁªìÊûÑÈîöÔºåËÉΩÂ§üÊúâÊïàÂºïÂØº‰ø°ÊÅØÊµÅÂä®„ÄÇÈÄöËøáÊìçÊéßÁâπÂÆöÁöÑÊ≥®ÊÑèÂäõÂ§¥ÔºåÊàë‰ª¨ÂèØ‰ª•ÊòæËëóÂΩ±ÂìçÊ®°ÂûãÁöÑÊ®°ÊÄÅË∑üÈöèËÉΩÂäõÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÈÄèÊòéÂ∫¶ÂíåÂèØÈù†ÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03647', 'title': 'Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration', 'url': 'https://huggingface.co/papers/2602.03647', 'abstract': "Search-R2 framework improves language agent reasoning through Actor-Refiner collaboration with targeted interventions and fine-grained reward supervision for better credit assignment in reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.", 'score': 5, 'issue_id': 892, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': 'c6747bdce6916ea5', 'authors': ['Bowei He', 'Minda Hu', 'Zenan Xu', 'Hongru Wang', 'Licheng Zong', 'Yankai Chen', 'Chen Ma', 'Xue Liu', 'Pluto Zhou', 'Irwin King'], 'affiliations': ['City University of Hong Kong', 'LLM Department, Tencent', 'McGill University', 'Mohamed bin Zayed University of Artificial Intelligence', 'The Chinese University of Hong Kong', 'The University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2602.03647.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#agents', '#rl', '#rag'], 'emoji': 'üîç', 'ru': {'title': '–î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ä–µ—Ñ–∏–Ω–∏—Ä–æ–≤–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –≤ –∞–≥–µ–Ω—Ç–∞—Ö –ø–æ–∏—Å–∫–∞', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Search-R2, –∫–æ—Ç–æ—Ä–∞—è —É–ª—É—á—à–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ Actor –∏ Refiner –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å —Ü–µ–ª–µ–≤—ã–º–∏ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π –ø—Ä–æ–±–ª–µ–º–µ –ø—Ä–∏–ø–∏—Å–∞–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∫–æ–≥–¥–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –Ω–µ —Ä–∞–∑–ª–∏—á–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ—Ç —Å–ª—É—á–∞–π–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –¥–∏–∑–∞–π–Ω –Ω–∞–≥—Ä–∞–¥—ã, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –ø–ª–æ—Ç–Ω–æ–π –Ω–∞–≥—Ä–∞–Ω–æ–π –∑–∞ –ø—Ä–æ—Ü–µ—Å—Å, –∏–∑–º–µ—Ä—è—é—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Search-R2 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç.'}, 'en': {'title': 'Enhancing Language Agent Reasoning with Search-R2 Framework', 'desc': "The Search-R2 framework enhances the reasoning capabilities of language agents by utilizing a collaborative approach between an Actor and a Meta-Refiner. This method addresses the multi-scale credit assignment problem in reinforcement learning by providing targeted interventions and fine-grained reward supervision. The Actor generates initial reasoning paths, while the Meta-Refiner corrects errors through a 'cut-and-regenerate' process, improving the overall reasoning quality. Experimental results show that Search-R2 outperforms existing models in reasoning accuracy across various question-answering datasets, demonstrating its effectiveness in optimizing agent performance."}, 'zh': {'title': 'ÈÄöËøáÊºîÂëò-‰øÆÊ≠£ËÄÖÂçè‰ΩúÊèêÂçáËØ≠Ë®Ä‰ª£ÁêÜÊé®ÁêÜËÉΩÂäõ', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Search-R2ÁöÑÊ°ÜÊû∂ÔºåÈÄöËøáÊºîÂëò-‰øÆÊ≠£ËÄÖÁöÑÂçè‰ΩúÊù•ÊîπÂñÑËØ≠Ë®Ä‰ª£ÁêÜÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂Ëß£ÂÜ≥‰∫ÜÂº∫ÂåñÂ≠¶‰π†‰∏≠ÁöÑÂ§öÂ∞∫Â∫¶‰ø°Áî®ÂàÜÈÖçÈóÆÈ¢òÔºåÈááÁî®‰∫ÜÈíàÂØπÊÄßÁöÑÂπ≤È¢ÑÂíåÁªÜÁ≤íÂ∫¶ÁöÑÂ•ñÂä±ÁõëÁù£„ÄÇÊºîÂëòË¥üË¥£ÁîüÊàêÂàùÊ≠•ÁöÑÊé®ÁêÜËΩ®ËøπÔºåËÄå‰øÆÊ≠£ËÄÖÂàôÈÄöËøá‚ÄúÂàáÂâ≤-ÂÜçÁîüÊàê‚ÄùÊú∫Âà∂Êù•ËØäÊñ≠Âíå‰øÆÂ§çÈîôËØØÊ≠•È™§„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSearch-R2Âú®Â§öÁßçÈóÆÁ≠îÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºò‰∫éÁé∞ÊúâÁöÑÂü∫Á∫øÊñπÊ≥ïÔºåÊòæËëóÊèêÈ´ò‰∫ÜÊé®ÁêÜÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01053', 'title': 'LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents', 'url': 'https://huggingface.co/papers/2602.01053', 'abstract': 'LRAgent is a KV cache sharing framework for multi-LoRA agents that decomposes cache into shared and adapter-dependent components, reducing memory and compute overhead while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-A multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.', 'score': 5, 'issue_id': 895, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': 'd5a1b3c5ea1bc139', 'authors': ['Hyesung Jeon', 'Hyeongju Ha', 'Jae-Joon Kim'], 'affiliations': ['Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2602.01053.jpg', 'data': {'categories': ['#agents', '#inference', '#architecture'], 'emoji': '‚ö°', 'ru': {'title': '–£–º–Ω–æ–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞ –¥–ª—è –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏', 'desc': 'LRAgent ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è KV –∫—ç—à–∞ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∫—ç—à–∞ –Ω–∞ –¥–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: –æ–±—â—É—é —á–∞—Å—Ç—å, –ø–æ–ª—É—á–µ–Ω–Ω—É—é –∏–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, –∏ –∑–∞–≤–∏—Å—è—â—É—é –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞ —á–∞—Å—Ç—å, –∫–æ—Ç–æ—Ä–∞—è —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–≤–æ–π —Ñ–æ—Ä–º–µ. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç Flash-LoRA-Attention ‚Äî —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ —è–¥—Ä–æ, –∫–æ—Ç–æ—Ä–æ–µ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è, –∏–∑–±–µ–≥–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏—è –Ω–∏–∑–∫–æ—Ä–∞–Ω–≥–æ–≤–æ–≥–æ –∫—ç—à–∞ –≤ –ø–æ–ª–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ—Ç–∫–ª–∏–∫–∞, –±–ª–∏–∑–∫–∏—Ö –∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—â–µ–º—É –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—é, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Efficient Cache Sharing for Multi-LoRA Agents', 'desc': 'LRAgent is a framework designed to optimize memory and computational efficiency in multi-LoRA agent systems by sharing key-value (KV) caches. It separates the cache into two parts: a shared component derived from the pretrained backbone and an adapter-dependent component specific to each agent. This approach minimizes memory usage by allowing agents to share the base cache while storing only the necessary low-rank adapter components. Additionally, LRAgent employs a novel kernel called Flash-LoRA-Attention to streamline attention computations, resulting in faster processing times without sacrificing accuracy.'}, 'zh': {'title': 'LRAgentÔºöÈ´òÊïàÁöÑÂ§öLoRA‰ª£ÁêÜÁºìÂ≠òÂÖ±‰∫´Ê°ÜÊû∂', 'desc': 'LRAgentÊòØ‰∏Ä‰∏™Áî®‰∫éÂ§öLoRA‰ª£ÁêÜÁöÑKVÁºìÂ≠òÂÖ±‰∫´Ê°ÜÊû∂ÔºåÂÆÉÂ∞ÜÁºìÂ≠òÂàÜËß£‰∏∫ÂÖ±‰∫´ÁªÑ‰ª∂ÂíåÈÄÇÈÖçÂô®‰æùËµñÁªÑ‰ª∂Ôºå‰ªéËÄåÂáèÂ∞ëÂÜÖÂ≠òÂíåËÆ°ÁÆóÂºÄÈîÄÔºåÂêåÊó∂‰øùÊåÅÂáÜÁ°ÆÊÄß„ÄÇËØ•Ê°ÜÊû∂Âà©Áî®ÂÖ±‰∫´ÁöÑÈ¢ÑËÆ≠ÁªÉÂü∫Á°ÄÊ®°ÂûãÊùÉÈáçÔºåÈôç‰Ωé‰∫ÜÂÜÖÂ≠òÂç†Áî®ÔºåÂπ∂ÈÄöËøá‰ª•‰ΩéÁß©ÂΩ¢ÂºèÂ≠òÂÇ®ÈÄÇÈÖçÂô®ÁªÑ‰ª∂ÔºåËøõ‰∏ÄÊ≠•ÂáèÂ∞ëËÆ°ÁÆóÂºÄÈîÄ„ÄÇLRAgentËøòÂºïÂÖ•‰∫ÜFlash-LoRA-AttentionÂÜÖÊ†∏Ôºå‰ºòÂåñ‰∫ÜÊ≥®ÊÑèÂäõËÆ°ÁÆóÔºåÈÅøÂÖç‰∫ÜÂ∞Ü‰ΩéÁß©ÁºìÂ≠òÊâ©Â±ïÂà∞ÂÖ®Áª¥Â∫¶„ÄÇÈÄöËøáËøô‰∫õÂàõÊñ∞ÔºåLRAgentÂú®‰ª£ÁêÜÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠ÂÆûÁé∞‰∫ÜÊé•ËøëÂÆåÂÖ®ÂÖ±‰∫´ÁºìÂ≠òÁöÑÂêûÂêêÈáèÂíåÈ¶ñÊ¨°ÂìçÂ∫îÂª∂ËøüÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊé•ËøëÈùûÂÖ±‰∫´ÁºìÂ≠òÁöÑÂáÜÁ°ÆÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00359', 'title': 'Position: Agentic Evolution is the Path to Evolving LLMs', 'url': 'https://huggingface.co/papers/2602.00359', 'abstract': 'Large language models face limitations in adapting to changing real-world environments, necessitating a new approach called agentic evolution that treats deployment-time improvement as a goal-directed optimization process.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.', 'score': 5, 'issue_id': 906, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '473194f48499c7c3', 'authors': ['Minhua Lin', 'Hanqing Lu', 'Zhan Shi', 'Bing He', 'Rui Mao', 'Zhiwei Zhang', 'Zongyu Wu', 'Xianfeng Tang', 'Hui Liu', 'Zhenwei Dai', 'Xiang Zhang', 'Suhang Wang', 'Benoit Dumoulin', 'Jian Pei'], 'affiliations': ['Amazon', 'Duke University', 'The Pennsylvania State University'], 'pdf_title_img': 'assets/pdf/title_img/2602.00359.jpg', 'data': {'categories': ['#agents', '#training'], 'emoji': 'üß¨', 'ru': {'title': '–û—Ç —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –∂–∏–≤–æ–π —ç–≤–æ–ª—é—Ü–∏–∏: –∞–≥–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∞—Ç—Å—è –Ω–∞ –ª–µ—Ç—É', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∏–∑–º–µ–Ω—è—é—â–∏—Ö—Å—è —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π –∞–≥–µ–Ω—Ç–∏–≤–Ω–æ–π —ç–≤–æ–ª—é—Ü–∏–µ–π. –ê–≤—Ç–æ—Ä—ã —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —Ä–∞–∑—Ä—ã–≤–æ–º –º–µ–∂–¥—É –≤—Ä–µ–º–µ–Ω–µ–º –æ–±—É—á–µ–Ω–∏—è –∏ –≤—Ä–µ–º–µ–Ω–µ–º —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ –º–æ–∂–µ—Ç —Å–ª–µ–¥–∏—Ç—å –∑–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã. –û–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ A-Evolve, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ —ç—Ç–∞–ø–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∫–∞–∫ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞–¥ —É—Å—Ç–æ–π—á–∏–≤—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º —Å–∏—Å—Ç–µ–º—ã. –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–≤–∏–≥–∞—é—Ç –≥–∏–ø–æ—Ç–µ–∑—É —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ—Ç–æ—Ä–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Ä–∞—Å—Ç—ë—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º —Ä–µ—Å—É—Ä—Å–∞–º, –≤—ã–¥–µ–ª—è–µ–º—ã–º –Ω–∞ —ç–≤–æ–ª—é—Ü–∏—é.'}, 'en': {'title': 'Empowering LLMs with Autonomous Evolution for Real-World Adaptation', 'desc': 'This paper introduces the concept of agentic evolution for Large Language Models (LLMs) to address their limitations in adapting to changing real-world environments. It argues that traditional methods like fine-tuning and memory accumulation are insufficient for effective deployment-time adaptation. The authors propose a framework called A-Evolve, which treats improvement as a goal-directed optimization process that allows LLMs to evolve autonomously. They also present the evolution-scaling hypothesis, suggesting that the ability to adapt increases with the computational resources dedicated to the evolution process.'}, 'zh': {'title': '‰ª£ÁêÜËøõÂåñÔºöÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊú™Êù•ÈÄÇÂ∫î‰πãË∑Ø', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÈÄÇÂ∫î‰∏çÊñ≠ÂèòÂåñÁöÑÁé∞ÂÆûÁéØÂ¢ÉÊó∂Èù¢‰∏¥Â±ÄÈôêÊÄßÔºåÂõ†Ê≠§ÈúÄË¶Å‰∏ÄÁßçÊñ∞ÁöÑÊñπÊ≥ïÔºåÁß∞‰∏∫‰ª£ÁêÜËøõÂåñÔºåÂÆÉÂ∞ÜÈÉ®ÁΩ≤Êó∂ÁöÑÊîπËøõËßÜ‰∏∫‰∏Ä‰∏™ÁõÆÊ†áÂØºÂêëÁöÑ‰ºòÂåñËøáÁ®ã„ÄÇÁé∞ÊúâÁöÑÈÉ®ÁΩ≤Êó∂Èó¥ÈÄÇÂ∫îÊñπÊ≥ïÁº∫‰πèÂøÖË¶ÅÁöÑÊàòÁï•ËÉΩÂäõÔºåÊó†Ê≥ïÊúâÊïàËØäÊñ≠Â§±Ë¥•Âπ∂‰∫ßÁîüÊåÅ‰πÖÁöÑÊîπËøõ„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑA-EvolveÊ°ÜÊû∂Â∞ÜÈÉ®ÁΩ≤Êó∂ÁöÑÊîπËøõËßÜ‰∏∫‰∏Ä‰∏™ÊúâÊÑèËØÜÁöÑ‰ºòÂåñËøáÁ®ãÔºåÂº∫Ë∞ÉËøõÂåñÁöÑËá™‰∏ªÊÄß„ÄÇÊàë‰ª¨ËøòÊèêÂá∫‰∫ÜËøõÂåñÊâ©Â±ïÂÅáËÆæÔºöÈÄÇÂ∫îËÉΩÂäõ‰∏éÂàÜÈÖçÁªôËøõÂåñÁöÑËÆ°ÁÆóËµÑÊ∫êÊàêÊ≠£ÊØîÔºå‰ª£ÁêÜËøõÂåñÊòØÂÆûÁé∞ÊåÅÁª≠ÂºÄÊîæÂºèÈÄÇÂ∫îÁöÑÂèØÊâ©Â±ïË∑ØÂæÑ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02537', 'title': 'WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2602.02537', 'abstract': 'WorldVQA is a benchmark for evaluating the visual world knowledge of multimodal large language models by separating visual knowledge retrieval from reasoning to measure memorized facts.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.', 'score': 5, 'issue_id': 892, 'pub_date': '2026-01-28', 'pub_date_card': {'ru': '28 —è–Ω–≤–∞—Ä—è', 'en': 'January 28', 'zh': '1Êúà28Êó•'}, 'hash': '6d9ce4ef7deff235', 'authors': ['Runjie Zhou', 'Youbo Shao', 'Haoyu Lu', 'Bowei Xing', 'Tongtong Bai', 'Yujie Chen', 'Jie Zhao', 'Lin Sui', 'Haotian Yao', 'Zijia Zhao', 'Hao Yang', 'Haoning Wu', 'Zaida Zhou', 'Jinguo Zhu', 'Zhiqi Huang', 'Yiping Bao', 'Yangyang Liu', 'Y. Charles', 'Xinyu Zhou'], 'affiliations': ['Moonshot AI'], 'pdf_title_img': 'assets/pdf/title_img/2602.02537.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#cv'], 'emoji': 'üåç', 'ru': {'title': '–ò–∑–º–µ—Ä–µ–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–æ–º–Ω–∏–ª–∞ –æ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –º–∏—Ä–µ', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç WorldVQA ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ì–ª–∞–≤–Ω–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞–Ω–∏—è –æ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —á—Ç–æ–±—ã –∏–∑–º–µ—Ä–∏—Ç—å –∏–º–µ–Ω–Ω–æ –∑–∞–ø–æ–º–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ñ–∞–∫—Ç—ã. –ë–µ–Ω—á–º–∞—Ä–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –±–∞–∑–æ–≤—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∏ –Ω–∞–∑—ã–≤–∞—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –∏–∑ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–∞–∫—Å–æ–Ω–æ–º–∏–∏ ‚Äî –æ—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –¥–æ —Ä–µ–¥–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏, —ç–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏—á–µ—Å–∫–æ–π –ø–æ–ª–Ω–æ—Ç—ã –∏ —É—Ä–æ–≤–Ω—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ –±—É–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'WorldVQA: Measuring Visual Knowledge in AI Models', 'desc': "WorldVQA is a new benchmark aimed at evaluating the visual knowledge of Multimodal Large Language Models (MLLMs). It distinguishes between the retrieval of visual knowledge and reasoning, allowing for a clearer assessment of what the model has memorized. The benchmark tests the model's ability to identify and name visual entities across a wide range of categories, from common objects to rare items. By doing so, WorldVQA aims to provide a standard for measuring the accuracy and comprehensiveness of visual knowledge in AI models."}, 'zh': {'title': 'WorldVQAÔºöËØÑ‰º∞ËßÜËßâÁü•ËØÜÁöÑÊñ∞Ê†áÂáÜ', 'desc': 'WorldVQAÊòØ‰∏Ä‰∏™Âü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜËßâ‰∏ñÁïåÁü•ËØÜ„ÄÇÂÆÉÂ∞ÜËßÜËßâÁü•ËØÜÊ£ÄÁ¥¢‰∏éÊé®ÁêÜÂàÜÂºÄÔºå‰ª•‰∏•Ê†ºÊµãÈáèÊ®°ÂûãÊâÄËÆ∞ÂøÜÁöÑ‰∫ãÂÆû„ÄÇËØ•Âü∫ÂáÜËØÑ‰º∞Ê®°ÂûãÂú®‰∏çÂêåÂ±ÇÊ¨°ÂàÜÁ±ª‰∏≠ÁöÑÂü∫Á°ÄËÉΩÂäõÔºåÂåÖÊã¨Â∏∏ËßÅÁâ©‰ΩìÂíåÁ®ÄÊúâÁâ©‰ΩìÁöÑËØÜÂà´‰∏éÂëΩÂêç„ÄÇÊàë‰ª¨Â∏åÊúõWorldVQAËÉΩÊàê‰∏∫ËØÑ‰º∞ËßÜËßâ‰∫ãÂÆûÊÄßÁöÑ‰∏•Ê†ºÊµãËØïÊ†áÂáÜÔºå‰ªéËÄå‰∏∫ÂΩìÂâçÂíå‰∏ã‰∏Ä‰ª£ÂâçÊ≤øÊ®°ÂûãÁöÑÁôæÁßëÂÖ®‰π¶ÂπøÂ∫¶ÂíåÂπªËßâÁéáÊèê‰æõËØÑ‰º∞‰æùÊçÆ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01753', 'title': 'ObjEmbed: Towards Universal Multimodal Object Embeddings', 'url': 'https://huggingface.co/papers/2602.01753', 'abstract': 'ObjEmbed is a novel multimodal language-model embedding approach that decomposes images into regional embeddings for improved object-level visual understanding and retrieval tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.', 'score': 4, 'issue_id': 892, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '453d05bbcb5933c8', 'authors': ['Shenghao Fu', 'Yukun Su', 'Fengyun Rao', 'Jing Lyu', 'Xiaohua Xie', 'Wei-Shi Zheng'], 'affiliations': ['Guangdong Province Key Laboratory of Information Security Technology, China', 'Independent Researcher', 'Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China', 'Pazhou Laboratory (Huangpu), China', 'Peng Cheng Laboratory, China', 'School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.01753.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#benchmark', '#cv'], 'emoji': 'üéØ', 'ru': {'title': '–û–±—ä–µ–∫—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'ObjEmbed ‚Äî —ç—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –¥–µ—Ç–∞–ª–µ–π. –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–≤–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å–º—ã—Å–ª—É –∏ IoU-—ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏. –°–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è visual grounding, –ª–æ–∫–∞–ª—å–Ω—ã–π –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—è –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã –≤ –æ–¥–Ω–æ–º forward pass –¥–ª—è –≤—ã—Å–æ–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ 18 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–∏–ª—å–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—é –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Enhancing Visual Understanding with Regional Embeddings', 'desc': 'ObjEmbed is a new approach in multimodal language modeling that enhances the understanding of images by breaking them down into regional embeddings. This method addresses the challenge of aligning specific image regions with their corresponding textual descriptions, which is crucial for tasks like visual grounding and image retrieval. By generating both object embeddings for semantic matching and IoU embeddings for localization quality, ObjEmbed improves the accuracy of object retrieval. Its efficient encoding allows for simultaneous processing of all objects and the full image, leading to superior performance across various benchmarks.'}, 'zh': {'title': 'ObjEmbedÔºöÊèêÂçáËßÜËßâÁêÜËß£ÁöÑÂ§öÊ®°ÊÄÅÂµåÂÖ•ÊñπÊ≥ï', 'desc': 'ObjEmbedÊòØ‰∏ÄÁßçÊñ∞È¢ñÁöÑÂ§öÊ®°ÊÄÅËØ≠Ë®ÄÊ®°ÂûãÂµåÂÖ•ÊñπÊ≥ïÔºåÂÆÉÂ∞ÜÂõæÂÉèÂàÜËß£‰∏∫Âå∫ÂüüÂµåÂÖ•Ôºå‰ª•ÊèêÈ´òÂØπË±°Á∫ßËßÜËßâÁêÜËß£ÂíåÊ£ÄÁ¥¢‰ªªÂä°ÁöÑÊïàÊûú„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÂõæÂÉèÂå∫Âüü‰∏éÁâπÂÆöÁü≠ËØ≠‰πãÈó¥ÁöÑÁªÜÁ≤íÂ∫¶ÂØπÈΩêÈóÆÈ¢òÔºåÊîØÊåÅËßÜËßâÂÆö‰Ωç„ÄÅÂ±ÄÈÉ®ÂõæÂÉèÊ£ÄÁ¥¢ÂíåÂÖ®Â±ÄÂõæÂÉèÊ£ÄÁ¥¢Á≠âÂ§öÁßçËßÜËßâÁêÜËß£‰ªªÂä°„ÄÇObjEmbedÂÖ∑Êúâ‰∏â‰∏™ÂÖ≥ÈîÆÁâπÊÄßÔºöÂØπË±°ÂØºÂêëË°®Á§∫„ÄÅÈÄöÁî®ÊÄßÂíåÈ´òÊïàÁºñÁ†ÅÔºåËÉΩÂ§üÂêåÊó∂Â§ÑÁêÜÂå∫ÂüüÁ∫ßÂíåÂõæÂÉèÁ∫ß‰ªªÂä°„ÄÇÈÄöËøáÂú®18‰∏™‰∏çÂêåÂü∫ÂáÜ‰∏äÁöÑ‰ºòË∂äË°®Áé∞ÔºåËØÅÊòé‰∫ÜÂÖ∂Âº∫Â§ßÁöÑËØ≠‰πâÂå∫ÂàÜËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2601.19103', 'title': 'Glance and Focus Reinforcement for Pan-cancer Screening', 'url': 'https://huggingface.co/papers/2601.19103', 'abstract': "A reinforcement learning framework with glance and focus models improves pan-cancer screening in CT scans by addressing foreground-background imbalance and reducing false positives through group relative learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Pan-cancer screening in large-scale CT scans remains challenging for existing AI methods, primarily due to the difficulty of localizing diverse types of tiny lesions in large CT volumes. The extreme foreground-background imbalance significantly hinders models from focusing on diseased regions, while redundant focus on healthy regions not only decreases the efficiency but also increases false positives. Inspired by radiologists' glance and focus diagnostic strategy, we introduce GF-Screen, a Glance and Focus reinforcement learning framework for pan-cancer screening. GF-Screen employs a Glance model to localize the diseased regions and a Focus model to precisely segment the lesions, where segmentation results of the Focus model are leveraged to reward the Glance model via Reinforcement Learning (RL). Specifically, the Glance model crops a group of sub-volumes from the entire CT volume and learns to select the sub-volumes with lesions for the Focus model to segment. Given that the selecting operation is non-differentiable for segmentation training, we propose to employ the segmentation results to reward the Glance model. To optimize the Glance model, we introduce a novel group relative learning paradigm, which employs group relative comparison to prioritize high-advantage predictions and discard low-advantage predictions within sub-volume groups, not only improving efficiency but also reducing false positives. In this way, for the first time, we effectively extend cutting-edge RL techniques to tackle the specific challenges in pan-cancer screening. Extensive experiments on 16 internal and 7 external datasets across 9 lesion types demonstrated the effectiveness of GF-Screen. Notably, GF-Screen leads the public validation leaderboard of MICCAI FLARE25 pan-cancer challenge, surpassing the FLARE24 champion solution by a large margin (+25.6% DSC and +28.2% NSD).", 'score': 4, 'issue_id': 892, 'pub_date': '2026-01-27', 'pub_date_card': {'ru': '27 —è–Ω–≤–∞—Ä—è', 'en': 'January 27', 'zh': '1Êúà27Êó•'}, 'hash': 'e23fd9bb0e094ee5', 'authors': ['Linshan Wu', 'Jiaxin Zhuang', 'Hao Chen'], 'affiliations': ['The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.19103.jpg', 'data': {'categories': ['#rl', '#cv', '#healthcare'], 'emoji': 'üîç', 'ru': {'title': '–î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π —Å–∫—Ä–∏–Ω–∏–Ω–≥ —Ä–∞–∫–∞ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≥—Ä—É–ø–ø–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', 'desc': '–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ GF-Screen –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —Å–∫—Ä–∏–Ω–∏–Ω–≥–∞ —Ä–∞–∫–∞ –ø–æ –ö–¢-—Å–Ω–∏–º–∫–∞–º, –∫–æ—Ç–æ—Ä–∞—è —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –±–æ–ª—å–Ω—ã–º–∏ –∏ –∑–¥–æ—Ä–æ–≤—ã–º–∏ —Ç–∫–∞–Ω—è–º–∏. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –º–æ–¥–µ–ª—å "–í–∑–≥–ª—è–¥" –ª–æ–∫–∞–ª–∏–∑—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø–æ—Ä–∞–∂—ë–Ω–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –∞ –º–æ–¥–µ–ª—å "–§–æ–∫—É—Å" —Ç–æ—á–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –ø–æ—Ä–∞–∂–µ–Ω–∏—è. –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ "–í–∑–≥–ª—è–¥" –æ—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ "–§–æ–∫—É—Å" —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –Ω–æ–≤–∞—è –ø–∞—Ä–∞–¥–∏–≥–º–∞ –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –≤–Ω—É—Ç—Ä–∏ –≥—Ä—É–ø–ø –ø–æ–¥–æ–±—ä—ë–º–æ–≤, –ø–æ–≤—ã—à–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–∂–Ω–æ–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.'}, 'en': {'title': 'Enhancing Pan-Cancer Screening with Glance and Focus Models', 'desc': 'This paper presents GF-Screen, a novel reinforcement learning framework designed to enhance pan-cancer screening in CT scans. It addresses the challenge of foreground-background imbalance by utilizing a Glance model to identify regions with lesions and a Focus model to accurately segment these lesions. The framework employs group relative learning to optimize the Glance model, allowing it to prioritize the most promising sub-volumes for analysis while minimizing false positives. Extensive testing on multiple datasets shows that GF-Screen significantly outperforms existing methods, achieving top results in a major pan-cancer challenge.'}, 'zh': {'title': 'GF-ScreenÔºöÊèêÂçáCTÊ≥õÁôåÁóáÁ≠õÊü•ÁöÑÊô∫ËÉΩËß£ÂÜ≥ÊñπÊ°à', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫GF-ScreenÁöÑÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂ÔºåÁî®‰∫éÊîπÂñÑCTÊâ´Êèè‰∏≠ÁöÑÊ≥õÁôåÁóáÁ≠õÊü•„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂºïÂÖ•‚ÄúÁû•ËßÜ‚ÄùÂíå‚ÄúËÅöÁÑ¶‚ÄùÊ®°ÂûãÔºåËß£ÂÜ≥‰∫ÜÂâçÊôØ‰∏éËÉåÊôØ‰∏çÂπ≥Ë°°ÁöÑÈóÆÈ¢òÔºåÂπ∂ÈÄöËøáÁæ§‰ΩìÁõ∏ÂØπÂ≠¶‰π†ÂáèÂ∞ë‰∫ÜÂÅáÈò≥ÊÄß„ÄÇGF-ScreenÂà©Áî®Áû•ËßÜÊ®°ÂûãÂÆö‰ΩçÁóÖÂèòÂå∫ÂüüÔºåÂπ∂ÈÄöËøáËÅöÁÑ¶Ê®°ÂûãÁ≤æÁ°ÆÂàÜÂâ≤ÁóÖÁÅ∂Ôºå‰ºòÂåñ‰∫ÜÊ®°ÂûãÁöÑÂ≠¶‰π†ËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåGF-ScreenÂú®Â§ö‰∏™Êï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåÊòæËëóÊèêÈ´ò‰∫ÜÁ≠õÊü•ÁöÑÂáÜÁ°ÆÊÄßÂíåÊïàÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03806', 'title': 'Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation', 'url': 'https://huggingface.co/papers/2602.03806', 'abstract': "Offline reinforcement learning method combines contextual bandit learning with partial trajectories to improve multi-turn code generation performance while reducing training costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.", 'score': 3, 'issue_id': 893, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '23c0a359167c4dfe', 'authors': ['Ziru Chen', 'Dongdong Chen', 'Ruinan Jin', 'Yingbin Liang', 'Yujia Xie', 'Huan Sun'], 'affiliations': ['Microsoft', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2602.03806.jpg', 'data': {'categories': ['#training', '#rl', '#plp'], 'emoji': 'üíª', 'ru': {'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ LLM —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –±–∞–Ω–¥–∏—Ç—ã –∏ –æ—Ñ—Ñ–ª–∞–π–Ω —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Cobalt, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–∞–Ω–¥–∏—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞ –Ω–∞ —è–∑—ã–∫–µ LLM. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∑–∞–¥–∞—á—É –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π, –ø–æ–∑–≤–æ–ª—è—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è –æ–¥–Ω–æ–ø—Ä–æ—Ö–æ–¥–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ú–µ—Ç–æ–¥ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –æ–Ω–ª–∞–π–Ω-–ø–æ–¥—Ö–æ–¥–∞–º–∏ RL –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç–∞—Ö –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º –º–æ–¥–µ–ª—å—é –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –µ—ë —Ä–µ—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –≤–æ–∑–º—É—â—ë–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏.'}, 'en': {'title': 'Cobalt: Bridging Offline and Online RL for Code Generation', 'desc': 'This paper introduces Cobalt, an offline reinforcement learning method that enhances multi-turn code generation by integrating contextual bandit learning with partial trajectories. By treating multi-turn code generation as a recoverable Markov decision process, Cobalt leverages previously collected code generation data to create contextual prompts for training. The method allows for efficient single-step code generation while significantly reducing training costs compared to traditional online reinforcement learning approaches. Experimental results show that Cobalt outperforms existing online RL methods, demonstrating its effectiveness in improving performance on code generation tasks.'}, 'zh': {'title': 'CobaltÔºöÊèêÂçáÂ§öËΩÆ‰ª£Á†ÅÁîüÊàêÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†Êñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÁ¶ªÁ∫øÂº∫ÂåñÂ≠¶‰π†ÊñπÊ≥ïCobaltÔºåÊó®Âú®ÊèêÈ´òÂ§öËΩÆ‰ª£Á†ÅÁîüÊàêÁöÑÊÄßËÉΩÔºåÂêåÊó∂Èôç‰ΩéËÆ≠ÁªÉÊàêÊú¨„ÄÇCobaltÁªìÂêà‰∫Ü‰∏ä‰∏ãÊñáËµåÂçöÂ≠¶‰π†ÂíåÈÉ®ÂàÜËΩ®ËøπÔºåÈÄöËøá‰ΩøÁî®ÂèÇËÄÉÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊî∂ÈõÜ‰ª£Á†ÅÁîüÊàêËΩ®ËøπÔºåÂπ∂Â∞ÜÂÖ∂ÂàÜÂâ≤‰∏∫‰∏ä‰∏ãÊñáÊèêÁ§∫„ÄÇËØ•ÊñπÊ≥ïÂú®Âú®Á∫øËµåÂçöÂ≠¶‰π†‰∏≠ËÆ≠ÁªÉÊ®°ÂûãÂÆåÊàêÊØè‰∏™ÈÉ®ÂàÜËΩ®ËøπÊèêÁ§∫ÔºåÊòæËëó‰ºò‰∫éÁé∞ÊúâÁöÑÂ§öËΩÆÂú®Á∫øÂº∫ÂåñÂ≠¶‰π†Âü∫Á∫ø„ÄÇÁ†îÁ©∂ËøòÂàÜÊûê‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏ä‰∏ãÊñá‰∏≠ÁöÑÂ•ñÂä±ÈªëÂÆ¢Ë°å‰∏∫ÔºåÂπ∂ÈÄöËøáÊâ∞Âä®ËΩ®ËøπÂ¢ûÂº∫CobaltÁöÑËÆ≠ÁªÉÔºå‰ª•ÂáèËΩªËøô‰∏ÄÈóÆÈ¢ò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03454', 'title': 'Contextualized Visual Personalization in Vision-Language Models', 'url': 'https://huggingface.co/papers/2602.03454', 'abstract': "CoViP addresses contextualized visual personalization by treating personalized image captioning as a core task and improving capabilities through reinforcement-learning-based post-training and caption-augmented generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.", 'score': 3, 'issue_id': 892, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '62f7fb59d0b1e6db', 'authors': ['Yeongtak Oh', 'Sangwon Yu', 'Junsung Park', 'Han Cheol Moon', 'Jisoo Mok', 'Sungroh Yoon'], 'affiliations': ['Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea', 'Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, Korea', 'Samsung Electronics, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2602.03454.jpg', 'data': {'categories': ['#rl', '#multimodal', '#benchmark', '#cv'], 'emoji': 'üì∏', 'ru': {'title': '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∑—Ä–µ–Ω–∏—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π vision-language', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ CoViP –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∑—Ä–µ–Ω–∏—è-—è–∑—ã–∫–∞. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –Ω–∞—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–ø–∏—Å–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å —É—á–µ—Ç–æ–º –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∞—Å—Å–æ—Ü–∏–∏—Ä—É—è –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ –∞–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –ø–æ–¥–ø–∏—Å—è–º–∏, –∞ —Ç–∞–∫–∂–µ –≤–≤–æ–¥—è—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏, –∏—Å–∫–ª—é—á–∞—é—â–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥–ø–∏—Å–µ–π, —Ç–∞–∫ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–º–µ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'Enhancing Personalization in Image Captioning with CoViP', 'desc': "CoViP is a framework designed to enhance personalized image captioning by integrating contextualized visual personalization. It addresses the limitations of current vision-language models (VLMs) that struggle to generate responses tailored to individual user experiences. By employing reinforcement learning for post-training and augmenting generation with captions, CoViP improves the model's ability to connect visual inputs with a user's unique context. The framework also includes diagnostic evaluations to ensure that VLMs effectively utilize visual context rather than relying on shortcuts, demonstrating significant improvements in personalized tasks."}, 'zh': {'title': 'CoViPÔºöÂÆûÁé∞‰∏ä‰∏ãÊñáÂåñÁöÑËßÜËßâ‰∏™ÊÄßÂåñ', 'desc': 'CoViPÊòØ‰∏Ä‰∏™Êñ∞ÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÈÄöËøá‰∏™ÊÄßÂåñÂõæÂÉèÊèèËø∞Êù•ÂÆûÁé∞‰∏ä‰∏ãÊñáÂåñÁöÑËßÜËßâ‰∏™ÊÄßÂåñ„ÄÇÂÆÉÂà©Áî®Âº∫ÂåñÂ≠¶‰π†ËøõË°åÂêéÊúüËÆ≠ÁªÉÔºåÂπ∂ÈÄöËøáÂ¢ûÂº∫ÊèèËø∞ÁîüÊàêÊù•ÊèêÂçáËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïËß£ÂÜ≥‰∫ÜÁé∞ÊúâËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÂú®ÁîüÊàê‰∏™ÊÄßÂåñÂìçÂ∫îÊó∂ÁöÑ‰∏çË∂≥ÔºåËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁªìÂêàÁî®Êà∑ÁöÑËßÜËßâÂíåÊñáÊú¨ËÉåÊôØ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCoViPÂú®‰∏™ÊÄßÂåñÂõæÂÉèÊèèËø∞ÂíåÂÖ∂‰ªñ‰∏™ÊÄßÂåñ‰ªªÂä°‰∏äÂùáË°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03295', 'title': 'POP: Prefill-Only Pruning for Efficient Large Model Inference', 'url': 'https://huggingface.co/papers/2602.03295', 'abstract': 'Stage-aware pruning method for large language and vision-language models that improves efficiency by selectively removing layers during different processing phases while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37times speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.', 'score': 3, 'issue_id': 901, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '0010032aca79834a', 'authors': ['Junhui He', 'Zhihui Fu', 'Jun Wang', 'Qingan Li'], 'affiliations': ['OPPO Research Institute', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2602.03295.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#multimodal'], 'emoji': '‚ö°', 'ru': {'title': '–ò–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ —Å–ª–æ–µ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–æ–ª–∏ —Å–ª–æ–µ–≤ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ê–≤—Ç–æ—Ä—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏ –∫—Ä–∏—Ç–∏—á–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞, –Ω–æ –∏–∑–ª–∏—à–Ω–∏ –ø—Ä–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –û–Ω–∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–µ—Ç–æ–¥ Prefill-Only Pruning, –∫–æ—Ç–æ—Ä—ã–π –±–µ–∑–æ–ø–∞—Å–Ω–æ —É–¥–∞–ª—è–µ—Ç –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—è—Ö Llama-3.1 –∏ Qwen3-VL –ø–æ–∫–∞–∑–∞–ª–∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ 1.37 —Ä–∞–∑–∞ –±–µ–∑ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏.'}, 'en': {'title': 'Efficient Layer Pruning for Enhanced Model Performance', 'desc': 'This paper presents a novel stage-aware pruning method for large language models (LLMs) and vision-language models (VLMs) that enhances computational efficiency without sacrificing accuracy. The authors identify that traditional pruning techniques often degrade performance because they do not consider the different roles of model layers during the prefill and decode stages. By implementing a Prefill-Only Pruning (POP) strategy, they selectively remove deep layers during the prefill phase, which is less sensitive to accuracy, while keeping the full model intact for the decode phase, where precision is crucial. Their experiments show that this approach can significantly speed up prefill latency by up to 1.37 times, demonstrating a successful balance between efficiency and model performance.'}, 'zh': {'title': 'Èò∂ÊÆµÊÑüÁü•Ââ™ÊûùÔºöÊèêÂçáÊïàÁéá‰∏éÂáÜÁ°ÆÊÄßÁöÑÂπ≥Ë°°', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÈò∂ÊÆµÊÑüÁü•ÁöÑÂâ™ÊûùÊñπÊ≥ïÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂíåËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÊïàÁéá„ÄÇÈÄöËøáÂú®‰∏çÂêåÂ§ÑÁêÜÈò∂ÊÆµÈÄâÊã©ÊÄßÂú∞ÁßªÈô§Â±ÇÔºå‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÊàë‰ª¨ÂºïÂÖ•‰∫ÜËôöÊãüÈó®Êú∫Âà∂ÔºåÂàÜÊûê‰∫ÜÊ∑±Â±ÇÂú®‰∏ã‰∏Ä‰∏™Ê†áËÆ∞È¢ÑÊµã‰∏≠ÁöÑÈáçË¶ÅÊÄßÔºåÂπ∂ÊèêÂá∫‰∫Ü‰ªÖÂú®È¢ÑÂ°´ÂÖÖÈò∂ÊÆµÂâ™ÊûùÁöÑÁ≠ñÁï•„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•ÊñπÊ≥ïÂú®È¢ÑÂ°´ÂÖÖÂª∂Ëøü‰∏äÂÆûÁé∞‰∫ÜÈ´òËææ1.37ÂÄçÁöÑÂä†ÈÄüÔºåÂêåÊó∂ÊÄßËÉΩÊçüÂ§±ÊúÄÂ∞è„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02419', 'title': 'SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration', 'url': 'https://huggingface.co/papers/2602.02419', 'abstract': 'SafeGround is a uncertainty-aware framework for GUI grounding models that uses distribution-aware uncertainty quantification and calibration to enable risk-aware predictions with controlled false discovery rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38% percentage points over Gemini-only inference.', 'score': 3, 'issue_id': 893, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'c6a8e53e2ac59cfd', 'authors': ['Qingni Wang', 'Yue Fan', 'Xin Eric Wang'], 'affiliations': ['University of California, Santa Barbara', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2602.02419.jpg', 'data': {'categories': ['#benchmark', '#inference', '#agents', '#security'], 'emoji': 'üõ°Ô∏è', 'ru': {'title': '–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å', 'desc': 'SafeGround ‚Äî —ç—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥—ã –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –Ω–∞ —ç–∫—Ä–∞–Ω–µ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Ä–∏—Å–∫–∞. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏. –í–æ –≤—Ä–µ–º—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ—Ä–æ–≥ —Ä–µ—à–µ–Ω–∏—è —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —á–∞—Å—Ç–æ—Ç—ã –ª–æ–∂–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π (FDR). –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SafeGround –ø–æ–≤—ã—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã –Ω–∞ 5,38% –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–¥—ë–∂–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –æ—à–∏–±–æ–∫ –ø—Ä–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å GUI.'}, 'en': {'title': 'Enhancing GUI Grounding with Uncertainty Awareness', 'desc': "SafeGround is a framework designed to improve the reliability of GUI grounding models by incorporating uncertainty awareness. It uses a method for quantifying uncertainty that captures how spread out the model's predictions are, which helps in understanding the risk of making incorrect decisions. The framework also includes a calibration process that sets a decision threshold to control the rate of false discoveries during testing. By applying SafeGround to various models, the results show significant improvements in accuracy and better differentiation between correct and incorrect predictions."}, 'zh': {'title': 'SafeGroundÔºöÊèêÂçáGUIÂÆö‰ΩçÊ®°ÂûãÁöÑÂèØÈù†ÊÄß‰∏éÂáÜÁ°ÆÊÄß', 'desc': 'SafeGroundÊòØ‰∏Ä‰∏™ÂÖ≥Ê≥®‰∏çÁ°ÆÂÆöÊÄßÁöÑÊ°ÜÊû∂ÔºåÊó®Âú®ÊèêÈ´òÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâÂÆö‰ΩçÊ®°ÂûãÁöÑÂèØÈù†ÊÄß„ÄÇÂÆÉÈÄöËøáÂàÜÂ∏ÉÊÑüÁü•ÁöÑ‰∏çÁ°ÆÂÆöÊÄßÈáèÂåñÂíåÊ†°ÂáÜÔºåËÉΩÂ§üÂú®ÊµãËØïÂâçËøõË°åÈ£éÈô©ÊÑèËØÜÁöÑÈ¢ÑÊµã„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÊçïÊçâÊ®°ÂûãËæìÂá∫ÁöÑÈöèÊú∫Ê†∑Êú¨ÁöÑÁ©∫Èó¥ÂàÜÊï£ÊÄßÔºåÊù•ÈáèÂåñ‰∏çÁ°ÆÂÆöÊÄßÔºåÂπ∂Âú®ÊµãËØïÊó∂ËÆæÂÆöÂÖ∑ÊúâÁªüËÆ°‰øùËØÅÁöÑÈîôËØØÂèëÁé∞ÁéáÔºàFDRÔºâÊéßÂà∂ÁöÑÂÜ≥Á≠ñÈòàÂÄº„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåSafeGroundÂú®Â§ö‰∏™GUIÂÆö‰ΩçÊ®°Âûã‰∏äÊòæËëóÊèêÈ´ò‰∫ÜÁ≥ªÁªüÁ∫ßÂáÜÁ°ÆÊÄßÔºåÊúÄÂ§öÂèØÊèêÂçá5.38‰∏™ÁôæÂàÜÁÇπ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03837', 'title': 'Accelerating Scientific Research with Gemini: Case Studies and Common Techniques', 'url': 'https://huggingface.co/papers/2602.03837', 'abstract': 'Advanced AI models demonstrate capability in supporting expert-level mathematical discovery and scientific research through collaborative approaches involving proof verification and automated code execution.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google\'s Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.', 'score': 2, 'issue_id': 892, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '477d690a8c841188', 'authors': ['David P. Woodruff', 'Vincent Cohen-Addad', 'Lalit Jain', 'Jieming Mao', 'Song Zuo', 'MohammadHossein Bateni', 'Simina Branzei', 'Michael P. Brenner', 'Lin Chen', 'Ying Feng', 'Lance Fortnow', 'Gang Fu', 'Ziyi Guan', 'Zahra Hadizadeh', 'Mohammad T. Hajiaghayi', 'Mahdi JafariRaviz', 'Adel Javanmard', 'Karthik C. S.', 'Ken-ichi Kawarabayashi', 'Ravi Kumar', 'Silvio Lattanzi', 'Euiwoong Lee', 'Yi Li', 'Ioannis Panageas', 'Dimitris Paparas', 'Benjamin Przybocki', 'Bernardo Subercaseaux', 'Ola Svensson', 'Shayan Taherijam', 'Xuan Wu', 'Eylon Yogev', 'Morteza Zadimoghaddam', 'Samson Zhou', 'Vahab Mirrokni'], 'affiliations': ['Bar-Ilan University', 'Carnegie Mellon University', 'EPFL', 'Google Research', 'Harvard University', 'Illinois Institute of Technology', 'MIT', 'Nanyang Technological University', 'National Institute of Informatics, Tokyo', 'Purdue University', 'Rutgers University', 'Texas A&M University', 'The University of Tokyo', 'University of California, Irvine', 'University of Maryland, College Park', 'University of Michigan', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2602.03837.jpg', 'data': {'categories': ['#reasoning', '#science', '#transfer_learning'], 'emoji': 'ü§ù', 'ru': {'title': 'AI –∫–∞–∫ –Ω–∞—É—á–Ω—ã–π –ø–∞—Ä—Ç–Ω—ë—Ä: –æ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫ –ø–æ–¥–ª–∏–Ω–Ω–æ–º—É —Å–æ—Ç–≤–æ—Ä—á–µ—Å—Ç–≤—É –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Google Gemini, –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∑–∞–¥–∞—á –∏ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ –∏ —Å–º–µ–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–µ–ª—è—é—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —á–µ–ª–æ–≤–µ–∫–æ-–º–∞—à–∏–Ω–Ω–æ–π –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ, –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—é –∑–∞–¥–∞—á –∏ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º–∏. –ú–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –∫–∞–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏, –Ω–æ –∏ –≤ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö‚Äî–≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ—Ü–µ–Ω–∑–µ–Ω—Ç–∞ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —Ç–æ–Ω–∫–∏—Ö –æ—à–∏–±–æ–∫ –≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞—Ö –∏ –≤ –Ω–µ–π—Ä–æ—Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏—Ö —Ü–∏–∫–ª–∞—Ö –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–∞–∫ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã—Ö –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –Ω–∞—É—á–Ω–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç–∏—è, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.'}, 'en': {'title': 'AI: A Collaborative Partner in Scientific Discovery', 'desc': "This paper explores how advanced AI models, particularly Google's Gemini, can assist researchers in making significant mathematical discoveries and conducting scientific research. It presents case studies where AI has helped solve open problems and generate new proofs in fields like theoretical computer science, economics, and physics. The authors identify effective collaboration techniques between humans and AI, such as iterative refinement and problem decomposition. Additionally, they showcase innovative uses of AI, including its role as a rigorous reviewer and its ability to autonomously verify complex proofs through code execution."}, 'zh': {'title': 'AI‰∏é‰∫∫Á±ªÂêà‰ΩúÔºåÊé®Âä®ÁßëÂ≠¶ÂèëÁé∞Êñ∞Á∫™ÂÖÉ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂÖàËøõÁöÑ‰∫∫Â∑•Êô∫ËÉΩÊ®°ÂûãÂú®Êï∞Â≠¶ÂèëÁé∞ÂíåÁßëÂ≠¶Á†îÁ©∂‰∏≠ÁöÑÂ∫îÁî®ÔºåÁâπÂà´ÊòØ‰∏éÁ†îÁ©∂‰∫∫ÂëòÁöÑÂêà‰Ωú„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåË∞∑Ê≠åÁöÑGeminiÊ®°ÂûãËÉΩÂ§üÂ∏ÆÂä©Ëß£ÂÜ≥ÂºÄÊîæÊÄßÈóÆÈ¢ò„ÄÅÂèçÈ©≥ÁåúÊÉ≥Âπ∂ÁîüÊàêÊñ∞ÁöÑËØÅÊòé„ÄÇÈÄöËøáÊ°à‰æãÁ†îÁ©∂ÔºåÊèêÁÇºÂá∫ÊúâÊïàÁöÑ‰∫∫Êú∫Âçè‰ΩúÊäÄÊúØÔºåÂ¶ÇËø≠‰ª£‰ºòÂåñ„ÄÅÈóÆÈ¢òÂàÜËß£ÂíåË∑®Â≠¶ÁßëÁü•ËØÜËΩ¨Áßª„ÄÇËÆ∫ÊñáÂº∫Ë∞ÉÔºåAI‰∏ç‰ªÖÊòØËá™Âä®ÂåñÂ∑•ÂÖ∑ÔºåÊõ¥ÊòØÁßëÂ≠¶ÂèëÁé∞ËøáÁ®ã‰∏≠ÁöÑÁúüÊ≠£Âêà‰Ωú‰ºô‰º¥„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02914', 'title': 'FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction', 'url': 'https://huggingface.co/papers/2602.02914', 'abstract': 'FaceLinkGen attack demonstrates that current privacy-preserving face recognition methods fail to protect identity information despite pixel-level distortion metrics suggesting adequate protection.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.', 'score': 2, 'issue_id': 892, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '8962edb941993f54', 'authors': ['Wenqi Guo', 'Shan Du'], 'affiliations': ['Department of CMPS, University of British Columbia, Kelowna, Canada', 'Weathon Software, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2602.02914.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#cv', '#security'], 'emoji': 'üîì', 'ru': {'title': '–ü–∏–∫—Å–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å –ª–∏—Ü', 'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∞—Ç–∞–∫–∞ FaceLinkGen, –∫–æ—Ç–æ—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ª–∏—Ü - –æ–Ω–∏ –Ω–µ –∑–∞—â–∏—â–∞—é—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—ã—Å–æ–∫–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø–∏–∫—Å–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –∏—Å–∫–∞–∂–µ–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –æ—Ü–µ–Ω–∫–µ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—é –ø–∏–∫—Å–µ–ª–µ–π (PSNR, SSIM), –Ω–æ —ç—Ç–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –∑–∞—â–∏—Ç—ã. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∞—Ç–∞–∫–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏ —Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ª–∏—Ü –ø—Ä—è–º–æ –∏–∑ –∑–∞—â–∏—â—ë–Ω–Ω—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤ –±–µ–∑ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –¥–æ—Å—Ç–∏–≥–∞—è –±–æ–ª–µ–µ 98.5% —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è. –†–∞–±–æ—Ç–∞ –≤—ã—è–≤–ª—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–∏–∫—Å–µ–ª—å–Ω–æ–≥–æ –∏—Å–∫–∞–∂–µ–Ω–∏—è –∏ —Ä–µ–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å—é, –¥–æ–∫–∞–∑—ã–≤–∞—è, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –æ–±—Ñ—É—Å–∫–∞—Ü–∏—è –æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —É—è–∑–≤–∏–º–æ–π –¥–ª—è –∞—Ç–∞–∫.'}, 'en': {'title': 'FaceLinkGen: Unmasking the Flaws in Privacy-Preserving Face Recognition', 'desc': 'The FaceLinkGen attack reveals that current privacy-preserving face recognition (PPFR) methods do not adequately protect individual identities, despite appearing effective based on pixel-level distortion metrics like PSNR and SSIM. This paper introduces FaceLinkGen, an attack that can extract and regenerate identities from protected face templates without needing to reconstruct the original pixel data. The results demonstrate that FaceLinkGen achieves over 98.5% matching accuracy and more than 96% regeneration success across various PPFR systems. This highlights a significant disconnect between traditional pixel distortion evaluations and actual privacy protection, showing that visual obfuscation techniques still leave identity information vulnerable to unauthorized access.'}, 'zh': {'title': 'Êè≠Á§∫ÈöêÁßÅ‰øùÊä§‰∫∫ËÑ∏ËØÜÂà´ÁöÑÁªìÊûÑÊÄßÁº∫Èô∑', 'desc': 'FaceLinkGenÊîªÂáªË°®ÊòéÔºåÂΩìÂâçÁöÑÈöêÁßÅ‰øùÊä§‰∫∫ËÑ∏ËØÜÂà´ÊñπÊ≥ïÂú®‰øùÊä§Ë∫´‰ªΩ‰ø°ÊÅØÊñπÈù¢Â≠òÂú®Áº∫Èô∑ÔºåÂ∞ΩÁÆ°ÂÉèÁ¥†Á∫ßÂ§±ÁúüÊåáÊ†áÁúã‰ººÊèê‰æõ‰∫ÜË∂≥Â§üÁöÑ‰øùÊä§„ÄÇÁé∞ÊúâËØÑ‰º∞‰∏ªË¶ÅÂ∞ÜÈöêÁßÅËßÜ‰∏∫ÂØπÂÉèÁ¥†Á∫ßÈáçÂª∫ÁöÑÊäµÊäóÔºå‰ΩøÁî®PSNRÂíåSSIMËøõË°åÊµãÈáè„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑFaceLinkGenÊîªÂáªËÉΩÂ§üÁõ¥Êé•‰ªéÂèó‰øùÊä§ÁöÑÊ®°Êùø‰∏≠ËøõË°åË∫´‰ªΩÈìæÊé•/ÂåπÈÖçÂíå‰∫∫ËÑ∏ÂÜçÁîüÔºåËÄåÊó†ÈúÄÊÅ¢Â§çÂéüÂßãÂÉèÁ¥†„ÄÇÂú®‰∏âÁßçÊúÄÊñ∞ÁöÑÈöêÁßÅ‰øùÊä§‰∫∫ËÑ∏ËØÜÂà´Á≥ªÁªü‰∏≠ÔºåFaceLinkGenÁöÑÂåπÈÖçÂáÜÁ°ÆÁéáË∂ÖËøá98.5%ÔºåÂÜçÁîüÊàêÂäüÁéáË∂ÖËøá96%ÔºåÂπ∂‰∏îÂú®Êé•ËøëÈõ∂Áü•ËØÜÁöÑÊÉÖÂÜµ‰∏ã‰ªçÁÑ∂Ë∂ÖËøá92%ÁöÑÂåπÈÖçÂíå94%ÁöÑÂÜçÁîüÊàêÂäüÁéá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02751', 'title': 'Scaling Small Agents Through Strategy Auctions', 'url': 'https://huggingface.co/papers/2602.02751', 'abstract': 'Small language models struggle with complex tasks but can be effectively coordinated through a marketplace-inspired framework that reduces costs and improves performance through strategic bidding and self-improvement mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Small language models are increasingly viewed as a promising, cost-effective approach to agentic AI, with proponents claiming they are sufficiently capable for agentic workflows. However, while smaller agents can closely match larger ones on simple tasks, it remains unclear how their performance scales with task complexity, when large models become necessary, and how to better leverage small agents for long-horizon workloads. In this work, we empirically show that small agents\' performance fails to scale with task complexity on deep search and coding tasks, and we introduce Strategy Auctions for Workload Efficiency (SALE), an agent framework inspired by freelancer marketplaces. In SALE, agents bid with short strategic plans, which are scored by a systematic cost-value mechanism and refined via a shared auction memory, enabling per-task routing and continual self-improvement without training a separate router or running all models to completion. Across deep search and coding tasks of varying complexity, SALE reduces reliance on the largest agent by 53%, lowers overall cost by 35%, and consistently improves upon the largest agent\'s pass@1 with only a negligible overhead beyond executing the final trace. In contrast, established routers that rely on task descriptions either underperform the largest agent or fail to reduce cost -- often both -- underscoring their poor fit for agentic workflows. These results suggest that while small agents may be insufficient for complex workloads, they can be effectively "scaled up" through coordinated task allocation and test-time self-improvement. More broadly, they motivate a systems-level view of agentic AI in which performance gains come less from ever-larger individual models and more from market-inspired coordination mechanisms that organize heterogeneous agents into efficient, adaptive ecosystems.', 'score': 2, 'issue_id': 905, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'ebdb4addd8141534', 'authors': ['Lisa Alazraki', 'William F. Shen', 'Yoram Bachrach', 'Akhil Mathur'], 'affiliations': ['Imperial College London', 'Meta Superintelligence Labs', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2602.02751.jpg', 'data': {'categories': ['#small_models', '#agents', '#training'], 'emoji': 'üè™', 'ru': {'title': '–ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å –∞–≥–µ–Ω—Ç–æ–≤: –∫–∞–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –º–∞–ª—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–º–µ—Ä', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç SALE (Strategy Auctions for Workload Efficiency) ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–∏–Ω—Ü–∏–ø–∞–º–∏ —Ñ—Ä–∏–ª–∞–Ω—Å-–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ –ø–ª–∞–Ω—ã —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º –∞—É–∫—Ü–∏–æ–Ω–∞. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—â—É—é –ø–∞–º—è—Ç—å –∞—É–∫—Ü–∏–æ–Ω–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É ¬´—Å—Ç–æ–∏–º–æ—Å—Ç—å-—Ü–µ–Ω–Ω–æ—Å—Ç—å¬ª –∏ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ SALE —Å–Ω–∏–∂–∞–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Å–∞–º–æ–π –∫—Ä—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ 53%, —É–º–µ–Ω—å—à–∞–µ—Ç –æ–±—â–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ 35% –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞–∏–±–æ–ª—å—à–µ–π –º–æ–¥–µ–ª–∏, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è, —á—Ç–æ –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.'}, 'en': {'title': 'Empowering Small Models through Strategic Coordination', 'desc': 'This paper discusses the limitations of small language models when handling complex tasks and introduces a new framework called Strategy Auctions for Workload Efficiency (SALE). SALE allows small agents to bid on tasks using strategic plans, which are evaluated based on cost and value, promoting self-improvement and efficient task allocation. The framework significantly reduces the need for larger models, cutting costs by 35% and improving performance on complex tasks. The findings suggest that instead of relying solely on larger models, a coordinated approach using smaller agents can enhance overall efficiency in agentic AI workflows.'}, 'zh': {'title': 'Â∞èÂûã‰ª£ÁêÜÁöÑÂ∏ÇÂú∫ÂåñÂçèË∞ÉÊèêÂçáÊÄßËÉΩ', 'desc': 'Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Â§ÑÁêÜÂ§çÊùÇ‰ªªÂä°Êó∂Ë°®Áé∞‰∏ç‰Ω≥Ôºå‰ΩÜÈÄöËøá‰∏ÄÁßçÂèóÂ∏ÇÂú∫ÂêØÂèëÁöÑÊ°ÜÊû∂ÂèØ‰ª•ÊúâÊïàÂçèË∞ÉÂÆÉ‰ª¨Ôºå‰ªéËÄåÈôç‰ΩéÊàêÊú¨Âπ∂ÊèêÈ´òÊÄßËÉΩ„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Á≠ñÁï•ÊãçÂçñÁöÑÂ∑•‰ΩúË¥üËΩΩÊïàÁéáÊ°ÜÊû∂ÔºåÂÖÅËÆ∏‰ª£ÁêÜÈÄöËøáÁü≠ÊúüÊàòÁï•ËÆ°ÂàíËøõË°åÁ´ûÊ†áÔºåÂπ∂ÈÄöËøáÁ≥ªÁªüÁöÑÊàêÊú¨-‰ª∑ÂÄºÊú∫Âà∂ËøõË°åËØÑÂàÜÂíå‰ºòÂåñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåËØ•Ê°ÜÊû∂Âú®Ê∑±Â∫¶ÊêúÁ¥¢ÂíåÁºñÁ†Å‰ªªÂä°‰∏≠ÊòæËëóÂáèÂ∞ë‰∫ÜÂØπÂ§ßÂûã‰ª£ÁêÜÁöÑ‰æùËµñÔºåÂπ∂Èôç‰Ωé‰∫ÜÊï¥‰ΩìÊàêÊú¨ÔºåÂêåÊó∂Âú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÊúÄÂ§ßÁöÑ‰ª£ÁêÜ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂ∞ΩÁÆ°Â∞èÂûã‰ª£ÁêÜÂú®Â§çÊùÇÂ∑•‰ΩúË¥üËΩΩ‰∏≠ÂèØËÉΩ‰∏çË∂≥Ôºå‰ΩÜÈÄöËøáÂçèË∞É‰ªªÂä°ÂàÜÈÖçÂíåËá™ÊàëÊîπËøõÔºåÂèØ‰ª•ÊúâÊïàÊèêÂçáÂÖ∂ÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01212', 'title': 'SimpleGPT: Improving GPT via A Simple Normalization Strategy', 'url': 'https://huggingface.co/papers/2602.01212', 'abstract': 'SimpleNorm normalization strategy stabilizes activation scales and enables larger stable learning rates in Transformer models by reducing Hessian spectral norm, leading to improved training performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3times-10times larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.', 'score': 2, 'issue_id': 896, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': 'caf81558dc906d12', 'authors': ['Marco Chen', 'Xianbiao Qi', 'Yelin He', 'Jiaquan Ye', 'Rong Xiao'], 'affiliations': ['Intellifusion Inc.', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01212.jpg', 'data': {'categories': ['#training', '#open_source', '#architecture', '#optimization', '#math'], 'emoji': 'üìà', 'ru': {'title': '–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –ø—É—Ç—å –∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ–º—É –∏ –±—ã—Å—Ç—Ä–æ–º—É –æ–±—É—á–µ–Ω–∏—é –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –≤–≤–æ–¥–∏—Ç SimpleNorm ‚Äî –ø—Ä–æ—Å—Ç—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –º–∞—Å—à—Ç–∞–±—ã –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ Transformer –º–æ–¥–µ–ª—è—Ö. –ê–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∞–Ω–∞–ª–∏–∑ –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ SimpleNorm —Å–Ω–∏–∂–∞–µ—Ç —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—É—é –Ω–æ—Ä–º—É –º–∞—Ç—Ä–∏—Ü—ã –ì–µ—Å—Å–∏–∞–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –±–æ–ª—å—à–∏—Ö GPT –º–æ–¥–µ–ª—è—Ö (–æ—Ç 1B –¥–æ 8B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ SimpleGPT —Å—Ç–∞–±–∏–ª–µ–Ω –ø—Ä–∏ —Å–∫–æ—Ä–æ—Å—Ç—è—Ö –æ–±—É—á–µ–Ω–∏—è –≤ 3-10 —Ä–∞–∑ –≤—ã—à–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏, —á–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã, —Å–Ω–∏–∂–∞—è —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –Ω–∞ 0.08 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å LLaMA2 –ø—Ä–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ 7B –º–æ–¥–µ–ª–∏.'}, 'en': {'title': 'Unlocking Higher Learning Rates with SimpleNorm', 'desc': "This paper introduces a new normalization technique called SimpleNorm, which helps stabilize the activation scales in Transformer models. By focusing on the Hessian matrix's spectral norm, SimpleNorm allows for larger and more stable learning rates during training. The authors demonstrate that their method leads to improved optimization stability and better performance on large-scale models. Extensive experiments show that models using SimpleNorm can tolerate learning rates that are 3 to 10 times higher than traditional methods, resulting in lower training loss compared to existing models."}, 'zh': {'title': 'SimpleNormÔºöÊèêÂçáTransformerÂ≠¶‰π†ÁéáÁöÑÁ®≥ÂÆöÊÄß', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂΩí‰∏ÄÂåñÁ≠ñÁï•ÔºåÁß∞‰∏∫SimpleNormÔºåÊó®Âú®Á®≥ÂÆöTransformerÊ®°Âûã‰∏≠ÁöÑÊøÄÊ¥ªÂ∞∫Â∫¶„ÄÇÈÄöËøáÂàÜÊûêÊçüÂ§±ÂáΩÊï∞ÁöÑHessianÁü©ÈòµÔºåÁ†îÁ©∂Ë°®ÊòéSimpleNormÊòæËëóÈôç‰Ωé‰∫ÜHessianÁöÑË∞±ËåÉÊï∞Ôºå‰ªéËÄåÂÖÅËÆ∏‰ΩøÁî®Êõ¥Â§ßÁöÑÁ®≥ÂÆöÂ≠¶‰π†Áéá„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÂü∫‰∫éSimpleNormÁöÑSimpleGPTÁΩëÁªúÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ËÉΩÂ§üÊâøÂèóÊØî‰º†ÁªüÊñπÊ≥ïÈ´òÂá∫3Âà∞10ÂÄçÁöÑÂ≠¶‰π†ÁéáÔºåÂπ∂‰∏î‰ºòÂåñÁ®≥ÂÆöÊÄßÊòæËëóÂ¢ûÂº∫„ÄÇÊúÄÁªàÔºåSimpleGPTÂú®ËÆ≠ÁªÉ7BËßÑÊ®°Ê®°ÂûãÊó∂ÔºåÊçüÂ§±ÂÄºÊØîLLaMA2‰Ωé0.08ÔºåË°®Áé∞Âá∫Êõ¥‰ºòÁöÑËÆ≠ÁªÉÊÄßËÉΩ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03320', 'title': 'MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning', 'url': 'https://huggingface.co/papers/2602.03320', 'abstract': 'MedSAM-Agent reformulates medical image segmentation as a multi-step decision-making process using hybrid prompting and a two-stage training pipeline with process rewards to improve autonomous reasoning and optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available https://github.com/CUHK-AIM-Group/MedSAM-Agent{here}.', 'score': 1, 'issue_id': 901, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '51c688c84d1de0db', 'authors': ['Shengyuan Liu', 'Liuxin Bao', 'Qi Yang', 'Wanting Geng', 'Boyun Zheng', 'Chenxin Li', 'Wenting Chen', 'Houwen Peng', 'Yixuan Yuan'], 'affiliations': ['Chinese University of Hong Kong, Hong Kong SAR, China', 'Dalian University of Technology, Dalian, China', 'Hunyuan Group, Tencent', 'Institute of Automation, the Chinese Academy of Sciences, Beijing, China', 'Stanford University, Stanford, USA'], 'pdf_title_img': 'assets/pdf/title_img/2602.03320.jpg', 'data': {'categories': ['#optimization', '#healthcare', '#rl', '#cv', '#rlhf', '#science', '#training', '#agents', '#reasoning', '#open_source'], 'emoji': 'üè•', 'ru': {'title': '–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π', 'desc': 'MedSAM-Agent –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–∞–∫ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞. –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∫–∞–∫ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã–º–∏ –Ω–∞–≥—Ä–∞–¥–∞–º–∏ –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ 21 –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ 6 –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.'}, 'en': {'title': 'Revolutionizing Medical Image Segmentation with Autonomous Decision-Making', 'desc': "MedSAM-Agent transforms medical image segmentation into a multi-step decision-making process, enhancing the model's ability to reason and optimize autonomously. It utilizes a hybrid prompting strategy to generate expert-curated trajectories, allowing the model to learn human-like decision-making and refinement techniques. The framework features a two-stage training pipeline that combines multi-turn outcome verification with a clinical-fidelity process reward system, improving interaction efficiency and reducing redundant actions. Extensive testing across various medical modalities shows that MedSAM-Agent achieves leading performance, effectively integrating autonomous reasoning with iterative optimization."}, 'zh': {'title': 'MedSAM-AgentÔºöÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤ÁöÑÊñ∞ÊÄùË∑Ø', 'desc': 'MedSAM-AgentÂ∞ÜÂåªÂ≠¶ÂõæÂÉèÂàÜÂâ≤ÈáçÊñ∞ÂÆö‰πâ‰∏∫‰∏Ä‰∏™Â§öÊ≠•È™§ÁöÑÂÜ≥Á≠ñËøáÁ®ãÔºåÂà©Áî®Ê∑∑ÂêàÊèêÁ§∫Âíå‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁÆ°ÈÅìÊù•ÊèêÈ´òËá™‰∏ªÊé®ÁêÜÂíå‰ºòÂåñËÉΩÂäõ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂºïÂÖ•‰∏ìÂÆ∂Á≠ñÂàíÁöÑËΩ®ËøπÁîüÊàêÁ≠ñÁï•Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂÜÖÂåñÁ±ª‰ºº‰∫∫Á±ªÁöÑÂÜ≥Á≠ñÂêØÂèëÂºèÂíåËá™ÈÄÇÂ∫î‰ºòÂåñÁ≠ñÁï•„ÄÇÂÆÉËøòÁªìÂêà‰∫ÜÂ§öËΩÆ„ÄÅÁ´ØÂà∞Á´ØÁöÑÁªìÊûúÈ™åËØÅ‰∏é‰∏¥Â∫ä‰øùÁúüÂ∫¶ÁöÑËøáÁ®ãÂ•ñÂä±ËÆæËÆ°Ôºå‰ª•‰øÉËøõ‰∫§‰∫íÁöÑÁÆÄÁ∫¶ÊÄßÂíåÂÜ≥Á≠ñÊïàÁéá„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåMedSAM-AgentÂú®ÂÖ≠ÁßçÂåªÂ≠¶Ê®°ÊÄÅÂíå21‰∏™Êï∞ÊçÆÈõÜ‰∏äÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩÔºåÊúâÊïàÂú∞Â∞ÜËá™‰∏ªÂåªÂ≠¶Êé®ÁêÜ‰∏éÂº∫Â§ßÁöÑËø≠‰ª£‰ºòÂåñÁõ∏ÁªìÂêà„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03238', 'title': 'The Necessity of a Unified Framework for LLM-Based Agent Evaluation', 'url': 'https://huggingface.co/papers/2602.03238', 'abstract': 'Large Language Models have advanced general-purpose agents, but current evaluation benchmarks suffer from confounding factors and lack of standardization, necessitating a unified framework for rigorous assessment.  \t\t\t\t\tAI-generated summary \t\t\t\t With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.', 'score': 1, 'issue_id': 895, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '1a9ff02f62d1b11b', 'authors': ['Pengyu Zhu', 'Li Sun', 'Philip S. Yu', 'Sen Su'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'Chongqing University of Posts and Telecommunications', 'University of Illinois Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2602.03238.jpg', 'data': {'categories': ['#agents', '#reasoning', '#benchmark'], 'emoji': '‚öñÔ∏è', 'ru': {'title': '–ï–¥–∏–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —á–µ—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM', 'desc': '–í —Ä–∞–±–æ—Ç–µ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º—Ç—ã, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –¥–∏–Ω–∞–º–∏–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π. –¢–µ–∫—É—â–∏–µ –º–µ—Ç–æ–¥–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã–µ, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –∫ –∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥—É –ø—Ä–æ–º—Ç–æ–≤ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–æ–∑–¥–∞—Ç—å —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM.'}, 'en': {'title': 'Standardizing Evaluation for Fair AI Agents', 'desc': 'This paper discusses the challenges in evaluating Large Language Models (LLMs) used as general-purpose agents. It highlights that current benchmarks are affected by various confounding factors, such as different prompts and tool configurations, which complicate the assessment of model performance. The authors argue that the lack of a standardized evaluation framework leads to unfairness and non-reproducible results in the field. To address these issues, they propose a unified framework for rigorous and consistent evaluation of agent performance.'}, 'zh': {'title': 'Áªü‰∏ÄËØÑ‰º∞Ê°ÜÊû∂ÔºåÊé®Âä®Êô∫ËÉΩ‰ΩìËØÑ‰º∞ÁöÑ‰∏•Ê†ºËøõÂ±ï', 'desc': 'Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊé®Âä®‰∫ÜÈÄöÁî®Êô∫ËÉΩ‰ΩìÁöÑËøõÊ≠•Ôºå‰ΩÜÂΩìÂâçÁöÑËØÑ‰º∞Âü∫ÂáÜÂ≠òÂú®Ê∑∑Ê∑ÜÂõ†Á¥†ÂíåÁº∫‰πèÊ†áÂáÜÂåñÁöÑÈóÆÈ¢ò„ÄÇËøô‰∫õËØÑ‰º∞Èù¢‰∏¥ÁöÑÊåëÊàò‰∏éÈùôÊÄÅÈóÆÁ≠îÂü∫ÂáÜ‰∏çÂêåÔºå‰∏ªË¶ÅÂèóÂà∞Á≥ªÁªüÊèêÁ§∫„ÄÅÂ∑•ÂÖ∑ÈÖçÁΩÆÂíåÁéØÂ¢ÉÂä®ÊÄÅÁ≠âÂ§ñÈÉ®Âõ†Á¥†ÁöÑÂΩ±Âìç„ÄÇÁé∞ÊúâÁöÑËØÑ‰º∞ÊñπÊ≥ïÂæÄÂæÄ‰æùËµñ‰∫éÂàÜÊï£ÁöÑ„ÄÅÁ†îÁ©∂ËÄÖÁâπÂÆöÁöÑÊ°ÜÊû∂ÔºåÂØºËá¥Èöæ‰ª•Â∞ÜÊÄßËÉΩÊèêÂçáÂΩíÂõ†‰∫éÊ®°ÂûãÊú¨Ë∫´„ÄÇÊ≠§Â§ñÔºåÁº∫‰πèÊ†áÂáÜÂåñÁöÑÁéØÂ¢ÉÊï∞ÊçÆ‰ΩøÂæóÈîôËØØÈöæ‰ª•ËøΩË∏™ÔºåÁªìÊûú‰πüÈöæ‰ª•ÈáçÂ§ç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02494', 'title': 'MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training', 'url': 'https://huggingface.co/papers/2602.02494', 'abstract': 'MEG-XL demonstrates improved brain-to-text decoding performance through extended pre-training with 2.5-minute MEG context, significantly outperforming previous models with less contextual data.  \t\t\t\t\tAI-generated summary \t\t\t\t Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .', 'score': 1, 'issue_id': 901, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '5fd183a0041f2728', 'authors': ['Dulhan Jayalath', 'Oiwi Parker Jones'], 'affiliations': ['PNPL, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2602.02494.jpg', 'data': {'categories': ['#science', '#transfer_learning', '#long_context', '#open_source'], 'emoji': 'üß†', 'ru': {'title': '–î–æ–ª–≥–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî –∫–ª—é—á –∫ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é –º–æ–∑–≥–∞', 'desc': 'MEG-XL ‚Äî —ç—Ç–æ –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ —Å–∏–≥–Ω–∞–ª–æ–≤ –º–æ–∑–≥–∞, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤ 2.5 –º–∏–Ω—É—Ç—ã —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω—Ü–µ—Ñ–∞–ª–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –≤ 5-300 —Ä–∞–∑ –±–æ–ª—å—à–µ, —á–µ–º –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö. –ë–ª–∞–≥–æ–¥–∞—Ä—è –¥–æ–ª–≥–æ–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª—å –ª—É—á—à–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–æ–∑–≥–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –∑–∞–¥–∞—á–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–ª–æ–≤ –∏–∑ —Å–∏–≥–Ω–∞–ª–æ–≤ –º–æ–∑–≥–∞ MEG-XL —Ç—Ä–µ–±—É–µ—Ç –≤—Å–µ–≥–æ 1 —á–∞—Å –æ–±—É—á–∞—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤–º–µ—Å—Ç–æ 50 —á–∞—Å–æ–≤, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–º –º–µ—Ç–æ–¥–∞–º. –î–æ–ª–≥–æ–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª—å—é, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—Å—è –Ω–∞ —Ü–µ–ª–µ–≤—É—é –∑–∞–¥–∞—á—É –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤–∞–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –Ω–µ–≤—Ä–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –º–æ–∑–≥-–∫–æ–º–ø—å—é—Ç–µ—Ä.'}, 'en': {'title': 'Unlocking Speech with Extended Neural Context', 'desc': "MEG-XL is a machine learning model that enhances brain-to-text decoding by using a longer context of 2.5 minutes of MEG data for pre-training. This approach allows the model to learn better statistical patterns across different subjects, which is crucial for decoding natural speech that occurs over extended periods. By fine-tuning on word decoding tasks, MEG-XL achieves performance comparable to models trained on much larger datasets, demonstrating its efficiency. The findings suggest that utilizing longer contexts during pre-training significantly improves the model's ability to generalize and transfer knowledge to specific tasks."}, 'zh': {'title': 'Èïø‰∏ä‰∏ãÊñáÈ¢ÑËÆ≠ÁªÉÔºåÊèêÂçáËÑë-ÊñáÊú¨Ëß£Á†ÅÊÄßËÉΩ', 'desc': 'MEG-XLÊòØ‰∏ÄÁßçÊîπËøõÁöÑËÑë-ÊñáÊú¨Ëß£Á†ÅÊ®°ÂûãÔºåÈÄöËøá‰ΩøÁî®2.5ÂàÜÈíüÁöÑMEG‰∏ä‰∏ãÊñáËøõË°åÊâ©Â±ïÈ¢ÑËÆ≠ÁªÉÔºåÊòæËëóÊèêÈ´ò‰∫ÜËß£Á†ÅÊÄßËÉΩ„ÄÇ‰∏é‰πãÂâçÂè™‰ΩøÁî®Âá†Áßí‰∏ä‰∏ãÊñáÁöÑÊ®°ÂûãÁõ∏ÊØîÔºåMEG-XLËÉΩÂ§üÊçïÊçâÊõ¥ÈïøÁöÑÁ•ûÁªè‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÊèêÈ´òÊï∞ÊçÆÊïàÁéáÂíåÊ≥õÂåñËÉΩÂäõ„ÄÇËØ•Ê®°ÂûãÂú®ËÑëÊï∞ÊçÆÁöÑÂçïËØçËß£Á†Å‰ªªÂä°‰∏äÔºå‰ΩøÁî®Êõ¥Â∞ëÁöÑÊï∞ÊçÆÔºà‰æãÂ¶Ç1Â∞èÊó∂ÂØπÊØî50Â∞èÊó∂ÔºâËææÂà∞‰∫Ü‰∏éÁõëÁù£Â≠¶‰π†Áõ∏ÂΩìÁöÑÊÄßËÉΩ„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÈïø‰∏ä‰∏ãÊñáÁöÑÈ¢ÑËÆ≠ÁªÉÊúâÂä©‰∫éÊõ¥Â•ΩÂú∞Âà©Áî®Á•ûÁªè‰∏ä‰∏ãÊñáÔºå‰ªéËÄåÊèêÂçáËß£Á†ÅÊïàÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02405', 'title': 'Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning', 'url': 'https://huggingface.co/papers/2602.02405', 'abstract': "Distribution Aligned Imitation Learning (DAIL) improves LLM reasoning by transforming expert solutions into in-distribution traces and using contrastive learning to focus on expert methodologies, achieving significant performance gains with minimal expert data.  \t\t\t\t\tAI-generated summary \t\t\t\t Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.", 'score': 1, 'issue_id': 903, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': 'cb88331c5eda8f68', 'authors': ['Ethan Mendes', 'Jungsoo Park', 'Alan Ritter'], 'affiliations': ['Georgia Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2602.02405.jpg', 'data': {'categories': ['#rlhf', '#training', '#optimization', '#reasoning'], 'emoji': 'üéØ', 'ru': {'title': '–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö', 'desc': '–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥ Distribution Aligned Imitation Learning (DAIL), –∫–æ—Ç–æ—Ä—ã–π —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –≤ –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ç—Ä–∞—Å—Å—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏, –ø–æ—Å–∫–æ–ª—å–∫—É –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–µ—è–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —á–∏—Ç–∞—Ç–µ–ª–µ–π. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è—Ö –∏ –∏–Ω—Å–∞–π—Ç–∞—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ DAIL –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (10-25% –ø—Ä–∏—Ä–æ—Å—Ç–∞), –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ 2-4 —Ä–∞–∑–∞ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –æ–±–æ–±—â–µ–Ω–∏–µ –Ω–∞ –∑–∞–¥–∞—á–∏ –∏–∑ –¥—Ä—É–≥–∏—Ö –¥–æ–º–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ–Ω–µ–µ 1000 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤.'}, 'en': {'title': 'Bridging the Gap: Expert Insights for Better AI Reasoning', 'desc': 'Distribution Aligned Imitation Learning (DAIL) enhances the reasoning abilities of large language models (LLMs) by converting expert solutions into in-distribution reasoning traces. This method uses contrastive learning to emphasize expert techniques, allowing the model to learn effectively from limited expert data. DAIL addresses the challenge of traditional imitation learning, which often fails due to the out-of-distribution nature of expert solutions. The approach demonstrates significant performance improvements, achieving up to 25% gains in model accuracy while improving reasoning efficiency and enabling better generalization to new tasks.'}, 'zh': {'title': 'ÂàÜÂ∏ÉÂØπÈΩêÔºåÊèêÂçáÊé®ÁêÜËÉΩÂäõÔºÅ', 'desc': 'ÂàÜÂ∏ÉÂØπÈΩêÊ®°‰ªøÂ≠¶‰π†ÔºàDAILÔºâÈÄöËøáÂ∞Ü‰∏ìÂÆ∂Ëß£ÂÜ≥ÊñπÊ°àËΩ¨Âåñ‰∏∫Á¨¶ÂêàÂàÜÂ∏ÉÁöÑÊé®ÁêÜËΩ®ËøπÔºåÊù•ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇËØ•ÊñπÊ≥ïÈááÁî®ÂØπÊØîÂ≠¶‰π†Ôºå‰∏ìÊ≥®‰∫é‰∏ìÂÆ∂ÁöÑÊñπÊ≥ïËÆ∫Ôºå‰ªéËÄåÂú®‰ΩøÁî®ÊúÄÂ∞ëÁöÑ‰∏ìÂÆ∂Êï∞ÊçÆÊó∂ÂÆûÁé∞ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇDAILÁöÑ‰∏§Ê≠•Ê≥ïÈ¶ñÂÖàÂ∞Ü‰∏ìÂÆ∂Ëß£ÂÜ≥ÊñπÊ°àËΩ¨Âåñ‰∏∫ËØ¶ÁªÜÁöÑÊé®ÁêÜËΩ®ËøπÔºåÁÑ∂ÂêéÂ∫îÁî®ÂØπÊØîÁõÆÊ†áÊù•Âº∫ÂåñÂ≠¶‰π†„ÄÇÂÆûÈ™åË°®ÊòéÔºåDAILËÉΩÂ§üÂà©Áî®Â∞ë‰∫é1000‰∏™È´òË¥®ÈáèÁöÑ‰∏ìÂÆ∂Ëß£ÂÜ≥ÊñπÊ°àÔºåÂú®Qwen2.5-InstructÂíåQwen3Ê®°Âûã‰∏äÂÆûÁé∞10-25%ÁöÑÊÄßËÉΩÊèêÂçáÔºåÂπ∂ÊèêÈ´òÊé®ÁêÜÊïàÁéá2Âà∞4ÂÄç„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.02220', 'title': 'LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation', 'url': 'https://huggingface.co/papers/2602.02220', 'abstract': 'HieraNav presents a multi-granularity, open-vocabulary navigation task with LangMap benchmark that enables agents to follow natural language instructions across different semantic levels in 3D environments.  \t\t\t\t\tAI-generated summary \t\t\t\t The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap', 'score': 1, 'issue_id': 901, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '1528f5b2c41137b4', 'authors': ['Bo Miao', 'Weijia Liu', 'Jun Luo', 'Lachlan Shinnick', 'Jian Liu', 'Thomas Hamilton-Smith', 'Yuhe Yang', 'Zijie Wu', 'Vanja Videnovic', 'Feras Dayoub', 'Anton van den Hengel'], 'affiliations': ['AIML, Adelaide University', 'Breaker Industries', 'East China Normal University', 'NERC-RVC, Hunan University', 'Singapore University of Technology and Design', 'University Western Australia'], 'pdf_title_img': 'assets/pdf/title_img/2602.02220.jpg', 'data': {'categories': ['#3d', '#multimodal', '#benchmark', '#dataset', '#agents', '#open_source'], 'emoji': 'üó∫Ô∏è', 'ru': {'title': '–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –ø–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É —è–∑—ã–∫—É –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö 3D-—Å—Ü–µ–Ω–∞—Ö', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∑–∞–¥–∞—á–∞ HieraNav –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ 3D-–æ–∫—Ä—É–∂–µ–Ω–∏—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –Ω–∞ —á–µ—Ç—ã—Ä—ë—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —É—Ä–æ–≤–Ω—è—Ö: —Å—Ü–µ–Ω–∞, –∫–æ–º–Ω–∞—Ç–∞, —Ä–µ–≥–∏–æ–Ω –∏ –æ—Ç–¥–µ–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç LangMap –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã—Ö —Å–∫–∞–Ω–æ–≤ –ø–æ–º–µ—â–µ–Ω–∏–π —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –æ–ø–∏—Å–∞–Ω–∏—è 414 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –±–æ–ª–µ–µ 18K –∑–∞–¥–∞—á –Ω–∞–≤–∏–≥–∞—Ü–∏–∏. –î–∞—Ç–∞—Å–µ—Ç –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞–º–∏ –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç–∏–ª—è—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –æ—Å—Ç–∞—é—Ç—Å—è –ø—Ä–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∫ –º–∞–ª—ã–º –æ–±—ä–µ–∫—Ç–∞–º, –¥–∞–ª—å–Ω–∏–º —Ü–µ–ª—è–º –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤—ã—Ö –∑–∞–¥–∞—á.'}, 'en': {'title': 'Navigating Language: HieraNav and LangMap for AI Agents', 'desc': "HieraNav introduces a new navigation task that allows AI agents to understand and follow natural language instructions at different levels of detail in 3D spaces. The LangMap benchmark provides a rich dataset with various semantic levels, including scene, room, region, and instance, enabling comprehensive evaluation of navigation tasks. It features high-quality annotations for over 414 object categories and includes more than 18,000 tasks, enhancing the agents' ability to interpret instructions. The study shows that while context and memory improve navigation success, challenges remain with complex goals and multi-goal scenarios."}, 'zh': {'title': 'HieraNavÔºöÂ§öÁ≤íÂ∫¶Ëá™ÁÑ∂ËØ≠Ë®ÄÂØºËà™ÁöÑÊú™Êù•', 'desc': 'HieraNavÊòØ‰∏Ä‰∏™Â§öÁ≤íÂ∫¶„ÄÅÂºÄÊîæËØçÊ±áÁöÑÂØºËà™‰ªªÂä°ÔºåÊó®Âú®ËÆ©Êô∫ËÉΩ‰ΩìÊ†πÊçÆËá™ÁÑ∂ËØ≠Ë®ÄÊåá‰ª§Âú®3DÁéØÂ¢É‰∏≠ËøõË°åÂØºËà™„ÄÇËØ•‰ªªÂä°ÈÄöËøáLangMapÂü∫ÂáÜÊµãËØïÂÆûÁé∞ÔºåÊ∂µÁõñ‰∫ÜÂú∫ÊôØ„ÄÅÊàøÈó¥„ÄÅÂå∫ÂüüÂíåÂÆû‰æãÂõõ‰∏™ËØ≠‰πâÂ±ÇÊ¨°„ÄÇLangMapÊòØ‰∏Ä‰∏™Â§ßËßÑÊ®°Âü∫ÂáÜÔºåÂü∫‰∫éÁúüÂÆûÁöÑ3DÂÆ§ÂÜÖÊâ´ÊèèÔºåÊèê‰æõ‰∫Ü‰∏∞ÂØåÁöÑ‰∫∫Á±ªÈ™åËØÅÊ≥®ÈáäÂíå‰ªªÂä°„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºå‰∏∞ÂØåÁöÑ‰∏ä‰∏ãÊñáÂíåËÆ∞ÂøÜÂèØ‰ª•ÊèêÈ´òÂØºËà™ÊàêÂäüÁéáÔºå‰ΩÜÂú®Â§ÑÁêÜÈïøÂ∞æ„ÄÅÂ∞èÂûã„ÄÅ‰æùËµñ‰∏ä‰∏ãÊñáÂíåËøúÁ®ãÁõÆÊ†áÊó∂‰ªçÁÑ∂Èù¢‰∏¥ÊåëÊàò„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01405', 'title': 'Feedback by Design: Understanding and Overcoming User Feedback Barriers in Conversational Agents', 'url': 'https://huggingface.co/papers/2602.01405', 'abstract': "High-quality feedback is essential for effective human-AI interaction. It bridges knowledge gaps, corrects digressions, and shapes system behavior; both during interaction and throughout model development. Yet despite its importance, human feedback to AI is often infrequent and low quality. This gap motivates a critical examination of human feedback during interactions with AIs. To understand and overcome the challenges preventing users from giving high-quality feedback, we conducted two studies examining feedback dynamics between humans and conversational agents (CAs). Our formative study, through the lens of Grice's maxims, identified four Feedback Barriers -- Common Ground, Verifiability, Communication, and Informativeness -- that prevent high-quality feedback by users. Building on these findings, we derive three design desiderata and show that systems incorporating scaffolds aligned with these desiderata enabled users to provide higher-quality feedback. Finally, we detail a call for action to the broader AI community for advances in Large Language Models capabilities to overcome Feedback Barriers.", 'score': 1, 'issue_id': 905, 'pub_date': '2026-02-01', 'pub_date_card': {'ru': '1 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 1', 'zh': '2Êúà1Êó•'}, 'hash': '149d6974be7e2108', 'authors': ['Nikhil Sharma', 'Zheng Zhang', 'Daniel Lee', 'Namita Krishnan', 'Guang-Jie Ren', 'Ziang Xiao', 'Yunyao Li'], 'affiliations': ['Adobe Inc.', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2602.01405.jpg', 'data': {'categories': [], 'emoji': 'üîÑ', 'ru': {'title': '–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –±–∞—Ä—å–µ—Ä–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤ —á–µ–ª–æ–≤–µ–∫–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ —Å AI', 'desc': '–í —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫ —Å–∏—Å—Ç–µ–º–∞–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –¥–≤–∞ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ –≤—ã—è–≤–∏–ª–∏ —á–µ—Ç—ã—Ä–µ –±–∞—Ä—å–µ—Ä–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö –ì—Ä–∞–π—Å–∞: –æ–±—â–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –ø—Ä–æ–≤–µ—Ä—è–µ–º–æ—Å—Ç—å, –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã. –†–∞–±–æ—Ç–∞ –ø—Ä–∏–∑—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ—Å—Ç–≤–æ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –±–∞—Ä—å–µ—Ä–æ–≤ –≤ —Å–∏—Å—Ç–µ–º—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —á–µ–ª–æ–≤–µ–∫–æ–º.'}, 'en': {'title': 'Bridging the Feedback Gap for Better AI Interactions', 'desc': 'This paper explores the importance of high-quality human feedback in improving interactions with AI systems, particularly conversational agents. It identifies four key barriers that hinder users from providing effective feedback: Common Ground, Verifiability, Communication, and Informativeness. The authors conducted studies to understand these barriers and proposed design principles to help AI systems facilitate better feedback from users. They advocate for advancements in Large Language Models to address these challenges and enhance the overall quality of human-AI interactions.'}, 'zh': {'title': 'ÊèêÂçá‰∫∫Êú∫‰∫§‰∫í‰∏≠ÁöÑÂèçÈ¶àË¥®Èáè', 'desc': 'È´òË¥®ÈáèÁöÑÂèçÈ¶àÂØπ‰∫∫Êú∫‰∫§‰∫íËá≥ÂÖ≥ÈáçË¶ÅÔºåÂÆÉÂèØ‰ª•Âº•Ë°•Áü•ËØÜÂ∑ÆË∑ù„ÄÅÁ∫†Ê≠£ÂÅèÂ∑ÆÂπ∂Â°ëÈÄ†Á≥ªÁªüË°å‰∏∫„ÄÇÂ∞ΩÁÆ°ÂèçÈ¶àÁöÑÈáçË¶ÅÊÄßÊòæËÄåÊòìËßÅÔºå‰ΩÜÁî®Êà∑ÂØπ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèçÈ¶àÂæÄÂæÄ‰∏çÂ§üÈ¢ëÁπÅ‰∏îË¥®ÈáèËæÉ‰Ωé„ÄÇÊú¨ÊñáÈÄöËøá‰∏§È°πÁ†îÁ©∂Êé¢ËÆ®‰∫Ü‰∫∫Á±ª‰∏éÂØπËØù‰ª£ÁêÜ‰πãÈó¥ÁöÑÂèçÈ¶àÂä®ÊÄÅÔºåËØÜÂà´Âá∫Âõõ‰∏™ÂèçÈ¶àÈöúÁ¢çÔºöÂÖ±ÂêåÂü∫Á°Ä„ÄÅÂèØÈ™åËØÅÊÄß„ÄÅÊ≤üÈÄöÂíå‰ø°ÊÅØÈáè„ÄÇÂü∫‰∫éËøô‰∫õÂèëÁé∞ÔºåÊèêÂá∫‰∫Ü‰∏â‰∏™ËÆæËÆ°Ë¶ÅÊ±ÇÔºåÂπ∂Â±ïÁ§∫‰∫ÜÁ¨¶ÂêàËøô‰∫õË¶ÅÊ±ÇÁöÑÁ≥ªÁªüÂ¶Ç‰ΩïÂ∏ÆÂä©Áî®Êà∑Êèê‰æõÊõ¥È´òË¥®ÈáèÁöÑÂèçÈ¶à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00682', 'title': 'RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment', 'url': 'https://huggingface.co/papers/2602.00682', 'abstract': "A novel dual semantic alignment framework for LLM-enhanced multimodal recommendation that addresses representational divergence between large models and recommendation systems through graph attention networks and cross-modal contrastive learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.", 'score': 1, 'issue_id': 900, 'pub_date': '2026-01-31', 'pub_date_card': {'ru': '31 —è–Ω–≤–∞—Ä—è', 'en': 'January 31', 'zh': '1Êúà31Êó•'}, 'hash': '193acf3b8bfd59bf', 'authors': ['Yuecheng Li', 'Hengwei Ju', 'Zeyu Song', 'Wei Yang', 'Chi Lu', 'Peng Jiang', 'Kun Gai'], 'affiliations': ['Fudan University', 'Kuaishou Technology', 'Unaffiliated', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2602.00682.jpg', 'data': {'categories': ['#benchmark', '#multimodal'], 'emoji': 'üéØ', 'ru': {'title': '–ì–∞—Ä–º–æ–Ω–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —á–µ—Ä–µ–∑ –¥–≤–æ–π–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏–∫–∏', 'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ RecGOAT ‚Äî –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ LLM –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π ID-–ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ç–æ–≤–∞—Ä–æ–≤. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –¥–≤–æ–π–Ω–∞—è —Å—Ö–µ–º–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≥—Ä–∞—Ñ–æ–≤—ã–µ —Å–µ—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç—Ä—ë—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ –æ–Ω–ª–∞–π–Ω-—Ä–µ–∫–ª–∞–º—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∞.'}, 'en': {'title': 'Bridging the Gap: Enhancing Recommendations with Dual Semantic Alignment', 'desc': 'This paper introduces RecGOAT, a dual semantic alignment framework designed to improve multimodal recommendation systems by addressing the differences in representation between large language models (LLMs) and traditional recommendation systems. It utilizes graph attention networks to enhance collaborative semantics by modeling relationships among users and items, while also leveraging LLM representations. The framework employs cross-modal contrastive learning and optimal adaptive transport to achieve both instance-level and distribution-level semantic alignment. Experimental results demonstrate that RecGOAT outperforms existing methods, confirming its effectiveness in real-world recommendation scenarios.'}, 'zh': {'title': 'ÂèåËØ≠‰πâÂØπÈΩêÔºåÊèêÂçáÂ§öÊ®°ÊÄÅÊé®ËçêÊïàÊûú', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÂèåËØ≠‰πâÂØπÈΩêÊ°ÜÊû∂RecGOATÔºåÁî®‰∫éÂ¢ûÂº∫Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÂ§öÊ®°ÊÄÅÊé®ËçêÁ≥ªÁªü„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂõæÊ≥®ÊÑèÂäõÁΩëÁªúÂíåË∑®Ê®°ÊÄÅÂØπÊØîÂ≠¶‰π†ÔºåËß£ÂÜ≥‰∫ÜÂ§ßÊ®°Âûã‰∏éÊé®ËçêÁ≥ªÁªü‰πãÈó¥ÁöÑË°®Á§∫Â∑ÆÂºÇÈóÆÈ¢ò„ÄÇRecGOATÈÄöËøáÂª∫Ê®°Áî®Êà∑‰∏éÁâ©ÂìÅ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÔºå‰∏∞ÂØå‰∫ÜÂçè‰ΩúËØ≠‰πâÔºåÂπ∂ÂÆûÁé∞‰∫ÜÂÆû‰æãÁ∫ßÂíåÂàÜÂ∏ÉÁ∫ßÁöÑËØ≠‰πâÂØπÈΩê„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåRecGOATÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÈ™åËØÅ‰∫ÜÂÖ∂ÁêÜËÆ∫ÊúâÊïàÊÄßÂíåÂú®Â∑•‰∏öÊé®ËçêÂú∫ÊôØ‰∏≠ÁöÑÂèØÊâ©Â±ïÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.00398', 'title': 'MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers', 'url': 'https://huggingface.co/papers/2602.00398', 'abstract': 'MemoryLLM decouples feed-forward networks from self-attention in transformers, enabling context-free token-wise neural retrieval memory that improves inference efficiency through pre-computed lookups.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.', 'score': 1, 'issue_id': 905, 'pub_date': '2026-01-30', 'pub_date_card': {'ru': '30 —è–Ω–≤–∞—Ä—è', 'en': 'January 30', 'zh': '1Êúà30Êó•'}, 'hash': '980b87a1189170b8', 'authors': ['Ajay Jaiswal', 'Lauren Hannah', 'Han-Byul Kim', 'Duc Hoang', 'Arnav Kundu', 'Mehrdad Farajtabar', 'Minsik Cho'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2602.00398.jpg', 'data': {'categories': [], 'emoji': 'üíæ', 'ru': {'title': '–ü–∞–º—è—Ç—å –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: –æ—Ç–¥–µ–ª–µ–Ω–∏–µ FFN –æ—Ç –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç MemoryLLM ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–µ—Ç–∏ (FFN) –∏ –º–µ—Ö–∞–Ω–∏–∑–º —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö, –ø–æ–∑–≤–æ–ª—è—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å FFN –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º—É—é –ø–∞–º—è—Ç—å —Å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º –ø–æ–∏—Å–∫–æ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –ø–æ—Å–∫–æ–ª—å–∫—É –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑—É—á–∞—Ç—å, –∫–∞–∫ –≤—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–∞—é—Ç –¥–æ—Å—Ç—É–ø –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –ø–∞–º—è—Ç–∏ –≤ FFN. –ë–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º –ø–æ–∏—Å–∫–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–º (ToLs) –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç Flex-MemoryLLM ‚Äî –≥–∏–±—Ä–∏–¥–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è —É—Ä–∞–≤–Ω–æ–≤–µ—à–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω–æ–π MemoryLLM.'}, 'en': {'title': 'Decoupling Memory for Efficient Inference in Transformers', 'desc': 'MemoryLLM is a novel approach that separates feed-forward networks (FFNs) from self-attention mechanisms in transformers. This separation allows FFNs to function as context-free token-wise neural retrieval memory, which enhances the efficiency of inference through pre-computed lookups. The study explores how input tokens can access specific memory locations within FFN parameters, highlighting the significance of FFN memory for various downstream tasks. Additionally, the introduction of Flex-MemoryLLM offers a middle ground between traditional transformer designs and MemoryLLM, addressing performance issues related to context-free embeddings.'}, 'zh': {'title': 'Ëß£ËÄ¶ÂâçÈ¶àÁΩëÁªúÔºåÊèêÂçáÊé®ÁêÜÊïàÁéá', 'desc': 'MemoryLLMÈÄöËøáÂ∞ÜÂâçÈ¶àÁΩëÁªú‰∏éËá™Ê≥®ÊÑèÂäõËß£ËÄ¶ÔºåÊèê‰æõ‰∫Ü‰∏ÄÁßçÊó†‰∏ä‰∏ãÊñáÁöÑÂü∫‰∫é‰ª§ÁâåÁöÑÁ•ûÁªèÊ£ÄÁ¥¢ËÆ∞ÂøÜ„ÄÇËøôÁßçÊñπÊ≥ï‰ΩøÂæóÂâçÈ¶àÁΩëÁªúËÉΩÂ§üÁã¨Á´ã‰∫éËá™Ê≥®ÊÑèÂäõËøõË°åËÆ≠ÁªÉÔºå‰ªéËÄåÊèêÈ´òÊé®ÁêÜÊïàÁéá„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂâçÈ¶àÁΩëÁªúÂú®‰∏çÂêå‰∏ãÊ∏∏‰ªªÂä°‰∏≠ÁöÑËÆ∞ÂøÜÈáçË¶ÅÊÄßÔºå‰ª•ÂèäËæìÂÖ•‰ª§ÁâåÂ¶Ç‰ΩïËÆøÈóÆÂâçÈ¶àÁΩëÁªúÂèÇÊï∞‰∏≠ÁöÑËÆ∞ÂøÜ‰ΩçÁΩÆ„ÄÇFlex-MemoryLLMÊû∂ÊûÑÂàôÂú®‰º†ÁªüÂèòÊç¢Âô®ËÆæËÆ°‰∏éMemoryLLM‰πãÈó¥Êû∂Ëµ∑‰∫ÜÊ°•Ê¢ÅÔºåÁº©Â∞è‰∫ÜÂõ†‰ΩøÁî®Êó†‰∏ä‰∏ãÊñá‰ª§ÁâåÂµåÂÖ•ËÄåÈÄ†ÊàêÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.03817', 'title': 'Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion', 'url': 'https://huggingface.co/papers/2602.03817', 'abstract': 'A fusion framework called FINCH combines audio and spatiotemporal predictors for bioacoustic classification by adaptively weighting evidence based on reliability estimates, outperforming fixed-weight methods and audio-only approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce Fusion under INdependent Conditional Hypotheses (FINCH), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family contains the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md{anonymous-repository}}', 'score': 0, 'issue_id': 892, 'pub_date': '2026-02-03', 'pub_date_card': {'ru': '3 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 3', 'zh': '2Êúà3Êó•'}, 'hash': '7300014e4792982e', 'authors': ['Oscar Ovanger', 'Levi Harris', 'Timothy H. Keitt'], 'affiliations': ['1', '2'], 'pdf_title_img': 'assets/pdf/title_img/2602.03817.jpg', 'data': {'categories': ['#audio', '#multimodal', '#benchmark'], 'emoji': 'üê¶', 'ru': {'title': '–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –∑–≤—É–∫–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å—é', 'desc': '–ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç FINCH ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Å–ª–∏—è–Ω–∏—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–∏–æ–∞–∫—É—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ-–≤—Ä–µ–º–µ–Ω–Ω—ã–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª–µ–º. –ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –≤–µ–Ω—Ç–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏. –§—Ä–µ–π–º–≤–æ—Ä–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç log-–ª–∏–Ω–µ–π–Ω–æ–µ —Å–ª–∏—è–Ω–∏–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ –≤–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –ø—Ä–∏ —ç—Ç–æ–º –∞—É–¥–∏–æ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ—Å—Ç–∞—ë—Ç—Å—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ FINCH –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–µ—Ç–æ–¥—ã —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–µ—Å–æ–≤—ã–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –∏ —á–∏—Å—Ç—ã–µ –∞—É–¥–∏–æ–ø–æ–¥—Ö–æ–¥—ã, –¥–æ—Å—Ç–∏–≥–∞—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö CBI –∏ BirdSet.'}, 'en': {'title': 'Adaptive Fusion for Enhanced Bioacoustic Classification', 'desc': 'The paper presents FINCH, a novel framework for bioacoustic classification that combines audio data with spatiotemporal predictors. It adaptively weighs the evidence from these sources based on their reliability, improving upon traditional fixed-weight methods. By using a gating function that assesses the uncertainty and informativeness of contextual information, FINCH enhances the robustness of predictions. The framework not only outperforms audio-only classifiers but also provides a clear fallback option, making it interpretable and effective even when contextual data is weak.'}, 'zh': {'title': 'Ëá™ÈÄÇÂ∫îËØÅÊçÆËûçÂêàÔºåÊèêÂçáÁîüÁâ©Â£∞Â≠¶ÂàÜÁ±ªÊÄßËÉΩ', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫FINCHÁöÑËûçÂêàÊ°ÜÊû∂ÔºåÁî®‰∫éÁîüÁâ©Â£∞Â≠¶ÂàÜÁ±ª„ÄÇËØ•Ê°ÜÊû∂ÁªìÂêà‰∫ÜÈü≥È¢ëÂíåÊó∂Á©∫È¢ÑÊµãÂô®ÔºåÈÄöËøáÊ†πÊçÆÂèØÈù†ÊÄß‰º∞ËÆ°Ëá™ÈÄÇÂ∫îÂä†ÊùÉËØÅÊçÆÔºå‰ªéËÄåË∂ÖË∂ä‰∫ÜÂõ∫ÂÆöÊùÉÈáçÊñπÊ≥ïÂíå‰ªÖ‰ΩøÁî®Èü≥È¢ëÁöÑÊñπÊ≥ï„ÄÇFINCHËÉΩÂ§üÊ†πÊçÆ‰∏çÁ°ÆÂÆöÊÄßÂíå‰ø°ÊÅØÈáèÁªüËÆ°Êù•Â≠¶‰π†ÊØè‰∏™Ê†∑Êú¨ÁöÑÈó®ÊéßÂáΩÊï∞ÔºåËØÑ‰º∞‰∏ä‰∏ãÊñá‰ø°ÊÅØÁöÑÂèØÈù†ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåFINCHÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÂíåÈîôËØØÊùÉË°°„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2602.01519', 'title': 'You Need an Encoder for Native Position-Independent Caching', 'url': 'https://huggingface.co/papers/2602.01519', 'abstract': 'Native position-independent caching enhances LLM inference efficiency by reintroducing encoders and developing a caching system that reduces latency while maintaining accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3times with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.', 'score': 0, 'issue_id': 901, 'pub_date': '2026-02-02', 'pub_date_card': {'ru': '2 —Ñ–µ–≤—Ä–∞–ª—è', 'en': 'February 2', 'zh': '2Êúà2Êó•'}, 'hash': '70f393ac8a1a2604', 'authors': ['Shiju Zhao', 'Junhao Hu', 'Jiaqi Zheng', 'Guihai Chen'], 'affiliations': ['School of Computer Science, Peking University, China', 'State Key Laboratory for Novel Software Technology, Nanjing University, China'], 'pdf_title_img': 'assets/pdf/title_img/2602.01519.jpg', 'data': {'categories': [], 'emoji': '‚ö°', 'ru': {'title': '–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–í —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è (PIC) –¥–ª—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Key-Value –∫–µ—à –±–µ–∑ —É—á—ë—Ç–∞ –ø–æ–∑–∏—Ü–∏–π —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ê–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ–≤–æ–¥—è—Ç –¥–µ–∫–æ–¥–µ—Ä-only –º–æ–¥–µ–ª–∏ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å —è–≤–Ω—ã–º —ç–Ω–∫–æ–¥–µ—Ä–æ–º –∏ –æ–±—É—á–∞—é—Ç –µ–≥–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ PIC. –û–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º—É COMB, –∫–æ—Ç–æ—Ä–∞—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–∞ 51-94% –∏ —Ç—Ä—ë—Ö–∫—Ä–∞—Ç–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤.'}, 'en': {'title': 'Boosting LLM Efficiency with Native Position-Independent Caching', 'desc': 'This paper presents a novel approach to improve the efficiency of Large Language Models (LLMs) during inference by implementing native position-independent caching (PIC). The authors reintroduce encoders into decoder-only LLMs, allowing for key-value (KV) cache reuse without being limited by positional constraints. They develop a new caching system called COMB, which integrates with existing frameworks and significantly reduces latency while maintaining accuracy. Experimental results indicate that COMB can decrease Time-to-First-Token (TTFT) by up to 94% and triple throughput, demonstrating its effectiveness across various decoder-only LLMs.'}, 'zh': {'title': 'ÊèêÂçáLLMÊé®ÁêÜÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'Êú¨ËÆ∫ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊú¨Âú∞‰ΩçÁΩÆÊó†ÂÖ≥ÁºìÂ≠òÔºàPICÔºâÊñπÊ≥ïÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊé®ÁêÜÊïàÁéá„ÄÇÈÄöËøáÈáçÊñ∞ÂºïÂÖ•ÁºñÁ†ÅÂô®Âπ∂ÂºÄÂèë‰∏Ä‰∏™ÁºìÂ≠òÁ≥ªÁªüÔºåÂáèÂ∞ë‰∫ÜÂª∂ËøüÔºåÂêåÊó∂‰øùÊåÅ‰∫ÜÊ®°ÂûãÁöÑÂáÜÁ°ÆÊÄß„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éÂâçÁºÄÁöÑÈîÆÂÄºÁºìÂ≠òÊïàÁéá‰Ωé‰∏ãÔºåËÄåÊú¨Á†îÁ©∂ÁöÑCOMBÁ≥ªÁªüËÉΩÂ§üÂú®‰∏çÂèó‰ΩçÁΩÆÈôêÂà∂ÁöÑÊÉÖÂÜµ‰∏ãÈáçÁî®ÁºìÂ≠ò„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåCOMBÂú®‰øùÊåÅÁõ∏‰ººÂáÜÁ°ÆÂ∫¶ÁöÑÂêåÊó∂ÔºåÊòæËëóÈôç‰Ωé‰∫ÜÈ¶ñÊ¨°ÁîüÊàêÊó∂Èó¥ÔºåÂπ∂ÊèêÈ´ò‰∫ÜÂêûÂêêÈáè„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (7)', '#agents (52)', '#agi', '#alignment (19)', '#architecture (43)', '#audio (8)', '#benchmark (92)', '#cv (27)', '#data (12)', '#dataset (45)', '#diffusion (20)', '#ethics (2)', '#games (2)', '#graphs (2)', '#hallucinations (2)', '#healthcare (3)', '#inference (27)', '#interpretability (13)', '#leakage (2)', '#long_context (22)', '#low_resource (7)', '#machine_translation (2)', '#math (4)', '#multilingual (9)', '#multimodal (61)', '#open_source (59)', '#optimization (78)', '#plp (14)', '#rag (14)', '#reasoning (69)', '#rl (54)', '#rlhf (24)', '#robotics (6)', '#science (14)', '#security (5)', '#small_models (13)', '#story_generation', '#survey (2)', '#synthetic (13)', '#training (108)', '#transfer_learning (8)', '#video (27)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2026-02-06 19:35',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-02-06 19:35')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-02-06 19:35')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    