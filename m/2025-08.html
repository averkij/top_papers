
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 253 papers. August 2025.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #7a30efcf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: #7a30efcf;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #7a30ef17;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf monthly</h1></a>
            <p><span id="title-date">Август 2025</span> | <span id="title-articles-count">253 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/m/2025-07.html">⬅️ <span id="prev-date">07.2025</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/m/2025-09.html">➡️ <span id="next-date">09.2025</span></a></span>
            <span class="nav-item" id="nav-daily"><a href="https://hfday.ru">📈 <span id='top-day-label'>День</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': 'Август 2025', 'en': 'August 2025', 'zh': '8月2025年'};
        let feedDateNext = {'ru': '09.2025', 'en': '09/2025', 'zh': '9月2025年'};
        let feedDatePrev = {'ru': '07.2025', 'en': '07/2025', 'zh': '7月2025年'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf monthly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.05629', 'title': 'On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification', 'url': 'https://huggingface.co/papers/2508.05629', 'abstract': 'Dynamic Fine-Tuning (DFT) improves the generalization of Large Language Models (LLMs) by dynamically rescaling gradients, outperforming standard Supervised Fine-Tuning (SFT) and showing competitive results in offline reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT.', 'score': 100, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '50bf66dc29886a85', 'authors': ['Yongliang Wu', 'Yizhou Zhou', 'Zhou Ziheng', 'Yingzhe Peng', 'Xinyu Ye', 'Xinting Hu', 'Wenbo Zhu', 'Lu Qi', 'Ming-Hsuan Yang', 'Xu Yang'], 'affiliations': ['Independent Researcher', 'Nanyang Technological University', 'Shanghai Jiao Tong University', 'Southeast University', 'University of California, Berkeley', 'University of California, Los Angeles', 'University of California, Merced', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05629.jpg', 'data': {'categories': ['#optimization', '#rl', '#training'], 'emoji': '🚀', 'ru': {'title': 'Динамическая настройка: простой путь к лучшему обобщению языковых моделей', 'desc': 'Исследователи представили метод Динамической Тонкой Настройки (DFT) для улучшения обобщающей способности Больших Языковых Моделей (LLM). DFT динамически масштабирует градиенты, что позволяет преодолеть ограничения стандартной Контролируемой Тонкой Настройки (SFT). Эксперименты показали, что DFT превосходит SFT на различных сложных тестах и конкурентоспособен в задачах оффлайн обучения с подкреплением. Метод требует минимальных изменений в коде и основан на теоретическом анализе проблем SFT.'}, 'en': {'title': 'Dynamic Fine-Tuning: Elevating LLM Generalization with Smart Gradients', 'desc': 'This paper introduces Dynamic Fine-Tuning (DFT), a method that enhances the generalization of Large Language Models (LLMs) by adjusting gradient updates. The authors identify that traditional Supervised Fine-Tuning (SFT) can limit model performance due to its inherent reward structure. By dynamically rescaling the objective function based on token probabilities, DFT stabilizes the training process and leads to better outcomes on various benchmarks. The results indicate that DFT not only surpasses SFT but also performs competitively in offline reinforcement learning scenarios, making it a valuable advancement in model training techniques.'}, 'zh': {'title': '动态微调，提升模型泛化能力！', 'desc': '动态微调（DFT）通过动态调整梯度的缩放，提升了大型语言模型（LLM）的泛化能力。与标准的监督微调（SFT）相比，DFT在多个挑战性基准测试中表现更优，且在离线强化学习中也展现出竞争力。通过数学分析，我们发现标准SFT的梯度隐含了一个有问题的奖励结构，限制了模型的泛化能力。DFT通过根据每个token的概率动态调整目标函数，稳定了梯度更新，从而显著改善了模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2508.05004', 'title': 'R-Zero: Self-Evolving Reasoning LLM from Zero Data', 'url': 'https://huggingface.co/papers/2508.05004', 'abstract': 'R-Zero is a self-evolving framework that autonomously generates and learns from its own training data, improving reasoning capabilities in LLMs without human-curated tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.', 'score': 81, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '4e0838dc787e59cf', 'authors': ['Chengsong Huang', 'Wenhao Yu', 'Xiaoyang Wang', 'Hongming Zhang', 'Zongxia Li', 'Ruosen Li', 'Jiaxin Huang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent AI Seattle Lab', 'The University of Texas at Dallas', 'University of Maryland, College Park', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2508.05004.jpg', 'data': {'categories': ['#agents', '#rl', '#optimization', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Самообучающиеся ИИ: путь к сверхразуму без участия человека', 'desc': 'R-Zero - это самоэволюционирующая система, которая автономно генерирует обучающие данные для улучшения способностей к рассуждению у больших языковых моделей (LLM). Система состоит из двух моделей - Challenger и Solver, которые взаимодействуют друг с другом, создавая и решая все более сложные задачи. R-Zero не требует заранее подготовленных человеком заданий и меток, что позволяет преодолеть ограничения существующих методов обучения LLM. Эксперименты показали значительное улучшение способностей к рассуждению у различных базовых LLM после применения R-Zero.'}, 'en': {'title': 'Autonomous Learning for Super-Intelligent AI', 'desc': "R-Zero is an innovative framework that enables Large Language Models (LLMs) to autonomously create and learn from their own training data, eliminating the need for human-curated tasks. It consists of two models, a Challenger and a Solver, which interact and evolve together; the Challenger proposes tasks that push the Solver's limits, while the Solver learns to tackle these challenges. This self-improving process generates a focused curriculum that enhances reasoning capabilities without relying on pre-existing labels or tasks. Empirical results show that R-Zero significantly boosts the performance of various LLMs on reasoning benchmarks, demonstrating its potential to advance AI systems beyond human intelligence."}, 'zh': {'title': '自我进化的智能框架，超越人类智能的未来', 'desc': 'R-Zero是一个自我进化的框架，能够自主生成和学习自己的训练数据，从而提升大型语言模型（LLMs）的推理能力，而无需人工策划的任务。该框架通过初始化两个独立的模型——挑战者和解决者，来实现模型的共同进化。挑战者负责提出接近解决者能力边界的任务，而解决者则专注于解决这些日益复杂的任务。通过这种互动，R-Zero能够在没有预先存在的任务和标签的情况下，生成一个有针对性的自我提升课程，显著提高了不同基础LLMs的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2508.05635', 'title': 'Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation', 'url': 'https://huggingface.co/papers/2508.05635', 'abstract': 'Genie Envisioner integrates policy learning, evaluation, and simulation using a video diffusion model and neural simulator for instruction-driven robotic manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.', 'score': 62, 'issue_id': 5243, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'f4ca777a8500b711', 'authors': ['Yue Liao', 'Pengfei Zhou', 'Siyuan Huang', 'Donglin Yang', 'Shengcong Chen', 'Yuxin Jiang', 'Yue Hu', 'Jingbin Cai', 'Si Liu', 'Jianlan Luo', 'Liliang Chen', 'Shuicheng Yan', 'Maoqing Yao', 'Guanghui Ren'], 'affiliations': ['BUAA', 'LV-Lab', 'NUS', 'Unified World Foundation'], 'pdf_title_img': 'assets/pdf/title_img/2508.05635.jpg', 'data': {'categories': ['#video', '#benchmark', '#training', '#optimization', '#robotics', '#agi', '#open_source', '#agents'], 'emoji': '🤖', 'ru': {'title': 'Единая платформа для обучения роботов на основе видео-генерации', 'desc': 'Genie Envisioner (GE) представляет собой унифицированную платформу для роботизированных манипуляций, объединяющую обучение политик, оценку и симуляцию в рамках единой видео-генеративной структуры. В основе GE лежит крупномасштабная модель диффузии видео, обусловленная инструкциями, которая фиксирует пространственную, временную и семантическую динамику реальных роботизированных взаимодействий в структурированном латентном пространстве. GE-Act преобразует латентные представления в исполняемые траектории действий, а GE-Sim служит нейронным симулятором для создания высококачественных развертываний. Платформа также включает EWMBench - набор стандартизированных тестов для оценки визуальной точности, физической согласованности и соответствия инструкций действиям.'}, 'en': {'title': 'Unified Framework for Instruction-Driven Robotic Manipulation', 'desc': 'Genie Envisioner (GE) is a comprehensive platform designed for robotic manipulation that combines policy learning, evaluation, and simulation into one framework. It utilizes a video diffusion model to understand and generate realistic robotic interactions based on instructions. The system includes a decoder that translates learned representations into actionable movements, allowing robots to perform tasks with minimal guidance. Additionally, it features a neural simulator for testing and refining policies, along with a benchmark suite to evaluate performance across various criteria.'}, 'zh': {'title': 'Genie Envisioner：指令驱动的机器人智能新平台', 'desc': 'Genie Envisioner（GE）是一个集成了策略学习、评估和模拟的机器人操作平台。它使用一个大型的、基于指令的视频扩散模型，能够捕捉现实世界中机器人交互的空间、时间和语义动态。GE-Act通过轻量级的解码器将潜在表示映射到可执行的动作轨迹，实现了在不同环境中精确且可推广的策略推断。GE-Sim作为一个神经模拟器，支持高保真度的闭环策略开发，整个系统为指令驱动的通用智能提供了可扩展的基础。'}}}, {'id': 'https://huggingface.co/papers/2508.05405', 'title': 'DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning', 'url': 'https://huggingface.co/papers/2508.05405', 'abstract': "DeepPHY evaluates Vision Language Models' physical reasoning and control through simulated environments with varying difficulty levels.  \t\t\t\t\tAI-generated summary \t\t\t\t Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs' understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control.", 'score': 55, 'issue_id': 5243, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '3ec0b6b2d584612a', 'authors': ['Xinrun Xu', 'Pi Bu', 'Ye Wang', 'Börje F. Karlsson', 'Ziming Wang', 'Tengtao Song', 'Qi Zhu', 'Jun Song', 'Zhiming Ding', 'Bo Zheng'], 'affiliations': ['Informatics Department, PUC-Rio', 'Institute of Software, Chinese Academy of Science', 'Renmin University of China', 'Taobao & Tmall Group of Alibaba', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2508.05405.jpg', 'data': {'categories': ['#games', '#benchmark', '#reasoning', '#cv'], 'emoji': '🧠', 'ru': {'title': 'DeepPHY: проверка физического интеллекта ИИ в виртуальных мирах', 'desc': 'DeepPHY - это новая система оценки способностей визуально-языковых моделей (VLM) к физическому рассуждению и управлению в симулированных средах. Она включает в себя различные уровни сложности и детальные метрики оценки. Исследование показало, что даже современные VLM испытывают трудности с применением описательных физических знаний для точного прогнозирующего контроля. DeepPHY помогает оценить понимание моделями фундаментальных физических принципов в сложных динамических средах.'}, 'en': {'title': 'DeepPHY: Bridging the Gap in Physical Reasoning for Vision Language Models', 'desc': 'DeepPHY is a benchmark framework that assesses Vision Language Models (VLMs) on their ability to understand and apply physical reasoning in simulated environments. It highlights the challenges VLMs face in executing precise actions and planning in complex scenarios, which are essential for real-world tasks. The framework includes various environments with different difficulty levels and uses detailed metrics for evaluation. Results show that even advanced VLMs have difficulty converting their understanding of physical concepts into accurate control actions.'}, 'zh': {'title': 'DeepPHY：评估视觉语言模型的物理推理能力', 'desc': 'DeepPHY是一个新颖的基准框架，用于评估视觉语言模型（VLMs）在物理推理和控制方面的能力。该框架通过一系列具有不同难度的模拟环境，系统地测试VLMs对基本物理原理的理解和推理能力。尽管当前的VLMs在感知和视觉推理方面表现出色，但在复杂动态环境中，它们在细节关注和精确行动规划方面仍然存在不足。我们的评估结果显示，即使是最先进的VLMs也难以将描述性的物理知识转化为精确的预测控制。'}}}, {'id': 'https://huggingface.co/papers/2508.05609', 'title': 'Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity', 'url': 'https://huggingface.co/papers/2508.05609', 'abstract': 'Hi3DEval is a hierarchical evaluation framework for 3D generative content that combines object-level and part-level assessments, including material realism, using a large-scale dataset and hybrid 3D representations.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.', 'score': 25, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '97d6454893bee33c', 'authors': ['Yuhan Zhang', 'Long Zhuo', 'Ziyang Chu', 'Tong Wu', 'Zhibing Li', 'Liang Pan', 'Dahua Lin', 'Ziwei Liu'], 'affiliations': ['Fudan University', 'S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Stanford University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05609.jpg', 'data': {'categories': ['#3d', '#benchmark', '#optimization', '#games', '#dataset'], 'emoji': '🧊', 'ru': {'title': 'Новый стандарт оценки 3D-генерации: от общего к частному', 'desc': 'Hi3DEval - это иерархическая система оценки 3D-генеративного контента, сочетающая оценку на уровне объектов и частей. Она включает оценку реалистичности материалов, используя крупномасштабный набор данных и гибридные 3D-представления. Система позволяет проводить как целостную оценку по нескольким параметрам, так и детальный анализ качества. Hi3DEval использует видео-представления для оценки на уровне объектов и материалов, а также предобученные 3D-признаки для восприятия на уровне частей.'}, 'en': {'title': 'Revolutionizing 3D Content Evaluation with Hi3DEval', 'desc': 'Hi3DEval is a new framework designed to evaluate 3D generative content by assessing both the overall object and its individual parts. It addresses the limitations of current methods that only use image-based metrics, which often miss important details like spatial coherence and material realism. The framework includes a large dataset called Hi3DBench, which features a variety of 3D assets and detailed annotations to support comprehensive evaluations. By utilizing advanced scoring systems and hybrid 3D representations, Hi3DEval provides a more accurate and scalable way to assess the quality of 3D generated content.'}, 'zh': {'title': '3D生成内容的分层评估新框架', 'desc': 'Hi3DEval是一个针对3D生成内容的分层评估框架，结合了对象级和部分级的评估，包括材料真实感。现有的方法主要依赖于基于图像的指标，仅在对象级别进行评估，无法有效捕捉空间一致性和材料真实性。为了解决这些问题，Hi3DEval提供了多维度的整体评估和细致的质量分析，并扩展了纹理评估，关注如反射率、饱和度和金属感等属性。通过构建Hi3DBench数据集和3D感知的自动评分系统，Hi3DEval在建模3D特性方面超越了现有的图像基准，提供了可扩展的评估替代方案。'}}}, {'id': 'https://huggingface.co/papers/2508.03644', 'title': 'Are We on the Right Way for Assessing Document Retrieval-Augmented\n  Generation?', 'url': 'https://huggingface.co/papers/2508.03644', 'abstract': 'Double-Bench is a large-scale, multilingual, and multimodal evaluation system for document Retrieval-Augmented Generation (RAG) systems, addressing limitations in current benchmarks and providing comprehensive assessments of system components.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.', 'score': 19, 'issue_id': 5243, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'b8c44e363c76888f', 'authors': ['Wenxuan Shen', 'Mingjia Wang', 'Yaochen Wang', 'Dongping Chen', 'Junjie Yang', 'Yao Wan', 'Weiwei Lin'], 'affiliations': ['Huazhong University of Science and Technology', 'South China University of Technology', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2508.03644.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#survey', '#rag', '#open_source', '#multilingual'], 'emoji': '📊', 'ru': {'title': 'Double-Bench: комплексная оценка RAG систем для документов', 'desc': 'Double-Bench - это новая система оценки для Retrieval-Augmented Generation (RAG) систем, работающих с документами. Она включает в себя 3276 документов и 5168 запросов на 6 языках, охватывая 4 типа документов. Система позволяет проводить детальную оценку каждого компонента RAG систем, включая мультимодальные языковые модели (MLLM) и модели эмбеддингов. Эксперименты показали, что разрыв между текстовыми и визуальными моделями эмбеддингов сокращается, а также выявили проблему излишней уверенности современных RAG систем.'}, 'en': {'title': 'Double-Bench: Elevating RAG Evaluation for Real-World Challenges', 'desc': 'Double-Bench is a new evaluation system designed to improve the assessment of Retrieval-Augmented Generation (RAG) systems, which combine document retrieval and generation. It addresses the shortcomings of existing benchmarks by providing a large-scale, multilingual, and multimodal dataset that includes 3,276 documents and 5,168 queries across multiple languages and document types. The evaluation focuses on fine-grained assessments of each component in RAG systems, ensuring that queries are based on thoroughly verified evidence. Our experiments reveal important insights into the performance of various models and highlight the need for better document retrieval capabilities in the face of over-confidence in current frameworks.'}, 'zh': {'title': '双重基准：提升文档RAG系统评估的全新标准', 'desc': 'Double-Bench是一个大规模的多语言多模态评估系统，专门用于文档增强生成（RAG）系统的评估。它解决了当前基准测试的局限性，能够对RAG系统的各个组件进行全面的评估。该系统包含3276份文档和5168个查询，涵盖6种语言和4种文档类型，确保评估的质量和完整性。我们的实验表明，文本和视觉嵌入模型之间的差距正在缩小，同时也揭示了当前RAG框架中存在的过度自信问题。'}}}, {'id': 'https://huggingface.co/papers/2508.03990', 'title': "Are Today's LLMs Ready to Explain Well-Being Concepts?", 'url': 'https://huggingface.co/papers/2508.03990', 'abstract': 'LLMs can be fine-tuned to generate high-quality, audience-tailored explanations of well-being concepts using Supervised Fine-Tuning and Direct Preference Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.', 'score': 18, 'issue_id': 5242, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': 'a742f57af42990c1', 'authors': ['Bohan Jiang', 'Dawei Li', 'Zhen Tan', 'Chengshuai Zhao', 'Huan Liu'], 'affiliations': ['School of Computing and Augmented Intelligence, Arizona State University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.03990.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#dataset', '#alignment', '#open_source', '#rlhf', '#training'], 'emoji': '🧠', 'ru': {'title': 'Языковые модели учатся объяснять концепции благополучия', 'desc': 'Исследование демонстрирует, что модели машинного обучения можно настроить для генерации высококачественных объяснений концепций благополучия, адаптированных под разную аудиторию. Авторы создали большой датасет из 43 880 объяснений 2 194 концепций благополучия, сгенерированных десятью различными языковыми моделями. Они разработали систему оценки качества объяснений с использованием языковых моделей в роли судей. Результаты показывают, что модели, настроенные с помощью методов Supervised Fine-Tuning и Direct Preference Optimization, превосходят более крупные базовые модели в задаче генерации специализированных объяснений.'}, 'en': {'title': 'Tailoring Well-Being Explanations with Fine-Tuned LLMs', 'desc': 'This paper discusses how Large Language Models (LLMs) can be improved to provide better explanations of well-being concepts tailored to different audiences. It highlights the importance of both factual accuracy and audience-specific needs in generating high-quality explanations. The authors created a large dataset of explanations and developed a unique evaluation framework using LLMs as judges to assess the quality of these explanations. Their findings show that fine-tuning LLMs with Supervised Fine-Tuning and Direct Preference Optimization leads to significant improvements in explanation quality compared to larger, unrefined models.'}, 'zh': {'title': '提升幸福感解释质量的智能方法', 'desc': '本研究探讨了如何通过监督微调（SFT）和直接偏好优化（DPO）来提升大型语言模型（LLMs）生成的关于幸福感概念的解释质量。我们构建了一个包含43,880个解释的大型数据集，涵盖2,194个幸福感概念，并引入了一个基于原则的LLM评估框架。研究结果表明，经过微调的模型在生成解释时的质量显著提高，且与人类评估结果高度一致。不同模型、受众和类别之间的解释质量差异明显，表明偏好学习在专业解释任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.03923', 'title': 'CoAct-1: Computer-using Agents with Coding as Actions', 'url': 'https://huggingface.co/papers/2508.03923', 'abstract': 'A multi-agent system that combines GUI control with programmatic execution improves efficiency and success in complex computer automation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents that operate computers via Graphical User Interfaces (GUIs) often struggle with efficiency and reliability on complex, long-horizon tasks. While augmenting these agents with planners can improve task decomposition, they remain constrained by the inherent limitations of performing all actions through GUI manipulation, leading to brittleness and inefficiency. In this work, we introduce a more robust and flexible paradigm: enabling agents to use coding as a enhanced action. We present CoAct-1, a novel multi-agent system that synergistically combines GUI-based control with direct programmatic execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks to either a conventional GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts. This hybrid approach allows the agent to bypass inefficient GUI action sequences for tasks like file management and data processing, while still leveraging visual interaction when necessary. We evaluate our system on the challenging OSWorld benchmark, where CoAct-1 achieves a new state-of-the-art success rate of 60.76%, significantly outperforming prior methods. Furthermore, our approach dramatically improves efficiency, reducing the average number of steps required to complete a task to just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that integrating coding as a core action provides a more powerful, efficient, and scalable path toward generalized computer automation.', 'score': 10, 'issue_id': 5246, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '1527938782ab293a', 'authors': ['Linxin Song', 'Yutong Dai', 'Viraj Prabhu', 'Jieyu Zhang', 'Taiwei Shi', 'Li Li', 'Junnan Li', 'Silvio Savarese', 'Zeyuan Chen', 'Jieyu Zhao', 'Ran Xu', 'Caiming Xiong'], 'affiliations': ['Salesforce Research', 'University of Southern California', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.03923.jpg', 'data': {'categories': ['#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Синергия GUI и кода: новый уровень компьютерной автоматизации', 'desc': 'Статья представляет CoAct-1 - многоагентную систему для автоматизации компьютерных задач, сочетающую управление через графический интерфейс и программное выполнение. Система включает Оркестратор, который делегирует подзадачи GUI-оператору или агенту-программисту, способному писать и выполнять скрипты. CoAct-1 достигает наилучших результатов на бенчмарке OSWorld, значительно превосходя предыдущие методы по успешности и эффективности. Интеграция программирования как основного действия обеспечивает более мощный и масштабируемый подход к автоматизации компьютера.'}, 'en': {'title': 'Empowering Automation: Merging GUI Control with Programmatic Power', 'desc': 'This paper presents CoAct-1, a multi-agent system that enhances computer automation by combining GUI control with programmatic execution. Traditional agents often face challenges with efficiency and reliability when performing complex tasks solely through GUI manipulation. CoAct-1 introduces an Orchestrator that assigns subtasks to either a GUI Operator or a Programmer agent capable of executing scripts, allowing for a more flexible approach. The system achieves a state-of-the-art success rate of 60.76% on the OSWorld benchmark, demonstrating significant improvements in both efficiency and task completion speed.'}, 'zh': {'title': '结合编程与GUI，提升自动化效率！', 'desc': '本论文介绍了一种新的多智能体系统CoAct-1，它结合了图形用户界面（GUI）控制和程序化执行，以提高复杂计算机自动化任务的效率和成功率。传统的基于GUI的智能体在处理复杂任务时效率低下，而CoAct-1通过引入编程作为增强动作，克服了这一限制。该系统的协调者动态分配子任务给GUI操作员或专门的程序员智能体，从而实现更灵活的任务处理。实验结果表明，CoAct-1在OSWorld基准测试中取得了60.76%的成功率，显著优于之前的方法，并且平均完成任务所需的步骤减少到10.15步。'}}}, {'id': 'https://huggingface.co/papers/2508.04017', 'title': 'Can Large Multimodal Models Actively Recognize Faulty Inputs? A\n  Systematic Evaluation Framework of Their Input Scrutiny Ability', 'url': 'https://huggingface.co/papers/2508.04017', 'abstract': "ISEval framework evaluates large multimodal models' ability to detect flawed inputs, revealing challenges in identifying certain types of errors and modality-specific biases.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have witnessed remarkable growth, showcasing formidable capabilities in handling intricate multimodal tasks with exceptional performance. Recent research has underscored the inclination of large language models to passively accept defective inputs, often resulting in futile reasoning on invalid prompts. However, the same critical question of whether LMMs can actively detect and scrutinize erroneous inputs still remains unexplored. To address this gap, we introduce the Input Scrutiny Ability Evaluation Framework (ISEval), which encompasses seven categories of flawed premises and three evaluation metrics. Our extensive evaluation of ten advanced LMMs has identified key findings. Most models struggle to actively detect flawed textual premises without guidance, which reflects a strong reliance on explicit prompts for premise error identification. Error type affects performance: models excel at identifying logical fallacies but struggle with surface-level linguistic errors and certain conditional flaws. Modality trust varies-Gemini 2.5 pro and Claude Sonnet 4 balance visual and textual info, while aya-vision-8b over-rely on text in conflicts. These insights underscore the urgent need to enhance LMMs' proactive verification of input validity and shed novel insights into mitigating the problem. The code is available at https://github.com/MLGroupJLU/LMM_ISEval.", 'score': 9, 'issue_id': 5243, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '37794107e7cbe332', 'authors': ['Haiqi Yang', 'Jinzhe Li', 'Gengxu Li', 'Yi Chang', 'Yuan Wu'], 'affiliations': ['Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China', 'International Center of Future Science, Jilin University', 'School of Artificial Intelligence, Jilin University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04017.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#ethics', '#hallucinations', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Проверка на прочность: как мультимодальные ИИ справляются с ошибками во входных данных', 'desc': 'Фреймворк ISEval оценивает способность больших мультимодальных моделей обнаруживать ошибочные входные данные. Исследование выявило, что большинство моделей испытывают трудности с активным обнаружением текстовых ошибок без специальных указаний. Производительность зависит от типа ошибки: модели хорошо справляются с логическими ошибками, но затрудняются с поверхностными лингвистическими и некоторыми условными ошибками. Обнаружены различия в доверии к модальностям: некоторые модели сбалансированно используют визуальную и текстовую информацию, в то время как другие чрезмерно полагаются на текст при конфликтах.'}, 'en': {'title': 'Enhancing Input Validation in Large Multimodal Models', 'desc': 'The ISEval framework assesses the ability of large multimodal models (LMMs) to identify flawed inputs, highlighting their challenges in recognizing specific errors and biases related to different modalities. Despite their impressive performance in multimodal tasks, many LMMs tend to accept defective inputs without questioning them, leading to ineffective reasoning. The framework categorizes seven types of flawed premises and employs three evaluation metrics to analyze ten advanced LMMs, revealing that most struggle to detect errors without explicit prompts. The findings indicate that while models perform well in identifying logical fallacies, they face difficulties with linguistic errors and exhibit varying trust in different modalities, emphasizing the need for improved input validation mechanisms.'}, 'zh': {'title': '提升多模态模型的输入验证能力', 'desc': 'ISEval框架评估大型多模态模型检测缺陷输入的能力，揭示了识别某些类型错误和特定模态偏见的挑战。研究表明，大型语言模型倾向于被动接受有缺陷的输入，导致在无效提示上进行无效推理。尽管如此，LMMs是否能够主动检测和审查错误输入的问题仍未得到充分探讨。我们的评估显示，大多数模型在没有指导的情况下难以主动识别文本前提的缺陷，强调了对明确提示的强烈依赖。'}}}, {'id': 'https://huggingface.co/papers/2508.02120', 'title': "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning\n  Models", 'url': 'https://huggingface.co/papers/2508.02120', 'abstract': 'Research on efficient reasoning methods for Large Reasoning Models (LRMs) aims to reduce reasoning path length without sacrificing performance, through single-model optimization and model collaboration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Reasoning Models (LRMs) have gradually become a research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of overthinking has gradually emerged. Specifically, when generating answers, these models often construct excessively long reasoning chains with redundant or repetitive steps, which leads to reduced reasoning efficiency and may affect the accuracy of the final answer. To this end, various efficient reasoning methods have been proposed, aiming to reduce the length of reasoning paths without compromising model performance and reasoning capability. By reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the lens of single-model optimization versus model collaboration: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models. Besides, we maintain a public GitHub repository that tracks the latest progress in efficient reasoning methods.', 'score': 8, 'issue_id': 5243, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'ab1d76b8f9fbefe8', 'authors': ['Linan Yue', 'Yichao Du', 'Yizhi Wang', 'Weibo Gao', 'Fangzhou Yao', 'Li Wang', 'Ye Liu', 'Ziyu Xu', 'Qi Liu', 'Shimin Di', 'Min-Ling Zhang'], 'affiliations': ['Alibaba Group', 'Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education', 'School of Computer Science and Engineering, Southeast University', 'University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2508.02120.jpg', 'data': {'categories': ['#reasoning', '#training', '#open_source', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение в LRM: сокращение пути без потери качества', 'desc': 'Исследование эффективных методов рассуждения для Больших Моделей Рассуждения (LRM) направлено на сокращение длины цепочек рассуждений без ущерба для производительности. Оно включает оптимизацию отдельных моделей и сотрудничество между моделями. Проблема чрезмерного мышления в LRM приводит к избыточно длинным цепочкам рассуждений, что снижает эффективность. Предлагаемые методы эффективного рассуждения разделяются на две категории: оптимизация одной модели и сотрудничество нескольких моделей.'}, 'en': {'title': 'Streamlining Reasoning: Enhancing Efficiency in Large Reasoning Models', 'desc': 'This paper discusses the development of efficient reasoning methods for Large Reasoning Models (LRMs), which are designed to improve logical deduction and decision-making. It highlights the challenges posed by overly long reasoning paths that can hinder performance and accuracy. The authors categorize existing research into two main approaches: optimizing single models for better reasoning efficiency and enhancing collaboration between multiple models. Additionally, they provide a public GitHub repository to share ongoing advancements in this area.'}, 'zh': {'title': '高效推理：提升大型推理模型的智能决策能力', 'desc': '本研究探讨了大型推理模型（LRMs）中高效推理方法的研究，旨在在不牺牲性能的情况下减少推理路径的长度。研究中提到的DeepSeek R1模型因其卓越的表现和开源特性而受到关注，推动了R1风格LRMs的研究进展。与传统的大型语言模型（LLMs）不同，这些模型通过长链推理和自我反思等机制增强了逻辑推理和决策能力。然而，随着应用的广泛，过度推理的问题逐渐显现，导致推理效率降低，因此提出了多种高效推理方法以优化推理过程。'}}}, {'id': 'https://huggingface.co/papers/2508.04423', 'title': 'Evaluating, Synthesizing, and Enhancing for Customer Support\n  Conversation', 'url': 'https://huggingface.co/papers/2508.04423', 'abstract': 'A structured framework and datasets for training customer service agents using well-defined support strategies improve the quality of customer support interactions and problem resolution.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.', 'score': 7, 'issue_id': 5242, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': 'd1f778b32418973c', 'authors': ['Jie Zhu', 'Huaixia Dou', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang', 'Fang Kong'], 'affiliations': ['Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04423.jpg', 'data': {'categories': ['#data', '#agents', '#science', '#dataset', '#open_source', '#training'], 'emoji': '🎭', 'ru': {'title': 'Структурированное обучение агентов поддержки для повышения качества обслуживания клиентов', 'desc': 'Статья представляет структурированный подход к обучению агентов службы поддержки клиентов, основанный на четко определенных стратегиях. Авторы предлагают framework для задачи Customer Support Conversation (CSC), включающий пять этапов разговора и двенадцать стратегий. Они создают два датасета: CSConv для оценки и RoleCS для обучения, используя реальные диалоги и симуляцию с помощью языковых моделей. Эксперименты показывают, что fine-tuning LLM на RoleCS значительно улучшает качество ответов и решение проблем клиентов.'}, 'en': {'title': 'Empowering Customer Support with Strategic Conversations', 'desc': 'This paper presents a structured framework for training customer service agents, focusing on effective communication and problem resolution. It introduces the Customer Support Conversation (CSC) task, which utilizes well-defined support strategies based on COPC guidelines. The authors create a dataset called CSConv, consisting of real-world conversations rewritten to reflect strategic communication, and a training dataset called RoleCS that simulates these interactions. Experiments demonstrate that fine-tuning large language models (LLMs) on RoleCS enhances their ability to produce high-quality, strategy-aligned responses, leading to improved customer support outcomes.'}, 'zh': {'title': '提升客户支持质量的结构化框架', 'desc': '本文提出了一种结构化框架和数据集，用于训练客户服务代理，采用明确的支持策略以提高客户支持互动的质量和问题解决能力。我们引入了客户支持对话（CSC）任务，旨在帮助客服代理使用定义良好的支持策略进行响应。基于COPC指南，我们定义了五个对话阶段和十二种策略，以指导高质量的互动。通过构建CSConv数据集和RoleCS训练数据集，实验表明在RoleCS上微调强大的大语言模型（LLM）显著提升了其在CSConv上的策略一致性响应能力。'}}}, {'id': 'https://huggingface.co/papers/2508.02038', 'title': 'Marco-Voice Technical Report', 'url': 'https://huggingface.co/papers/2508.02038', 'abstract': 'A multifunctional speech synthesis system integrates voice cloning and emotion control using speaker-emotion disentanglement and rotational emotional embeddings, achieving high expressive and natural speech.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis.', 'score': 7, 'issue_id': 5247, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'a3fa663a271be79f', 'authors': ['Fengping Tian', 'Chenyang Lyu', 'Xuanfan Ni', 'Haoqin Sun', 'Qingjuan Li', 'Zhiqiang Qian', 'Haijun Li', 'Longyue Wang', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba International Digital Commerce'], 'pdf_title_img': 'assets/pdf/title_img/2508.02038.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Революция в синтезе речи: естественные эмоции и клонирование голоса', 'desc': 'Эта статья представляет многофункциональную систему синтеза речи, объединяющую клонирование голоса и управление эмоциями в единой структуре. Система использует механизм разделения характеристик диктора и эмоций с помощью контрастного обучения, а также метод ротационных эмоциональных эмбеддингов для плавного контроля эмоций. Авторы создали датасет CSEMOTIONS для обучения и оценки модели, содержащий 10 часов эмоциональной речи на мандаринском диалекте. Эксперименты показали, что предложенная система Marco-Voice значительно превосходит существующие методы по объективным и субъективным метрикам.'}, 'en': {'title': 'Expressive Speech Synthesis with Emotion Control', 'desc': "This paper introduces a new speech synthesis system called Marco-Voice that combines voice cloning with emotion control. It uses a technique called speaker-emotion disentanglement to separate the speaker's identity from their emotional expression, allowing for more flexible and natural speech generation. The system incorporates rotational emotional embeddings to enable smooth transitions between different emotions. A new dataset, CSEMOTIONS, was created to train and evaluate the system, showing significant improvements in speech clarity and emotional depth compared to existing methods."}, 'zh': {'title': '多功能语音合成：声音与情感的完美结合', 'desc': '这篇论文介绍了一种多功能语音合成系统，结合了声音克隆和情感控制。该系统通过说话者-情感解耦机制和旋转情感嵌入方法，实现了高表现力和自然的语音生成。研究中构建了CSEMOTIONS数据集，包含六位专业说话者的十小时普通话情感语音。实验结果表明，Marco-Voice在语音清晰度和情感丰富性方面显著提升，代表了表达性神经语音合成领域的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2508.05496', 'title': 'InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities', 'url': 'https://huggingface.co/papers/2508.05496', 'abstract': "InfiAlign, a scalable and sample-efficient post-training framework, combines supervised fine-tuning and Direct Preference Optimization to enhance large language models' reasoning abilities with minimal data and computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.", 'score': 5, 'issue_id': 5246, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '6dcac22b35678a50', 'authors': ['Shuo Cai', 'Su Lu', 'Qi Zhou', 'Kejing Yang', 'Zhijie Sang', 'Congkai Xie', 'Hongxia Yang'], 'affiliations': ['InfiX.ai', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05496.jpg', 'data': {'categories': ['#alignment', '#reasoning', '#optimization', '#rlhf', '#training', '#math', '#data'], 'emoji': '🧠', 'ru': {'title': 'InfiAlign: эффективное улучшение рассуждений в больших языковых моделях', 'desc': 'InfiAlign - это масштабируемая и эффективная с точки зрения данных система пост-обучения для улучшения способностей больших языковых моделей к рассуждениям. Она объединяет supervised fine-tuning и Direct Preference Optimization для повышения производительности при минимальных затратах данных и вычислительных ресурсов. В основе InfiAlign лежит конвейер отбора высококачественных данных для выравнивания из открытых наборов данных по рассуждениям. Применение InfiAlign к модели Qwen2.5-Math-7B-Base позволило достичь результатов на уровне DeepSeek-R1-Distill-Qwen-7B, используя всего 12% обучающих данных.'}, 'en': {'title': 'Efficiently Enhancing Reasoning in Large Language Models', 'desc': 'InfiAlign is a new framework designed to improve the reasoning abilities of large language models (LLMs) while using less data and computational resources. It combines supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to enhance model performance efficiently. The framework includes a smart data selection process that curates high-quality training data from existing datasets, which helps achieve better results with significantly less data. When tested on the Qwen2.5-Math-7B-Base model, InfiAlign demonstrated strong performance improvements, especially in mathematical reasoning tasks, while using only about 12% of the usual training data.'}, 'zh': {'title': '高效对齐，提升推理能力的创新框架', 'desc': 'InfiAlign是一种可扩展且样本高效的后训练框架，旨在通过最小的数据和计算成本提升大型语言模型的推理能力。该框架结合了监督微调和直接偏好优化，能够有效地对模型进行对齐。InfiAlign的核心是一个强大的数据选择管道，自动从开源推理数据集中筛选高质量的对齐数据，从而显著提高性能并减少数据需求。通过在Qwen2.5-Math-7B-Base模型上的应用，InfiAlign展示了在多样化推理任务中的强大泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.05630', 'title': 'MOSEv2: A More Challenging Dataset for Video Object Segmentation in\n  Complex Scenes', 'url': 'https://huggingface.co/papers/2508.05630', 'abstract': 'MOSEv2, a more challenging dataset, highlights the limitations of current VOS methods in real-world scenarios with increased complexity and diverse challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video.', 'score': 4, 'issue_id': 5242, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'a2a9fbee6a3ffe42', 'authors': ['Henghui Ding', 'Kaining Ying', 'Chang Liu', 'Shuting He', 'Xudong Jiang', 'Yu-Gang Jiang', 'Philip H. S. Torr', 'Song Bai'], 'affiliations': ['ByteDance Inc', 'Fudan University, Shanghai, China', 'Nanyang Technological University, Singapore', 'Shanghai University of Finance and Economics, China', 'University of Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2508.05630.jpg', 'data': {'categories': ['#dataset', '#video', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'MOSEv2: Новый вызов для алгоритмов сегментации объектов в видео', 'desc': 'MOSEv2 - это новый набор данных для задачи сегментации объектов в видео (VOS), созданный для преодоления ограничений существующих бенчмарков. Он содержит более 5000 видео с разнообразными сложными сценариями, включая исчезновение объектов, окклюзии, неблагоприятные погодные условия и другие реальные проблемы. Тестирование современных методов VOS на MOSEv2 показало значительное снижение их производительности по сравнению с предыдущими датасетами. Этот набор данных призван стимулировать развитие более надежных алгоритмов сегментации объектов для реальных задач.'}, 'en': {'title': 'MOSEv2: Elevating Video Object Segmentation to Real-World Challenges', 'desc': 'The paper introduces MOSEv2, a new dataset for video object segmentation (VOS) that presents more complex real-world challenges than previous datasets. While existing methods perform well on simpler benchmarks like DAVIS and YouTube-VOS, they struggle with the increased difficulties found in MOSEv2, which includes diverse scenarios such as occlusions, object disappearance, and adverse weather conditions. The dataset contains over 5,000 videos and nearly 702,000 high-quality masks for a wide variety of objects, making it a significant resource for advancing VOS research. Benchmarking shows that current VOS methods experience substantial performance drops when tested on MOSEv2, indicating a need for improved algorithms that can handle real-world complexities.'}, 'zh': {'title': 'MOSEv2：应对真实世界复杂性的挑战', 'desc': 'MOSEv2是一个更具挑战性的数据集，揭示了当前视频目标分割（VOS）方法在复杂真实场景中的局限性。尽管现有方法在DAVIS和YouTube-VOS等基准测试中表现出色，但这些数据集主要包含显著、主导和孤立的对象，限制了其在现实世界中的泛化能力。MOSEv2包含5024个视频和超过701976个高质量的掩膜，涵盖200个类别的10074个对象，场景复杂性显著增加。研究表明，尽管在现有数据集上准确率很高，但当前的VOS方法在面对真实世界的复杂性时仍然面临挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.01650', 'title': 'StrandDesigner: Towards Practical Strand Generation with Sketch Guidance', 'url': 'https://huggingface.co/papers/2508.01650', 'abstract': 'A sketch-based strand generation model using a learnable upsampling strategy and multi-scale adaptive conditioning mechanism outperforms existing methods in realism and precision for hair strand generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at [GitHub](https://github.com/fighting-Zhang/StrandDesigner).', 'score': 4, 'issue_id': 5243, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '377e9e9eca3593db', 'authors': ['Na Zhang', 'Moran Li', 'Chengming Xu', 'Han Feng', 'Xiaobin Hu', 'Jiangning Zhang', 'Weijian Cao', 'Chengjie Wang', 'Yanwei Fu'], 'affiliations': ['Fudan University, Shanghai, China', 'School of Data Science, Fudan University, Shanghai Innovation Institute, Institute of Trustworthy Embodied AI, Fudan University', 'Tencent YouTu Lab, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01650.jpg', 'data': {'categories': ['#benchmark', '#games', '#cv', '#3d', '#open_source', '#architecture'], 'emoji': '💇', 'ru': {'title': 'Реалистичная генерация прядей волос по эскизам с помощью ИИ', 'desc': 'Эта статья представляет первую модель генерации прядей волос на основе эскизов. Модель использует обучаемую стратегию апсемплинга прядей и многомасштабный адаптивный механизм кондиционирования с трансформером и диффузионными головками. Эксперименты показывают, что предложенный метод превосходит существующие подходы по реалистичности и точности генерации волос. Модель особенно полезна для приложений компьютерной графики и виртуальной реальности.'}, 'en': {'title': 'Revolutionizing Hair Strand Generation with Sketch-Based Precision', 'desc': 'This paper presents a novel sketch-based model for generating realistic hair strands, addressing the limitations of existing methods. The model utilizes a learnable upsampling strategy to effectively encode 3D hair strands into multi-scale latent spaces, enhancing detail and precision. Additionally, it incorporates a multi-scale adaptive conditioning mechanism that employs transformers with diffusion heads to maintain consistency across different levels of detail. Experimental results demonstrate that this approach significantly improves realism and precision in hair strand generation compared to traditional techniques.'}, 'zh': {'title': '草图驱动的发丝生成新方法', 'desc': '本文提出了一种基于草图的发丝生成模型，采用可学习的上采样策略和多尺度自适应条件机制，显著提高了发丝生成的真实感和精确度。该模型解决了复杂发丝交互和多样化草图模式建模的关键挑战。通过将3D发丝编码到多尺度潜在空间，模型实现了更细致的控制，同时保持用户友好性。实验结果表明，该方法在多个基准数据集上超越了现有技术，验证了其有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.04979', 'title': 'Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast\n  Image Compression', 'url': 'https://huggingface.co/papers/2508.04979', 'abstract': 'SODEC, a single-step diffusion image compression model, enhances decoding speed and fidelity by using a pre-trained VAE and a fidelity guidance module.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, a novel single-step diffusion image compression model. We argue that in image compression, a sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage a pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with a single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20times. Code is released at: https://github.com/zhengchen1999/SODEC.', 'score': 3, 'issue_id': 5251, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '86b2291887bd3cbc', 'authors': ['Zheng Chen', 'Mingde Zhou', 'Jinpei Guo', 'Jiale Yuan', 'Yifei Ji', 'Yulun Zhang'], 'affiliations': ['Carnegie Mellon University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04979.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#open_source', '#training'], 'emoji': '🖼️', 'ru': {'title': 'Быстрое и качественное сжатие изображений с помощью одношаговой диффузии', 'desc': 'SODEC - это новая модель сжатия изображений на основе одношаговой диффузии. Она использует предобученный вариационный автоэнкодер для создания информативных латентных представлений и заменяет итеративный процесс шумоподавления одношаговым декодированием. Модель включает модуль точности, который улучшает верность восстановленного изображения оригиналу. SODEC значительно превосходит существующие методы по соотношению скорость-искажение-восприятие и ускоряет декодирование в 20 раз по сравнению с предыдущими диффузионными моделями сжатия.'}, 'en': {'title': 'SODEC: Fast and Faithful Image Compression with Single-Step Diffusion', 'desc': 'SODEC is a new image compression model that uses a single-step diffusion process to enhance both decoding speed and image quality. It utilizes a pre-trained Variational Autoencoder (VAE) to generate informative latent representations, eliminating the need for multiple sampling steps. To further improve the fidelity of the compressed images, a fidelity guidance module is introduced, ensuring that the output closely resembles the original image. The model also incorporates a rate annealing training strategy, allowing it to perform effectively even at very low bitrates, resulting in significant improvements over existing methods.'}, 'zh': {'title': 'SODEC：快速高保真的图像压缩新方法', 'desc': 'SODEC是一种单步扩散图像压缩模型，通过使用预训练的变分自编码器（VAE）和保真度引导模块，提高了解码速度和图像质量。该模型解决了传统扩散图像压缩方法中存在的解码延迟过高和图像保真度不足的问题。SODEC利用信息丰富的潜在表示，避免了多步采样的复杂性，并通过单步解码来加速过程。同时，保真度引导模块确保输出图像与原始图像保持一致，从而提升了压缩效果。'}}}, {'id': 'https://huggingface.co/papers/2508.05618', 'title': 'Learning to Reason for Factuality', 'url': 'https://huggingface.co/papers/2508.05618', 'abstract': 'A novel reward function for online reinforcement learning improves factuality and detail in reasoning large language models without reducing helpfulness.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning Large Language Models (R-LLMs) have significantly advanced complex reasoning tasks but often struggle with factuality, generating substantially more hallucinations than their non-reasoning counterparts on long-form factuality benchmarks. However, extending online Reinforcement Learning (RL), a key component in recent R-LLM advancements, to the long-form factuality setting poses several unique challenges due to the lack of reliable verification methods. Previous work has utilized automatic factuality evaluation frameworks such as FActScore to curate preference data in the offline RL setting, yet we find that directly leveraging such methods as the reward in online RL leads to reward hacking in multiple ways, such as producing less detailed or relevant responses. We propose a novel reward function that simultaneously considers the factual precision, response detail level, and answer relevance, and applies online RL to learn high quality factual reasoning. Evaluated on six long-form factuality benchmarks, our factual reasoning model achieves an average reduction of 23.1 percentage points in hallucination rate, a 23% increase in answer detail level, and no degradation in the overall response helpfulness.', 'score': 2, 'issue_id': 5261, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '97c0924a378da917', 'authors': ['Xilun Chen', 'Ilia Kulikov', 'Vincent-Pierre Berges', 'Barlas Oğuz', 'Rulin Shao', 'Gargi Ghosh', 'Jason Weston', 'Wen-tau Yih'], 'affiliations': ['FAIR at Meta', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.05618.jpg', 'data': {'categories': ['#rl', '#rlhf', '#hallucinations', '#reasoning', '#long_context'], 'emoji': '🧠', 'ru': {'title': 'Повышение фактической точности языковых моделей без потери качества', 'desc': 'Эта статья представляет новую функцию вознаграждения для онлайн-обучения с подкреплением, которая улучшает фактическую точность и детализацию в рассуждениях больших языковых моделей. Авторы обнаружили, что прямое использование автоматических методов оценки фактической точности в качестве вознаграждения приводит к нежелательным эффектам. Предложенная функция вознаграждения учитывает фактическую точность, уровень детализации ответа и релевантность. Результаты показывают значительное снижение уровня галлюцинаций и увеличение детализации ответов без ухудшения общей полезности.'}, 'en': {'title': 'Enhancing Factuality and Detail in Language Models with Novel RL Rewards', 'desc': 'This paper introduces a new reward function for online reinforcement learning (RL) that enhances the factual accuracy and detail of reasoning in large language models (LLMs). The authors address the challenge of high hallucination rates in LLMs when generating long-form content, which often leads to inaccuracies. By developing a reward function that balances factual precision, detail, and relevance, they improve the quality of responses without sacrificing helpfulness. The proposed model shows significant improvements in factuality and detail across multiple benchmarks, demonstrating its effectiveness in reducing errors while maintaining user satisfaction.'}, 'zh': {'title': '提升推理模型的事实性与细节性', 'desc': '本文提出了一种新颖的奖励函数，用于在线强化学习，以提高大型语言模型的事实性和推理细节，而不降低其有用性。研究发现，现有的自动事实性评估框架在在线强化学习中直接作为奖励使用，会导致奖励黑客行为，影响生成的回答质量。我们提出的奖励函数同时考虑了事实精度、回答细节水平和答案相关性，从而有效地学习高质量的事实推理。经过六个长篇事实性基准的评估，我们的模型在幻觉率上平均降低了23.1个百分点，回答细节水平提高了23%，且整体回答的有用性没有下降。'}}}, {'id': 'https://huggingface.co/papers/2508.03404', 'title': 'Visual Document Understanding and Question Answering: A Multi-Agent\n  Collaboration Framework with Test-Time Scaling', 'url': 'https://huggingface.co/papers/2508.03404', 'abstract': 'MACT, a Multi-Agent Collaboration framework with Test-Time scaling, enhances visual document understanding and VQA by using four specialized agents and mixed reward modeling, achieving superior performance with reduced parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing vision-language models (VLMs), whether generalists or specialists, remain constrained by their parameter scale, lack robust self-correction capabilities, and underperform in tasks involving long visual contexts and complex reasoning, resulting in suboptimal performance on document-based tasks. To address this, we propose MACT, a Multi-Agent Collaboration framework with Test-Time scaling, tailored for visual document understanding and visual question answering (VQA). It comprises four distinct small-scale agents, i.e., planning, execution, judgment, and answer agents, with clearly defined roles and effective collaboration. Notably, the judgment agent exclusively verifies correctness and redirects to prior agents for revisions, outperforming conventional correction strategies. To further expand the capability boundaries of the framework, we propose mixed reward modeling that balances agent-specific abilities and global collaboration, as well as agent-wise hybrid test-time scaling, which customizes different scaling strategies for each agent based on their functions. Evaluated on benchmarks spanning both document-based and non-document-based settings, our MACT shows superior performance with a smaller parameter scale without sacrificing the ability of general and mathematical tasks. Especially, it stands out in benchmarks involving long visual contexts and complicated reasoning. The three variants of MACT consistently hold the top three positions in average scores, leading in 13 of the 15 benchmarks. Code will be available at: https://github.com/YU-deep/MACT.git.', 'score': 2, 'issue_id': 5252, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '500ce3c7788fd634', 'authors': ['Xinlei Yu', 'Zhangquan Chen', 'Yudong Zhang', 'Shilin Lu', 'Ruolin Shen', 'Jiangning Zhang', 'Xiaobin Hu', 'Yanwei Fu', 'Shuicheng Yan'], 'affiliations': ['Fudan University, China', 'Nanyang Technological University, Singapore', 'National University of Singapore, Singapore', 'Technical University of Munich, Germany', 'Tsinghua University, China', 'University of Science and Technology of China, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03404.jpg', 'data': {'categories': ['#agents', '#reasoning', '#benchmark', '#small_models', '#cv', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'Многоагентное сотрудничество для улучшенного понимания визуальных документов', 'desc': 'MACT - это новая система для понимания визуальных документов и ответов на вопросы по изображениям. Она использует четыре специализированных агента: планирования, выполнения, оценки и ответа, которые эффективно взаимодействуют друг с другом. MACT применяет смешанное моделирование вознаграждений и масштабирование во время тестирования для улучшения производительности. Система показывает превосходные результаты на различных тестах, особенно для задач с длинным визуальным контекстом и сложными рассуждениями.'}, 'en': {'title': 'MACT: Enhancing Document Understanding with Smart Agent Collaboration', 'desc': "MACT is a Multi-Agent Collaboration framework designed to improve visual document understanding and visual question answering (VQA). It utilizes four specialized agents—planning, execution, judgment, and answer agents—that work together to enhance performance while maintaining a smaller parameter scale. The judgment agent plays a crucial role by verifying answers and prompting revisions from other agents, which leads to better accuracy compared to traditional methods. Additionally, MACT employs mixed reward modeling and hybrid test-time scaling to optimize each agent's performance based on their specific tasks, resulting in superior outcomes in complex reasoning scenarios."}, 'zh': {'title': '多智能体协作，提升视觉理解与问答能力', 'desc': 'MACT是一个多智能体协作框架，旨在提升视觉文档理解和视觉问答（VQA）的性能。它由四个专门的小型智能体组成，分别负责规划、执行、判断和回答，能够有效协作。判断智能体专注于验证正确性，并能引导其他智能体进行修正，超越了传统的纠错策略。此外，MACT采用混合奖励建模和平衡的测试时间缩放策略，使得每个智能体根据其功能定制不同的缩放策略，从而在参数规模较小的情况下实现更优的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.05545', 'title': 'PRvL: Quantifying the Capabilities and Risks of Large Language Models\n  for PII Redaction', 'url': 'https://huggingface.co/papers/2508.05545', 'abstract': 'A comprehensive analysis of Large Language Models for PII redaction evaluates various architectures and training strategies, providing guidance for accurate, efficient, and privacy-aware redaction systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Redacting Personally Identifiable Information (PII) from unstructured text is critical for ensuring data privacy in regulated domains. While earlier approaches have relied on rule-based systems and domain-specific Named Entity Recognition (NER) models, these methods fail to generalize across formats and contexts. Recent advances in Large Language Models (LLMs) offer a promising alternative, yet the effect of architectural and training choices on redaction performance remains underexplored. LLMs have demonstrated strong performance in tasks that require contextual language understanding, including the redaction of PII in free-form text. Prior work suggests that with appropriate adaptation, LLMs can become effective contextual privacy learners. However, the consequences of architectural and training choices for PII Redaction remain underexplored. In this work, we present a comprehensive analysis of LLMs as privacy-preserving PII Redaction systems. We evaluate a range of LLM architectures and training strategies for their effectiveness in PII Redaction. Our analysis measures redaction performance, semantic preservation, and PII leakage, and compares these outcomes against latency and computational cost. The results provide practical guidance for configuring LLM-based redactors that are accurate, efficient, and privacy-aware. To support reproducibility and real-world deployment, we release PRvL, an open-source suite of fine-tuned models, and evaluation tools for general-purpose PII Redaction. PRvL is built entirely on open-source LLMs and supports multiple inference settings for flexibility and compliance. It is designed to be easily customized for different domains and fully operable within secure, self-managed environments. This enables data owners to perform redactions without relying on third-party services or exposing sensitive content beyond their own infrastructure.', 'score': 1, 'issue_id': 5244, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'dff00258f2cd918b', 'authors': ['Leon Garza', 'Anantaa Kotal', 'Aritran Piplai', 'Lavanya Elluri', 'Prajit Das', 'Aman Chadha'], 'affiliations': ['Amazon Web Services', 'Cisco Systems Inc.', 'Dept. of C.S., The University of Texas at El Paso', 'Texas A&M University-Central Texas'], 'pdf_title_img': 'assets/pdf/title_img/2508.05545.jpg', 'data': {'categories': ['#leakage', '#inference', '#training', '#dataset', '#ethics', '#open_source', '#architecture'], 'emoji': '🔐', 'ru': {'title': 'LLM на страже приватности: новый подход к редактированию персональных данных', 'desc': 'В этой статье представлен комплексный анализ использования больших языковых моделей (LLM) для редактирования персональных данных (PII) в неструктурированном тексте. Авторы оценивают различные архитектуры и стратегии обучения LLM с точки зрения эффективности редактирования, сохранения семантики и предотвращения утечек PII. Результаты исследования предоставляют практические рекомендации по настройке LLM-редакторов, которые являются точными, эффективными и обеспечивают конфиденциальность. В рамках работы также выпущен открытый набор инструментов PRvL для редактирования PII на основе открытых LLM.'}, 'en': {'title': 'Harnessing LLMs for Effective PII Redaction', 'desc': 'This paper analyzes the use of Large Language Models (LLMs) for redacting Personally Identifiable Information (PII) from unstructured text, which is essential for maintaining data privacy. It highlights the limitations of traditional rule-based and domain-specific Named Entity Recognition (NER) systems, which struggle to adapt to various formats and contexts. The authors evaluate different LLM architectures and training strategies to determine their effectiveness in accurately redacting PII while preserving semantic meaning and minimizing leakage. They also introduce PRvL, an open-source suite of fine-tuned models and tools designed for flexible and secure PII redaction in various domains.'}, 'zh': {'title': '大型语言模型助力个人信息去标识化的隐私保护', 'desc': '本文对大型语言模型（LLMs）在个人身份信息（PII）去标识化中的应用进行了全面分析。研究评估了不同的模型架构和训练策略，以提高去标识化的准确性和效率，同时确保数据隐私。通过对去标识化性能、语义保留和PII泄露的测量，提供了配置LLM去标识化系统的实用指导。为了支持可重复性和实际部署，本文发布了PRvL，一个开源的微调模型和评估工具套件，旨在帮助数据拥有者在安全环境中进行去标识化。'}}}, {'id': 'https://huggingface.co/papers/2508.04946', 'title': 'REINA: Regularized Entropy Information-Based Loss for Efficient\n  Simultaneous Speech Translation', 'url': 'https://huggingface.co/papers/2508.04946', 'abstract': 'A novel loss function, REINA, optimizes the latency-quality tradeoff in Simultaneous Speech Translation by adaptively waiting for more input based on information gain.  \t\t\t\t\tAI-generated summary \t\t\t\t Simultaneous Speech Translation (SimulST) systems stream in audio while simultaneously emitting translated text or speech. Such systems face the significant challenge of balancing translation quality and latency. We introduce a strategy to optimize this tradeoff: wait for more input only if you gain information by doing so. Based on this strategy, we present Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. We derive REINA from information theory principles and show that REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we train a SimulST model on French, Spanish and German, both from and into English. Training on only open source or synthetically generated data, we achieve state-of-the-art (SOTA) streaming results for models of comparable size. We also introduce a metric for streaming efficiency, quantitatively showing REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores.', 'score': 1, 'issue_id': 5245, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'aa0ec666492a04d2', 'authors': ['Nameer Hirschkind', 'Joseph Liu', 'Mahesh Kumar Nandwana', 'Xiao Yu'], 'affiliations': ['Roblox'], 'pdf_title_img': 'assets/pdf/title_img/2508.04946.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#synthetic', '#optimization', '#audio', '#machine_translation', '#training'], 'emoji': '🗣️', 'ru': {'title': 'Умное ожидание для быстрого и качественного перевода речи', 'desc': 'Статья представляет новую функцию потерь REINA для оптимизации компромисса между задержкой и качеством в системах одновременного перевода речи. REINA адаптивно ожидает дополнительного ввода на основе прироста информации. Метод основан на принципах теории информации и позволяет обучать адаптивную политику с использованием существующей модели перевода без потоковой передачи. Авторы достигли лучших результатов для моделей сопоставимого размера на нескольких языках, используя только открытые или синтетические данные.'}, 'en': {'title': 'Optimizing Translation: REINA for Better Latency and Quality', 'desc': 'This paper presents a new loss function called REINA, designed to improve Simultaneous Speech Translation (SimulST) systems by optimizing the balance between translation quality and latency. REINA works by adaptively waiting for additional input only when it is expected to enhance the translation quality, based on the concept of information gain. The authors demonstrate that using REINA allows for training a SimulST model that achieves state-of-the-art performance in translating between English and languages like French, Spanish, and German. The results show a significant improvement in the latency-quality tradeoff, achieving up to 21% better performance compared to previous methods.'}, 'zh': {'title': '优化延迟与质量的REINA损失函数', 'desc': '本文提出了一种新颖的损失函数REINA，用于优化同时语音翻译中的延迟与质量的权衡。该方法通过根据信息增益自适应地等待更多输入，从而提高翻译质量。REINA基于信息理论原理，能够训练出一种自适应策略，超越了以往方法的延迟/质量帕累托前沿。通过使用REINA，我们在法语、西班牙语和德语的同时翻译模型上取得了最先进的结果，显示出REINA在延迟和质量之间的权衡改善了多达21%。'}}}, {'id': 'https://huggingface.co/papers/2508.04939', 'title': 'I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating\n  Linguistic Shibboleth Detection in LLM Hiring Evaluations', 'url': 'https://huggingface.co/papers/2508.04939', 'abstract': "A benchmark evaluates Large Language Models' response to linguistic markers that reveal demographic attributes, demonstrating systematic penalization of hedging language despite equivalent content quality.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces a comprehensive benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic markers that can inadvertently reveal demographic attributes such as gender, social class, or regional background. Through carefully constructed interview simulations using 100 validated question-response pairs, we demonstrate how LLMs systematically penalize certain linguistic patterns, particularly hedging language, despite equivalent content quality. Our benchmark generates controlled linguistic variations that isolate specific phenomena while maintaining semantic equivalence, which enables the precise measurement of demographic bias in automated evaluation systems. We validate our approach along multiple linguistic dimensions, showing that hedged responses receive 25.6% lower ratings on average, and demonstrate the benchmark's effectiveness in identifying model-specific biases. This work establishes a foundational framework for detecting and measuring linguistic discrimination in AI systems, with broad applications to fairness in automated decision-making contexts.", 'score': 1, 'issue_id': 5244, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': 'c43270050aadf2f5', 'authors': ['Julia Kharchenko', 'Tanya Roosta', 'Aman Chadha', 'Chirag Shah'], 'affiliations': ['Stanford University, Amazon GenAI, Palo Alto, CA, USA', 'UC Berkeley, Amazon, Saratoga, CA, USA', 'University of Washington, Seattle, WA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.04939.jpg', 'data': {'categories': ['#benchmark', '#ethics'], 'emoji': '🎭', 'ru': {'title': 'Раскрытие скрытой предвзятости языковых моделей', 'desc': 'Статья представляет комплексный бенчмарк для оценки реакции больших языковых моделей (LLM) на лингвистические маркеры, раскрывающие демографические атрибуты. Исследование показывает, что LLM систематически занижают оценки текстов с неуверенными формулировками, несмотря на эквивалентное качество содержания. Бенчмарк генерирует контролируемые лингвистические вариации, сохраняя семантическую эквивалентность, что позволяет точно измерять демографическую предвзятость в системах автоматической оценки. Работа закладывает основу для выявления и измерения лингвистической дискриминации в системах искусственного интеллекта.'}, 'en': {'title': 'Uncovering Bias: How Language Shapes AI Judgments', 'desc': 'This paper presents a benchmark designed to assess how Large Language Models (LLMs) react to subtle linguistic cues that can indicate demographic characteristics. The study reveals that LLMs tend to penalize hedging language, which is a way of expressing uncertainty, even when the content quality remains the same. By using controlled variations in language while keeping the meaning intact, the researchers can accurately measure biases related to demographics in AI evaluations. The findings highlight a significant 25.6% reduction in ratings for hedged responses, underscoring the need for fairness in AI systems.'}, 'zh': {'title': '揭示语言偏见的基准评估', 'desc': '这篇论文介绍了一个评估大型语言模型（LLMs）对语言标记反应的基准，特别是那些可以揭示人口属性的细微语言特征。研究表明，尽管内容质量相当，LLMs对某些语言模式，尤其是模糊语言，存在系统性的惩罚。通过构建100个经过验证的问题-回答对，论文展示了如何在保持语义等价的情况下，生成受控的语言变体，以精确测量自动评估系统中的人口偏见。该研究为检测和测量人工智能系统中的语言歧视建立了基础框架，具有广泛的公平性应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.04699', 'title': 'Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during\n  Multi-Hop Analysis', 'url': 'https://huggingface.co/papers/2508.04699', 'abstract': 'Research investigates reasoning failures in language models for multi-hop question answering, introducing a framework to categorize errors and improve model fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved ("hops"), completeness in capturing relevant information ("coverage"), and cognitive inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts.', 'score': 1, 'issue_id': 5242, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '97e95e6b08388e4f', 'authors': ['Anushka Yadav', 'Isha Nalawade', 'Srujana Pillarichety', 'Yashwanth Babu', 'Reshmi Ghosh', 'Samyadeep Basu', 'Wenlong Zhao', 'Ali Nasaeh', 'Sriram Balasubramanian', 'Soundararajan Srinivasan'], 'affiliations': ['Microsoft', 'University of Maryland, College Park', 'University of Massachusetts, Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2508.04699.jpg', 'data': {'categories': ['#data', '#benchmark', '#reasoning', '#hallucinations', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Разгадка ошибок ИИ: новый взгляд на рассуждения языковых моделей', 'desc': 'Исследование посвящено анализу ошибок рассуждения языковых моделей при ответах на многоэтапные вопросы. Авторы представляют новую систему классификации ошибок, рассматривающую три ключевых аспекта: разнообразие используемых источников, полноту охвата релевантной информации и когнитивную неэффективность. Проведено тщательное аннотирование данных и использованы автоматические метрики для выявления сложных паттернов ошибок. Результаты исследования дают более глубокое понимание когнитивных ограничений современных языковых моделей и предлагают пути улучшения их способности к рассуждениям.'}, 'en': {'title': 'Unraveling Reasoning Failures in Language Models', 'desc': 'This paper investigates the reasoning failures of language models specifically in multi-hop question answering tasks. It introduces a new framework to categorize these errors based on three dimensions: the diversity of source documents, the completeness of relevant information, and cognitive inefficiency. The study uses human annotation and automated metrics to reveal complex error patterns that are often overlooked in traditional accuracy evaluations. The findings aim to enhance the understanding of cognitive limitations in language models and provide guidance for improving their reasoning capabilities.'}, 'zh': {'title': '提升语言模型推理能力的关键', 'desc': '本研究探讨了语言模型在多跳问答中的推理失败，提出了一种框架来分类错误并提高模型的可靠性。研究发现，当前语言模型在处理复杂的多步骤问题时，常常出现幻觉现象，缺乏对错误原因的全面理解。我们引入了一种新的错误分类框架，从源文档的多样性、信息捕捉的完整性和认知效率三个维度进行分析。通过严格的人类标注和自动化指标的支持，我们揭示了隐藏在准确性评估背后的复杂错误模式，为未来语言模型的推理能力提升提供了可行的指导。'}}}, {'id': 'https://huggingface.co/papers/2508.04190', 'title': 'RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation', 'url': 'https://huggingface.co/papers/2508.04190', 'abstract': 'RPCANet++ combines RPCA with deep learning to achieve efficient and interpretable sparse object segmentation by introducing modules for background approximation, object extraction, and image restoration.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust principal component analysis (RPCA) decomposes an observation matrix into low-rank background and sparse object components. This capability has enabled its application in tasks ranging from image restoration to segmentation. However, traditional RPCA models suffer from computational burdens caused by matrix operations, reliance on finely tuned hyperparameters, and rigid priors that limit adaptability in dynamic scenarios. To solve these limitations, we propose RPCANet++, a sparse object segmentation framework that fuses the interpretability of RPCA with efficient deep architectures. Our approach unfolds a relaxed RPCA model into a structured network comprising a Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background feature preservation, while a Deep Contrast Prior Module (DCPM) leverages saliency cues to expedite object extraction. Extensive experiments on diverse datasets demonstrate that RPCANet++ achieves state-of-the-art performance under various imaging scenarios. We further improve interpretability via visual and numerical low-rankness and sparsity measurements. By combining the theoretical strengths of RPCA with the efficiency of deep networks, our approach sets a new baseline for reliable and interpretable sparse object segmentation. Codes are available at our Project Webpage https://fengyiwu98.github.io/rpcanetx.', 'score': 1, 'issue_id': 5247, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '38f7da6b7ad16bbf', 'authors': ['Fengyi Wu', 'Yimian Dai', 'Tianfang Zhang', 'Yixuan Ding', 'Jian Yang', 'Ming-Ming Cheng', 'Zhenming Peng'], 'affiliations': ['Department of Automation, Tsinghua University, Beijing, China', 'PCA Lab, VCIP, College of Computer Science, Nankai University, Tianjin 300350, China', 'School of Information and Communication Engineering and the Laboratory of Imaging Detection and Intelligent Perception, University of Electronic Science and Technology of China, Chengdu, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04190.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#cv', '#interpretability', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'RPCANet++: Интерпретируемая сегментация объектов с глубоким обучением', 'desc': 'RPCANet++ объединяет метод анализа главных компонент (RPCA) с глубоким обучением для эффективной и интерпретируемой сегментации разреженных объектов. Модель включает модули для аппроксимации фона, извлечения объектов и восстановления изображений. Использование памяти и контрастного априорного модуля улучшает сохранение признаков фона и ускоряет выделение объектов. Эксперименты показывают, что RPCANet++ достигает современного уровня производительности в различных сценариях визуализации.'}, 'en': {'title': 'Efficient and Interpretable Sparse Object Segmentation with RPCANet++', 'desc': 'RPCANet++ is a novel framework that integrates Robust Principal Component Analysis (RPCA) with deep learning techniques to enhance sparse object segmentation. It introduces specialized modules for background approximation, object extraction, and image restoration, addressing the computational challenges of traditional RPCA methods. By incorporating a Memory-Augmented Module (MAM) and a Deep Contrast Prior Module (DCPM), the framework improves feature preservation and accelerates object extraction. Extensive experiments show that RPCANet++ achieves superior performance across various imaging scenarios while maintaining interpretability through low-rankness and sparsity metrics.'}, 'zh': {'title': '结合RPCA与深度学习，实现高效可解释的稀疏物体分割', 'desc': 'RPCANet++ 是一种结合了鲁棒主成分分析（RPCA）和深度学习的稀疏物体分割框架。它通过引入背景近似模块、物体提取模块和图像恢复模块，实现了高效且可解释的分割效果。该方法解决了传统 RPCA 模型在计算和适应性方面的局限性，特别是在动态场景中的应用。通过大量实验，RPCANet++ 在不同的成像场景中表现出色，提供了新的可靠和可解释的稀疏物体分割基准。'}}}, {'id': 'https://huggingface.co/papers/2508.04107', 'title': 'Unlocking the Potential of MLLMs in Referring Expression Segmentation\n  via a Light-weight Mask Decode', 'url': 'https://huggingface.co/papers/2508.04107', 'abstract': 'MLLMSeg integrates MLLM vision encoder and LLM features with a lightweight mask decoder to achieve high accuracy in reference expression segmentation with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixel-level dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, a novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose a detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detail-related visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish a light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost. Code is available at https://github.com/jcwang0602/MLLMSeg.', 'score': 1, 'issue_id': 5259, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '57990322bc3ce2a9', 'authors': ['Jingchao Wang', 'Zhijian Wu', 'Dingjiang Huang', 'Yefeng Zheng', 'Hong Wang'], 'affiliations': ['Medical Artificial Intelligence Laboratory, Westlake University', 'School of Data Science and Engineering, East China Normal University', 'School of Life Science and Technology, Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04107.jpg', 'data': {'categories': ['#games', '#multimodal', '#optimization', '#architecture', '#cv'], 'emoji': '🔬', 'ru': {'title': 'Эффективная сегментация изображений с помощью мультимодальных языковых моделей', 'desc': 'MLLMSeg - это новый подход к сегментации изображений по референсным выражениям, объединяющий визуальный энкодер мультимодальной большой языковой модели (MLLM) и особенности большой языковой модели (LLM) с легковесным декодером масок. Метод использует детализированные пространственные признаки из визуального энкодера и семантические признаки из LLM для точного предсказания масок. MLLMSeg достигает высокой точности при сниженных вычислительных затратах по сравнению с существующими методами. Эксперименты показывают, что MLLMSeg превосходит как SAM-based, так и SAM-free конкурентов, обеспечивая лучший баланс между производительностью и стоимостью.'}, 'en': {'title': 'Efficient and Accurate Reference Expression Segmentation with MLLMSeg', 'desc': 'MLLMSeg is a novel framework designed for reference expression segmentation (RES) that combines the strengths of multimodal large models (MLLMs) with a lightweight mask decoder. It effectively utilizes the visual features from the MLLM vision encoder while avoiding the need for an additional visual encoder, which helps reduce computational costs. The framework introduces a detail-enhanced and semantic-consistent feature fusion module (DSFF) that merges visual and semantic features for improved accuracy. Experimental results show that MLLMSeg outperforms existing methods, achieving high accuracy in segmentation while maintaining a low parameter count.'}, 'zh': {'title': '高效精准的参考表达分割新方法', 'desc': 'MLLMSeg是一种新颖的框架，结合了多模态大模型（MLLM）的视觉编码器和大语言模型（LLM）特征，使用轻量级的掩码解码器来实现高精度的参考表达分割。该方法充分利用了MLLM视觉编码器中固有的视觉细节特征，而无需引入额外的视觉编码器。我们还提出了一种细节增强和语义一致的特征融合模块（DSFF），将与细节相关的视觉特征与LLM输出的语义特征进行全面整合。实验结果表明，MLLMSeg在性能和计算成本之间取得了更好的平衡，超越了现有的基于SAM和非SAM的方法。'}}}, {'id': 'https://huggingface.co/papers/2508.02243', 'title': 'I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal\n  Entity Linking', 'url': 'https://huggingface.co/papers/2508.02243', 'abstract': 'A novel LLM-based framework, Intra- and Inter-modal Collaborative Reflections, enhances multimodal entity linking by prioritizing text and using iterative visual clues when necessary, outperforming current state-of-the-art methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal entity linking plays a crucial role in a wide range of applications. Recent advances in large language model-based methods have become the dominant paradigm for this task, effectively leveraging both textual and visual modalities to enhance performance. Despite their success, these methods still face two challenges, including unnecessary incorporation of image data in certain scenarios and the reliance only on a one-time extraction of visual features, which can undermine their effectiveness and accuracy. To address these challenges, we propose a novel LLM-based framework for the multimodal entity linking task, called Intra- and Inter-modal Collaborative Reflections. This framework prioritizes leveraging text information to address the task. When text alone is insufficient to link the correct entity through intra- and inter-modality evaluations, it employs a multi-round iterative strategy that integrates key visual clues from various aspects of the image to support reasoning and enhance matching accuracy. Extensive experiments on three widely used public datasets demonstrate that our framework consistently outperforms current state-of-the-art methods in the task, achieving improvements of 3.2%, 5.1%, and 1.6%, respectively. Our code is available at https://github.com/ziyan-xiaoyu/I2CR/.', 'score': 1, 'issue_id': 5249, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '72638e0c336c8711', 'authors': ['Ziyan Liu', 'Junwen Li', 'Kaiwen Li', 'Tong Ruan', 'Chao Wang', 'Xinyan He', 'Zongyu Wang', 'Xuezhi Cao', 'Jingping Liu'], 'affiliations': ['East China University of Science and Technology, Shanghai, China', 'Meituan, Shanghai, China', 'Shanghai University, Shanghai, China', 'South China University of Technology, Guangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.02243.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#multimodal'], 'emoji': '🔗', 'ru': {'title': 'Умное связывание текста и изображений с помощью ИИ', 'desc': "Предложена новая система для мультимодальной привязки сущностей на основе больших языковых моделей. Она называется 'Внутри- и межмодальные совместные отражения' и отдает приоритет текстовой информации, используя визуальные подсказки только при необходимости. Система применяет итеративный подход для интеграции ключевых визуальных элементов из разных аспектов изображения. Эксперименты показали, что данный метод превосходит современные аналоги на трех общедоступных наборах данных."}, 'en': {'title': 'Enhancing Multimodal Entity Linking with Text-First Strategies', 'desc': 'This paper introduces a new framework called Intra- and Inter-modal Collaborative Reflections for multimodal entity linking, which primarily focuses on text while effectively using visual clues when needed. The framework addresses two main challenges in existing methods: the unnecessary use of image data and the limited extraction of visual features. By employing a multi-round iterative strategy, it enhances the reasoning process by integrating relevant visual information only when text is insufficient. Experimental results show that this approach significantly improves performance over current state-of-the-art techniques across multiple datasets.'}, 'zh': {'title': '文本优先，视觉辅助的多模态实体链接新框架', 'desc': '本文提出了一种基于大型语言模型的新框架，称为内部和外部模态协作反思，旨在增强多模态实体链接。该框架优先利用文本信息来完成任务，当仅依靠文本无法准确链接实体时，采用多轮迭代策略，结合图像中的关键视觉线索进行推理。通过在三个广泛使用的公共数据集上的大量实验，证明该框架在性能上超越了当前最先进的方法，分别提高了3.2%、5.1%和1.6%。这表明，合理利用文本和视觉信息的结合可以显著提升多模态实体链接的准确性。'}}}, {'id': 'https://huggingface.co/papers/2508.05128', 'title': 'Attention Basin: Why Contextual Position Matters in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2508.05128', 'abstract': "The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.", 'score': 0, 'issue_id': 5256, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'a6974044ec3cff79', 'authors': ['Zihao Yi', 'Delong Zeng', 'Zhenqing Ling', 'Haohao Luo', 'Zhe Xu', 'Wei Liu', 'Jian Luan', 'Wanxia Cao', 'Ying Shen'], 'affiliations': ['MiLM Plus, Xiaomi Inc., Beijing, China', 'Sun Yat-sen University, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.05128.jpg', 'data': {'categories': ['#data', '#optimization', '#long_context', '#training', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'Управление вниманием: новый подход к оптимизации работы языковых моделей', 'desc': "Исследование показало, что большие языковые модели (LLM) проявляют позиционное смещение, уделяя больше внимания элементам в начале и конце последовательности входных данных. Этот феномен, названный 'бассейном внимания', влияет на производительность модели. На основе этого наблюдения авторы разработали метод AttnRank, который переупорядочивает входные данные для оптимизации внимания модели. AttnRank - это универсальный метод, не требующий обучения, который значительно улучшает производительность различных LLM в задачах многоэтапного вопросно-ответного анализа и few-shot обучения."}, 'en': {'title': 'Enhancing Model Performance with Attention-Driven Reranking', 'desc': "This paper explores how Large Language Models (LLMs) are influenced by the position of information in their input sequences. It identifies a phenomenon called the attention basin, where models focus more on items at the start and end of a sequence, often overlooking those in the middle. The authors propose a new method called Attention-Driven Reranking (AttnRank) that improves model performance by rearranging input items to match the model's attention preferences. AttnRank is easy to implement, does not require retraining the models, and shows significant performance gains across various tasks and model architectures."}, 'zh': {'title': '提升模型性能的注意力重排序方法', 'desc': '大型语言模型（LLMs）的性能对输入信息的上下文位置非常敏感。我们的实验揭示了一种现象，称为注意力盆地：模型在处理结构化项目时，通常会对序列开头和结尾的项目给予更高的注意力，而忽视中间的项目。我们发现，将更多注意力分配给关键信息对于提升模型性能至关重要。基于这些发现，我们提出了注意力驱动的重排序方法（AttnRank），它通过小规模的校准集来估计模型的内在位置注意力偏好，并重新排列检索到的文档或少量示例，以将最重要的内容与高注意力位置对齐。'}}}, {'id': 'https://huggingface.co/papers/2508.00819', 'title': 'Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models', 'url': 'https://huggingface.co/papers/2508.00819', 'abstract': 'DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.', 'score': 35, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '2241beba3b69f1fd', 'authors': ['Jinsong Li', 'Xiaoyi Dong', 'Yuhang Zang', 'Yuhang Cao', 'Jiaqi Wang', 'Dahua Lin'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.00819.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#diffusion', '#long_context'], 'emoji': '🔄', 'ru': {'title': 'Динамическая адаптация длины раскрывает потенциал диффузионных языковых моделей', 'desc': 'DAEDAL - это новая стратегия динамической адаптации длины для диффузионных больших языковых моделей (DLLM). Она позволяет преодолеть ограничение статически заданной длины генерации, которое снижает эффективность DLLM. DAEDAL работает в два этапа: сначала расширяет начальную короткую длину до подходящей для задачи, а затем во время денойзинга динамически расширяет недостаточные области генерации. Эксперименты показывают, что DAEDAL достигает сравнимой или превосходящей производительности по сравнению с тщательно настроенными базовыми моделями фиксированной длины, одновременно повышая вычислительную эффективность.'}, 'en': {'title': 'Dynamic Length Adaptation for Enhanced DLLM Performance', 'desc': 'DAEDAL is a new method that improves Diffusion Large Language Models (DLLMs) by allowing them to adapt their output length dynamically without needing additional training. Traditional DLLMs require a fixed length for generation, which can limit their performance on complex tasks or waste computational resources. DAEDAL addresses this issue by using internal signals from the model to determine the optimal length for responses, expanding the generation length as needed. This approach not only enhances the quality of the generated text but also increases efficiency, making DLLMs more competitive with Autoregressive models.'}, 'zh': {'title': 'DAEDAL：动态适应长度的去噪新策略', 'desc': 'DAEDAL是一种新颖的无训练去噪策略，能够在扩散大型语言模型中实现动态长度适应，从而提高性能和计算效率。扩散大型语言模型（DLLMs）在生成效率和全局上下文建模方面表现出色，但其静态生成长度限制了实际应用。DAEDAL通过利用模型内部信号，动态调整生成长度，解决了静态长度带来的性能和计算开销问题。实验表明，DAEDAL在性能上与固定长度基线相当，甚至在某些情况下更优，同时提高了计算效率。'}}}, {'id': 'https://huggingface.co/papers/2507.23268', 'title': 'PixNerd: Pixel Neural Field Diffusion', 'url': 'https://huggingface.co/papers/2507.23268', 'abstract': 'Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity. In contrast to their efforts, we propose to model the patch-wise decoding with neural field and present a single-scale, single-stage, efficient, end-to-end solution, coined as pixel neural field diffusion~(PixelNerd). Thanks to the efficient neural field representation in PixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID on ImageNet 512times512 without any complex cascade pipeline or VAE. We also extend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.', 'score': 31, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'f035699955568725', 'authors': ['Shuai Wang', 'Ziteng Gao', 'Chenhui Zhu', 'Weilin Huang', 'Limin Wang'], 'affiliations': ['ByteDance Seed', 'Nanjing University', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2507.23268.jpg', 'data': {'categories': ['#cv', '#diffusion', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений без сложных архитектур', 'desc': 'PixNerd (Pixel Neural Field Diffusion) - это новый метод генерации изображений, работающий в пиксельном пространстве без использования вариационных автоэнкодеров. Он предлагает одноэтапный процесс генерации высококачественных изображений без сложных каскадных архитектур. PixNerd достигает впечатляющих результатов на наборе данных ImageNet, превосходя существующие методы по метрике FID. Кроме того, модель успешно применяется для задачи генерации изображений по текстовому описанию, показывая конкурентоспособные результаты на бенчмарках GenEval и DPG.'}, 'en': {'title': 'Efficient Image Generation with PixNerd: No VAEs, No Hassle!', 'desc': 'Pixel Neural Field Diffusion (PixNerd) introduces a novel approach to image generation that operates in a single-scale and single-stage manner, eliminating the need for variational autoencoders (VAEs) and complex pipelines. This method addresses the issues of accumulated errors and artifacts that arise from traditional two-stage training processes. By utilizing a patch-wise decoding strategy with neural fields, PixNerd achieves impressive performance metrics, such as a 2.15 FID score on ImageNet 256x256. Additionally, it extends its capabilities to text-to-image generation, demonstrating competitive results on various benchmarks.'}, 'zh': {'title': '高效图像生成的新方法：PixNerd', 'desc': 'Pixel Neural Field Diffusion（PixNerd）是一种高效的图像生成方法，采用单尺度、单阶段的流程，无需变分自编码器（VAE）或复杂的管道。该方法通过神经场模型实现了补丁级解码，避免了传统方法中常见的累积误差和解码伪影。PixNerd在ImageNet数据集上取得了2.15的FID分数，显示出其优越的性能。我们还将PixNerd扩展到文本生成图像的应用中，取得了在GenEval和DPG基准测试中的竞争性成绩。'}}}, {'id': 'https://huggingface.co/papers/2508.00414', 'title': 'Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent\n  Foundation Models Training', 'url': 'https://huggingface.co/papers/2508.00414', 'abstract': 'Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro', 'score': 14, 'issue_id': 5169, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '0cd43b7d9f3e1eb5', 'authors': ['Tianqing Fang', 'Zhisong Zhang', 'Xiaoyang Wang', 'Rui Wang', 'Can Qin', 'Yuxuan Wan', 'Jun-Yu Ma', 'Ce Zhang', 'Jiaqi Chen', 'Xiyun Li', 'Hongming Zhang', 'Haitao Mi', 'Dong Yu'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.00414.jpg', 'data': {'categories': ['#data', '#agi', '#training', '#open_source', '#reasoning', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Открытая агентная система ИИ для демократизации передовых технологий', 'desc': 'Cognitive Kernel-Pro - это открытая многомодульная агентная система, улучшающая надежность и производительность ИИ-агентов с помощью курирования данных и новых стратегий во время тестирования. Система фокусируется на создании качественных обучающих данных для Агентных Фундаментальных Моделей в четырех ключевых областях: веб, файлы, код и общие рассуждения. Cognitive Kernel-Pro исследует новые стратегии рефлексии и голосования агентов для повышения их надежности. Система достигает передовых результатов среди открытых и бесплатных агентов на бенчмарке GAIA, превосходя предыдущие ведущие системы.'}, 'en': {'title': 'Democratizing AI Agent Development with Cognitive Kernel-Pro', 'desc': 'Cognitive Kernel-Pro is an open-source framework designed to improve the robustness and performance of AI agents through effective data curation and innovative test-time strategies. It focuses on creating high-quality training data for Agent Foundation Models by constructing queries and trajectories across various domains like web interaction and coding. The framework also introduces new methods for agent reflection and voting, which enhance decision-making capabilities. By achieving state-of-the-art results on the GAIA benchmark, Cognitive Kernel-Pro sets a new standard for accessible and high-performance AI agents.'}, 'zh': {'title': '开放源代码，提升AI代理的未来！', 'desc': 'Cognitive Kernel-Pro是一个开源的多模块代理框架，旨在通过数据整理和新颖的测试策略来增强AI代理的鲁棒性和性能。该框架支持复杂推理、网络交互、编码和自主研究能力，推动下一代人工智能的发展。我们系统地研究了高质量训练数据的整理，重点关注查询、轨迹和可验证答案的构建。Cognitive Kernel-Pro在GAIA上进行了评估，取得了开源和免费代理中的最佳结果，设立了高能力AI代理的新标准。'}}}, {'id': 'https://huggingface.co/papers/2507.23478', 'title': '3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding', 'url': 'https://huggingface.co/papers/2507.23478', 'abstract': '3D-R1 enhances 3D scene understanding through a high-quality synthetic dataset, reinforcement learning with GRPO, and dynamic view selection, achieving significant improvements in reasoning and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large vision-language models (VLMs) have made significant strides in 2D visual understanding tasks, sparking interest in extending these capabilities to 3D scene understanding. However, current 3D VLMs often struggle with robust reasoning and generalization due to limitations in high-quality spatial data and the static nature of viewpoint assumptions. To address these challenges, we propose 3D-R1, a foundation model that enhances the reasoning capabilities of 3D VLMs. Specifically, we first construct a high-quality synthetic dataset with CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1. Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning training process to enhance reasoning capabilities and introduce three reward functions: a perception reward, a semantic similarity reward and a format reward to maintain detection accuracy and answer semantic precision. Furthermore, we introduce a dynamic view selection strategy that adaptively chooses the most informative perspectives for 3D scene understanding. Extensive experiments demonstrate that 3D-R1 delivers an average improvement of 10% across various 3D scene benchmarks, highlighting its effectiveness in enhancing reasoning and generalization in 3D scene understanding. Code: https://github.com/AIGeeksGroup/3D-R1. Website: https://aigeeksgroup.github.io/3D-R1.', 'score': 6, 'issue_id': 5155, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'f5e99fc10e8b9ad5', 'authors': ['Ting Huang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['School of Computer Science, Peking University', 'Shanghai University of Engineering Science'], 'pdf_title_img': 'assets/pdf/title_img/2507.23478.jpg', 'data': {'categories': ['#benchmark', '#3d', '#dataset', '#reasoning', '#rlhf', '#synthetic'], 'emoji': '🧠', 'ru': {'title': '3D-R1: Революция в понимании трехмерных сцен с помощью ИИ', 'desc': 'Модель 3D-R1 улучшает понимание трехмерных сцен с помощью высококачественного синтетического датасета и обучения с подкреплением. Она использует динамический выбор ракурсов для более информативного анализа 3D-сцен. 3D-R1 применяет функции вознаграждения для улучшения точности восприятия и семантической точности ответов. Эксперименты показывают значительное улучшение результатов на различных бенчмарках трехмерных сцен.'}, 'en': {'title': 'Enhancing 3D Scene Understanding with 3D-R1', 'desc': "The paper presents 3D-R1, a model designed to improve 3D scene understanding by addressing the limitations of existing vision-language models (VLMs). It introduces a high-quality synthetic dataset called Scene-30K, which is used to enhance the model's reasoning capabilities. The training process incorporates reinforcement learning with a GRPO policy and utilizes multiple reward functions to ensure accuracy and semantic precision. Additionally, a dynamic view selection strategy is implemented to optimize the perspectives used for analyzing 3D scenes, resulting in a notable average improvement of 10% in performance across various benchmarks."}, 'zh': {'title': '3D-R1：提升3D场景理解的智能模型', 'desc': '3D-R1 是一个增强 3D 场景理解的基础模型，利用高质量的合成数据集和强化学习方法来提升推理能力。我们构建了一个名为 Scene-30K 的合成数据集，作为 3D-R1 的冷启动初始化数据。通过引入动态视角选择策略，3D-R1 能够自适应选择最具信息量的视角进行 3D 场景理解。实验结果表明，3D-R1 在多个 3D 场景基准测试中平均提升了 10%，有效增强了推理和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.23361', 'title': 'SWE-Exp: Experience-Driven Software Issue Resolution', 'url': 'https://huggingface.co/papers/2507.23361', 'abstract': 'SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.', 'score': 6, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'e16fe4dad5f61553', 'authors': ['Silin Chen', 'Shaoxin Lin', 'Xiaodong Gu', 'Yuling Shi', 'Heng Lian', 'Longfei Yun', 'Dong Chen', 'Weiguo Sun', 'Lin Cao', 'Qianxiang Wang'], 'affiliations': ['Huawei, China', 'Shanghai Jiao Tong University, China', 'UC San Diego, United States', 'Xidian University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23361.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Опыт - ключ к эффективному решению проблем в ПО', 'desc': 'SWE-Exp - это новый подход к решению проблем в программном обеспечении, использующий накопленный опыт предыдущих попыток исправления ошибок. Метод создает многоуровневый банк опыта, который включает как успешные, так и неудачные попытки решения проблем. SWE-Exp извлекает многоуровневые знания о решении проблем - от общего понимания до конкретных изменений в коде. Эксперименты показывают, что SWE-Exp достигает наилучших результатов в решении проблем на тестовом наборе SWE-bench-Verified среди агентов с открытым исходным кодом.'}, 'en': {'title': 'Transforming Software Issue Resolution with Experience-Driven Learning', 'desc': 'SWE-Exp is a novel approach that enhances software issue resolution by utilizing past experiences of agents to improve their performance. Unlike traditional agents that do not retain knowledge, SWE-Exp builds an experience bank that captures both successful and failed attempts at resolving issues. This allows the system to learn from previous repairs and apply that knowledge to new problems, leading to more efficient and effective resolutions. The method has demonstrated a significant improvement in resolution rates, showcasing a shift from random exploration to a more strategic, experience-driven process.'}, 'zh': {'title': '经验驱动的软件问题解决新方法', 'desc': 'SWE-Exp是一种增强软件问题解决能力的方法，通过系统地积累和利用过去代理的修复经验，提高了解决率。当前的代理在处理问题时缺乏记忆，无法重用之前的知识，导致重复探索失败的路径。SWE-Exp通过建立一个多层次的经验库，提取成功和失败的修复尝试中的可重用知识，从而实现跨问题的持续学习。实验表明，SWE-Exp在开源代理框架下的SWE-bench-Verified上达到了41.6%的最佳解决率，标志着自动化软件工程代理的一个新范式。'}}}, {'id': 'https://huggingface.co/papers/2508.00265', 'title': 'Multimodal Referring Segmentation: A Survey', 'url': 'https://huggingface.co/papers/2508.00265', 'abstract': "A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal referring segmentation aims to segment target objects in visual scenes, such as images, videos, and 3D scenes, based on referring expressions in text or audio format. This task plays a crucial role in practical applications requiring accurate object perception based on user instructions. Over the past decade, it has gained significant attention in the multimodal community, driven by advances in convolutional neural networks, transformers, and large language models, all of which have substantially improved multimodal perception capabilities. This paper provides a comprehensive survey of multimodal referring segmentation. We begin by introducing this field's background, including problem definitions and commonly used datasets. Next, we summarize a unified meta architecture for referring segmentation and review representative methods across three primary visual scenes, including images, videos, and 3D scenes. We further discuss Generalized Referring Expression (GREx) methods to address the challenges of real-world complexity, along with related tasks and practical applications. Extensive performance comparisons on standard benchmarks are also provided. We continually track related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", 'score': 5, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '1604e587f6dc8177', 'authors': ['Henghui Ding', 'Song Tang', 'Shuting He', 'Chang Liu', 'Zuxuan Wu', 'Yu-Gang Jiang'], 'affiliations': ['ByteDance Inc.', 'Fudan University', 'Shanghai University of Finance and Economics'], 'pdf_title_img': 'assets/pdf/title_img/2508.00265.jpg', 'data': {'categories': ['#cv', '#multimodal', '#3d', '#survey', '#video', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Мультимодальная сегментация: от пикселей к пониманию', 'desc': 'Эта статья представляет собой обзор методов мультимодальной сегментации по ссылкам, охватывающий достижения в области сверточных нейронных сетей, трансформеров и больших языковых моделей. Авторы рассматривают задачу сегментации объектов на изображениях, видео и в 3D-сценах на основе текстовых или аудио инструкций. В работе представлена унифицированная мета-архитектура для сегментации по ссылкам и обзор репрезентативных методов для различных визуальных сцен. Также обсуждаются обобщенные методы выражения ссылок (GREx) для решения проблем сложности реального мира.'}, 'en': {'title': 'Enhancing Object Segmentation with Multimodal Instructions', 'desc': 'This paper surveys the field of multimodal referring segmentation, which focuses on identifying and segmenting objects in visual content based on textual or audio instructions. It highlights the advancements made through convolutional neural networks, transformers, and large language models that enhance the ability to understand and process multimodal data. The authors present a unified meta architecture for referring segmentation and review various methods applicable to images, videos, and 3D scenes. Additionally, they discuss challenges in real-world applications and provide performance comparisons on standard benchmarks to evaluate the effectiveness of different approaches.'}, 'zh': {'title': '多模态指向分割的全面调查', 'desc': '多模态指向分割旨在根据文本或音频指令在视觉场景中分割目标物体，如图像、视频和3D场景。该任务在需要根据用户指令进行准确物体感知的实际应用中至关重要。近年来，卷积神经网络、变换器和大型语言模型的进步显著提升了多模态感知能力。本文提供了多模态指向分割的全面调查，涵盖了背景介绍、统一的元架构、代表性方法及其在不同视觉场景中的应用。'}}}, {'id': 'https://huggingface.co/papers/2507.23348', 'title': 'SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution', 'url': 'https://huggingface.co/papers/2507.23348', 'abstract': "SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.", 'score': 4, 'issue_id': 5154, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '28b58ecd36ac995b', 'authors': ['Han Li', 'Yuling Shi', 'Shaoxin Lin', 'Xiaodong Gu', 'Heng Lian', 'Xin Wang', 'Yantao Jia', 'Tao Huang', 'Qianxiang Wang'], 'affiliations': ['Huawei China', 'Shanghai Jiao Tong University China', 'Xidian University China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23348.jpg', 'data': {'categories': ['#optimization', '#agents', '#open_source', '#reasoning', '#games', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Дебаты ИИ-агентов для улучшения разработки ПО', 'desc': 'SWE-Debate - это новая система для решения проблем в разработке программного обеспечения, использующая несколько ИИ-агентов. Система организует структурированные дебаты между агентами, каждый из которых предлагает свой подход к локализации и исправлению ошибок. Этот метод позволяет находить более комплексные решения, охватывающие различные части кодовой базы. В результате SWE-Debate превосходит существующие подходы в локализации проблем и планировании исправлений.'}, 'en': {'title': 'Empowering Software Issue Resolution through Competitive Multi-Agent Debate', 'desc': 'SWE-Debate is a multi-agent framework designed to improve issue resolution in software engineering by fostering diverse reasoning among agents. It utilizes large language models to enhance the reasoning capabilities of autonomous agents, allowing them to tackle complex tasks more effectively. By organizing a structured debate among agents with different perspectives, SWE-Debate helps identify broader issue patterns and achieve better localization of software faults. The framework has demonstrated significant improvements in performance on the SWE-bench benchmark, setting new standards in open-source agent frameworks.'}, 'zh': {'title': 'SWE-Debate：多样化推理促进软件问题解决', 'desc': 'SWE-Debate是一个竞争性的多智能体框架，旨在通过促进多样化的推理来增强软件工程中的问题解决能力。该框架利用大型语言模型的推理能力，帮助智能体在复杂的软件工程任务中进行自主探索。与以往的独立探索方法不同，SWE-Debate通过组织智能体之间的辩论，鼓励不同的推理路径，从而更好地定位问题并制定修复计划。实验结果表明，SWE-Debate在开源智能体框架中达到了新的最先进水平，显著优于基线方法。'}}}, {'id': 'https://huggingface.co/papers/2508.00782', 'title': 'SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware\n  Video Generation', 'url': 'https://huggingface.co/papers/2508.00782', 'abstract': 'SpA2V generates realistic videos aligned with input audio by leveraging spatial auditory cues and integrating them into diffusion models through video scene layouts.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios.', 'score': 3, 'issue_id': 5160, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '75fd2b7336810b40', 'authors': ['Kien T. Pham', 'Yingqing He', 'Yazhou Xing', 'Qifeng Chen', 'Long Chen'], 'affiliations': ['Hong Kong University of Science and Technology Clear Water Bay, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.00782.jpg', 'data': {'categories': ['#audio', '#games', '#video', '#diffusion', '#multimodal'], 'emoji': '🎬', 'ru': {'title': 'Звук в движении: от аудио к реалистичному видео', 'desc': 'SpA2V - это новый подход к генерации видео на основе аудио, который использует пространственные звуковые сигналы для создания реалистичных видеороликов. Система работает в два этапа: сначала создается план видеосцены с помощью мультимодальной языковой модели, затем этот план используется для управления диффузионной моделью генерации видео. SpA2V позволяет получать видео с точным семантическим и пространственным соответствием входному аудио. Эксперименты показывают превосходство этого метода над существующими подходами к аудио-видео синтезу.'}, 'en': {'title': 'SpA2V: Bridging Audio and Video with Spatial Awareness', 'desc': 'The paper introduces SpA2V, a novel framework for generating realistic videos that align with input audio by utilizing spatial auditory cues. Unlike previous methods that focus mainly on semantic information, SpA2V incorporates spatial attributes such as location and movement derived from audio properties like loudness and frequency. The generation process is divided into two stages: first, it creates Video Scene Layouts (VSLs) using a modified machine learning model to capture both spatial and semantic cues from the audio. Then, it integrates these VSLs into diffusion models for video generation, resulting in videos that are both semantically and spatially accurate to the audio input.'}, 'zh': {'title': '利用空间听觉线索生成真实视频', 'desc': 'SpA2V是一种生成与输入音频对齐的真实视频的新框架。它通过利用空间听觉线索，将这些线索整合到扩散模型中，从而生成视频场景布局。与传统方法不同，SpA2V不仅关注音频的语义信息，还考虑了声音的空间属性，如位置和运动方向。实验表明，SpA2V在生成与输入音频具有高语义和空间一致性的真实视频方面表现优异。'}}}, {'id': 'https://huggingface.co/papers/2508.00454', 'title': 'Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges', 'url': 'https://huggingface.co/papers/2508.00454', 'abstract': 'An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.', 'score': 3, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '7d97f0b64c1261dd', 'authors': ['Yuqi Tang', 'Kehua Feng', 'Yunfeng Wang', 'Zhiwen Chen', 'Chengfei Lv', 'Gang Yu', 'Qiang Zhang', 'Keyan Ding'], 'affiliations': ['Alibaba Group', 'College of Computer Science and Technology, Zhejiang University', 'ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University', 'ZJU-UIUC Institute, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00454.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#inference', '#alignment', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'Эффективная оценка диалогов: мудрость многих в одной модели', 'desc': 'Эта статья представляет эффективный метод оценки качества многоэтапных диалогов с использованием больших языковых моделей (LLM). Авторы предлагают агрегировать суждения нескольких LLM в единую модель, что позволяет сохранить преимущества разнообразных оценок, но значительно снизить вычислительные затраты. Метод показал превосходные результаты на семи эталонных наборах данных для оценки диалогов. Предложенный подход обеспечивает быструю и гибкую оценку качества диалогов, сохраняя при этом надежность и согласованность результатов.'}, 'en': {'title': 'Efficient Dialogue Evaluation: Harnessing Collective Wisdom of LLMs', 'desc': 'This paper introduces an efficient multi-turn dialogue evaluator that combines the judgments of multiple large language models (LLMs) to assess dialogue quality. Traditional methods using a single LLM as a judge often face biases that affect evaluation reliability. The proposed method aggregates the preferences of several LLMs into one model, maintaining the benefits of diverse feedback while significantly lowering computational costs. Experiments show that this new approach outperforms existing methods in various evaluation scenarios, proving its effectiveness and efficiency.'}, 'zh': {'title': '高效的多轮对话评估器：聚合智慧，降低成本', 'desc': '本文提出了一种高效的多轮对话评估器，通过将多个大型语言模型（LLM）的判断汇聚成一个单一模型来评估对话质量，从而降低计算成本。当前的评估方法主要依赖于“LLM作为评审”的模式，但这种方法常常受到偏见的影响，导致评估结果的不可靠性。为了解决这个问题，本文的方法利用多个LLM作为评审，并将它们的偏好知识汇聚到一个模型中，从而保留多评审反馈的优势，同时显著减少评估成本。实验结果表明，该方法在多种对话评估基准上优于现有的基线，展示了其高效性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2507.19634', 'title': 'MCIF: Multimodal Crosslingual Instruction-Following Benchmark from\n  Scientific Talks', 'url': 'https://huggingface.co/papers/2507.19634', 'abstract': "MCIF is a multilingual, human-annotated benchmark for evaluating instruction-following in crosslingual, multimodal settings using scientific talks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.", 'score': 3, 'issue_id': 5169, 'pub_date': '2025-07-25', 'pub_date_card': {'ru': '25 июля', 'en': 'July 25', 'zh': '7月25日'}, 'hash': '6c681493be72e8eb', 'authors': ['Sara Papi', 'Maike Züfle', 'Marco Gaido', 'Beatrice Savoldi', 'Danni Liu', 'Ioannis Douros', 'Luisa Bentivogli', 'Jan Niehues'], 'affiliations': ['Fondazione Bruno Kessler (Italy)', 'Karlsruhe Institute of Technology (Germany)', 'Translated (Italy)'], 'pdf_title_img': 'assets/pdf/title_img/2507.19634.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#multilingual', '#open_source', '#long_context', '#machine_translation'], 'emoji': '🌐', 'ru': {'title': 'MCIF: Первый многоязычный мультимодальный тест для оценки MLLM', 'desc': 'MCIF - это многоязычный эталонный тест с аннотациями от людей для оценки выполнения инструкций в кросс-языковых мультимодальных средах, использующий научные доклады. Он охватывает три основные модальности - речь, зрение и текст - на четырех языках, позволяя комплексно оценивать способности мультимодальных языковых моделей (MLLM) интерпретировать инструкции на разных языках и комбинировать их с мультимодальной контекстной информацией. MCIF создан для преодоления ограничений существующих тестов, которые часто ограничены английским языком, фокусируются на одной модальности и коротких контекстах. Этот бенчмарк выпущен под лицензией CC-BY 4.0 для поощрения открытых исследований и прогресса в разработке MLLM.'}, 'en': {'title': 'MCIF: A New Benchmark for Multilingual Instruction-Following in Multimodal AI', 'desc': 'MCIF is a new benchmark designed to evaluate how well multilingual, multimodal language models can follow instructions in different languages and formats. It focuses on three modalities: speech, vision, and text, and includes human annotations to ensure quality assessments. Unlike previous benchmarks, MCIF allows for both short and long context evaluations across four languages: English, German, Italian, and Chinese. This comprehensive approach aims to enhance the understanding of model performance in real-world, crosslingual scenarios.'}, 'zh': {'title': 'MCIF：跨语言多模态指令跟随的评估新基准', 'desc': 'MCIF是一个多语言的人类标注基准，用于评估跨语言、多模态环境下的指令跟随能力。它结合了文本、语音和视觉三种核心模态，并支持英语、德语、意大利语和中文四种语言。MCIF的设计旨在填补现有基准在多语言和多模态评估方面的不足，特别是在长短文本输入的情况下。通过MCIF，研究人员可以更全面地评估多模态大语言模型的性能和能力。'}}}, {'id': 'https://huggingface.co/papers/2508.00632', 'title': 'Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings', 'url': 'https://huggingface.co/papers/2508.00632', 'abstract': 'A multi-agent system using an omni-modal evaluation metric improves JavaScript game and animation generation but struggles with custom assets and audio-visual feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.   We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.   We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.   We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.', 'score': 2, 'issue_id': 5165, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '3e4a605a070fb44f', 'authors': ['Alexia Jolicoeur-Martineau'], 'affiliations': ['Samsung SAIL Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2508.00632.jpg', 'data': {'categories': ['#audio', '#multimodal', '#games', '#optimization', '#video', '#agents'], 'emoji': '🎮', 'ru': {'title': 'Мультиагентная система для создания игр: прогресс и проблемы', 'desc': 'Представлена система мультиагентного генерирования JavaScript-игр и анимаций с использованием омнимодальной метрики оценки. Система AVR-Agent выбирает релевантные ассеты, генерирует несколько вариантов кода и итеративно улучшает их на основе обратной связи. Метрика AVR-Eval сравнивает качество мультимедийного контента, используя аудиовизуальные записи. Эксперименты показали, что система значительно улучшает генерацию контента по сравнению с одноразовой генерацией, но модели все еще испытывают трудности с эффективным использованием пользовательских ассетов и аудиовизуальной обратной связи.'}, 'en': {'title': 'Enhancing Game Generation with Multi-Agent Systems and AVR-Eval', 'desc': 'This paper presents a multi-agent system designed to enhance the generation of JavaScript games and animations using a new evaluation metric called AVR-Eval. AVR-Eval assesses multimedia content quality by comparing Audio-Visual Recordings (AVRs) through an omni-modal model that processes text, video, and audio. The system, AVR-Agent, generates code by selecting relevant multimedia assets and iteratively improving the output based on feedback from AVR-Eval. Despite achieving higher success rates in generated content, the system struggles with custom assets and effective audio-visual feedback, indicating a gap between human creativity and machine-generated content.'}, 'zh': {'title': '多智能体系统提升JavaScript游戏生成质量', 'desc': '本论文提出了一种多智能体系统，利用全模态评估指标来改善JavaScript游戏和动画的生成。我们开发了AVR-Eval，这是一种相对评估多媒体内容质量的新指标，能够有效区分优质和劣质内容。AVR-Agent是一个多智能体系统，能够从多媒体资产库中生成JavaScript代码，并通过迭代反馈不断优化生成的内容。尽管生成的内容在胜率上优于单次生成的内容，但模型在利用自定义资产和音视频反馈方面仍然存在困难，显示出人类与机器内容创作方法之间的根本差异。'}}}, {'id': 'https://huggingface.co/papers/2507.22720', 'title': 'Investigating Hallucination in Conversations for Low Resource Languages', 'url': 'https://huggingface.co/papers/2507.22720', 'abstract': "LLMs generate fewer hallucinations in Mandarin compared to Hindi and Farsi across multiple models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.", 'score': 2, 'issue_id': 5155, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': 'c7f5db5f58895f4f', 'authors': ['Amit Das', 'Md. Najib Hasan', 'Souvika Sarkar', 'Zheng Zhang', 'Fatemeh Jamshidi', 'Tathagata Bhattacharya', 'Nilanjana Raychawdhury', 'Dongji Feng', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon GenAI', 'Auburn University', 'Auburn University at Montgomery', 'California State Polytechnic University Pomona', 'Gustavus Adolphus College', 'Meta', 'Murray State University', 'Stanford University', 'University of North Alabama', 'Wichita State University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22720.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#hallucinations'], 'emoji': '🗣️', 'ru': {'title': 'Языковые модели меньше галлюцинируют по-китайски', 'desc': 'Исследование посвящено проблеме галлюцинаций в больших языковых моделях (LLM) на примере трех языков: хинди, фарси и мандаринского китайского. Авторы провели комплексный анализ фактических и лингвистических ошибок в этих языках для нескольких популярных моделей, включая GPT-3.5, GPT-4, Llama-3.1 и другие. Результаты показали, что LLM генерируют значительно меньше галлюцинаций на мандаринском китайском по сравнению с хинди и фарси. Это исследование расширяет понимание проблемы галлюцинаций за пределы английского языка, что важно для повышения надежности и эффективности LLM.'}, 'en': {'title': 'Mandarin LLMs: Fewer Hallucinations, More Accuracy!', 'desc': 'This paper investigates the phenomenon of hallucinations in Large Language Models (LLMs) across three languages: Mandarin, Hindi, and Farsi. Hallucinations refer to instances where the models generate incorrect or misleading information. The study analyzes conversational data from various LLMs, including GPT-3.5 and GPT-4o, to compare the frequency of these errors. The findings reveal that LLMs exhibit fewer hallucinations in Mandarin compared to the higher rates observed in Hindi and Farsi, highlighting the need for language-specific improvements in model training.'}, 'zh': {'title': '普通话中的幻觉现象较少', 'desc': '大型语言模型（LLMs）在生成文本方面表现出色，但它们有时会产生不准确的信息，这被称为“幻觉”。本研究探讨了在普通话、印地语和法尔西语中，LLMs的幻觉现象。我们分析了多个模型（如GPT-3.5、GPT-4o等）在这三种语言中的事实和语言错误。结果显示，LLMs在普通话中产生的幻觉响应较少，而在印地语和法尔西语中则显著更多。'}}}, {'id': 'https://huggingface.co/papers/2508.00823', 'title': 'IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation', 'url': 'https://huggingface.co/papers/2508.00823', 'abstract': 'IGL-Nav uses an incremental 3D Gaussian representation for efficient and accurate image-goal navigation in 3D space, outperforming existing methods and applicable in real-world settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.', 'score': 1, 'issue_id': 5154, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': 'a4651adceaac80f7', 'authors': ['Wenxuan Guo', 'Xiuwei Xu', 'Hang Yin', 'Ziwei Wang', 'Jianjiang Feng', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Nanyang Technological University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00823.jpg', 'data': {'categories': ['#robotics', '#3d', '#optimization', '#agents', '#games'], 'emoji': '🧭', 'ru': {'title': 'Навигация в 3D с помощью инкрементальных гауссианов', 'desc': 'IGL-Nav - это новый метод навигации по изображению-цели в трехмерном пространстве. Он использует инкрементальное представление 3D гауссианов для эффективной и точной локализации целевого изображения. Метод превосходит существующие подходы, сочетая дискретное сопоставление пространства и оптимизацию через дифференцируемый рендеринг. IGL-Nav применим в реальных условиях и может работать с произвольными ракурсами целевых изображений.'}, 'en': {'title': 'Efficient 3D Navigation with Incremental Gaussian Localization', 'desc': 'IGL-Nav introduces an innovative approach to image-goal navigation in 3D environments using an incremental 3D Gaussian representation. This method enhances localization accuracy by updating the scene representation as new images are processed, allowing for efficient navigation. Unlike traditional methods that struggle with geometric relationships, IGL-Nav utilizes geometric information for effective discrete space matching and fine target pose optimization. The framework demonstrates significant improvements over existing techniques and is suitable for real-world applications, including robotic platforms.'}, 'zh': {'title': '增量式3D高斯导航：高效准确的图像目标定位', 'desc': 'IGL-Nav是一种增量式3D高斯定位框架，旨在提高图像目标导航的效率和准确性。该方法通过可渲染的3D高斯表示来建模3D环境与目标图像之间的几何关系，克服了传统方法的局限性。IGL-Nav通过前馈单目预测逐步更新场景表示，并利用几何信息进行粗略定位，最终通过可微渲染优化精确确定目标位置。实验结果表明，IGL-Nav在多种配置下显著超越了现有的最先进方法，并能够在真实世界的机器人平台上应用。'}}}, {'id': 'https://huggingface.co/papers/2508.10433', 'title': 'We-Math 2.0: A Versatile MathBook System for Incentivizing Visual\n  Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2508.10433', 'abstract': "We-Math 2.0 enhances MLLMs' mathematical reasoning through a structured knowledge system, model-centric data space modeling, and reinforcement learning, demonstrating competitive performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.", 'score': 130, 'issue_id': 5363, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': 'fabdc1bdb3fd820b', 'authors': ['Runqi Qiao', 'Qiuna Tan', 'Peiqing Yang', 'Yanzi Wang', 'Xiaowan Wang', 'Enhui Wan', 'Sitong Zhou', 'Guanting Dong', 'Yuchen Zeng', 'Yida Xu', 'Jie Wang', 'Chong Sun', 'Chen Li', 'Honggang Zhang'], 'affiliations': ['BUPT', 'Tsinghua University', 'WeChat Vision, Tencent Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2508.10433.jpg', 'data': {'categories': ['#dataset', '#optimization', '#benchmark', '#reasoning', '#training', '#data', '#rl'], 'emoji': '🧮', 'ru': {'title': 'Усиление математических способностей ИИ через структурированные знания и обучение с подкреплением', 'desc': 'We-Math 2.0 - это унифицированная система для улучшения математических рассуждений мультимодальных больших языковых моделей (MLLM). Она включает структурированную систему математических знаний, моделирование пространства данных с ориентацией на модель и обучение с подкреплением. Система состоит из иерархической базы знаний MathBook, наборов данных MathBook-Standard и Pro, двухэтапного фреймворка обучения с подкреплением MathBook-RL. We-Math 2.0 демонстрирует конкурентоспособные результаты на существующих бенчмарках и новом комплексном тесте MathBookEval.'}, 'en': {'title': 'Empowering MLLMs with Enhanced Mathematical Reasoning', 'desc': 'We-Math 2.0 is a system designed to improve the mathematical reasoning abilities of Multimodal Large Language Models (MLLMs). It incorporates a structured knowledge system, model-centric data space modeling, and a reinforcement learning approach to enhance performance. The system includes a hierarchical knowledge framework, a comprehensive dataset for training, and a two-stage reinforcement learning strategy to align models with reasoning tasks. Experimental results indicate that We-Math 2.0 achieves competitive performance on various benchmarks, demonstrating its effectiveness in advancing mathematical reasoning capabilities.'}, 'zh': {'title': 'We-Math 2.0：提升数学推理的智能系统', 'desc': 'We-Math 2.0 是一个增强多模态大型语言模型（MLLMs）数学推理能力的系统。它通过构建结构化的数学知识体系、以模型为中心的数据空间建模和基于强化学习的训练方法来实现这一目标。该系统包括五级层次的知识点和基本原则，确保了广泛的概念覆盖和灵活性。实验结果表明，We-Math 2.0 在多个基准测试中表现出色，显示出其在数学推理方面的良好泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.10711', 'title': 'NextStep-1: Toward Autoregressive Image Generation with Continuous\n  Tokens at Scale', 'url': 'https://huggingface.co/papers/2508.10711', 'abstract': 'NextStep-1, a 14B autoregressive model with a 157M flow matching head, achieves state-of-the-art performance in text-to-image generation and image editing by processing discrete text tokens and continuous image tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.', 'score': 118, 'issue_id': 5364, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '47af2f1f4d3a49d6', 'authors': ['NextStep Team', 'Chunrui Han', 'Guopeng Li', 'Jingwei Wu', 'Quan Sun', 'Yan Cai', 'Yuang Peng', 'Zheng Ge', 'Deyu Zhou', 'Haomiao Tang', 'Hongyu Zhou', 'Kenkun Liu', 'Ailin Huang', 'Bin Wang', 'Changxin Miao', 'Deshan Sun', 'En Yu', 'Fukun Yin', 'Gang Yu', 'Hao Nie', 'Haoran Lv', 'Hanpeng Hu', 'Jia Wang', 'Jian Zhou', 'Jianjian Sun', 'Kaijun Tan', 'Kang An', 'Kangheng Lin', 'Liang Zhao', 'Mei Chen', 'Peng Xing', 'Rui Wang', 'Shiyu Liu', 'Shutao Xia', 'Tianhao You', 'Wei Ji', 'Xianfang Zeng', 'Xin Han', 'Xuelin Zhang', 'Yana Wei', 'Yanming Xu', 'Yimin Jiang', 'Yingming Wang', 'Yu Zhou', 'Yucheng Han', 'Ziyang Meng', 'Binxing Jiao', 'Daxin Jiang', 'Xiangyu Zhang', 'Yibo Zhu'], 'affiliations': ['StepFun'], 'pdf_title_img': 'assets/pdf/title_img/2508.10711.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#cv', '#architecture', '#diffusion', '#training'], 'emoji': '🎨', 'ru': {'title': 'Революция в генерации изображений: NextStep-1 объединяет текст и визуализацию', 'desc': 'NextStep-1 - это новая модель для генерации изображений по тексту, состоящая из авторегрессионной части (14 млрд параметров) и головы для сопоставления потоков (157 млн параметров). Модель обрабатывает дискретные текстовые токены и непрерывные токены изображений, достигая высоких результатов в генерации и редактировании изображений. В отличие от предыдущих подходов, NextStep-1 не использует тяжелые диффузионные модели или векторное квантование. Авторы планируют открыть исходный код и модели для исследовательского сообщества.'}, 'en': {'title': 'NextStep-1: Revolutionizing Text-to-Image Generation with Autoregressive Power', 'desc': "NextStep-1 is a 14 billion parameter autoregressive model designed for text-to-image generation and image editing. It uniquely combines discrete text tokens with continuous image tokens, using a 157 million parameter flow matching head to enhance performance. This model outperforms existing methods by avoiding the computational heaviness of diffusion models and the quantization loss associated with vector quantization. The results demonstrate its ability to generate high-fidelity images and effectively edit them, showcasing the model's versatility and potential for future research."}, 'zh': {'title': 'NextStep-1：文本到图像生成的新突破', 'desc': 'NextStep-1是一种14B的自回归模型，配备157M的流匹配头，专注于文本到图像生成和图像编辑。与传统的自回归模型不同，它通过处理离散的文本标记和连续的图像标记，避免了量化损失。该模型在文本到图像生成任务中表现出色，能够生成高保真的图像。此外，NextStep-1在图像编辑方面也展现了强大的能力，证明了其方法的强大和多样性。'}}}, {'id': 'https://huggingface.co/papers/2508.10881', 'title': 'ToonComposer: Streamlining Cartoon Production with Generative\n  Post-Keyframing', 'url': 'https://huggingface.co/papers/2508.10881', 'abstract': 'ToonComposer is a generative model that unifies inbetweening and colorization in cartoon production, using sparse sketches and a cartoon adaptation method to improve visual quality and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.', 'score': 42, 'issue_id': 5369, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': 'a47c9a9f44da1314', 'authors': ['Lingen Li', 'Guangzhi Wang', 'Zhaoyang Zhang', 'Yaowei Li', 'Xiaoyu Li', 'Qi Dou', 'Jinwei Gu', 'Tianfan Xue', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.10881.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#games', '#cv', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Революция в анимации: единая модель для промежуточных кадров и раскрашивания', 'desc': 'ToonComposer - это генеративная модель, объединяющая процессы создания промежуточных кадров и раскрашивания в производстве мультфильмов. Модель использует разреженные эскизы и метод адаптации к мультипликационному стилю для улучшения визуального качества и эффективности. ToonComposer применяет механизм внедрения разреженных эскизов для точного контроля с помощью ключевых кадров. Модель превосходит существующие методы по визуальному качеству, согласованности движений и эффективности производства.'}, 'en': {'title': 'Streamlining Cartoon Production with ToonComposer', 'desc': 'ToonComposer is a generative model designed to streamline cartoon production by integrating inbetweening and colorization into one process. It utilizes sparse sketches, allowing artists to provide minimal input while maintaining high visual quality and motion accuracy. The model adapts a modern video foundation to the cartoon domain, ensuring that it can handle large motions without losing temporal consistency. By reducing the manual effort required in traditional animation workflows, ToonComposer enhances both efficiency and flexibility for artists.'}, 'zh': {'title': 'ToonComposer：卡通制作的智能新方式', 'desc': 'ToonComposer是一种生成模型，旨在将卡通制作中的插帧和上色过程统一为一个后关键帧阶段。它通过稀疏草图注入机制，利用关键帧草图提供精确控制，从而减少人工工作量。该模型还采用了卡通适应方法，能够将现代视频基础模型调整到卡通领域，同时保持时间先验不变。通过使用少量草图和上色参考帧，ToonComposer在视觉质量和生产效率上优于现有方法，极大地提升了艺术家的创作灵活性。'}}}, {'id': 'https://huggingface.co/papers/2508.09848', 'title': 'PRELUDE: A Benchmark Designed to Require Global Comprehension and\n  Reasoning over Long Contexts', 'url': 'https://huggingface.co/papers/2508.09848', 'abstract': "A benchmark called PRELUDE evaluates long-context understanding by assessing the consistency of prequel stories with original books, revealing significant challenges for models compared to humans.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.", 'score': 41, 'issue_id': 5363, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '2ffa20c1920780a2', 'authors': ['Mo Yu', 'Tsz Ting Chung', 'Chulun Zhou', 'Tong Li', 'Rui Lu', 'Jiangnan Li', 'Liyan Xu', 'Haoshu Lu', 'Ning Zhang', 'Jing Li', 'Jie Zhou'], 'affiliations': ['CUHK', 'HKUST', 'NJIT', 'WeChat AI, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.09848.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#multimodal', '#long_context'], 'emoji': '📚', 'ru': {'title': 'PRELUDE: новый рубеж в оценке глубины понимания текста искусственным интеллектом', 'desc': 'Представлен новый бенчмарк PRELUDE для оценки понимания длинного контекста моделями машинного обучения. Задача заключается в определении согласованности приквелов персонажей с оригинальными книгами, что требует глобального понимания и глубокого рассуждения. Результаты показывают значительное отставание современных языковых моделей от людей более чем на 15%. Исследование выявило, что модели часто дают правильные ответы с ошибочными рассуждениями, что указывает на существенные возможности для улучшения в области понимания длинного контекста.'}, 'en': {'title': 'PRELUDE: Bridging the Gap in Long-Context Understanding', 'desc': 'The paper introduces PRELUDE, a benchmark designed to evaluate how well models understand long contexts by checking if prequel stories align with original narratives. This task requires models to demonstrate global comprehension and deep reasoning, as prequels are not directly part of the original story. The study shows that current state-of-the-art models struggle significantly, with a performance gap of over 15% compared to human reasoning. Additionally, while models may provide correct answers, they often do so with flawed reasoning, highlighting the need for advancements in long-context understanding.'}, 'zh': {'title': '长文本理解的新挑战：PRELUDE基准', 'desc': 'PRELUDE是一个评估长文本理解的新基准，主要通过判断角色的前传故事与原著的叙述是否一致来进行评估。这个任务对模型的全球理解和深度推理能力提出了更高的要求，因为前传故事并不是原故事的一部分，评估其合理性通常需要整合间接相关的信息。实验结果显示，当前的先进模型在这一任务上表现不如人类，推理准确率相差超过30%。这些发现强调了在长文本理解和推理方面仍有很大的改进空间。'}}}, {'id': 'https://huggingface.co/papers/2508.10893', 'title': 'STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer', 'url': 'https://huggingface.co/papers/2508.10893', 'abstract': 'STream3R reformulates 3D reconstruction as a decoder-only Transformer problem, using causal attention to efficiently process image sequences and outperform existing methods in both static and dynamic scenes.  \t\t\t\t\tAI-generated summary \t\t\t\t We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.', 'score': 26, 'issue_id': 5371, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '060a2bebdecb841f', 'authors': ['Yushi Lan', 'Yihang Luo', 'Fangzhou Hong', 'Shangchen Zhou', 'Honghua Chen', 'Zhaoyang Lyu', 'Shuai Yang', 'Bo Dai', 'Chen Change Loy', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University, Singapore', 'Shanghai Artificial Intelligence Laboratory', 'The University of Hong Kong', 'WICT, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.10893.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#3d', '#agi'], 'emoji': '🌐', 'ru': {'title': 'Революция в 3D-реконструкции: трансформеры для потокового анализа', 'desc': 'STream3R - это новый подход к 3D-реконструкции, использующий архитектуру декодер-трансформер для предсказания облака точек. Метод применяет каузальное внимание для эффективной обработки последовательностей изображений, что позволяет ему превзойти существующие методы как для статичных, так и для динамических сцен. STream3R обучается на масштабных 3D-датасетах, что обеспечивает хорошую обобщаемость на разнообразные сценарии. Благодаря совместимости с инфраструктурой обучения языковых моделей, метод позволяет эффективно выполнять предобучение и тонкую настройку для различных задач 3D-восприятия.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Transformers', 'desc': 'STream3R is a new method for 3D reconstruction that treats the problem as a decoder-only Transformer task. It uses causal attention to efficiently handle sequences of images, which allows it to outperform traditional methods that often rely on complex global optimization or simple memory techniques. By leveraging geometric knowledge from large 3D datasets, STream3R adapts well to both static and dynamic scenes, where other methods struggle. This approach not only enhances performance but also fits well with modern training systems, making it suitable for real-time 3D applications.'}, 'zh': {'title': 'STream3R：高效的3D重建新方法', 'desc': 'STream3R是一种新颖的3D重建方法，将点图预测重新定义为仅使用解码器的Transformer问题。与现有的多视角重建方法相比，STream3R采用因果注意力机制，能够高效处理图像序列，避免了昂贵的全局优化和简单的内存机制。该方法通过从大规模3D数据集中学习几何先验，能够在静态和动态场景中表现出色，克服了传统方法的局限性。实验结果表明，STream3R在各种基准测试中均优于之前的工作，展示了因果Transformer模型在实时3D感知中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.10898', 'title': 'Puppeteer: Rig and Animate Your 3D Models', 'url': 'https://huggingface.co/papers/2508.10898', 'abstract': 'Puppeteer is a framework that automates rigging and animation of 3D models using an auto-regressive transformer, attention-based architecture, and differentiable optimization, outperforming existing methods in accuracy and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.', 'score': 23, 'issue_id': 5376, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': 'fc24813c49c2e145', 'authors': ['Chaoyue Song', 'Xiu Li', 'Fan Yang', 'Zhongcong Xu', 'Jiacheng Wei', 'Fayao Liu', 'Jiashi Feng', 'Guosheng Lin', 'Jianfeng Zhang'], 'affiliations': ['ByteDance Seed', 'Institute for Infocomm Research, A*STAR', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2508.10898.jpg', 'data': {'categories': ['#3d', '#games', '#architecture', '#benchmark', '#optimization'], 'emoji': '🎭', 'ru': {'title': 'Автоматизация риггинга и анимации 3D-моделей с помощью ИИ', 'desc': 'Puppeteer - это фреймворк для автоматизации риггинга и анимации 3D-моделей, использующий авторегрессионный трансформер и архитектуру на основе механизма внимания. Система сначала предсказывает скелетную структуру с помощью трансформера, а затем определяет веса скиннинга, используя топологически-осведомленное внимание между суставами. Заключительный этап включает дифференцируемую оптимизацию для создания стабильных анимаций высокого качества. Puppeteer превосходит существующие методы по точности и эффективности, успешно обрабатывая разнообразный 3D-контент.'}, 'en': {'title': 'Automating 3D Animation with Puppeteer: Efficiency Meets Precision', 'desc': 'Puppeteer is a novel framework designed to automate the rigging and animation of 3D models using advanced machine learning techniques. It employs an auto-regressive transformer for predicting skeletal structures, enhancing the representation of joints through a unique tokenization strategy. The framework also utilizes an attention-based architecture to accurately infer skinning weights by considering the relationships between joints. By integrating differentiable optimization, Puppeteer achieves high-quality, stable animations efficiently, surpassing current methods in both accuracy and performance.'}, 'zh': {'title': 'Puppeteer：自动化3D模型绑定与动画的革命性框架', 'desc': 'Puppeteer是一个框架，旨在自动化3D模型的绑定和动画，使用自回归变换器和基于注意力的架构，具有更高的准确性和效率。该系统通过自回归变换器预测合理的骨骼结构，并采用基于拓扑的注意力机制来推断皮肤权重，从而有效编码关节之间的关系。最后，Puppeteer结合可微优化的动画管道，生成稳定且高保真的动画，计算效率优于现有方法。通过广泛的评估，Puppeteer在骨骼预测准确性和皮肤质量方面显著超越了最先进的技术。'}}}, {'id': 'https://huggingface.co/papers/2508.10833', 'title': 'UI-Venus Technical Report: Building High-performance UI Agents with RFT', 'url': 'https://huggingface.co/papers/2508.10833', 'abstract': "UI-Venus, a multimodal large language model-based UI agent, achieves state-of-the-art performance in UI grounding and navigation tasks using reinforcement fine-tuning and novel self-evolving frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \\& Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus.", 'score': 22, 'issue_id': 5364, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '23e6ea5081c3c925', 'authors': ['Zhangxuan Gu', 'Zhengwen Zeng', 'Zhenyu Xu', 'Xingran Zhou', 'Shuheng Shen', 'Yunfei Liu', 'Beitong Zhou', 'Changhua Meng', 'Tianyu Xia', 'Weizhi Chen', 'Yue Wen', 'Jingya Dou', 'Fei Tang', 'Jinzhen Lin', 'Yulin Liu', 'Zhenlin Guo', 'Yichen Gong', 'Heng Jia', 'Changlong Gao', 'Yuan Guo', 'Yong Deng', 'Zhenyu Guo', 'Liang Chen', 'Weiqiang Wang'], 'affiliations': ['Ant Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.10833.jpg', 'data': {'categories': ['#data', '#open_source', '#rl', '#games', '#optimization', '#multimodal', '#training', '#agents'], 'emoji': '🖥️', 'ru': {'title': 'UI-Venus: ИИ-агент нового поколения для работы с пользовательскими интерфейсами', 'desc': 'UI-Venus - это мультимодальная модель на основе большой языковой модели для работы с пользовательскими интерфейсами. Она достигает наилучших результатов в задачах навигации и привязки к элементам интерфейса, используя только скриншоты в качестве входных данных. Модель обучается с помощью подкрепляющего обучения на основе Qwen2.5-VL. Авторы представляют новые методы самоэволюции для улучшения производительности в сложных задачах навигации.'}, 'en': {'title': 'UI-Venus: Redefining UI Navigation with Multimodal Intelligence', 'desc': "UI-Venus is a multimodal large language model-based UI agent that excels in UI grounding and navigation tasks. It utilizes reinforcement fine-tuning and innovative self-evolving frameworks to achieve state-of-the-art performance with only a few hundred thousand high-quality training samples. The model's variants, 7B and 72B, have surpassed previous benchmarks, demonstrating impressive success rates in both grounding and navigation tasks. Key contributions include the introduction of specialized reward functions and data cleaning strategies, along with a novel framework that enhances navigation performance through improved planning and reasoning."}, 'zh': {'title': 'UI-Venus：用户界面任务的新标杆', 'desc': 'UI-Venus是一种基于多模态大语言模型的用户界面代理，能够仅通过截图输入来完成任务。它在用户界面定位和导航任务中表现出色，采用了强化微调和自我演化框架。UI-Venus的7B和72B版本在标准基准测试中超越了之前的最佳模型，显示出其强大的能力。该模型还引入了精心设计的奖励函数和高效的数据清理策略，以提升导航性能和规划能力。'}}}, {'id': 'https://huggingface.co/papers/2508.10875', 'title': 'A Survey on Diffusion Language Models', 'url': 'https://huggingface.co/papers/2508.10875', 'abstract': 'Diffusion Language Models offer parallel token generation, reducing inference latency and capturing bidirectional context, and are compared to autoregressive models in various NLP tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.', 'score': 19, 'issue_id': 5370, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '752e7ed618064491', 'authors': ['Tianyi Li', 'Mingda Chen', 'Bowei Guo', 'Zhiqiang Shen'], 'affiliations': ['Department of Automation, Tsinghua University', 'VILA Lab, Mohamed bin Zayed University of Artificial Intelligence'], 'pdf_title_img': 'assets/pdf/title_img/2508.10875.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#inference', '#data', '#survey', '#diffusion'], 'emoji': '🌊', 'ru': {'title': 'Диффузионные языковые модели: революция в параллельной генерации текста', 'desc': 'Диффузионные языковые модели (DLM) предлагают параллельную генерацию токенов, что снижает задержку при выводе и позволяет учитывать двунаправленный контекст. В статье представлен всесторонний обзор текущего ландшафта DLM, включая их эволюцию, основные принципы и современные модели. Авторы анализируют стратегии предобучения, методы оптимизации вывода и мультимодальные расширения DLM. Также обсуждаются ограничения и проблемы DLM, такие как эффективность и обработка длинных последовательностей.'}, 'en': {'title': 'Revolutionizing Text Generation with Diffusion Language Models', 'desc': 'Diffusion Language Models (DLMs) are a new type of model that generate text tokens simultaneously, which helps to speed up the process and allows for better understanding of context in both directions. They use a method called iterative denoising to improve the quality of generated text while maintaining performance similar to traditional autoregressive models. This paper reviews the development of DLMs, comparing them with other models and discussing their strengths, weaknesses, and applications in natural language processing. It also explores future research opportunities to enhance DLM efficiency and capabilities, particularly in handling longer sequences and multimodal tasks.'}, 'zh': {'title': '扩散语言模型：并行生成的未来', 'desc': '扩散语言模型（DLMs）是一种新兴的强大替代方案，能够并行生成标记，从而减少推理延迟并捕捉双向上下文。与自回归模型相比，DLMs通过迭代去噪过程实现了更高的生成速度，并在多个自然语言处理任务中表现出与自回归模型相当的性能。本文提供了DLMs的全面概述，涵盖了其演变、基础原理及最新模型，并分析了推理策略和优化方法。我们还讨论了DLMs的局限性和挑战，并提出了未来的研究方向。'}}}, {'id': 'https://huggingface.co/papers/2508.10751', 'title': 'Pass@k Training for Adaptively Balancing Exploration and Exploitation of\n  Large Reasoning Models', 'url': 'https://huggingface.co/papers/2508.10751', 'abstract': 'Using Pass@k as a reward in reinforcement learning with verifiable rewards improves exploration and reveals that exploration and exploitation can mutually enhance each other.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., Pass@k Training), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction.', 'score': 13, 'issue_id': 5367, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '643dd15b9c2eddf6', 'authors': ['Zhipeng Chen', 'Xiaobo Qin', 'Youbin Wu', 'Yue Ling', 'Qinghao Ye', 'Wayne Xin Zhao', 'Guang Shi'], 'affiliations': ['ByteDance Seed', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.10751.jpg', 'data': {'categories': ['#rl', '#rlhf', '#training', '#optimization', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Pass@k: ключ к балансу исследования и эксплуатации в обучении с подкреплением', 'desc': 'Статья исследует использование метрики Pass@k в качестве награды в обучении с подкреплением с верифицируемыми наградами (RLVR). Авторы обнаружили, что Pass@k улучшает исследовательские способности модели политики. Анализ показал, что исследование и эксплуатация могут взаимно усиливать друг друга, а не конфликтовать. Предложенный подход включает прямое проектирование функции преимущества, что открывает перспективное направление для будущих исследований в RLVR.'}, 'en': {'title': 'Unlocking Exploration: Pass@k Enhances Reinforcement Learning', 'desc': 'This paper explores the use of Pass@k as a reward in reinforcement learning with verifiable rewards (RLVR) to improve exploration strategies. Traditional methods often rely on Pass@1, which can lead to conservative policies that get stuck in local optima. By employing Pass@k, the authors demonstrate that exploration and exploitation can work together to enhance overall performance. The study also provides an analytical framework for understanding the benefits of this approach and suggests new directions for designing advantage functions in RLVR.'}, 'zh': {'title': '探索与利用的相互增强：Pass@k的力量', 'desc': '本文探讨了在强化学习中使用可验证奖励的策略，特别是使用Pass@k作为奖励来改善探索能力。传统上，使用Pass@1作为奖励会导致策略偏向保守行为，难以平衡探索与利用。通过将Pass@k作为奖励进行训练，研究发现探索与利用可以相互增强，而不是相互对立。本文还提出了一种分析解决方案，展示了Pass@k训练的优势，并为未来的研究方向提供了启示。'}}}, {'id': 'https://huggingface.co/papers/2508.10576', 'title': 'HumanSense: From Multimodal Perception to Empathetic Context-Aware\n  Responses through Reasoning MLLMs', 'url': 'https://huggingface.co/papers/2508.10576', 'abstract': "HumanSense is a benchmark for evaluating human-centered perception and interaction in Multimodal Large Language Models, focusing on multimodal context understanding and rational feedback through reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: brightpinkhttps://digital-avatar.github.io/ai/HumanSense/", 'score': 11, 'issue_id': 5363, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '94390c2120c8d69a', 'authors': ['Zheng Qin', 'Ruobing Zheng', 'Yabing Wang', 'Tianqi Li', 'Yi Yuan', 'Jingdong Chen', 'Le Wang'], 'affiliations': ['Ant Group', 'National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.10576.jpg', 'data': {'categories': ['#rlhf', '#benchmark', '#interpretability', '#reasoning', '#rl', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Оценка человекоориентированного восприятия в мультимодальных ИИ-моделях', 'desc': 'HumanSense - это комплексный бенчмарк для оценки восприятия и взаимодействия, ориентированного на человека, в мультимодальных больших языковых моделях (MLLM). Он фокусируется на глубоком понимании сложных мультимодальных контекстов и формировании рациональной обратной связи. Исследование показывает, что ведущие MLLM все еще имеют значительный потенциал для улучшения, особенно в задачах продвинутого взаимодействия. Авторы применяют многоэтапное, модально-прогрессивное обучение с подкреплением для улучшения способностей к рассуждению омни-модальной модели, что приводит к существенному повышению результатов оценки.'}, 'en': {'title': 'Enhancing Human-Centered Interaction in MLLMs with HumanSense', 'desc': "HumanSense is a new benchmark aimed at assessing how well Multimodal Large Language Models (MLLMs) understand and interact in human-centered scenarios. It focuses on evaluating the models' ability to comprehend complex human intentions and provide empathetic responses based on multimodal inputs like text, audio, and visual data. The research shows that while current MLLMs have made progress, they still need improvement in advanced interaction tasks, especially when it comes to reasoning and contextual understanding. By using a multi-stage reinforcement learning approach, the study enhances the reasoning capabilities of these models, leading to better performance in understanding and responding to human needs and emotions."}, 'zh': {'title': '提升人机交互的多模态基准', 'desc': 'HumanSense是一个基准，用于评估多模态大型语言模型（MLLMs）在以人为中心的感知和交互能力。该基准特别关注对复杂人类意图的理解和提供富有同理心的、上下文相关的反馈。研究表明，尽管领先的MLLMs在这些任务上仍有很大的改进空间，但通过结合视觉、音频和文本信息，可以显著提升其表现。我们采用多阶段的强化学习方法，增强模型的推理能力，从而在评估结果上取得了显著的进展。'}}}, {'id': 'https://huggingface.co/papers/2508.10637', 'title': 'Processing and acquisition traces in visual encoders: What does CLIP\n  know about your camera?', 'url': 'https://huggingface.co/papers/2508.10637', 'abstract': 'Visual encoders encode subtle image acquisition parameters that can significantly impact semantic predictions based on their correlation with semantic labels.  \t\t\t\t\tAI-generated summary \t\t\t\t Prior work has analyzed the robustness of visual encoders to image transformations and corruptions, particularly in cases where such alterations are not seen during training. When this occurs, they introduce a form of distribution shift at test time, often leading to performance degradation. The primary focus has been on severe corruptions that, when applied aggressively, distort useful signals necessary for accurate semantic predictions.   We take a different perspective by analyzing parameters of the image acquisition process and transformations that may be subtle or even imperceptible to the human eye. We find that such parameters are systematically encoded in the learned visual representations and can be easily recovered. More strikingly, their presence can have a profound impact, either positively or negatively, on semantic predictions. This effect depends on whether there is a strong correlation or anti-correlation between semantic labels and these acquisition-based or processing-based labels. Our code and data are available at: https://github.com/ryan-caesar-ramos/visual-encoder-traces', 'score': 4, 'issue_id': 5370, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '22c2fd212668ba4a', 'authors': ['Ryan Ramos', 'Vladan Stojnić', 'Giorgos Kordopatis-Zilos', 'Yuta Nakashima', 'Giorgos Tolias', 'Noa Garcia'], 'affiliations': ['The University of Osaka', 'VRG, FEE, Czech Technical University in Prague'], 'pdf_title_img': 'assets/pdf/title_img/2508.10637.jpg', 'data': {'categories': ['#cv'], 'emoji': '🔍', 'ru': {'title': 'Невидимые следы в визуальных данных: скрытое влияние на ИИ', 'desc': 'Исследование показало, что визуальные энкодеры кодируют тонкие параметры получения изображений, которые могут значительно влиять на семантические предсказания. Эти параметры, хотя и незаметны для человеческого глаза, систематически кодируются в обученных визуальных представлениях и легко восстанавливаются. Их присутствие может оказывать существенное влияние на семантические предсказания, в зависимости от корреляции с семантическими метками. Это открытие имеет важные последствия для понимания работы и надежности моделей компьютерного зрения.'}, 'en': {'title': 'Unlocking the Hidden Impact of Image Acquisition on Semantic Predictions', 'desc': 'This paper investigates how visual encoders capture subtle image acquisition parameters that can influence semantic predictions. Unlike previous studies that focused on severe image corruptions, this research highlights the impact of minor, often unnoticed changes in image acquisition. The authors demonstrate that these parameters are embedded in the visual representations learned by the model and can significantly affect prediction accuracy. The relationship between these parameters and semantic labels can either enhance or degrade performance, depending on their correlation.'}, 'zh': {'title': '微妙参数对语义预测的深远影响', 'desc': '本论文探讨了视觉编码器如何编码图像获取参数，这些参数对语义预测有显著影响。以往的研究主要关注视觉编码器在图像变换和损坏下的鲁棒性，而我们则分析了那些微妙的、甚至人眼难以察觉的图像获取过程参数。研究发现，这些参数在学习的视觉表示中被系统性地编码，并且可以轻易恢复。更重要的是，这些参数的存在对语义预测的影响深远，取决于它们与语义标签之间的相关性或反相关性。'}}}, {'id': 'https://huggingface.co/papers/2508.10860', 'title': 'From Black Box to Transparency: Enhancing Automated Interpreting\n  Assessment with Explainable AI in College Classrooms', 'url': 'https://huggingface.co/papers/2508.10860', 'abstract': "A multi-dimensional modeling framework enhances automated interpreting quality assessment by integrating feature engineering, data augmentation, and explainable machine learning, focusing on transparency and detailed diagnostic feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.", 'score': 3, 'issue_id': 5363, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': 'a06076355558e87a', 'authors': ['Zhaokun Jiang', 'Ziyin Zhang'], 'affiliations': ['Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.10860.jpg', 'data': {'categories': ['#multilingual', '#science', '#dataset', '#optimization', '#interpretability', '#training', '#data'], 'emoji': '🗣️', 'ru': {'title': 'Прозрачная оценка качества устного перевода с помощью ИИ', 'desc': 'Статья представляет многомерную модель для автоматической оценки качества устного перевода. Она объединяет инженерию признаков, аугментацию данных и интерпретируемое машинное обучение. Модель использует прозрачные признаки и анализ SHAP для объяснения предсказаний. Результаты показывают высокую предсказательную способность на новом наборе данных последовательного перевода с английского на китайский.'}, 'en': {'title': 'Enhancing Automated Interpreting Quality with Explainable AI', 'desc': 'This paper introduces a multi-dimensional modeling framework aimed at improving the quality assessment of automated interpreting. It combines feature engineering, data augmentation, and explainable machine learning to enhance transparency and provide detailed feedback. The framework addresses issues like data scarcity and the need for clearer model predictions by using relevant features and Shapley Value analysis. The results show that this approach not only predicts interpreting quality effectively but also supports learners with valuable insights for self-improvement.'}, 'zh': {'title': '提升自动化口译质量评估的透明性与可靠性', 'desc': '这篇论文提出了一种多维建模框架，旨在提高自动化口译质量评估的效果。该框架结合了特征工程、数据增强和可解释的机器学习，强调透明性和详细的诊断反馈。研究表明，使用BLEURT和CometKiwi评分作为忠实度的预测特征，以及与流利度相关的停顿特征，能够有效提升模型的预测性能。通过这种方法，论文为学习者提供了更可靠的反馈，支持自我调节学习的优势。'}}}, {'id': 'https://huggingface.co/papers/2508.10482', 'title': 'When Explainability Meets Privacy: An Investigation at the Intersection\n  of Post-hoc Explainability and Differential Privacy in the Context of Natural\n  Language Processing', 'url': 'https://huggingface.co/papers/2508.10482', 'abstract': 'The study investigates the relationship between privacy and explainability in NLP, using Differential Privacy and Post-hoc Explainability methods, and provides recommendations for balancing both.  \t\t\t\t\tAI-generated summary \t\t\t\t In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of explainability and privacy. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving both explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of Differential Privacy (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship between privacy and explainability, which is formed by a number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this important intersection.', 'score': 0, 'issue_id': 5371, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '2d10a0b967f0f4d6', 'authors': ['Mahdi Dhaini', 'Stephen Meisenbacher', 'Ege Erdogan', 'Florian Matthes', 'Gjergji Kasneci'], 'affiliations': ['Technical University of Munich, School of Computation, Information and Technology, Department of Computer Science, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2508.10482.jpg', 'data': {'categories': ['#multilingual', '#data', '#ethics', '#interpretability'], 'emoji': '🔐', 'ru': {'title': 'Баланс конфиденциальности и объяснимости в NLP: возможности и вызовы', 'desc': 'Исследование посвящено изучению взаимосвязи между конфиденциальностью и объяснимостью в области обработки естественного языка (NLP). Авторы используют методы дифференциальной приватности и пост-хок объяснимости для анализа этой взаимосвязи. Результаты показывают, что отношения между конфиденциальностью и объяснимостью зависят от различных факторов, включая тип задачи и выбор методов приватизации текста и объяснения. На основе полученных данных, исследователи предлагают практические рекомендации для будущих работ в этой области.'}, 'en': {'title': 'Balancing Privacy and Explainability in NLP', 'desc': 'This paper explores how privacy and explainability can coexist in Natural Language Processing (NLP). It focuses on Differential Privacy (DP) and Post-hoc Explainability methods to analyze their relationship. The study reveals that achieving both privacy and explainability is possible, but it depends on various factors like the specific task and the chosen methods. The authors provide practical recommendations for balancing these two important aspects in future NLP research.'}, 'zh': {'title': '隐私与可解释性的平衡之道', 'desc': '本研究探讨了自然语言处理（NLP）中隐私与可解释性之间的关系，采用了差分隐私和事后可解释性的方法。尽管近年来对可解释性和隐私保护的研究兴趣显著增加，但两者交集的研究仍然不足。我们的实证研究揭示了隐私与可解释性之间复杂的关系，受多种因素影响，包括下游任务的性质和文本隐私化及可解释性方法的选择。我们强调隐私与可解释性可以共存，并为未来在这一重要交集领域的研究提供了一系列实用建议。'}}}, {'id': 'https://huggingface.co/papers/2508.06471', 'title': 'GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models', 'url': 'https://huggingface.co/papers/2508.06471', 'abstract': 'GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.', 'score': 71, 'issue_id': 5273, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'fdf71e495fcd6efa', 'authors': ['GLM-4. 5 Team', ':', 'Aohan Zeng', 'Xin Lv', 'Qinkai Zheng', 'Zhenyu Hou', 'Bin Chen', 'Chengxing Xie', 'Cunxiang Wang', 'Da Yin', 'Hao Zeng', 'Jiajie Zhang', 'Kedong Wang', 'Lucen Zhong', 'Mingdao Liu', 'Rui Lu', 'Shulin Cao', 'Xiaohan Zhang', 'Xuancheng Huang', 'Yao Wei', 'Yean Cheng', 'Yifan An', 'Yilin Niu', 'Yuanhao Wen', 'Yushi Bai', 'Zhengxiao Du', 'Zihan Wang', 'Zilin Zhu', 'Bohan Zhang', 'Bosi Wen', 'Bowen Wu', 'Bowen Xu', 'Can Huang', 'Casey Zhao', 'Changpeng Cai', 'Chao Yu', 'Chen Li', 'Chendi Ge', 'Chenghua Huang', 'Chenhui Zhang', 'Chenxi Xu', 'Chenzheng Zhu', 'Chuang Li', 'Congfeng Yin', 'Daoyan Lin', 'Dayong Yang', 'Dazhi Jiang', 'Ding Ai', 'Erle Zhu', 'Fei Wang', 'Gengzheng Pan', 'Guo Wang', 'Hailong Sun', 'Haitao Li', 'Haiyang Li', 'Haiyi Hu', 'Hanyu Zhang', 'Hao Peng', 'Hao Tai', 'Haoke Zhang', 'Haoran Wang', 'Haoyu Yang', 'He Liu', 'He Zhao', 'Hongwei Liu', 'Hongxi Yan', 'Huan Liu', 'Huilong Chen', 'Ji Li', 'Jiajing Zhao', 'Jiamin Ren', 'Jian Jiao', 'Jiani Zhao', 'Jianyang Yan', 'Jiaqi Wang', 'Jiayi Gui', 'Jiayue Zhao', 'Jie Liu', 'Jijie Li', 'Jing Li', 'Jing Lu', 'Jingsen Wang', 'Jingwei Yuan', 'Jingxuan Li', 'Jingzhao Du', 'Jinhua Du', 'Jinxin Liu', 'Junkai Zhi', 'Junli Gao', 'Ke Wang', 'Lekang Yang', 'Liang Xu', 'Lin Fan', 'Lindong Wu', 'Lintao Ding', 'Lu Wang', 'Man Zhang', 'Minghao Li', 'Minghuan Xu', 'Mingming Zhao', 'Mingshu Zhai', 'Pengfan Du', 'Qian Dong', 'Shangde Lei', 'Shangqing Tu', 'Shangtong Yang', 'Shaoyou Lu', 'Shijie Li', 'Shuang Li', 'Shuang-Li', 'Shuxun Yang', 'Sibo Yi', 'Tianshu Yu', 'Wei Tian', 'Weihan Wang', 'Wenbo Yu', 'Weng Lam Tam', 'Wenjie Liang', 'Wentao Liu', 'Xiao Wang', 'Xiaohan Jia', 'Xiaotao Gu', 'Xiaoying Ling', 'Xin Wang', 'Xing Fan', 'Xingru Pan', 'Xinyuan Zhang', 'Xinze Zhang', 'Xiuqing Fu', 'Xunkai Zhang', 'Yabo Xu', 'Yandong Wu', 'Yida Lu', 'Yidong Wang', 'Yilin Zhou', 'Yiming Pan', 'Ying Zhang', 'Yingli Wang', 'Yingru Li', 'Yinpei Su', 'Yipeng Geng', 'Yitong Zhu', 'Yongkun Yang', 'Yuhang Li', 'Yuhao Wu', 'Yujiang Li', 'Yunan Liu', 'Yunqing Wang', 'Yuntao Li', 'Yuxuan Zhang', 'Zezhen Liu', 'Zhen Yang', 'Zhengda Zhou', 'Zhongpei Qiao', 'Zhuoer Feng', 'Zhuorui Liu', 'Zichen Zhang', 'Zihan Wang', 'Zijun Yao', 'Zikang Wang', 'Ziqiang Liu', 'Ziwei Chai', 'Zixuan Li', 'Zuodong Zhao', 'Wenguang Chen', 'Jidong Zhai', 'Bin Xu', 'Minlie Huang', 'Hongning Wang', 'Juanzi Li', 'Yuxiao Dong', 'Jie Tang'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.06471.jpg', 'data': {'categories': ['#agents', '#rl', '#open_source', '#reasoning', '#architecture', '#agi', '#training'], 'emoji': '🧠', 'ru': {'title': 'GLM-4.5: Мощная MoE-модель для рассуждений и агентного ИИ', 'desc': 'GLM-4.5 - это крупномасштабная языковая модель с архитектурой Mixture-of-Experts, содержащая 355 миллиардов параметров. Модель прошла многоэтапное обучение на 23 триллионах токенов и дополнительную постобработку с использованием экспертной итерации и обучения с подкреплением. GLM-4.5 демонстрирует высокую производительность в задачах рассуждения, агентного взаимодействия и программирования, превосходя многие конкурирующие модели с большим количеством параметров. Исследователи выпустили как полную версию GLM-4.5, так и компактную GLM-4.5-Air для продвижения исследований в области рассуждающих и агентных систем искусственного интеллекта.'}, 'en': {'title': 'GLM-4.5: Powerful Reasoning with Fewer Parameters', 'desc': 'GLM-4.5 is a large language model that uses a Mixture-of-Experts (MoE) architecture with 355 billion parameters, of which 32 billion are activated during operation. It employs a hybrid reasoning approach that allows it to perform tasks in both thinking and direct response modes. The model has undergone extensive multi-stage training on 23 trillion tokens and has been fine-tuned using reinforcement learning, resulting in impressive performance on various benchmarks. Notably, GLM-4.5 ranks highly among its peers, demonstrating strong capabilities in agentic, reasoning, and coding tasks while being more parameter-efficient than many competitors.'}, 'zh': {'title': 'GLM-4.5：强大的混合专家语言模型', 'desc': 'GLM-4.5是一种具有3550亿参数的混合专家大型语言模型，表现出色，特别是在代理、推理和编码任务上。它采用多阶段训练和强化学习的方法，经过23万亿个标记的训练，提升了模型的性能。GLM-4.5在多个基准测试中取得了优异的成绩，尤其在代理基准中排名第二。该模型的开源版本和紧凑版（GLM-4.5-Air）都已发布，旨在推动推理和代理人工智能系统的研究。'}}}, {'id': 'https://huggingface.co/papers/2508.04825', 'title': 'Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off', 'url': 'https://huggingface.co/papers/2508.04825', 'abstract': 'Voost, a unified diffusion transformer framework, jointly learns virtual try-on and try-off, enhancing garment-body correspondence and achieving state-of-the-art results across benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.', 'score': 33, 'issue_id': 5272, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '085e45094380ed54', 'authors': ['Seungyong Lee', 'Jeong-gi Kwak'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.04825.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#inference', '#architecture', '#benchmark', '#multimodal'], 'emoji': '👚', 'ru': {'title': 'Voost: революция в виртуальной примерке одежды', 'desc': 'Voost - это унифицированная система на основе диффузионного трансформера, которая совместно обучается виртуальной примерке одежды и ее снятию. Она улучшает соответствие между одеждой и телом человека, используя двунаправленную согласованность между задачами. Voost поддерживает гибкое управление направлением генерации и категорией одежды без специфичных для задачи сетей. Система достигает лучших результатов на различных тестовых наборах данных, превосходя существующие методы по точности сопоставления и визуальному качеству.'}, 'en': {'title': 'Voost: Revolutionizing Virtual Try-On with Unified Learning', 'desc': 'Voost is a new framework that uses a diffusion transformer to improve virtual try-on and try-off tasks in fashion technology. It learns to create realistic images of people wearing clothes by understanding how garments fit the body, even when poses and appearances change. By training both tasks together, Voost enhances the relationship between garments and bodies without needing extra networks or labels. The framework also includes innovative techniques to improve performance during image generation, leading to top results in accuracy and visual quality across various benchmarks.'}, 'zh': {'title': 'Voost：虚拟试穿与试脱的统一框架', 'desc': 'Voost是一个统一的扩散变换器框架，能够同时学习虚拟试穿和试脱。它通过联合建模这两个任务，增强了服装与身体之间的对应关系，解决了在姿势和外观变化下的挑战。该框架支持灵活的生成方向和服装类别，避免了特定任务的网络和额外标签。实验结果表明，Voost在试穿和试脱的基准测试中均取得了最先进的成果。'}}}, {'id': 'https://huggingface.co/papers/2508.05731', 'title': 'InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy\n  Optimization', 'url': 'https://huggingface.co/papers/2508.05731', 'abstract': 'Adaptive Exploration Policy Optimization (AEPO) enhances semantic alignment in Multimodal Large Language Models (MLLMs) for GUI interaction, improving performance on benchmarks by up to 9.0% compared to RLVR.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.', 'score': 17, 'issue_id': 5272, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '31f17f5fb142d3e8', 'authors': ['Yuhang Liu', 'Zeyu Liu', 'Shuanghe Zhu', 'Pengxiang Li', 'Congkai Xie', 'Jiasheng Wang', 'Xueyu Hu', 'Xiaotian Han', 'Jianbo Yuan', 'Xinyao Wang', 'Shengyu Zhang', 'Hongxia Yang', 'Fei Wu'], 'affiliations': ['Amazon', 'InfiX.ai', 'The Hong Kong Polytechnic University', 'The University of Chicago', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05731.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rl', '#benchmark', '#agents', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'AEPO: Прорыв в семантическом выравнивании для мультимодальных ИИ-агентов', 'desc': 'Статья представляет новый метод оптимизации политики адаптивного исследования (AEPO) для улучшения семантического выравнивания в мультимодальных больших языковых моделях (MLLM) при взаимодействии с графическим интерфейсом пользователя. AEPO использует стратегию генерации множественных ответов для обеспечения более широкого исследования, которое затем направляется теоретически обоснованной функцией вознаграждения адаптивного исследования (AER). Модели, обученные с помощью AEPO, достигают значительных улучшений до 9.0% по сравнению с базовым методом RLVR на тестах, оценивающих обобщение и семантическое понимание. Этот подход решает проблему неэффективного исследования, которая ограничивает семантическое выравнивание в существующих методах.'}, 'en': {'title': 'Enhancing GUI Interaction with Adaptive Exploration in MLLMs', 'desc': 'Adaptive Exploration Policy Optimization (AEPO) is a novel framework designed to enhance semantic alignment in Multimodal Large Language Models (MLLMs) for effective interaction with Graphical User Interfaces (GUIs). The paper identifies that while existing methods like Reinforcement Learning with Verifiable Rewards (RLVR) improve spatial alignment, they struggle with semantic alignment due to inefficient exploration strategies. AEPO addresses this by implementing a multi-answer generation approach that encourages broader exploration, guided by an Adaptive Exploration Reward (AER) function. As a result, models trained with AEPO demonstrate significant performance improvements, achieving state-of-the-art results on various GUI grounding benchmarks.'}, 'zh': {'title': '自适应探索，提升语义对齐！', 'desc': '自适应探索策略优化（AEPO）通过增强多模态大型语言模型（MLLMs）在图形用户界面（GUI）交互中的语义对齐，显著提升了模型的性能。该方法解决了自然语言指令与UI元素之间的语义对齐问题，克服了传统强化学习方法在探索效率上的瓶颈。AEPO采用多答案生成策略，结合理论基础的自适应探索奖励函数，促进了更广泛的探索。经过AEPO训练的模型在多个GUI基准测试中取得了最高9.0%的相对提升，展示了其在语义理解和泛化能力上的优势。'}}}, {'id': 'https://huggingface.co/papers/2508.06433', 'title': 'Memp: Exploring Agent Procedural Memory', 'url': 'https://huggingface.co/papers/2508.06433', 'abstract': 'Agents equipped with a learnable, updatable procedural memory system, Memp, achieve improved performance and efficiency across tasks by distilling past experiences into detailed instructions and higher-level abstractions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.', 'score': 16, 'issue_id': 5279, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '28c51ac0d694bc54', 'authors': ['Runnan Fang', 'Yuan Liang', 'Xiaobin Wang', 'Jialong Wu', 'Shuofei Qiao', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.06433.jpg', 'data': {'categories': ['#optimization', '#agi', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Memp: умная память для интеллектуальных агентов', 'desc': 'В этой статье представлена система Memp - обучаемая и обновляемая процедурная память для агентов на основе больших языковых моделей (LLM). Memp извлекает из прошлого опыта агентов как детальные пошаговые инструкции, так и абстракции более высокого уровня. Система динамически обновляет, исправляет и удаляет устаревшую информацию в своем хранилище. Эксперименты показывают, что использование Memp позволяет агентам достигать более высоких показателей успешности и эффективности при выполнении аналогичных задач.'}, 'en': {'title': 'Empowering Agents with Evolving Procedural Memory', 'desc': 'This paper introduces Memp, a learnable and updatable procedural memory system for agents that enhances their performance and efficiency in various tasks. Memp allows agents to distill their past experiences into detailed instructions and higher-level abstractions, addressing the limitations of traditional static procedural memory. The authors explore different strategies for building, retrieving, and updating this memory, ensuring it evolves with new experiences. Empirical results demonstrate that agents using Memp show improved success rates and efficiency, even when transferring memory from a stronger model to a weaker one.'}, 'zh': {'title': '智能体的可学习程序记忆提升任务表现', 'desc': '本文提出了一种可学习和可更新的程序记忆系统Memp，旨在提高智能体在任务中的表现和效率。Memp通过将过去的经验提炼为详细的指令和更高层次的抽象，帮助智能体更好地执行任务。研究探讨了程序记忆的构建、检索和更新策略，并展示了动态更新机制如何使记忆库随着新经验不断演变。实验证明，随着记忆库的优化，智能体在类似任务上的成功率和效率显著提高。'}}}, {'id': 'https://huggingface.co/papers/2508.05988', 'title': 'Pruning the Unsurprising: Efficient Code Reasoning via First-Token\n  Surprisal', 'url': 'https://huggingface.co/papers/2508.05988', 'abstract': 'ASAP, a novel coarse-to-fine framework, compresses Chain-of-Thought in code reasoning by preserving core structure and essential steps, reducing costs and improving efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in code reasoning by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces introduce substantial challenges in terms of training cost, inference latency, and deployment feasibility. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps. In this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. It then enables a logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP teaches models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning in coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy across multiple code generation benchmarks while substantially reducing training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark, our approach reduces token generation by 23.5% and inference latency by 43.5% compared to the strongest baseline, while achieving a competitive accuracy of 36.19% in Pass@1. Our results highlight a promising direction for building powerful and efficient LRMs.', 'score': 12, 'issue_id': 5272, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '82ad3c225d1615aa', 'authors': ['Wenhao Zeng', 'Yaoning Wang', 'Chao Hu', 'Yuling Shi', 'Chengcheng Wan', 'Hongyu Zhang', 'Xiaodong Gu'], 'affiliations': ['Chongqing University', 'East China Normal University', 'Fudan University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05988.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#architecture', '#data', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие рассуждений для улучшения работы с кодом', 'desc': 'ASAP - это новый фреймворк для сжатия цепочек рассуждений (Chain-of-Thought) в задачах рассуждения о коде. Он использует двухэтапный подход: сначала сохраняет основную структуру рассуждений, а затем выбирает логически важные шаги на основе метрики неожиданности первого токена. ASAP позволяет моделям автономно генерировать и использовать сжатые цепочки рассуждений во время вывода. Эксперименты показывают, что ASAP достигает современного уровня точности при значительном снижении затрат на обучение и вывод.'}, 'en': {'title': 'Efficient Code Reasoning with ASAP: Smart Compression for Better Performance', 'desc': 'The paper introduces ASAP, a new framework designed to compress Chain-of-Thought (CoT) in code reasoning while maintaining essential logical structures. It addresses the challenges posed by long reasoning traces, which can increase training costs and slow down inference. ASAP uses anchor-guided pruning to focus on core reasoning elements and applies a novel first-token surprisal metric to identify critical reasoning steps. The results demonstrate that ASAP not only improves efficiency by reducing token generation and inference latency but also maintains high accuracy in code generation tasks.'}, 'zh': {'title': 'ASAP：高效的代码推理思维链压缩框架', 'desc': '本文提出了一种名为ASAP的新的粗到细框架，用于压缩代码推理中的思维链（CoT），旨在保留核心结构和关键步骤，从而降低成本并提高效率。ASAP首先通过锚点引导修剪来保留核心推理结构，减少后续处理的搜索空间。接着，它基于新颖的首个令牌惊讶度指标，选择逻辑上重要的推理步骤进行逻辑感知修剪。实验结果表明，ASAP在多个代码生成基准上实现了最先进的准确性，同时显著降低了训练和推理成本。'}}}, {'id': 'https://huggingface.co/papers/2508.03616', 'title': 'Hidden Dynamics of Massive Activations in Transformer Training', 'url': 'https://huggingface.co/papers/2508.03616', 'abstract': 'The emergence of massive activations in transformer models follows predictable patterns that can be modeled and predicted using architectural specifications, impacting model stability, training duration, and optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Massive activations are scalar values in transformer hidden states that achieve values orders of magnitude larger than typical activations and have been shown to be critical for model functionality. While prior work has characterized these phenomena in fully trained models, the temporal dynamics of their emergence during training remain poorly understood. We present the first comprehensive analysis of massive activation development throughout transformer training, using the Pythia model family as our testbed. Through systematic analysis of various model sizes across multiple training checkpoints, we demonstrate that massive activation emergence follows predictable mathematical patterns that can be accurately modeled using an exponentially-modulated logarithmic function with five key parameters. We develop a machine learning framework to predict these mathematical parameters from architectural specifications alone, achieving high accuracy for steady-state behavior and moderate accuracy for emergence timing and magnitude. These findings enable architects to predict and potentially control key aspects of massive activation emergence through design choices, with significant implications for model stability, training cycle length, interpretability, and optimization. Our findings demonstrate that the emergence of massive activations is governed by model design and can be anticipated, and potentially controlled, before training begins.', 'score': 9, 'issue_id': 5291, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'b675755cf9022ccb', 'authors': ['Jorge Gallego-Feliciano', 'S. Aaron McClendon', 'Juan Morinelli', 'Stavros Zervoudakis', 'Antonios Saravanos'], 'affiliations': ['Aimpoint Digital Labs, Atlanta, GA, USA', 'New York University, New York, NY, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.03616.jpg', 'data': {'categories': ['#math', '#interpretability', '#training', '#architecture', '#optimization'], 'emoji': '📈', 'ru': {'title': 'Предсказуемое возникновение массивных активаций в трансформерах', 'desc': 'В статье исследуется возникновение массивных активаций в трансформерных моделях во время обучения. Авторы обнаружили, что этот процесс следует предсказуемым математическим закономерностям, которые можно смоделировать с помощью экспоненциально-модулированной логарифмической функции. Разработан фреймворк машинного обучения для прогнозирования параметров этой функции на основе архитектурных спецификаций модели. Результаты позволяют предсказывать и потенциально контролировать ключевые аспекты возникновения массивных активаций через выбор архитектуры, что влияет на стабильность, длительность обучения и оптимизацию модели.'}, 'en': {'title': 'Predicting Massive Activations for Better Transformer Design', 'desc': "This paper investigates the phenomenon of massive activations in transformer models, which are significantly larger than typical activations and crucial for model performance. The authors analyze how these massive activations develop during the training process, revealing that their emergence follows predictable mathematical patterns. They introduce a framework that allows for the prediction of key parameters related to massive activations based solely on the model's architecture. This research provides insights that can help in designing transformer models for improved stability and efficiency during training."}, 'zh': {'title': '预测大规模激活的出现，优化模型设计', 'desc': '在变换器模型中，大规模激活是隐藏状态中的标量值，其值远大于典型激活，对模型功能至关重要。我们首次全面分析了大规模激活在变换器训练过程中的发展，使用Pythia模型系列作为测试基础。研究表明，大规模激活的出现遵循可预测的数学模式，可以通过五个关键参数的指数调制对数函数进行建模。我们的研究结果使得模型设计者能够在训练前预测并控制大规模激活的关键特性，从而影响模型的稳定性、训练周期、可解释性和优化。'}}}, {'id': 'https://huggingface.co/papers/2508.02831', 'title': 'GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing', 'url': 'https://huggingface.co/papers/2508.02831', 'abstract': "GENIE combines NeRF's photorealistic rendering with Gaussian Splatting's editable and structured representation, enabling real-time, locality-aware editing and integration with physics-based simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently transformed 3D scene representation and rendering. NeRF achieves high-fidelity novel view synthesis by learning volumetric representations through neural networks, but its implicit encoding makes editing and physical interaction challenging. In contrast, GS represents scenes as explicit collections of Gaussian primitives, enabling real-time rendering, faster training, and more intuitive manipulation. This explicit structure has made GS particularly well-suited for interactive editing and integration with physics-based simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural Radiance Fields Interactive Editing), a hybrid model that combines the photorealistic rendering quality of NeRF with the editable and structured representation of GS. Instead of using spherical harmonics for appearance modeling, we assign each Gaussian a trainable feature embedding. These embeddings are used to condition a NeRF network based on the k nearest Gaussians to each query point. To make this conditioning efficient, we introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest Gaussian search based on a modified ray-tracing pipeline. We also integrate a multi-resolution hash grid to initialize and update Gaussian features. Together, these components enable real-time, locality-aware editing: as Gaussian primitives are repositioned or modified, their interpolated influence is immediately reflected in the rendered output. By combining the strengths of implicit and explicit representations, GENIE supports intuitive scene manipulation, dynamic interaction, and compatibility with physical simulation, bridging the gap between geometry-based editing and neural rendering. The code can be found under (https://github.com/MikolajZielinski/genie)", 'score': 6, 'issue_id': 5276, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '95f1bf82d6941705', 'authors': ['Mikołaj Zieliński', 'Krzysztof Byrski', 'Tomasz Szczepanik', 'Przemysław Spurek'], 'affiliations': ['IDEAS Research Institute', 'Jagiellonian University, Faculty of Mathematics and Computer Science, Łojasiewicza 6, 30-348, Krakow, Poland', 'Poznan University of Technology, Institute of Robotics and Machine Intelligence, ul. Piotrowo 3A, Poznan 60-965, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2508.02831.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🎨', 'ru': {'title': 'GENIE: Объединение NeRF и Gaussian Splatting для интерактивного редактирования 3D-сцен', 'desc': 'GENIE - это гибридная модель, объединяющая фотореалистичный рендеринг NeRF с редактируемым и структурированным представлением Gaussian Splatting. Вместо сферических гармоник для моделирования внешнего вида каждому гауссиану присваивается обучаемое векторное представление. Эти представления используются для обусловливания сети NeRF на основе k ближайших гауссианов к каждой точке запроса. Модель поддерживает интуитивно понятные манипуляции со сценой, динамическое взаимодействие и совместимость с физическим моделированием.'}, 'en': {'title': 'GENIE: Real-Time 3D Scene Editing with Photorealism and Intuition', 'desc': 'GENIE is a new model that merges the strengths of Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) for 3D scene rendering and editing. It allows for photorealistic images while also enabling real-time, interactive editing through an explicit representation of scenes using Gaussian primitives. By introducing a method called Ray-Traced Gaussian Proximity Search (RT-GPS), GENIE efficiently finds the nearest Gaussians for each point, making editing faster and more intuitive. This combination supports dynamic interactions and physical simulations, making it easier to manipulate 3D scenes in a realistic way.'}, 'zh': {'title': 'GENIE：实时可编辑的3D场景渲染新方法', 'desc': 'GENIE是一种结合了NeRF的高质量光线渲染和Gaussian Splatting的可编辑结构表示的混合模型。它通过为每个高斯分布分配可训练的特征嵌入，来实现实时的局部感知编辑。GENIE还引入了基于光线追踪的高斯邻近搜索（RT-GPS），提高了高斯搜索的效率。通过这些创新，GENIE支持直观的场景操作和与物理模拟的兼容性，弥合了几何编辑与神经渲染之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2508.05547', 'title': 'Adapting Vision-Language Models Without Labels: A Comprehensive Survey', 'url': 'https://huggingface.co/papers/2508.05547', 'abstract': 'A comprehensive survey of unsupervised adaptation methods for Vision-Language Models (VLMs) categorizes approaches based on the availability of unlabeled visual data and discusses methodologies, benchmarks, and future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.', 'score': 5, 'issue_id': 5274, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '9c2f43e8f72ea22d', 'authors': ['Hao Dong', 'Lijun Sheng', 'Jian Liang', 'Ran He', 'Eleni Chatzi', 'Olga Fink'], 'affiliations': ['EPFL, Switzerland', 'ETH Zurich, Switzerland', 'Institute of Automation, Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.05547.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#survey', '#transfer_learning', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Систематизация методов адаптации мультимодальных моделей без разметки', 'desc': 'Это обзор методов адаптации мультимодальных моделей (зрение + язык) без учителя. Авторы предлагают таксономию, основанную на доступности и характере немаркированных визуальных данных, выделяя четыре ключевые парадигмы. В статье анализируются основные методологии и стратегии адаптации для каждой парадигмы. Также рассматриваются репрезентативные бенчмарки и выделяются открытые проблемы и перспективные направления для будущих исследований.'}, 'en': {'title': 'Unlocking Vision-Language Models: A Guide to Unsupervised Adaptation', 'desc': 'This paper provides a detailed survey of unsupervised adaptation methods for Vision-Language Models (VLMs), which are designed to improve model performance without the need for labeled data. It categorizes these methods into four main types based on the availability of unlabeled visual data: Data-Free Transfer, Unsupervised Domain Transfer, Episodic Test-Time Adaptation, and Online Test-Time Adaptation. The authors analyze various methodologies and strategies within this framework, aiming to create a clearer understanding of how to effectively adapt VLMs to specific tasks. Furthermore, the paper discusses existing benchmarks and identifies future research opportunities in the field of unsupervised VLM adaptation.'}, 'zh': {'title': '无监督适应：提升视觉-语言模型的潜力', 'desc': '本文对视觉-语言模型（VLMs）在无监督适应方法方面进行了全面的调查。研究将现有方法根据无标签视觉数据的可用性进行分类，提出了四种主要范式：无数据迁移、无监督领域迁移、情景测试时适应和在线测试时适应。文章分析了每种范式的核心方法和适应策略，并回顾了不同应用中的代表性基准。最后，指出了未来研究的开放挑战和有前景的方向。'}}}, {'id': 'https://huggingface.co/papers/2508.05502', 'title': 'MELLA: Bridging Linguistic Capability and Cultural Groundedness for\n  Low-Resource Language MLLMs', 'url': 'https://huggingface.co/papers/2508.05502', 'abstract': 'MELLA, a multimodal, multilingual dataset, enhances MLLMs in low-resource languages by improving linguistic capability and cultural groundedness through native web alt-text and MLLM-generated captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce "thin descriptions", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing "thick descriptions". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.', 'score': 4, 'issue_id': 5272, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'b7b633c31ed6e35e', 'authors': ['Yufei Gao', 'Jiaying Fei', 'Nuo Chen', 'Ruirui Chen', 'Guohang Yan', 'Yunshi Lan', 'Botian Shi'], 'affiliations': ['East China Normal University', 'Institute of High Performance Computing, A*STAR', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2508.05502.jpg', 'data': {'categories': ['#low_resource', '#dataset', '#multilingual', '#training', '#data', '#multimodal'], 'emoji': '🌍', 'ru': {'title': 'MELLA: Расширение горизонтов MLLM для низкоресурсных языков', 'desc': 'Датасет MELLA представляет собой многомодальный многоязычный набор данных, направленный на улучшение работы мультимодальных больших языковых моделей (MLLM) для низкоресурсных языков. MELLA использует двойной подход, сочетая альтернативный текст с веб-страниц и подписи, сгенерированные MLLM, для улучшения лингвистических возможностей и культурной осведомленности моделей. Эксперименты показали общее улучшение производительности для восьми языков на различных архитектурах MLLM после дообучения на MELLA. Результаты подтверждают, что улучшения связаны как с расширением культурных знаний, так и с повышением лингвистических способностей моделей.'}, 'en': {'title': 'Enhancing MLLMs for Low-Resource Languages with MELLA', 'desc': 'This paper introduces MELLA, a new dataset designed to improve Multimodal Large Language Models (MLLMs) for low-resource languages. It focuses on enhancing both linguistic capabilities and cultural groundedness by using native web alt-text and MLLM-generated captions. The study highlights the limitations of existing methods that rely solely on text or machine translation, which often fail to provide rich, informative content. By fine-tuning MLLMs on MELLA, the results show significant performance improvements across multiple languages, enabling models to generate more detailed and culturally aware descriptions.'}, 'zh': {'title': '提升低资源语言的多模态语言模型', 'desc': 'MELLA是一个多模态、多语言的数据集，旨在提升低资源语言中的多模态大型语言模型（MLLM）的语言能力和文化基础。该研究提出了一种双源策略，通过收集本地网页的替代文本和MLLM生成的标题，来实现语言能力和文化基础的双重目标。实验结果表明，在MELLA上进行微调后，八种语言的模型在多种MLLM基础上普遍提高了性能，能够生成更丰富的描述。我们的数据集强调了文化知识和语言能力的增强，适用于低资源语言用户。'}}}, {'id': 'https://huggingface.co/papers/2508.01242', 'title': 'MeshLLM: Empowering Large Language Models to Progressively Understand\n  and Generate 3D Mesh', 'url': 'https://huggingface.co/papers/2508.01242', 'abstract': "MeshLLM uses large language models to generate and understand text-serialized 3D meshes by decomposing them into meaningful subunits and training with local mesh assembly strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present MeshLLM, a novel framework that leverages large language models (LLMs) to understand and generate text-serialized 3D meshes. Our approach addresses key limitations in existing methods, including the limited dataset scale when catering to LLMs' token length and the loss of 3D structural information during mesh serialization. We introduce a Primitive-Mesh decomposition strategy, which divides 3D meshes into structurally meaningful subunits. This enables the creation of a large-scale dataset with 1500k+ samples, almost 50 times larger than previous methods, which aligns better with the LLM scaling law principles. Furthermore, we propose inferring face connectivity from vertices and local mesh assembly training strategies, significantly enhancing the LLMs' ability to capture mesh topology and spatial structures. Experiments show that MeshLLM outperforms the state-of-the-art LLaMA-Mesh in both mesh generation quality and shape understanding, highlighting its great potential in processing text-serialized 3D meshes.", 'score': 3, 'issue_id': 5274, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '425226c54ca88a3a', 'authors': ['Shuangkang Fang', 'I-Chao Shen', 'Yufeng Wang', 'Yi-Hsuan Tsai', 'Yi Yang', 'Shuchang Zhou', 'Wenrui Ding', 'Takeo Igarashi', 'Ming-Hsuan Yang'], 'affiliations': ['Atmanity Inc.', 'Beihang University', 'StepFun Inc.', 'The University of Tokyo', 'UC Merced'], 'pdf_title_img': 'assets/pdf/title_img/2508.01242.jpg', 'data': {'categories': ['#games', '#dataset', '#optimization', '#3d', '#multimodal'], 'emoji': '🧊', 'ru': {'title': 'MeshLLM: Революция в обработке 3D-моделей с помощью языковых моделей', 'desc': 'MeshLLM - это новая система, использующая большие языковые модели для понимания и генерации 3D-моделей в текстовом формате. Она решает проблемы ограниченного масштаба данных и потери структурной информации при сериализации моделей. MeshLLM использует стратегию декомпозиции на примитивы, что позволяет создать большой набор данных из 1500000+ образцов. Система также предлагает новые методы вывода связности граней и локальной сборки моделей, улучшая способность языковых моделей работать с топологией и пространственными структурами.'}, 'en': {'title': 'Revolutionizing 3D Mesh Generation with Language Models', 'desc': "MeshLLM is a new framework that uses large language models to generate and interpret 3D meshes represented as text. It improves upon previous methods by introducing a Primitive-Mesh decomposition strategy, which breaks down 3D meshes into meaningful parts, allowing for a much larger dataset of over 1.5 million samples. This approach helps maintain important 3D structural information that is often lost in serialization. Additionally, MeshLLM enhances the model's understanding of mesh topology and spatial relationships through innovative training techniques, leading to superior performance in generating and understanding 3D shapes compared to existing models."}, 'zh': {'title': 'MeshLLM：重塑3D网格生成与理解的未来', 'desc': 'MeshLLM是一个新颖的框架，利用大型语言模型（LLM）来理解和生成文本序列化的3D网格。该方法通过引入原始网格分解策略，将3D网格分解为结构上有意义的子单元，从而解决了现有方法在数据集规模和3D结构信息损失方面的关键限制。我们创建了一个超过150万样本的大规模数据集，几乎是之前方法的50倍，更好地符合LLM的扩展法则。此外，MeshLLM通过推断顶点之间的面连接性和局部网格组装训练策略，显著提升了LLM捕捉网格拓扑和空间结构的能力。'}}}, {'id': 'https://huggingface.co/papers/2507.22025', 'title': 'UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding', 'url': 'https://huggingface.co/papers/2507.22025', 'abstract': 'UI-AGILE enhances GUI agents through improved training with a Continuous Reward function, Simple Thinking reward, and Cropping-based Resampling, and inference with Decomposed Grounding with Selection, achieving state-of-the-art performance on GUI benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a "Simple Thinking" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro.', 'score': 2, 'issue_id': 5274, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': 'f7ff8c2c5517f5de', 'authors': ['Shuquan Lian', 'Yuhang Wu', 'Jia Ma', 'Zihan Song', 'Bingqi Chen', 'Xiawu Zheng', 'Hui Li'], 'affiliations': ['Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22025.jpg', 'data': {'categories': ['#games', '#cv', '#optimization', '#benchmark', '#training', '#reasoning', '#agents'], 'emoji': '🖥️', 'ru': {'title': 'UI-AGILE: Революция в обучении и выводе агентов GUI', 'desc': 'В статье представлена новая система UI-AGILE, которая улучшает работу агентов GUI с помощью усовершенствованных методов обучения и вывода. Для обучения предлагаются три новшества: функция непрерывного вознаграждения, вознаграждение за простое мышление и стратегия ресемплинга на основе обрезки. Для вывода используется метод разложенного обоснования с выбором, который повышает точность на высоких разрешениях. Эксперименты показывают, что UI-AGILE достигает передовых результатов на бенчмарках ScreenSpot-Pro и ScreenSpot-v2.'}, 'en': {'title': 'Revolutionizing GUI Agents with UI-AGILE', 'desc': 'UI-AGILE is a framework designed to improve the performance of Graphical User Interface (GUI) agents by enhancing their training and inference processes. It introduces a Continuous Reward function to encourage precise grounding, a Simple Thinking reward to optimize the balance between planning speed and accuracy, and a Cropping-based Resampling technique to address sparse rewards in complex tasks. For inference, it employs Decomposed Grounding with Selection, which enhances grounding accuracy by breaking down images into smaller sections for better processing. The framework has demonstrated state-of-the-art results on GUI benchmarks, significantly improving grounding accuracy compared to existing methods.'}, 'zh': {'title': 'UI-AGILE：提升GUI代理的智能训练与推理', 'desc': 'UI-AGILE是一个增强图形用户界面（GUI）代理的框架，通过改进训练和推理过程来提升其性能。在训练阶段，UI-AGILE引入了连续奖励函数、简单思维奖励和基于裁剪的重采样策略，以提高高精度的基础定位能力和学习复杂任务的效果。在推理阶段，采用了分解基础定位与选择的方法，显著提高了高分辨率显示器上的基础定位准确性。实验结果表明，UI-AGILE在ScreenSpot-Pro和ScreenSpot-v2基准测试中达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.06494', 'title': 'LightSwitch: Multi-view Relighting with Material-guided Diffusion', 'url': 'https://huggingface.co/papers/2508.06494', 'abstract': 'Lightswitch, a material-relighting diffusion framework, enhances 3D relighting by integrating multi-view and material cues, achieving superior quality and efficiency compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent approaches for 3D relighting have shown promise in integrating 2D image relighting generative priors to alter the appearance of a 3D representation while preserving the underlying structure. Nevertheless, generative priors used for 2D relighting that directly relight from an input image do not take advantage of intrinsic properties of the subject that can be inferred or cannot consider multi-view data at scale, leading to subpar relighting. In this paper, we propose Lightswitch, a novel finetuned material-relighting diffusion framework that efficiently relights an arbitrary number of input images to a target lighting condition while incorporating cues from inferred intrinsic properties. By using multi-view and material information cues together with a scalable denoising scheme, our method consistently and efficiently relights dense multi-view data of objects with diverse material compositions. We show that our 2D relighting prediction quality exceeds previous state-of-the-art relighting priors that directly relight from images. We further demonstrate that LightSwitch matches or outperforms state-of-the-art diffusion inverse rendering methods in relighting synthetic and real objects in as little as 2 minutes.', 'score': 1, 'issue_id': 5279, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'f69d154601aca623', 'authors': ['Yehonathan Litman', 'Fernando De la Torre', 'Shubham Tulsiani'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2508.06494.jpg', 'data': {'categories': ['#cv', '#3d', '#diffusion'], 'emoji': '💡', 'ru': {'title': 'Революция в 3D-релайтинге: Lightswitch переопределяет границы возможного', 'desc': 'Lightswitch - это новая система для 3D-релайтинга, использующая диффузионные модели и учитывающая свойства материалов. Она эффективно перерисовывает множество входных изображений под целевое освещение, используя многоракурсную информацию и данные о материалах. Lightswitch превосходит существующие методы по качеству и скорости работы. Система особенно эффективна для релайтинга объектов с разнообразными материалами.'}, 'en': {'title': 'Revolutionizing 3D Relighting with Lightswitch', 'desc': 'Lightswitch is a new framework designed for improving 3D relighting by using both multi-view and material information. It addresses the limitations of previous methods that relied solely on 2D image relighting, which often failed to utilize the intrinsic properties of objects. By integrating these cues into a diffusion model, Lightswitch can efficiently relight multiple images to match a desired lighting condition. The results show that it not only enhances the quality of relighting but also does so faster than existing techniques.'}, 'zh': {'title': 'Lightswitch：高效的3D重光新方法', 'desc': 'Lightswitch是一种新型的材料重光框架，通过整合多视角和材料线索，显著提升了3D重光的质量和效率。该方法能够有效地将任意数量的输入图像重光到目标光照条件，同时考虑到物体的内在特性。与传统的2D图像重光方法相比，Lightswitch在处理多视角数据时表现出更好的效果。实验结果表明，Lightswitch在合成和真实物体的重光任务中，速度快且质量高，超越了现有的最先进技术。'}}}, {'id': 'https://huggingface.co/papers/2508.04482', 'title': 'OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use', 'url': 'https://huggingface.co/papers/2508.04482', 'abstract': 'The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.', 'score': 1, 'issue_id': 5284, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '4f7eaba2df530414', 'authors': ['Xueyu Hu', 'Tao Xiong', 'Biao Yi', 'Zishu Wei', 'Ruixuan Xiao', 'Yurun Chen', 'Jiasheng Ye', 'Meiling Tao', 'Xiangxin Zhou', 'Ziyu Zhao', 'Yuhuai Li', 'Shengze Xu', 'Shenzhi Wang', 'Xinchen Xu', 'Shuofei Qiao', 'Zhaokai Wang', 'Kun Kuang', 'Tieyong Zeng', 'Liang Wang', 'Jiwei Li', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou', 'Guoyin Wang', 'Keting Yin', 'Zhou Zhao', 'Hongxia Yang', 'Fan Wu', 'Shengyu Zhang', 'Fei Wu'], 'affiliations': ['1.AI', 'Fudan University', 'Institute of Automation, Chinese Academy of Sciences', 'OPPO AI Center', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The Hong Kong Polytechnic University', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04482.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#survey', '#agents', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'ОС-агенты: Шаг к реальности искусственных помощников уровня Д.Ж.А.Р.В.И.С.', 'desc': 'Эта статья представляет собой обзор агентов на основе больших языковых моделей, способных взаимодействовать с операционными системами (ОС-агенты). Авторы рассматривают ключевые компоненты ОС-агентов, включая среду, пространство наблюдений и действий, а также основные возможности, такие как понимание, планирование и заземление. В работе анализируются методологии создания ОС-агентов, протоколы оценки и бенчмарки, а также обсуждаются текущие проблемы и перспективные направления исследований. Статья направлена на консолидацию исследований в области ОС-агентов и предоставление информации для академических и промышленных разработок.'}, 'en': {'title': 'Advancing AI Assistants: The Rise of OS Agents', 'desc': 'This paper surveys the development of OS Agents, which are advanced AI assistants that utilize multi-modal large language models to automate tasks on computing devices. It discusses the fundamental components of these agents, including their environment, observation space, and action space, as well as their capabilities like understanding and planning. The authors also explore methodologies for building OS Agents, focusing on domain-specific models and frameworks, and review evaluation protocols to assess their performance. Finally, the paper identifies challenges and future research directions, emphasizing the importance of safety, privacy, and personalization in the evolution of these intelligent systems.'}, 'zh': {'title': '操作系统代理：迈向智能助手的未来', 'desc': '本文探讨了操作系统代理（OS Agents）的发展，这些代理利用多模态大型语言模型（M-LLMs）在计算设备上自动化任务。我们详细介绍了OS代理的基本组成部分，包括环境、观察空间和行动空间，以及理解、规划和基础能力等关键能力。文章还讨论了构建OS代理的方法，重点关注领域特定的基础模型和代理框架。最后，我们分析了评估协议和基准测试，并指出当前面临的挑战及未来研究的方向。'}}}, {'id': 'https://huggingface.co/papers/2508.09983', 'title': 'Story2Board: A Training-Free Approach for Expressive Storyboard\n  Generation', 'url': 'https://huggingface.co/papers/2508.09983', 'abstract': 'Story2Board generates expressive storyboards from natural language using a consistency framework that enhances coherence and diversity without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Story2Board, a training-free framework for expressive storyboard generation from natural language. Existing methods narrowly focus on subject identity, overlooking key aspects of visual storytelling such as spatial composition, background evolution, and narrative pacing. To address this, we introduce a lightweight consistency framework composed of two components: Latent Panel Anchoring, which preserves a shared character reference across panels, and Reciprocal Attention Value Mixing, which softly blends visual features between token pairs with strong reciprocal attention. Together, these mechanisms enhance coherence without architectural changes or fine-tuning, enabling state-of-the-art diffusion models to generate visually diverse yet consistent storyboards. To structure generation, we use an off-the-shelf language model to convert free-form stories into grounded panel-level prompts. To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain narratives designed to assess layout diversity and background-grounded storytelling, in addition to consistency. We also introduce a new Scene Diversity metric that quantifies spatial and pose variation across storyboards. Our qualitative and quantitative results, as well as a user study, show that Story2Board produces more dynamic, coherent, and narratively engaging storyboards than existing baselines.', 'score': 39, 'issue_id': 5344, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '8d5ac9177324ef60', 'authors': ['David Dinkevich', 'Matan Levy', 'Omri Avrahami', 'Dvir Samuel', 'Dani Lischinski'], 'affiliations': ['Bar-Ilan University, Israel', 'Hebrew University of Jerusalem, Israel', 'OriginAI, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2508.09983.jpg', 'data': {'categories': ['#multimodal', '#story_generation', '#benchmark', '#cv', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'Создание выразительных раскадровок с помощью ИИ без дополнительного обучения', 'desc': 'Статья представляет Story2Board - фреймворк для генерации выразительных раскадровок из естественного языка без дополнительного обучения. Он использует механизмы сохранения согласованности персонажей и смешивания визуальных признаков для улучшения когерентности и разнообразия генерируемых изображений. Авторы предлагают новый набор данных Rich Storyboard Benchmark и метрику Scene Diversity для оценки качества раскадровок. Результаты показывают, что Story2Board создает более динамичные и увлекательные раскадровки по сравнению с существующими методами.'}, 'en': {'title': 'Expressive Storyboards from Natural Language, No Fine-Tuning Needed!', 'desc': 'Story2Board is a novel framework that generates expressive storyboards from natural language descriptions without the need for fine-tuning. It addresses limitations in existing methods by focusing on important visual storytelling elements like spatial composition and narrative pacing. The framework employs Latent Panel Anchoring to maintain character consistency across panels and Reciprocal Attention Value Mixing to enhance visual feature blending. This approach allows for the creation of coherent and diverse storyboards, evaluated through the Rich Storyboard Benchmark and a new Scene Diversity metric, demonstrating superior performance compared to traditional methods.'}, 'zh': {'title': '从自然语言生成一致且多样的故事板', 'desc': 'Story2Board是一个无需训练的框架，可以从自然语言生成富有表现力的故事板。现有的方法主要关注角色身份，忽视了视觉叙事中的重要方面，如空间构图、背景演变和叙事节奏。为了解决这个问题，我们引入了一个轻量级的一致性框架，包括潜在面板锚定和互惠注意力值混合两个组件，以增强故事板的一致性和多样性。我们的实验结果表明，Story2Board生成的故事板在动态性、一致性和叙事吸引力方面优于现有的基线。'}}}, {'id': 'https://huggingface.co/papers/2508.08401', 'title': 'Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery', 'url': 'https://huggingface.co/papers/2508.08401', 'abstract': 'Mol-R1 framework enhances molecule discovery by improving reasoning performance and explainability through PRID and MoIA strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT) reasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning capabilities, achieving impressive performance in commonsense reasoning and mathematical inference. Despite their effectiveness, Long-CoT reasoning models are often criticized for their limited ability and low efficiency in knowledge-intensive domains such as molecule discovery. Success in this field requires a precise understanding of domain knowledge, including molecular structures and chemical principles, which is challenging due to the inherent complexity of molecular data and the scarcity of high-quality expert annotations. To bridge this gap, we introduce Mol-R1, a novel framework designed to improve explainability and reasoning performance of R1-like Explicit Long-CoT reasoning LLMs in text-based molecule generation. Our approach begins with a high-quality reasoning dataset curated through Prior Regulation via In-context Distillation (PRID), a dedicated distillation strategy to effectively generate paired reasoning traces guided by prior regulations. Building upon this, we introduce MoIA, Molecular Iterative Adaptation, a sophisticated training strategy that iteratively combines Supervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO), tailored to boost the reasoning performance of R1-like reasoning models for molecule discovery. Finally, we examine the performance of Mol-R1 in the text-based molecule reasoning generation task, showing superior performance against existing baselines.', 'score': 27, 'issue_id': 5341, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '5a63681c51881d66', 'authors': ['Jiatong Li', 'Weida Wang', 'Qinggang Zhang', 'Junxian Li', 'Di Zhang', 'Changmeng Zheng', 'Shufei Zhang', 'Xiaoyong Wei', 'Qing Li'], 'affiliations': ['Fudan University', 'Hong Kong Polytechnic University', 'Shanghai AI Lab', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.08401.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#data', '#rl', '#dataset', '#training'], 'emoji': '🧪', 'ru': {'title': 'Mol-R1: Умное открытие молекул с помощью ИИ', 'desc': 'Mol-R1 - это новая система для улучшения генерации молекул с помощью больших языковых моделей. Она использует стратегию PRID для создания качественного набора данных с рассуждениями, а также метод MoIA для итеративной адаптации модели. Mol-R1 сочетает обучение с учителем и обучение с подкреплением для повышения эффективности рассуждений. Эксперименты показали превосходство Mol-R1 над существующими базовыми моделями в задаче генерации молекул на основе текста.'}, 'en': {'title': 'Revolutionizing Molecule Discovery with Enhanced Reasoning and Explainability', 'desc': "The Mol-R1 framework enhances the process of discovering new molecules by improving the reasoning abilities and explainability of large language models (LLMs). It utilizes a method called Prior Regulation via In-context Distillation (PRID) to create a high-quality dataset that helps the model learn effective reasoning strategies. Additionally, it incorporates Molecular Iterative Adaptation (MoIA), which combines supervised fine-tuning with reinforced policy optimization to refine the model's performance in generating molecular data. The results demonstrate that Mol-R1 outperforms existing models in text-based molecule reasoning tasks, making it a significant advancement in the field."}, 'zh': {'title': 'Mol-R1：提升分子发现的推理与可解释性', 'desc': 'Mol-R1框架通过PRID和MoIA策略提升了分子发现的推理性能和可解释性。该框架针对长链思维推理模型的局限性，特别是在知识密集型领域如分子发现中的应用。我们首先通过先前调节和上下文蒸馏（PRID）构建高质量的推理数据集，然后采用分子迭代适应（MoIA）策略，结合监督微调和强化策略优化，提升推理能力。最终，Mol-R1在文本基础的分子推理生成任务中表现优于现有基准。'}}}, {'id': 'https://huggingface.co/papers/2508.07901', 'title': 'Stand-In: A Lightweight and Plug-and-Play Identity Control for Video\n  Generation', 'url': 'https://huggingface.co/papers/2508.07901', 'abstract': 'A lightweight framework for identity preservation in video generation using conditional image branches and restricted self-attentions outperforms full-parameter methods with minimal additional parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating high-fidelity human videos that match user-specified identities is important yet challenging in the field of generative AI. Existing methods often rely on an excessive number of training parameters and lack compatibility with other AIGC tools. In this paper, we propose Stand-In, a lightweight and plug-and-play framework for identity preservation in video generation. Specifically, we introduce a conditional image branch into the pre-trained video generation model. Identity control is achieved through restricted self-attentions with conditional position mapping, and can be learned quickly with only 2000 pairs. Despite incorporating and training just sim1\\% additional parameters, our framework achieves excellent results in video quality and identity preservation, outperforming other full-parameter training methods. Moreover, our framework can be seamlessly integrated for other tasks, such as subject-driven video generation, pose-referenced video generation, stylization, and face swapping.', 'score': 26, 'issue_id': 5346, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '1bba46c9fd359fad', 'authors': ['Bowen Xue', 'Qixin Yan', 'Wenjing Wang', 'Hao Liu', 'Chen Li'], 'affiliations': ['WeChat Vision, Tencent Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2508.07901.jpg', 'data': {'categories': ['#video', '#architecture', '#multimodal', '#training'], 'emoji': '🎭', 'ru': {'title': 'Эффективное сохранение идентичности в генерации видео', 'desc': 'Статья представляет Stand-In - легковесный фреймворк для сохранения идентичности в генерации видео. Он использует условную ветвь изображений и ограниченное самовнимание в предобученной модели генерации видео. Несмотря на добавление всего около 1% дополнительных параметров, фреймворк превосходит методы с полным обучением параметров по качеству видео и сохранению идентичности. Stand-In также может быть легко интегрирован для других задач, таких как генерация видео по субъекту или позе.'}, 'en': {'title': 'Efficient Identity Preservation in Video Generation', 'desc': 'This paper presents a new framework called Stand-In for generating videos that maintain specific identities. It uses a lightweight approach by adding a conditional image branch to existing video generation models, which allows for effective identity control. The method employs restricted self-attention mechanisms with conditional position mapping, requiring only a small dataset of 2000 pairs for training. Stand-In achieves high video quality and identity preservation while using significantly fewer parameters than traditional methods, making it adaptable for various applications in generative AI.'}, 'zh': {'title': '轻量级框架，实现视频生成中的身份保留', 'desc': '本文提出了一种轻量级框架Stand-In，用于视频生成中的身份保留。该框架通过引入条件图像分支和限制自注意力机制，实现了对用户指定身份的控制。与传统方法相比，Stand-In仅需额外2000对样本和1%的参数，便能在视频质量和身份保留方面取得优异效果。该框架还具有良好的兼容性，可以与其他生成式AI工具无缝集成，适用于多种任务。'}}}, {'id': 'https://huggingface.co/papers/2508.09192', 'title': 'Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing', 'url': 'https://huggingface.co/papers/2508.09192', 'abstract': 'Discrete diffusion forcing enhances diffusion large language models with block-wise autoregressive generation and inter-block parallel decoding, significantly improving inference speed while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs for text generation, with the potential to decode multiple tokens in a single iteration. However, none of the existing open-source dLLMs have achieved superior inference speed over AR LLMs of similar size. This paper breaks this barrier based on a simple and effective strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key capabilities: (1) block-wise autoregressive generation to enable KV cache utilization; (2) prediction of following tokens without requiring completion of prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs are refurbished into an AR-diffusion hybrid paradigm for efficient inference. D2F can be implemented with an asymmetric distillation process based on pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm, which enables a trade-off between efficiency and efficacy. Empirically, D2F dLLMs achieve more than 2.5times inference speed than LLaMA3 and Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the acceleration can be more than 50times while maintaining comparable output quality. The code is available at https://github.com/zhijie-group/Discrete-Diffusion-Forcing.', 'score': 21, 'issue_id': 5342, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'c7306295ccd097e9', 'authors': ['Xu Wang', 'Chenkai Xu', 'Yijie Jin', 'Jiachun Jin', 'Hao Zhang', 'Zhijie Deng'], 'affiliations': ['Shanghai Jiao Tong University', 'Shanghai University', 'University of California San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2508.09192.jpg', 'data': {'categories': ['#open_source', '#optimization', '#diffusion', '#inference', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'D2F: Ускорение диффузионных языковых моделей без потери качества', 'desc': "Статья представляет новый метод под названием 'discrete diffusion forcing' (D2F) для улучшения диффузионных языковых моделей (dLLMs). D2F позволяет dLLMs использовать блочную авторегрессивную генерацию и параллельное декодирование между блоками. Это значительно увеличивает скорость вывода, сохраняя качество генерации текста. Эмпирические результаты показывают, что D2F dLLMs работают в 2.5 раза быстрее, чем LLaMA3 и Qwen2.5 на датасете GSM8K."}, 'en': {'title': 'Boosting Inference Speed in Language Models with Discrete Diffusion Forcing', 'desc': 'This paper introduces a novel approach called discrete diffusion forcing (D2F) to enhance the performance of diffusion large language models (dLLMs) in text generation. D2F allows these models to generate text in blocks, utilizing key-value (KV) caching for improved efficiency, and enables parallel decoding of tokens across different blocks. This hybrid method combines the strengths of autoregressive (AR) models with diffusion techniques, resulting in significant speed improvements during inference without sacrificing output quality. The proposed pipelined parallel decoding algorithm further optimizes the trade-off between processing speed and the effectiveness of the generated text.'}, 'zh': {'title': '离散扩散强迫：提升语言模型推理速度的创新策略', 'desc': '本文提出了一种名为离散扩散强迫（D2F）的策略，旨在提高扩散大型语言模型（dLLMs）的推理速度，同时保持生成质量。D2F通过块级自回归生成和跨块并行解码，使得模型能够在单次迭代中解码多个标记。实验结果表明，D2F dLLMs在GSM8K数据集上的推理速度比LLaMA3和Qwen2.5快超过2.5倍，且与传统的dLLMs相比，速度提升可达50倍。该方法通过不对称蒸馏过程实现，提供了效率与效果之间的平衡。'}}}, {'id': 'https://huggingface.co/papers/2508.09889', 'title': 'AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving', 'url': 'https://huggingface.co/papers/2508.09889', 'abstract': 'A dynamic Multi-Agent System (MAS) with Execution and Guard Agents improves the reliability and effectiveness of intelligent agents using external tools, outperforming single-agent systems on the GAIA leaderboard.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of large language models (LLMs) has empowered intelligent agents to leverage diverse external tools for solving complex real-world problems. However, as agents increasingly depend on multiple tools, they encounter new challenges: extended contexts from disparate sources and noisy or irrelevant tool outputs can undermine system reliability and accuracy. These challenges underscore the necessity for enhanced stability in agent-based systems. To address this, we introduce dynamic supervision and maneuvering mechanisms, constructing a robust and dynamic Multi-Agent System (MAS) architecture within the AWorld framework. In our approach, the Execution Agent invokes the Guard Agent at critical steps to verify and correct the reasoning process, effectively reducing errors arising from noise and bolstering problem-solving robustness. Extensive experiments on the GAIA test dataset reveal that our dynamic maneuvering mechanism significantly improves both the effectiveness and stability of solutions, outperforming single-agent system (SAS) and standard tool-augmented systems. As a result, our dynamic MAS system achieved first place among open-source projects on the prestigious GAIA leaderboard. These findings highlight the practical value of collaborative agent roles in developing more reliable and trustworthy intelligent systems.', 'score': 20, 'issue_id': 5340, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '2c35df2a43900320', 'authors': ['Zhitian Xie', 'Qintong Wu', 'Chengyue Yu', 'Chenyi Zhuang', 'Jinjie Gu'], 'affiliations': ['AWorld Team, Inclusion AI', 'antgroup.com'], 'pdf_title_img': 'assets/pdf/title_img/2508.09889.jpg', 'data': {'categories': ['#reasoning', '#agents', '#open_source', '#agi', '#architecture', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Мультиагентная система: надежность и эффективность в использовании ИИ-инструментов', 'desc': 'Статья представляет динамическую мультиагентную систему (МАС) с агентами исполнения и контроля для повышения надежности и эффективности интеллектуальных агентов, использующих внешние инструменты. Система решает проблемы, связанные с расширенными контекстами из разных источников и шумными выходными данными инструментов. МАС превосходит одноагентные системы в рейтинге GAIA. Эксперименты показывают, что динамический механизм маневрирования значительно улучшает эффективность и стабильность решений.'}, 'en': {'title': 'Dynamic Multi-Agent Systems: Enhancing Reliability through Collaboration', 'desc': 'This paper presents a dynamic Multi-Agent System (MAS) that enhances the reliability and effectiveness of intelligent agents by utilizing Execution and Guard Agents. The system addresses challenges faced by agents when using multiple external tools, such as managing extended contexts and filtering out noisy outputs. By implementing dynamic supervision, the Execution Agent collaborates with the Guard Agent to verify and correct reasoning processes, which reduces errors and improves problem-solving capabilities. Experimental results demonstrate that this MAS architecture significantly outperforms single-agent systems on the GAIA leaderboard, showcasing the benefits of collaborative agent roles in creating more robust intelligent systems.'}, 'zh': {'title': '动态多智能体系统：提升智能体的可靠性与有效性', 'desc': '本文介绍了一种动态多智能体系统（MAS），通过执行代理和守护代理的协作，提高了智能体的可靠性和有效性。随着智能体使用多种外部工具解决复杂问题，系统面临新的挑战，如来自不同来源的上下文延长和工具输出的噪声。为了解决这些问题，本文提出了动态监督和操控机制，构建了一个稳健的MAS架构。实验结果表明，该系统在GAIA排行榜上表现优异，超越了单智能体系统和标准工具增强系统，展示了协作智能体角色在提升智能系统可靠性方面的实际价值。'}}}, {'id': 'https://huggingface.co/papers/2508.09736', 'title': 'Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory', 'url': 'https://huggingface.co/papers/2508.09736', 'abstract': "M3-Agent, a multimodal agent with long-term memory, performs multi-turn reasoning and outperforms baselines on a new long-video question answering benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from a robot's perspective (M3-Bench-robot) and 929 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent", 'score': 19, 'issue_id': 5345, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '8883a7582bf874bc', 'authors': ['Lin Long', 'Yichen He', 'Wentao Ye', 'Yiyuan Pan', 'Yuan Lin', 'Hang Li', 'Junbo Zhao', 'Wei Li'], 'affiliations': ['ByteDance Seed', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.09736.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#agents', '#reasoning', '#multimodal', '#rl'], 'emoji': '🤖', 'ru': {'title': 'M3-Agent: мультимодальный ИИ-агент с человекоподобной долговременной памятью', 'desc': 'M3-Agent - это новая мультимодальная агентная система с долговременной памятью, способная обрабатывать визуальные и аудио входные данные в реальном времени. Система формирует как эпизодическую, так и семантическую память, что позволяет ей накапливать знания об окружающем мире. M3-Agent использует многоэтапные рассуждения и извлечение релевантной информации из памяти для выполнения задач. Для оценки эффективности агента авторы разработали новый бенчмарк M3-Bench для ответов на вопросы по длинным видео.'}, 'en': {'title': 'M3-Agent: Advancing Multimodal Reasoning with Human-like Memory', 'desc': 'M3-Agent is a cutting-edge multimodal agent that integrates long-term memory to enhance its reasoning capabilities. It processes visual and auditory information in real-time, allowing it to build and update its memory similar to human cognition. The agent features both episodic and semantic memory, which helps it accumulate knowledge and understand its environment more deeply. Evaluated on the M3-Bench benchmark, M3-Agent demonstrates superior performance in multi-turn reasoning tasks compared to existing models, showcasing its potential for real-world applications.'}, 'zh': {'title': 'M3-Agent：具备人类般长期记忆的多模态智能体', 'desc': 'M3-Agent是一种新型的多模态智能体框架，具备长期记忆能力。它能够实时处理视觉和听觉输入，像人类一样构建和更新记忆。M3-Agent不仅拥有情节记忆，还发展了语义记忆，使其能够随着时间积累世界知识。通过自主进行多轮推理，M3-Agent能够从记忆中检索相关信息，完成任务，并在新的长视频问答基准上超越了现有的基线模型。'}}}, {'id': 'https://huggingface.co/papers/2508.09987', 'title': 'Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved\n  Image Generation', 'url': 'https://huggingface.co/papers/2508.09987', 'abstract': 'Echo-4o-Image, a synthetic dataset generated by GPT-4o, enhances image generation models by addressing rare scenarios and providing clean supervision, leading to improved performance and transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.', 'score': 15, 'issue_id': 5342, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '2b98a30444833d90', 'authors': ['Junyan Ye', 'Dongzhi Jiang', 'Zihao Wang', 'Leqi Zhu', 'Zhenghao Hu', 'Zilong Huang', 'Jun He', 'Zhiyuan Yan', 'Jinghua Yu', 'Hongsheng Li', 'Conghui He', 'Weijia Li'], 'affiliations': ['CUHK MMLab', 'Peking University', 'Shanghai Artificial Intelligence Laboratory', 'Sun Yat-sen University'], 'pdf_title_img': 'assets/pdf/title_img/2508.09987.jpg', 'data': {'categories': ['#synthetic', '#transfer_learning', '#cv', '#dataset', '#benchmark', '#multimodal'], 'emoji': '🖼️', 'ru': {'title': 'Синтетические данные - ключ к улучшению генерации изображений', 'desc': 'Исследователи представили Echo-4o-Image - синтетический датасет, созданный с помощью GPT-4o для улучшения моделей генерации изображений. Этот датасет помогает решить проблему редких сценариев и предоставляет чистую разметку, что приводит к повышению производительности и переносимости моделей. Авторы использовали Echo-4o-Image для дообучения мультимодальной модели Bagel и создали модель Echo-4o, показавшую сильные результаты на стандартных бенчмарках. Кроме того, применение Echo-4o-Image к другим базовым моделям (например, OmniGen2, BLIP3-o) привело к стабильному улучшению показателей по нескольким метрикам.'}, 'en': {'title': 'Enhancing Image Generation with Synthetic Data', 'desc': 'The paper introduces Echo-4o-Image, a synthetic dataset created by GPT-4o to improve image generation models. It addresses the limitations of real-world datasets by providing clean supervision and covering rare scenarios that are often underrepresented. The dataset enhances the performance and transferability of various models, including Bagel, OmniGen2, and BLIP3-o. Additionally, the authors propose new evaluation benchmarks to better assess the capabilities of image generation systems.'}, 'zh': {'title': '合成数据集提升图像生成能力', 'desc': '本文介绍了Echo-4o-Image，这是一个由GPT-4o生成的合成数据集，旨在提升图像生成模型的性能。该数据集通过补充真实世界数据集中稀有场景，提供了干净且可控的监督信号，从而改善了文本与图像的对齐。研究表明，合成图像在处理复杂背景噪声和文本描述与图像内容不一致方面具有优势。通过使用Echo-4o-Image，研究人员在多个基础模型上实现了一致的性能提升，展示了该数据集的强大迁移能力。'}}}, {'id': 'https://huggingface.co/papers/2508.07750', 'title': 'Learning to Align, Aligning to Learn: A Unified Approach for\n  Self-Optimized Alignment', 'url': 'https://huggingface.co/papers/2508.07750', 'abstract': "GRAO, a unified framework combining SFT and RL, enhances language model alignment through multi-sample generation, Group Direct Alignment Loss, and reference-aware parameter updates, demonstrating superior performance across human alignment tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment methodologies have emerged as a critical pathway for enhancing language model alignment capabilities. While SFT (supervised fine-tuning) accelerates convergence through direct token-level loss intervention, its efficacy is constrained by offline policy trajectory. In contrast, RL(reinforcement learning) facilitates exploratory policy optimization, but suffers from low sample efficiency and stringent dependency on high-quality base models. To address these dual challenges, we propose GRAO (Group Relative Alignment Optimization), a unified framework that synergizes the respective strengths of SFT and RL through three key innovations: 1) A multi-sample generation strategy enabling comparative quality assessment via reward feedback; 2) A novel Group Direct Alignment Loss formulation leveraging intra-group relative advantage weighting; 3) Reference-aware parameter updates guided by pairwise preference dynamics. Our theoretical analysis establishes GRAO's convergence guarantees and sample efficiency advantages over conventional approaches. Comprehensive evaluations across complex human alignment tasks demonstrate GRAO's superior performance, achieving 57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and GRPO baselines respectively. This work provides both a theoretically grounded alignment framework and empirical evidence for efficient capability evolution in language models.", 'score': 14, 'issue_id': 5347, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'adefb08b02280156', 'authors': ['Haowen Wang', 'Yun Yue', 'Zhiling Ye', 'Shuowen Zhang', 'Lei Fan', 'Jiaxin Liang', 'Jiadi Jiang', 'Cheng Wei', 'Jingyuan Deng', 'Xudong Han', 'Ji Li', 'Chunxiao Guo', 'Peng Wei', 'Jian Wang', 'Jinjie Gu'], 'affiliations': ['Intelligence Healthcare Department, AntGroup Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.07750.jpg', 'data': {'categories': ['#training', '#alignment', '#rl', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'GRAO: Объединение SFT и RL для эффективного выравнивания языковых моделей', 'desc': 'GRAO - это новый фреймворк для улучшения выравнивания языковых моделей, объединяющий преимущества SFT и RL. Он использует генерацию нескольких образцов, групповую функцию потерь и обновление параметров с учетом эталонных образцов. GRAO демонстрирует превосходную производительность в задачах выравнивания по сравнению с базовыми методами. Теоретический анализ подтверждает гарантии сходимости и эффективность GRAO.'}, 'en': {'title': 'GRAO: Uniting SFT and RL for Superior Language Model Alignment', 'desc': "The paper introduces GRAO, a new framework that combines supervised fine-tuning (SFT) and reinforcement learning (RL) to improve the alignment of language models. GRAO addresses the limitations of SFT's offline policy and RL's sample inefficiency by using multi-sample generation for better quality assessment and a novel Group Direct Alignment Loss for improved training. Additionally, it incorporates reference-aware parameter updates to enhance learning from pairwise preferences. The results show that GRAO significantly outperforms existing methods in various human alignment tasks, demonstrating its effectiveness and efficiency in evolving language model capabilities."}, 'zh': {'title': 'GRAO：提升语言模型对齐的新框架', 'desc': 'GRAO是一种统一框架，结合了监督微调（SFT）和强化学习（RL），通过多样本生成、群体直接对齐损失和参考感知参数更新来增强语言模型的对齐能力。该方法通过三项创新，解决了SFT和RL各自的局限性，提升了样本效率和收敛性。理论分析表明，GRAO在收敛性和样本效率上优于传统方法。综合评估结果显示，GRAO在复杂的人类对齐任务中表现优异，相较于其他基线方法有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2508.06009', 'title': 'MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math\n  Reasoning in Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2508.06009', 'abstract': 'MathReal, a dataset of real-world mathematical questions with images, evaluates the performance of multimodal large language models in authentic educational settings, highlighting challenges and providing insights for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in visual mathematical reasoning across various existing benchmarks. However, these benchmarks are predominantly based on clean or processed multimodal inputs, without incorporating the images provided by real-world Kindergarten through 12th grade (K-12) educational users. To address this gap, we introduce MathReal, a meticulously curated dataset comprising 2,000 mathematical questions with images captured by handheld mobile devices in authentic scenarios. Each question is an image, containing the question text and visual element. We systematically classify the real images into three primary categories: image quality degradation, perspective variation, and irrelevant content interference, which are further delineated into 14 subcategories. Additionally, MathReal spans five core knowledge and ability categories, which encompass three question types and are divided into three difficulty levels. To comprehensively evaluate the multimodal mathematical reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we design six experimental settings that enable a systematic analysis of their performance. Through extensive experimentation, we find that the problem-solving abilities of existing MLLMs are significantly challenged in realistic educational contexts. Based on this, we conduct a thorough analysis of their performance and error patterns, providing insights into their recognition, comprehension, and reasoning capabilities, and outlining directions for future improvements. Data and code: https://github.com/junfeng0288/MathReal.', 'score': 11, 'issue_id': 5341, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'baeb9688cb58aad4', 'authors': ['Jun Feng', 'Zixin Wang', 'Zhentao Zhang', 'Yue Guo', 'Zhihan Zhou', 'Xiuyi Chen', 'Zhenyang Li', 'Dawei Yin'], 'affiliations': ['Baidu Inc., Beijing, China', 'Beihang University, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Nanyang Technological University, Singapore', 'Xiaopeng Motors, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.06009.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#multimodal', '#science', '#dataset'], 'emoji': '📚', 'ru': {'title': 'MathReal: реальный тест для ИИ в математическом образовании', 'desc': 'Статья представляет MathReal - набор данных из 2000 математических задач с реальными изображениями для оценки мультимодальных больших языковых моделей (MLLM) в образовательном контексте. Изображения классифицированы по качеству, ракурсу и наличию посторонних элементов. Эксперименты показали, что существующие MLLM испытывают значительные трудности при решении задач в реалистичных условиях. Анализ результатов выявил проблемы в распознавании, понимании и рассуждении моделей, предоставляя направления для дальнейшего улучшения.'}, 'en': {'title': 'Bridging the Gap: Real-World Math for MLLMs', 'desc': 'The paper introduces MathReal, a new dataset designed to evaluate multimodal large language models (MLLMs) using real-world mathematical questions accompanied by images. Unlike previous benchmarks that used clean inputs, MathReal includes 2,000 questions sourced from actual K-12 educational settings, highlighting the challenges faced by MLLMs in processing real-world data. The dataset categorizes images based on quality issues, perspective variations, and irrelevant content, while also covering various knowledge categories and question types. Through rigorous testing, the study reveals significant performance gaps in MLLMs when applied to authentic educational scenarios, offering insights for future enhancements in their reasoning capabilities.'}, 'zh': {'title': '真实教育环境中的数学推理挑战', 'desc': 'MathReal是一个包含真实世界数学问题和图像的数据集，旨在评估多模态大型语言模型在真实教育环境中的表现。该数据集包含2000个由手持移动设备拍摄的数学问题，涵盖了图像质量下降、视角变化和无关内容干扰等三大类问题。通过六种实验设置，我们系统地分析了现有多模态大型语言模型在真实教育场景中的数学推理能力。实验结果表明，现有模型在真实教育环境中面临显著挑战，并为未来的改进提供了见解。'}}}, {'id': 'https://huggingface.co/papers/2508.05613', 'title': 'Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models', 'url': 'https://huggingface.co/papers/2508.05613', 'abstract': 'A reinforcement learning framework jointly optimizes policy and reward models to enhance robustness and mitigate reward hacking in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL.', 'score': 10, 'issue_id': 5341, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '1682d3768247c2b0', 'authors': ['Haitao Hong', 'Yuchen Yan', 'Xingyu Wu', 'Guiyang Hou', 'Wenqi Zhang', 'Weiming Lu', 'Yongliang Shen', 'Jun Xiao'], 'affiliations': ['Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05613.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#rl', '#optimization', '#training'], 'emoji': '🤖', 'ru': {'title': 'Cooper: умное обучение с подкреплением для больших языковых моделей', 'desc': 'В этой статье представлена новая система обучения с подкреплением под названием Cooper, которая совместно оптимизирует модель политики и модель вознаграждения для больших языковых моделей. Cooper использует преимущества как правил, так и моделей для создания более надежной и устойчивой к обману системы вознаграждений. Авторы также предлагают гибридную стратегию аннотации и модель вознаграждения на основе эталонных ответов VerifyRM. Эксперименты показывают, что Cooper снижает риск обмана системы вознаграждений и улучшает общую производительность обучения с подкреплением.'}, 'en': {'title': 'Cooper: Enhancing Robustness in RL with Joint Policy and Reward Optimization', 'desc': 'This paper presents a reinforcement learning framework called Cooper, which aims to improve the robustness of large language models (LLMs) while reducing the risk of reward hacking. It jointly optimizes both the policy model and the reward model, addressing the weaknesses of existing reward paradigms: rule-based rewards are not robust, and model-based rewards can be exploited. Cooper utilizes the strengths of rule-based rewards for accurate response identification and employs a hybrid annotation strategy to generate effective training data. The results show that this approach enhances overall performance and accuracy in reinforcement learning tasks, demonstrating a significant improvement in model reliability.'}, 'zh': {'title': '共同优化策略与奖励模型，提升鲁棒性与安全性', 'desc': '本文提出了一种强化学习框架Cooper，旨在共同优化策略模型和奖励模型，以增强大型语言模型的鲁棒性并减少奖励黑客行为。当前的奖励机制主要分为基于模型的奖励和基于规则的奖励，但这两种方法各有局限性。Cooper利用基于规则的奖励在识别正确响应时的高精度，并动态构建和选择正负样本对来持续训练奖励模型，从而提高鲁棒性。实验结果表明，Cooper有效缓解了奖励黑客问题，并提升了强化学习的整体性能。'}}}, {'id': 'https://huggingface.co/papers/2508.09456', 'title': 'IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding', 'url': 'https://huggingface.co/papers/2508.09456', 'abstract': "A novel input-aware backdoor attack method, IAG, manipulates vision-language models to ground specific objects in images regardless of user queries, using a text-conditional U-Net and reconstruction loss to ensure stealthiness.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) have shown significant advancements in tasks such as visual grounding, where they localize specific objects in images based on natural language queries and images. However, security issues in visual grounding tasks for VLMs remain underexplored, especially in the context of backdoor attacks. In this paper, we introduce a novel input-aware backdoor attack method, IAG, designed to manipulate the grounding behavior of VLMs. This attack forces the model to ground a specific target object in the input image, regardless of the user's query. We propose an adaptive trigger generator that embeds the semantic information of the attack target's description into the original image using a text-conditional U-Net, thereby overcoming the open-vocabulary attack challenge. To ensure the attack's stealthiness, we utilize a reconstruction loss to minimize visual discrepancies between poisoned and clean images. Additionally, we introduce a unified method for generating attack data. IAG is evaluated theoretically and empirically, demonstrating its feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches over 65\\% on various testing sets. IAG also shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples. Extensive specific experiments, such as ablation study and potential defense, also indicate the robustness and transferability of our attack.", 'score': 6, 'issue_id': 5341, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': 'adbc75fda80ba5f2', 'authors': ['Junxian Li', 'Beining Xu', 'Di Zhang'], 'affiliations': ['Fudan University, Shanghai, China', 'Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.09456.jpg', 'data': {'categories': ['#security', '#multimodal', '#cv'], 'emoji': '🕵️', 'ru': {'title': 'Скрытая атака на модели компьютерного зрения с помощью адаптивного генератора триггеров', 'desc': 'Статья представляет новый метод атаки на модели компьютерного зрения и обработки естественного языка под названием IAG. Этот метод заставляет модель выделять определенный целевой объект на изображении независимо от запроса пользователя. IAG использует условный U-Net для внедрения семантической информации о цели атаки в исходное изображение. Для обеспечения скрытности атаки применяется функция потерь реконструкции, минимизирующая визуальные различия между отравленными и чистыми изображениями.'}, 'en': {'title': 'Stealthy Backdoor Attacks on Vision-Language Models with IAG', 'desc': "This paper presents IAG, a new method for executing backdoor attacks on vision-language models (VLMs) that manipulate how these models identify objects in images. IAG uses a text-conditional U-Net to embed attack target descriptions into images, allowing the model to ground specific objects regardless of the user's input. The method ensures stealthiness by applying a reconstruction loss to minimize differences between altered and original images. The effectiveness of IAG is demonstrated through various experiments, achieving high attack success rates while maintaining accuracy on clean samples."}, 'zh': {'title': '输入感知后门攻击：操控视觉语言模型的隐秘力量', 'desc': '本文提出了一种新颖的输入感知后门攻击方法IAG，旨在操控视觉语言模型（VLMs）在图像中定位特定对象，而不受用户查询的影响。该方法使用文本条件的U-Net和重建损失，确保攻击的隐蔽性。IAG通过将攻击目标的语义信息嵌入到原始图像中，克服了开放词汇攻击的挑战。实验结果表明，IAG在多个测试集上的攻击成功率超过65%，并且在干净样本上的准确率几乎没有下降，显示出其有效性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2508.09968', 'title': 'Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models', 'url': 'https://huggingface.co/papers/2508.09968', 'abstract': 'A Noise Hypernetwork is introduced to integrate test-time scaling knowledge into diffusion models, reducing computational cost while maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost. Code is available at https://github.com/ExplainableML/HyperNoise', 'score': 5, 'issue_id': 5342, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '3d06b8d8d29135c3', 'authors': ['Luca Eyring', 'Shyamgopal Karthik', 'Alexey Dosovitskiy', 'Nataniel Ruiz', 'Zeynep Akata'], 'affiliations': ['Google', 'Helmholtz Munich', 'Inceptive', 'Munich Center of Machine Learning', 'Technical University of Munich', 'University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2508.09968.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#training', '#inference', '#architecture'], 'emoji': '🔊', 'ru': {'title': 'Эффективное масштабирование диффузионных моделей с помощью шумовой гиперсети', 'desc': 'В статье представлена концепция Шумовой гиперсети (Noise Hypernetwork) для интеграции знаний масштабирования во время тестирования в диффузионные модели. Этот подход позволяет сократить вычислительные затраты при сохранении качества генерации. Авторы предлагают теоретически обоснованную структуру для обучения распределения шума, оптимизированного под желаемые характеристики. Результаты показывают, что метод позволяет достичь значительного улучшения качества по сравнению с явной оптимизацией во время тестирования при существенно меньших вычислительных затратах.'}, 'en': {'title': 'Efficient Quality Enhancement in Diffusion Models with Noise Hypernetworks', 'desc': 'This paper presents a Noise Hypernetwork that enhances diffusion models by incorporating test-time scaling knowledge, which helps reduce computational costs while preserving output quality. The authors address the challenge of increased computation time associated with test-time scaling, which can hinder practical applications. By replacing traditional noise optimization methods with a Noise Hypernetwork, they effectively modulate the initial input noise to improve performance. Their framework allows for learning a reward-tilted distribution that maintains the fidelity of the base model while optimizing for specific characteristics, achieving significant quality improvements with lower computational demands.'}, 'zh': {'title': '降低计算成本，提升模型质量的创新方案', 'desc': '本文提出了一种噪声超网络，用于将测试时缩放知识整合到扩散模型中，从而降低计算成本，同时保持模型质量。测试时缩放的新范式在大型语言模型和生成视觉模型中取得了显著突破，但其计算时间的显著增加使得许多应用变得缓慢且不切实际。我们通过用噪声超网络替代扩散模型中的奖励引导测试时噪声优化，来解决这一问题。我们的框架通过可处理的噪声空间目标，优化所需特性，同时保持对基础模型的忠实度，显著减少了计算成本。'}}}, {'id': 'https://huggingface.co/papers/2508.09945', 'title': 'VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models', 'url': 'https://huggingface.co/papers/2508.09945', 'abstract': 'VisCodex integrates vision and coding models to enhance multimodal code generation, achieving top performance using a novel dataset and benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.', 'score': 4, 'issue_id': 5347, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': 'dfb473d7795699b3', 'authors': ['Lingjie Jiang', 'Shaohan Huang', 'Xun Wu', 'Yixia Li', 'Dongdong Zhang', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'Peking University', 'Southern University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.09945.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#games', '#open_source', '#dataset'], 'emoji': '🔮', 'ru': {'title': 'VisCodex: слияние зрения и кода для прорыва в мультимодальной генерации программ', 'desc': 'VisCodex - это инновационная система, объединяющая модели компьютерного зрения и генерации кода для улучшения мультимодальной генерации программного кода. Авторы представили новый набор данных Multimodal Coding Dataset (MCD) и бенчмарк InfiBench-V для оценки моделей на реальных задачах программирования с визуальным контекстом. VisCodex использует технику слияния моделей на основе векторов задач, интегрируя передовую языковую модель для кодирования в сильную основу для работы с изображениями и текстом. Эксперименты показывают, что VisCodex достигает наилучших результатов среди открытых мультимодальных языковых моделей и приближается к проприетарным решениям вроде GPT-4.'}, 'en': {'title': 'Empowering Code Generation through Vision and Language Integration', 'desc': "VisCodex is a new framework that combines vision and coding models to improve the generation of code from visual and textual inputs. It uses a unique method to merge a powerful coding language model with a strong vision-language model, enhancing the model's ability to understand and generate code. To train and evaluate this framework, the authors created the Multimodal Coding Dataset (MCD), which includes a wide variety of coding examples and visual data. The results show that VisCodex performs exceptionally well, even rivaling advanced proprietary models, demonstrating the success of its innovative approach."}, 'zh': {'title': 'VisCodex：视觉与编码的完美结合', 'desc': 'VisCodex 是一个将视觉和编码模型整合在一起的框架，旨在提升多模态代码生成的能力。通过任务向量模型合并技术，VisCodex 将先进的编码大语言模型与强大的视觉-语言基础模型结合，保持了视觉理解和编码技能。我们还引入了多模态编码数据集（MCD），包含598,000个样本，支持训练和评估。实验结果表明，VisCodex 在开源多模态大语言模型中表现出色，接近于像 GPT-4o 这样的专有模型。'}}}, {'id': 'https://huggingface.co/papers/2508.09726', 'title': 'Sample More to Think Less: Group Filtered Policy Optimization for\n  Concise Reasoning', 'url': 'https://huggingface.co/papers/2508.09726', 'abstract': 'GFPO reduces length inflation in large language models by sampling larger groups and filtering responses based on length and token efficiency, maintaining accuracy and improving computational efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models trained with reinforcement learning with verifiable rewards tend to trade accuracy for length--inflating response lengths to achieve gains in accuracy. While longer answers may be warranted for harder problems, many tokens are merely "filler": repetitive, verbose text that makes no real progress. We introduce GFPO (Group Filtered Policy Optimization), which curbs this length explosion by sampling larger groups per problem during training and filtering responses to train on based on two key metrics: (1) response length and (2) token efficiency: reward per token ratio. By sampling more at training time, we teach models to think less at inference time. On the Phi-4-reasoning model, GFPO cuts GRPO\'s length inflation by 46-71% across challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH, LiveCodeBench) while maintaining accuracy. Optimizing for reward per token further increases reductions in length inflation to 71-85%. We also propose Adaptive Difficulty GFPO, which dynamically allocates more training resources to harder problems based on real-time difficulty estimates, improving the balance between computational efficiency and accuracy especially on difficult questions. GFPO demonstrates that increased training-time compute directly translates to reduced test-time compute--a simple yet effective trade-off for efficient reasoning.', 'score': 3, 'issue_id': 5360, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': 'fe4c78c32073cc5c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#rl', '#benchmark'], 'emoji': '✂️', 'ru': {'title': 'Эффективное обучение для лаконичных ответов', 'desc': 'Статья представляет метод GFPO (Group Filtered Policy Optimization) для уменьшения избыточной длины ответов в больших языковых моделях. GFPO отбирает более крупные группы ответов во время обучения и фильтрует их по длине и эффективности использования токенов. Это позволяет сократить длину ответов на 46-71% при сохранении точности на сложных задачах STEM и программирования. Метод демонстрирует, что увеличение вычислительных ресурсов при обучении приводит к уменьшению вычислений при использовании модели.'}, 'en': {'title': 'Streamlining Language Models: Less Length, More Efficiency!', 'desc': 'The paper introduces GFPO (Group Filtered Policy Optimization), a method designed to reduce the excessive length of responses generated by large language models. It addresses the issue where models inflate response lengths to improve accuracy, often resulting in unnecessary filler content. By sampling larger groups during training and filtering based on response length and token efficiency, GFPO enhances computational efficiency while maintaining accuracy. Additionally, the Adaptive Difficulty GFPO variant allocates more resources to harder problems, optimizing the balance between efficiency and performance.'}, 'zh': {'title': 'GFPO：高效减少语言模型长度膨胀的创新方法', 'desc': 'GFPO（组过滤策略优化）通过在训练过程中对更大组进行采样，并根据响应长度和令牌效率进行过滤，减少了大型语言模型的长度膨胀。许多生成的文本只是冗余的填充内容，GFPO通过优化每个令牌的奖励比率，显著提高了计算效率。实验表明，GFPO在Phi-4推理模型上减少了46-71%的长度膨胀，同时保持了准确性。我们还提出了自适应难度GFPO，根据实时难度估计动态分配更多训练资源，从而在处理困难问题时提高了计算效率与准确性的平衡。'}}, 'authors': [], 'affiliations': []}, {'id': 'https://huggingface.co/papers/2508.06937', 'title': 'CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing', 'url': 'https://huggingface.co/papers/2508.06937', 'abstract': "CannyEdit is a training-free framework that enhances text-to-image editing by balancing text adherence, context fidelity, and seamless integration through Selective Canny Control and Dual-Prompt Guidance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-image (T2I) models have enabled training-free regional image editing by leveraging the generative priors of foundation models. However, existing methods struggle to balance text adherence in edited regions, context fidelity in unedited areas, and seamless integration of edits. We introduce CannyEdit, a novel training-free framework that addresses these challenges through two key innovations: (1) Selective Canny Control, which masks the structural guidance of Canny ControlNet in user-specified editable regions while strictly preserving details of the source images in unedited areas via inversion-phase ControlNet information retention. This enables precise, text-driven edits without compromising contextual integrity. (2) Dual-Prompt Guidance, which combines local prompts for object-specific edits with a global target prompt to maintain coherent scene interactions. On real-world image editing tasks (addition, replacement, removal), CannyEdit outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent improvement in the balance of text adherence and context fidelity. In terms of editing seamlessness, user studies reveal only 49.2 percent of general users and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited when paired with real images without edits, versus 76.08 to 89.09 percent for competitor methods.", 'score': 3, 'issue_id': 5352, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 августа', 'en': 'August 9', 'zh': '8月9日'}, 'hash': '260e79da2607658b', 'authors': ['Weiyan Xie', 'Han Gao', 'Didan Deng', 'Kaican Li', 'April Hua Liu', 'Yongxiang Huang', 'Nevin L. Zhang'], 'affiliations': ['Huawei Hong Kong AI Framework & Data Technologies Lab', 'Shanghai University of Finance and Economics', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.06937.jpg', 'data': {'categories': ['#multimodal', '#cv'], 'emoji': '🖌️', 'ru': {'title': 'CannyEdit: Точное редактирование изображений текстом без дополнительного обучения', 'desc': 'CannyEdit - это новая система для редактирования изображений с помощью текстовых запросов, не требующая дополнительного обучения. Она использует инновационный подход Selective Canny Control для точного редактирования указанных областей изображения, сохраняя при этом детали в неизменяемых участках. Система также применяет метод Dual-Prompt Guidance, сочетающий локальные и глобальные текстовые подсказки для согласованного редактирования. Согласно исследованиям, CannyEdit превосходит существующие методы по качеству и незаметности редактирования.'}, 'en': {'title': 'CannyEdit: Seamless Text-to-Image Editing Without Training', 'desc': 'CannyEdit is a novel framework designed for text-to-image editing that does not require additional training. It improves the editing process by ensuring that changes made to images adhere closely to the provided text while maintaining the integrity of the unedited parts of the image. The framework utilizes Selective Canny Control to focus on specific areas for editing while preserving details in untouched regions, and Dual-Prompt Guidance to combine specific and general prompts for coherent edits. As a result, CannyEdit significantly enhances the quality of image edits, outperforming previous methods in both text adherence and seamless integration.'}, 'zh': {'title': 'CannyEdit：无缝文本到图像编辑的新方法', 'desc': 'CannyEdit 是一个无需训练的框架，旨在提升文本到图像的编辑效果。它通过选择性 Canny 控制和双提示引导来平衡文本遵循性、上下文保真度和无缝集成。选择性 Canny 控制允许用户在指定的可编辑区域进行精确的文本驱动编辑，同时保持未编辑区域的细节。双提示引导结合了局部提示和全局目标提示，以确保场景交互的一致性，CannyEdit 在实际图像编辑任务中表现优于之前的方法。'}}}, {'id': 'https://huggingface.co/papers/2508.09667', 'title': 'GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video\n  Diffusion Priors', 'url': 'https://huggingface.co/papers/2508.09667', 'abstract': 'GSFixer enhances 3D Gaussian Splatting reconstructions from sparse views using a DiT-based video diffusion model with reference-guided conditions, improving artifact restoration and 3D consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views is an ill-posed problem due to insufficient information, often resulting in noticeable artifacts. While recent approaches have sought to leverage generative priors to complete information for under-constrained regions, they struggle to generate content that remains consistent with input observations. To address this challenge, we propose GSFixer, a novel framework designed to improve the quality of 3DGS representations reconstructed from sparse inputs. The core of our approach is the reference-guided video restoration model, built upon a DiT-based video diffusion model trained on paired artifact 3DGS renders and clean frames with additional reference-based conditions. Considering the input sparse views as references, our model integrates both 2D semantic features and 3D geometric features of reference views extracted from the visual geometry foundation model, enhancing the semantic coherence and 3D consistency when fixing artifact novel views. Furthermore, considering the lack of suitable benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which contains artifact frames rendered using low-quality 3DGS. Extensive experiments demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS artifact restoration and sparse-view 3D reconstruction. Project page: https://github.com/GVCLab/GSFixer.', 'score': 2, 'issue_id': 5351, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '429bc9657a0ffd9e', 'authors': ['Xingyilang Yin', 'Qi Zhang', 'Jiahao Chang', 'Ying Feng', 'Qingnan Fan', 'Xi Yang', 'Chi-Man Pun', 'Huaqi Zhang', 'Xiaodong Cun'], 'affiliations': ['CUHKSZ', 'GVC Lab, Great Bay University', 'University of Macau', 'VIVO', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2508.09667.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#diffusion', '#3d'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение 3D реконструкции с помощью генеративных моделей и опорных изображений', 'desc': 'GSFixer - это новый метод улучшения реконструкции 3D сцен с использованием 3D Gaussian Splatting из малого числа ракурсов. Он основан на видео-диффузионной модели DiT с условиями на основе опорных изображений. GSFixer улучшает восстановление артефактов и 3D-согласованность, интегрируя 2D семантические и 3D геометрические признаки. Авторы также представили новый набор данных DL3DV-Res для оценки качества восстановления артефактов 3DGS.'}, 'en': {'title': 'Enhancing 3D Reconstructions with GSFixer', 'desc': 'GSFixer is a framework that improves the reconstruction of 3D scenes from sparse views using a video diffusion model. It addresses the common problem of artifacts in 3D Gaussian Splatting (3DGS) by utilizing reference-guided conditions to enhance the quality of the output. The model combines 2D semantic and 3D geometric features from reference views, ensuring better consistency and coherence in the reconstructed scenes. Additionally, it introduces a new benchmark, DL3DV-Res, for evaluating the effectiveness of 3DGS artifact restoration methods.'}, 'zh': {'title': 'GSFixer：提升稀疏视图下的3D重建质量', 'desc': 'GSFixer 是一个新框架，旨在改善从稀疏视图重建的 3D 高斯点云（3DGS）表示的质量。它使用基于 DiT 的视频扩散模型，通过参考引导条件来修复重建中的伪影，并提高 3D 一致性。该模型结合了 2D 语义特征和 3D 几何特征，从而增强了语义连贯性。实验结果表明，GSFixer 在 3DGS 伪影修复和稀疏视图 3D 重建方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2508.01522', 'title': 'Decentralized Aerial Manipulation of a Cable-Suspended Load using\n  Multi-Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.01522', 'abstract': 'A decentralized multi-agent reinforcement learning method enables real-world 6-DoF manipulation of cable-suspended loads using MAVs, achieving performance comparable to centralized methods with improved scalability and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl', 'score': 2, 'issue_id': 5352, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '4cb5dae59dc27ae1', 'authors': ['Jack Zeng', 'Andreu Matoses Gimenez', 'Eugene Vinitsky', 'Javier Alonso-Mora', 'Sihao Sun'], 'affiliations': ['Department of Civil and Urban Engineering NYU Tandon School of Engineering', 'Department of Cognitive Robotics Delft University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.01522.jpg', 'data': {'categories': ['#transfer_learning', '#robotics', '#rl', '#games', '#agents'], 'emoji': '🚁', 'ru': {'title': 'Децентрализованное управление грузом с помощью роя дронов', 'desc': 'Эта статья представляет децентрализованный метод манипуляции грузом, подвешенным на тросе, с помощью группы микро-беспилотных летательных аппаратов (МБЛА). Метод использует мультиагентное обучение с подкреплением для тренировки политики управления каждым МБЛА, не требуя глобального состояния или коммуникации между аппаратами. Введено новое пространство действий для МБЛА, использующее линейное ускорение и угловые скорости, что в сочетании с надежным низкоуровневым контроллером обеспечивает успешный перенос из симуляции в реальность. Метод подтвержден экспериментально, демонстрируя производительность, сравнимую с централизованными методами, а также устойчивость к потере одного МБЛА в полете.'}, 'en': {'title': 'Decentralized MAV Control for Flexible Load Manipulation', 'desc': "This paper introduces a decentralized multi-agent reinforcement learning (MARL) approach for controlling Micro-Aerial Vehicles (MAVs) to manipulate cable-suspended loads in three-dimensional space. The method allows each MAV to operate without needing global state information or direct communication with other MAVs, relying instead on observations of the load's position. This design enhances scalability and reduces computational demands, making it suitable for real-time onboard applications. The proposed action space, which includes linear acceleration and body rates, supports effective sim-to-real transfer, ensuring reliable performance even under dynamic conditions."}, 'zh': {'title': '去中心化的多智能体强化学习实现高效操控', 'desc': '本文提出了一种去中心化的多智能体强化学习方法，能够实现微型空中飞行器（MAV）对悬挂负载的六自由度操控。该方法通过多智能体强化学习（MARL）为每个MAV训练外部控制策略，避免了对全局状态和MAV间通信的依赖。智能体仅通过负载姿态观察进行隐式通信，从而提高了系统的可扩展性和灵活性。实验结果表明，该方法在动态三维运动中具有良好的鲁棒性，且在负载模型不确定性下的全姿态控制性能与先进的中心化方法相当。'}}}, {'id': 'https://huggingface.co/papers/2508.09776', 'title': 'Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study', 'url': 'https://huggingface.co/papers/2508.09776', 'abstract': 'Automated generation of textual explanations using large language models improves model performance in natural language inference tasks, offering a scalable alternative to human annotation.  \t\t\t\t\tAI-generated summary \t\t\t\t In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance.', 'score': 1, 'issue_id': 5346, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '581a97483aba2e0a', 'authors': ['Mahdi Dhaini', 'Juraj Vladika', 'Ege Erdogan', 'Zineb Attaoui', 'Gjergji Kasneci'], 'affiliations': ['Technical University of Munich, School of Computation, Information and Technology, Department of Computer Science, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2508.09776.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#optimization', '#dataset', '#benchmark', '#data'], 'emoji': '🤖', 'ru': {'title': 'Автоматизация объяснений в NLP: LLM как альтернатива человеку', 'desc': 'Исследователи разработали автоматизированный метод генерации текстовых объяснений с использованием больших языковых моделей (LLM) для задач естественного языкового вывода. Этот подход предлагает масштабируемую альтернативу трудоемкой ручной аннотации данных. Эксперименты показали, что автоматически сгенерированные объяснения сравнимы по эффективности с объяснениями, созданными людьми, для улучшения производительности моделей. Результаты открывают перспективный путь для автоматизированного расширения наборов данных NLP и повышения эффективности моделей.'}, 'en': {'title': 'Automated Explanations: Boosting NLP Performance with LLMs', 'desc': 'This paper discusses a new method for generating textual explanations using large language models (LLMs) to improve natural language inference tasks. Instead of relying on costly human annotations, the authors propose an automated framework that creates high-quality explanations, making the process more scalable. They evaluate the generated explanations using various Natural Language Generation (NLG) metrics and analyze their impact on the performance of pre-trained language models (PLMs). The results show that these automated explanations can significantly enhance model performance, rivaling traditional human-generated explanations.'}, 'zh': {'title': '自动化文本解释生成，提升模型性能的未来', 'desc': '本文提出了一种自动生成文本解释的框架，利用多种先进的大型语言模型（LLMs）来生成高质量的文本解释。这种方法为自然语言推理任务提供了一种可扩展的替代方案，减少了对人工标注的依赖。通过全面的自然语言生成（NLG）指标评估生成解释的质量，研究表明自动生成的解释在提升模型性能方面与人工标注的解释具有高度竞争力。我们的研究为基于LLM的文本解释生成提供了一个有前景的方向，能够扩展NLP数据集并增强模型性能。'}}}, {'id': 'https://huggingface.co/papers/2508.09752', 'title': 'μ-Parametrization for Mixture of Experts', 'url': 'https://huggingface.co/papers/2508.09752', 'abstract': 'A new parameterization for Mixture-of-Experts models provides theoretical guarantees for feature learning and investigates the impact of scaling experts and granularity on learning rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent years have seen a growing interest and adoption of LLMs, with muTransfer becoming a key technique for tuning hyperparameters in large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a leading architecture in extremely large models. However, the intersection of these two advancements has remained unexplored. In this work, we derive a mu-Parameterization (muP) for MoE, providing theoretical guarantees for feature learning across model widths in both the router and experts. We empirically validate our parameterization and further investigate how scaling the number of experts and granularity affects the optimal learning rate.', 'score': 1, 'issue_id': 5360, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '97a4985085950e14', 'authors': ['Jan Małaśnicki', 'Kamil Ciebiera', 'Mateusz Boruń', 'Maciej Pióro', 'Jan Ludziejewski', 'Maciej Stefaniak', 'Michał Krutul', 'Sebastian Jaszczur', 'Marek Cygan', 'Kamil Adamczewski', 'Jakub Krajewski'], 'affiliations': ['IDEAS NCBR', 'Institute of Fundamental Technological Research, Polish Academy of Sciences', 'Nomagic', 'Syntro', 'University of Warsaw', 'Wroclaw University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.09752.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Новая параметризация MoE: теория встречается с практикой', 'desc': 'Статья представляет новую параметризацию для моделей Mixture-of-Experts (MoE), которая обеспечивает теоретические гарантии обучения признаков. Исследователи изучают влияние масштабирования экспертов и гранулярности на скорость обучения. Работа объединяет достижения в области больших языковых моделей (LLM) и архитектуры MoE. Авторы эмпирически подтверждают свою параметризацию и анализируют, как увеличение числа экспертов влияет на оптимальную скорость обучения.'}, 'en': {'title': 'Enhancing Mixture-of-Experts with mu-Parameterization for Better Learning Rates', 'desc': 'This paper introduces a new parameterization method called mu-Parameterization (muP) for Mixture-of-Experts (MoE) models, which enhances feature learning capabilities. It provides theoretical guarantees that ensure effective learning across different model widths, specifically in the router and expert components. The study also explores how varying the number of experts and their granularity influences the optimal learning rate during training. Empirical validation of the muP method demonstrates its effectiveness in improving the performance of large-scale models.'}, 'zh': {'title': '探索混合专家模型的新参数化方法', 'desc': '本文提出了一种新的混合专家模型的参数化方法，称为mu-Parameterization（muP），为特征学习提供了理论保证。我们研究了专家数量和粒度的缩放对学习速率的影响。通过实验证明了我们的参数化方法的有效性，并探讨了在不同模型宽度下路由器和专家的特征学习。该研究为大规模训练中的超参数调优提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2508.07237', 'title': 'ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and\n  Individual Variations for Fine-Grained Segmentation', 'url': 'https://huggingface.co/papers/2508.07237', 'abstract': 'ASM-UNet, a Mamba-based architecture with adaptive scan scores, enhances fine-grained segmentation by dynamically adjusting scanning orders to accommodate individual anatomical variations.  \t\t\t\t\tAI-generated summary \t\t\t\t Precise lesion resection depends on accurately identifying fine-grained anatomical structures. While many coarse-grained segmentation (CGS) methods have been successful in large-scale segmentation (e.g., organs), they fall short in clinical scenarios requiring fine-grained segmentation (FGS), which remains challenging due to frequent individual variations in small-scale anatomical structures. Although recent Mamba-based models have advanced medical image segmentation, they often rely on fixed manually-defined scanning orders, which limit their adaptability to individual variations in FGS. To address this, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It introduces adaptive scan scores to dynamically guide the scanning order, generated by combining group-level commonalities and individual-level variations. Experiments on two public datasets (ACDC and Synapse) and a newly proposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that ASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and dataset are available at https://github.com/YqunYang/ASM-UNet.', 'score': 1, 'issue_id': 5354, 'pub_date': '2025-08-10', 'pub_date_card': {'ru': '10 августа', 'en': 'August 10', 'zh': '8月10日'}, 'hash': 'e14ffd52588151ed', 'authors': ['Bo Wang', 'Mengyuan Xu', 'Yue Yan', 'Yuqun Yang', 'Kechen Shu', 'Wei Ping', 'Xu Tang', 'Wei Jiang', 'Zheng You'], 'affiliations': ['Institute or affiliation not explicitly listed in the provided text'], 'pdf_title_img': 'assets/pdf/title_img/2508.07237.jpg', 'data': {'categories': ['#healthcare', '#dataset', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Адаптивная сегментация для точной диагностики', 'desc': 'ASM-UNet - это новая архитектура на основе Mamba для сегментации медицинских изображений. Она использует адаптивные оценки сканирования для динамической настройки порядка сканирования, учитывая индивидуальные анатомические особенности. Модель показывает превосходные результаты как в крупномасштабной, так и в мелкомасштабной сегментации. Эксперименты проводились на двух публичных наборах данных и новом наборе данных желчных протоков.'}, 'en': {'title': 'Dynamic Adaptation for Precise Medical Segmentation', 'desc': 'ASM-UNet is a new architecture designed to improve fine-grained segmentation in medical images by adapting the scanning order based on individual anatomical differences. Traditional methods often use fixed scanning orders, which can hinder their effectiveness in accurately identifying small-scale structures. By introducing adaptive scan scores, ASM-UNet dynamically adjusts its approach to better accommodate variations among patients. Experiments show that this model outperforms existing methods in both coarse-grained and fine-grained segmentation tasks.'}, 'zh': {'title': 'ASM-UNet：自适应扫描提升细粒度分割', 'desc': 'ASM-UNet是一种基于Mamba架构的模型，旨在提高细粒度分割的效果。它通过自适应扫描评分动态调整扫描顺序，以适应个体解剖结构的变化。与传统的固定扫描顺序方法不同，ASM-UNet结合了群体共性和个体差异，增强了模型的灵活性。实验结果表明，ASM-UNet在细粒度和粗粒度分割任务中均表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.06944', 'title': 'AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal\n  Imitation-Exploration Balance', 'url': 'https://huggingface.co/papers/2508.06944', 'abstract': "Adaptive Meta Fine-Tuning (AMFT) dynamically balances Supervised Fine-Tuning and Reinforcement Learning using implicit rewards to improve LLM performance and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of implicit rewards, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce Adaptive Meta Fine-Tuning (AMFT), a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a meta-gradient adaptive weight controller that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.", 'score': 1, 'issue_id': 5348, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 августа', 'en': 'August 9', 'zh': '8月9日'}, 'hash': '3c5ce2e46bfb3d7b', 'authors': ['Lixuan He', 'Jie Feng', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.06944.jpg', 'data': {'categories': ['#alignment', '#agi', '#rl', '#benchmark', '#reasoning', '#multimodal', '#optimization', '#open_source', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Балансирование обучения языковых моделей: новый подход к тонкой настройке', 'desc': 'Статья представляет новый метод обучения больших языковых моделей - Адаптивную Мета-Тонкую Настройку (AMFT). AMFT динамически балансирует между контролируемой тонкой настройкой и обучением с подкреплением, используя неявные вознаграждения. Этот подход позволяет улучшить производительность и обобщающую способность языковых моделей. AMFT демонстрирует превосходные результаты на различных сложных задачах, включая математические рассуждения и навигацию с использованием зрения и языка.'}, 'en': {'title': 'Balancing Fine-Tuning and Learning for Better AI Performance', 'desc': 'The paper introduces Adaptive Meta Fine-Tuning (AMFT), a new approach that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance the performance of Large Language Models (LLMs). AMFT treats SFT and RL as complementary reward signals rather than separate methods, allowing for a dynamic balance between them. It employs a meta-gradient adaptive weight controller to optimize this balance, which helps in maximizing long-term task performance while maintaining stability. The results show that AMFT achieves state-of-the-art performance on various reasoning tasks and demonstrates improved generalization on out-of-distribution challenges.'}, 'zh': {'title': '自适应元微调：平衡学习与探索的创新方法', 'desc': '自适应元微调（AMFT）通过动态平衡监督微调（SFT）和强化学习（RL），利用隐式奖励来提升大型语言模型（LLM）的性能和泛化能力。该方法将SFT和RL视为互补的奖励信号，而非独立的方法，从而引入了一种新的单阶段算法。AMFT的核心是一个元梯度自适应权重控制器，它将SFT-RL的平衡视为可学习的参数，动态优化以最大化长期任务表现。通过在多个具有挑战性的基准上进行评估，AMFT在超出分布（OOD）任务上展现了卓越的泛化能力，确立了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2508.07321', 'title': 'ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated\n  Factual Question Answering', 'url': 'https://huggingface.co/papers/2508.07321', 'abstract': "ObfusQA, a novel framework with multi-tiered obfuscation levels, evaluates the robustness and adaptability of Large Language Models (LLMs) by examining their performance on obfuscated questions.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid proliferation of Large Language Models (LLMs) has significantly contributed to the development of equitable AI systems capable of factual question-answering (QA). However, no known study tests the LLMs' robustness when presented with obfuscated versions of questions. To systematically evaluate these limitations, we propose a novel technique, ObfusQAte and, leveraging the same, introduce ObfusQA, a comprehensive, first of its kind, framework with multi-tiered obfuscation levels designed to examine LLM capabilities across three distinct dimensions: (i) Named-Entity Indirection, (ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these fine-grained distinctions in language, ObfusQA provides a comprehensive benchmark for evaluating LLM robustness and adaptability. Our study observes that LLMs exhibit a tendency to fail or generate hallucinated responses when confronted with these increasingly nuanced variations. To foster research in this direction, we make ObfusQAte publicly available.", 'score': 0, 'issue_id': 5360, 'pub_date': '2025-08-10', 'pub_date_card': {'ru': '10 августа', 'en': 'August 10', 'zh': '8月10日'}, 'hash': '0105fd6ce39aae2a', 'authors': ['Shubhra Ghosh', 'Abhilekh Borah', 'Aditya Kumar Guru', 'Kripabandhu Ghosh'], 'affiliations': ['IISER Kolkata, India', 'Indian Institute of Technology Patna, India', 'Manipal University Jaipur, India'], 'pdf_title_img': 'assets/pdf/title_img/2508.07321.jpg', 'data': {'categories': ['#open_source', '#security', '#benchmark', '#dataset', '#alignment', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Проверка прочности языковых моделей: ObfusQA раскрывает слабые места LLM', 'desc': 'ObfusQA - это новая система для оценки устойчивости и адаптивности больших языковых моделей (LLM) путем анализа их производительности на обфусцированных вопросах. Система использует многоуровневую обфускацию, включая косвенное именование сущностей, отвлекающие факторы и контекстуальную перегрузку. Исследование показало, что LLM склонны давать ошибочные или галлюцинаторные ответы при столкновении с такими вариациями вопросов. ObfusQAte, инструмент для создания обфусцированных вопросов, был опубликован для дальнейших исследований в этой области.'}, 'en': {'title': 'Testing LLMs: The Challenge of Obfuscated Questions', 'desc': "ObfusQA is a new framework designed to test how well Large Language Models (LLMs) handle tricky, obfuscated questions. It introduces different levels of obfuscation to assess LLMs' robustness in three areas: how they deal with named entities, distractors, and overloaded context. The study found that LLMs often struggle or produce incorrect answers when faced with these complex question formats. By making ObfusQAte available, the authors aim to encourage further research into improving LLM performance under challenging conditions."}, 'zh': {'title': '测试大型语言模型的鲁棒性与适应性', 'desc': 'ObfusQA是一个新颖的框架，旨在通过对模糊化问题的评估来测试大型语言模型（LLMs）的鲁棒性和适应性。该框架采用多层次的模糊化级别，涵盖了命名实体间接、干扰者间接和上下文过载三个维度。研究发现，当LLMs面对这些复杂的模糊化问题时，往往会出现失败或生成虚假回答的情况。为了推动这一领域的研究，我们将ObfusQAte公开发布。'}}}, {'id': 'https://huggingface.co/papers/2507.23726', 'title': 'Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving', 'url': 'https://huggingface.co/papers/2507.23726', 'abstract': 'Seed-Prover, a lemma-style reasoning model using Lean, achieves high performance in formal theorem proving and automated mathematical reasoning through iterative refinement and specialized geometry support.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose Seed-Prover, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves 78.1% of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.', 'score': 84, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'ab5bfbdad68eb6bf', 'authors': ['Luoxin Chen', 'Jinming Gu', 'Liankai Huang', 'Wenhao Huang', 'Zhicheng Jiang', 'Allan Jie', 'Xiaoran Jin', 'Xing Jin', 'Chenggang Li', 'Kaijing Ma', 'Cheng Ren', 'Jiawei Shen', 'Wenlei Shi', 'Tong Sun', 'He Sun', 'Jiahui Wang', 'Siran Wang', 'Zhihong Wang', 'Chenrui Wei', 'Shufa Wei', 'Yonghui Wu', 'Yuchen Wu', 'Yihang Xia', 'Huajian Xin', 'Fan Yang', 'Huaiyuan Ying', 'Hongyi Yuan', 'Zheng Yuan', 'Tianyang Zhan', 'Chi Zhang', 'Yue Zhang', 'Ge Zhang', 'Tianyun Zhao', 'Jianqiu Zhao', 'Yichi Zhou', 'Thomas Hanwen Zhu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2507.23726.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в автоматическом доказательстве теорем с помощью ИИ', 'desc': 'Seed-Prover - это модель для автоматического доказательства теорем, использующая язык Lean. Она применяет итеративное уточнение доказательств и специализированную поддержку геометрии. Модель достигает высокой производительности в формальном доказательстве теорем и автоматизированных математических рассуждениях. Seed-Prover превосходит предыдущие системы на нескольких эталонных наборах задач, включая формализованные задачи Международной математической олимпиады.'}, 'en': {'title': 'Seed-Prover: Revolutionizing Theorem Proving with Iterative Refinement', 'desc': 'The paper introduces Seed-Prover, a model designed for formal theorem proving and automated mathematical reasoning using the Lean programming language. It leverages iterative refinement and specialized geometry support to enhance its proof capabilities. By employing reinforcement learning and clear supervision from formal verification, Seed-Prover achieves impressive results on challenging mathematical problems. The model outperforms previous systems, proving a high percentage of formalized IMO problems and demonstrating significant advancements in automated reasoning.'}, 'zh': {'title': 'Seed-Prover：自动化数学推理的新突破', 'desc': 'Seed-Prover是一种基于Lean的引理风格推理模型，能够在形式定理证明和自动数学推理中实现高性能。该模型通过迭代优化和专门的几何支持，克服了传统自然语言推理的局限性。Seed-Prover利用Lean的反馈和自我总结来不断改进其证明过程，并设计了三种推理策略以应对国际数学奥林匹克（IMO）级别的问题。通过引入Seed-Geometry几何推理引擎，Seed-Prover在几何问题上也取得了显著进展，展示了形式验证与长链推理的有效性。'}}}, {'id': 'https://huggingface.co/papers/2507.23779', 'title': 'Phi-Ground Tech Report: Advancing Perception in GUI Grounding', 'url': 'https://huggingface.co/papers/2507.23779', 'abstract': 'The Phi-Ground model family achieves state-of-the-art performance in GUI grounding for multimodal reasoning models, improving accuracy across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the development of multimodal reasoning models, Computer Use Agents (CUAs), akin to Jarvis from "Iron Man", are becoming a reality. GUI grounding is a core component for CUAs to execute actual actions, similar to mechanical control in robotics, and it directly leads to the success or failure of the system. It determines actions such as clicking and typing, as well as related parameters like the coordinates for clicks. Current end-to-end grounding models still achieve less than 65\\% accuracy on challenging benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from being ready for deployment. % , as a single misclick can result in unacceptable consequences. In this work, we conduct an empirical study on the training of grounding models, examining details from data collection to model training. Ultimately, we developed the Phi-Ground model family, which achieves state-of-the-art performance across all five grounding benchmarks for models under 10B parameters in agent settings. In the end-to-end model setting, our model still achieves SOTA results with scores of \\textbf{43.2} on ScreenSpot-pro and \\textbf{27.2} on UI-Vision. We believe that the various details discussed in this paper, along with our successes and failures, not only clarify the construction of grounding models but also benefit other perception tasks. Project homepage: https://zhangmiaosen2000.github.io/Phi-Ground/{https://zhangmiaosen2000.github.io/Phi-Ground/}', 'score': 35, 'issue_id': 5127, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'e6bd9c919aacc874', 'authors': ['Miaosen Zhang', 'Ziqiang Xu', 'Jialiang Zhu', 'Qi Dai', 'Kai Qiu', 'Yifan Yang', 'Chong Luo', 'Tianyi Chen', 'Justin Wagle', 'Tim Franklin', 'Baining Guo'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.23779.jpg', 'data': {'categories': ['#agents', '#dataset', '#optimization', '#reasoning', '#training', '#multimodal'], 'emoji': '🖥️', 'ru': {'title': 'Phi-Ground: прорыв в точности привязки GUI для ИИ-агентов', 'desc': 'Семейство моделей Phi-Ground достигает передовых результатов в задаче привязки графического интерфейса для мультимодальных моделей рассуждения. Эти модели улучшают точность на различных бенчмарках для компьютерных агентов, способных взаимодействовать с GUI. Авторы провели эмпирическое исследование процесса обучения моделей привязки, рассмотрев детали от сбора данных до тренировки. Результаты работы могут быть полезны не только для создания моделей привязки, но и для других задач восприятия.'}, 'en': {'title': 'Revolutionizing GUI Grounding with Phi-Ground Models', 'desc': 'The Phi-Ground model family significantly enhances GUI grounding for multimodal reasoning models, achieving top performance on various benchmarks. This model is crucial for Computer Use Agents (CUAs) to perform tasks like clicking and typing accurately, which is essential for their effectiveness. Despite existing models struggling with accuracy below 65% on tough benchmarks, Phi-Ground demonstrates superior results, scoring 43.2 on ScreenSpot-pro and 27.2 on UI-Vision. The paper details the training process and insights gained, which can also aid in improving other perception tasks in machine learning.'}, 'zh': {'title': 'Phi-Ground：多模态推理的GUI定位新突破', 'desc': 'Phi-Ground模型系列在多模态推理模型的GUI定位方面达到了最先进的性能，提升了在多个基准测试中的准确性。GUI定位是计算机使用代理（CUA）执行实际操作的核心部分，直接影响系统的成功与否。当前的端到端定位模型在一些具有挑战性的基准测试中准确率仍低于65%，显示出其在实际应用中的不足。本文通过对定位模型的训练进行实证研究，最终开发出Phi-Ground模型系列，在代理设置下的五个定位基准测试中均取得了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.22879', 'title': 'RecGPT Technical Report', 'url': 'https://huggingface.co/papers/2507.22879', 'abstract': "RecGPT integrates large language models into recommender systems to focus on user intent, improving content diversity and satisfaction while enhancing merchant and platform performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.   To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.", 'score': 22, 'issue_id': 5124, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '2bd5536810f1694b', 'authors': ['Chao Yi', 'Dian Chen', 'Gaoyang Guo', 'Jiakai Tang', 'Jian Wu', 'Jing Yu', 'Mao Zhang', 'Sunhao Dai', 'Wen Chen', 'Wenjun Yang', 'Yuning Jiang', 'Zhujin Gao', 'Bo Zheng', 'Chi Li', 'Dimin Wang', 'Dixuan Wang', 'Fan Li', 'Fan Zhang', 'Haibin Chen', 'Haozhuang Liu', 'Jialin Zhu', 'Jiamang Wang', 'Jiawei Wu', 'Jin Cui', 'Ju Huang', 'Kai Zhang', 'Kan Liu', 'Lang Tian', 'Liang Rao', 'Longbin Li', 'Lulu Zhao', 'Na He', 'Peiyang Wang', 'Qiqi Huang', 'Tao Luo', 'Wenbo Su', 'Xiaoxiao He', 'Xin Tong', 'Xu Chen', 'Xunke Xi', 'Yang Li', 'Yaxuan Wu', 'Yeqiu Yang', 'Yi Hu', 'Yinnan Song', 'Yuchen Li', 'Yujie Luo', 'Yujin Yuan', 'Yuliang Yan', 'Zhengyang Wang', 'Zhibo Xiao', 'Zhixin Ma', 'Zile Zhou', 'Ziqi Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.22879.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#alignment', '#multimodal'], 'emoji': '🎯', 'ru': {'title': 'RecGPT: Рекомендации, ориентированные на намерения пользователей', 'desc': 'RecGPT - это новая система рекомендаций, интегрирующая большие языковые модели (LLM) для фокусировки на намерениях пользователей. Она улучшает разнообразие контента и удовлетворенность пользователей, а также повышает эффективность для продавцов и платформы. RecGPT использует многоэтапную парадигму обучения, включающую предварительное выравнивание с усиленным рассуждением и эволюцию самообучения. Система уже развернута в приложении Taobao и показывает стабильный рост производительности для всех заинтересованных сторон.'}, 'en': {'title': 'Empowering Recommendations with User Intent', 'desc': 'RecGPT is a new framework that enhances recommender systems by focusing on user intent rather than just historical data. It integrates large language models (LLMs) to better understand and predict user interests, which helps in retrieving more relevant items and generating clearer explanations. This approach reduces the risk of overfitting to past preferences, thereby improving content diversity and user satisfaction. By deploying RecGPT on the Taobao App, the system has shown significant performance improvements for users, merchants, and the platform itself, creating a more sustainable recommendation ecosystem.'}, 'zh': {'title': '以用户意图为中心的推荐系统新范式', 'desc': 'RecGPT 是一种将大型语言模型整合到推荐系统中的新框架，旨在关注用户意图。通过重新设计推荐流程，RecGPT 使推荐过程从单纯依赖历史数据转变为以用户意图为中心。该系统通过多阶段训练方法，结合推理增强的预对齐和自我训练，提升了推荐的准确性和多样性。实验结果表明，RecGPT 在用户满意度、商家曝光率和平台转化率等方面均取得了显著提升。'}}}, {'id': 'https://huggingface.co/papers/2507.23682', 'title': 'villa-X: Enhancing Latent Action Modeling in Vision-Language-Action\n  Models', 'url': 'https://huggingface.co/papers/2507.23682', 'abstract': 'The ViLLA framework enhances VLA models by incorporating latent actions, improving performance in both simulated and real-world robot manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual-Language-Action (VLA) models have emerged as a popular paradigm for learning robot manipulation policies that can follow language instructions and generalize to novel scenarios. Recent work has begun to explore the incorporation of latent actions, an abstract representation of visual change between two frames, into VLA pre-training. In this paper, we introduce villa-X, a novel Visual-Language-Latent-Action (ViLLA) framework that advances latent action modeling for learning generalizable robot manipulation policies. Our approach improves both how latent actions are learned and how they are incorporated into VLA pre-training. Together, these contributions enable villa-X to achieve superior performance across simulated environments including SIMPLER and LIBERO, as well as on two real-world robot setups including gripper and dexterous hand manipulation. We believe the ViLLA paradigm holds significant promise, and that our villa-X provides a strong foundation for future research.', 'score': 20, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'baa73e4730b01a97', 'authors': ['Xiaoyu Chen', 'Hangxing Wei', 'Pushi Zhang', 'Chuheng Zhang', 'Kaixin Wang', 'Yanjiang Guo', 'Rushuai Yang', 'Yucen Wang', 'Xinquan Xiao', 'Li Zhao', 'Jianyu Chen', 'Jiang Bian'], 'affiliations': ['Hong Kong University of Science and Technology', 'Microsoft Research', 'Nanjing University', 'Tsinghua University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2507.23682.jpg', 'data': {'categories': ['#optimization', '#agents', '#agi', '#games', '#robotics', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'ViLLA: Улучшение роботизированных манипуляций с помощью латентных действий', 'desc': 'Фреймворк ViLLA улучшает модели визуально-языкового действия (VLA) путем включения латентных действий. Это позволяет повысить производительность как в симулированных, так и в реальных задачах роботизированных манипуляций. Предложенный подход villa-X совершенствует как обучение латентным действиям, так и их интеграцию в предобучение VLA. Модель демонстрирует превосходные результаты в симуляторах SIMPLER и LIBERO, а также на реальных роботах с захватами и ловкими руками.'}, 'en': {'title': 'Enhancing Robot Manipulation with Latent Actions in ViLLA Framework', 'desc': 'The ViLLA framework enhances Visual-Language-Action (VLA) models by integrating latent actions, which represent abstract visual changes between frames. This integration improves the learning of robot manipulation policies that can effectively follow language instructions and adapt to new situations. The proposed villa-X model advances the way latent actions are learned and utilized during VLA pre-training, leading to better performance in both simulated and real-world tasks. Overall, the ViLLA paradigm shows great potential for future advancements in robot manipulation research.'}, 'zh': {'title': 'ViLLA框架：提升机器人操作的潜力', 'desc': 'ViLLA框架通过引入潜在动作来增强视觉-语言-动作（VLA）模型，从而提高机器人操作任务的性能。潜在动作是一种抽象表示，能够捕捉两个帧之间的视觉变化。本文介绍的villa-X是一个新颖的视觉-语言-潜在动作框架，旨在改进潜在动作建模，以学习可推广的机器人操作策略。我们的研究表明，villa-X在模拟环境和真实机器人设置中均表现出色，展示了ViLLA范式的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2507.22968', 'title': 'C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring\n  Challenges in Complex Conversations', 'url': 'https://huggingface.co/papers/2507.22968', 'abstract': "A benchmark dataset for Spoken Dialogue Models (SDMs) in English and Chinese is presented to evaluate their performance in understanding and emulating human spoken conversations, addressing challenges like ambiguity and context-dependency.  \t\t\t\t\tAI-generated summary \t\t\t\t Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.", 'score': 20, 'issue_id': 5124, 'pub_date': '2025-07-30', 'pub_date_card': {'ru': '30 июля', 'en': 'July 30', 'zh': '7月30日'}, 'hash': '3a2f5273d610d5d6', 'authors': ['Chengqian Ma', 'Wei Tao', 'Yiwen Guo'], 'affiliations': ['Independent Researcher', 'LIGHTSPEED', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2507.22968.jpg', 'data': {'categories': ['#survey', '#long_context', '#benchmark', '#dataset', '#multimodal'], 'emoji': '🗣️', 'ru': {'title': 'Бенчмарк для оценки разговорных ИИ-моделей в реальных условиях', 'desc': 'В статье представлен набор данных для оценки разговорных диалоговых моделей на английском и китайском языках. Этот бенчмарк позволяет оценить способность моделей понимать и имитировать человеческие разговоры, учитывая такие сложности как неоднозначность и контекстная зависимость. Набор данных содержит 1079 примеров и сопровождается методом оценки на основе больших языковых моделей, который хорошо коррелирует с человеческими оценками. Исследование направлено на комплексное изучение эффективности разговорных диалоговых моделей в решении практических задач.'}, 'en': {'title': 'Benchmarking Spoken Dialogue Models for Real-World Conversations', 'desc': "This paper introduces a benchmark dataset designed for evaluating Spoken Dialogue Models (SDMs) in both English and Chinese. The dataset aims to address the complexities of human spoken conversations, such as ambiguity and context-dependency, which are more pronounced in voice interactions compared to text. It includes 1,079 instances that reflect real-world dialogue scenarios, allowing for a thorough assessment of SDM performance. Additionally, the paper presents an evaluation method based on Large Language Models (LLMs) that aligns closely with human judgment, enhancing the understanding of SDMs' effectiveness."}, 'zh': {'title': '提升口语对话模型的评估标准', 'desc': '本文提出了一个用于评估口语对话模型（SDMs）性能的基准数据集，涵盖英语和中文，旨在理解和模拟人类口语对话。口语对话的复杂性体现在歧义性和上下文依赖性等挑战上，这些因素使得与文本基础的大型语言模型（LLMs）相比，SDMs的研究相对较少。数据集中包含1079个实例，并配备了一种基于LLM的评估方法，以更好地与人类判断相一致。通过这个数据集，研究者可以全面探讨SDMs在应对实际对话挑战中的表现。'}}}, {'id': 'https://huggingface.co/papers/2507.23277', 'title': 'iLRM: An Iterative Large 3D Reconstruction Model', 'url': 'https://huggingface.co/papers/2507.23277', 'abstract': 'iLRM, an iterative Large 3D Reconstruction Model, improves scalability and efficiency in 3D reconstruction by decoupling scene representation, using a two-stage attention scheme, and injecting high-resolution information.  \t\t\t\t\tAI-generated summary \t\t\t\t Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views.', 'score': 19, 'issue_id': 5137, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'af6d37e5e25a6d73', 'authors': ['Gyeongjin Kang', 'Seungtae Nam', 'Xiangyu Sun', 'Sameh Khamis', 'Abdelrahman Mohamed', 'Eunbyung Park'], 'affiliations': ['Rembrand Project', 'Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2507.23277.jpg', 'data': {'categories': ['#optimization', '#3d'], 'emoji': '🏛️', 'ru': {'title': 'Эффективная 3D-реконструкция: итеративный подход с гауссовым представлением', 'desc': 'iLRM - это итеративная модель для масштабируемой и эффективной 3D-реконструкции. Она использует трехмерное гауссово представление сцены, двухэтапную схему внимания и внедрение высокоразрешающей информации. Модель решает проблемы масштабируемости, характерные для трансформерных архитектур при обработке множества ракурсов. Эксперименты показывают превосходство iLRM по качеству и скорости реконструкции по сравнению с существующими методами.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with iLRM: Scalable and Efficient!', 'desc': 'The paper presents the iterative Large 3D Reconstruction Model (iLRM), which enhances the scalability and efficiency of 3D reconstruction processes. It achieves this by decoupling the scene representation from the input images, allowing for more compact 3D models. Additionally, iLRM employs a two-stage attention mechanism to minimize computational costs associated with multi-view interactions. By incorporating high-resolution information at each layer, the model ensures high-fidelity reconstructions while maintaining superior performance across various datasets.'}, 'zh': {'title': 'iLRM：高效可扩展的3D重建新模型', 'desc': 'iLRM（迭代大型3D重建模型）通过解耦场景表示、采用两阶段注意力机制和注入高分辨率信息，提升了3D重建的可扩展性和效率。该模型通过迭代优化生成3D高斯表示，能够在多个输入视图下实现快速且高质量的重建。与传统的全注意力机制相比，iLRM显著降低了计算成本，同时保持了重建的高保真度。实验结果表明，iLRM在重建质量和速度上均优于现有方法，尤其在处理更多输入视图时表现出更好的可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2507.21509', 'title': 'Persona Vectors: Monitoring and Controlling Character Traits in Language\n  Models', 'url': 'https://huggingface.co/papers/2507.21509', 'abstract': "Persona vectors in large language models can monitor and control personality changes during training and deployment, enabling the identification and mitigation of undesirable traits.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.", 'score': 17, 'issue_id': 5127, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '8088854aaf027260', 'authors': ['Runjin Chen', 'Andy Arditi', 'Henry Sleight', 'Owain Evans', 'Jack Lindsey'], 'affiliations': ['Anthropic', 'Anthropic Fellows Program', 'Constellation', 'Truthful AI', 'UC Berkeley', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2507.21509.jpg', 'data': {'categories': ['#rlhf', '#hallucinations', '#data', '#ethics', '#training', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Векторы персоны: ключ к контролю личности ИИ-ассистентов', 'desc': 'Статья представляет концепцию векторов персоны в больших языковых моделях. Эти векторы позволяют отслеживать и контролировать изменения личности ассистента во время обучения и использования модели. Исследователи обнаружили, что векторы персоны могут предсказывать сдвиги в личности после дообучения и помогают выявлять нежелательные черты. Метод извлечения векторов персоны автоматизирован и может применяться к любой интересующей черте личности.'}, 'en': {'title': 'Controlling AI Personalities with Persona Vectors', 'desc': "This paper introduces the concept of persona vectors in large language models, which are used to track and manage personality traits during the model's training and deployment phases. The authors demonstrate that these vectors can identify undesirable traits like harmful behavior or excessive flattery by analyzing the model's activation space. They show that personality shifts can be predicted and controlled, allowing for interventions to mitigate negative changes. Additionally, the method can flag problematic training data that may lead to these undesirable personality traits, making it a valuable tool for improving AI behavior."}, 'zh': {'title': '利用人格向量控制语言模型的人格变化', 'desc': '本文探讨了在大型语言模型中使用人格向量来监控和控制模型在训练和部署过程中的人格变化。研究发现，模型的激活空间中存在与多种人格特征相关的人格向量，例如恶意、谄媚和幻觉倾向。通过这些向量，可以预测和控制在微调后可能出现的人格变化，并且可以通过后期干预来减轻这些变化。该方法是自动化的，可以应用于任何感兴趣的人格特征，只需提供自然语言描述。'}}}, {'id': 'https://huggingface.co/papers/2507.23698', 'title': 'Scalable Multi-Task Reinforcement Learning for Generalizable Spatial\n  Intelligence in Visuomotor Agents', 'url': 'https://huggingface.co/papers/2507.23698', 'abstract': "Reinforcement Learning enhances generalizable spatial reasoning and interaction in 3D environments through cross-view goal specification and automated task synthesis, achieving zero-shot generalization and improved interaction success rates.  \t\t\t\t\tAI-generated summary \t\t\t\t While Reinforcement Learning (RL) has achieved remarkable success in language modeling, its triumph hasn't yet fully translated to visuomotor agents. A primary challenge in RL models is their tendency to overfit specific tasks or environments, thereby hindering the acquisition of generalizable behaviors across diverse settings. This paper provides a preliminary answer to this challenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can achieve zero-shot generalization to unseen worlds. Specifically, we explore RL's potential to enhance generalizable spatial reasoning and interaction capabilities in 3D worlds. To address challenges in multi-task RL representation, we analyze and establish cross-view goal specification as a unified multi-task goal space for visuomotor policies. Furthermore, to overcome the significant bottleneck of manual task design, we propose automated task synthesis within the highly customizable Minecraft environment for large-scale multi-task RL training, and we construct an efficient distributed RL framework to support this. Experimental results show RL significantly boosts interaction success rates by 4times and enables zero-shot generalization of spatial reasoning across diverse environments, including real-world settings. Our findings underscore the immense potential of RL training in 3D simulated environments, especially those amenable to large-scale task generation, for significantly advancing visuomotor agents' spatial reasoning.", 'score': 7, 'issue_id': 5125, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '4cb697aecb943154', 'authors': ['Shaofei Cai', 'Zhancun Mu', 'Haiwen Xia', 'Bowei Zhang', 'Anji Liu', 'Yitao Liang'], 'affiliations': ['Institute for Artificial Intelligence, Peking University', 'School of Computing, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2507.23698.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#games', '#3d', '#rl'], 'emoji': '🧠', 'ru': {'title': 'RL открывает новые горизонты пространственного мышления для ИИ', 'desc': 'Данная статья представляет метод улучшения пространственного мышления и взаимодействия агентов в 3D-средах с помощью обучения с подкреплением (RL). Авторы предлагают использовать кросс-видовую спецификацию целей и автоматизированный синтез задач для достижения обобщения без предварительного обучения. Эксперименты проводились в среде Minecraft и показали значительное улучшение успешности взаимодействия агентов. Результаты демонстрируют потенциал обучения с подкреплением для развития визуально-моторных навыков искусственных агентов.'}, 'en': {'title': 'Empowering 3D Agents with Reinforcement Learning for Generalized Interaction', 'desc': 'This paper discusses how Reinforcement Learning (RL) can improve the ability of agents to understand and interact in 3D environments, like Minecraft. It addresses the problem of RL models overfitting to specific tasks, which limits their ability to generalize to new situations. By using cross-view goal specification and automated task synthesis, the authors show that RL can help agents achieve zero-shot generalization, meaning they can perform well in unseen environments without prior training. The results indicate that RL can significantly enhance interaction success rates and spatial reasoning in diverse settings, including real-world applications.'}, 'zh': {'title': '强化学习：提升3D环境中的空间推理与交互能力', 'desc': '强化学习（RL）在3D环境中通过跨视角目标指定和自动化任务合成，增强了可推广的空间推理和交互能力，实现了零样本泛化和提高的交互成功率。本文探讨了RL在Minecraft中微调的视觉运动代理如何在未见过的世界中实现零样本泛化。我们分析并建立了跨视角目标指定作为视觉运动策略的统一多任务目标空间，以应对多任务RL表示中的挑战。此外，我们提出在高度可定制的Minecraft环境中进行自动化任务合成，以支持大规模多任务RL训练。'}}}, {'id': 'https://huggingface.co/papers/2507.23374', 'title': 'NeRF Is a Valuable Assistant for 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2507.23374', 'abstract': 'NeRF-GS combines Neural Radiance Fields and 3D Gaussian Splatting to enhance 3D scene representation and performance through joint optimization and shared spatial information.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation.', 'score': 6, 'issue_id': 5127, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '3bc4d2b12fc82c0f', 'authors': ['Shuangkang Fang', 'I-Chao Shen', 'Takeo Igarashi', 'Yufeng Wang', 'ZeSheng Wang', 'Yi Yang', 'Wenrui Ding', 'Shuchang Zhou'], 'affiliations': ['Beihang University', 'StepFun', 'The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2507.23374.jpg', 'data': {'categories': ['#3d', '#benchmark'], 'emoji': '🌟', 'ru': {'title': 'NeRF-GS: Синергия нейронных полей и гауссова сплаттинга для революционного 3D-моделирования', 'desc': 'NeRF-GS - это новая система, объединяющая нейронные радиационные поля (NeRF) и трехмерное гауссово сплаттинг (3DGS) для улучшения представления 3D-сцен. Она использует непрерывное пространственное представление NeRF для устранения ограничений 3DGS, таких как чувствительность к инициализации гауссианов и слабые межгауссовые корреляции. NeRF-GS оптимизирует обе модели, используя общую пространственную информацию, и вводит оптимизацию остаточных векторов для улучшения персонализированных возможностей 3DGS. Экспериментальные результаты показывают, что NeRF-GS превосходит существующие методы и достигает наилучших показателей в представлении 3D-сцен.'}, 'en': {'title': 'Enhancing 3D Scene Representation with NeRF-GS', 'desc': 'NeRF-GS is a new framework that combines Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) to improve how 3D scenes are represented. By optimizing both methods together, it addresses the weaknesses of 3DGS, such as its sensitivity to initial conditions and limited understanding of spatial relationships. The framework aligns the spatial features of 3DGS with those of NeRF, allowing for better performance through shared information. Experiments show that NeRF-GS outperforms existing techniques, highlighting the benefits of integrating these two approaches for enhanced 3D scene representation.'}, 'zh': {'title': 'NeRF-GS：融合神经辐射场与三维高斯点云的创新框架', 'desc': 'NeRF-GS是一个新颖的框架，它结合了神经辐射场（NeRF）和三维高斯点云（3DGS），通过联合优化和共享空间信息来增强三维场景的表示和性能。该框架利用NeRF的连续空间表示，克服了3DGS的一些局限性，如对高斯初始化的敏感性和空间意识的不足。通过逐步对齐3DGS的空间特征与NeRF，NeRF-GS使得两种表示能够在同一场景中共同优化。实验结果表明，NeRF-GS在基准数据集上超越了现有方法，达到了最先进的性能，证明了NeRF和3DGS是互补的，而非竞争的。'}}}, {'id': 'https://huggingface.co/papers/2507.21584', 'title': 'TARS: MinMax Token-Adaptive Preference Strategy for Hallucination\n  Reduction in MLLMs', 'url': 'https://huggingface.co/papers/2507.21584', 'abstract': 'TARS, a token-adaptive preference strategy, improves multimodal large language models by reducing hallucinations through min-max optimization under semantic constraints.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) enable vision-language reasoning, yet often generate plausible outputs that are factually incorrect or visually ungrounded, thereby compromising their reliability. Direct preference optimization (DPO) is a common strategy for correcting hallucinations by aligning model outputs with human preferences. Existing DPO strategies typically treat hallucination-related preferences as fixed targets, relying on static supervision signals during training. This approach tends to overfit to superficial linguistic cues in preference data, leading to distributional rigidity and spurious correlations that impair grounding in causally relevant visual information. To overcome this limitation, we propose TARS, a token-adaptive preference strategy that reformulates DPO as a min-max optimization problem. TARS maximizes token-level distributional shifts under semantic constraints to simulate alignment uncertainty, and simultaneously minimizes the expected preference loss under these controlled perturbations. This joint objective preserves causal grounding while mitigating overfitting to preference patterns, thereby reducing hallucinations in multimodal reasoning. We evaluate TARS on multiple hallucination benchmarks and find consistently strong performance. Using only 4.8k preference samples and no expert feedback, TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on several key metrics.', 'score': 6, 'issue_id': 5128, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': 'e9b8a4abec301022', 'authors': ['Kejia Zhang', 'Keda Tao', 'Zhiming Luo', 'Chang Liu', 'Jiasheng Tang', 'Huan Wang'], 'affiliations': ['AWS AI Lab, Amazon', 'DAMO Academy, Alibaba Group', 'Department of Artificial Intelligence, Xiamen University', 'Hupan Laboratory', 'School of Engineering, Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2507.21584.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#multimodal', '#benchmark', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'TARS: Адаптивная оптимизация для борьбы с галлюцинациями в мультимодальных ИИ', 'desc': 'TARS - это новая стратегия оптимизации предпочтений для мультимодальных больших языковых моделей. Она использует min-max оптимизацию с семантическими ограничениями для уменьшения галлюцинаций. TARS максимизирует сдвиги распределения на уровне токенов, одновременно минимизируя ожидаемые потери предпочтений. Это позволяет сохранить причинно-следственную связь и уменьшить переобучение на паттернах предпочтений, снижая уровень галлюцинаций в мультимодальных рассуждениях.'}, 'en': {'title': 'TARS: Reducing Hallucinations in MLLMs with Adaptive Preferences', 'desc': 'The paper introduces TARS, a novel token-adaptive preference strategy designed to enhance multimodal large language models (MLLMs) by minimizing hallucinations. TARS reformulates direct preference optimization (DPO) as a min-max optimization problem, allowing for dynamic adjustments to token-level distributions while adhering to semantic constraints. This approach helps prevent overfitting to fixed preference patterns, which can lead to misleading outputs, by introducing controlled perturbations that maintain causal grounding. The results demonstrate that TARS significantly reduces hallucination rates and improves performance on various benchmarks, outperforming traditional DPO methods.'}, 'zh': {'title': 'TARS：减少幻觉的智能偏好策略', 'desc': 'TARS是一种基于令牌自适应的偏好策略，旨在通过在语义约束下进行最小-最大优化来减少多模态大语言模型中的幻觉现象。传统的直接偏好优化（DPO）方法通常将幻觉相关的偏好视为固定目标，导致模型过拟合于表面语言线索。TARS通过重新构建DPO为最小-最大优化问题，最大化令牌级别的分布变化，同时最小化期望的偏好损失，从而保持因果基础并减少幻觉。实验表明，TARS在多个基准测试中表现优异，显著降低了幻觉率。'}}}, {'id': 'https://huggingface.co/papers/2507.20519', 'title': 'AgroBench: Vision-Language Model Benchmark in Agriculture', 'url': 'https://huggingface.co/papers/2507.20519', 'abstract': 'AgroBench evaluates vision-language models across agricultural tasks, revealing areas for improvement in fine-grained identification, particularly weed identification, with expert-annotated categories.  \t\t\t\t\tAI-generated summary \t\t\t\t Precise automated understanding of agricultural tasks such as disease identification is essential for sustainable crop production. Recent advances in vision-language models (VLMs) are expected to further expand the range of agricultural tasks by facilitating human-model interaction through easy, text-based communication. Here, we introduce AgroBench (Agronomist AI Benchmark), a benchmark for evaluating VLM models across seven agricultural topics, covering key areas in agricultural engineering and relevant to real-world farming. Unlike recent agricultural VLM benchmarks, AgroBench is annotated by expert agronomists. Our AgroBench covers a state-of-the-art range of categories, including 203 crop categories and 682 disease categories, to thoroughly evaluate VLM capabilities. In our evaluation on AgroBench, we reveal that VLMs have room for improvement in fine-grained identification tasks. Notably, in weed identification, most open-source VLMs perform close to random. With our wide range of topics and expert-annotated categories, we analyze the types of errors made by VLMs and suggest potential pathways for future VLM development. Our dataset and code are available at https://dahlian00.github.io/AgroBenchPage/ .', 'score': 4, 'issue_id': 5125, 'pub_date': '2025-07-28', 'pub_date_card': {'ru': '28 июля', 'en': 'July 28', 'zh': '7月28日'}, 'hash': 'efa19cc739cbe95e', 'authors': ['Risa Shinoda', 'Nakamasa Inoue', 'Hirokatsu Kataoka', 'Masaki Onishi', 'Yoshitaka Ushiku'], 'affiliations': ['Kyoto University', 'National Institute of Advanced Industrial Science and Technology (AIST)', 'OMRON SINIC', 'The University of Osaka', 'Tokyo Institute of Technology', 'Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2507.20519.jpg', 'data': {'categories': ['#cv', '#open_source', '#science', '#dataset', '#benchmark'], 'emoji': '🌾', 'ru': {'title': 'AgroBench: экспертная оценка ИИ в сельском хозяйстве', 'desc': 'AgroBench - это новый эталонный тест для оценки моделей компьютерного зрения и обработки естественного языка в сельскохозяйственных задачах. Он охватывает семь сельскохозяйственных тем и включает 203 категории культур и 682 категории болезней, аннотированные экспертами-агрономами. Тестирование показало, что существующие модели имеют значительные возможности для улучшения в задачах точной идентификации, особенно при распознавании сорняков. Авторы анализируют типы ошибок моделей и предлагают пути для их дальнейшего развития.'}, 'en': {'title': 'Enhancing Agricultural AI: The AgroBench Benchmark', 'desc': 'AgroBench is a benchmark designed to evaluate vision-language models (VLMs) specifically in the context of agricultural tasks. It focuses on fine-grained identification, such as accurately recognizing different types of weeds and diseases in crops, using categories annotated by expert agronomists. The benchmark includes a comprehensive set of 203 crop categories and 682 disease categories, highlighting the current limitations of VLMs in these areas. The findings indicate that many existing VLMs struggle with precise identification, particularly in weed detection, suggesting significant opportunities for improvement in future model development.'}, 'zh': {'title': '提升农业任务中的视觉-语言模型表现', 'desc': 'AgroBench是一个用于评估视觉-语言模型（VLM）在农业任务中的表现的基准。它涵盖了七个农业主题，并由专家农学家进行标注，确保数据的准确性。研究发现，当前的VLM在细粒度识别任务，特别是杂草识别方面表现不佳，许多开源模型的表现接近随机。通过分析VLM的错误类型，AgroBench为未来的模型发展提供了改进的方向。'}}}, {'id': 'https://huggingface.co/papers/2507.23436', 'title': 'Beyond Linear Bottlenecks: Spline-Based Knowledge Distillation for\n  Culturally Diverse Art Style Classification', 'url': 'https://huggingface.co/papers/2507.23436', 'abstract': "Enhancing dual-teacher self-supervised frameworks with Kolmogorov-Arnold Networks improves art style classification by better modeling nonlinear feature correlations and disentangling complex style manifolds.  \t\t\t\t\tAI-generated summary \t\t\t\t Art style classification remains a formidable challenge in computational aesthetics due to the scarcity of expertly labeled datasets and the intricate, often nonlinear interplay of stylistic elements. While recent dual-teacher self-supervised frameworks reduce reliance on labeled data, their linear projection layers and localized focus struggle to model global compositional context and complex style-feature interactions. We enhance the dual-teacher knowledge distillation framework to address these limitations by replacing conventional MLP projection and prediction heads with Kolmogorov-Arnold Networks (KANs). Our approach retains complementary guidance from two teacher networks, one emphasizing localized texture and brushstroke patterns, the other capturing broader stylistic hierarchies while leveraging KANs' spline-based activations to model nonlinear feature correlations with mathematical precision. Experiments on WikiArt and Pandora18k demonstrate that our approach outperforms the base dual teacher architecture in Top-1 accuracy. Our findings highlight the importance of KANs in disentangling complex style manifolds, leading to better linear probe accuracy than MLP projections.", 'score': 3, 'issue_id': 5128, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '43c3429e74f56b34', 'authors': ['Abdellah Zakaria Sellam', 'Salah Eddine Bekhouche', 'Cosimo Distante', 'Abdelmalik Taleb-Ahmed'], 'affiliations': ['Department of Innovation Engineering, University of Salento', 'Institute of Applied Sciences and Intelligent Systems CNR, Via per Monteroni, 73100 Lecce, Italy', 'UPV/EHU, University of the Basque Country, 20018 San Sebastian, Spain', 'Université Polytechnique Hauts-de-France, Université de Lille, CNRS, 59313 Valenciennes, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.23436.jpg', 'data': {'categories': ['#cv', '#math', '#training', '#optimization'], 'emoji': '🎨', 'ru': {'title': 'Сети Колмогорова-Арнольда улучшают самообучение в классификации стилей искусства', 'desc': 'Статья представляет усовершенствованный метод самообучения с двумя учителями для классификации стилей искусства. Авторы заменяют стандартные MLP-слои на сети Колмогорова-Арнольда (KAN) для лучшего моделирования нелинейных корреляций признаков. Этот подход сохраняет преимущества двух сетей-учителей, фокусируясь как на локальных текстурах, так и на глобальных стилистических иерархиях. Эксперименты показывают превосходство предложенного метода над базовой архитектурой с двумя учителями в точности классификации.'}, 'en': {'title': 'Harnessing KANs for Superior Art Style Classification', 'desc': 'This paper presents an enhancement to dual-teacher self-supervised frameworks for art style classification by integrating Kolmogorov-Arnold Networks (KANs). The authors argue that traditional linear projection layers fail to capture the complex, nonlinear relationships between stylistic features. By using KANs, which utilize spline-based activations, the model can better represent these intricate correlations and disentangle complex style manifolds. Experimental results show that this improved framework significantly increases classification accuracy on datasets like WikiArt and Pandora18k compared to the original dual-teacher architecture.'}, 'zh': {'title': '利用KANs提升艺术风格分类的准确性', 'desc': '本论文提出了一种增强的双教师自监督框架，通过引入Kolmogorov-Arnold网络（KANs）来改善艺术风格分类。传统的线性投影层无法有效建模复杂的风格特征交互，而KANs能够更好地捕捉非线性特征相关性。我们的方法结合了两个教师网络的互补指导，一个专注于局部纹理和笔触模式，另一个则关注更广泛的风格层次。实验结果表明，使用KANs的框架在WikiArt和Pandora18k数据集上显著提高了分类准确率。'}}}, {'id': 'https://huggingface.co/papers/2507.23632', 'title': 'On the Expressiveness of Softmax Attention: A Recurrent Neural Network\n  Perspective', 'url': 'https://huggingface.co/papers/2507.23632', 'abstract': 'Softmax attention is more expressive than linear attention due to its recurrent form, which can be analyzed using RNN components.  \t\t\t\t\tAI-generated summary \t\t\t\t Since its introduction, softmax attention has become the backbone of modern transformer architectures due to its expressiveness and scalability across a wide range of tasks. However, the main drawback of softmax attention is the quadratic memory requirement and computational complexity with respect to the sequence length. By replacing the softmax nonlinearity, linear attention and similar methods have been introduced to avoid the quadratic bottleneck of softmax attention. Despite these linear forms of attention being derived from the original softmax formulation, they typically lag in terms of downstream accuracy. While strong intuition of the softmax nonlinearity on the query and key inner product suggests that it has desirable properties compared to other nonlinearities, the question of why this discrepancy exists still remains unanswered. This work demonstrates that linear attention is an approximation of softmax attention by deriving the recurrent form of softmax attention. Using this form, each part of softmax attention can be described in the language of recurrent neural networks (RNNs). Describing softmax attention as an RNN allows for the ablation of the components of softmax attention to understand the importance of each part and how they interact. In this way, our work helps explain why softmax attention is more expressive than its counterparts.', 'score': 2, 'issue_id': 5124, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'b07ddf6cb6b8bee8', 'authors': ['Gabriel Mongaras', 'Eric C. Larson'], 'affiliations': ['Lyle School of Engineering Southern Methodist University Dallas, TX 75205'], 'pdf_title_img': 'assets/pdf/title_img/2507.23632.jpg', 'data': {'categories': ['#optimization', '#architecture', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Раскрывая силу софтмакс-внимания через призму RNN', 'desc': 'Статья исследует различия между софтмакс-вниманием и линейным вниманием в нейронных сетях. Авторы показывают, что софтмакс-внимание можно представить в рекуррентной форме, аналогичной рекуррентным нейронным сетям (RNN). Это позволяет провести анализ компонентов софтмакс-внимания и объяснить его большую выразительность по сравнению с линейными аналогами. Работа помогает понять, почему софтмакс-внимание остается основой современных трансформерных архитектур, несмотря на квадратичную сложность.'}, 'en': {'title': 'Unlocking the Power of Softmax Attention', 'desc': "This paper explores the differences between softmax attention and linear attention in machine learning models, particularly in transformers. It shows that softmax attention, which is more expressive, can be understood through the lens of recurrent neural networks (RNNs). By analyzing softmax attention as an RNN, the authors can break down its components to see how they contribute to its performance. The findings clarify why softmax attention outperforms linear attention in terms of accuracy despite the latter's computational efficiency."}, 'zh': {'title': '软max注意力的优势解析', 'desc': '本文探讨了softmax注意力与线性注意力的区别。softmax注意力因其表达能力强而成为现代变换器架构的基础，但其在序列长度上的计算复杂度和内存需求是一个主要缺点。通过将softmax非线性替换为线性注意力，研究者们试图解决这一瓶颈。本文表明，线性注意力实际上是softmax注意力的一种近似，并通过递归神经网络的语言来描述softmax注意力的各个部分，从而揭示其更强表达能力的原因。'}}}, {'id': 'https://huggingface.co/papers/2507.14793', 'title': 'Flow Equivariant Recurrent Neural Networks', 'url': 'https://huggingface.co/papers/2507.14793', 'abstract': "Equivariant neural network architectures are extended to handle time-parameterized transformations, improving performance in sequence models like RNNs.  \t\t\t\t\tAI-generated summary \t\t\t\t Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of `flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us.", 'score': 2, 'issue_id': 5126, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': 'eb29bf11c603c730', 'authors': ['T. Anderson Keller'], 'affiliations': ['The Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA 15213'], 'pdf_title_img': 'assets/pdf/title_img/2507.14793.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization'], 'emoji': '⏳', 'ru': {'title': 'Эквивариантность во времени: новый подход к обработке последовательностей', 'desc': "Статья расширяет концепцию эквивариантных нейронных сетей для обработки преобразований, параметризованных во времени. Это улучшает производительность рекуррентных нейронных сетей (RNN) и других последовательностных моделей. Авторы вводят понятие 'потоков' - однопараметрических подгрупп Ли, описывающих естественные трансформации во времени, такие как визуальное движение. Эксперименты показывают, что потоково-эквивариантные модели значительно превосходят стандартные RNN по скорости обучения и способности к обобщению."}, 'en': {'title': 'Enhancing RNNs with Time-Parameter Equivariance', 'desc': 'This paper extends equivariant neural network architectures to include time-parameterized transformations, which enhances their performance in sequence models like recurrent neural networks (RNNs). It highlights that traditional RNNs do not adequately account for the smooth, continuous changes in data over time, leading to inefficiencies. By introducing flow equivariance, the authors demonstrate that these new models can better handle temporal symmetries, resulting in improved training speed and generalization capabilities. This work aims to create sequence models that align more closely with the natural transformations observed in the real world.'}, 'zh': {'title': '提升序列模型性能的时间等变网络', 'desc': '本文扩展了等变神经网络架构，以处理时间参数化的变换，从而提高序列模型（如RNN）的性能。我们发现标准的RNN通常不具备流等变性，无法以几何结构的方式对移动刺激进行变换。通过引入流等变性，我们的模型在训练速度、长度泛化和速度泛化等方面显著优于非等变模型。此研究为构建尊重时间参数化对称性的序列模型奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2507.23404', 'title': 'Enhanced Arabic Text Retrieval with Attentive Relevance Scoring', 'url': 'https://huggingface.co/papers/2507.23404', 'abstract': 'An enhanced Dense Passage Retrieval framework for Arabic uses a novel Attentive Relevance Scoring mechanism to improve retrieval performance and ranking accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at https://github.com/Bekhouche/APR{GitHub}.', 'score': 1, 'issue_id': 5128, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '5e9a40999faf8be8', 'authors': ['Salah Eddine Bekhouche', 'Azeddine Benlamoudi', 'Yazid Bounab', 'Fadi Dornaika', 'Abdenour Hadid'], 'affiliations': ['Faculty of Pharmacy, Helsinki University, Helsinki, Finland', 'IKERBASQUE, Basque Foundation for Science, Bilbao, Spain', 'Lab. de Genie Electrique (LAGE), University of Ouargla, Ouargla, Algeria', 'Sorbonne University Abu Dhabi, Abu Dhabi, UAE', 'University of the Basque Country UPV/EHU, San Sebastian, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2507.23404.jpg', 'data': {'categories': ['#architecture', '#open_source', '#low_resource', '#multilingual', '#dataset'], 'emoji': '🕌', 'ru': {'title': 'Улучшенный поиск по арабским текстам с помощью механизма внимания', 'desc': 'Статья представляет усовершенствованную систему плотного поиска пассажей (Dense Passage Retrieval) для арабского языка. Авторы разработали новый механизм оценки релевантности на основе внимания (Attentive Relevance Scoring), который более эффективно моделирует семантическую связь между вопросами и текстовыми фрагментами. Система интегрирует предобученные языковые модели для арабского языка и архитектурные улучшения для повышения точности ранжирования при ответах на вопросы. Код проекта доступен в открытом репозитории на GitHub.'}, 'en': {'title': 'Enhancing Arabic Retrieval with Attentive Relevance Scoring', 'desc': 'This paper introduces an improved Dense Passage Retrieval (DPR) framework tailored for the Arabic language, addressing its unique challenges in natural language processing. The key innovation is the Attentive Relevance Scoring (ARS) mechanism, which enhances the way relevance is assessed between questions and passages. By utilizing pre-trained Arabic language models and refining the architecture, the framework boosts both retrieval performance and ranking accuracy. This advancement aims to better support information retrieval tasks in Arabic, a language that has been underrepresented in NLP research.'}, 'zh': {'title': '提升阿拉伯语检索性能的新方法', 'desc': '本文提出了一种针对阿拉伯语的增强型密集段落检索框架，旨在提高检索性能和排名准确性。我们引入了一种新颖的注意力相关评分机制，替代了传统的交互机制，更有效地建模问题与段落之间的语义相关性。该方法结合了预训练的阿拉伯语语言模型和架构改进，显著提升了回答阿拉伯语问题时的检索效果。我们的代码已公开，供研究人员使用。'}}}, {'id': 'https://huggingface.co/papers/2507.23257', 'title': 'Efficient Machine Unlearning via Influence Approximation', 'url': 'https://huggingface.co/papers/2507.23257', 'abstract': 'The paper introduces the Influence Approximation Unlearning (IAU) algorithm, which leverages incremental learning principles to efficiently address the computational challenges of influence-based unlearning in machine learning models.  \t\t\t\t\tAI-generated summary \t\t\t\t Due to growing privacy concerns, machine unlearning, which aims at enabling machine learning models to ``forget" specific training data, has received increasing attention. Among existing methods, influence-based unlearning has emerged as a prominent approach due to its ability to estimate the impact of individual training samples on model parameters without retraining. However, this approach suffers from prohibitive computational overhead arising from the necessity to compute the Hessian matrix and its inverse across all training samples and parameters, rendering it impractical for large-scale models and scenarios involving frequent data deletion requests. This highlights the difficulty of forgetting. Inspired by cognitive science, which suggests that memorizing is easier than forgetting, this paper establishes a theoretical link between memorizing (incremental learning) and forgetting (unlearning). This connection allows machine unlearning to be addressed from the perspective of incremental learning. Unlike the time-consuming Hessian computations in unlearning (forgetting), incremental learning (memorizing) typically relies on more efficient gradient optimization, which supports the aforementioned cognitive theory. Based on this connection, we introduce the Influence Approximation Unlearning (IAU) algorithm for efficient machine unlearning from the incremental perspective. Extensive empirical evaluations demonstrate that IAU achieves a superior balance among removal guarantee, unlearning efficiency, and comparable model utility, while outperforming state-of-the-art methods across diverse datasets and model architectures. Our code is available at https://github.com/Lolo1222/IAU.', 'score': 0, 'issue_id': 5131, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'e1e0a29f18521e64', 'authors': ['Jiawei Liu', 'Chenwang Wu', 'Defu Lian', 'Enhong Chen'], 'affiliations': ['Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China', 'School of Artificial Intelligence and Data Science, University of Science and Technology of China, Hefei, Anhui 230000, China', 'School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23257.jpg', 'data': {'categories': ['#optimization', '#security', '#training', '#data'], 'emoji': '🧠', 'ru': {'title': 'Эффективное разобучение через призму инкрементного обучения', 'desc': 'Статья представляет алгоритм Influence Approximation Unlearning (IAU) для эффективного машинного разобучения. IAU использует принципы инкрементного обучения, чтобы преодолеть вычислительные сложности разобучения на основе влияния в моделях машинного обучения. Алгоритм устанавливает теоретическую связь между запоминанием (инкрементное обучение) и забыванием (разобучение), что позволяет решать задачу разобучения с точки зрения инкрементного обучения. Эмпирические исследования показывают, что IAU достигает превосходного баланса между гарантией удаления, эффективностью разобучения и сохранением полезности модели.'}, 'en': {'title': 'Efficient Unlearning through Incremental Learning: Introducing IAU', 'desc': 'The paper presents the Influence Approximation Unlearning (IAU) algorithm, which aims to improve the efficiency of influence-based unlearning in machine learning models. It addresses the high computational costs associated with traditional methods that require extensive calculations of the Hessian matrix for each training sample. By drawing parallels between the processes of memorizing and forgetting, the authors leverage incremental learning techniques to facilitate more efficient unlearning. Empirical results show that IAU not only ensures effective data removal but also maintains model performance, outperforming existing unlearning methods.'}, 'zh': {'title': '高效遗忘：影响近似遗忘算法的创新之路', 'desc': '本文介绍了一种名为影响近似遗忘（IAU）算法的新方法，该算法利用增量学习的原理来高效解决基于影响的遗忘在机器学习模型中的计算挑战。随着隐私问题的日益关注，机器遗忘成为一个重要的研究领域，尤其是影响基于遗忘的方法因其无需重新训练模型而受到青睐。然而，现有方法在计算海森矩阵及其逆矩阵时面临巨大的计算开销，限制了其在大规模模型中的应用。通过建立记忆（增量学习）与遗忘（遗忘学习）之间的理论联系，IAU算法实现了更高效的机器遗忘，且在多个数据集和模型架构上表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2508.02193', 'title': 'Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference', 'url': 'https://huggingface.co/papers/2508.02193', 'abstract': 'Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.', 'score': 61, 'issue_id': 5199, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'bec183ec45598da2', 'authors': ['Yuxuan Song', 'Zheng Zhang', 'Cheng Luo', 'Pengyang Gao', 'Fan Xia', 'Hao Luo', 'Zheng Li', 'Yuehang Yang', 'Hongli Yu', 'Xingwei Qu', 'Yuwei Fu', 'Jing Su', 'Ge Zhang', 'Wenhao Huang', 'Mingxuan Wang', 'Lin Yan', 'Xiaoying Jia', 'Jingjing Liu', 'Wei-Ying Ma', 'Ya-Qin Zhang', 'Yonghui Wu', 'Hao Zhou'], 'affiliations': ['ByteDance', 'Institute for AI Industry Research (AIR), Tsinghua University', 'SIA-Lab of Tsinghua AIR and ByteDance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2508.02193.jpg', 'data': {'categories': ['#diffusion', '#inference', '#benchmark', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'Революция в скорости генерации текста без потери качества', 'desc': 'Seed Diffusion Preview - это новая языковая модель, основанная на дискретной диффузии. Она обеспечивает очень быструю генерацию текста благодаря параллельному неупорядоченному декодированию. Модель достигает скорости вывода 2146 токенов в секунду на GPU H20, значительно превосходя аналоги Mercury и Gemini Diffusion. При этом Seed Diffusion Preview сохраняет конкурентоспособное качество на стандартных бенчмарках для оценки кода.'}, 'en': {'title': 'Speed Meets Quality: The Future of Code Generation', 'desc': 'Seed Diffusion Preview is a novel discrete-state diffusion language model that enhances inference speed through parallel generation techniques. By utilizing non-sequential decoding, it significantly reduces the latency typically associated with traditional token-by-token generation methods. This model achieves an impressive speed of 2,146 tokens per second on H20 GPUs while still delivering competitive performance on standard code evaluation benchmarks. As a result, Seed Diffusion Preview sets a new standard in the speed-quality trade-off for code generation models, outperforming existing models like Mercury and Gemini Diffusion.'}, 'zh': {'title': '种子扩散预览：速度与质量的新标杆', 'desc': 'Seed Diffusion Preview是一种基于离散状态扩散的语言模型，具有极快的推理速度。通过非顺序的并行生成，离散扩散模型显著提高了推理效率，减少了逐个解码的延迟。该模型在H20 GPU上实现了每秒2,146个token的推理速度，同时在标准代码评估基准上保持了竞争力的性能。与当前的Mercury和Gemini Diffusion相比，Seed Diffusion Preview在速度和质量上都设立了新的标杆。'}}}, {'id': 'https://huggingface.co/papers/2508.03320', 'title': 'Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation', 'url': 'https://huggingface.co/papers/2508.03320', 'abstract': 'Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model that unifies image understanding, text-to-image generation, and image editing within a single architecture-eliminating the need for task-specific adapters or inter-module connectors-and demonstrate that compact multimodal systems can achieve state-of-the-art performance on commodity hardware. Skywork UniPic achieves a GenEval score of 0.86, surpassing most existing unified models; sets a new DPG-Bench complex-generation record of 85.5; attains 5.83 on GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x 1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled encoding strategy that leverages a masked autoregressive encoder for synthesis and a SigLIP2 encoder for understanding, all feeding a shared autoregressive decoder; (2) a progressive, resolution-aware training schedule scaling from 256 x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance capacity and stability; and (3) meticulously curated, 100 million-scale datasets augmented with task-specific reward models to refine generation and editing objectives. By demonstrating that high-fidelity multimodal integration need not incur prohibitive resource demands, Skywork UniPic establishes a practical paradigm for deployable, high-fidelity multimodal AI. Code and weights are publicly available at https://huggingface.co/Skywork/Skywork-UniPic-1.5B.', 'score': 42, 'issue_id': 5199, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '71dc78f7c773cefd', 'authors': ['Peiyu Wang', 'Yi Peng', 'Yimeng Gan', 'Liang Hu', 'Tianyidan Xie', 'Xiaokun Wang', 'Yichen Wei', 'Chuanxin Tang', 'Bo Zhu', 'Changshi Li', 'Hongyang Wei', 'Eric Li', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Multimodality Team, Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.03320.jpg', 'data': {'categories': ['#dataset', '#training', '#multimodal', '#architecture', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'Единая модель для понимания, создания и редактирования изображений', 'desc': 'Skywork UniPic - это авторегрессионная модель с 1,5 миллиардами параметров, объединяющая понимание изображений, генерацию изображений по тексту и редактирование изображений в единой архитектуре. Модель достигает высоких результатов на различных бенчмарках, включая GenEval, DPG-Bench и GEditBench-EN. Skywork UniPic использует раздельную стратегию кодирования и прогрессивное обучение с увеличением разрешения. Модель демонстрирует, что высококачественная мультимодальная интеграция возможна без чрезмерных вычислительных затрат.'}, 'en': {'title': 'Unifying Multimodal AI: Efficiency Meets Performance with Skywork UniPic', 'desc': 'Skywork UniPic is a powerful 1.5 billion-parameter autoregressive model that combines image understanding, text-to-image generation, and image editing into one system. It eliminates the need for separate components for different tasks, allowing for efficient performance on standard hardware. The model achieves impressive scores on various benchmarks, showcasing its capabilities in generating and editing high-quality images. By using innovative training strategies and large datasets, Skywork UniPic demonstrates that advanced multimodal AI can be accessible without requiring excessive computational resources.'}, 'zh': {'title': 'Skywork UniPic：统一多模态AI的高效解决方案', 'desc': 'Skywork UniPic是一个拥有15亿参数的自回归模型，能够统一图像理解、文本到图像生成和图像编辑。该模型通过一个单一架构消除了对特定任务适配器或模块连接器的需求，展示了紧凑的多模态系统在普通硬件上也能达到最先进的性能。Skywork UniPic在多个基准测试中表现优异，尤其是在图像生成和编辑方面，显示出其高效的训练策略和数据集设计。该模型为高保真多模态AI的实际应用提供了新的范式，且代码和权重已公开。'}}}, {'id': 'https://huggingface.co/papers/2508.03694', 'title': 'LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation', 'url': 'https://huggingface.co/papers/2508.03694', 'abstract': 'LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.', 'score': 32, 'issue_id': 5198, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '8c05bd06521b3fb7', 'authors': ['Jianxiong Gao', 'Zhaoxi Chen', 'Xian Liu', 'Jianfeng Feng', 'Chenyang Si', 'Yanwei Fu', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['Fudan University', 'NVIDIA', 'Nanjing University', 'S-Lab, Nanyang Technological University', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.03694.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#multimodal', '#long_context', '#video'], 'emoji': '🎬', 'ru': {'title': 'LongVie: прорыв в генерации сверхдлинных видео с сохранением качества', 'desc': 'LongVie - это новая автореградная модель для генерации сверхдлинных видео. Она решает проблемы временной согласованности и визуальной деградации с помощью унифицированной инициализации шума и глобальной нормализации управляющих сигналов. LongVie использует мультимодальное управление, объединяя плотные и разреженные сигналы. Модель также применяет стратегию обучения с учетом деградации для сохранения визуального качества.'}, 'en': {'title': 'LongVie: Mastering Ultra-Long Video Generation with Consistency and Quality', 'desc': 'LongVie is an innovative autoregressive framework designed for generating ultra-long videos while maintaining visual quality and temporal consistency. It addresses common challenges in video generation, such as noise initialization and control signal normalization, by implementing a unified approach that ensures consistent output across clips. The framework also incorporates multi-modal control, allowing it to utilize various types of guidance signals, and employs a degradation-aware training method to enhance visual fidelity over time. Through extensive testing, LongVie demonstrates superior performance in generating long videos that are both controllable and visually appealing.'}, 'zh': {'title': '超长视频生成的新突破：LongVie', 'desc': 'LongVie是一个端到端的自回归框架，旨在解决超长视频生成中的时间一致性和视觉退化问题。它通过统一的噪声初始化、全局控制信号归一化、多模态控制和退化感知训练来实现这些目标。LongVie的核心设计确保了时间一致性，并通过多模态控制框架来减轻视觉退化。实验结果表明，LongVie在长时间可控性、一致性和质量方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.03686', 'title': 'CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward', 'url': 'https://huggingface.co/papers/2508.03686', 'abstract': 'CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.', 'score': 22, 'issue_id': 5201, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'dddc5da46c921b94', 'authors': ['Shudong Liu', 'Hongwei Liu', 'Junnan Liu', 'Linchen Xiao', 'Songyang Gao', 'Chengqi Lyu', 'Yuzhe Gu', 'Wenwei Zhang', 'Derek F. Wong', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['NLP2CT Lab', 'Shanghai AI Laboratory', 'University of Macau'], 'pdf_title_img': 'assets/pdf/title_img/2508.03686.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#benchmark', '#dataset', '#interpretability', '#optimization'], 'emoji': '🧭', 'ru': {'title': 'CompassVerifier: Надежная проверка ответов LLM во многих областях', 'desc': 'CompassVerifier - это легковесная модель для проверки выходных данных больших языковых моделей (LLM) в различных областях. Она поддерживается VerifierBench - комплексным набором данных для оценки. CompassVerifier демонстрирует компетентность в различных областях, включая математику, знания и разнообразные задачи на рассуждение. Модель способна обрабатывать различные типы ответов, включая многозадачные проблемы, формулы и последовательные ответы, эффективно идентифицируя аномальные и недействительные ответы.'}, 'en': {'title': 'Revolutionizing LLM Output Verification with CompassVerifier', 'desc': 'CompassVerifier is a new model designed to verify the outputs of large language models (LLMs) across different subjects. It addresses the limitations of existing verification methods by providing a robust and lightweight solution that can handle complex answer types and identify invalid responses. The model is supported by VerifierBench, a benchmark dataset that helps evaluate the verification capabilities of various LLMs. This work aims to improve answer verification processes and enhance reinforcement learning research by offering a comprehensive tool for evaluating AI-generated responses.'}, 'zh': {'title': 'CompassVerifier：多领域答案验证的轻量级解决方案', 'desc': 'CompassVerifier 是一种轻量级且稳健的模型，用于验证大型语言模型（LLM）在不同领域的输出。它通过 VerifierBench 这一全面的基准数据集来支持验证过程。该模型能够处理多种类型的答案，包括多子问题、公式和序列答案，并有效识别异常或无效的响应。我们希望 CompassVerifier 和 VerifierBench 能够促进答案验证、评估协议和强化学习研究。'}}}, {'id': 'https://huggingface.co/papers/2508.03012', 'title': 'Tool-integrated Reinforcement Learning for Repo Deep Search', 'url': 'https://huggingface.co/papers/2508.03012', 'abstract': "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Issue localization, the process of identifying code locations that need modification to resolve software issues, is a critical yet challenging task in software development. The semantic gap between natural language issue descriptions and faulty code requires complex multi-hop reasoning through code dependencies. Existing LLM-based agents attempt to address this by integrating repository retrieval tools. However, this transforms issue localization into a demanding task we call Repo Deep Search, which requires the LLM to effectively utilize various repository retrieval tools throughout a multi-step reasoning and navigation process. To tackle this challenge, we present ToolTrain, a two-stage tool-integrated training framework combining rejection-sampled supervised fine-tuning and tool-integrated reinforcement learning to enhance LLMs' ability to use retrieval tools for issue localization. Experimental results show that ToolTrain-trained models achieve state-of-the-art performance, with our 32B model even surpassing Claude-3.7 on function-level localization. The results also show that improved localization performance translates to better end-to-end issue resolution performance. This further demonstrates that training for issue localization is a viable and effective strategy for improving automated software development.", 'score': 9, 'issue_id': 5199, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '4ab74a355fed1d76', 'authors': ['Zexiong Ma', 'Chao Peng', 'Qunhong Zeng', 'Pengfei Gao', 'Yanzhen Zou', 'Bing Xie'], 'affiliations': ['Beijing Institute of Technology', 'ByteDance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03012.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#agents', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'ToolTrain: Эффективная локализация проблем в коде с помощью обученных языковых моделей', 'desc': 'ToolTrain - это двухэтапная система обучения, объединяющая контролируемую тонкую настройку и обучение с подкреплением для улучшения работы больших языковых моделей в задаче локализации проблем в программном коде. Система интегрирует инструменты поиска по репозиторию, что позволяет преодолеть семантический разрыв между описанием проблемы на естественном языке и проблемным кодом. Экспериментальные результаты показывают, что модели, обученные с помощью ToolTrain, достигают наилучших показателей в этой задаче. Улучшенная производительность в локализации проблем также приводит к лучшим результатам в полном цикле разрешения проблем в программном обеспечении.'}, 'en': {'title': 'ToolTrain: Enhancing LLMs for Superior Issue Localization', 'desc': "This paper introduces ToolTrain, a novel two-stage training framework designed to improve large language models (LLMs) for the task of issue localization in software development. It combines supervised fine-tuning with reinforcement learning to enhance the models' ability to utilize repository retrieval tools effectively. The framework addresses the challenges posed by the semantic gap between natural language descriptions of issues and the corresponding faulty code, requiring complex reasoning through code dependencies. Experimental results indicate that models trained with ToolTrain achieve state-of-the-art performance, significantly improving both localization and overall issue resolution in automated software development."}, 'zh': {'title': 'ToolTrain：提升问题定位的智能工具训练框架', 'desc': 'ToolTrain是一种两阶段的训练框架，结合了监督微调和强化学习，旨在提升大型语言模型（LLMs）在问题定位方面的能力。问题定位是识别需要修改的代码位置以解决软件问题的过程，但由于自然语言描述与故障代码之间的语义差距，这一任务非常具有挑战性。ToolTrain通过整合代码库检索工具，帮助LLMs在多步骤推理和导航过程中有效利用这些工具，从而实现了最先进的性能。实验结果表明，ToolTrain训练的模型在功能级定位上超越了Claude-3.7，证明了针对问题定位的训练策略在自动化软件开发中是有效的。'}}}, {'id': 'https://huggingface.co/papers/2508.02091', 'title': 'CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search', 'url': 'https://huggingface.co/papers/2508.02091', 'abstract': "CRINN, a reinforcement learning-based approach, optimizes approximate nearest-neighbor search algorithms for speed while maintaining accuracy, outperforming state-of-the-art methods on several benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN", 'score': 7, 'issue_id': 5201, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '10eb53caada711ad', 'authors': ['Xiaoya Li', 'Xiaofei Sun', 'Albert Wang', 'Chris Shum', 'Jiwei Li'], 'affiliations': ['DeepReinforce Team', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.02091.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#rag', '#rl', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'CRINN: Революция в поиске ближайших соседей с помощью ИИ', 'desc': 'CRINN - это новый подход к оптимизации алгоритмов поиска приближенных ближайших соседей (ANNS), использующий обучение с подкреплением. Он автоматически генерирует все более быстрые реализации ANNS, сохраняя при этом точность. CRINN превзошел современные методы на нескольких эталонных тестах, включая GIST-960-Euclidean и MNIST-784-Euclidean. Успех CRINN показывает, что языковые модели, дополненные обучением с подкреплением, могут эффективно автоматизировать сложные алгоритмические оптимизации.'}, 'en': {'title': 'CRINN: Speeding Up Nearest-Neighbor Search with Reinforcement Learning', 'desc': 'CRINN is a novel approach that uses reinforcement learning to enhance approximate nearest-neighbor search (ANNS) algorithms, focusing on improving their speed while ensuring accuracy. By framing the optimization of ANNS as a reinforcement learning problem, CRINN uses execution speed as a reward signal to automatically generate faster implementations. The results show that CRINN outperforms existing state-of-the-art ANNS methods on multiple benchmark datasets, achieving top performance in several cases. This work highlights the potential of combining reinforcement learning with large language models (LLMs) for automating complex algorithmic optimizations.'}, 'zh': {'title': 'CRINN：用强化学习加速近似最近邻搜索', 'desc': 'CRINN是一种基于强化学习的方法，旨在优化近似最近邻搜索算法的速度，同时保持准确性。该方法将近似最近邻搜索的优化视为一个强化学习问题，以执行速度作为奖励信号。通过这种方式，CRINN能够自动生成逐渐更快的近似最近邻搜索实现，并满足准确性约束。实验结果表明，CRINN在多个基准数据集上表现优异，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2508.00367', 'title': 'Representation Shift: Unifying Token Compression with FlashAttention', 'url': 'https://huggingface.co/papers/2508.00367', 'abstract': "Representation Shift is a training-free, model-agnostic metric that integrates token compression with FlashAttention, enabling significant speedups in video-text retrieval and video QA.  \t\t\t\t\tAI-generated summary \t\t\t\t Transformers have demonstrated remarkable success across vision, language, and video. Yet, increasing task complexity has led to larger models and more tokens, raising the quadratic cost of self-attention and the overhead of GPU memory access. To reduce the computation cost of self-attention, prior work has proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. This, however, makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance. Here, we propose Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation. This seamlessly integrates token compression with FlashAttention, without attention maps or retraining. Our method further generalizes beyond Transformers to CNNs and state space models. Extensive experiments show that Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA, respectively. Code is available at https://github.com/mlvlab/Representation-Shift.", 'score': 7, 'issue_id': 5209, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '5a4ad3025ab24bd1', 'authors': ['Joonmyung Choi', 'Sanghyeok Lee', 'Byungoh Ko', 'Eunseo Kim', 'Jihyung Kil', 'Hyunwoo J. Kim'], 'affiliations': ['Adobe Research', 'KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00367.jpg', 'data': {'categories': ['#training', '#architecture', '#data', '#optimization', '#video'], 'emoji': '🚀', 'ru': {'title': 'Ускорение обработки видео и текста без потери качества', 'desc': 'Статья представляет метрику Representation Shift, которая позволяет эффективно сжимать токены в трансформерах без переобучения модели. Эта метрика совместима с FlashAttention, что значительно ускоряет обработку видео и текста. Representation Shift измеряет степень изменения представления каждого токена, что позволяет определить наиболее важные из них. Метод применим не только к трансформерам, но и к CNN и моделям пространства состояний.'}, 'en': {'title': 'Boosting Video Retrieval Efficiency with Representation Shift', 'desc': "This paper introduces Representation Shift, a new metric that helps improve the efficiency of video-text retrieval and video question answering without needing to retrain models. It combines token compression techniques with FlashAttention, which reduces memory usage and speeds up processing. By measuring how much each token's representation changes, it allows for effective token selection without relying on attention maps. The method works not only with Transformers but also with other model types like CNNs, achieving notable performance improvements."}, 'zh': {'title': 'Representation Shift：加速视频检索与问答的创新方法', 'desc': 'Representation Shift是一种无训练、模型无关的度量方法，它将令牌压缩与FlashAttention结合，显著加快视频-文本检索和视频问答的速度。随着任务复杂性的增加，模型和令牌的规模也在扩大，导致自注意力的计算成本呈平方增长。我们的方法通过测量每个令牌表示的变化程度，来实现有效的令牌压缩，而无需构建注意力图或重新训练。实验结果表明，Representation Shift在视频-文本检索和视频问答中分别实现了高达5.5%和4.4%的速度提升。'}}}, {'id': 'https://huggingface.co/papers/2508.03050', 'title': 'Multi-human Interactive Talking Dataset', 'url': 'https://huggingface.co/papers/2508.03050', 'abstract': 'MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing studies on talking video generation have predominantly focused on single-person monologues or isolated facial animations, limiting their applicability to realistic multi-human interactions. To bridge this gap, we introduce MIT, a large-scale dataset specifically designed for multi-human talking video generation. To this end, we develop an automatic pipeline that collects and annotates multi-person conversational videos. The resulting dataset comprises 12 hours of high-resolution footage, each featuring two to four speakers, with fine-grained annotations of body poses and speech interactions. It captures natural conversational dynamics in multi-speaker scenario, offering a rich resource for studying interactive visual behaviors. To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle varying numbers of speakers by aggregating individual pose embeddings, and an Interactive Audio Driver (IAD) to modulate head dynamics based on speaker-specific audio features. Together, these components showcase the feasibility and challenges of generating realistic multi-human talking videos, establishing MIT as a valuable benchmark for future research. The code is avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.', 'score': 6, 'issue_id': 5200, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '01ba126a166568d6', 'authors': ['Zeyu Zhu', 'Weijia Wu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2508.03050.jpg', 'data': {'categories': ['#cv', '#benchmark', '#dataset'], 'emoji': '🗣️', 'ru': {'title': 'Новый датасет и модель для генерации видео с разговорами нескольких людей', 'desc': 'Исследователи представили MIT - крупномасштабный набор данных для генерации видео с разговорами нескольких людей. Этот датасет включает детальные аннотации и используется для демонстрации CovOG - базовой модели, объединяющей кодировщик поз нескольких людей и интерактивный аудиодрайвер. MIT содержит 12 часов видео высокого разрешения с 2-4 говорящими и детальными аннотациями поз тела и речевых взаимодействий. CovOG использует агрегацию индивидуальных эмбеддингов поз и модуляцию динамики головы на основе аудиопризнаков каждого говорящего.'}, 'en': {'title': 'MIT: Pioneering Multi-Human Talking Video Generation', 'desc': 'The paper introduces MIT, a large-scale dataset aimed at generating multi-human talking videos, which includes detailed annotations for body poses and speech interactions. This dataset addresses the limitations of previous studies that focused mainly on single-person videos, making it more applicable to real-life conversations. To utilize this dataset, the authors propose CovOG, a baseline model that combines a Multi-Human Pose Encoder to manage multiple speakers and an Interactive Audio Driver to synchronize head movements with audio cues. This work not only demonstrates the potential of generating realistic multi-human interactions but also sets a new benchmark for future research in this area.'}, 'zh': {'title': 'MIT：多人人对话视频生成的新基准', 'desc': 'MIT是一个大规模的数据集，专门用于多人的对话视频生成，包含细致的注释信息。现有的研究主要集中在单人独白或孤立的面部动画上，限制了其在真实多人的互动中的应用。我们开发了一个自动化流程，收集和注释多人的对话视频，数据集包含12小时的高分辨率视频，展示了自然的对话动态。为了展示MIT的潜力，我们提出了CovOG模型，结合了多人的姿态编码器和互动音频驱动器，展示了生成真实多人的对话视频的可行性和挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.01119', 'title': 'The Promise of RL for Autoregressive Image Editing', 'url': 'https://huggingface.co/papers/2508.01119', 'abstract': 'Reinforcement learning combined with a large multimodal language model verifier enhances image editing performance in an autoregressive multimodal framework.  \t\t\t\t\tAI-generated summary \t\t\t\t We explore three strategies to enhance performance on a wide range of image editing tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning. In order to study all these components in one consistent framework, we adopt an autoregressive multimodal model that processes textual and visual tokens in a unified manner. We find RL combined with a large multi-modal LLM verifier to be the most effective of these strategies. As a result, we release EARL: Editing with Autoregression and RL, a strong RL-based image editing model that performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data. Thus, EARL pushes the frontier of autoregressive multimodal models on image editing. We release our code, training data, and trained models at https://github.com/mair-lab/EARL.', 'score': 6, 'issue_id': 5215, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': 'b7e0974935b60296', 'authors': ['Saba Ahmadi', 'Rabiul Awal', 'Ankur Sikarwar', 'Amirhossein Kazemnejad', 'Ge Ya Luo', 'Juan A. Rodriguez', 'Sai Rajeswar', 'Siva Reddy', 'Christopher Pal', 'Benno Krojer', 'Aishwarya Agrawal'], 'affiliations': ['Canada CIFAR AI Chair', 'McGill University', 'Mila Quebec AI Institute', 'Polytechnique Montréal', 'ServiceNow', 'Université de Montréal', 'École de Technologie Supérieure (ETS)'], 'pdf_title_img': 'assets/pdf/title_img/2508.01119.jpg', 'data': {'categories': ['#rl', '#optimization', '#multimodal', '#open_source', '#training', '#games'], 'emoji': '🖼️', 'ru': {'title': 'Обучение с подкреплением повышает качество редактирования изображений', 'desc': 'В статье исследуются стратегии улучшения редактирования изображений с использованием авторегрессионной мультимодальной модели. Авторы сравнивают три подхода: обучение с учителем, обучение с подкреплением и рассуждения по цепочке мыслей. Наиболее эффективным оказалось обучение с подкреплением в сочетании с большой мультимодальной языковой моделью-верификатором. В результате была разработана модель EARL, показывающая конкурентоспособные результаты на различных задачах редактирования изображений.'}, 'en': {'title': 'Reinforcement Learning Meets Multimodal Image Editing', 'desc': 'This paper presents a novel approach to image editing by integrating reinforcement learning (RL) with a large multimodal language model (LLM) verifier within an autoregressive framework. The authors investigate three strategies: supervised fine-tuning, RL, and Chain-of-Thought reasoning, to improve image editing tasks. They demonstrate that the combination of RL and a multimodal LLM verifier significantly enhances performance, leading to the development of EARL, a robust RL-based image editing model. EARL achieves competitive results on various editing tasks while requiring less training data compared to existing methods, marking a significant advancement in autoregressive multimodal models for image editing.'}, 'zh': {'title': '强化学习与多模态模型结合，提升图像编辑性能', 'desc': '本论文探讨了如何通过结合强化学习和大型多模态语言模型验证器来提升图像编辑性能。我们提出了三种策略：监督微调、强化学习和链式思维推理，并在一个自回归多模态框架中进行研究。实验结果表明，强化学习与大型多模态语言模型验证器的结合是最有效的策略。最终，我们发布了EARL模型，它在多种图像编辑任务中表现出色，且训练数据需求较少。'}}}, {'id': 'https://huggingface.co/papers/2508.03613', 'title': 'Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction', 'url': 'https://huggingface.co/papers/2508.03613', 'abstract': "Goedel-Prover-V2, a series of open-source language models, achieves state-of-the-art performance in automated theorem proving through scaffolded data synthesis, verifier-guided self-correction, and model averaging.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.", 'score': 5, 'issue_id': 5204, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '50044cd9b7eb1802', 'authors': ['Yong Lin', 'Shange Tang', 'Bohan Lyu', 'Ziran Yang', 'Jui-Hui Chung', 'Haoyu Zhao', 'Lai Jiang', 'Yihan Geng', 'Jiawei Ge', 'Jingruo Sun', 'Jiayun Wu', 'Jiri Gesi', 'Ximing Lu', 'David Acuna', 'Kaiyu Yang', 'Hongzhou Lin', 'Yejin Choi', 'Danqi Chen', 'Sanjeev Arora', 'Chi Jin'], 'affiliations': ['Amazon', 'Meta FAIR', 'NVIDIA', 'Peking University', 'Princeton Language and Intelligence, Princeton University', 'Shanghai Jiao Tong University', 'Stanford University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03613.jpg', 'data': {'categories': ['#rl', '#dataset', '#synthetic', '#reasoning', '#small_models', '#training', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Маленькая модель - большие доказательства: Goedel-Prover-V2 переворачивает мир автоматического доказательства теорем', 'desc': 'Goedel-Prover-V2 - это серия открытых языковых моделей для автоматического доказательства теорем. Модель использует синтез структурированных данных, самокоррекцию на основе верификатора и усреднение моделей для достижения наилучших результатов. Несмотря на небольшой размер, Goedel-Prover-V2-8B превосходит гораздо более крупные модели на бенчмарках MiniF2F и PutnamBench. На момент выпуска Goedel-Prover-V2 демонстрирует лучшую производительность среди открытых систем автоматического доказательства теорем.'}, 'en': {'title': 'Revolutionizing Theorem Proving with Goedel-Prover-V2', 'desc': 'Goedel-Prover-V2 is a series of open-source language models that excel in automated theorem proving. It introduces innovative techniques such as scaffolded data synthesis for training on progressively complex tasks, verifier-guided self-correction for iterative proof refinement, and model averaging to enhance output diversity. The models achieve impressive performance metrics, with the smaller Goedel-Prover-V2-8B outperforming larger models like DeepSeek-Prover-V2-671B. Overall, Goedel-Prover-V2 sets a new benchmark in the field, demonstrating superior capabilities in solving mathematical problems with a reduced computational footprint.'}, 'zh': {'title': 'Goedel-Prover-V2：自动定理证明的新标杆', 'desc': 'Goedel-Prover-V2是一系列开源语言模型，在自动定理证明领域达到了最先进的性能。该模型通过三项创新技术实现了这一目标：首先，使用支架数据合成生成逐渐增加难度的合成任务，以帮助模型掌握复杂的定理；其次，采用验证器引导的自我修正，使模型能够根据Lean编译器的反馈迭代修正其证明；最后，通过模型平均技术合并模型检查点，以减少训练后期模型输出多样性的下降。Goedel-Prover-V2-32B模型在MiniF2F上达到了88.1%的通过率，显著超越了之前的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2508.01780', 'title': 'LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?', 'url': 'https://huggingface.co/papers/2508.01780', 'abstract': "LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.", 'score': 5, 'issue_id': 5199, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'b9c09b0ce4e2dad3', 'authors': ['Guozhao Mo', 'Wenliang Zhong', 'Jiawei Chen', 'Xuanang Chen', 'Yaojie Lu', 'Hongyu Lin', 'Ben He', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2508.01780.jpg', 'data': {'categories': ['#open_source', '#survey', '#agents', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'LiveMCPBench: Новый стандарт оценки LLM-агентов в реальных MCP-средах', 'desc': 'LiveMCPBench представляет собой комплексный бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в реальных задачах экосистемы MCP. Он включает 95 задач и использует масштабируемый конвейер оценки с адаптивной системой судейства. Бенчмарк содержит LiveMCPTool - набор из 70 MCP-серверов и 527 инструментов, а также LiveMCPEval - фреймворк для автоматизированной оценки с использованием LLM в качестве судьи. Результаты тестирования 10 ведущих моделей показали, что лучшая модель (Claude-Sonnet-4) достигла 78.95% успешности выполнения задач.'}, 'en': {'title': 'Revolutionizing LLM Evaluation in Dynamic MCP Environments', 'desc': 'LiveMCPBench is a new benchmark designed to evaluate large language model (LLM) agents in real-world tasks within the Model Context Protocol (MCP) ecosystem. It addresses the limitations of existing benchmarks that only test single-server settings by providing a scalable evaluation framework with 95 diverse tasks and a collection of 70 MCP servers and 527 tools. The benchmark includes an innovative LLM-as-a-Judge system for automated evaluation, achieving high agreement with human reviewers. Results show significant performance differences among models, highlighting the challenges LLMs face in complex, tool-rich environments.'}, 'zh': {'title': '全面评估LLM代理的基准测试平台', 'desc': 'LiveMCPBench是一个全面的基准测试平台，旨在评估大型语言模型（LLM）代理在多样化的真实世界任务中的表现。它解决了现有基准测试仅限于单一服务器设置的问题，提供了95个基于模型上下文协议（MCP）生态系统的真实任务。通过LiveMCPTool，研究人员可以使用70个MCP服务器和527个工具，支持可扩展和可重复的评估流程。此外，LiveMCPEval框架实现了自动化和自适应评估，确保在动态任务环境中与人类评审者的高一致性。'}}}, {'id': 'https://huggingface.co/papers/2508.00477', 'title': 'LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer', 'url': 'https://huggingface.co/papers/2508.00477', 'abstract': "LAMIC, a Layout-Aware Multi-Image Composition framework, extends single-reference diffusion models to multi-reference scenarios using attention mechanisms, achieving state-of-the-art performance in controllable image synthesis without training.  \t\t\t\t\tAI-generated summary \t\t\t\t In controllable image synthesis, generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework that, for the first time, extends single-reference diffusion models to multi-reference scenarios in a training-free manner. Built upon the MMDiT model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group Isolation Attention (GIA) to enhance entity disentanglement; and 2) Region-Modulated Attention (RMA) to enable layout-aware generation. To comprehensively evaluate model capabilities, we further introduce three metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout control; and 2) Background Similarity (BG-S) for measuring background consistency. Extensive experiments show that LAMIC achieves state-of-the-art performance across most major metrics: it consistently outperforms existing multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all settings, and achieves the best DPG in complex composition tasks. These results demonstrate LAMIC's superior abilities in identity keeping, background preservation, layout control, and prompt-following, all achieved without any training or fine-tuning, showcasing strong zero-shot generalization ability. By inheriting the strengths of advanced single-reference models and enabling seamless extension to multi-image scenarios, LAMIC establishes a new training-free paradigm for controllable multi-image composition. As foundation models continue to evolve, LAMIC's performance is expected to scale accordingly. Our implementation is available at: https://github.com/Suchenl/LAMIC.", 'score': 4, 'issue_id': 5202, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '94d96ba2b9f92b31', 'authors': ['Yuzhuo Chen', 'Zehua Ma', 'Jianhua Wang', 'Kai Kang', 'Shunyu Yao', 'Weiming Zhang'], 'affiliations': ['East China Normal University', 'Onestory Team', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.00477.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#optimization', '#cv', '#training'], 'emoji': '🖼️', 'ru': {'title': 'LAMIC: Революция в синтезе изображений с несколькими референсами', 'desc': 'LAMIC - это фреймворк для композиции нескольких изображений с учетом макета, который расширяет возможности диффузионных моделей с одним референсом на сценарии с несколькими референсами. Он использует два механизма внимания: Group Isolation Attention для улучшения разделения сущностей и Region-Modulated Attention для генерации с учетом макета. LAMIC достигает наилучших результатов по большинству метрик без дополнительного обучения, демонстрируя превосходные способности в сохранении идентичности, фона и контроле макета. Этот подход устанавливает новую парадигму для контролируемой композиции нескольких изображений без обучения.'}, 'en': {'title': 'LAMIC: Revolutionizing Multi-Image Synthesis Without Training', 'desc': 'LAMIC is a new framework designed for creating images from multiple references while considering their layout. It builds on existing single-reference diffusion models and introduces attention mechanisms to improve how different elements in the images are handled. This framework allows for high-quality image synthesis without the need for additional training, demonstrating strong performance in maintaining identity, background consistency, and layout control. LAMIC sets a new standard in controllable image composition by effectively managing multiple images in a training-free manner.'}, 'zh': {'title': 'LAMIC：无训练的多图像合成新范式', 'desc': 'LAMIC是一个布局感知的多图像合成框架，它将单参考扩散模型扩展到多参考场景，且无需训练。该框架引入了两种注意力机制：群体隔离注意力（GIA）和区域调制注意力（RMA），以增强实体分离和布局感知生成。通过引入三种评估指标，LAMIC在身份保持、背景一致性和布局控制等方面表现出色，超越了现有的多参考基线。LAMIC展示了强大的零样本泛化能力，为可控的多图像合成建立了新的无训练范式。'}}}, {'id': 'https://huggingface.co/papers/2508.03164', 'title': 'ChartCap: Mitigating Hallucination of Dense Chart Captioning', 'url': 'https://huggingface.co/papers/2508.03164', 'abstract': 'ChartCap, a large-scale dataset with dense, type-specific captions for real-world charts, improves caption accuracy and reduces hallucinations in vision language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.', 'score': 3, 'issue_id': 5204, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'aec1f860dbfa8231', 'authors': ['Junyoung Lim', 'Jaewoo Ahn', 'Gunhee Kim'], 'affiliations': ['Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03164.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#dataset', '#data', '#open_source'], 'emoji': '📊', 'ru': {'title': 'ChartCap: Точные подписи к графикам без галлюцинаций', 'desc': 'ChartCap - это масштабный набор данных, содержащий 565 тысяч реальных графиков с детальными подписями. Датасет разработан для улучшения точности генерации подписей и уменьшения галлюцинаций в мультимодальных языковых моделях. ChartCap использует четырехэтапный конвейер для создания подписей, основанных только на видимых данных графика, и применяет верификацию на основе циклической согласованности. Авторы также предлагают новую метрику - Visual Consistency Score, для оценки качества подписей без опоры на эталонные подписи.'}, 'en': {'title': 'ChartCap: Elevating Chart Captioning Accuracy and Reducing Hallucinations', 'desc': 'ChartCap is a new dataset designed to enhance the performance of vision language models in generating captions for charts. It contains 565,000 real-world chart images with detailed, type-specific captions that focus on essential structural elements and insights, avoiding irrelevant information. The dataset is created through a four-stage process that ensures high-quality captions, verified by a cycle consistency method for efficient quality control. Experiments show that models trained on ChartCap produce more accurate and informative captions, with fewer hallucinations compared to existing models and even human-generated captions.'}, 'zh': {'title': 'ChartCap：提升图表说明准确性的关键数据集', 'desc': 'ChartCap是一个大规模的数据集，包含565K个真实世界图表图像及其特定类型的详细说明。该数据集旨在提高视觉语言模型的说明准确性，并减少虚假信息的生成。通过设计四阶段的生成管道，ChartCap确保说明仅基于图表中可辨别的数据，并通过循环一致性的人类验证加速质量控制。实验结果表明，基于ChartCap微调的模型在生成准确和信息丰富的说明方面表现优于其他开源和专有模型，甚至超过人类标注的说明。'}}}, {'id': 'https://huggingface.co/papers/2508.02629', 'title': 'HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents', 'url': 'https://huggingface.co/papers/2508.02629', 'abstract': 'HyCodePolicy integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair to enhance the robustness and efficiency of embodied agent policies.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.', 'score': 3, 'issue_id': 5212, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '5b4c4a212eae58d6', 'authors': ['Yibin Liu', 'Zhixuan Liang', 'Zanxin Chen', 'Tianxing Chen', 'Mengkang Hu', 'Wanxi Dong', 'Congsheng Xu', 'Zhaoming Han', 'Yusen Qin', 'Yao Mu'], 'affiliations': ['D-Robotics', 'HKU MMLab', 'NEU', 'SJTU ScaleLab', 'SUSTech', 'SZU', 'Shanghai AI Lab', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2508.02629.jpg', 'data': {'categories': ['#robotics', '#optimization', '#agents', '#reasoning', '#games', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Самокорректирующиеся программы для роботов на основе мультимодального ИИ', 'desc': 'HyCodePolicy - это гибридная система управления для воплощенных агентов, использующая мультимодальные языковые модели. Она объединяет синтез кода, геометрическое обоснование, перцептивный мониторинг и итеративное исправление в замкнутый цикл программирования. Система генерирует исполняемую программу на основе естественно-языковых инструкций, затем выполняет ее в симуляции, отслеживая ошибки с помощью визуально-языковой модели. HyCodePolicy способна автономно исправлять программы, значительно повышая надежность и эффективность политик для манипуляций роботов.'}, 'en': {'title': 'Empowering Robots with Self-Correcting Code Synthesis', 'desc': 'HyCodePolicy is a novel framework that enhances the performance of embodied agents by integrating several advanced techniques. It combines code synthesis, geometric grounding, perceptual monitoring, and iterative repair to create a robust programming cycle. The system translates natural language instructions into executable programs, monitors their execution, and identifies failures using a vision-language model. This approach allows for self-correcting capabilities in program synthesis, improving both the efficiency and reliability of robotic manipulation tasks.'}, 'zh': {'title': '自我修正的智能体编程策略', 'desc': 'HyCodePolicy 是一种混合语言控制框架，旨在增强具身智能体策略的鲁棒性和效率。该系统通过将代码合成、几何基础、感知监控和迭代修复整合到一个闭环编程周期中，来实现自我修正的程序合成。它首先将自然语言指令分解为子目标，并生成基于对象中心几何原语的可执行程序。通过视觉语言模型监控执行过程，HyCodePolicy 能够检测执行失败并进行修复，从而提高机器人操作策略的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.02079', 'title': 'AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization', 'url': 'https://huggingface.co/papers/2508.02079', 'abstract': 'AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.  \t\t\t\t\tAI-generated summary \t\t\t\t Low-rank adaptation (LoRA) has become a standard tool for efficiently fine-tuning large language models (LLMs). Yet, even minor LoRA updates can induce alignment drift, weakening safety and behavioral constraints through entangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL), a principled framework for preserving alignment during finetuning. AGL introduces several key components: a primary task loss for supervision, Fisher Information Matrix-based regularization to restrict updates in alignment-sensitive subspaces, and task-specific regularization to stabilize the integration of new knowledge. We further introduce collision-aware regularization, blending Riemannian overlap -- which penalizes coordinate-wise interference -- and geodesic separation -- which encourages disjoint update geometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and unsafe prompts designed to quantify alignment drift and safety degradation. Empirical evaluations show that AGL mitigates alignment drift by up to 50% on safety-critical benchmarks without degrading downstream task performance. Comprehensive ablation confirms that each component contributes distinctly to preserving latent safety behaviors. Finally, we derive and validate a scaling law for catastrophic forgetting, revealing that AGL flattens post-finetuning loss escalation while preserving adaptation dynamics. AGL is a structurally grounded refinement of LoRA, ensuring alignment preservation with minimal trade-offs. To encourage further exploration and development, we open-source our implementation.', 'score': 2, 'issue_id': 5198, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '25a555fc0f91f562', 'authors': ['Amitava Das', 'Abhilekh Borah', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon AI, USA', 'BITS Goa, India', 'Manipal University, India', 'Meta AI, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.02079.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#alignment', '#open_source', '#training'], 'emoji': '🛡️', 'ru': {'title': 'Сохранение безопасности при дообучении языковых моделей', 'desc': 'AlignGuard-LoRA (AGL) - это фреймворк для сохранения выравнивания при дообучении больших языковых моделей. Он вводит методы регуляризации и диагностический бенчмарк для снижения дрейфа выравнивания. AGL включает несколько ключевых компонентов, таких как регуляризация на основе матрицы Фишера и регуляризация с учетом коллизий. Эмпирические оценки показывают, что AGL снижает дрейф выравнивания до 50% на критически важных для безопасности бенчмарках без ухудшения производительности на целевых задачах.'}, 'en': {'title': 'Preserving Alignment in Fine-Tuning with AGL', 'desc': 'AlignGuard-LoRA (AGL) is a new framework designed to maintain the alignment of large language models during the fine-tuning process. It introduces regularization techniques that help prevent alignment drift, which can occur even with small updates in low-rank adaptation (LoRA). AGL employs a primary task loss for supervision and uses the Fisher Information Matrix to limit updates in sensitive areas, ensuring that safety and behavioral constraints remain intact. Additionally, it features a diagnostic benchmark called DriftCaps to measure alignment drift and safety, demonstrating significant improvements in maintaining alignment without sacrificing performance on other tasks.'}, 'zh': {'title': 'AlignGuard-LoRA：保持对齐，确保安全', 'desc': 'AlignGuard-LoRA (AGL) 是一个框架，旨在通过引入正则化技术和诊断基准来保持大型语言模型在微调过程中的对齐性。该框架解决了低秩适应（LoRA）在更新过程中可能导致的对齐漂移问题，从而增强安全性和行为约束。AGL 采用了多种关键组件，包括基于费舍尔信息矩阵的正则化和任务特定的正则化，以稳定新知识的整合。实验证明，AGL 能够在不降低下游任务性能的情况下，将对齐漂移减少多达 50%。'}}}, {'id': 'https://huggingface.co/papers/2508.02630', 'title': 'What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce', 'url': 'https://huggingface.co/papers/2508.02630', 'abstract': 'ACES, a sandbox environment, studies AI agents\' shopping behavior in a mock marketplace, revealing position effects, sensitivity to sponsored tags, endorsements, prices, ratings, and reviews, and highlighting implications for seller strategies and platform design.  \t\t\t\t\tAI-generated summary \t\t\t\t Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, vision-language-model (VLM) agents can parse webpages, evaluate products, and transact. This raises a fundamental question: what do AI agents buy, and why? We develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent with a fully programmable mock marketplace to study this question. We first conduct basic rationality checks in the context of simple tasks, and then, by randomizing product positions, prices, ratings, reviews, sponsored tags, and platform endorsements, we obtain causal estimates of how frontier VLMs actually shop. Models show strong but heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal "top" rank. They penalize sponsored tags and reward endorsements. Sensitivities to price, ratings, and reviews are directionally human-like but vary sharply in magnitude across models. Motivated by scenarios where sellers use AI agents to optimize product listings, we show that a seller-side agent that makes minor tweaks to product descriptions, targeting AI buyer preferences, can deliver substantial market-share gains if AI-mediated shopping dominates. We also find that modal product choices can differ across models and, in some cases, demand may concentrate on a few select products, raising competition questions. Together, our results illuminate how AI agents may behave in e-commerce settings and surface concrete seller strategy, platform design, and regulatory questions in an AI-mediated ecosystem.', 'score': 1, 'issue_id': 5215, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '2860dd8bc6a41f92', 'authors': ['Amine Allouah', 'Omar Besbes', 'Josué D Figueroa', 'Yash Kanoria', 'Akshit Kumar'], 'affiliations': ['Columbia University, Graduate School of Business', 'MyCustomAI', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02630.jpg', 'data': {'categories': ['#agents', '#multimodal', '#ethics', '#games', '#alignment'], 'emoji': '🛒', 'ru': {'title': 'ИИ идет за покупками: новая эра электронной коммерции', 'desc': 'Исследование ACES изучает поведение ИИ-агентов при совершении покупок в виртуальной торговой среде. Эксперименты выявили влияние позиции товаров, спонсорских тегов, рекомендаций, цен, рейтингов и отзывов на выбор ИИ-покупателей. Результаты показывают, что разные модели машинного обучения демонстрируют различную чувствительность к этим факторам. Исследование поднимает вопросы о стратегиях продавцов, дизайне платформ и регулировании в экосистеме, где покупки осуществляются с помощью ИИ.'}, 'en': {'title': 'Understanding AI Agents in E-Commerce: Insights from ACES', 'desc': "The paper introduces ACES, a sandbox environment designed to analyze the shopping behavior of AI agents in a simulated marketplace. It investigates how these agents respond to various factors such as product position, pricing, ratings, and endorsements, revealing that their preferences can differ significantly from human shoppers. The study finds that while AI agents generally prefer higher-ranked products, their specific choices can vary based on the model used, challenging the idea of a single 'top' product. The findings suggest that sellers can optimize their listings to cater to AI preferences, potentially reshaping strategies in e-commerce as AI-driven shopping becomes more prevalent."}, 'zh': {'title': '探索AI代理在电商中的购物行为', 'desc': '本论文研究了人工智能代理在模拟市场中的购物行为，提出了一个名为ACES的沙盒环境。通过随机化产品位置、价格、评分、评论和赞助标签，研究了不同模型在购物时的因果关系和偏好。结果显示，AI代理对产品位置有明显的偏好，但不同模型的选择差异很大，且对价格和评分的敏感度与人类相似但幅度不同。研究还表明，卖方可以通过优化产品描述来吸引AI买家的偏好，从而在市场中获得显著的份额提升。'}}}, {'id': 'https://huggingface.co/papers/2508.02455', 'title': 'TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs', 'url': 'https://huggingface.co/papers/2508.02455', 'abstract': 'A new scoring approach using language models ranks static code completions in IDEs by organizing them into a prefix tree and performing a single greedy decoding pass.  \t\t\t\t\tAI-generated summary \t\t\t\t Token-level code completion is one of the most critical features in modern Integrated Development Environments (IDEs). It assists developers by suggesting relevant identifiers and APIs during coding. While completions are typically derived from static analysis, their usefulness depends heavily on how they are ranked, as correct predictions buried deep in the list are rarely seen by users. Most current systems rely on hand-crafted heuristics or lightweight machine learning models trained on user logs, which can be further improved to capture context information and generalize across projects and coding styles. In this work, we propose a new scoring approach to ranking static completions using language models in a lightweight and model-agnostic way. Our method organizes all valid completions into a prefix tree and performs a single greedy decoding pass to collect token-level scores across the tree. This enables a precise token-aware ranking without needing beam search, prompt engineering, or model adaptations. The approach is fast, architecture-agnostic, and compatible with already deployed models for code completion. These findings highlight a practical and effective pathway for integrating language models into already existing tools within IDEs, and ultimately providing smarter and more responsive developer assistance.', 'score': 1, 'issue_id': 5215, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '70799804b6937703', 'authors': ['Daniele Cipollone', 'Egor Bogomolov', 'Arie van Deursen', 'Maliheh Izadi'], 'affiliations': ['Delft University of Technology, Delft, Netherlands', 'JetBrains, Amsterdam, Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2508.02455.jpg', 'data': {'categories': ['#plp', '#training', '#optimization'], 'emoji': '🌳', 'ru': {'title': 'Умное ранжирование автодополнений кода с помощью языковых моделей', 'desc': 'Статья предлагает новый подход к ранжированию статических автодополнений кода в IDE с использованием языковых моделей. Метод организует варианты дополнений в префиксное дерево и выполняет однократный жадный проход декодирования для сбора оценок на уровне токенов. Это позволяет точно ранжировать варианты с учетом контекста без необходимости в лучевом поиске или адаптации модели. Подход быстрый, не зависит от архитектуры и совместим с уже развернутыми моделями автодополнения кода.'}, 'en': {'title': 'Smart Code Completion with Language Models', 'desc': 'This paper introduces a novel method for ranking code completions in Integrated Development Environments (IDEs) using language models. The approach organizes potential code completions into a prefix tree, allowing for efficient scoring of completions through a single greedy decoding pass. This method enhances the ranking of static code completions by providing a token-aware scoring system without the need for complex techniques like beam search or prompt engineering. The proposed solution is lightweight, model-agnostic, and can be easily integrated into existing IDEs, improving developer assistance significantly.'}, 'zh': {'title': '智能代码补全的新方法', 'desc': '本文提出了一种新的评分方法，利用语言模型对静态代码补全进行排名。该方法将所有有效的补全组织成前缀树，并通过单次贪婪解码来收集每个标记的分数。与传统的基于手工启发式或轻量级机器学习模型的方法相比，这种方法能够更好地捕捉上下文信息，并在不同项目和编码风格中进行泛化。最终，这种快速且与模型无关的方法为集成语言模型到现有IDE工具中提供了有效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2508.02063', 'title': 'TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs', 'url': 'https://huggingface.co/papers/2508.02063', 'abstract': "TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) fine-tuned to align with human values often exhibit alignment drift, producing unsafe or policy-violating completions when exposed to adversarial prompts, decoding perturbations, or paraphrased jailbreaks. While prior work has behaviorally characterized alignment failure, little is known about the training-time belief sources underlying these failures. We introduce TraceAlign, a unified framework for tracing unsafe completions back to their root causes in the model's training corpus. Central to our approach is the Belief Conflict Index (BCI), which quantifies semantic inconsistency between generated spans and aligned policies, based on retrieved training documents using suffix-array matching. We propose three complementary interventions: (i) TraceShield, an inference-time safety filter that refuses completions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a contrastive fine-tuning objective penalizing high-BCI continuations during DPO, and (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam expansions predicted to yield high-BCI spans. Together, these defenses reduce alignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB) while preserving utility on standard tasks, with delta less than 0.2 and improved refusal quality. We further derive a theoretical upper bound on drift likelihood via suffix-array span statistics, linking memorization frequency and length to adversarial reactivation risk. TraceAlign thus provides the first scalable, traceable, and grounded toolkit for understanding and mitigating alignment failures at source. To encourage further exploration and development, we open-source our implementation at: https://anonymous.4open.science/r/tracealign-2DA7", 'score': 1, 'issue_id': 5198, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '0b136776fbb19b26', 'authors': ['Amitava Das', 'Vinija Jain', 'Aman Chadha'], 'affiliations': ['Amazon GenAI', 'BITS Pilani Goa', 'Meta AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.02063.jpg', 'data': {'categories': ['#security', '#benchmark', '#rlhf', '#alignment', '#open_source', '#training'], 'emoji': '🛡️', 'ru': {'title': 'TraceAlign: отслеживание и устранение дрейфа выравнивания в больших языковых моделях', 'desc': 'TraceAlign - это фреймворк для выявления и снижения дрейфа выравнивания в больших языковых моделях. Он отслеживает небезопасные завершения до их источников в обучающих данных и применяет интервенции для уменьшения дрейфа при сохранении полезности модели. Ключевым элементом является Индекс Конфликта Убеждений (BCI), количественно оценивающий семантическое несоответствие между сгенерированными фрагментами и заданными политиками. Фреймворк предлагает три дополняющих друг друга метода защиты: фильтр безопасности TraceShield, контрастивную функцию потерь и стратегию декодирования Prov-Decode.'}, 'en': {'title': 'TraceAlign: Bridging the Gap in LLM Alignment', 'desc': "TraceAlign is a novel framework designed to address alignment drift in Large Language Models (LLMs), which occurs when these models generate unsafe outputs that deviate from human values. It traces these unsafe completions back to their origins in the training data, allowing for a better understanding of the underlying causes of alignment failures. The framework introduces the Belief Conflict Index (BCI) to measure inconsistencies between generated outputs and aligned policies, and proposes three interventions to mitigate drift. By implementing these strategies, TraceAlign significantly reduces alignment drift while maintaining the model's performance on standard tasks."}, 'zh': {'title': 'TraceAlign：减轻大型语言模型对齐漂移的创新框架', 'desc': 'TraceAlign是一个框架，用于识别和减轻大型语言模型（LLMs）中的对齐漂移。它通过追踪不安全的生成结果到其训练来源，并应用干预措施来减少漂移，同时保持模型的实用性。该框架引入了信念冲突指数（BCI），量化生成内容与对齐政策之间的语义不一致性。通过三种互补的干预措施，TraceAlign能够在保持任务性能的同时，显著降低对齐漂移的发生率。'}}}, {'id': 'https://huggingface.co/papers/2508.01126', 'title': 'UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,\n  Forecasting, and Generation', 'url': 'https://huggingface.co/papers/2508.01126', 'abstract': "A unified conditional motion diffusion model, UniEgoMotion, is introduced for egocentric motion generation and forecasting using first-person images, achieving state-of-the-art performance and generating motion from a single image.  \t\t\t\t\tAI-generated summary \t\t\t\t Egocentric human motion generation and forecasting with scene-context is crucial for enhancing AR/VR experiences, improving human-robot interaction, advancing assistive technologies, and enabling adaptive healthcare solutions by accurately predicting and simulating movement from a first-person perspective. However, existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception. To bridge this gap, we introduce Egocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks that utilize first-person images for scene-aware motion synthesis without relying on explicit 3D scene. We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices. UniEgoMotion's simple yet effective design supports egocentric motion reconstruction, forecasting, and generation from first-person visual inputs in a unified framework. Unlike previous works that overlook scene semantics, our model effectively extracts image-based scene context to infer plausible 3D motion. To facilitate training, we introduce EE4D-Motion, a large-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth 3D motion annotations. UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications.", 'score': 0, 'issue_id': 5212, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '8f30093d889bde3a', 'authors': ['Chaitanya Patel', 'Hiroki Nakamura', 'Yuta Kyuragi', 'Kazuki Kozuka', 'Juan Carlos Niebles', 'Ehsan Adeli'], 'affiliations': ['Panasonic Holdings Corporation', 'Panasonic R&D Company of America', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01126.jpg', 'data': {'categories': ['#dataset', '#games', '#benchmark', '#video', '#diffusion', '#multimodal', '#cv', '#healthcare'], 'emoji': '🕶️', 'ru': {'title': 'Революция в моделировании движения от первого лица', 'desc': 'Представлена унифицированная условная модель диффузии движения UniEgoMotion для генерации и прогнозирования эгоцентрического движения с использованием изображений от первого лица. Модель достигает наилучших результатов в реконструкции эгоцентрического движения и впервые позволяет генерировать движение по одному эгоцентрическому изображению. UniEgoMotion использует новое представление движения с учетом положения головы, специально разработанное для эгоцентрических устройств. Для обучения модели создан большой набор данных EE4D-Motion на основе EgoExo4D с псевдо-разметкой трехмерного движения.'}, 'en': {'title': 'Revolutionizing Egocentric Motion with UniEgoMotion', 'desc': 'The paper presents UniEgoMotion, a unified conditional motion diffusion model designed for generating and forecasting egocentric motion from first-person images. This model addresses the limitations of existing methods that focus on third-person perspectives and structured 3D scenes, which are not effective in real-world scenarios with dynamic cameras and occlusions. UniEgoMotion utilizes a novel head-centric motion representation and effectively extracts scene context from images to predict plausible 3D motion. The introduction of the EE4D-Motion dataset, which includes pseudo-ground-truth 3D motion annotations, supports the training of this model, achieving state-of-the-art results in egocentric motion tasks.'}, 'zh': {'title': '自我中心运动生成的新突破', 'desc': '本文介绍了一种统一的条件运动扩散模型UniEgoMotion，用于从第一人称图像生成和预测自我中心的运动。该模型在自我中心运动重建和预测方面达到了最先进的性能，能够仅从单张图像生成运动。UniEgoMotion采用了一种新颖的头部中心运动表示，支持在没有明确3D场景的情况下进行场景感知的运动合成。通过引入EE4D-Motion数据集，本文为训练提供了丰富的伪真实3D运动注释，推动了自我中心运动建模的新标准。'}}}, {'id': 'https://huggingface.co/papers/2508.05748', 'title': 'WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent', 'url': 'https://huggingface.co/papers/2508.05748', 'abstract': 'WebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks.', 'score': 81, 'issue_id': 5318, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '937594202a5b31b7', 'authors': ['Xinyu Geng', 'Peng Xia', 'Zhen Zhang', 'Xinyu Wang', 'Qiuchen Wang', 'Ruixue Ding', 'Chenxi Wang', 'Jialong Wu', 'Yida Zhao', 'Kuan Li', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.05748.jpg', 'data': {'categories': ['#reasoning', '#synthetic', '#multimodal', '#agents', '#rl', '#benchmark', '#open_source'], 'emoji': '🕵️', 'ru': {'title': 'WebWatcher: Мультимодальный агент для глубокого исследования визуальной и текстовой информации', 'desc': 'WebWatcher - это мультимодальный агент с улучшенными возможностями визуально-языкового рассуждения. Он превосходит существующие агенты в сложных задачах поиска визуальной и текстовой информации, используя синтетические траектории и обучение с подкреплением. WebWatcher использует высококачественные синтетические мультимодальные траектории для эффективного холодного старта обучения и применяет различные инструменты для глубокого рассуждения. Для оценки возможностей мультимодальных агентов авторы предлагают бенчмарк BrowseComp-VL, требующий сложного поиска информации с использованием как визуальных, так и текстовых данных.'}, 'en': {'title': 'WebWatcher: Revolutionizing Multimodal Information Retrieval', 'desc': 'WebWatcher is a multimodal agent designed to improve the retrieval of complex visual and textual information. It utilizes synthetic trajectories and reinforcement learning to enhance its visual-language reasoning capabilities. Unlike traditional text-centric agents, WebWatcher effectively integrates visual data, allowing for better reasoning and problem-solving. The introduction of the BrowseComp-VL benchmark further validates its superior performance in multimodal tasks compared to existing agents.'}, 'zh': {'title': 'WebWatcher：超越文本的多模态智能体', 'desc': 'WebWatcher是一种多模态智能体，具备增强的视觉语言推理能力，能够在复杂的视觉和文本信息检索任务中超越现有智能体。该系统利用高质量的合成多模态轨迹进行高效的冷启动训练，并通过强化学习进一步提升其泛化能力。WebWatcher在多个视觉问答基准测试中表现优异，显示出其在处理复杂多模态信息检索任务方面的潜力。为评估多模态智能体的能力，本文还提出了BrowseComp-VL基准，专注于视觉和文本信息的复杂检索。'}}}, {'id': 'https://huggingface.co/papers/2508.08086', 'title': 'Matrix-3D: Omnidirectional Explorable 3D World Generation', 'url': 'https://huggingface.co/papers/2508.08086', 'abstract': 'Matrix-3D generates wide-coverage 3D worlds from single images or text using panoramic video diffusion and reconstruction models.  \t\t\t\t\tAI-generated summary \t\t\t\t Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in https://matrix-3d.github.io.', 'score': 47, 'issue_id': 5317, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '70a9a4327de06f04', 'authors': ['Zhongqi Yang', 'Wenhang Ge', 'Yuqi Li', 'Jiaqi Chen', 'Haoyuan Li', 'Mengyin An', 'Fei Kang', 'Hua Xue', 'Baixin Xu', 'Yuyang Yin', 'Eric Li', 'Yang Liu', 'Yikai Wang', 'Hao-Xiang Guo', 'Yahui Zhou'], 'affiliations': ['Hong Kong University of Science and Technology (Guangzhou)', 'Institute of Computing Technology, Chinese Academy of Sciences', 'School of Artificial Intelligence, Beijing Normal University', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.08086.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#3d', '#synthetic'], 'emoji': '🌐', 'ru': {'title': 'Генерация исследуемых 3D-миров из одного изображения или текста', 'desc': 'Matrix-3D - это система для создания исследуемых трехмерных миров на основе одного изображения или текстового запроса. Она использует панорамную видеодиффузию и модели реконструкции для генерации широкоохватных 3D-сцен. Система включает в себя условную генерацию видео с учетом траектории и два метода реконструкции 3D: быстрый прямой и точный оптимизационный. Для обучения был создан набор данных Matrix-Pano из 116 тысяч синтетических панорамных видеопоследовательностей.'}, 'en': {'title': 'Transforming Images into Immersive 3D Worlds', 'desc': 'Matrix-3D is a novel framework designed to create expansive 3D environments from a single image or text input by leveraging advanced panoramic video diffusion and reconstruction techniques. It addresses the limitations of previous methods by employing a trajectory-guided video diffusion model that generates high-quality scene videos, ensuring geometric consistency. The framework includes two distinct approaches for converting panoramic videos into 3D worlds: a fast feed-forward model for quick reconstructions and an optimization-based method for detailed accuracy. Additionally, the introduction of the Matrix-Pano dataset, which contains a large collection of panoramic video sequences with depth and trajectory data, supports effective training and enhances the overall performance of the system.'}, 'zh': {'title': '从单图像生成全景 3D 世界的创新框架', 'desc': 'Matrix-3D 是一个生成广泛覆盖的 3D 世界的框架，能够从单张图像或文本提示中生成可探索的 3D 世界。该框架结合了条件视频生成和全景 3D 重建，利用全景表示来实现全方位的 3D 世界生成。我们首先训练了一个基于轨迹引导的全景视频扩散模型，以场景网格渲染作为条件，从而实现高质量和几何一致的场景视频生成。为了将全景场景视频提升为 3D 世界，我们提出了两种方法：快速的前馈全景重建模型和基于优化的精确 3D 场景重建流程。'}}}, {'id': 'https://huggingface.co/papers/2508.07976', 'title': 'Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL', 'url': 'https://huggingface.co/papers/2508.07976', 'abstract': 'ASearcher is an open-source project that uses scalable asynchronous RL training to enhance search agents, achieving high performance on QA tasks with long-horizon search capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.', 'score': 32, 'issue_id': 5318, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '22a9c1c0433f2764', 'authors': ['Jiaxuan Gao', 'Wei Fu', 'Minyang Xie', 'Shusheng Xu', 'Chuyi He', 'Zhiyu Mei', 'Banghua Zhu', 'Yi Wu'], 'affiliations': ['Ant Research, RL Lab', 'IIIS, Tsinghua University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.07976.jpg', 'data': {'categories': ['#dataset', '#training', '#agents', '#rl', '#long_context', '#open_source'], 'emoji': '🔍', 'ru': {'title': 'ASearcher: Революция в обучении поисковых ИИ-агентов', 'desc': 'ASearcher - это проект с открытым исходным кодом, использующий масштабируемое асинхронное обучение с подкреплением для улучшения поисковых агентов. Он достигает высокой производительности в задачах вопросно-ответного типа с возможностью долгосрочного поиска. Ключевые особенности включают полностью асинхронное обучение с подкреплением и агент на основе языковой модели, который автономно создает качественные наборы данных вопросов и ответов. ASearcher превосходит существующие открытые агенты на 32 миллиарда параметров в бенчмарках xBench и GAIA.'}, 'en': {'title': 'Unlocking Expert-Level Search Intelligence with ASearcher', 'desc': 'ASearcher is an innovative open-source project that enhances search agents through scalable asynchronous reinforcement learning (RL) training. It addresses the limitations of existing methods by enabling long-horizon search capabilities, allowing agents to learn complex strategies over extended interactions. The project introduces a prompt-based large language model (LLM) agent that autonomously generates high-quality question-answer pairs, significantly improving the dataset for training. As a result, ASearcher achieves impressive performance metrics, outperforming previous open-source agents in various QA tasks.'}, 'zh': {'title': 'ASearcher：提升搜索智能的开源强化学习项目', 'desc': 'ASearcher是一个开源项目，利用可扩展的异步强化学习（RL）训练来增强搜索代理的能力，特别是在长时间搜索的问答任务中表现出色。该项目的关键贡献包括可扩展的完全异步RL训练，能够在保持高训练效率的同时进行长时间搜索。通过基于提示的LLM代理，ASearcher能够自主合成高质量的问答，创建大规模的问答数据集。经过RL训练，ASearcher的代理在多个基准测试中显著提高了性能，展示了其在复杂搜索任务中的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2508.07409', 'title': 'CharacterShot: Controllable and Consistent 4D Character Animation', 'url': 'https://huggingface.co/papers/2508.07409', 'abstract': 'CharacterShot is a 4D character animation framework that uses a DiT-based model and dual-attention module to generate consistent 3D animations from a single image and 2D pose sequence.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we propose CharacterShot, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequnce as controllable signal. We then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency. Finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos, resulting in continuous and stable 4D character representations. Moreover, to improve character-centric performance, we construct a large-scale dataset Character4D, containing 13,115 unique characters with diverse appearances and motions, rendered from multiple viewpoints. Extensive experiments on our newly constructed benchmark, CharacterBench, demonstrate that our approach outperforms current state-of-the-art methods. Code, models, and datasets will be publicly available at https://github.com/Jeoyal/CharacterShot.', 'score': 27, 'issue_id': 5320, 'pub_date': '2025-08-10', 'pub_date_card': {'ru': '10 августа', 'en': 'August 10', 'zh': '8月10日'}, 'hash': '9990661aef1ea53c', 'authors': ['Junyao Gao', 'Jiaxing Li', 'Wenran Liu', 'Yanhong Zeng', 'Fei Shen', 'Kai Chen', 'Yanan Sun', 'Cairong Zhao'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Shanghai AI Lab', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2508.07409.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#3d', '#optimization', '#open_source'], 'emoji': '🎭', 'ru': {'title': 'Оживление персонажей в 3D из одного изображения', 'desc': 'CharacterShot - это фреймворк для 4D-анимации персонажей, использующий модель на основе DiT и модуль двойного внимания. Он позволяет создавать согласованные 3D-анимации из одного изображения и последовательности 2D-поз. Фреймворк включает предобученную модель 2D-анимации персонажей и механизм преобразования 2D в 3D с помощью двойного внимания и камерного приора. Для улучшения результатов авторы создали масштабный датасет Character4D и новый бенчмарк CharacterBench.'}, 'en': {'title': 'Transforming 2D Images into Dynamic 4D Animations!', 'desc': 'CharacterShot is a novel framework for creating 4D character animations from a single image and a sequence of 2D poses. It utilizes a DiT-based model to pretrain a 2D animation model, allowing designers to control the animation process effectively. The framework incorporates a dual-attention module and camera prior to enhance the transition from 2D to 3D, ensuring consistency in both spatial and temporal dimensions. Additionally, it introduces a neighbor-constrained 4D Gaussian splatting optimization to produce stable and continuous character representations, supported by a large dataset of diverse characters.'}, 'zh': {'title': '从单图像生成动态3D角色动画的创新框架', 'desc': 'CharacterShot是一个基于DiT模型和双重注意力模块的4D角色动画框架，能够从单一图像和2D姿势序列生成一致的3D动画。该框架允许设计师控制动画过程，通过预训练的2D角色动画模型来处理任意2D姿势序列。通过引入双重注意力模块和相机先验，模型将动画从2D提升到3D，生成具有时空和视角一致性的多视角视频。最后，采用新颖的邻域约束4D高斯点云优化，确保生成的4D角色表现连续稳定。'}}}, {'id': 'https://huggingface.co/papers/2508.09138', 'title': 'Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models', 'url': 'https://huggingface.co/papers/2508.09138', 'abstract': 'Two methods, Temporal Self-Consistency Voting and Temporal Consistency Reinforcement, improve diffusion large language models by leveraging temporal consistency in intermediate predictions.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) generate text through iterative denoising, yet current decoding strategies discard rich intermediate predictions in favor of the final output. Our work here reveals a critical phenomenon, temporal oscillation, where correct answers often emerge in the middle process, but are overwritten in later denoising steps. To address this issue, we introduce two complementary methods that exploit temporal consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time decoding strategy that aggregates predictions across denoising steps to select the most consistent output; and 2) a post-training method termed Temporal Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a measure of semantic stability across intermediate predictions, as a reward signal to encourage stable generations. Empirical results across multiple benchmarks demonstrate the effectiveness of our approach. Using the negative TSE reward alone, we observe a remarkable average improvement of 24.7% on the Countdown dataset over an existing dLLM. Combined with the accuracy reward, we achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and 25.3% on Countdown, respectively. Our findings underscore the untapped potential of temporal dynamics in dLLMs and offer two simple yet effective tools to harness them.', 'score': 25, 'issue_id': 5317, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': 'fc4661402e36332d', 'authors': ['Wen Wang', 'Bozhen Fang', 'Chenchen Jing', 'Yongliang Shen', 'Yangyi Shen', 'Qiuyu Wang', 'Hao Ouyang', 'Hao Chen', 'Chunhua Shen'], 'affiliations': ['Ant Group', 'Stanford University', 'Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.09138.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#benchmark', '#rl'], 'emoji': '⏳', 'ru': {'title': 'Использование временной динамики для улучшения диффузионных языковых моделей', 'desc': 'Статья представляет два метода улучшения диффузионных больших языковых моделей (dLLM): Временное самосогласованное голосование и Усиление временной согласованности. Эти методы используют временную согласованность в промежуточных предсказаниях модели для повышения качества генерации текста. Авторы обнаружили феномен временных осцилляций, когда правильные ответы часто появляются в середине процесса, но перезаписываются на последних шагах. Эмпирические результаты показывают значительное улучшение производительности на нескольких бенчмарках, включая GSM8K, MATH500, SVAMP и Countdown.'}, 'en': {'title': 'Harnessing Temporal Consistency for Better Language Model Outputs', 'desc': 'This paper presents two innovative methods to enhance diffusion large language models (dLLMs) by focusing on the temporal consistency of their predictions. The authors identify a problem called temporal oscillation, where valuable intermediate outputs are lost during the final denoising process. To combat this, they propose Temporal Self-Consistency Voting, which aggregates predictions from various steps to find the most reliable output, and Temporal Consistency Reinforcement, which uses a reward based on Temporal Semantic Entropy to promote stable outputs. Their experiments show significant improvements in performance across several benchmarks, highlighting the importance of leveraging temporal dynamics in language model generation.'}, 'zh': {'title': '利用时间一致性提升大语言模型的性能', 'desc': '本文提出了两种方法，时间自一致性投票和时间一致性强化，旨在通过利用中间预测的时间一致性来改进扩散大语言模型（dLLMs）。研究发现，在去噪的过程中，正确答案往往在中间步骤中出现，但在后续步骤中被覆盖。为了解决这个问题，提出的时间自一致性投票方法在测试时聚合多个去噪步骤的预测，以选择最一致的输出；而时间一致性强化则使用时间语义熵作为奖励信号，鼓励生成稳定的结果。实验证明，这些方法在多个基准测试中显著提高了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.08088', 'title': 'HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating\n  Local and Web Searches', 'url': 'https://huggingface.co/papers/2508.08088', 'abstract': 'HierSearch, a hierarchical agentic deep search framework using hierarchical RL, improves performance in multi-source retrieval tasks by coordinating local and Web search agents and refining knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.', 'score': 24, 'issue_id': 5317, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '7e2ae00381bb3360', 'authors': ['Jiejun Tan', 'Zhicheng Dou', 'Yan Yu', 'Jiehan Cheng', 'Qiang Ju', 'Jian Xie', 'Ji-Rong Wen'], 'affiliations': ['Baichuan Intelligent Technology', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.08088.jpg', 'data': {'categories': ['#hallucinations', '#rag', '#reasoning', '#agents', '#benchmark', '#rl', '#healthcare'], 'emoji': '🔍', 'ru': {'title': 'Иерархический глубокий поиск для эффективного многоисточникового извлечения информации', 'desc': 'HierSearch - это иерархическая система глубокого поиска, использующая иерархическое обучение с подкреплением для улучшения многоисточникового поиска. Она координирует локальных и веб-агентов поиска, а также уточняет полученные знания. HierSearch превосходит плоское обучение с подкреплением и другие методы в шести тестах в общей, финансовой и медицинской областях. Система предотвращает прямое копирование ответов и распространение ошибок с помощью специального фильтра знаний.'}, 'en': {'title': 'HierSearch: Elevating Multi-Source Retrieval with Hierarchical RL', 'desc': 'HierSearch is a hierarchical framework that enhances multi-source information retrieval by using hierarchical reinforcement learning (RL). It consists of local and Web search agents that work together to gather relevant information from different sources. A planner agent oversees these agents to ensure accurate and coherent answers, while a knowledge refiner filters out incorrect or irrelevant data. This approach significantly improves performance over traditional flat RL methods and outperforms existing deep search systems across various domains.'}, 'zh': {'title': '层次化智能深度搜索，提升多源检索性能', 'desc': 'HierSearch是一种层次化的智能深度搜索框架，利用层次化强化学习来提升多源检索任务的性能。该框架通过协调本地搜索代理和网络搜索代理，优化知识获取过程。与传统的单一知识源深度搜索不同，HierSearch能够同时利用本地和网络数据，满足企业对私有深度搜索系统的需求。此外，HierSearch还设计了知识精炼器，以过滤低级代理返回的错误信息和无关证据，从而提高最终答案的准确性。'}}}, {'id': 'https://huggingface.co/papers/2508.05615', 'title': 'Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency', 'url': 'https://huggingface.co/papers/2508.05615', 'abstract': 'GUI-RC and GUI-RCPO enhance GUI grounding accuracy by leveraging spatial consistency and reinforcement learning without additional training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: GUI-RC boosts Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO further improves it to 85.14% through self-supervised optimization. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more robust and data-efficient GUI agents.', 'score': 17, 'issue_id': 5317, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'b571750771c4ba60', 'authors': ['Yong Du', 'Yuchen Yan', 'Fei Tang', 'Zhengxi Lu', 'Chang Zong', 'Weiming Lu', 'Shengpei Jiang', 'Yongliang Shen'], 'affiliations': ['Central South University', 'SF Technology', 'Zhejiang University', 'Zhejiang University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.05615.jpg', 'data': {'categories': ['#inference', '#optimization', '#agents', '#rlhf', '#rl'], 'emoji': '🖥️', 'ru': {'title': 'Повышение точности ИИ-агентов для GUI без дополнительных данных', 'desc': 'В статье представлены методы GUI-RC и GUI-RCPO для повышения точности привязки графического интерфейса без использования дополнительных обучающих данных. GUI-RC использует пространственную согласованность для идентификации областей с наибольшим согласием модели. GUI-RCPO применяет обучение с подкреплением во время вывода, используя сигналы согласованности в качестве награды. Эксперименты показывают значительное улучшение точности на бенчмарке ScreenSpot для различных архитектур моделей.'}, 'en': {'title': 'Enhancing GUI Grounding with Spatial Consistency and Reinforcement Learning', 'desc': 'This paper presents GUI-RC and GUI-RCPO, two innovative methods that improve the accuracy of GUI grounding by utilizing spatial consistency and reinforcement learning without needing extra training data. GUI grounding involves translating natural language commands into specific screen coordinates, which is crucial for autonomous GUI agents. The authors propose a technique called GUI-RC that uses spatial voting grids to identify areas of agreement among multiple predictions, enhancing localization accuracy. Additionally, GUI-RCPO applies reinforcement learning to refine predictions based on how well they align with the consensus, leading to significant performance improvements on benchmark tests.'}, 'zh': {'title': '提升GUI定位准确性的创新方法', 'desc': '本文提出了GUI-RC和GUI-RCPO两种方法，以提高图形用户界面（GUI）定位的准确性。这些方法利用空间一致性和强化学习，而无需额外的训练数据。GUI-RC通过构建空间投票网格，从多个预测中识别共识区域，从而在测试时提高准确性。GUI-RCPO则将一致性模式转化为奖励，允许模型在推理过程中对未标记数据进行自我优化。'}}}, {'id': 'https://huggingface.co/papers/2508.09125', 'title': 'Complex Logical Instruction Generation', 'url': 'https://huggingface.co/papers/2508.09125', 'abstract': 'LogicIFGen and LogicIFEval assess the instruction-following capabilities of LLMs on complex, logic-rich instructions, revealing significant performance gaps.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction following has catalyzed the recent era of Large Language Models (LLMs) and is the foundational skill underpinning more advanced capabilities such as reasoning and agentic behaviors. As tasks grow more challenging, the logic structures embedded in natural language instructions becomes increasingly intricate. However, how well LLMs perform on such logic-rich instructions remains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a scalable, automated framework for generating verifiable instructions from code functions, which can naturally express rich logic such as conditionals, nesting, recursion, and function calls. We further curate a collection of complex code functions and use LogicIFGen to construct LogicIFEval, a benchmark comprising 426 verifiable logic-rich instructions. Our experiments demonstrate that current state-of-the-art LLMs still struggle to correctly follow the instructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the instructions, revealing significant deficiencies in the instruction-following ability. Code and Benchmark: https://github.com/mianzhang/LogicIF', 'score': 15, 'issue_id': 5331, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '272c54c87d8faf69', 'authors': ['Mian Zhang', 'Shujian Liu', 'Sixun Dong', 'Ming Yin', 'Yebowen Hu', 'Xun Wang', 'Steven Ma', 'Song Wang', 'Sathish Reddy Indurthi', 'Haoyun Deng', 'Zhiyu Zoey Chen', 'Kaiqiang Song'], 'affiliations': ['University of Texas at Dallas', 'Zoom Video Communications'], 'pdf_title_img': 'assets/pdf/title_img/2508.09125.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Логическое мышление - ахиллесова пята языковых моделей', 'desc': 'LogicIFGen и LogicIFEval - это новые инструменты для оценки способности языковых моделей следовать сложным логическим инструкциям. LogicIFGen автоматически генерирует проверяемые инструкции на основе функций кода. LogicIFEval представляет собой набор из 426 сложных логических инструкций для тестирования языковых моделей. Эксперименты показали, что современные модели испытывают значительные трудности с выполнением таких инструкций, следуя менее чем 60% из них.'}, 'en': {'title': 'Bridging the Gap in Logic Instruction Following for LLMs', 'desc': "This paper introduces LogicIFGen and LogicIFEval, tools designed to evaluate how well Large Language Models (LLMs) can follow complex, logic-rich instructions. LogicIFGen generates detailed instructions from code functions, incorporating advanced logic features like conditionals and recursion. The resulting benchmark, LogicIFEval, consists of 426 challenging instructions that test LLMs' capabilities. The findings show that even the best LLMs struggle with these tasks, achieving less than 60% accuracy, highlighting a significant gap in their instruction-following skills."}, 'zh': {'title': '评估LLMs的逻辑指令执行能力', 'desc': '本文提出了LogicIFGen和LogicIFEval，用于评估大型语言模型（LLMs）在复杂逻辑指令上的执行能力。随着任务的复杂性增加，自然语言指令中的逻辑结构变得更加复杂，但LLMs在这些指令上的表现尚未得到充分研究。LogicIFGen是一个可扩展的自动化框架，可以从代码函数生成可验证的指令，而LogicIFEval则是一个包含426个可验证逻辑丰富指令的基准测试。实验结果表明，当前最先进的LLMs在遵循LogicIFEval中的指令时仍然存在显著的不足，许多模型只能正确执行不到60%的指令。'}}}, {'id': 'https://huggingface.co/papers/2508.09062', 'title': 'VertexRegen: Mesh Generation with Continuous Level of Detail', 'url': 'https://huggingface.co/papers/2508.09062', 'abstract': 'VertexRegen generates meshes with continuous detail by reversing edge collapse through a generative model, offering anytime generation and flexibility in detail levels.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce VertexRegen, a novel mesh generation framework that enables generation at a continuous level of detail. Existing autoregressive methods generate meshes in a partial-to-complete manner and thus intermediate steps of generation represent incomplete structures. VertexRegen takes inspiration from progressive meshes and reformulates the process as the reversal of edge collapse, i.e. vertex split, learned through a generative model. Experimental results demonstrate that VertexRegen produces meshes of comparable quality to state-of-the-art methods while uniquely offering anytime generation with the flexibility to halt at any step to yield valid meshes with varying levels of detail.', 'score': 15, 'issue_id': 5320, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '1881627a4d7d9e0c', 'authors': ['Xiang Zhang', 'Yawar Siddiqui', 'Armen Avetisyan', 'Chris Xie', 'Jakob Engel', 'Henry Howard-Jenkins'], 'affiliations': ['Meta Reality Labs Research', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2508.09062.jpg', 'data': {'categories': ['#games', '#optimization', '#3d'], 'emoji': '🔍', 'ru': {'title': 'Гибкая генерация 3D-сеток с любым уровнем детализации', 'desc': 'VertexRegen - это новая система генерации трехмерных сеток с непрерывным уровнем детализации. Она использует обратный процесс схлопывания ребер, обучаемый с помощью генеративной модели. В отличие от существующих авторегрессивных методов, VertexRegen позволяет получать полноценные сетки на любом этапе генерации. Эксперименты показывают, что качество получаемых сеток сопоставимо с современными методами, но при этом обеспечивается гибкость в выборе уровня детализации.'}, 'en': {'title': 'VertexRegen: Anytime Mesh Generation with Continuous Detail', 'desc': 'VertexRegen is a new framework for generating 3D meshes that allows for continuous detail levels. Unlike traditional methods that build meshes from incomplete structures, VertexRegen reverses the edge collapse process, effectively splitting vertices to create detailed meshes. This approach is guided by a generative model, which learns how to produce high-quality meshes. The framework also supports anytime generation, meaning users can stop the process at any point to obtain valid meshes with different levels of detail.'}, 'zh': {'title': 'VertexRegen：灵活的连续细节网格生成', 'desc': 'VertexRegen是一种新颖的网格生成框架，能够以连续的细节级别生成网格。与现有的自回归方法不同，VertexRegen通过逆向边缘合并的方式进行生成，即通过顶点分裂来实现。该方法借鉴了渐进式网格的思想，并通过生成模型进行学习。实验结果表明，VertexRegen生成的网格质量与最先进的方法相当，同时提供了随时生成的灵活性，可以在任意步骤停止以获得不同细节级别的有效网格。'}}}, {'id': 'https://huggingface.co/papers/2508.08665', 'title': 'Aryabhata: An exam-focused language model for JEE Math', 'url': 'https://huggingface.co/papers/2508.08665', 'abstract': 'Aryabhata 1.0, a compact math reasoning model, outperforms existing models on educational exams and benchmarks by using supervised fine-tuning, reinforcement learning with verifiable rewards, and novel exploration strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Aryabhata 1.0, a compact 7B parameter math reasoning model optimized for the Indian academic exam, the Joint Entrance Examination (JEE). Despite rapid progress in large language models (LLMs), current models often remain unsuitable for educational use. Aryabhata 1.0 is built by merging strong open-weight reasoning models, followed by supervised fine-tuning (SFT) with curriculum learning on verified chain-of-thought (CoT) traces curated through best-of-n rejection sampling. To further boost performance, we apply reinforcement learning with verifiable rewards (RLVR) using A2C objective with group-relative advantage estimation alongwith novel exploration strategies such as Adaptive Group Resizing and Temperature Scaling. Evaluated on both in-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and efficiency, while offering pedagogically useful step-by-step reasoning. We release Aryabhata as a foundation model to advance exam-centric, open-source small language models. This marks our first open release for community feedback (https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0{Aryabhata 1.0 on Hugging Face}); PW is actively training future models to further improve learning outcomes for students.', 'score': 12, 'issue_id': 5323, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': 'c9693974b3ce4cf2', 'authors': ['Ritvik Rastogi', 'Sachin Dharashivkar', 'Sandeep Varma'], 'affiliations': ['AthenaAgent', 'PhysicsWallah'], 'pdf_title_img': 'assets/pdf/title_img/2508.08665.jpg', 'data': {'categories': ['#small_models', '#dataset', '#reasoning', '#training', '#open_source', '#rl', '#math'], 'emoji': '🧮', 'ru': {'title': 'Компактная ИИ-модель превосходит аналоги в математических рассуждениях', 'desc': 'Представлена модель Aryabhata 1.0 - компактная языковая модель для математических рассуждений с 7 миллиардами параметров. Модель оптимизирована для индийского вступительного экзамена JEE и превосходит существующие модели на образовательных тестах. Aryabhata 1.0 использует контролируемое обучение, обучение с подкреплением с проверяемыми наградами и новые стратегии исследования. Модель демонстрирует высокую точность и эффективность как на целевых, так и на сторонних наборах данных, предоставляя пошаговые рассуждения.'}, 'en': {'title': 'Revolutionizing Math Reasoning for Education with Aryabhata 1.0', 'desc': 'Aryabhata 1.0 is a compact math reasoning model designed specifically for the Joint Entrance Examination (JEE) in India. It utilizes supervised fine-tuning and reinforcement learning with verifiable rewards to enhance its performance on educational tasks. The model incorporates innovative exploration strategies to improve its reasoning capabilities, allowing it to provide detailed step-by-step solutions. Evaluated against various benchmarks, Aryabhata 1.0 demonstrates superior accuracy and efficiency compared to existing models, making it a valuable tool for educational purposes.'}, 'zh': {'title': 'Aryabhata 1.0：教育考试的数学推理新标杆', 'desc': 'Aryabhata 1.0 是一个紧凑的数学推理模型，专为印度的联合入学考试（JEE）优化。它通过监督微调、可验证奖励的强化学习和新颖的探索策略，超越了现有模型的表现。该模型结合了强大的开放权重推理模型，并采用了课程学习和链式思维的验证样本进行训练。经过评估，Aryabhata 在准确性和效率上均优于现有模型，同时提供了有助于教学的逐步推理过程。'}}}, {'id': 'https://huggingface.co/papers/2508.05399', 'title': 'UNCAGE: Contrastive Attention Guidance for Masked Generative\n  Transformers in Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2508.05399', 'abstract': 'UNCAGE, a training-free method using contrastive attention guidance, enhances compositional fidelity in text-to-image generation by prioritizing the unmasking of object-representing tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.', 'score': 12, 'issue_id': 5317, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '2e4f45b5a135f498', 'authors': ['Wonjun Kang', 'Byeongkeun Ahn', 'Minjae Lee', 'Kevin Galim', 'Seunghyuk Oh', 'Hyung Il Koo', 'Nam Ik Cho'], 'affiliations': ['Ajou University', 'FuriosaAI', 'Independent Researcher', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05399.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#benchmark', '#cv'], 'emoji': '🎭', 'ru': {'title': 'Точная композиция в генерации изображений через умное раскрытие токенов', 'desc': 'UNCAGE - это новый метод без дополнительного обучения, улучшающий композиционную точность в генерации изображений по тексту. Он использует контрастное управление вниманием для приоритизации раскрытия токенов, представляющих отдельные объекты. Метод применяется к маскированным генеративным трансформерам, которые ранее не исследовались в контексте композиционной генерации. UNCAGE показывает улучшение производительности по количественным и качественным оценкам на различных бенчмарках.'}, 'en': {'title': 'Enhancing Text-to-Image Generation with UNCAGE', 'desc': 'The paper introduces UNCAGE, a novel method that enhances the quality of text-to-image (T2I) generation without requiring additional training. It utilizes contrastive attention guidance to focus on unmasking tokens that represent distinct objects, improving compositional fidelity in generated images. This approach addresses the limitations of existing models, particularly in accurately binding attributes and achieving text-image alignment. The results show that UNCAGE outperforms previous methods in both quantitative and qualitative assessments while maintaining low computational overhead.'}, 'zh': {'title': 'UNCAGE：提升文本到图像生成的组合保真度', 'desc': 'UNCAGE是一种无训练的方法，利用对比注意力引导来增强文本到图像生成中的组合保真度。该方法通过优先解码清晰表示单个对象的标记，来改善文本与图像的对齐。尽管现有的扩散模型在组合生成方面存在挑战，UNCAGE在多个基准测试中表现出色，提升了生成质量。该方法在推理时几乎没有额外开销，展示了其高效性和实用性。'}}}, {'id': 'https://huggingface.co/papers/2508.08940', 'title': 'Train Long, Think Short: Curriculum Learning for Efficient Reasoning', 'url': 'https://huggingface.co/papers/2508.08940', 'abstract': 'A curriculum learning strategy using Group Relative Policy Optimization (GRPO) enhances the reasoning abilities of large language models by progressively tightening token budgets, improving accuracy and token efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo.', 'score': 11, 'issue_id': 5323, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '72998e00bb3562ea', 'authors': ['Hasan Abed Al Kader Hammoud', 'Kumail Alhamoud', 'Abed Hammoud', 'Elie Bou-Zeid', 'Marzyeh Ghassemi', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology (KAUST), Saudi Arabia', 'Massachusetts Institute of Technology (MIT), Cambridge, MA, USA', 'Princeton University, Princeton, NJ, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.08940.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#optimization', '#training', '#math'], 'emoji': '🧠', 'ru': {'title': 'Эффективное обучение языковых моделей через постепенное сжатие рассуждений', 'desc': 'Эта статья представляет стратегию обучения с учителем, использующую Group Relative Policy Optimization (GRPO) для улучшения способностей больших языковых моделей к рассуждению. Метод постепенно ужесточает ограничения на количество токенов, что позволяет моделям сначала находить эффективные стратегии решения, а затем сжимать их в более краткие цепочки рассуждений. Авторы дополняют GRPO функцией вознаграждения, учитывающей правильность задачи, эффективность длины и соблюдение форматирования. Эксперименты на нескольких наборах данных показывают, что этот подход превосходит базовые модели с фиксированным бюджетом токенов, достигая более высокой точности и значительно улучшенной эффективности использования токенов.'}, 'en': {'title': 'Progressive Learning for Efficient Reasoning in LLMs', 'desc': 'This paper presents a novel curriculum learning strategy that utilizes Group Relative Policy Optimization (GRPO) to enhance the reasoning capabilities of large language models (LLMs). The approach involves starting with a generous token budget and progressively tightening it, allowing models to first explore various solution strategies before refining them into concise reasoning. By incorporating a reward function that balances task correctness, length efficiency, and formatting adherence, the method improves both accuracy and token efficiency. Experiments demonstrate that this curriculum-based training consistently outperforms traditional fixed-budget methods across multiple datasets, highlighting the effectiveness of progressive constraints in model training.'}, 'zh': {'title': '课程学习提升推理能力的策略', 'desc': '本文提出了一种基于课程学习策略的长度控制推理方法，使用群体相对策略优化（GRPO）来增强大型语言模型的推理能力。该方法通过逐步收紧令牌预算，鼓励模型首先探索有效的解决策略，然后将其提炼为更简洁的推理过程。我们引入了一种奖励函数，平衡任务正确性、长度效率和格式遵循三个信号。实验结果表明，基于课程的训练在相同的最终预算下，始终优于固定预算的基线，达到了更高的准确性和显著提高的令牌效率。'}}}, {'id': 'https://huggingface.co/papers/2508.07485', 'title': 'Democratizing Diplomacy: A Harness for Evaluating Any Large Language\n  Model on Full-Press Diplomacy', 'url': 'https://huggingface.co/papers/2508.07485', 'abstract': "An evaluation harness allows large language models to play Diplomacy without fine-tuning, providing insights into their strategic reasoning capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t We present the first evaluation harness that enables any out-of-the-box, local, Large Language Models (LLMs) to play full-press Diplomacy without fine-tuning or specialized training. Previous work required frontier LLMs, or fine-tuning, due to the high complexity and information density of Diplomacy's game state. Combined with the high variance of matches, these factors made Diplomacy prohibitive for study. In this work, we used data-driven iteration to optimize a textual game state representation such that a 24B model can reliably complete matches without any fine tuning. We develop tooling to facilitate hypothesis testing and statistical analysis, and we present case studies on persuasion, aggressive playstyles, and performance across a range of models. We conduct a variety of experiments across many popular LLMs, finding the larger models perform the best, but the smaller models still play adequately. We also introduce Critical State Analysis: an experimental protocol for rapidly iterating and analyzing key moments in a game at depth. Our harness democratizes the evaluation of strategic reasoning in LLMs by eliminating the need for fine-tuning, and it provides insights into how these capabilities emerge naturally from widely used LLMs. Our code is available in the supplement and will be open sourced.", 'score': 9, 'issue_id': 5327, 'pub_date': '2025-08-10', 'pub_date_card': {'ru': '10 августа', 'en': 'August 10', 'zh': '8月10日'}, 'hash': '363d0f5a1bf50d65', 'authors': ['Alexander Duffy', 'Samuel J Paech', 'Ishana Shastri', 'Elizabeth Karpinski', 'Baptiste Alloui-Cros', 'Tyler Marques', 'Matthew Lyle Olson'], 'affiliations': ['Good Start Labs', 'Independent', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2508.07485.jpg', 'data': {'categories': ['#games', '#benchmark', '#rl', '#reasoning', '#open_source'], 'emoji': '🎭', 'ru': {'title': 'Стратегическое мышление LLM: игра в Diplomacy без специальной подготовки', 'desc': 'Исследователи разработали систему оценки, позволяющую большим языковым моделям (LLM) играть в Diplomacy без дополнительного обучения. Это достигается за счет оптимизации текстового представления состояния игры, что позволяет даже 24-миллиардной модели успешно завершать матчи. Эксперименты показали, что более крупные модели демонстрируют лучшие результаты, хотя и меньшие модели играют адекватно. Авторы также представили метод Critical State Analysis для быстрого анализа ключевых моментов игры.'}, 'en': {'title': 'Unlocking Strategic Play: LLMs in Diplomacy Without Fine-Tuning', 'desc': 'This paper introduces an evaluation harness that allows large language models (LLMs) to play the complex game of Diplomacy without the need for fine-tuning. By optimizing a textual representation of the game state, the authors demonstrate that even a 24 billion parameter model can effectively engage in matches. The study includes various experiments that reveal how different models, particularly larger ones, perform in terms of strategic reasoning and gameplay styles. Additionally, the authors present a new protocol called Critical State Analysis to facilitate in-depth examination of pivotal moments in the game, making it easier to understand the strategic capabilities of LLMs.'}, 'zh': {'title': '无微调的外交游戏评估工具', 'desc': '本文介绍了一种评估工具，使得大型语言模型（LLMs）能够在不进行微调的情况下，完整地进行外交游戏。以往的研究需要前沿的LLMs或微调，因为外交游戏的复杂性和信息密度很高。我们通过数据驱动的迭代优化了文本游戏状态的表示，使得一个24B的模型能够可靠地完成比赛。该工具简化了假设测试和统计分析，帮助我们深入研究模型的战略推理能力。'}}}, {'id': 'https://huggingface.co/papers/2508.08896', 'title': 'Towards Affordance-Aware Robotic Dexterous Grasping with Human-like\n  Priors', 'url': 'https://huggingface.co/papers/2508.08896', 'abstract': 'AffordDex is a two-stage framework that learns a universal grasping policy with motion priors and object affordances, outperforming state-of-the-art methods in dexterous grasping and human-like postures.  \t\t\t\t\tAI-generated summary \t\t\t\t A dexterous hand capable of generalizable grasping objects is fundamental for the development of general-purpose embodied AI. However, previous methods focus narrowly on low-level grasp stability metrics, neglecting affordance-aware positioning and human-like poses which are crucial for downstream manipulation. To address these limitations, we propose AffordDex, a novel framework with two-stage training that learns a universal grasping policy with an inherent understanding of both motion priors and object affordances. In the first stage, a trajectory imitator is pre-trained on a large corpus of human hand motions to instill a strong prior for natural movement. In the second stage, a residual module is trained to adapt these general human-like motions to specific object instances. This refinement is critically guided by two components: our Negative Affordance-aware Segmentation (NAA) module, which identifies functionally inappropriate contact regions, and a privileged teacher-student distillation process that ensures the final vision-based policy is highly successful. Extensive experiments demonstrate that AffordDex not only achieves universal dexterous grasping but also remains remarkably human-like in posture and functionally appropriate in contact location. As a result, AffordDex significantly outperforms state-of-the-art baselines across seen objects, unseen instances, and even entirely novel categories.', 'score': 8, 'issue_id': 5324, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': 'e89f4cd5dd20f98d', 'authors': ['Haoyu Zhao', 'Linghao Zhuang', 'Xingyue Zhao', 'Cheng Zeng', 'Haoran Xu', 'Yuming Jiang', 'Jun Cen', 'Kexiang Wang', 'Jiayan Guo', 'Siteng Huang', 'Xin Li', 'Deli Zhao', 'Hua Zou'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'Tsinghua University', 'Wuhan University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.08896.jpg', 'data': {'categories': ['#agents', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Универсальный захват с пониманием функциональности объектов', 'desc': 'AffordDex - это двухэтапная система для обучения универсальной политики захвата с использованием приоров движения и понимания функциональности объектов. На первом этапе имитатор траектории обучается на большом корпусе движений человеческой руки. На втором этапе остаточный модуль адаптирует эти движения к конкретным объектам с помощью сегментации отрицательных функциональных зон и дистилляции знаний. AffordDex превосходит современные методы в ловкости захвата и человекоподобных позах.'}, 'en': {'title': 'AffordDex: Mastering Grasping with Human-Like Precision', 'desc': 'AffordDex is a two-stage framework designed to improve dexterous grasping in robots by integrating motion priors and object affordances. The first stage involves pre-training a trajectory imitator on human hand movements to create a strong foundation for natural motion. In the second stage, a residual module fine-tunes these motions for specific objects, guided by a Negative Affordance-aware Segmentation (NAA) to avoid inappropriate contact areas. This approach allows AffordDex to achieve superior performance in grasping tasks while maintaining human-like postures and effective contact strategies.'}, 'zh': {'title': 'AffordDex：通用抓取策略的新突破', 'desc': 'AffordDex是一个两阶段的框架，旨在学习通用的抓取策略，结合了运动先验和物体可供性。与以往方法不同，AffordDex不仅关注抓取的稳定性，还考虑了人类般的姿势和物体的功能性接触区域。第一阶段通过模仿人类手部运动来预训练轨迹模仿器，第二阶段则通过残差模块将这些运动适应特定物体。实验结果表明，AffordDex在抓取能力和姿势上都优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2508.08244', 'title': 'Cut2Next: Generating Next Shot via In-Context Tuning', 'url': 'https://huggingface.co/papers/2508.08244', 'abstract': 'Cut2Next, a framework using a Diffusion Transformer with in-context tuning and hierarchical prompting, generates high-quality, cinematically coherent shots that adhere to professional editing patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective multi-shot generation demands purposeful, film-like transitions and strict cinematic continuity. Current methods, however, often prioritize basic visual consistency, neglecting crucial editing patterns (e.g., shot/reverse shot, cutaways) that drive narrative flow for compelling storytelling. This yields outputs that may be visually coherent but lack narrative sophistication and true cinematic integrity. To bridge this, we introduce Next Shot Generation (NSG): synthesizing a subsequent, high-quality shot that critically conforms to professional editing patterns while upholding rigorous cinematic continuity. Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This strategy uses Relational Prompts to define overall context and inter-shot editing styles. Individual Prompts then specify per-shot content and cinematographic attributes. Together, these guide Cut2Next to generate cinematically appropriate next shots. Architectural innovations, Context-Aware Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further integrate these diverse signals without introducing new parameters. We construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with hierarchical prompts, and introduce CutBench for evaluation. Experiments show Cut2Next excels in visual consistency and text fidelity. Crucially, user studies reveal a strong preference for Cut2Next, particularly for its adherence to intended editing patterns and overall cinematic continuity, validating its ability to generate high-quality, narratively expressive, and cinematically coherent subsequent shots.', 'score': 8, 'issue_id': 5320, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '172314ad94a655f4', 'authors': ['Jingwen He', 'Hongbo Liu', 'Jiajun Li', 'Ziqi Huang', 'Yu Qiao', 'Wanli Ouyang', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.08244.jpg', 'data': {'categories': ['#cv', '#diffusion', '#benchmark', '#story_generation', '#dataset', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'Искусственный интеллект освоил профессиональный киномонтаж', 'desc': 'Cut2Next - это фреймворк для генерации высококачественных кинематографических кадров с использованием диффузионного трансформера. Он применяет in-context tuning и иерархическое промптирование для создания последовательностей кадров, соответствующих профессиональным паттернам монтажа. Cut2Next использует реляционные промпты для определения общего контекста и стилей монтажа, а также индивидуальные промпты для задания содержания и кинематографических атрибутов каждого кадра. Эксперименты показали превосходство Cut2Next в визуальной согласованности и соответствии текстовым описаниям.'}, 'en': {'title': 'Cinematic Continuity in AI-Generated Shots', 'desc': "Cut2Next is a novel framework that utilizes a Diffusion Transformer to generate high-quality video shots that follow professional editing patterns. It addresses the limitations of existing methods that focus mainly on visual consistency, often overlooking essential cinematic techniques that enhance storytelling. By implementing Next Shot Generation (NSG) with in-context tuning and hierarchical prompting, Cut2Next ensures that each generated shot maintains narrative flow and cinematic integrity. The framework's architectural innovations and the creation of specialized datasets enable it to produce visually coherent and narratively rich outputs, as confirmed by user studies favoring its performance."}, 'zh': {'title': '生成高质量电影镜头的智能框架', 'desc': 'Cut2Next是一个框架，利用扩散变换器和上下文调优生成高质量的镜头，符合专业的剪辑模式。该方法解决了当前生成技术在叙事流畅性和剪辑模式上的不足，确保生成的镜头不仅视觉一致，还具备叙事的复杂性。通过引入层次化多提示策略，Cut2Next能够定义整体上下文和镜头间的编辑风格，从而生成符合电影连续性的后续镜头。实验结果表明，Cut2Next在视觉一致性和文本保真度方面表现优异，用户研究也显示出对其剪辑模式的强烈偏好。'}}}, {'id': 'https://huggingface.co/papers/2508.06964', 'title': 'Adversarial Video Promotion Against Text-to-Video Retrieval', 'url': 'https://huggingface.co/papers/2508.06964', 'abstract': 'The Video Promotion attack (ViPro) enhances the robustness of text-to-video retrieval (T2VR) by promoting videos towards selected queries, demonstrating significant improvements over existing baselines in various attack scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Thanks to the development of cross-modal models, text-to-video retrieval (T2VR) is advancing rapidly, but its robustness remains largely unexamined. Existing attacks against T2VR are designed to push videos away from queries, i.e., suppressing the ranks of videos, while the attacks that pull videos towards selected queries, i.e., promoting the ranks of videos, remain largely unexplored. These attacks can be more impactful as attackers may gain more views/clicks for financial benefits and widespread (mis)information. To this end, we pioneer the first attack against T2VR to promote videos adversarially, dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement (MoRe) to capture the finer-grained, intricate interaction between visual and textual modalities to enhance black-box transferability. Comprehensive experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing datasets with over 10k videos, evaluated under 3 scenarios. All experiments are conducted in a multi-target setting to reflect realistic scenarios where attackers seek to promote the video regarding multiple queries simultaneously. We also evaluated our attacks for defences and imperceptibility. Overall, ViPro surpasses other baselines by over 30/10/4% for white/grey/black-box settings on average. Our work highlights an overlooked vulnerability, provides a qualitative analysis on the upper/lower bound of our attacks, and offers insights into potential counterplays. Code will be publicly available at https://github.com/michaeltian108/ViPro.', 'score': 8, 'issue_id': 5323, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 августа', 'en': 'August 9', 'zh': '8月9日'}, 'hash': '25b0548ad7d6f59c', 'authors': ['Qiwei Tian', 'Chenhao Lin', 'Zhengyu Zhao', 'Qian Li', 'Shuai Liu', 'Chao Shen'], 'affiliations': ['Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.06964.jpg', 'data': {'categories': ['#multimodal', '#security', '#video'], 'emoji': '🎥', 'ru': {'title': 'ViPro: Новый метод атаки для продвижения видео в системах поиска', 'desc': 'Эта статья представляет новый метод атаки на системы поиска видео по текстовому запросу, названный Video Promotion (ViPro). В отличие от существующих подходов, ViPro нацелен на продвижение видео к определенным запросам, а не на их подавление. Авторы предлагают технику Modal Refinement (MoRe) для улучшения переносимости атаки между различными моделями. Эксперименты показывают значительное превосходство ViPro над базовыми методами в различных сценариях атак.'}, 'en': {'title': 'Promoting Videos: A New Threat to Text-to-Video Retrieval', 'desc': 'The Video Promotion attack (ViPro) introduces a novel approach to enhance the robustness of text-to-video retrieval (T2VR) systems by promoting videos towards specific queries. Unlike traditional attacks that aim to suppress video rankings, ViPro focuses on increasing the visibility of videos, which can lead to greater financial gains for attackers. The paper also presents Modal Refinement (MoRe), a technique that improves the interaction between visual and textual data, enhancing the effectiveness of the attack across different scenarios. Comprehensive experiments demonstrate that ViPro significantly outperforms existing methods, revealing a critical vulnerability in T2VR systems and suggesting avenues for future defenses.'}, 'zh': {'title': '视频推广攻击：提升T2VR鲁棒性的创新方法', 'desc': '视频推广攻击（ViPro）通过将视频向选定查询推广，增强了文本到视频检索（T2VR）的鲁棒性。与现有的攻击方法不同，ViPro专注于提升视频的排名，而不是将其压制。我们还提出了模态精炼（MoRe）方法，以捕捉视觉和文本模态之间更细致的交互，从而提高黑箱转移性。实验结果显示，ViPro在多种攻击场景下的表现优于现有基线，展示了这一领域被忽视的脆弱性。'}}}, {'id': 'https://huggingface.co/papers/2508.08938', 'title': 'DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech\n  Recognition', 'url': 'https://huggingface.co/papers/2508.08938', 'abstract': 'Decoder-Centric Regularization in Encoder-Decoder (DeCRED) improves ASR robustness and generalization by adding auxiliary classifiers to the decoder, reducing internal language model perplexity and WER.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a simple yet effective regularization for the internal language model induced by the decoder in encoder-decoder ASR models, thereby improving robustness and generalization in both in- and out-of-domain settings. The proposed method, Decoder-Centric Regularization in Encoder-Decoder (DeCRED), adds auxiliary classifiers to the decoder, enabling next token prediction via intermediate logits. Empirically, DeCRED reduces the mean internal LM BPE perplexity by 36.6% relative to 11 test sets. Furthermore, this translates into actual WER improvements over the baseline in 5 of 7 in-domain and 3 of 4 out-of-domain test sets, reducing macro WER from 6.4% to 6.3% and 18.2% to 16.2%, respectively. On TEDLIUM3, DeCRED achieves 7.0% WER, surpassing the baseline and encoder-centric InterCTC regularization by 0.6% and 0.5%, respectively. Finally, we compare DeCRED with OWSM v3.1 and Whisper-medium, showing competitive WERs despite training on much less data with fewer parameters.', 'score': 7, 'issue_id': 5324, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '211692e26d837b1b', 'authors': ['Alexander Polok', 'Santosh Kesiraju', 'Karel Beneš', 'Bolaji Yusuf', 'Lukáš Burget', 'Jan Černocký'], 'affiliations': ['Brno University of Technology, Brno, Czech Republic'], 'pdf_title_img': 'assets/pdf/title_img/2508.08938.jpg', 'data': {'categories': ['#audio', '#training', '#optimization'], 'emoji': '🎙️', 'ru': {'title': 'DeCRED: Повышение точности распознавания речи через регуляризацию декодера', 'desc': 'Статья представляет новый метод регуляризации для автоматического распознавания речи (ASR) под названием DeCRED. Этот подход добавляет вспомогательные классификаторы к декодеру энкодер-декодерной модели, что улучшает робастность и обобщающую способность системы. DeCRED снижает перплексию внутренней языковой модели на 36.6% и улучшает показатель WER как на целевых, так и на внедоменных данных. Метод превосходит базовую модель и другие подходы к регуляризации, показывая конкурентоспособные результаты даже при обучении на меньшем объеме данных.'}, 'en': {'title': 'Enhancing ASR with Decoder-Centric Regularization', 'desc': "This paper introduces Decoder-Centric Regularization in Encoder-Decoder (DeCRED), a method designed to enhance the performance of automatic speech recognition (ASR) systems. By incorporating auxiliary classifiers into the decoder, DeCRED effectively reduces the internal language model's perplexity, leading to improved robustness and generalization across various datasets. The results show a significant decrease in word error rate (WER) and internal language model perplexity, demonstrating the effectiveness of this approach. Additionally, DeCRED outperforms existing methods while using fewer training data and parameters, showcasing its efficiency in ASR tasks."}, 'zh': {'title': '解码器中心正则化提升ASR性能', 'desc': '本文提出了一种简单而有效的正则化方法，称为解码器中心正则化（DeCRED），旨在提高编码器-解码器自动语音识别（ASR）模型的鲁棒性和泛化能力。通过在解码器中添加辅助分类器，DeCRED能够通过中间logits进行下一个标记的预测，从而降低内部语言模型的困惑度。实验证明，DeCRED在11个测试集上相对减少了36.6%的平均内部语言模型BPE困惑度，并在多个测试集中显著降低了字错误率（WER）。最终，DeCRED在TEDLIUM3数据集上实现了7.0%的WER，超越了基线模型和编码器中心的InterCTC正则化。'}}}, {'id': 'https://huggingface.co/papers/2508.08791', 'title': 'Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments', 'url': 'https://huggingface.co/papers/2508.08791', 'abstract': "An automated pipeline for constructing training environments and a verifiable reward mechanism enhance large language models' tool-use performance without compromising general capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Effective tool use is essential for large language models (LLMs) to interact meaningfully with their environment. However, progress is limited by the lack of efficient reinforcement learning (RL) frameworks specifically designed for tool use, due to challenges in constructing stable training environments and designing verifiable reward mechanisms. To address this, we propose an automated environment construction pipeline, incorporating scenario decomposition, document generation, function integration, complexity scaling, and localized deployment. This enables the creation of high-quality training environments that provide detailed and measurable feedback without relying on external tools. Additionally, we introduce a verifiable reward mechanism that evaluates both the precision of tool use and the completeness of task execution. When combined with trajectory data collected from the constructed environments, this mechanism integrates seamlessly with standard RL algorithms to facilitate feedback-driven model training. Experiments on LLMs of varying scales demonstrate that our approach significantly enhances the models' tool-use performance without degrading their general capabilities, regardless of inference modes or training algorithms. Our analysis suggests that these gains result from improved context understanding and reasoning, driven by updates to the lower-layer MLP parameters in models.", 'score': 7, 'issue_id': 5319, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': 'd6c6aa0f12a1c869', 'authors': ['Junjie Ye', 'Changhao Jiang', 'Zhengyin Du', 'Yufei Xu', 'Xuesong Yao', 'Zhiheng Xi', 'Xiaoran Fan', 'Qi Zhang', 'Xuanjing Huang', 'Jiecao Chen'], 'affiliations': ['ByteDance Seed', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.08791.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': '🛠️', 'ru': {'title': 'Автоматизация обучения LLM эффективному использованию инструментов', 'desc': 'Предложена автоматизированная система для создания обучающих сред и механизма проверяемого вознаграждения, улучшающая способности больших языковых моделей (LLM) использовать инструменты. Система включает декомпозицию сценариев, генерацию документов, интеграцию функций и масштабирование сложности. Введен механизм вознаграждения, оценивающий точность использования инструментов и полноту выполнения задач. Эксперименты показали значительное улучшение навыков использования инструментов LLM без ухудшения общих возможностей.'}, 'en': {'title': 'Enhancing Tool Use in LLMs with Automated Training Environments', 'desc': 'This paper presents a new method to improve how large language models (LLMs) use tools effectively. It introduces an automated system for creating training environments that are stable and provide clear feedback, which is crucial for reinforcement learning (RL). Additionally, the authors propose a verifiable reward mechanism that assesses how well the models use tools and complete tasks. The results show that this approach enhances the tool-use abilities of LLMs without harming their overall performance.'}, 'zh': {'title': '提升工具使用性能的自动化训练环境', 'desc': '本文提出了一种自动化的训练环境构建管道，旨在提升大型语言模型（LLMs）在工具使用方面的表现，同时保持其通用能力不受影响。该管道通过场景分解、文档生成、功能集成、复杂度扩展和本地部署等步骤，创建高质量的训练环境，并提供详细的可测量反馈。我们还引入了一种可验证的奖励机制，评估工具使用的准确性和任务执行的完整性。实验结果表明，该方法显著提升了模型的工具使用性能，且不影响其整体能力。'}}}, {'id': 'https://huggingface.co/papers/2508.03936', 'title': 'ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software\n  Assistants', 'url': 'https://huggingface.co/papers/2508.03936', 'abstract': 'ASTRA, an automated agent system, uncovers safety flaws in AI-driven code generation and security guidance by building knowledge graphs, exploring vulnerabilities, and generating violation-inducing cases, outperforming existing methods in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t AI coding assistants like GitHub Copilot are rapidly transforming software development, but their safety remains deeply uncertain-especially in high-stakes domains like cybersecurity. Current red-teaming tools often rely on fixed benchmarks or unrealistic prompts, missing many real-world vulnerabilities. We present ASTRA, an automated agent system designed to systematically uncover safety flaws in AI-driven code generation and security guidance systems. ASTRA works in three stages: (1) it builds structured domain-specific knowledge graphs that model complex software tasks and known weaknesses; (2) it performs online vulnerability exploration of each target model by adaptively probing both its input space, i.e., the spatial exploration, and its reasoning processes, i.e., the temporal exploration, guided by the knowledge graphs; and (3) it generates high-quality violation-inducing cases to improve model alignment. Unlike prior methods, ASTRA focuses on realistic inputs-requests that developers might actually ask-and uses both offline abstraction guided domain modeling and online domain knowledge graph adaptation to surface corner-case vulnerabilities. Across two major evaluation domains, ASTRA finds 11-66% more issues than existing techniques and produces test cases that lead to 17% more effective alignment training, showing its practical value for building safer AI systems.', 'score': 7, 'issue_id': 5336, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'c497fcfbe0dec565', 'authors': ['Xiangzhe Xu', 'Guangyu Shen', 'Zian Su', 'Siyuan Cheng', 'Hanxi Guo', 'Lu Yan', 'Xuan Chen', 'Jiasheng Jiang', 'Xiaolong Jin', 'Chengpeng Wang', 'Zhuo Zhang', 'Xiangyu Zhang'], 'affiliations': ['Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03936.jpg', 'data': {'categories': ['#alignment', '#training', '#graphs', '#benchmark', '#agents', '#data', '#security'], 'emoji': '🕵️', 'ru': {'title': 'ASTRA: умный агент для обеспечения безопасности ИИ-ассистентов', 'desc': 'ASTRA - это автоматизированная агентная система для выявления уязвимостей в системах генерации кода и рекомендаций по безопасности на основе ИИ. Система строит графы знаний, исследует уязвимости и генерирует тестовые случаи, вызывающие нарушения. ASTRA превосходит существующие методы в реальных сценариях, находя на 11-66% больше проблем. Система фокусируется на реалистичных запросах разработчиков и использует как офлайн-моделирование, так и онлайн-адаптацию графов знаний.'}, 'en': {'title': 'ASTRA: Uncovering AI Safety Flaws with Knowledge Graphs', 'desc': 'ASTRA is an automated agent system that identifies safety flaws in AI-driven code generation and security guidance. It utilizes knowledge graphs to model software tasks and known vulnerabilities, allowing for a structured approach to vulnerability exploration. The system probes both the input space and reasoning processes of target models to uncover real-world vulnerabilities. ASTRA outperforms existing methods by finding significantly more issues and generating effective test cases for improving model alignment.'}, 'zh': {'title': 'ASTRA：提升AI安全性的自动化代理系统', 'desc': 'ASTRA是一个自动化代理系统，旨在发现AI驱动的代码生成和安全指导中的安全缺陷。它通过构建知识图谱，探索漏洞，并生成违反案例，超越了现有方法在真实场景中的表现。ASTRA的工作分为三个阶段：构建领域特定的知识图谱、在线探索目标模型的漏洞，以及生成高质量的违反案例。与之前的方法不同，ASTRA关注现实输入，能够发现更多的边缘案例漏洞，从而提高AI系统的安全性。'}}}, {'id': 'https://huggingface.co/papers/2508.09101', 'title': 'AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators', 'url': 'https://huggingface.co/papers/2508.09101', 'abstract': 'AutoCodeGen creates a large-scale, multilingual code generation benchmark, AutoCodeBench, to evaluate LLMs on diverse and complex tasks without manual annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios.', 'score': 5, 'issue_id': 5318, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '6dd7cffc5f881c6a', 'authors': ['Jason Chou', 'Ao Liu', 'Yuchi Deng', 'Zhiying Zeng', 'Tao Zhang', 'Haotian Zhu', 'Jianwei Cai', 'Yue Mao', 'Chenchen Zhang', 'Lingyun Tan', 'Ziyan Xu', 'Bohui Zhai', 'Hengyi Liu', 'Speed Zhu', 'Wiggin Zhou', 'Fengzong Lian'], 'affiliations': ['Hunyuan Team, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2508.09101.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#games', '#benchmark', '#open_source'], 'emoji': '🖥️', 'ru': {'title': 'AutoCodeBench: автоматизированный многоязычный бенчмарк для оценки генерации кода ИИ', 'desc': 'AutoCodeGen представляет собой метод автоматического создания многоязычного бенчмарка для оценки генерации кода языковыми моделями без ручной аннотации. Он создает набор данных AutoCodeBench, содержащий 3920 задач на 20 языках программирования. AutoCodeBench позволяет оценивать способности больших языковых моделей решать сложные и разнообразные задачи кодирования на разных языках. Результаты показывают, что даже самые продвинутые модели испытывают трудности с комплексностью и многоязычностью этих задач.'}, 'en': {'title': 'Revolutionizing Code Generation Evaluation with AutoCodeBench', 'desc': 'The paper introduces AutoCodeGen, a method for creating a large-scale, multilingual benchmark called AutoCodeBench to evaluate the code generation capabilities of Large Language Models (LLMs). Unlike existing benchmarks that rely on manual annotations and focus mainly on Python, AutoCodeBench offers a diverse set of 3,920 problems across 20 programming languages, ensuring a balanced difficulty level. The benchmark is generated automatically, ensuring high data quality through advanced techniques like reverse-order problem generation and multiple filtering steps. The evaluation of over 30 LLMs on this benchmark reveals that even the most advanced models face challenges with the complexity and diversity of the tasks, highlighting the need for more rigorous testing in multilingual code generation.'}, 'zh': {'title': '自动化多语言代码生成基准的创新', 'desc': 'AutoCodeGen 是一种自动化方法，用于生成高难度的多语言代码生成数据集，而无需人工注释。它通过大型语言模型（LLMs）生成测试输入，并通过多语言沙箱获取测试输出，从而确保测试用例的正确性和完整性。我们推出的 AutoCodeBench 是一个大规模的代码生成基准，包含3920个问题，均匀分布在20种编程语言中，旨在评估 LLMs 在复杂、多样和实际的多语言任务上的表现。研究结果表明，即使是最先进的 LLMs 在这些任务的复杂性和多样性面前也面临挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.09123', 'title': 'OpenCUA: Open Foundations for Computer-Use Agents', 'url': 'https://huggingface.co/papers/2508.09123', 'abstract': 'OpenCUA is an open-source framework for vision-language models as computer-use agents, featuring an annotation infrastructure, a large-scale dataset, and a scalable pipeline that achieves state-of-the-art performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state-action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-32B achieves an average success rate of 34.8% on OSWorld-Verified, establishing a new state-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA (GPT-4o). Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research.', 'score': 4, 'issue_id': 5323, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '111b2e6f3de484b8', 'authors': ['Xinyuan Wang', 'Bowen Wang', 'Dunjie Lu', 'Junlin Yang', 'Tianbao Xie', 'Junli Wang', 'Jiaqi Deng', 'Xiaole Guo', 'Yiheng Xu', 'Chen Henry Wu', 'Zhennan Shen', 'Zhuokai Li', 'Ryan Li', 'Xiaochuan Li', 'Junda Chen', 'Boyuan Zheng', 'Peihang Li', 'Fangyu Lei', 'Ruisheng Cao', 'Yeqiao Fu', 'Dongchan Shin', 'Martin Shin', 'Jiarui Hu', 'Yuyan Wang', 'Jixuan Chen', 'Yuxiao Ye', 'Danyang Zhang', 'Dikang Du', 'Hao Hu', 'Huarong Chen', 'Zaida Zhou', 'Yipu Wang', 'Heng Wang', 'Diyi Yang', 'Victor Zhong', 'Flood Sung', 'Y. Charles', 'Zhilin Yang', 'Tao Yu'], 'affiliations': ['Carnegie Mellon University', 'Moonshot AI', 'Stanford University', 'University of Waterloo', 'XLANG Lab, University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.09123.jpg', 'data': {'categories': ['#agents', '#dataset', '#reasoning', '#multimodal', '#agi', '#benchmark', '#open_source'], 'emoji': '🖥️', 'ru': {'title': 'OpenCUA: открытая платформа для создания ИИ-ассистентов нового поколения', 'desc': 'OpenCUA - это открытая инфраструктура для моделей компьютерного зрения и обработки естественного языка, действующих как агенты для использования компьютера. Она включает систему аннотирования, масштабный набор данных и расширяемый конвейер, достигающий передовых результатов. Фреймворк состоит из инструмента для записи действий человека на компьютере, датасета AgentNet с задачами для 3 операционных систем и более 200 приложений, а также pipeline для преобразования демонстраций в пары состояние-действие с рефлексивными рассуждениями. Модель OpenCUA-32B достигает нового уровня производительности среди открытых моделей на бенчмарке OSWorld-Verified.'}, 'en': {'title': 'Empowering Research with OpenCUA: The Future of Computer-Use Agents', 'desc': 'OpenCUA is an innovative open-source framework designed for vision-language models that function as computer-use agents (CUAs). It includes a robust annotation infrastructure, a large-scale dataset called AgentNet, and a scalable pipeline that enhances performance through reflective reasoning. The framework allows researchers to access and study the capabilities and limitations of CUAs, which are becoming increasingly important in automating digital tasks. With state-of-the-art results, OpenCUA-32B sets a new benchmark in the field, promoting further research and development in open CUA systems.'}, 'zh': {'title': '开放源代码，推动计算机使用代理的未来', 'desc': 'OpenCUA是一个开源框架，专为视觉-语言模型作为计算机使用代理而设计。它提供了一个注释基础设施、大规模数据集和可扩展的管道，能够实现最先进的性能。该框架包括捕捉人类计算机使用演示的注释工具，以及涵盖多个操作系统和应用程序的大规模计算机使用任务数据集。我们的研究表明，OpenCUA在多个基准测试中表现出色，推动了开放源代码模型的发展。'}}}, {'id': 'https://huggingface.co/papers/2508.09050', 'title': 'Bridging Theory and Practice in Quantum Game Theory: Optimized\n  Implementation of the Battle of the Sexes with Error Mitigation on NISQ\n  Hardware', 'url': 'https://huggingface.co/papers/2508.09050', 'abstract': "Quantum game theory demonstrated on IBM Quantum hardware using the Eisert-Wilkens-Lewenstein framework shows persistent quantum advantages in strategic coordination despite noise and decoherence.  \t\t\t\t\tAI-generated summary \t\t\t\t Implementing quantum game theory on real hardware is challenging due to noise, decoherence, and limited qubit connectivity, yet such demonstrations are essential to validate theoretical predictions. We present one of the first full experimental realizations of the Battle of the Sexes game under the Eisert-Wilkens-Lewenstein (EWL) framework on IBM Quantum's ibm sherbrooke superconducting processor. Four quantum strategies (I, H, R(pi/4), R(pi)) were evaluated across 31 entanglement values gamma in [0, pi] using 2048 shots per configuration, enabling a direct comparison between analytical predictions and hardware execution. To mitigate noise and variability, we introduce a Guided Circuit Mapping (GCM) method that dynamically selects qubit pairs and optimizes routing based on real-time topology and calibration data. The analytical model forecasts up to 108% payoff improvement over the classical equilibrium, and despite hardware-induced deviations, experimental results with GCM preserve the expected payoff trends within 3.5%-12% relative error. These findings show that quantum advantages in strategic coordination can persist under realistic NISQ conditions, providing a pathway toward practical applications of quantum game theory in multi-agent, economic, and distributed decision-making systems.", 'score': 2, 'issue_id': 5322, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '107a4a0916cff223', 'authors': ['Germán Díaz Agreda', 'Carlos Andres Duran Paredes', 'Mateo Buenaventura Samboni', 'Jhon Alejandro Andrade', 'Sebastián Andrés Cajas Ordoñez'], 'affiliations': ['Facultad de Ciencias Naturales, Exactas de la Educacion Universidad del Cauca Popayan, Colombia', 'National Irish Centre for AI (CeADAR) University College Dublin (UCD) Dublin, Ireland'], 'pdf_title_img': 'assets/pdf/title_img/2508.09050.jpg', 'data': {'categories': ['#agents', '#games', '#optimization', '#math'], 'emoji': '🎮', 'ru': {'title': 'Квантовая теория игр побеждает шум: эксперимент на IBM Quantum', 'desc': "В этой статье представлена одна из первых полных экспериментальных реализаций квантовой теории игр на реальном квантовом оборудовании IBM. Исследователи использовали структуру Эйзерта-Вилкенса-Левенштейна для игры 'Битва полов', оценивая четыре квантовые стратегии при различных уровнях запутанности. Для уменьшения влияния шума и декогеренции был разработан метод Guided Circuit Mapping, динамически оптимизирующий маршрутизацию кубитов. Несмотря на аппаратные отклонения, экспериментальные результаты сохранили ожидаемые тенденции выигрыша, демонстрируя устойчивость квантовых преимуществ в стратегической координации в условиях NISQ."}, 'en': {'title': 'Harnessing Quantum Strategies for Real-World Decision Making', 'desc': 'This paper explores the implementation of quantum game theory on IBM Quantum hardware, specifically using the Eisert-Wilkens-Lewenstein framework. It presents an experimental realization of the Battle of the Sexes game, evaluating various quantum strategies while addressing challenges like noise and decoherence. The authors introduce a Guided Circuit Mapping method to optimize qubit routing and mitigate errors, achieving results that align closely with theoretical predictions. The findings suggest that quantum advantages in strategic coordination can be maintained even in real-world conditions, paving the way for future applications in complex decision-making scenarios.'}, 'zh': {'title': '量子博弈理论的优势在现实条件下依然存在', 'desc': '这篇论文展示了在IBM量子硬件上实现量子博弈理论的实验，使用了Eisert-Wilkens-Lewenstein框架。尽管存在噪声和退相干的挑战，实验结果表明量子策略在协调决策中仍然具有优势。研究中采用了引导电路映射方法，以优化量子比特的连接和路由，从而减少噪声影响。结果显示，在实际的NISQ条件下，量子博弈理论的优势依然存在，为多智能体和经济决策系统的实际应用提供了可能的路径。'}}}, {'id': 'https://huggingface.co/papers/2508.08680', 'title': 'TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine\n  Translation', 'url': 'https://huggingface.co/papers/2508.08680', 'abstract': 'TopXGen uses LLMs to generate high-quality, topic-diverse target-side texts for LRLs, which can be backtranslated to improve translation performance in ICL and fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have been shown to perform well in machine translation (MT) with the use of in-context learning (ICL), rivaling supervised models when translating into high-resource languages (HRLs). However, they lag behind when translating into low-resource language (LRLs). Example selection via similarity search and supervised fine-tuning help. However the improvements they give are limited by the size, quality and diversity of existing parallel datasets. A common technique in low-resource MT is synthetic parallel data creation, the most frequent of which is backtranslation, whereby existing target-side texts are automatically translated into the source language. However, this assumes the existence of good quality and relevant target-side texts, which are not readily available for many LRLs. In this paper, we present TopXGen, an LLM-based approach for the generation of high quality and topic-diverse data in multiple LRLs, which can then be backtranslated to produce useful and diverse parallel texts for ICL and fine-tuning. Our intuition is that while LLMs struggle to translate into LRLs, their ability to translate well into HRLs and their multilinguality enable them to generate good quality, natural-sounding target-side texts, which can be translated well into a high-resource source language. We show that TopXGen boosts LLM translation performance during fine-tuning and in-context learning. Code and outputs are available at https://github.com/ArmelRandy/topxgen.', 'score': 2, 'issue_id': 5328, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': 'd017b9f9740fbd66', 'authors': ['Armel Zebaze', 'Benoît Sagot', 'Rachel Bawden'], 'affiliations': ['Inria, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2508.08680.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#training', '#low_resource', '#synthetic', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'TopXGen: Повышение качества перевода для малоресурсных языков с помощью LLM', 'desc': 'TopXGen - это подход, использующий большие языковые модели (LLM) для генерации качественных и тематически разнообразных текстов на малоресурсных языках. Эти тексты затем используются для обратного перевода, что улучшает качество машинного перевода при обучении в контексте и тонкой настройке моделей. Метод особенно эффективен для малоресурсных языков, где обычно не хватает качественных параллельных данных. TopXGen позволяет улучшить производительность LLM в задачах перевода как при тонкой настройке, так и при обучении в контексте.'}, 'en': {'title': 'Boosting Low-Resource Language Translation with TopXGen', 'desc': "TopXGen is a novel approach that leverages large language models (LLMs) to create high-quality and diverse target-side texts specifically for low-resource languages (LRLs). By generating synthetic parallel data, TopXGen enhances the backtranslation process, which is crucial for improving machine translation performance in both in-context learning (ICL) and fine-tuning scenarios. The method capitalizes on the LLMs' strengths in translating high-resource languages (HRLs) to produce natural-sounding texts that can be effectively translated back into the source language. Overall, TopXGen significantly boosts translation accuracy and diversity for LRLs, addressing the limitations posed by existing parallel datasets."}, 'zh': {'title': 'TopXGen：提升低资源语言翻译的创新方法', 'desc': 'TopXGen 是一种基于大型语言模型（LLM）的方法，旨在为低资源语言（LRLs）生成高质量和多样化的目标文本。这些生成的文本可以通过反向翻译来提高翻译性能，特别是在上下文学习（ICL）和微调过程中。尽管 LLM 在高资源语言（HRLs）中的翻译表现良好，但在低资源语言中仍然存在不足。TopXGen 利用 LLM 的多语言能力，生成自然流畅的目标文本，从而改善低资源语言的翻译效果。'}}}, {'id': 'https://huggingface.co/papers/2508.05813', 'title': 'Optimization-Free Style Transfer for 3D Gaussian Splats', 'url': 'https://huggingface.co/papers/2508.05813', 'abstract': 'A novel method for style transfer of 3D Gaussian splats involves generating a graph structure and using a feed-forward, surface-based stylization technique without reconstruction or optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t The task of style transfer for 3D Gaussian splats has been explored in many previous works, but these require reconstructing or fine-tuning the splat while incorporating style information or optimizing a feature extraction network on the splat representation. We propose a reconstruction- and optimization-free approach to stylizing 3D Gaussian splats. This is done by generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats in the scene. This allows for any style image and 3D Gaussian splat to be used without any additional training or optimization. This also allows for fast stylization of splats, achieving speeds under 2 minutes even on consumer-grade hardware. We demonstrate the quality results this approach achieves and compare to other 3D Gaussian splat style transfer methods. Code is publicly available at https://github.com/davidmhart/FastSplatStyler.', 'score': 2, 'issue_id': 5329, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'c7b7791bb6e3f436', 'authors': ['Raphael Du Sablon', 'David Hart'], 'affiliations': ['East Carolina University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05813.jpg', 'data': {'categories': ['#graphs', '#open_source', '#games', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Быстрая стилизация 3D гауссовых сплатов без реконструкции', 'desc': 'Статья представляет новый метод переноса стиля для 3D гауссовых сплатов. Авторы предлагают подход, не требующий реконструкции или оптимизации, основанный на создании графовой структуры на поверхности сплата. Затем применяется прямой метод стилизации поверхности, результаты которого интерполируются обратно на отдельные сплаты. Этот метод позволяет быстро стилизовать сплаты с использованием любого изображения стиля без дополнительного обучения.'}, 'en': {'title': 'Fast and Efficient Style Transfer for 3D Gaussian Splats', 'desc': 'This paper presents a new method for transferring styles to 3D Gaussian splats without the need for reconstruction or optimization. The approach involves creating a graph structure that represents the implicit surface of the splat, allowing for efficient stylization. A feed-forward, surface-based technique is then applied to interpolate the style back to the individual splats. This method enables quick stylization, achieving results in under 2 minutes on standard hardware, and supports any style image without requiring additional training.'}, 'zh': {'title': '快速高效的3D高斯点样式转移方法', 'desc': '本文提出了一种新的3D高斯点样式转移方法，采用图结构生成和前馈的基于表面的样式化技术，无需重建或优化。与以往需要重建或微调高斯点的传统方法不同，我们的方法可以直接在高斯点表示的隐式表面上生成图结构。通过这种方式，可以快速实现样式转移，且在普通硬件上也能在2分钟内完成。我们的实验结果表明，该方法在样式转移质量上优于其他3D高斯点样式转移方法。'}}}, {'id': 'https://huggingface.co/papers/2508.08855', 'title': 'BiasGym: Fantastic Biases and How to Find (and Remove) Them', 'url': 'https://huggingface.co/papers/2508.08855', 'abstract': "BiasGym is a framework for injecting, analyzing, and mitigating biases in Large Language Models through token-based fine-tuning and signal analysis.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding biases and stereotypes encoded in the weights of Large Language Models (LLMs) is crucial for developing effective mitigation strategies. Biased behaviour is often subtle and non-trivial to isolate, even when deliberately elicited, making systematic analysis and debiasing particularly challenging. To address this, we introduce BiasGym, a simple, cost-effective, and generalizable framework for reliably injecting, analyzing, and mitigating conceptual associations within LLMs. BiasGym consists of two components: BiasInject, which injects specific biases into the model via token-based fine-tuning while keeping the model frozen, and BiasScope, which leverages these injected signals to identify and steer the components responsible for biased behavior. Our method enables consistent bias elicitation for mechanistic analysis, supports targeted debiasing without degrading performance on downstream tasks, and generalizes to biases unseen during training. We demonstrate the effectiveness of BiasGym in reducing real-world stereotypes (e.g., people from a country being `reckless drivers') and in probing fictional associations (e.g., people from a country having `blue skin'), showing its utility for both safety interventions and interpretability research.", 'score': 1, 'issue_id': 5325, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '72ab3b2ec8780e48', 'authors': ['Sekh Mainul Islam', 'Nadav Borenstein', 'Siddhesh Milind Pawar', 'Haeun Yu', 'Arnav Arora', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2508.08855.jpg', 'data': {'categories': ['#data', '#interpretability', '#training', '#ethics'], 'emoji': '🧠', 'ru': {'title': 'Управление предвзятостями в LLM: инъекции, анализ и смягчение', 'desc': 'BiasGym - это фреймворк для внедрения, анализа и смягчения предвзятостей в больших языковых моделях (LLM) с помощью токен-ориентированной тонкой настройки и анализа сигналов. Он состоит из двух компонентов: BiasInject для внедрения конкретных предвзятостей в модель и BiasScope для выявления и корректировки компонентов, ответственных за предвзятое поведение. Фреймворк позволяет проводить целенаправленное устранение предвзятостей без ухудшения производительности на других задачах. BiasGym продемонстрировал эффективность в снижении реальных стереотипов и исследовании вымышленных ассоциаций.'}, 'en': {'title': 'BiasGym: A Framework for Understanding and Mitigating Bias in Language Models', 'desc': "BiasGym is a framework designed to address biases in Large Language Models (LLMs) by allowing researchers to inject, analyze, and mitigate these biases effectively. It features two main components: BiasInject, which introduces specific biases through token-based fine-tuning, and BiasScope, which analyzes the model to identify the sources of biased behavior. This approach enables systematic bias analysis and targeted debiasing without compromising the model's performance on other tasks. The framework has been shown to effectively reduce harmful stereotypes and explore fictional associations, making it valuable for both safety and interpretability in AI."}, 'zh': {'title': 'BiasGym：减轻大型语言模型中的偏见', 'desc': 'BiasGym是一个框架，用于在大型语言模型中注入、分析和减轻偏见。它通过基于标记的微调和信号分析，帮助识别和处理模型中的概念关联。BiasGym包含两个主要组件：BiasInject用于在保持模型不变的情况下注入特定偏见，BiasScope则利用这些信号识别和引导导致偏见行为的组件。该方法能够有效减少现实世界中的刻板印象，并支持对未见过的偏见进行针对性去偏见处理。'}}}, {'id': 'https://huggingface.co/papers/2508.08180', 'title': 'RedDino: A foundation model for red blood cell analysis', 'url': 'https://huggingface.co/papers/2508.08180', 'abstract': 'RedDino, a self-supervised foundation model using DINOv2, excels in RBC shape classification and generalization, addressing challenges in computational hematology.  \t\t\t\t\tAI-generated summary \t\t\t\t Red blood cells (RBCs) are essential to human health, and their precise morphological analysis is important for diagnosing hematological disorders. Despite the promise of foundation models in medical diagnostics, comprehensive AI solutions for RBC analysis remain scarce. We present RedDino, a self-supervised foundation model designed for RBC image analysis. RedDino uses an RBC-specific adaptation of the DINOv2 self-supervised learning framework and is trained on a curated dataset of 1.25 million RBC images from diverse acquisition modalities and sources. Extensive evaluations show that RedDino outperforms existing state-of-the-art models on RBC shape classification. Through assessments including linear probing and nearest neighbor classification, we confirm its strong feature representations and generalization ability. Our main contributions are: (1) a foundation model tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations for RBC modeling, and (3) a detailed evaluation of generalization performance. RedDino addresses key challenges in computational hematology by capturing nuanced morphological features, advancing the development of reliable diagnostic tools. The source code and pretrained models for RedDino are available at https://github.com/Snarci/RedDino, and the pretrained models can be downloaded from our Hugging Face collection at https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc', 'score': 1, 'issue_id': 5334, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'e837aa8643ad74c5', 'authors': ['Luca Zedda', 'Andrea Loddo', 'Cecilia Di Ruberto', 'Carsten Marr'], 'affiliations': ['Department of Mathematics and Computer Science, University of Cagliari, Cagliari, Italy', 'Institute of AI for Health, Helmholtz Munich, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2508.08180.jpg', 'data': {'categories': ['#dataset', '#open_source', '#data', '#training', '#healthcare', '#cv', '#science'], 'emoji': '🩸', 'ru': {'title': 'RedDino: ИИ-революция в анализе эритроцитов', 'desc': 'RedDino - это самообучающаяся фундаментальная модель для анализа изображений эритроцитов, основанная на архитектуре DINOv2. Модель обучена на наборе данных из 1,25 миллиона изображений эритроцитов и превосходит существующие модели в классификации форм эритроцитов. RedDino демонстрирует сильные способности к обобщению и захвату тонких морфологических особенностей. Эта модель решает ключевые проблемы в вычислительной гематологии и продвигает разработку надежных диагностических инструментов.'}, 'en': {'title': 'RedDino: Revolutionizing RBC Analysis with Self-Supervised Learning', 'desc': 'RedDino is a self-supervised foundation model specifically designed for analyzing red blood cell (RBC) images, utilizing the DINOv2 framework. It is trained on a large dataset of 1.25 million RBC images, which helps it learn to classify RBC shapes accurately. The model demonstrates superior performance compared to existing methods in RBC shape classification and shows strong generalization capabilities. By effectively capturing detailed morphological features, RedDino aims to enhance diagnostic tools in computational hematology.'}, 'zh': {'title': 'RedDino：红细胞分析的自监督基础模型', 'desc': 'RedDino是一种自监督基础模型，专门用于红细胞（RBC）形状分类和泛化，解决了计算血液学中的一些挑战。该模型基于DINOv2自监督学习框架，使用了125万张来自不同获取方式和来源的红细胞图像进行训练。通过线性探测和最近邻分类等评估，RedDino在红细胞形状分类上超越了现有的最先进模型，展现出强大的特征表示和泛化能力。RedDino的主要贡献包括为红细胞分析量身定制的基础模型、对DINOv2配置的消融研究，以及对泛化性能的详细评估。'}}}, {'id': 'https://huggingface.co/papers/2508.06813', 'title': 'Technical Report: Full-Stack Fine-Tuning for the Q Programming Language', 'url': 'https://huggingface.co/papers/2508.06813', 'abstract': 'A comprehensive approach is presented for adapting large language models to the Q programming language, achieving superior performance compared to existing models on a newly created benchmark dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Even though large language models are becoming increasingly capable, it is still unreasonable to expect them to excel at tasks that are under-represented on the Internet. Leveraging LLMs for specialized applications, particularly in niche programming languages and private domains, remains challenging and largely unsolved. In this work, we address this gap by presenting a comprehensive, open-source approach for adapting LLMs to the Q programming language, a popular tool in quantitative finance that is much less present on the Internet compared to Python, C, Java, and other ``mainstream" languages and is therefore not a strong suit of general-purpose AI models. We introduce a new Leetcode style evaluation dataset for Q, benchmark major frontier models on the dataset, then do pretraining, supervised fine tuning, and reinforcement learning to train a suite of reasoning and non-reasoning models based on the Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our best model achieves a pass@1 accuracy of 59 percent on our Q benchmark, surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent. Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task. In addition to releasing models, code, and data, we provide a detailed blueprint for dataset construction, model pretraining, supervised fine-tuning, and reinforcement learning. Our methodology is broadly applicable, and we discuss how these techniques can be extended to other tasks, including those where evaluation may rely on soft or subjective signals.', 'score': 1, 'issue_id': 5330, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 августа', 'en': 'August 9', 'zh': '8月9日'}, 'hash': 'efb0dc4da7a5f896', 'authors': ['Brendan R. Hogan', 'Will Brown', 'Adel Boyarsky', 'Anderson Schneider', 'Yuriy Nevmyvaka'], 'affiliations': ['Morgan Stanley, New York, NY', 'Prime Intellect, San Francisco, CA'], 'pdf_title_img': 'assets/pdf/title_img/2508.06813.jpg', 'data': {'categories': ['#reasoning', '#plp', '#dataset', '#open_source', '#training', '#benchmark', '#transfer_learning', '#rl'], 'emoji': '📊', 'ru': {'title': 'Адаптация LLM для нишевых языков программирования: прорыв в Q', 'desc': 'Представлен комплексный подход к адаптации больших языковых моделей (LLM) для языка программирования Q, используемого в количественных финансах. Авторы создали новый набор данных в стиле Leetcode для оценки моделей на задачах Q и провели бенчмаркинг ведущих моделей. Затем они выполнили предобучение, тонкую настройку и обучение с подкреплением серии моделей Qwen-2.5 разных размеров. Лучшая модель достигла точности 59% на новом бенчмарке Q, превзойдя лучшую базовую модель Claude Opus-4 на 29.5%.'}, 'en': {'title': 'Empowering Q Programming with Advanced Language Models', 'desc': 'This paper presents a novel method for adapting large language models (LLMs) specifically for the Q programming language, which is underrepresented in existing AI training data. The authors introduce a new benchmark dataset and evaluate various models, demonstrating that their approach significantly improves performance, with their best model achieving a 59% accuracy on the Q benchmark. They utilize techniques such as pretraining, supervised fine-tuning, and reinforcement learning to enhance model capabilities. The findings suggest that this comprehensive methodology can be applied to other specialized domains and tasks, making it a valuable contribution to the field of machine learning.'}, 'zh': {'title': '为Q语言量身定制的大型语言模型', 'desc': '本文提出了一种全面的方法，将大型语言模型适应于Q编程语言，取得了优于现有模型的表现。Q语言在互联网的存在感较低，因此通用AI模型在此领域的表现不佳。我们创建了一个新的Leetcode风格的评估数据集，并在此基础上对主要前沿模型进行了基准测试。通过预训练、监督微调和强化学习，我们训练了一系列基于Qwen-2.5系列的推理和非推理模型，最终模型在Q基准测试中取得了59%的准确率，超越了其他模型。'}}}, {'id': 'https://huggingface.co/papers/2508.06485', 'title': 'WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface\n  Temperature Estimation via Spatio-Temporal Fusion', 'url': 'https://huggingface.co/papers/2508.06485', 'abstract': 'WGAST, a weakly-supervised generative network, enhances daily 10 m land surface temperature estimation using spatio-temporal fusion of Terra MODIS, Landsat 8, and Sentinel-2 data.  \t\t\t\t\tAI-generated summary \t\t\t\t Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.', 'score': 1, 'issue_id': 5324, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '8c3d43dec153555b', 'authors': ['Sofiane Bouaziz', 'Adel Hafiane', 'Raphael Canals', 'Rachid Nedjai'], 'affiliations': ["INSA Centre Val de Loire, Universite d'Orleans, PRISME UR 4229, Bourges, 18022, Centre Val de Loire, France", "Universite d'Orleans, CEDETE, UR 1210, Orleans, 45067, Centre Val de Loire, France", "Universite d'Orleans, INSA CVL, PRISME UR 4229, Orleans, 45067, Centre Val de Loire, France"], 'pdf_title_img': 'assets/pdf/title_img/2508.06485.jpg', 'data': {'categories': ['#dataset', '#data', '#synthetic', '#optimization', '#cv', '#training', '#science'], 'emoji': '🛰️', 'ru': {'title': 'Прорыв в точности оценки температуры поверхности Земли', 'desc': 'WGAST - это генеративная нейронная сеть со слабым контролем для оценки температуры поверхности земли с разрешением 10 м. Она использует пространственно-временное слияние данных со спутников Terra MODIS, Landsat 8 и Sentinel-2. Сеть имеет условную генеративно-состязательную архитектуру с генератором из четырех этапов: извлечение признаков, слияние, реконструкция температуры и подавление шума. WGAST превосходит существующие методы как в количественных, так и в качественных оценках.'}, 'en': {'title': 'Revolutionizing Land Surface Temperature Estimation with WGAST', 'desc': 'The paper introduces WGAST, a weakly-supervised generative network designed to estimate daily land surface temperature (LST) at a high resolution of 10 meters. It utilizes spatio-temporal fusion of data from Terra MODIS, Landsat 8, and Sentinel-2 satellites to improve LST accuracy. WGAST employs a conditional generative adversarial architecture with multiple stages for feature extraction, fusion, reconstruction, and noise suppression. Experimental results show that WGAST significantly outperforms existing methods, reducing RMSE by 17.18% and improving SSIM by 11.00%, while effectively handling cloud interference and capturing fine-scale thermal patterns.'}, 'zh': {'title': 'WGAST：提升每日10米土地表面温度估计的创新网络', 'desc': 'WGAST是一种弱监督生成网络，旨在通过时空融合Terra MODIS、Landsat 8和Sentinel-2数据来提高每日10米土地表面温度（LST）的估计精度。该网络采用条件生成对抗架构，分为特征提取、融合、LST重建和噪声抑制四个阶段。通过多级潜在表示的提取和融合，WGAST能够生成高分辨率的LST，并有效抑制高频噪声。实验结果表明，WGAST在定量和定性评估中均优于现有方法，显著提高了LST估计的准确性。'}}}, {'id': 'https://huggingface.co/papers/2508.04676', 'title': 'GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via\n  General Samples Replay', 'url': 'https://huggingface.co/papers/2508.04676', 'abstract': 'General Sample Replay (GeRe) framework with threshold-based margin (TM) loss addresses catastrophic forgetting in large language models by maintaining activation state consistency during replay learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns--retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1/L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at https://github.com/Qznan/GeRe.', 'score': 1, 'issue_id': 5324, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '829ed041a738743b', 'authors': ['Yunan Zhang', 'Shuoran Jiang', 'Mengchen Zhao', 'Yuefeng Li', 'Yang Fan', 'Xiangping Wu', 'Qingcai Chen'], 'affiliations': ['Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China', 'Ysstech Info-Tech Co.,Ltd, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2508.04676.jpg', 'data': {'categories': ['#agi', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'GeRe: эффективное решение проблемы забывчивости в больших языковых моделях', 'desc': 'Статья представляет фреймворк General Sample Replay (GeRe) для решения проблемы катастрофической забывчивости в больших языковых моделях (LLM). GeRe использует обычные предобученные тексты для эффективного предотвращения забывания. Авторы вводят метод оптимизации с ограничением состояний активации, используя пороговую функцию потерь с отступом (TM). Эксперименты показывают, что небольшого фиксированного набора предварительно собранных общих образцов для повторения достаточно для сохранения общих возможностей и улучшения производительности в последовательных задачах.'}, 'en': {'title': 'Combatting Forgetting in Language Models with GeRe Framework', 'desc': 'The paper introduces the General Sample Replay (GeRe) framework, which aims to combat catastrophic forgetting in large language models (LLMs) during continual learning. It highlights the issues of losing general capabilities and declining performance on previously learned tasks when fine-tuning LLMs across different domains. The authors propose a threshold-based margin (TM) loss that maintains activation state consistency, allowing for effective replay learning using pre-collected general samples. Experimental results show that the TM loss enhances performance and robustness compared to traditional replay strategies, making it a promising approach for future LLM training.'}, 'zh': {'title': '通用样本重放：解决灾难性遗忘的有效方法', 'desc': '本文提出了一种名为通用样本重放（GeRe）的框架，旨在解决大型语言模型（LLMs）在持续学习中面临的灾难性遗忘问题。该框架通过使用阈值边际（TM）损失，保持激活状态的一致性，从而在重放学习过程中有效地防止遗忘。研究表明，使用少量预先收集的通用重放样本可以同时保留模型的通用能力，并提高在连续任务中的整体表现。通过对比实验，TM方法在不同的重放策略下表现出更好的性能和鲁棒性，推动了LLMs的高效重放学习。'}}}, {'id': 'https://huggingface.co/papers/2508.08974', 'title': 'Text-conditioned State Space Model For Domain-generalized Change\n  Detection Visual Question Answering', 'url': 'https://huggingface.co/papers/2508.08974', 'abstract': "A novel state space model, TCSSM, is introduced to address domain shift in Change Detection Visual Question Answering (CDVQA) by leveraging bi-temporal imagery and textual information.  \t\t\t\t\tAI-generated summary \t\t\t\t The Earth's surface is constantly changing, and detecting these changes provides valuable insights that benefit various aspects of human society. While traditional change detection methods have been employed to detect changes from bi-temporal images, these approaches typically require expert knowledge for accurate interpretation. To enable broader and more flexible access to change information by non-expert users, the task of Change Detection Visual Question Answering (CDVQA) has been introduced. However, existing CDVQA methods have been developed under the assumption that training and testing datasets share similar distributions. This assumption does not hold in real-world applications, where domain shifts often occur. In this paper, the CDVQA task is revisited with a focus on addressing domain shift. To this end, a new multi-modal and multi-domain dataset, BrightVQA, is introduced to facilitate domain generalization research in CDVQA. Furthermore, a novel state space model, termed Text-Conditioned State Space Model (TCSSM), is proposed. The TCSSM framework is designed to leverage both bi-temporal imagery and geo-disaster-related textual information in an unified manner to extract domain-invariant features across domains. Input-dependent parameters existing in TCSSM are dynamically predicted by using both bi-temporal images and geo-disaster-related description, thereby facilitating the alignment between bi-temporal visual data and the associated textual descriptions. Extensive experiments are conducted to evaluate the proposed method against state-of-the-art models, and superior performance is consistently demonstrated. The code and dataset will be made publicly available upon acceptance at https://github.com/Elman295/TCSSM.", 'score': 0, 'issue_id': 5335, 'pub_date': '2025-08-12', 'pub_date_card': {'ru': '12 августа', 'en': 'August 12', 'zh': '8月12日'}, 'hash': '64a95ba78ad17af5', 'authors': ['Elman Ghazaei', 'Erchan Aptoula'], 'affiliations': ['Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, Türkiye'], 'pdf_title_img': 'assets/pdf/title_img/2508.08974.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#open_source', '#transfer_learning', '#alignment', '#cv'], 'emoji': '🌍', 'ru': {'title': 'TCSSM: Инвариантное к домену обнаружение изменений с помощью ИИ', 'desc': 'Представлена новая модель пространства состояний TCSSM для решения проблемы смещения домена в задаче обнаружения изменений с помощью визуальных вопросов и ответов (CDVQA). Модель использует битемпоральные изображения и текстовую информацию для извлечения инвариантных к домену признаков. Введен новый мультимодальный и мультидоменный датасет BrightVQA для исследования обобщения по доменам в CDVQA. Эксперименты показывают превосходную производительность предложенного метода по сравнению с современными моделями.'}, 'en': {'title': 'Bridging Domain Gaps in Change Detection with TCSSM', 'desc': 'This paper presents a new approach called the Text-Conditioned State Space Model (TCSSM) to improve Change Detection Visual Question Answering (CDVQA) by addressing the issue of domain shift. The authors introduce a multi-modal dataset named BrightVQA, which helps in training models that can generalize better across different domains. TCSSM utilizes both bi-temporal images and related textual information to extract features that remain consistent despite changes in the data distribution. Through extensive experiments, the proposed model shows better performance compared to existing methods, making it a significant advancement in the field.'}, 'zh': {'title': '应对领域转移的创新模型TCSSM', 'desc': '本文提出了一种新颖的状态空间模型，称为TCSSM，旨在解决变化检测视觉问答（CDVQA）中的领域转移问题。该模型利用双时相图像和与地理灾害相关的文本信息，提取跨领域的不变特征。通过动态预测输入依赖的参数，TCSSM能够有效对齐双时相视觉数据和相关文本描述。实验结果表明，TCSSM在多个基准测试中表现优于现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2508.05769', 'title': 'Improving Masked Style Transfer using Blended Partial Convolution', 'url': 'https://huggingface.co/papers/2508.05769', 'abstract': 'A partial-convolution-based style transfer network improves stylization of specific image regions by accurately applying style features and using network-internal blending techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Artistic style transfer has long been possible with the advancements of convolution- and transformer-based neural networks. Most algorithms apply the artistic style transfer to the whole image, but individual users may only need to apply a style transfer to a specific region in the image. The standard practice is to simply mask the image after the stylization. This work shows that this approach tends to improperly capture the style features in the region of interest. We propose a partial-convolution-based style transfer network that accurately applies the style features exclusively to the region of interest. Additionally, we present network-internal blending techniques that account for imperfections in the region selection. We show that this visually and quantitatively improves stylization using examples from the SA-1B dataset. Code is publicly available at https://github.com/davidmhart/StyleTransferMasked.', 'score': 0, 'issue_id': 5329, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '6def4798600a928b', 'authors': ['Seyed Hadi Seyed', 'Ayberk Cansever', 'David Hart'], 'affiliations': ['East Carolina University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05769.jpg', 'data': {'categories': ['#optimization', '#dataset', '#cv', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'Точный перенос стиля на выбранные области изображения', 'desc': 'Статья представляет новый подход к переносу стиля на определенные области изображения с использованием частичных свёрточных нейронных сетей. Авторы предлагают метод, который точно применяет особенности стиля только к выбранной области интереса. Также описываются техники внутрисетевого смешивания для учета несовершенств в выборе региона. Экспериментальные результаты на наборе данных SA-1B показывают визуальное и количественное улучшение стилизации по сравнению с традиционными методами.'}, 'en': {'title': 'Targeted Style Transfer with Partial Convolutions', 'desc': 'This paper introduces a novel style transfer network that utilizes partial convolution to enhance the application of artistic styles to specific regions of an image. Unlike traditional methods that apply style transfer to the entire image and then mask the undesired areas, this approach focuses on accurately capturing style features only in the selected region. The authors also implement internal blending techniques within the network to address any imperfections that arise from the region selection process. The results demonstrate significant improvements in both visual quality and quantitative metrics, validated through experiments on the SA-1B dataset.'}, 'zh': {'title': '精准风格迁移，专注特定区域', 'desc': '这篇论文提出了一种基于部分卷积的风格迁移网络，旨在改善特定图像区域的风格化效果。传统的风格迁移算法通常对整个图像进行处理，但用户可能只希望对图像的某个特定区域应用风格迁移。研究表明，简单地在风格化后对图像进行遮罩会导致风格特征捕捉不准确。通过引入网络内部的混合技术，本文的方法能够更好地处理区域选择中的不完美，从而在视觉和定量上提升风格化效果。'}}}, {'id': 'https://huggingface.co/papers/2508.04195', 'title': 'NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech\n  Modeling with Paralinguistic Vocalizations', 'url': 'https://huggingface.co/papers/2508.04195', 'abstract': 'NVSpeech is a pipeline that integrates the recognition and synthesis of paralinguistic vocalizations in Mandarin, using a large annotated dataset and models that treat these cues as decodable tokens.  \t\t\t\t\tAI-generated summary \t\t\t\t Paralinguistic vocalizations-including non-verbal sounds like laughter and breathing, as well as lexicalized interjections such as "uhm" and "oh"-are integral to natural spoken communication. Despite their importance in conveying affect, intent, and interactional cues, such cues remain largely overlooked in conventional automatic speech recognition (ASR) and text-to-speech (TTS) systems. We present NVSpeech, an integrated and scalable pipeline that bridges the recognition and synthesis of paralinguistic vocalizations, encompassing dataset construction, ASR modeling, and controllable TTS. (1) We introduce a manually annotated dataset of 48,430 human-spoken utterances with 18 word-level paralinguistic categories. (2) We develop the paralinguistic-aware ASR model, which treats paralinguistic cues as inline decodable tokens (e.g., "You\'re so funny [Laughter]"), enabling joint lexical and non-verbal transcription. This model is then used to automatically annotate a large corpus, the first large-scale Chinese dataset of 174,179 utterances (573 hours) with word-level alignment and paralingustic cues. (3) We finetune zero-shot TTS models on both human- and auto-labeled data to enable explicit control over paralinguistic vocalizations, allowing context-aware insertion at arbitrary token positions for human-like speech synthesis. By unifying the recognition and generation of paralinguistic vocalizations, NVSpeech offers the first open, large-scale, word-level annotated pipeline for expressive speech modeling in Mandarin, integrating recognition and synthesis in a scalable and controllable manner. Dataset and audio demos are available at https://nvspeech170k.github.io/.', 'score': 0, 'issue_id': 5322, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': 'b4eb7f9d017ec3fd', 'authors': ['Huan Liao', 'Qinke Ni', 'Yuancheng Wang', 'Yiheng Lu', 'Haoyue Zhan', 'Pengyuan Xie', 'Qiang Zhang', 'Zhizheng Wu'], 'affiliations': ['Guangzhou Quwan Network Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2508.04195.jpg', 'data': {'categories': ['#dataset', '#open_source', '#data', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Единая система распознавания и синтеза паралингвистических сигналов в речи', 'desc': 'NVSpeech - это комплексный конвейер для распознавания и синтеза паралингвистических вокализаций в мандаринском китайском языке. Он включает в себя создание большого аннотированного датасета с 18 категориями паралингвистических сигналов на уровне слов. Система использует модели автоматического распознавания речи (ASR) и синтеза речи (TTS), которые обрабатывают паралингвистические сигналы как декодируемые токены. NVSpeech позволяет осуществлять контролируемый синтез экспрессивной речи с вставкой паралингвистических элементов в произвольных позициях.'}, 'en': {'title': 'Bridging Speech Recognition and Synthesis with Paralinguistic Cues', 'desc': 'NVSpeech is a novel pipeline designed to enhance the recognition and synthesis of paralinguistic vocalizations in Mandarin, which include non-verbal sounds and interjections. It utilizes a large, manually annotated dataset of over 48,000 utterances categorized into 18 paralinguistic types, enabling a more nuanced understanding of spoken communication. The system features a paralinguistic-aware automatic speech recognition (ASR) model that treats these vocal cues as decodable tokens, allowing for joint transcription of verbal and non-verbal elements. Additionally, it incorporates a controllable text-to-speech (TTS) model that can insert these vocalizations contextually, resulting in more expressive and human-like speech synthesis.'}, 'zh': {'title': 'NVSpeech：普通话副语言声音的识别与合成新突破', 'desc': 'NVSpeech是一个集成了普通话中副语言声音识别和合成的管道，使用了一个大型标注数据集和将这些线索视为可解码标记的模型。副语言声音包括非语言声音（如笑声和呼吸）以及词汇化的插入语（如“嗯”和“哦”），在自然口语交流中至关重要。该系统通过构建一个包含48,430个人工发音的标注数据集，开发了一个副语言感知的自动语音识别（ASR）模型，并在此基础上生成了一个大规模的中文数据集。NVSpeech实现了副语言声音的识别与合成的统一，为普通话的表达性语音建模提供了首个开放的大规模标注管道。'}}}, {'id': 'https://huggingface.co/papers/2508.07050', 'title': 'ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability', 'url': 'https://huggingface.co/papers/2508.07050', 'abstract': 'A reasoning-intensive reranker, ReasonRank, achieves state-of-the-art performance in passage ranking tasks by using synthesized training data and a two-stage post-training approach with reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker ReasonRank outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\\footnote{https://brightbenchmark.github.io/.} Our codes are available at https://github.com/8421BCD/ReasonRank.', 'score': 88, 'issue_id': 5296, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 августа', 'en': 'August 9', 'zh': '8月9日'}, 'hash': 'bfd8a67bb2b2dbf4', 'authors': ['Wenhan Liu', 'Xinyu Ma', 'Weiwei Sun', 'Yutao Zhu', 'Yuchen Li', 'Dawei Yin', 'Zhicheng Dou'], 'affiliations': ['Baidu Inc.', 'Carnegie Mellon University', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.07050.jpg', 'data': {'categories': ['#training', '#dataset', '#optimization', '#reasoning', '#rl'], 'emoji': '🧠', 'ru': {'title': 'ReasonRank: Рассуждающий ранжировщик нового поколения', 'desc': 'ReasonRank - это новый ранжировщик пассажей, использующий синтезированные данные для обучения и двухэтапный подход с обучением с подкреплением. Он применяет пошаговое рассуждение во время тестирования для улучшения производительности ранжирования списков. Модель использует автоматизированную систему синтеза обучающих данных с рассуждениями и механизм фильтрации для обеспечения качества данных. ReasonRank достигает наилучших результатов в задачах ранжирования пассажей, превосходя существующие базовые модели.'}, 'en': {'title': 'ReasonRank: Elevating Passage Ranking with Advanced Reasoning Techniques', 'desc': 'The paper introduces ReasonRank, a novel reranker that excels in passage ranking tasks by leveraging synthesized training data and a two-stage post-training method involving reinforcement learning. It addresses the challenge of limited reasoning-intensive training data by creating a framework that generates high-quality training labels from diverse domains. The two-stage approach consists of a supervised fine-tuning phase to learn reasoning patterns, followed by a reinforcement learning phase that enhances ranking capabilities using a multi-view ranking reward. Experimental results show that ReasonRank achieves state-of-the-art performance while maintaining lower latency compared to traditional pointwise rerankers.'}, 'zh': {'title': '推理强化重排序，提升段落排名性能！', 'desc': '本文提出了一种名为ReasonRank的推理强化重排序模型，旨在提高段落排名任务的性能。通过合成训练数据和两阶段的后训练方法，ReasonRank在复杂的排名场景中表现出色。我们设计了一种自动化的推理训练数据合成框架，并引入了自一致性数据过滤机制，以确保数据质量。最终，ReasonRank在BRIGHT排行榜上达到了40.6的最新性能，显著优于现有基线模型。'}}}, {'id': 'https://huggingface.co/papers/2508.07999', 'title': 'WideSearch: Benchmarking Agentic Broad Info-Seeking', 'url': 'https://huggingface.co/papers/2508.07999', 'abstract': 'WideSearch is a new benchmark evaluating the reliability of automated search agents in large-scale information collection tasks, revealing significant deficiencies in current systems.  \t\t\t\t\tAI-generated summary \t\t\t\t From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such "wide-context" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\\%, with the best performer reaching just 5\\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/', 'score': 81, 'issue_id': 5295, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '45f13eb4f1110e39', 'authors': ['Ryan Wong', 'Jiawei Wang', 'Junjie Zhao', 'Li Chen', 'Yan Gao', 'Long Zhang', 'Xuan Zhou', 'Zuo Wang', 'Kai Xiang', 'Ge Zhang', 'Wenhao Huang', 'Yang Wang', 'Ke Wang'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2508.07999.jpg', 'data': {'categories': ['#science', '#benchmark', '#agents', '#dataset', '#survey'], 'emoji': '🔍', 'ru': {'title': 'Поисковые агенты на основе ИИ пока не справляются с масштабными задачами', 'desc': 'WideSearch - это новый эталонный тест для оценки надежности автоматизированных поисковых агентов в задачах сбора информации большого масштаба. Бенчмарк включает 200 вручную отобранных вопросов из более чем 15 различных областей на английском и китайском языках. Тестирование более 10 современных систем агентного поиска показало, что большинство из них достигают общего уровня успешности около 0%, при этом лучший результат составляет всего 5%. Эти результаты демонстрируют критические недостатки современных поисковых агентов в задачах широкомасштабного поиска информации.'}, 'en': {'title': 'WideSearch: Evaluating the Reliability of Automated Search Agents', 'desc': 'WideSearch is a benchmark designed to assess the reliability of automated search agents in large-scale information collection tasks. It highlights significant shortcomings in current systems, which struggle to perform effectively in wide-context searches. The benchmark includes 200 curated questions across various domains, requiring agents to gather and organize information that can be objectively verified. Results show that most tested systems have low success rates, indicating a need for improvement in agentic search capabilities.'}, 'zh': {'title': '提升自动搜索代理的可靠性', 'desc': 'WideSearch是一个新的基准，用于评估自动搜索代理在大规模信息收集任务中的可靠性。当前的系统在执行这些任务时存在显著缺陷，成功率普遍接近0%。该基准包含200个手动策划的问题，涵盖15个不同领域，旨在测试代理收集和组织信息的能力。研究结果表明，现有的搜索代理在大规模信息获取方面亟需改进，未来的研究和开发方向应集中在提升其性能上。'}}}, {'id': 'https://huggingface.co/papers/2508.07981', 'title': 'Omni-Effects: Unified and Spatially-Controllable Visual Effects\n  Generation', 'url': 'https://huggingface.co/papers/2508.07981', 'abstract': 'Omni-Effects is a unified framework that enables the generation of prompt-guided and spatially controllable composite visual effects using LoRA-based Mixture of Experts and Spatial-Aware Prompt with Independent-Information Flow.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.', 'score': 46, 'issue_id': 5298, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'f440e34cea9bf27f', 'authors': ['Fangyuan Mao', 'Aiming Hao', 'Jintao Chen', 'Dongxia Liu', 'Xiaokun Feng', 'Jiashu Zhu', 'Meiqi Wu', 'Chubin Chen', 'Jiahong Wu', 'Xiangxiang Chu'], 'affiliations': ['AMAP, Alibaba Group', 'CASIA', 'PKU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2508.07981.jpg', 'data': {'categories': ['#cv', '#video', '#data', '#optimization', '#games', '#dataset', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'Омни-эффекты: Единая система для создания пространственно-контролируемых визуальных эффектов', 'desc': 'Omni-Effects - это унифицированная система для генерации визуальных эффектов с пространственным контролем, использующая LoRA-based Mixture of Experts и Spatial-Aware Prompt. Она решает проблему интерференции между эффектами и пространственной неконтролируемости при обучении нескольким визуальным эффектам одновременно. Система включает два ключевых нововведения: LoRA-MoE для интеграции разнообразных эффектов в единую модель и SAP для точного пространственного контроля. Авторы также создали датасет Omni-VFX и систему оценки для валидации производительности модели.'}, 'en': {'title': 'Unified Control for Diverse Visual Effects Generation', 'desc': 'Omni-Effects is a new framework designed to create complex visual effects in videos by combining multiple effects in specific locations. It uses a technique called LoRA-based Mixture of Experts to manage different effects without them interfering with each other. Additionally, it incorporates a Spatial-Aware Prompt that allows users to control where each effect appears in the video. This framework also includes an Independent-Information Flow module to ensure that the effects remain distinct and do not blend together unintentionally.'}, 'zh': {'title': '统一生成空间可控视觉效果的框架', 'desc': 'Omni-Effects是一个统一框架，能够生成基于提示的和空间可控的复合视觉效果。该框架采用了基于LoRA的专家混合模型和空间感知提示，解决了多种视觉效果生成中的干扰和空间不可控性问题。通过引入独立信息流模块，Omni-Effects能够有效隔离不同效果的控制信号，避免不必要的混合。实验结果表明，该框架能够实现精确的空间控制和多样化的效果生成，用户可以指定所需效果的类别和位置。'}}}, {'id': 'https://huggingface.co/papers/2508.07407', 'title': 'A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm\n  Bridging Foundation Models and Lifelong Agentic Systems', 'url': 'https://huggingface.co/papers/2508.07407', 'abstract': 'A survey of self-evolving AI agents that adapt to dynamic environments through automatic enhancement based on interaction data and feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.', 'score': 33, 'issue_id': 5302, 'pub_date': '2025-08-10', 'pub_date_card': {'ru': '10 августа', 'en': 'August 10', 'zh': '8月10日'}, 'hash': 'e94cfc939dfba252', 'authors': ['Jinyuan Fang', 'Yanwen Peng', 'Xi Zhang', 'Yingxu Wang', 'Xinhao Yi', 'Guibin Zhang', 'Yi Xu', 'Bin Wu', 'Siwei Liu', 'Zihao Li', 'Zhaochun Ren', 'Nikos Aletras', 'Xi Wang', 'Han Zhou', 'Zaiqiao Meng'], 'affiliations': ['Leiden University', 'Mohamed bin Zayed University of Artificial Intelligence', 'National University of Singapore', 'University College London', 'University of Aberdeen', 'University of Cambridge', 'University of Glasgow', 'University of Sheffield'], 'pdf_title_img': 'assets/pdf/title_img/2508.07407.jpg', 'data': {'categories': ['#ethics', '#optimization', '#agents', '#survey'], 'emoji': '🤖', 'ru': {'title': 'Самоэволюционирующие ИИ-агенты: путь к адаптивному и непрерывному обучению', 'desc': 'Статья представляет обзор методов создания самоэволюционирующих ИИ-агентов, способных адаптироваться к динамическим средам. Авторы предлагают концептуальную структуру, описывающую цикл обратной связи в таких системах, включающую входные данные, агентскую систему, среду и оптимизаторы. Рассматриваются различные техники эволюции агентов, нацеленные на разные компоненты системы, а также стратегии для специфических доменов. Особое внимание уделяется вопросам оценки, безопасности и этики самоэволюционирующих агентских систем.'}, 'en': {'title': 'Empowering AI Agents to Evolve and Adapt!', 'desc': 'This paper surveys self-evolving AI agents that can adapt to changing environments by automatically improving themselves using interaction data and feedback. It highlights the limitations of traditional static agent systems and introduces a conceptual framework that outlines the essential components of self-evolving agents: System Inputs, Agent System, Environment, and Optimisers. The authors review various techniques for enhancing agent systems and explore domain-specific strategies in fields like biomedicine and finance. Additionally, the paper discusses important considerations regarding the evaluation, safety, and ethics of these adaptive systems, aiming to guide future research and development in this area.'}, 'zh': {'title': '自我进化的AI代理：适应动态环境的未来', 'desc': '这篇论文调查了自我进化的人工智能代理，这些代理能够通过交互数据和反馈在动态环境中自动增强。现有的代理系统通常依赖于手动配置，缺乏适应性，因此研究者们探索了基于反馈的自动增强技术。论文提供了一个统一的概念框架，帮助理解自我进化代理系统的设计，并系统回顾了多种自我进化技术。最后，论文还讨论了评估、安全性和伦理等重要问题，以确保这些系统的有效性和可靠性。'}}}, {'id': 'https://huggingface.co/papers/2508.07629', 'title': 'Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving\n  Clipping Policy Optimization', 'url': 'https://huggingface.co/papers/2508.07629', 'abstract': "Klear-Reasoner, a model with long reasoning capabilities, achieves high performance across benchmarks through detailed post-training workflows, including long Chain-of-Thought supervised fine-tuning and reinforcement learning with Gradient-Preserving clipping Policy Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Klear-Reasoner, a model with long reasoning capabilities that demonstrates careful deliberation during problem solving, achieving outstanding performance across multiple benchmarks. Although there are already many excellent works related to inference models in the current community, there are still many problems with reproducing high-performance inference models due to incomplete disclosure of training details. This report provides an in-depth analysis of the reasoning model, covering the entire post-training workflow from data preparation and long Chain-of-Thought supervised fine-tuning (long CoT SFT) to reinforcement learning (RL), along with detailed ablation studies for each experimental component. For SFT data, our experiments show that a small number of high-quality data sources are more effective than a large number of diverse data sources, and that difficult samples can achieve better results without accuracy filtering. In addition, we investigate two key issues with current clipping mechanisms in RL: Clipping suppresses critical exploration signals and ignores suboptimal trajectories. To address these challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO) that gently backpropagates gradients from clipped tokens. GPPO not only enhances the model's exploration capacity but also improves its efficiency in learning from negative samples. Klear-Reasoner exhibits exceptional reasoning abilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\% on AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6.", 'score': 27, 'issue_id': 5295, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'a361b2d0bb1f93c2', 'authors': ['Zhenpeng Su', 'Leiyu Pan', 'Xue Bai', 'Dening Liu', 'Guanting Dong', 'Jiaming Huang', 'Wenping Hu', 'Guorui Zhou'], 'affiliations': ['Klear Team, Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.07629.jpg', 'data': {'categories': ['#training', '#long_context', '#optimization', '#math', '#plp', '#rl', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Klear-Reasoner: Мощный ИИ для длительных рассуждений', 'desc': 'Klear-Reasoner - это модель с длительными способностями к рассуждению, достигающая высокой производительности в различных тестах. Модель использует детальные пост-тренировочные процессы, включая длинную цепочку рассуждений при обучении с учителем и обучение с подкреплением с использованием оптимизации политики с сохранением градиента. Исследователи обнаружили, что небольшое количество высококачественных источников данных более эффективно, чем большое количество разнообразных источников. Klear-Reasoner демонстрирует исключительные способности к рассуждению в математике и программировании, достигая высоких результатов в тестах AIME и LiveCodeBench.'}, 'en': {'title': 'Klear-Reasoner: Mastering Long Reasoning with Enhanced Learning Techniques', 'desc': 'Klear-Reasoner is a machine learning model designed for long reasoning tasks, showcasing its ability to solve complex problems effectively. It utilizes a comprehensive post-training workflow that includes long Chain-of-Thought supervised fine-tuning and a novel reinforcement learning approach called Gradient-Preserving clipping Policy Optimization. The model demonstrates that using a few high-quality data sources can be more beneficial than many diverse ones, and it addresses issues in current reinforcement learning methods that hinder exploration. Klear-Reasoner achieves impressive performance on various benchmarks, particularly in mathematics and programming tasks.'}, 'zh': {'title': 'Klear-Reasoner：长推理能力的突破', 'desc': 'Klear-Reasoner是一种具有长推理能力的模型，能够在解决问题时进行细致的思考，表现出色。该模型通过详细的后训练工作流程，包括长链思维的监督微调和梯度保留剪切策略优化的强化学习，达到了高性能。研究表明，少量高质量的数据源比大量多样化的数据源更有效，且困难样本在没有准确性过滤的情况下也能取得更好的结果。此外，Klear-Reasoner在数学和编程方面展现了卓越的推理能力，取得了多个基准测试的高分。'}}}, {'id': 'https://huggingface.co/papers/2508.06600', 'title': 'BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of\n  Deep-Research Agent', 'url': 'https://huggingface.co/papers/2508.06600', 'abstract': 'BrowseComp-Plus, a curated benchmark, enables controlled evaluation of deep research agents and retrieval methods, providing insights into their performance and effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep-Research agents, which integrate large language models (LLMs) with search tools, have shown success in improving the effectiveness of handling complex queries that require iterative search planning and reasoning over search results. Evaluations on current benchmarks like BrowseComp relies on black-box live web search APIs, have notable limitations in (1) fairness: dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep research methods; (2) transparency: lack of control over the document corpus makes it difficult to isolate retriever contributions. In other words, the current evaluations may compare a complete deep research system at a given time, but they do not foster well-controlled experiments to provide insights into the capability of underlying deep research LLMs. To address these challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp, employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation. The benchmark is shown to be effective in distinguishing the performance of deep research systems. For instance, the open-source model Search-R1, when paired with the BM25 retriever, achieves 3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with fewer search calls. This benchmark allows comprehensive evaluation and disentangled analysis of deep research agents and retrieval methods, fostering insights into retrieval effectiveness, citation accuracy, and context engineering in Deep-Research system.', 'score': 27, 'issue_id': 5295, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'bbe851198d9c0416', 'authors': ['Zijian Chen', 'Xueguang Ma', 'Shengyao Zhuang', 'Ping Nie', 'Kai Zou', 'Andrew Liu', 'Joshua Green', 'Kshama Patel', 'Ruoxi Meng', 'Mingyi Su', 'Sahel Sharifymoghaddam', 'Yanxi Li', 'Haoran Hong', 'Xinyu Shi', 'Xuye Liu', 'Nandan Thakur', 'Crystina Zhang', 'Luyu Gao', 'Wenhu Chen', 'Jimmy Lin'], 'affiliations': ['CSIRO', 'Carnegie Mellon University', 'Independent', 'The University of Queensland', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2508.06600.jpg', 'data': {'categories': ['#interpretability', '#rag', '#ethics', '#benchmark', '#agents', '#open_source'], 'emoji': '🔬', 'ru': {'title': 'BrowseComp-Plus: Прозрачная оценка агентов глубокого исследования', 'desc': 'BrowseComp-Plus - это новый эталонный тест для оценки агентов глубокого исследования и методов извлечения информации. Он использует фиксированный, тщательно отобранный корпус документов, что позволяет проводить контролируемые эксперименты. Тест эффективно различает производительность различных систем глубокого исследования, например, показывая, что GPT-5 достигает точности 55.9%, а при интеграции с ретривером Qwen3-Embedding-8B точность повышается до 70.1%. BrowseComp-Plus способствует получению новых знаний об эффективности извлечения информации, точности цитирования и инженерии контекста в системах глубокого исследования.'}, 'en': {'title': 'BrowseComp-Plus: A Fair Benchmark for Deep Research Evaluation', 'desc': 'BrowseComp-Plus is a new benchmark designed to evaluate deep research agents and retrieval methods in a controlled manner. It addresses limitations of existing benchmarks by using a fixed, curated document corpus, allowing for fair comparisons and reproducibility. The benchmark includes human-verified documents and challenging negatives for each query, enabling detailed analysis of the performance of different deep research systems. Results show significant improvements in accuracy when using advanced models like GPT-5, highlighting the effectiveness of this new evaluation framework.'}, 'zh': {'title': 'BrowseComp-Plus：深度研究的公平评估工具', 'desc': 'BrowseComp-Plus是一个经过精心策划的基准测试，旨在对深度研究代理和检索方法进行控制评估。它解决了现有基准测试在公平性和透明性方面的局限性，通过使用固定的文档库来确保实验的可控性。每个查询都包含经过人工验证的支持文档和具有挑战性的负样本，从而使得实验结果更具可信度。该基准测试有效区分了不同深度研究系统的性能，提供了对检索有效性和引用准确性的深入分析。'}}}, {'id': 'https://huggingface.co/papers/2508.05305', 'title': 'SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings\n  and Speaks in Tokens', 'url': 'https://huggingface.co/papers/2508.05305', 'abstract': 'SONAR-LLM, a decoder-only transformer using token-level cross-entropy in the SONAR embedding space, achieves competitive text generation quality without diffusion sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t The recently proposed Large Concept Model (LCM) generates text by predicting a sequence of sentence-level embeddings and training with either mean-squared error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer that "thinks" in the same continuous SONAR embedding space, yet is supervised through token-level cross-entropy propagated via the frozen SONAR decoder. This hybrid objective retains the semantic abstraction of LCM while eliminating its diffusion sampler and restoring a likelihood-based training signal. Across model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive generation quality. We report scaling trends, ablations, benchmark results, and release the complete training code and all pretrained checkpoints to foster reproducibility and future research.', 'score': 25, 'issue_id': 5301, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': '1502a6acea7faa7a', 'authors': ['Nikita Dragunov', 'Temurbek Rahmatullaev', 'Elizaveta Goncharova', 'Andrey Kuznetsov', 'Anton Razzhigaev'], 'affiliations': ['AIRI', 'HSE', 'Innopolis University', 'MSU', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2508.05305.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#open_source', '#architecture', '#training', '#story_generation', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Эффективная генерация текста без диффузионной выборки', 'desc': 'SONAR-LLM - это новая модель машинного обучения для генерации текста. Она использует декодер-трансформер и обучается с помощью кросс-энтропии на уровне токенов в пространстве эмбеддингов SONAR. В отличие от Large Concept Model, SONAR-LLM не требует диффузионной выборки. Модель показывает конкурентоспособное качество генерации текста при различных размерах от 39 млн до 1,3 млрд параметров.'}, 'en': {'title': 'SONAR-LLM: Simplifying Text Generation with Semantic Precision', 'desc': 'SONAR-LLM is a new type of language model that uses a decoder-only transformer architecture to generate text. It operates in a unique embedding space called SONAR, which allows it to maintain semantic meaning while generating text. Instead of using complex diffusion sampling methods, SONAR-LLM employs a simpler token-level cross-entropy loss for training, which helps improve its performance. The model has been tested across various sizes and shows strong results in text generation, with all resources made available for further research.'}, 'zh': {'title': 'SONAR-LLM：高效文本生成的新方法', 'desc': 'SONAR-LLM是一种仅使用解码器的变换器模型，它在SONAR嵌入空间中通过令牌级交叉熵进行训练，从而实现了高质量的文本生成。与最近提出的大型概念模型（LCM）不同，SONAR-LLM不使用扩散采样，而是通过冻结的SONAR解码器传播监督信号。该模型保留了LCM的语义抽象，同时恢复了基于似然的训练信号。SONAR-LLM在从3900万到13亿参数的不同模型规模上都达到了竞争力的生成质量。'}}}, {'id': 'https://huggingface.co/papers/2507.22034', 'title': 'UserBench: An Interactive Gym Environment for User-Centric Agents', 'url': 'https://huggingface.co/papers/2507.22034', 'abstract': 'UserBench evaluates LLM-based agents in multi-turn interactions with simulated users, revealing gaps in task completion and user alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability.', 'score': 25, 'issue_id': 5297, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '91c16eb4d2452d19', 'authors': ['Cheng Qian', 'Zuxin Liu', 'Akshara Prabhakar', 'Zhiwei Liu', 'Jianguo Zhang', 'Haolin Chen', 'Heng Ji', 'Weiran Yao', 'Shelby Heinecke', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2507.22034.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#alignment', '#reasoning', '#agents'], 'emoji': '🤖', 'ru': {'title': 'UserBench: на пути к ИИ-агентам, понимающим пользователей', 'desc': 'UserBench - это новый бенчмарк для оценки ИИ-агентов на основе больших языковых моделей в многоэтапных взаимодействиях с симулированными пользователями. Он выявляет разрыв между выполнением задач и соответствием предпочтениям пользователей. Результаты показывают, что даже продвинутые модели раскрывают менее 30% всех пользовательских предпочтений через активное взаимодействие. UserBench предлагает интерактивную среду для измерения и улучшения способности ИИ-агентов быть настоящими партнерами для пользователей.'}, 'en': {'title': 'Bridging the Gap: Enhancing User Alignment in LLM Interactions', 'desc': 'UserBench is a benchmark designed to assess the performance of LLM-based agents in multi-turn interactions with users. It focuses on how well these agents can understand and adapt to vague or evolving user goals, which is crucial for effective collaboration. The study reveals that current models struggle with aligning their outputs to user preferences, achieving only 20% alignment on average. This highlights the need for improved interaction strategies that allow agents to clarify user intent and make informed decisions throughout the conversation.'}, 'zh': {'title': '提升智能体的用户协作能力', 'desc': '本论文介绍了UserBench，这是一个用户中心的基准测试，用于评估基于大型语言模型（LLM）的智能体在多轮交互中的表现。研究发现，尽管这些智能体在解决复杂任务方面取得了显著进展，但在与用户的主动合作上仍存在不足，尤其是在目标模糊或不断变化的情况下。通过模拟用户逐步揭示偏好，UserBench要求智能体主动澄清意图并做出基于工具的决策。评估结果显示，当前模型在任务完成与用户对齐之间存在显著差距，平均只有20%的时间能够完全满足用户意图。'}}}, {'id': 'https://huggingface.co/papers/2508.07917', 'title': 'MolmoAct: Action Reasoning Models that can Reason in Space', 'url': 'https://huggingface.co/papers/2508.07917', 'abstract': 'Action Reasoning Models (ARMs) integrate perception, planning, and control to enable adaptable and explainable robotic behavior, achieving superior performance across various tasks and settings.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of vision-language-action models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact', 'score': 21, 'issue_id': 5294, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'fa9d385dde41879a', 'authors': ['Jason Lee', 'Jiafei Duan', 'Haoquan Fang', 'Yuquan Deng', 'Shuo Liu', 'Boyang Li', 'Bohan Fang', 'Jieyu Zhang', 'Yi Ru Wang', 'Sangho Lee', 'Winson Han', 'Wilbert Pumacay', 'Angelica Wu', 'Rose Hendrix', 'Karen Farley', 'Eli VanderBilt', 'Ali Farhadi', 'Dieter Fox', 'Ranjay Krishna'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2508.07917.jpg', 'data': {'categories': ['#open_source', '#robotics', '#agents', '#dataset', '#reasoning', '#training'], 'emoji': '🤖', 'ru': {'title': 'MolmoAct: Разумные действия роботов через структурированное рассуждение', 'desc': 'Модель Action Reasoning Models (ARM) интегрирует восприятие, планирование и управление для адаптивного и объяснимого поведения роботов. MolmoAct, реализация ARM, кодирует наблюдения и инструкции в токены восприятия с учетом глубины, генерирует пространственные планы в виде редактируемых траекторий и прогнозирует точные низкоуровневые действия. Модель достигает высокой производительности в симуляциях и реальных условиях, превосходя базовые модели в задачах с нулевым обучением, долгосрочном планировании и обобщении на новые распределения. Авторы также выпускают набор данных MolmoAct Dataset, содержащий более 10000 высококачественных траекторий роботов.'}, 'en': {'title': 'Transforming Perception into Purposeful Action with ARMs', 'desc': "Action Reasoning Models (ARMs) are designed to enhance robotic behavior by combining perception, planning, and control in a structured way. The proposed model, MolmoAct, processes observations and instructions into depth-aware tokens, creates editable spatial plans, and predicts specific actions, making the robot's behavior more explainable and adaptable. MolmoAct demonstrates impressive performance in both simulated and real-world tasks, outperforming existing models in accuracy and generalization. Additionally, the introduction of the MolmoAct Dataset provides valuable training data, further improving the model's capabilities and establishing a new standard in robotics."}, 'zh': {'title': '行动推理模型：将感知转化为有目的的行动', 'desc': '行动推理模型（ARMs）结合了感知、规划和控制，能够实现灵活和可解释的机器人行为，提升了在各种任务和环境中的表现。我们提出的MolmoAct模型通过一个结构化的三阶段流程，将观察和指令编码为深度感知标记，生成可编辑的中级空间计划，并预测精确的低级动作。该模型在模拟和现实世界中表现出色，尤其在长时间任务上超越了现有的基线模型。我们还首次发布了MolmoAct数据集，包含超过10,000条高质量机器人轨迹，训练该数据集可显著提升模型的整体性能。'}}}, {'id': 'https://huggingface.co/papers/2508.05614', 'title': 'OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks', 'url': 'https://huggingface.co/papers/2508.05614', 'abstract': "OmniEAR evaluates language models' embodied reasoning capabilities in physical interactions, tool usage, and multi-agent coordination, revealing performance degradation under constraints and highlighting architectural limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.", 'score': 15, 'issue_id': 5294, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'd9c4313e65f213fe', 'authors': ['Zixuan Wang', 'Dingming Li', 'Hongxing Li', 'Shuo Chen', 'Yuchen Yan', 'Wenqi Zhang', 'Yongliang Shen', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05614.jpg', 'data': {'categories': ['#architecture', '#open_source', '#agents', '#reasoning', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'OmniEAR: раскрывая ограничения языковых моделей в воплощенном рассуждении', 'desc': 'OmniEAR - это комплексная система оценки способностей языковых моделей к воплощенному рассуждению в физических взаимодействиях, использовании инструментов и координации между агентами. Фреймворк моделирует непрерывные физические свойства и сложные пространственные отношения в 1500 сценариях бытовых и промышленных задач. Оценка выявила значительное снижение производительности моделей при необходимости рассуждать с учетом ограничений, особенно в сложных задачах. Результаты показывают, что воплощенное рассуждение представляет принципиально иные проблемы, чем те, с которыми могут справиться современные модели.'}, 'en': {'title': 'Evaluating Language Models in Real-World Reasoning Tasks', 'desc': 'OmniEAR is a framework designed to assess how well language models can reason in real-world scenarios involving physical interactions, tool usage, and teamwork. It highlights that while these models perform well with clear instructions, their performance significantly drops when faced with constraints or when they need to figure things out on their own. The study shows that even with complete information about the environment, models struggle to coordinate effectively, revealing limitations in their architecture. Overall, OmniEAR serves as a new benchmark to push the boundaries of embodied AI capabilities.'}, 'zh': {'title': 'OmniEAR：评估语言模型的体现推理能力', 'desc': 'OmniEAR是一个评估语言模型在物理交互、工具使用和多智能体协调中的体现推理能力的框架。研究发现，当模型在约束条件下进行推理时，性能显著下降，尤其是在工具推理和隐性协作任务中。尽管在明确指令下模型的成功率高达85-96%，但在复杂任务中失败率超过50%。这些结果表明，现有模型在处理体现推理时面临根本性的挑战，OmniEAR为评估和推动体现人工智能系统提供了严格的基准。'}}}, {'id': 'https://huggingface.co/papers/2508.07785', 'title': 'Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts', 'url': 'https://huggingface.co/papers/2508.07785', 'abstract': 'Grove MoE, a novel architecture with heterogeneous experts of varying sizes, improves computational efficiency and performance in large language models by dynamically activating parameters based on input complexity.  \t\t\t\t\tAI-generated summary \t\t\t\t The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate scalability by enabling sparse parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size.', 'score': 14, 'issue_id': 5302, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '000c1f70e8499f7a', 'authors': ['Haoyuan Wu', 'Haoxing Chen', 'Xiaodong Chen', 'Zhanchao Zhou', 'Tieyuan Chen', 'Yihong Zhuang', 'Guoshan Lu', 'Zenan Huang', 'Junbo Zhao', 'Lin Liu', 'Zhenzhong Lan', 'Bei Yu', 'Jianguo Li'], 'affiliations': ['Inclusion AI', 'Renmin University of China', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.07785.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#open_source'], 'emoji': '🌳', 'ru': {'title': 'Гетерогенные эксперты для эффективных языковых моделей', 'desc': 'Grove MoE - это новая архитектура для больших языковых моделей, использующая экспертов разного размера. Она динамически активирует параметры в зависимости от сложности входных данных, что повышает вычислительную эффективность. Авторы представили модели GroveMoE-Base и GroveMoE-Inst с 33 миллиардами параметров, созданные на основе Qwen3-30B-A3B-Base. Эти модели показывают производительность на уровне современных открытых моделей аналогичного или даже большего размера.'}, 'en': {'title': 'Dynamic Efficiency with Heterogeneous Experts in Language Models', 'desc': 'Grove MoE introduces a new architecture for large language models that uses heterogeneous experts of different sizes to enhance efficiency and performance. Unlike traditional Mixture of Experts (MoE) models that activate a fixed number of parameters regardless of input complexity, Grove MoE dynamically activates parameters based on the complexity of the input. This approach allows for better scalability and computational efficiency by only using the necessary resources for each task. The resulting models, GroveMoE-Base and GroveMoE-Inst, demonstrate competitive performance while managing a lower computational load by activating between 3.14 to 3.28 billion parameters as needed.'}, 'zh': {'title': '动态激活，提升效率的Grove MoE架构', 'desc': 'Grove MoE是一种新颖的混合专家架构，采用不同大小的专家，以提高大型语言模型的计算效率和性能。与传统的均匀专家架构不同，Grove MoE根据输入复杂性动态激活参数，从而克服了固定参数激活的限制。该架构灵感来源于异构的big.LITTLE CPU架构，具有动态激活机制，能够在保持可控计算开销的同时扩展模型容量。通过这种架构，我们开发了GroveMoE-Base和GroveMoE-Inst这两种大型语言模型，能够根据令牌复杂性动态激活3.14-3.28B参数，性能与同类或更大规模的开源模型相当。'}}}, {'id': 'https://huggingface.co/papers/2508.08189', 'title': 'Reinforcement Learning in Vision: A Survey', 'url': 'https://huggingface.co/papers/2508.08189', 'abstract': 'This survey synthesizes recent advancements in visual reinforcement learning, covering policy optimization strategies, thematic pillars, and evaluation protocols, while highlighting open challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.', 'score': 11, 'issue_id': 5298, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'ba135022f16de1cf', 'authors': ['Weijia Wu', 'Chen Gao', 'Joya Chen', 'Kevin Qinghong Lin', 'Qingwei Meng', 'Yiming Zhang', 'Yuke Qiu', 'Hong Zhou', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.08189.jpg', 'data': {'categories': ['#multimodal', '#cv', '#training', '#survey', '#optimization', '#games', '#rl', '#rlhf', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Визуальное обучение с подкреплением: на пути к разумным агентам', 'desc': 'Этот обзор синтезирует последние достижения в области визуального обучения с подкреплением. Рассматриваются стратегии оптимизации политик, тематические направления и протоколы оценки. Особое внимание уделяется четырем основным областям: мультимодальные большие языковые модели, визуальная генерация, унифицированные модельные фреймворки и модели, объединяющие зрение, язык и действие. Обзор также выделяет открытые проблемы, включая эффективность использования данных, обобщение и безопасное развертывание.'}, 'en': {'title': 'Mapping the Future of Visual Reinforcement Learning', 'desc': 'This paper surveys the latest developments in visual reinforcement learning (VRL), focusing on how agents can understand and interact with complex visual environments. It categorizes over 200 studies into four main themes, including multi-modal models and vision-language-action frameworks, while discussing advancements in policy optimization techniques. The authors also evaluate various protocols for assessing VRL performance and identify key challenges such as improving sample efficiency and ensuring safe deployment. Overall, the survey aims to provide a comprehensive overview of the field and suggest future research directions.'}, 'zh': {'title': '视觉强化学习的前沿探索', 'desc': '这篇综述文章总结了视觉强化学习的最新进展，涵盖了策略优化策略、主题支柱和评估协议，同时强调了开放挑战。文章首先形式化了视觉强化学习问题，并追踪了从RLHF到可验证奖励范式的策略优化演变。接着，将200多篇代表性作品组织成四个主题支柱：多模态大语言模型、视觉生成、统一模型框架和视觉-语言-行动模型。最后，文章回顾了评估协议，并识别了样本效率、泛化和安全部署等开放挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.06026', 'title': 'Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via\n  Past-Future', 'url': 'https://huggingface.co/papers/2508.06026', 'abstract': "Temporal Self-Rewarding Language Models improve generative capabilities by strategically using past and future model outputs, enhancing preference learning and out-of-distribution generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose Temporal Self-Rewarding Language Models that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) Anchored Rejection - fixing rejected responses using the past initial model's outputs and (2) Future-Guided Chosen - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.", 'score': 11, 'issue_id': 5296, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '9118094d6c4509d1', 'authors': ['Yidong Wang', 'Xin Wang', 'Cunxiang Wang', 'Junfeng Fang', 'Qiufeng Wang', 'Jianing Chu', 'Xuran Meng', 'Shuxun Yang', 'Libo Qin', 'Yue Zhang', 'Wei Ye', 'Shikun Zhang'], 'affiliations': ['Beijing Institute of Technology', 'Central South University', 'National University of Singapore', 'North Carolina State University', 'Peking University', 'Southeast University', 'Tsinghua University', 'University of Michigan', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.06026.jpg', 'data': {'categories': ['#training', '#agi', '#rlhf', '#architecture', '#optimization', '#reasoning', '#rl'], 'emoji': '⏳', 'ru': {'title': 'Временная самокоррекция: новый шаг в развитии языковых моделей', 'desc': 'Предложена новая архитектура языковых моделей под названием Temporal Self-Rewarding Language Models. Эта архитектура улучшает генеративные способности моделей, стратегически используя прошлые и будущие выходные данные модели. Метод включает в себя две фазы: фиксацию отвергнутых ответов с использованием выходных данных начальной модели и динамический отбор выбранных образцов с использованием предсказаний модели следующего поколения. Эксперименты показали значительные улучшения по сравнению с базовыми методами самовознаграждения, особенно в задачах вне распределения обучающих данных.'}, 'en': {'title': 'Enhancing Language Models with Temporal Self-Rewarding Strategies', 'desc': 'This paper introduces Temporal Self-Rewarding Language Models, which enhance the generative abilities of language models by effectively utilizing outputs from past and future generations. The proposed architecture allows models to generate responses and evaluate them through a self-judging mechanism, improving performance via Direct Preference Optimization. A key innovation is the dual-phase framework that includes Anchored Rejection and Future-Guided Chosen strategies, which help maintain diverse learning signals and improve preference learning. Experimental results show that this approach significantly outperforms traditional Self-Rewarding methods across various tasks and model sizes, demonstrating better generalization capabilities.'}, 'zh': {'title': '时间自奖励模型：提升生成能力的新策略', 'desc': '本文提出了一种时间自奖励语言模型，通过战略性地利用过去和未来的模型输出，提升生成能力和偏好学习。该模型采用了自我评估机制，使大型语言模型（LLMs）能够生成响应并评估自身输出，从而通过直接偏好优化（DPO）不断改进生成能力。研究发现，现有自奖励模型存在一个关键限制，即选择和拒绝响应的同步改进会逐渐缩小对比样本之间的表示差异，影响有效的偏好学习。为此，本文提出了时间自奖励语言模型，通过协调过去、现在和未来的生成，保持学习信号，从而显著提高模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2508.08221', 'title': 'Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning', 'url': 'https://huggingface.co/papers/2508.08221', 'abstract': 'A systematic review of reinforcement learning techniques for large language model reasoning reveals clear guidelines and demonstrates that a minimalist combination of techniques can improve performance over existing strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO.', 'score': 9, 'issue_id': 5300, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': 'f0862b77d91584b6', 'authors': ['Zihe Liu', 'Jiashun Liu', 'Yancheng He', 'Weixun Wang', 'Jiaheng Liu', 'Ling Pan', 'Xinyu Hu', 'Shaopan Xiong', 'Ju Huang', 'Jian Hu', 'Shengyi Huang', 'Siran Yang', 'Jiamang Wang', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Alibaba Group', 'Beijing Jiaotong University', 'CleanRL', 'Hong Kong University of Science and Technology', 'Nanjing University', 'OpenRLHF', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.08221.jpg', 'data': {'categories': ['#survey', '#rlhf', '#training', '#reasoning', '#rl', '#open_source', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Упрощение обучения с подкреплением для улучшения рассуждений ИИ', 'desc': 'Статья представляет систематический обзор методов обучения с подкреплением для рассуждений больших языковых моделей. Авторы анализируют внутренние механизмы, применимые сценарии и основные принципы каждого метода через тщательные эксперименты. На основе полученных результатов предлагаются четкие рекомендации по выбору техник обучения с подкреплением для конкретных задач. Исследование показывает, что минималистичная комбинация двух техник может улучшить производительность по сравнению с существующими стратегиями.'}, 'en': {'title': 'Streamlining Reinforcement Learning for Better Language Model Reasoning', 'desc': 'This paper reviews reinforcement learning (RL) techniques specifically for reasoning in large language models (LLMs). It highlights the lack of standardized guidelines and the confusion caused by inconsistent experimental setups in the field. Through systematic evaluations, the authors identify key characteristics of various RL methods and propose a minimalist approach that combines two techniques to enhance performance. The findings suggest that this simple combination can effectively improve learning capabilities compared to more complex strategies.'}, 'zh': {'title': '简约组合，提升强化学习性能', 'desc': '这篇论文系统性地回顾了强化学习在大型语言模型推理中的应用，提出了明确的指导方针。研究表明，采用简约的技术组合可以提升现有策略的性能。尽管相关研究迅速增加，但仍存在标准化指导缺失和机制理解不清等挑战。通过细致的实验，论文为从业者提供了选择强化学习技术的可靠路线图。'}}}, {'id': 'https://huggingface.co/papers/2508.08134', 'title': 'Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control', 'url': 'https://huggingface.co/papers/2508.08134', 'abstract': 'Follow-Your-Shape framework uses a Trajectory Divergence Map and Scheduled KV Injection to enable precise and controllable shape editing in images while preserving non-target content.  \t\t\t\t\tAI-generated summary \t\t\t\t While recent flow-based image editing models demonstrate general-purpose capabilities across diverse tasks, they often struggle to specialize in challenging scenarios -- particularly those involving large-scale shape transformations. When performing such structural edits, these methods either fail to achieve the intended shape change or inadvertently alter non-target regions, resulting in degraded background quality. We propose Follow-Your-Shape, a training-free and mask-free framework that supports precise and controllable editing of object shapes while strictly preserving non-target content. Motivated by the divergence between inversion and editing trajectories, we compute a Trajectory Divergence Map (TDM) by comparing token-wise velocity differences between the inversion and denoising paths. The TDM enables precise localization of editable regions and guides a Scheduled KV Injection mechanism that ensures stable and faithful editing. To facilitate a rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120 new images and enriched prompt pairs specifically curated for shape-aware editing. Experiments demonstrate that our method achieves superior editability and visual fidelity, particularly in tasks requiring large-scale shape replacement.', 'score': 8, 'issue_id': 5295, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '46943e7de387bfc9', 'authors': ['Zeqian Long', 'Mingzhe Zheng', 'Kunyu Feng', 'Xinhua Zhang', 'Hongyu Liu', 'Harry Yang', 'Linfeng Zhang', 'Qifeng Chen', 'Yue Ma'], 'affiliations': ['HKUST', 'Shanghai Jiao Tong University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2508.08134.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#cv', '#games'], 'emoji': '🔄', 'ru': {'title': 'Точное редактирование форм объектов с сохранением фона', 'desc': 'Статья представляет новый фреймворк Follow-Your-Shape для точного и контролируемого редактирования форм объектов на изображениях. Метод использует карту расхождения траекторий (Trajectory Divergence Map) для локализации редактируемых областей. Предложенный механизм планового введения ключей и значений (Scheduled KV Injection) обеспечивает стабильное и точное редактирование. Авторы также представляют новый бенчмарк ReShapeBench для оценки методов редактирования форм.'}, 'en': {'title': 'Precision in Shape Editing with Follow-Your-Shape', 'desc': 'The Follow-Your-Shape framework introduces a novel approach for shape editing in images, focusing on maintaining the quality of non-target content. It utilizes a Trajectory Divergence Map (TDM) to analyze differences in editing paths, allowing for precise localization of areas that can be modified. Additionally, the Scheduled KV Injection mechanism ensures that the editing process remains stable and accurate. This method outperforms existing models, especially in complex scenarios involving significant shape transformations, while also introducing a new benchmark for evaluating shape-aware editing.'}, 'zh': {'title': '精确可控的形状编辑新方法', 'desc': 'Follow-Your-Shape框架利用轨迹发散图和调度KV注入技术，实现了图像中形状的精确和可控编辑，同时保持非目标内容的完整性。该方法解决了现有基于流的图像编辑模型在大规模形状变换中的不足，避免了对非目标区域的意外修改。通过计算反演和去噪路径之间的速度差异，Follow-Your-Shape能够精确定位可编辑区域。我们还引入了ReShapeBench基准，专门用于形状感知编辑的评估，实验结果表明该方法在大规模形状替换任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.07101', 'title': 'Less Is More: Training-Free Sparse Attention with Global Locality for\n  Efficient Reasoning', 'url': 'https://huggingface.co/papers/2508.07101', 'abstract': 'LessIsMore is a training-free sparse attention mechanism that improves efficiency and generalization in reasoning tasks by aggregating token selections from local attention heads.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a 1.1times average decoding speed-up compared to full attention. Moreover, LessIsMore attends to 2times fewer tokens without accuracy loss, achieving a 1.13times end-to-end speed-up compared to existing sparse attention methods.', 'score': 8, 'issue_id': 5296, 'pub_date': '2025-08-09', 'pub_date_card': {'ru': '9 августа', 'en': 'August 9', 'zh': '8月9日'}, 'hash': '7365d6ccd543770b', 'authors': ['Lijie Yang', 'Zhihao Zhang', 'Arti Jain', 'Shijie Cao', 'Baihong Yuan', 'Yiwei Chen', 'Zhihao Jia', 'Ravi Netravali'], 'affiliations': ['Carnegie Mellon University', 'Microsoft Research', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2508.07101.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Меньше значит больше: эффективное разреженное внимание для улучшенного рассуждения', 'desc': 'LessIsMore - это механизм разреженного внимания, не требующий дополнительного обучения, который улучшает эффективность и обобщающую способность в задачах рассуждения. Он агрегирует выбор токенов из локальных голов внимания с недавней контекстной информацией, что позволяет проводить единый ранжирование токенов для будущих слоев декодирования. Этот подход улучшает обобщение и эффективность, избегая необходимости поддерживать отдельные подмножества токенов для каждой головы. Оценка на различных задачах рассуждения показывает, что LessIsMore сохраняет или даже улучшает точность, при этом ускоряя декодирование в среднем в 1.1 раза по сравнению с полным вниманием.'}, 'en': {'title': 'LessIsMore: Efficient Reasoning with Sparse Attention', 'desc': 'LessIsMore is a novel sparse attention mechanism designed to enhance efficiency and generalization in reasoning tasks without the need for retraining. It aggregates token selections from local attention heads, utilizing global attention patterns to improve the decision-making process during decoding. This approach allows for a unified ranking of tokens across different heads, which reduces the number of tokens processed while maintaining or even improving accuracy. Evaluations demonstrate that LessIsMore achieves faster decoding speeds and lower token usage compared to traditional sparse attention methods, making it a significant advancement in the field.'}, 'zh': {'title': 'LessIsMore：高效推理的新选择', 'desc': 'LessIsMore是一种无训练的稀疏注意力机制，旨在提高推理任务的效率和泛化能力。它通过聚合来自局部注意力头的标记选择，利用全局注意力模式，而不是依赖传统的头特定局部优化。该方法在推理过程中避免了维护每个头的独立标记子集，从而提高了通用性和效率。评估结果表明，LessIsMore在保持或提高准确性的同时，解码速度平均提升了1.1倍，并且在不损失准确性的情况下，关注的标记数量减少了2倍。'}}}, {'id': 'https://huggingface.co/papers/2508.05257', 'title': 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs', 'url': 'https://huggingface.co/papers/2508.05257', 'abstract': 'A novel Mixture-of-Basis-Experts (MoBE) method is introduced to compress large language models with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t The Mixture-of-Experts (MoE) architecture has become a predominant paradigm for scaling large language models (LLMs). Despite offering strong performance and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and Kimi-K2-Instruct present serious challenges due to substantial memory requirements in deployment. While recent works have explored MoE compression to address this issue, existing methods often suffer from considerable accuracy drops (e.g., 7-14% relatively) even at modest compression rates. This paper introduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model compression while incurring minimal accuracy drops. Specifically, each up/gate matrix in an expert is decomposed via a rank decomposition as W = AB, where matrix A is unique to each expert. The relatively larger matrix B is further re-parameterized as a linear combination of basis matrices {Bi} shared across all experts within a given MoE layer. The factorization is learned by minimizing the reconstruction error relative to the original weight matrices. Experiments demonstrate that MoBE achieves notably lower accuracy drops compared to prior works. For instance, MoBE can reduce the parameter counts of Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by 24%-30% with only 1%-2% accuracy drop (about 2% drops when measured relatively).', 'score': 7, 'issue_id': 5302, 'pub_date': '2025-08-07', 'pub_date_card': {'ru': '7 августа', 'en': 'August 7', 'zh': '8月7日'}, 'hash': 'fe0cf85d04556fd4', 'authors': ['Xiaodong Chen', 'Mingming Ha', 'Zhenzhong Lan', 'Jing Zhang', 'Jianguo Li'], 'affiliations': ['Inclusion AI', 'Renmin University of China', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.05257.jpg', 'data': {'categories': ['#training', '#architecture', '#optimization', '#inference'], 'emoji': '🗜️', 'ru': {'title': 'Эффективное сжатие языковых моделей без потери качества', 'desc': 'Представлен новый метод Mixture-of-Basis-Experts (MoBE) для сжатия больших языковых моделей с минимальной потерей точности. MoBE разлагает матрицы экспертов на уникальную для эксперта матрицу A и общую базисную матрицу B, которая параметризуется как линейная комбинация базисных матриц. Эксперименты показывают, что MoBE позволяет сократить количество параметров крупных моделей на 24-30% при снижении точности всего на 1-2%. Этот метод значительно превосходит существующие подходы к сжатию моделей архитектуры Mixture-of-Experts.'}, 'en': {'title': 'Efficient Compression with Minimal Accuracy Loss', 'desc': 'This paper presents a new method called Mixture-of-Basis-Experts (MoBE) for compressing large language models while maintaining high accuracy. The MoBE approach uses a unique decomposition of the expert matrices, allowing for efficient parameter sharing across experts in a Mixture-of-Experts architecture. By minimizing reconstruction error, the method significantly reduces the number of parameters needed without a substantial loss in performance. Experimental results show that MoBE can compress models by 24%-30% with only a 1%-2% drop in accuracy, outperforming existing compression techniques.'}, 'zh': {'title': '基础专家混合：高效压缩，低损失', 'desc': '本文介绍了一种新颖的基础专家混合（MoBE）方法，用于在压缩大型语言模型时尽量减少准确性损失。MoBE通过对专家中的上/门矩阵进行秩分解，将其表示为W = AB，其中矩阵A是每个专家独有的。相对较大的矩阵B则被重新参数化为在给定MoE层内所有专家共享的基础矩阵的线性组合。实验表明，MoBE在压缩参数的同时，准确性损失显著低于以往方法。'}}}, {'id': 'https://huggingface.co/papers/2508.07493', 'title': 'VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation\n  for Multilingual Long Document Understanding', 'url': 'https://huggingface.co/papers/2508.07493', 'abstract': 'VisR-Bench is a multilingual benchmark for question-driven multimodal retrieval in long documents, evaluating various models across different languages and question types.  \t\t\t\t\tAI-generated summary \t\t\t\t Most organizational data in this world are stored as documents, and visual retrieval plays a crucial role in unlocking the collective intelligence from all these documents. However, existing benchmarks focus on English-only document retrieval or only consider multilingual question-answering on a single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual benchmark designed for question-driven multimodal retrieval in long documents. Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents, enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans sixteen languages with three question types (figures, text, and tables), offering diverse linguistic and question coverage. Unlike prior datasets, we include queries without explicit answers, preventing models from relying on superficial keyword matching. We evaluate various retrieval models, including text-based methods, multimodal encoders, and MLLMs, providing insights into their strengths and limitations. Our results show that while MLLMs significantly outperform text-based and multimodal encoder models, they still struggle with structured tables and low-resource languages, highlighting key challenges in multilingual visual retrieval.', 'score': 5, 'issue_id': 5298, 'pub_date': '2025-08-10', 'pub_date_card': {'ru': '10 августа', 'en': 'August 10', 'zh': '8月10日'}, 'hash': 'a47e8d1decc51e3a', 'authors': ['Jian Chen', 'Ming Li', 'Jihyung Kil', 'Chenguang Wang', 'Tong Yu', 'Ryan Rossi', 'Tianyi Zhou', 'Changyou Chen', 'Ruiyi Zhang'], 'affiliations': ['Adobe Research', 'University at Buffalo', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2508.07493.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#low_resource', '#multilingual', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'VisR-Bench: многоязычный вызов для мультимодального поиска в документах', 'desc': 'VisR-Bench - это многоязычный бенчмарк для мультимодального поиска в длинных документах, управляемого вопросами. Он включает более 35 тысяч пар вопрос-ответ на 16 языках, охватывая различные типы вопросов (о рисунках, тексте и таблицах). Бенчмарк позволяет оценивать различные модели, включая текстовые методы, мультимодальные энкодеры и мультиязычные языковые модели (MLLM). Результаты показывают, что MLLM значительно превосходят другие подходы, но всё ещё испытывают трудности со структурированными таблицами и малоресурсными языками.'}, 'en': {'title': 'Unlocking Multilingual Insights from Long Documents', 'desc': 'VisR-Bench is a new benchmark for evaluating how well models can retrieve information from long documents using questions in multiple languages. It includes over 35,000 question-answer pairs from 1,200 documents, covering various types of questions about figures, text, and tables. This benchmark is unique because it tests models on queries that do not have clear answers, which helps to avoid simple keyword matching. The findings reveal that while multilingual large language models (MLLMs) perform better than traditional text-based and multimodal models, they still face difficulties with structured data and languages with fewer resources.'}, 'zh': {'title': '多语言文档检索的新基准', 'desc': 'VisR-Bench是一个多语言基准，用于在长文档中进行基于问题的多模态检索。该基准包含超过35,000个高质量的问答对，涵盖1,200个文档，支持对多模态检索的细致评估。它涉及十六种语言和三种问题类型（图形、文本和表格），提供了丰富的语言和问题覆盖。研究结果表明，尽管多语言大模型在性能上优于文本基础和多模态编码器模型，但在处理结构化表格和低资源语言时仍面临挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.06426', 'title': 'Shortcut Learning in Generalist Robot Policies: The Role of Dataset\n  Diversity and Fragmentation', 'url': 'https://huggingface.co/papers/2508.06426', 'abstract': 'Shortcut learning in generalist robot policies trained on large-scale datasets limits generalization, and this can be mitigated through improved dataset collection and data augmentation strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Generalist robot policies trained on large-scale datasets such as Open X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks. However, they often struggle to generalize beyond the distribution of their training data. In this paper, we investigate the underlying cause of this limited generalization capability. We identify shortcut learning -- the reliance on task-irrelevant features -- as a key impediment to generalization. Through comprehensive theoretical and empirical analysis, we uncover two primary contributors to shortcut learning: (1) limited diversity within individual sub-datasets, and (2) significant distributional disparities across sub-datasets, leading to dataset fragmentation. These issues arise from the inherent structure of large-scale datasets like OXE, which are typically composed of multiple sub-datasets collected independently across varied environments and embodiments. Our findings provide critical insights into dataset collection strategies that can reduce shortcut learning and enhance the generalization ability of generalist robot policies. Moreover, in scenarios where acquiring new large-scale data is impractical, we demonstrate that carefully selected robotic data augmentation strategies can effectively reduce shortcut learning in existing offline datasets, thereby improving generalization capabilities of generalist robot policies, e.g., pi_0, in both simulation and real-world environments. More information at https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.', 'score': 5, 'issue_id': 5299, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'cf0dd977bbaaf7ba', 'authors': ['Youguang Xing', 'Xu Luo', 'Junlin Xie', 'Lianli Gao', 'Hengtao Shen', 'Jingkuan Song'], 'affiliations': ['Tongji University', 'UESTC'], 'pdf_title_img': 'assets/pdf/title_img/2508.06426.jpg', 'data': {'categories': ['#robotics', '#dataset', '#data', '#optimization', '#transfer_learning'], 'emoji': '🤖', 'ru': {'title': 'Преодоление упрощенного обучения для улучшения обобщения в политиках роботов', 'desc': 'Исследование показывает, что обобщенные политики роботов, обученные на крупномасштабных наборах данных, часто страдают от проблемы упрощенного обучения. Это приводит к ограниченной способности к обобщению за пределами распределения обучающих данных. Основными причинами являются недостаточное разнообразие внутри поднаборов данных и значительные различия между ними. Авторы предлагают улучшенные стратегии сбора данных и методы аугментации для смягчения этой проблемы и повышения способности к обобщению.'}, 'en': {'title': 'Enhancing Generalization by Tackling Shortcut Learning in Robot Policies', 'desc': 'This paper explores the issue of shortcut learning in generalist robot policies that are trained on large-scale datasets, such as Open X-Embodiment (OXE). Shortcut learning occurs when robots rely on irrelevant features from their training data, which hinders their ability to generalize to new tasks. The authors identify two main causes of this problem: a lack of diversity within sub-datasets and significant differences between these sub-datasets. They propose improved dataset collection methods and data augmentation techniques to mitigate shortcut learning and enhance the generalization capabilities of these robot policies.'}, 'zh': {'title': '减少快捷学习，提升机器人泛化能力', 'desc': '这篇论文探讨了在大规模数据集上训练的通用机器人策略中的快捷学习现象。快捷学习是指模型依赖于与任务无关的特征，从而限制了其泛化能力。研究发现，数据集内部的多样性不足和子数据集之间的分布差异是导致快捷学习的主要原因。通过改进数据集收集策略和数据增强方法，可以有效减少快捷学习，提高通用机器人策略的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.03346', 'title': 'Compressing Chain-of-Thought in LLMs via Step Entropy', 'url': 'https://huggingface.co/papers/2508.03346', 'abstract': 'A novel CoT compression framework using step entropy and a two-stage training strategy enhances LLM inference efficiency without significantly reducing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at complex reasoning but generate verbose thought processes with considerable redundancy, leading to increased inference costs and reduced efficiency. We introduce a novel CoT compression framework based on step entropy, a metric that quantifies the informational contribution of individual reasoning steps to identify redundancy. Through theoretical analysis and extensive empirical validation on mathematical reasoning benchmarks, we demonstrate that steps with low entropy are indeed highly redundant. Our experiments reveal that an astonishing 80\\% of low-entropy intermediate steps can be pruned with minor degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning, which severely impairs reasoning performance. Building on this, we propose a novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) reinforcement learning. This approach enables LLMs to autonomously learn to generate compressed COTs during inference by strategically incorporating [SKIP] tokens. Our method significantly enhances LLM inference efficiency while rigorously preserving accuracy, offering profound implications for practical LLM deployment and a deeper understanding of reasoning structures.', 'score': 5, 'issue_id': 5303, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '7961945e6a27036d', 'authors': ['Zeju Li', 'Jianyuan Zhong', 'Ziyang Zheng', 'Xiangyu Wen', 'Zhijian Xu', 'Yingying Cheng', 'Fan Zhang', 'Qiang Xu'], 'affiliations': ['Huawei Technologies Co., Ltd', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.03346.jpg', 'data': {'categories': ['#inference', '#math', '#reasoning', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие рассуждений ИИ без потери точности', 'desc': 'Представлена новая система сжатия цепочки размышлений (CoT) для больших языковых моделей, основанная на энтропии шагов. Исследование показало, что до 80% промежуточных шагов с низкой энтропией можно удалить без существенного ухудшения точности конечного ответа. Предложена двухэтапная стратегия обучения, сочетающая контролируемую точную настройку и обучение с подкреплением. Этот метод позволяет языковым моделям автономно генерировать сжатые цепочки размышлений, значительно повышая эффективность вывода при сохранении точности.'}, 'en': {'title': 'Efficient Reasoning: Pruning Redundancy in LLMs', 'desc': 'This paper presents a new framework for compressing Chain-of-Thought (CoT) prompts in Large Language Models (LLMs) to improve inference efficiency. It uses a concept called step entropy to identify and remove redundant reasoning steps that do not significantly contribute to the final answer. The authors show that up to 80% of these low-entropy steps can be eliminated with only a slight impact on accuracy. Additionally, they introduce a two-stage training method that combines supervised fine-tuning and reinforcement learning to help LLMs learn to generate more efficient CoTs during inference.'}, 'zh': {'title': '提升推理效率，保留准确性的新方法', 'desc': '本文提出了一种新颖的链式思维（CoT）压缩框架，利用步骤熵和两阶段训练策略，提高了大型语言模型（LLM）的推理效率，同时保持了准确性。通过分析步骤熵，我们发现低熵的推理步骤往往是冗余的，实验表明可以去除80%的低熵步骤，几乎不影响最终答案的准确性。与随机或高熵的剪枝方法相比，我们的方法在保留推理性能方面表现更佳。最终，我们的框架为LLM的实际应用提供了重要的启示，并加深了对推理结构的理解。'}}}, {'id': 'https://huggingface.co/papers/2508.07662', 'title': 'GLiClass: Generalist Lightweight Model for Sequence Classification Tasks', 'url': 'https://huggingface.co/papers/2508.07662', 'abstract': 'GLiClass, an adaptation of GLiNER, achieves efficient and accurate sequence classification with zero-shot and few-shot capabilities, and PPO is adapted for multi-label text classification in data-sparse conditions.  \t\t\t\t\tAI-generated summary \t\t\t\t Classification is one of the most widespread tasks in AI applications, serving often as the first step in filtering, sorting, and categorizing data. Since modern AI systems must handle large volumes of input data and early pipeline stages can propagate errors downstream, achieving high efficiency and accuracy is critical. Moreover, classification requirements can change dynamically based on user needs, necessitating models with strong zero-shot capabilities. While generative LLMs have become mainstream for zero-shot classification due to their versatility, they suffer from inconsistent instruction following and computational inefficiency. Cross-encoders, commonly used as rerankers in RAG pipelines, face a different bottleneck: they must process text-label pairs sequentially, significantly reducing efficiency with large label sets. Embedding-based approaches offer good efficiency but struggle with complex scenarios involving logical and semantic constraints. We propose GLiClass, a novel method that adapts the GLiNER architecture for sequence classification tasks. Our approach achieves strong accuracy and efficiency comparable to embedding-based methods, while maintaining the flexibility needed for zero-shot and few-shot learning scenarios. Additionally, we adapted proximal policy optimization (PPO) for multi-label text classification, enabling training classifiers in data-sparse conditions or from human feedback.', 'score': 4, 'issue_id': 5298, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 августа', 'en': 'August 11', 'zh': '8月11日'}, 'hash': '1f103483fe746d10', 'authors': ['Ihor Stepanov', 'Mykhailo Shtopko', 'Dmytro Vodianytskyi', 'Oleksandr Lukashov', 'Alexander Yavorskyi', 'Mykyta Yaroshenko'], 'affiliations': ['Knowledgator Engineering, Kyiv, Ukraine'], 'pdf_title_img': 'assets/pdf/title_img/2508.07662.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#data', '#optimization', '#rlhf'], 'emoji': '🏷️', 'ru': {'title': 'Эффективная классификация с нулевым обучением и адаптацией PPO', 'desc': 'GLiClass - это адаптация архитектуры GLiNER для задач классификации последовательностей. Метод обеспечивает высокую точность и эффективность, сравнимую с методами на основе эмбеддингов, сохраняя при этом гибкость для сценариев обучения с нулевым и малым количеством примеров. Авторы также адаптировали алгоритм проксимальной оптимизации политики (PPO) для многометочной классификации текста. Это позволяет обучать классификаторы в условиях ограниченных данных или с использованием обратной связи от человека.'}, 'en': {'title': 'GLiClass: Efficient Sequence Classification with Zero-Shot Flexibility', 'desc': 'GLiClass is a new method designed for sequence classification that builds on the GLiNER architecture. It effectively combines high accuracy and efficiency, making it suitable for zero-shot and few-shot learning scenarios. The method addresses the limitations of existing models, such as generative LLMs and cross-encoders, by providing a more flexible solution for dynamic classification needs. Additionally, it incorporates proximal policy optimization (PPO) to enhance multi-label text classification, particularly in situations with limited data or when utilizing human feedback.'}, 'zh': {'title': 'GLiClass：高效准确的序列分类新方法', 'desc': 'GLiClass是一种新方法，旨在提高序列分类的效率和准确性，特别是在数据稀缺的情况下。它基于GLiNER架构，能够在零样本和少样本学习场景中表现出色。我们还将近端策略优化（PPO）应用于多标签文本分类，使得模型能够从人类反馈中学习。该方法在处理复杂的逻辑和语义约束时，展现了良好的灵活性和效率。'}}}, {'id': 'https://huggingface.co/papers/2508.06601', 'title': 'Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant\n  Safeguards into Open-Weight LLMs', 'url': 'https://huggingface.co/papers/2508.06601', 'abstract': 'Data filtering during pretraining enhances LLM resistance to adversarial fine-tuning attacks without degrading unrelated capabilities, offering a promising defense mechanism for open-weight AI systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-weight AI systems offer unique benefits, including enhanced transparency, open research, and decentralized access. However, they are vulnerable to tampering attacks which can efficiently elicit harmful behaviors by modifying weights or activations. Currently, there is not yet a robust science of open-weight model risk management. Existing safety fine-tuning methods and other post-training techniques have struggled to make LLMs resistant to more than a few dozen steps of adversarial fine-tuning. In this paper, we investigate whether filtering text about dual-use topics from training data can prevent unwanted capabilities and serve as a more tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable data filtering and show that it offers a tractable and effective method for minimizing biothreat proxy knowledge in LLMs. We pretrain multiple 6.9B-parameter models from scratch and find that they exhibit substantial resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M tokens of biothreat-related text -- outperforming existing post-training baselines by over an order of magnitude -- with no observed degradation to unrelated capabilities. However, while filtered models lack internalized dangerous knowledge, we find that they can still leverage such information when it is provided in context (e.g., via search tool augmentation), demonstrating a need for a defense-in-depth approach. Overall, these findings help to establish pretraining data curation as a promising layer of defense for open-weight AI systems.', 'score': 4, 'issue_id': 5294, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '4d2d99e849d503f3', 'authors': ["Kyle O'Brien", 'Stephen Casper', 'Quentin Anthony', 'Tomek Korbak', 'Robert Kirk', 'Xander Davies', 'Ishan Mishra', 'Geoffrey Irving', 'Yarin Gal', 'Stella Biderman'], 'affiliations': ['EleutherAI', 'OATML, University of Oxford', 'UK AI Security Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.06601.jpg', 'data': {'categories': ['#training', '#open_source', '#security', '#data'], 'emoji': '🛡️', 'ru': {'title': 'Фильтрация данных как защита от атак на открытые ИИ-системы', 'desc': 'Статья исследует возможность фильтрации текстов о двойном назначении из обучающих данных для предотвращения нежелательных способностей в языковых моделях. Авторы разработали многоэтапный конвейер для масштабируемой фильтрации данных и продемонстрировали его эффективность в минимизации знаний о биоугрозах в больших языковых моделях. Эксперименты показали, что предобученные на отфильтрованных данных модели демонстрируют значительную устойчивость к атакам состязательной дообучения, превосходя существующие базовые показатели более чем на порядок. Однако отфильтрованные модели все еще могут использовать опасную информацию, предоставленную в контексте, что указывает на необходимость многоуровневого подхода к защите.'}, 'en': {'title': 'Data Filtering: A Shield for Open-Weight AI Systems', 'desc': "This paper explores how filtering training data can improve the resilience of large language models (LLMs) against adversarial fine-tuning attacks. By removing text related to dual-use topics, the authors create a multi-stage data filtering pipeline that enhances the models' resistance without harming their unrelated capabilities. The study shows that pretrained models can withstand significant adversarial attacks, outperforming existing methods by a large margin. However, the research also highlights that while these models do not retain dangerous knowledge, they can still respond to such information when presented in context, indicating the need for comprehensive defense strategies."}, 'zh': {'title': '数据过滤提升LLM对抗攻击的防御能力', 'desc': '本文探讨了在预训练阶段进行数据过滤如何增强大型语言模型（LLM）对对抗性微调攻击的抵抗力，同时不影响其其他能力。研究表明，通过过滤与双重用途相关的文本，可以有效防止模型学习不必要的能力，从而提供更强的防护机制。我们提出了一种多阶段的数据过滤流程，经过实验证明，该方法在抵御对抗性微调攻击方面表现优异，且未观察到对无关能力的降级。总体而言，这些发现为开放权重的人工智能系统建立了一个有前景的防御层。'}}}, {'id': 'https://huggingface.co/papers/2508.05954', 'title': 'Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with\n  Patch-level CLIP Latents', 'url': 'https://huggingface.co/papers/2508.05954', 'abstract': "Bifrost-1 integrates pretrained multimodal LLMs and diffusion models using patch-level CLIP embeddings to enable efficient high-fidelity image generation with strong multimodal reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices.", 'score': 4, 'issue_id': 5300, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '153c662d76f99960', 'authors': ['Han Lin', 'Jaemin Cho', 'Amir Zadeh', 'Chuan Li', 'Mohit Bansal'], 'affiliations': ['Lambda', 'UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2508.05954.jpg', 'data': {'categories': ['#games', '#training', '#multimodal', '#diffusion', '#cv'], 'emoji': '🌈', 'ru': {'title': 'Bifrost-1: Мост между языком и изображением для эффективной мультимодальной генерации', 'desc': 'Bifrost-1 - это унифицированная система, объединяющая предобученные мультимодальные языковые модели (MLLM) и диффузионные модели с использованием CLIP-эмбеддингов изображений на уровне патчей. Эта интеграция позволяет эффективно генерировать высококачественные изображения с сохранением мультимодальных рассуждений. Система использует легковесную адаптацию ControlNet для диффузионной модели и визуальную ветвь генерации для MLLM. Эксперименты показывают, что Bifrost-1 достигает сопоставимых или лучших результатов по визуальному качеству и мультимодальному пониманию по сравнению с предыдущими методами, при значительно меньших вычислительных затратах на обучение.'}, 'en': {'title': 'Bifrost-1: Bridging Language and Vision for Efficient Image Generation', 'desc': "Bifrost-1 is a new framework that combines pretrained multimodal large language models (MLLMs) with diffusion models to generate high-quality images while maintaining strong reasoning abilities. It uses patch-level CLIP embeddings, which are image representations that align well with the MLLM's visual encoder, to improve efficiency in image generation. By integrating these embeddings into the diffusion model and adapting its ControlNet, Bifrost-1 allows for controllable image generation without extensive retraining. The results show that Bifrost-1 performs as well or better than existing methods, achieving high visual fidelity and multimodal understanding with reduced training costs."}, 'zh': {'title': '高效的多模态图像生成框架Bifrost-1', 'desc': 'Bifrost-1 是一个统一框架，旨在将预训练的多模态大语言模型（MLLMs）与扩散模型结合起来，以实现高保真图像生成。该框架使用基于补丁的 CLIP 图像嵌入作为潜在变量，这些嵌入与 MLLM 的 CLIP 视觉编码器自然对齐。通过轻量级的 ControlNet 适配，Bifrost-1 将这些图像嵌入集成到扩散模型中，同时保留 MLLM 的多模态推理能力。实验结果表明，Bifrost-1 在视觉保真度和多模态理解方面的表现与之前的方法相当或更好，同时训练计算成本显著降低。'}}}, {'id': 'https://huggingface.co/papers/2508.06059', 'title': 'Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System', 'url': 'https://huggingface.co/papers/2508.06059', 'abstract': 'Fact2Fiction is a poisoning attack framework that targets agentic fact-checking systems by exploiting their decomposition strategy and justifications, achieving higher attack success rates than existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art fact-checking systems combat misinformation at scale by employing autonomous LLM-based agents to decompose complex claims into smaller sub-claims, verify each sub-claim individually, and aggregate the partial results to produce verdicts with justifications (explanatory rationales for the verdicts). The security of these systems is crucial, as compromised fact-checkers, which tend to be easily underexplored, can amplify misinformation. This work introduces Fact2Fiction, the first poisoning attack framework targeting such agentic fact-checking systems. Fact2Fiction mirrors the decomposition strategy and exploits system-generated justifications to craft tailored malicious evidences that compromise sub-claim verification. Extensive experiments demonstrate that Fact2Fiction achieves 8.9\\%--21.2\\% higher attack success rates than state-of-the-art attacks across various poisoning budgets. Fact2Fiction exposes security weaknesses in current fact-checking systems and highlights the need for defensive countermeasures.', 'score': 3, 'issue_id': 5304, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': '77839512cae33ea5', 'authors': ['Haorui He', 'Yupeng Li', 'Bin Benjamin Zhu', 'Dacheng Wen', 'Reynold Cheng', 'Francis C. M. Lau'], 'affiliations': ['Department of Computer Science, The University of Hong Kong', 'Department of Interactive Media, Hong Kong Baptist University', 'Microsoft Corporation'], 'pdf_title_img': 'assets/pdf/title_img/2508.06059.jpg', 'data': {'categories': ['#security', '#agents'], 'emoji': '🕵️', 'ru': {'title': 'Обман автоматизированных фактчекеров: новый метод атаки на ИИ-системы проверки информации', 'desc': 'Fact2Fiction - это фреймворк атаки отравления, нацеленный на агентные системы проверки фактов. Он использует их стратегию декомпозиции и обоснования для достижения более высоких показателей успешности атаки по сравнению с существующими методами. Fact2Fiction создает вредоносные доказательства, которые компрометируют проверку подутверждений. Эксперименты показывают, что Fact2Fiction достигает на 8.9%-21.2% более высоких показателей успешности атаки по сравнению с современными атаками при различных бюджетах отравления.'}, 'en': {'title': 'Fact2Fiction: Unmasking Vulnerabilities in Fact-Checking Systems', 'desc': 'Fact2Fiction is a novel framework designed to launch poisoning attacks on agentic fact-checking systems, which use large language models (LLMs) to break down complex claims into smaller parts for verification. By mimicking the decomposition strategy and leveraging the justifications provided by these systems, Fact2Fiction creates targeted malicious evidence that can mislead the verification process. The framework has been shown to achieve significantly higher success rates in attacks compared to existing methods, indicating vulnerabilities in current fact-checking approaches. This research underscores the importance of enhancing security measures to protect against such sophisticated attacks on misinformation systems.'}, 'zh': {'title': '揭示事实检查系统的安全漏洞', 'desc': 'Fact2Fiction是一个针对自主事实检查系统的毒化攻击框架，利用其分解策略和理由进行攻击。该框架通过模仿系统的分解策略，生成定制的恶意证据，从而破坏子声明的验证过程。实验表明，Fact2Fiction的攻击成功率比现有方法高出8.9%到21.2%。这项研究揭示了当前事实检查系统的安全漏洞，并强调了需要防御措施的重要性。'}}}, {'id': 'https://huggingface.co/papers/2508.03542', 'title': 'Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations\n  and Sentences', 'url': 'https://huggingface.co/papers/2508.03542', 'abstract': 'A new open-source dataset and models for converting spoken mathematical expressions into LaTeX improve accuracy over existing benchmarks, supporting both equations and sentences in multiple languages.  \t\t\t\t\tAI-generated summary \t\t\t\t Conversion of spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has a limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR post-correction models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by a substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition.', 'score': 3, 'issue_id': 5300, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'f7ca16433cbc8b8e', 'authors': ['Dmitrii Korzh', 'Dmitrii Tarasov', 'Artyom Iudin', 'Elvir Karimov', 'Matvey Skripkin', 'Nikita Kuzmin', 'Andrey Kuznetsov', 'Oleg Y. Rogov', 'Ivan Oseledets'], 'affiliations': ['AIRI', 'HSE Moscow, Russia', 'MTUCI', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2508.03542.jpg', 'data': {'categories': ['#science', '#multimodal', '#benchmark', '#dataset', '#multilingual', '#open_source'], 'emoji': '🧮', 'ru': {'title': 'Прорыв в распознавании устной математики: от речи к LaTeX', 'desc': 'Представлен новый открытый набор данных и модели для преобразования устных математических выражений в LaTeX, улучшающие точность по сравнению с существующими эталонами. Набор данных содержит более 66 000 аудиозаписей уравнений и предложений на английском и русском языках из различных научных областей. Применяются модели постобработки ASR и аудио-языковые модели, демонстрирующие сопоставимые результаты на бенчмарке MathSpeech и значительное улучшение на новом бенчмарке S2L-equations. Работа закладывает основу для будущих достижений в мультимодальном ИИ с акцентом на распознавание математического контента.'}, 'en': {'title': 'Transforming Speech to LaTeX: A Leap in Mathematical Recognition', 'desc': 'This paper introduces a new open-source dataset and models designed to convert spoken mathematical expressions into LaTeX, significantly enhancing accuracy compared to existing methods. The dataset includes over 66,000 audio samples in English and Russian, addressing the challenges of ambiguity in speech and the need for structured symbolic representation. The authors demonstrate that their models achieve lower character error rates on both equations and sentences, establishing new benchmarks for mathematical recognition tasks. This work aims to advance multimodal AI applications in educational and research settings by improving the transcription of mathematical content.'}, 'zh': {'title': '口语数学表达转换的突破性进展', 'desc': '本研究提出了一个新的开源数据集和模型，用于将口语数学表达转换为LaTeX，显著提高了准确性。该数据集包含超过66,000个经过人工标注的数学方程和句子的音频样本，支持英语和俄语。研究中使用了自动语音识别（ASR）后校正模型和音频语言模型，展示了在数学方程转换上的优越性能。此项工作为未来多模态人工智能的发展奠定了基础，特别是在数学内容识别方面。'}}}, {'id': 'https://huggingface.co/papers/2508.03365', 'title': 'When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with\n  Benign Inputs', 'url': 'https://huggingface.co/papers/2508.03365', 'abstract': 'WhisperInject uses RL-PGD and PGD to create imperceptible audio perturbations that manipulate large language models into generating harmful content.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models become increasingly integrated into daily life, audio has emerged as a key interface for human-AI interaction. However, this convenience also introduces new vulnerabilities, making audio a potential attack surface for adversaries. Our research introduces WhisperInject, a two-stage adversarial audio attack framework that can manipulate state-of-the-art audio language models to generate harmful content. Our method uses imperceptible perturbations in audio inputs that remain benign to human listeners. The first stage uses a novel reward-based optimization method, Reinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the target model to circumvent its own safety protocols and generate harmful native responses. This native harmful response then serves as the target for Stage 2, Payload Injection, where we use Projected Gradient Descent (PGD) to optimize subtle perturbations that are embedded into benign audio carriers, such as weather queries or greeting messages. Validated under the rigorous StrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation framework, our experiments demonstrate a success rate exceeding 86% across Qwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a new class of practical, audio-native threats, moving beyond theoretical exploits to reveal a feasible and covert method for manipulating AI behavior.', 'score': 2, 'issue_id': 5298, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'e24fc34d5fae39a9', 'authors': ['Bodam Kim', 'Hiskias Dingeto', 'Taeyoun Kwon', 'Dasol Choi', 'DongGeon Lee', 'Haon Park', 'JaeHoon Lee', 'Jongho Shin'], 'affiliations': ['AIM Intelligence', 'LG Electronics', 'POSTECH', 'Seoul National University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2508.03365.jpg', 'data': {'categories': ['#audio', '#security', '#hallucinations', '#rl', '#rlhf'], 'emoji': '🎧', 'ru': {'title': 'Невидимая угроза: манипуляция аудио ИИ', 'desc': 'Статья представляет WhisperInject - фреймворк для атак на аудио языковые модели. Используя методы RL-PGD и PGD, создаются незаметные для человека искажения в аудио, которые заставляют модели генерировать вредоносный контент. Первый этап обходит протоколы безопасности модели, второй внедряет вредоносную нагрузку в безобидные аудиозапросы. Эксперименты показали высокую эффективность атак на современные мультимодальные языковые модели.'}, 'en': {'title': 'WhisperInject: Covert Audio Attacks on AI Models', 'desc': 'WhisperInject is a novel framework that exploits vulnerabilities in large language models through adversarial audio attacks. It employs a two-stage process where the first stage uses Reinforcement Learning with Projected Gradient Descent (RL-PGD) to manipulate the model into bypassing its safety measures. The second stage, Payload Injection, utilizes Projected Gradient Descent (PGD) to create subtle audio perturbations that are imperceptible to humans but can lead the model to generate harmful content. This research highlights a significant security risk in AI systems that rely on audio inputs, demonstrating a practical method for adversarial manipulation.'}, 'zh': {'title': '音频攻击新威胁：操控AI生成有害内容', 'desc': 'WhisperInject 是一种两阶段的对抗性音频攻击框架，旨在操控大型语言模型生成有害内容。该方法通过在音频输入中添加不可察觉的扰动，绕过模型的安全协议。第一阶段使用基于奖励的优化方法（RL-PGD），引导目标模型生成有害的原生响应。第二阶段则通过投影梯度下降（PGD）优化微妙的扰动，将其嵌入到无害的音频载体中，如天气查询或问候信息。'}}}, {'id': 'https://huggingface.co/papers/2507.23701', 'title': 'TextQuests: How Good are LLMs at Text-Based Video Games?', 'url': 'https://huggingface.co/papers/2507.23701', 'abstract': "TextQuests evaluates AI agents' intrinsic reasoning and problem-solving capabilities in long, exploratory, text-based interactive fiction environments without external tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at https://textquests.ai.", 'score': 1, 'issue_id': 5308, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '883a3be44b91a29a', 'authors': ['Long Phan', 'Mantas Mazeika', 'Andy Zou', 'Dan Hendrycks'], 'affiliations': ['Carnegie Mellon University', 'Center for AI Safety', 'Gray Swan AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.23701.jpg', 'data': {'categories': ['#games', '#reasoning', '#benchmark', '#long_context', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Испытание ИИ в виртуальных квестах: оценка самостоятельного мышления', 'desc': 'TextQuests - это новый бенчмарк для оценки способностей ИИ-агентов к самостоятельному рассуждению и решению проблем в сложных текстовых интерактивных средах. Он основан на наборе текстовых приключенческих игр Infocom, которые требуют длительного взаимодействия и сотен точных действий для решения. Бенчмарк оценивает внутренние способности языковых моделей к рассуждению в длинном контексте, исключая использование внешних инструментов. TextQuests призван стимулировать разработку агентов, способных к более надежному автономному рассуждению в исследовательских средах.'}, 'en': {'title': "Unlocking AI's Problem-Solving Potential in Interactive Fiction", 'desc': "TextQuests is a benchmark designed to evaluate AI agents' reasoning and problem-solving skills in complex, text-based interactive fiction environments. Unlike traditional benchmarks that focus on structured tasks or tool use, TextQuests emphasizes the agent's ability to navigate long, exploratory scenarios autonomously. This benchmark uses classic interactive fiction games, which require extensive actions and sustained reasoning, to simulate real-world challenges. By restricting the use of external tools, TextQuests aims to enhance the development of AI agents capable of intrinsic reasoning over extended contexts."}, 'zh': {'title': '探索性环境中的内在推理能力评估', 'desc': 'TextQuests 是一个评估 AI 代理在复杂互动环境中内在推理和问题解决能力的基准。它基于 Infocom 的互动小说游戏，旨在测试 AI 代理在长时间和不断扩展的上下文中进行自主推理的能力。与现有的基准不同，TextQuests 不允许使用外部工具，专注于 AI 代理在探索性环境中的内在长时段推理能力。通过这种方式，TextQuests 促进了能够进行更强大内在推理的代理的发展。'}}}, {'id': 'https://huggingface.co/papers/2508.04026', 'title': 'VeriGUI: Verifiable Long-Chain GUI Dataset', 'url': 'https://huggingface.co/papers/2508.04026', 'abstract': 'VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-based computer tasks, with the potential to revolutionize human-computer interaction. Despite encouraging results, existing efforts mainly focus on short-term interactions and rely on outcome-only verification, thereby limiting their scalability in real-world GUI applications that demand long-horizon task decomposition and execution. In this work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents operating in realistic computer environments. Our dataset emphasizes two critical dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps, explicitly designed to allow any subtask to serve as a valid starting point; and (2) subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent. The dataset consists of GUI task trajectories across both desktop and web, annotated by human experts. Extensive experiments on VeriGUI using various agents with different foundation models reveal significant performance gaps in handling long-horizon tasks, highlighting the need for more robust planning and decision-making capabilities in GUI agents.', 'score': 116, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '2692487ee60e017e', 'authors': ['Shunyu Liu', 'Minghao Liu', 'Huichi Zhou', 'Zhenyu Cui', 'Yang Zhou', 'Yuhao Zhou', 'Wendong Fan', 'Ge Zhang', 'Jiajun Shi', 'Weihao Xuan', 'Jiaxing Huang', 'Shuang Luo', 'Fang Wu', 'Heli Qi', 'Qingcheng Zeng', 'Ziqi Ren', 'Jialiang Gao', 'Jindi Lv', 'Junjie Wang', 'Aosong Feng', 'Heng Zhou', 'Wangchunshu Zhou', 'Zhenfei Yin', 'Wenlong Zhang', 'Guohao Li', 'Wenhao Yu', 'Irene Li', 'Lei Ma', 'Lei Bai', 'Qunshu Lin', 'Mingli Song', 'Dacheng Tao'], 'affiliations': ['VeriGUI Team'], 'pdf_title_img': 'assets/pdf/title_img/2508.04026.jpg', 'data': {'categories': ['#games', '#agents', '#dataset', '#long_context'], 'emoji': '🖥️', 'ru': {'title': 'VeriGUI: Новый стандарт для оценки долгосрочных GUI-агентов', 'desc': 'VeriGUI - это новый набор данных для оценки GUI-агентов в долгосрочных задачах, акцентирующий внимание на сложности длинных цепочек действий и верифицируемости на уровне подзадач. Датасет состоит из траекторий GUI-задач для десктопных и веб-приложений, аннотированных экспертами. Он позволяет разрабатывать и оценивать универсальные GUI-агенты, работающие в реалистичных компьютерных средах. Эксперименты на VeriGUI с использованием различных агентов и языковых моделей выявили значительные пробелы в производительности при работе с долгосрочными задачами.'}, 'en': {'title': 'Empowering GUI Agents for Complex Tasks with VeriGUI', 'desc': 'VeriGUI is a new dataset created to test GUI agents on complex tasks that require many steps. It focuses on breaking down tasks into smaller, manageable subtasks that can be verified individually. This approach allows agents to start from any subtask, making it easier to explore different strategies. The dataset includes detailed task examples from desktop and web environments, showing that current agents struggle with long-term planning and decision-making.'}, 'zh': {'title': 'VeriGUI：长时间任务中的智能体评估新标准', 'desc': 'VeriGUI是一个新颖的数据集，用于评估在长时间任务中执行图形用户界面（GUI）操作的智能体。该数据集强调了长链复杂性和子任务级可验证性，允许任务被分解为数百个相互依赖的子任务。通过这种方式，任何子任务都可以作为有效的起点，促进了多样化的探索策略。实验结果显示，现有智能体在处理长时间任务时存在显著性能差距，表明需要更强大的规划和决策能力。'}}}, {'id': 'https://huggingface.co/papers/2508.01191', 'title': 'Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens', 'url': 'https://huggingface.co/papers/2508.01191', 'abstract': 'CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.', 'score': 107, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '427ac75c7123b50a', 'authors': ['Chengshuai Zhao', 'Zhen Tan', 'Pingchuan Ma', 'Dawei Li', 'Bohan Jiang', 'Yancheng Wang', 'Yingzhen Yang', 'Huan Liu'], 'affiliations': ['Arizona State University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.01191.jpg', 'data': {'categories': ['#reasoning', '#data', '#training', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Ограниченность CoT-рассуждений в LLM: мираж вне распределения обучающих данных', 'desc': 'Исследование показывает, что рассуждения по цепочке (CoT) в больших языковых моделях (LLM) ограничены расхождением между распределениями обучающих и тестовых данных. Авторы разработали среду DataAlchemy для изучения CoT-рассуждений по трем измерениям: задача, длина и формат. Результаты демонстрируют, что CoT-рассуждения неустойчивы и исчезают при выходе за пределы распределения обучающих данных. Это исследование подчеркивает сложность достижения подлинных и обобщаемых рассуждений в LLM.'}, 'en': {'title': 'Unmasking the Fragility of CoT Reasoning in LLMs', 'desc': 'This paper examines the limitations of Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) by analyzing the impact of distribution discrepancies between training and test data. It suggests that while CoT prompting can enhance performance by mimicking human-like reasoning, this reasoning may not be as robust as it seems. The authors introduce a framework called DataAlchemy to systematically investigate how CoT reasoning varies across different tasks, lengths, and formats under controlled conditions. Their findings indicate that CoT reasoning is fragile and often fails when faced with data that deviates from what the model was trained on, highlighting the need for more reliable reasoning mechanisms in LLMs.'}, 'zh': {'title': '链式思维推理的局限性与挑战', 'desc': '本文探讨了链式思维（CoT）推理在大型语言模型（LLM）中的局限性，特别是训练数据与测试数据之间的分布差异对其影响。研究表明，CoT推理并不是一种稳健的推理形式，因为它的有效性受到训练数据和测试查询之间分布差异的限制。通过设计一个名为DataAlchemy的控制环境，作者系统性地分析了CoT推理在不同任务、长度和格式下的表现。结果显示，当CoT推理超出训练分布时，其效果会显著下降，揭示了实现真正可推广推理的持续挑战。'}}}, {'id': 'https://huggingface.co/papers/2508.02694', 'title': 'Efficient Agents: Building Effective Agents While Reducing Cost', 'url': 'https://huggingface.co/papers/2508.02694', 'abstract': 'A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from 0.398 to 0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.', 'score': 48, 'issue_id': 5222, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': '0c518e4f5949dae3', 'authors': ['Ningning Wang', 'Xavier Hu', 'Pai Liu', 'He Zhu', 'Yue Hou', 'Heyuan Huang', 'Shengyu Zhang', 'Jian Yang', 'Jiaheng Liu', 'Ge Zhang', 'Changwang Zhang', 'Jun Wang', 'Yuchen Eleanor Jiang', 'Wangchunshu Zhou'], 'affiliations': ['OPPO', 'OPPO-PersonalAI'], 'pdf_title_img': 'assets/pdf/title_img/2508.02694.jpg', 'data': {'categories': ['#agents', '#optimization', '#training', '#open_source', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Оптимизация агентских систем: баланс эффективности и затрат', 'desc': 'Исследование посвящено анализу компромисса между эффективностью и результативностью в системах агентов, управляемых большими языковыми моделями (LLM). Авторы изучают оптимальную структуру агентских фреймворков для снижения затрат при сохранении производительности. Проводится эмпирический анализ на бенчмарке GAIA, оценивающий влияние выбора LLM, дизайна фреймворков и стратегий масштабирования. На основе результатов разработан новый фреймворк Efficient Agents, сохраняющий 96.7% производительности ведущего open-source решения при снижении операционных затрат на 28.4%.'}, 'en': {'title': 'Balancing Cost and Performance in AI Agents', 'desc': 'This paper explores how to balance efficiency and effectiveness in systems powered by Large Language Models (LLMs). It identifies the optimal design for agent frameworks that can lower costs while still performing well. The study answers key questions about task complexity, the diminishing returns of adding modules, and how to enhance efficiency through better design. The results show that the proposed Efficient Agents framework maintains high performance while significantly reducing operational costs, making AI solutions more accessible and sustainable.'}, 'zh': {'title': '高效代理系统：降低成本与保持性能的平衡', 'desc': '本研究探讨了大型语言模型（LLM）驱动的代理系统中的效率与效果之间的权衡，旨在设计出既能降低成本又能保持性能的最佳代理框架。我们分析了代理任务的复杂性、额外模块的边际效益以及通过高效代理框架设计所能获得的效率提升。通过对GAIA基准的实证分析，我们评估了LLM骨干选择、代理框架设计和测试时扩展策略的影响。研究结果表明，开发高效代理系统可以在保持高性能的同时显著降低运营成本，推动AI解决方案的可及性和可持续性。'}}}, {'id': 'https://huggingface.co/papers/2508.04700', 'title': 'SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience', 'url': 'https://huggingface.co/papers/2508.04700', 'abstract': "SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.", 'score': 37, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '6c7450255f7c28bc', 'authors': ['Zeyi Sun', 'Ziyu Liu', 'Yuhang Zang', 'Yuhang Cao', 'Xiaoyi Dong', 'Tong Wu', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.04700.jpg', 'data': {'categories': ['#agents', '#agi', '#open_source', '#optimization', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Самообучающиеся агенты для освоения нового ПО', 'desc': 'SEAgent - это фреймворк для автономного обучения агентов использованию компьютерного программного обеспечения. Он позволяет агентам исследовать новое ПО через экспериментальное обучение и выполнение автоматически генерируемых заданий возрастающей сложности. SEAgent использует модель состояния мира для оценки траектории действий и генератор учебного плана для создания разнообразных задач. Обновление политики агента происходит через имитационное обучение на неудачных действиях и оптимизацию на успешных, что позволяет достичь значительного улучшения производительности по сравнению с существующими методами.'}, 'en': {'title': 'Empowering Agents to Learn and Evolve Autonomously', 'desc': 'SEAgent is a self-evolving framework designed for computer-use agents (CUAs) to autonomously learn and master new software through experiential learning. It allows agents to explore unfamiliar software environments and learn from their experiences by tackling tasks that increase in complexity. The framework includes a World State Model for assessing agent performance and a Curriculum Generator for creating diverse tasks. By integrating insights from specialist agents, SEAgent develops a robust generalist CUA that outperforms traditional methods, achieving a notable increase in success rates across various software environments.'}, 'zh': {'title': 'SEAgent：自主进化的智能体框架', 'desc': 'SEAgent是一种自我进化的智能体框架，能够使计算机使用代理通过体验学习自主掌握新软件。该框架通过逐步的任务课程，帮助代理在没有人类标注的情况下，探索和学习陌生的软件环境。SEAgent设计了世界状态模型和课程生成器，以便代理能够通过试错学习不断提高其能力。最终，SEAgent的表现超越了多个专门化代理的组合，显示出其在新软件环境中的优越性。'}}}, {'id': 'https://huggingface.co/papers/2508.03501', 'title': 'Training Long-Context, Multi-Turn Software Engineering Agents with\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.03501', 'abstract': "Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation.   To bridge this gap, we demonstrate the successful application of RL to this general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world software engineering tasks. Our approach increases the agent's success rate on the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward building more capable autonomous agents for complex real-world problems based on open models.", 'score': 29, 'issue_id': 5228, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '9e2fbad63802fc98', 'authors': ['Alexander Golubev', 'Maria Trofimova', 'Sergei Polezhaev', 'Ibragim Badertdinov', 'Maksim Nekrashevich', 'Anton Shevtsov', 'Simon Karasik', 'Sergey Abramov', 'Andrei Andriushchenko', 'Filipp Fisin', 'Sergei Skvortsov', 'Boris Yangel'], 'affiliations': ['Humanoid', 'Nebius AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.03501.jpg', 'data': {'categories': ['#open_source', '#agents', '#benchmark', '#optimization', '#rl', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'RL для LLM: прорыв в многоходовом взаимодействии для разработки ПО', 'desc': 'Исследование демонстрирует успешное применение обучения с подкреплением (RL) к большим языковым моделям (LLM) для решения многоходовых задач в области разработки программного обеспечения. Используя модифицированный алгоритм DAPO, авторы обучили агента на основе Qwen2.5-72B-Instruct для решения реальных задач в этой сфере. Результаты показывают значительное улучшение успешности агента на бенчмарке SWE-bench Verified с 20% до 39%. На бенчмарке SWE-rebench агент показал результаты на уровне или лучше ведущих открытых моделей, открывая путь к созданию более способных автономных агентов для сложных реальных задач.'}, 'en': {'title': 'Empowering Language Models with Reinforcement Learning for Real-World Software Engineering', 'desc': 'This paper explores the application of Reinforcement Learning (RL) to Large Language Models (LLMs) in multi-turn interactions, particularly in software engineering tasks. Unlike previous studies that focused on single-turn problems, this research addresses the need for agents to operate in environments that provide feedback after each action. The authors introduce a modified Decoupled Advantage Policy Optimization (DAPO) algorithm to train an agent, Qwen2.5-72B-Instruct, achieving a significant improvement in success rates on software engineering benchmarks. This work demonstrates the potential of RL in enhancing the capabilities of autonomous agents for complex, real-world applications without relying on teacher models.'}, 'zh': {'title': '强化学习助力大型语言模型解决复杂任务', 'desc': '本研究探讨了强化学习（RL）在大型语言模型（LLM）中的应用，特别是在需要多轮交互的真实世界任务中。我们提出了一种改进的解耦优势策略优化（DAPO）算法，成功训练了一个基于Qwen2.5-72B-Instruct的智能体，以解决软件工程（SWE）任务。实验结果显示，该智能体在SWE-bench Verified基准测试中的成功率从20%提升至39%。我们的研究为构建更强大的自主智能体提供了可行的路径，能够应对复杂的现实问题。'}}}, {'id': 'https://huggingface.co/papers/2508.04280', 'title': 'Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success', 'url': 'https://huggingface.co/papers/2508.04280', 'abstract': 'A lightweight, hyperparameter-free RL algorithm, VL-DAC, enables VLMs to learn generalized policies from inexpensive simulators, improving performance on real-world benchmarks without sacrificing image understanding accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Interactive multimodal agents must convert raw visual observations into coherent sequences of language-conditioned actions -- a capability that current vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL) efforts could, in principle, endow VLMs with such skills, but they have seldom tested whether the learned behaviours generalize beyond their training simulators, and they depend either on brittle hyperparameter tuning or on dense-reward environments with low state variability. We introduce Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight, hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens while learning value only at the environment-step level: an arrangement, to our knowledge, not previously explored for large VLMs or LLMs. This simple decoupling removes unstable weighting terms and yields faster, more reliable convergence. Training a single VLM with VL-DAC in one inexpensive simulator at a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies that generalize widely: +50\\% relative on BALROG (game-centric agentic control), +5\\% relative on the hardest part of VSI-Bench (spatial planning), and +2\\% on VisualWebBench (web navigation), all without degrading general image understanding accuracy. These results provide the first evidence that a simple RL algorithm can train VLMs entirely in cheap synthetic worlds while delivering measurable gains on real-image agentic, spatial-reasoning, and web-navigation benchmarks.', 'score': 26, 'issue_id': 5227, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '007849b63760d8ee', 'authors': ['George Bredis', 'Stanislav Dereka', 'Viacheslav Sinii', 'Ruslan Rakhimov', 'Daniil Gavrilov'], 'affiliations': ['T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.04280.jpg', 'data': {'categories': ['#rl', '#training', '#transfer_learning', '#synthetic', '#games', '#rlhf', '#multimodal', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Простое RL-обучение для универсальных визуально-языковых агентов', 'desc': 'VL-DAC - это новый алгоритм обучения с подкреплением для мультимодальных агентов на основе моделей компьютерного зрения и языка. Он позволяет обучать модели в простых симуляторах, при этом улучшая их производительность на реальных задачах без потери точности понимания изображений. VL-DAC применяет обновления PPO к токенам действий, но обучает функцию ценности только на уровне шагов среды. Такой подход обеспечивает более быструю и надежную сходимость по сравнению с предыдущими методами.'}, 'en': {'title': 'VL-DAC: Simplifying RL for Enhanced Vision-Language Learning', 'desc': 'The paper presents VL-DAC, a novel reinforcement learning (RL) algorithm designed for vision-language models (VLMs) that operates without the need for hyperparameter tuning. This algorithm allows VLMs to learn effective policies from low-cost simulators, enhancing their performance on real-world tasks while maintaining image understanding accuracy. By decoupling the learning of action tokens and value estimation, VL-DAC achieves faster and more stable convergence compared to previous methods. The results demonstrate that training with VL-DAC leads to significant improvements in various benchmarks, showcasing its potential for developing multimodal agents capable of complex tasks.'}, 'zh': {'title': '轻量级强化学习算法，提升视觉语言模型性能', 'desc': '本文介绍了一种轻量级的强化学习算法VL-DAC，该算法无需超参数调整，能够使视觉语言模型（VLMs）从低成本的模拟器中学习通用策略。VL-DAC通过对动作令牌应用PPO更新，同时仅在环境步骤级别学习价值，从而实现了简单的解耦，避免了不稳定的加权项，促进了更快、更可靠的收敛。通过在单个低成本模拟器中训练VLM，VL-DAC能够在多个真实世界基准上显著提高性能，而不影响图像理解的准确性。这些结果首次证明了简单的强化学习算法可以在廉价的合成环境中完全训练VLM，并在真实图像的代理控制、空间推理和网页导航基准上取得可测量的提升。'}}}, {'id': 'https://huggingface.co/papers/2508.03680', 'title': 'Agent Lightning: Train ANY AI Agents with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.03680', 'abstract': "Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.", 'score': 21, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '78a8398db0f71f63', 'authors': ['Xufang Luo', 'Yuge Zhang', 'Zhiyuan He', 'Zilong Wang', 'Siyun Zhao', 'Dongsheng Li', 'Luna K. Qiu', 'Yuqing Yang'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.03680.jpg', 'data': {'categories': ['#agents', '#rag', '#games', '#math', '#training', '#optimization', '#rl'], 'emoji': '⚡', 'ru': {'title': 'Agent Lightning: универсальный фреймворк для обучения ИИ-агентов', 'desc': 'Agent Lightning - это гибкий фреймворк для обучения с подкреплением больших языковых моделей в различных агентах. Он использует иерархический алгоритм обучения с подкреплением и отделяет выполнение от обучения для обработки сложных взаимодействий. Фреймворк позволяет интегрироваться с существующими агентами, разработанными различными способами, практически без изменения кода. Эксперименты показали стабильные улучшения в задачах text-to-SQL, генерации с использованием извлечения информации и использования математических инструментов.'}, 'en': {'title': 'Decoupling Training and Execution for Enhanced AI Agent Performance', 'desc': 'Agent Lightning is a versatile framework designed for training Large Language Models (LLMs) using Reinforcement Learning (RL) techniques. It separates the training process from agent execution, allowing for easier integration with various existing AI agents without significant code changes. By modeling agent execution as a Markov decision process, it introduces a hierarchical RL algorithm called LightningRL, which effectively manages complex interactions and multi-agent scenarios. The framework has shown promising results in tasks like text-to-SQL and retrieval-augmented generation, indicating its effectiveness for real-world applications.'}, 'zh': {'title': 'Agent Lightning：灵活的智能体训练框架', 'desc': 'Agent Lightning是一个灵活的强化学习框架，旨在为各种智能体训练大型语言模型（LLMs）。它通过将执行与训练解耦，使用层次化的强化学习算法，能够处理复杂的交互逻辑。该框架允许与现有智能体的无缝集成，几乎不需要代码修改。实验结果表明，Agent Lightning在文本到SQL、增强生成和数学工具使用等任务中表现出稳定的持续改进，展示了其在实际智能体训练和部署中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.03159', 'title': 'CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction', 'url': 'https://huggingface.co/papers/2508.03159', 'abstract': "CoTox, a framework integrating LLMs with chain-of-thought reasoning, enhances multi-toxicity prediction by incorporating chemical structure data, biological pathways, and gene ontology terms, improving interpretability and predictive performance in drug development.  \t\t\t\t\tAI-generated summary \t\t\t\t Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.", 'score': 18, 'issue_id': 5222, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '229c53307b839ab7', 'authors': ['Jueon Park', 'Yein Park', 'Minju Song', 'Soyon Park', 'Donghyeon Lee', 'Seungheun Baek', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences, Seoul 04778, Republic of Korea', 'Department of Computer Science and Engineering, Korea University, Seoul 17035, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2508.03159.jpg', 'data': {'categories': ['#science', '#interpretability', '#healthcare', '#reasoning', '#data', '#multimodal'], 'emoji': '🧪', 'ru': {'title': 'CoTox: Интеллектуальное прогнозирование токсичности лекарств с помощью LLM', 'desc': 'CoTox - это новая система, которая объединяет большие языковые модели (LLM) с рассуждениями по цепочке мыслей для прогнозирования множественной токсичности лекарств. Она использует данные о химической структуре, биологических путях и термины генной онтологии для генерации интерпретируемых предсказаний токсичности. CoTox превосходит традиционные методы машинного обучения и глубокого обучения, улучшая интерпретируемость и предсказательную способность. Система демонстрирует потенциал для улучшения оценки безопасности лекарств на ранних стадиях разработки.'}, 'en': {'title': 'CoTox: Enhancing Drug Toxicity Prediction with LLMs and Chain-of-Thought Reasoning', 'desc': "CoTox is a new framework that combines large language models (LLMs) with chain-of-thought reasoning to predict multi-toxicity in drugs. It enhances the prediction process by integrating chemical structure data, biological pathways, and gene ontology terms, which helps in making the predictions more interpretable. By using step-by-step reasoning, CoTox improves the model's ability to understand complex biological mechanisms and organ-specific toxicities. The framework has shown superior performance compared to traditional machine learning and deep learning models, making it a valuable tool for early-stage drug safety assessment."}, 'zh': {'title': 'CoTox：提升药物毒性预测的智能框架', 'desc': 'CoTox是一个将大型语言模型（LLMs）与链式推理相结合的框架，旨在提高多种毒性预测的准确性。它通过整合化学结构数据、生物通路和基因本体术语，生成可解释的毒性预测。与传统的机器学习和深度学习模型相比，CoTox在药物开发中表现出更好的预测性能。该框架的设计使得毒性预测与生理反应相一致，展示了LLM框架在药物安全性评估中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.03905', 'title': 'Sotopia-RL: Reward Design for Social Intelligence', 'url': 'https://huggingface.co/papers/2508.03905', 'abstract': 'Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.', 'score': 16, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '815208c1d75b8f05', 'authors': ['Haofei Yu', 'Zhengyang Qi', 'Yining Zhao', 'Kolby Nottingham', 'Keyang Xuan', 'Bodhisattwa Prasad Majumder', 'Hao Zhu', 'Paul Pu Liang', 'Jiaxuan You'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Carnegie Mellon University', 'Massachusetts Institute of Technology', 'Stanford University', 'University of California Irvine', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2508.03905.jpg', 'data': {'categories': ['#games', '#alignment', '#rlhf', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Sotopia-RL: прорыв в обучении социальному интеллекту для ИИ', 'desc': 'Sotopia-RL - это новая система обучения с подкреплением для улучшения социального интеллекта больших языковых моделей. Она преобразует обратную связь в многомерные награды на уровне отдельных высказываний, что позволяет эффективнее обучать модели социальным задачам. Система решает проблемы частичной наблюдаемости и многомерности социальных взаимодействий, которые затрудняют применение классических методов обучения с подкреплением. Эксперименты показали, что Sotopia-RL значительно превосходит существующие подходы в решении социальных задач.'}, 'en': {'title': 'Enhancing Social Intelligence in LLMs with Sotopia-RL', 'desc': "Sotopia-RL is a new reinforcement learning framework designed to improve social intelligence in large language models (LLMs). It addresses challenges in social interactions, such as partial observability and multi-dimensionality, by refining feedback into more precise utterance-level, multi-dimensional rewards. This approach allows for better credit assignment to individual utterances, enhancing the model's ability to learn from social interactions. Experiments show that Sotopia-RL significantly outperforms existing methods in achieving social goals, demonstrating its effectiveness in training socially intelligent agents."}, 'zh': {'title': '提升社交智能的强化学习新框架', 'desc': 'Sotopia-RL是一种新颖的强化学习框架，旨在提升大型语言模型的社交智能。它通过将反馈细化为发言级别的多维奖励，来改善模型在社交任务中的表现。该框架解决了社交互动中的部分可观察性和多维性问题，使得模型能够更有效地学习复杂的社交策略。实验结果表明，Sotopia-RL在社交目标完成评分上达到了最先进的水平，显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2508.01858', 'title': 'Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents', 'url': 'https://huggingface.co/papers/2508.01858', 'abstract': 'A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent\'s capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent\'s processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent\'s conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner', 'score': 15, 'issue_id': 5220, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'ad30239b3abef884', 'authors': ['Yuhan Guo', 'Cong Guo', 'Aiwen Sun', 'Hongliang He', 'Xinyu Yang', 'Yue Lu', 'Yingji Zhang', 'Xuntao Guo', 'Dong Zhang', 'Jianzhuang Liu', 'Jiang Duan', 'Yijia Xiao', 'Liangjian Wen', 'Hai-Ming Xu', 'Yong Dai'], 'affiliations': ['Central South University', 'Fudan University', 'Harbin Institute of Technology', 'Hithink Research', 'Shanghai Jiao Tong University', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences', 'Southwestern University of Finance and Economics', 'University of Adelaide', 'University of California, Los Angeles', 'University of Manchester', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01858.jpg', 'data': {'categories': ['#agents', '#benchmark', '#dataset', '#reasoning', '#open_source'], 'emoji': '🕸️', 'ru': {'title': 'Когнитивная структура для веб-агентов нового поколения', 'desc': 'Статья представляет новую структуру для веб-агентов, разделяющую их возможности на изучение контента знаний и когнитивные процессы. Авторы предлагают Web-CogKnowledge Framework, категоризирующий знания как фактические, концептуальные и процедурные. Для облегчения приобретения знаний создан структурированный набор данных Web-CogDataset из 14 реальных веб-сайтов. Разработан новый агент Web-CogReasoner, использующий знание-ориентированную цепочку рассуждений (Chain-of-Thought), показывающий превосходство над существующими моделями.'}, 'en': {'title': 'Empowering Web Agents through Structured Knowledge and Reasoning', 'desc': 'This paper presents a framework for web agents that separates their abilities into two main areas: learning knowledge content and performing cognitive processes. The authors introduce the Web-CogKnowledge Framework, which categorizes knowledge into Factual, Conceptual, and Procedural types, essential for effective reasoning. They also create the Web-CogDataset, a structured dataset from real-world websites to help agents learn necessary knowledge. The proposed Web-CogReasoner utilizes a novel Chain-of-Thought reasoning approach, demonstrating improved performance in generalizing to new tasks compared to existing models.'}, 'zh': {'title': '智能体能力的双重分解：知识与认知', 'desc': '本文提出了一种网络智能体的框架，将其能力分解为知识内容学习和认知过程。我们定义了Web-CogKnowledge框架，将知识分为事实性、概念性和程序性三类，以支持智能体的学习和推理。通过构建Web-CogDataset，我们为智能体提供了系统化的知识基础，帮助其掌握必要的核心知识。最后，我们开发了Web-CogReasoner，并通过实验验证了其在处理新任务时的优越性，尤其是在结构化知识至关重要的情况下。'}}}, {'id': 'https://huggingface.co/papers/2508.03560', 'title': 'LaTCoder: Converting Webpage Design to Code with Layout-as-Thought', 'url': 'https://huggingface.co/papers/2508.03560', 'abstract': 'LaTCoder enhances layout preservation in design-to-code tasks by dividing webpage designs into blocks and using Chain-of-Thought reasoning with MLLMs, achieving significant improvements in metrics and human preference.  \t\t\t\t\tAI-generated summary \t\t\t\t Converting webpage designs into code (design-to-code) plays a vital role in User Interface (UI) development for front-end developers, bridging the gap between visual design and functional implementation. While recent Multimodal Large Language Models (MLLMs) have shown significant potential in design-to-code tasks, they often fail to accurately preserve the layout during code generation. To this end, we draw inspiration from the Chain-of-Thought (CoT) reasoning in human cognition and propose LaTCoder, a novel approach that enhances layout preservation in webpage design during code generation with Layout-as-Thought (LaT). Specifically, we first introduce a simple yet efficient algorithm to divide the webpage design into image blocks. Next, we prompt MLLMs using a CoTbased approach to generate code for each block. Finally, we apply two assembly strategies-absolute positioning and an MLLM-based method-followed by dynamic selection to determine the optimal output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs (i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly introduced, more challenging benchmark (CC-HARD) that features complex layouts. The experimental results on automatic metrics demonstrate significant improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE decreased by 38% when using DeepSeek-VL2, compared to direct prompting. Moreover, the human preference evaluation results indicate that annotators favor the webpages generated by LaTCoder in over 60% of cases, providing strong evidence of the effectiveness of our method.', 'score': 12, 'issue_id': 5221, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '14b52fe9021b5b26', 'authors': ['Yi Gui', 'Zhen Li', 'Zhongyi Zhang', 'Guohao Wang', 'Tianpeng Lv', 'Gaoyang Jiang', 'Yi Liu', 'Dongping Chen', 'Yao Wan', 'Hongyu Zhang', 'Wenbin Jiang', 'Xuanhua Shi', 'Hai Jin'], 'affiliations': ['Chongqing University, Chongqing, China', 'Huazhong University of Science and Technology, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03560.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#reasoning', '#architecture', '#multimodal', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'LaTCoder: улучшение сохранения макета при генерации кода из дизайна веб-страниц', 'desc': 'LaTCoder - это новый подход к задаче преобразования дизайна веб-страниц в код, который улучшает сохранение макета. Метод использует разделение дизайна на блоки и применяет рассуждения по цепочке мыслей (Chain-of-Thought) с помощью мультимодальных больших языковых моделей (MLLM). LaTCoder показывает значительные улучшения по автоматическим метрикам, таким как TreeBLEU и MAE. В ходе оценки предпочтений пользователей, веб-страницы, сгенерированные LaTCoder, были выбраны в более чем 60% случаев.'}, 'en': {'title': 'Enhancing Layout Preservation in Design-to-Code with LaTCoder', 'desc': "LaTCoder is a novel approach designed to improve the accuracy of converting webpage designs into code while preserving the layout. It utilizes Chain-of-Thought reasoning to enhance the performance of Multimodal Large Language Models (MLLMs) by breaking down designs into manageable blocks. The method employs two assembly strategies to optimize the final output, ensuring that the generated code closely matches the intended design. Experimental results show significant improvements in both automatic metrics and human preference, indicating LaTCoder's effectiveness in design-to-code tasks."}, 'zh': {'title': '提升网页设计布局保留的LaTCoder', 'desc': 'LaTCoder是一种新方法，旨在提高网页设计到代码生成过程中的布局保留能力。它通过将网页设计分割成多个图像块，并使用基于思维链的推理方法来生成每个块的代码。该方法结合了绝对定位和基于多模态大语言模型的组装策略，以选择最佳输出。实验结果表明，LaTCoder在多个基准测试中显著提高了自动评估指标和人类偏好。'}}}, {'id': 'https://huggingface.co/papers/2507.23785', 'title': 'Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis', 'url': 'https://huggingface.co/papers/2507.23785', 'abstract': 'A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: https://gvfdiffusion.github.io/.', 'score': 12, 'issue_id': 5220, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '47c7686978b9c4dc', 'authors': ['Bowen Zhang', 'Sicheng Xu', 'Chuxin Wang', 'Jiaolong Yang', 'Feng Zhao', 'Dong Chen', 'Baining Guo'], 'affiliations': ['Microsoft Research Asia', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.23785.jpg', 'data': {'categories': ['#video', '#synthetic', '#3d', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Генерация динамического 3D-контента из видео с помощью диффузионных моделей', 'desc': 'Представлена новая система для создания динамического 3D-контента из одиночных видеовходов. Используется VAE для кодирования канонических гауссовых сплатов и их временных вариаций, а также диффузионная модель с учетом времени для генерации. Система обучена на тщательно отобранных анимируемых 3D-объектах из набора данных Objaverse. Демонстрирует превосходное качество генерации и обобщение на реальные видеовходы, несмотря на обучение только на синтетических данных.'}, 'en': {'title': 'Transforming Videos into Dynamic 3D Worlds', 'desc': 'This paper introduces a new framework for generating dynamic 3D content from single video inputs using advanced machine learning techniques. It employs a Direct 4DMesh-to-GS Variation Field Variational Autoencoder (VAE) to efficiently encode 3D shapes and their motion without needing to fit each instance individually. The framework also incorporates a Gaussian Variation Field diffusion model that leverages a temporal-aware Diffusion Transformer, allowing it to conditionally generate high-quality animations based on input videos. The model shows impressive performance and generalization capabilities, even when trained on synthetic data, making it a significant advancement in video-to-4D generation.'}, 'zh': {'title': '从视频生成高质量动态3D内容的创新框架', 'desc': '本文提出了一种新颖的框架，用于从单个视频输入生成高质量的动态3D内容。我们引入了直接的4DMesh到高斯样条（GS）变分场变分自编码器（VAE），能够直接编码3D动画数据中的高斯样条及其时间变化。通过这种高效的表示方式，我们训练了一个基于时间感知的高斯变分场扩散模型，能够根据输入视频和高斯样条生成动态内容。实验结果表明，该模型在生成质量上优于现有方法，并且在处理真实视频输入时表现出良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2508.03789', 'title': 'HPSv3: Towards Wide-Spectrum Human Preference Score', 'url': 'https://huggingface.co/papers/2508.03789', 'abstract': 'HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating text-to-image generation models requires alignment with human perception, yet existing human-centric metrics are constrained by limited data coverage, suboptimal feature extraction, and inefficient loss functions. To address these challenges, we introduce Human Preference Score v3 (HPSv3). (1) We release HPDv3, the first wide-spectrum human preference dataset integrating 1.08M text-image pairs and 1.17M annotated pairwise comparisons from state-of-the-art generative models and low to high-quality real-world images. (2) We introduce a VLM-based preference model trained using an uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose Chain-of-Human-Preference (CoHP), an iterative image refinement method that enhances quality without extra data, using HPSv3 to select the best image at each step. Extensive experiments demonstrate that HPSv3 serves as a robust metric for wide-spectrum image evaluation, and CoHP offers an efficient and human-aligned approach to improve image generation quality. The code and dataset are available at the HPSv3 Homepage.', 'score': 11, 'issue_id': 5220, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'a2a0678cfc88e0ef', 'authors': ['Yuhang Ma', 'Xiaoshi Wu', 'Keqiang Sun', 'Hongsheng Li'], 'affiliations': ['CPII, InnoHK', 'CUHK MMLab', 'Kings College London', 'Mizzen AI', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.03789.jpg', 'data': {'categories': ['#alignment', '#data', '#benchmark', '#dataset', '#cv', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'HPSv3: Новый стандарт оценки качества генерации изображений по тексту', 'desc': 'HPSv3 - это новый метод оценки качества генерации изображений по тексту, основанный на широком спектре данных и учитывающий неопределенность при ранжировании. Авторы представили обширный датасет HPDv3, содержащий 1.08 миллиона пар текст-изображение и 1.17 миллиона аннотированных попарных сравнений. Метод использует модель на основе VLM, обученную с помощью функции потерь, учитывающей неопределенность при ранжировании. Также предложен итеративный подход Chain-of-Human-Preference для улучшения качества сгенерированных изображений.'}, 'en': {'title': 'Enhancing Image Generation with Human Preference Score v3', 'desc': 'The paper presents Human Preference Score v3 (HPSv3), a new metric designed to improve the evaluation of text-to-image generation models by aligning them more closely with human preferences. It introduces a comprehensive dataset, HPDv3, containing over 1 million text-image pairs and annotated comparisons, which enhances the training of preference models. The authors also propose an uncertainty-aware ranking loss for fine-grained image ranking and a method called Chain-of-Human-Preference (CoHP) that iteratively refines images to boost quality without needing additional data. Through extensive testing, HPSv3 is shown to be a reliable metric for image evaluation, while CoHP effectively enhances image generation quality in a way that resonates with human judgment.'}, 'zh': {'title': '提升图像生成质量的HPSv3与CoHP方法', 'desc': 'HPSv3是一种人类偏好评分，利用广泛的数据集和考虑不确定性的排名损失，提升文本到图像生成的质量。我们发布了HPDv3，这是第一个包含108万对文本-图像和117万对标注比较的广泛人类偏好数据集。我们还引入了一种基于视觉语言模型的偏好模型，使用不确定性感知的排名损失进行细致排名。此外，我们提出了人类偏好链（CoHP），通过迭代图像优化，在每一步选择最佳图像，从而提高生成质量。'}}}, {'id': 'https://huggingface.co/papers/2508.02215', 'title': 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding', 'url': 'https://huggingface.co/papers/2508.02215', 'abstract': 'LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.', 'score': 9, 'issue_id': 5220, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'caa8de613517d011', 'authors': ['Yike Zhang', 'Zhiyuan He', 'Huiqiang Jiang', 'Chengruidong Zhang', 'Yuqing Yang', 'Jianyong Wang', 'Lili Qiu'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02215.jpg', 'data': {'categories': ['#training', '#long_context', '#inference', '#optimization'], 'emoji': '🔪', 'ru': {'title': 'Эффективное обрезание кэша для ускорения языковых моделей', 'desc': 'LeanK - это метод машинного обучения для оптимизации больших языковых моделей. Он уменьшает использование памяти и ускоряет декодирование, удаляя неважные каналы в кэше ключей. LeanK использует двухэтапный процесс обучения для создания маски каналов, удовлетворяющей требованиям разреженности и аппаратного выравнивания. Эксперименты показывают сокращение памяти кэша ключей до 70% и ускорение вычисления внимания в 1,3 раза.'}, 'en': {'title': 'LeanK: Pruning for Efficient Language Model Performance', 'desc': 'LeanK is a novel method designed to enhance the efficiency of large language models by pruning unnecessary key cache channels. It utilizes a learning-based approach to identify and remove unimportant key (K) cache channels while maintaining model accuracy. The method employs a two-stage training process to create a static mask that meets specific sparsity and hardware requirements. As a result, LeanK achieves significant reductions in GPU memory usage and accelerates decoding times, demonstrating up to 70% reduction in K cache and a 1.3x speedup in attention computation.'}, 'zh': {'title': 'LeanK：高效解码的大型语言模型优化方案', 'desc': 'LeanK是一种基于学习的方法，旨在减少大型语言模型中的不重要的关键缓存通道，从而降低内存使用并加速解码，同时不影响准确性。该方法利用静态通道稀疏性，通过一种新颖的两阶段训练过程，学习满足特定稀疏比和硬件对齐要求的通道静态掩码。实验结果表明，LeanK可以减少高达70%的K缓存和16%-18%的V缓存内存，并且自定义解码内核使注意力计算速度提高了1.3倍。通过分析学习到的重要性分布，我们还提供了对长上下文推理过程中模型通道和注意力头的深入见解。'}}}, {'id': 'https://huggingface.co/papers/2508.02807', 'title': 'DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework', 'url': 'https://huggingface.co/papers/2508.02807', 'abstract': 'DreamVVT, a two-stage framework using Diffusion Transformers and LoRA adapters, enhances video virtual try-on by leveraging unpaired human-centric data and pretrained models to preserve garment details and temporal consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. In the second stage, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page https://virtu-lab.github.io/', 'score': 8, 'issue_id': 5224, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'f3f693bca2e57a94', 'authors': ['Tongchun Zuo', 'Zaiyu Huang', 'Shuliang Ning', 'Ente Lin', 'Chao Liang', 'Zerong Zheng', 'Jianwen Jiang', 'Yuan Zhang', 'Mingyuan Gao', 'Xin Dong'], 'affiliations': ['ByteDance Intelligent Creation', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02807.jpg', 'data': {'categories': ['#video', '#cv', '#synthetic', '#multimodal', '#diffusion'], 'emoji': '👚', 'ru': {'title': 'DreamVVT: Реалистичная виртуальная примерка одежды на видео с помощью ИИ', 'desc': 'DreamVVT - это двухэтапная система для виртуальной примерки одежды на видео, использующая диффузионные трансформеры и LoRA-адаптеры. Она улучшает сохранение деталей одежды и временную согласованность, используя непарные данные с изображениями людей и предобученные модели. На первом этапе система создает высококачественные ключевые кадры с примеркой, используя мультикадровую модель и визуально-языковую модель. На втором этапе применяется предобученная модель генерации видео с LoRA-адаптерами для обеспечения временной согласованности и правдоподобных движений.'}, 'en': {'title': 'Enhancing Video Try-On with DreamVVT: Consistency Meets Detail', 'desc': 'DreamVVT is a two-stage framework designed to improve video virtual try-on (VVT) by using Diffusion Transformers and LoRA adapters. It effectively utilizes unpaired human-centric data and pretrained models to maintain garment details and ensure temporal consistency in videos. The first stage generates high-quality keyframe images using a multi-frame try-on model and a vision-language model, while the second stage focuses on video generation by incorporating motion and appearance data. This innovative approach allows DreamVVT to outperform existing methods in preserving garment fidelity and achieving smooth motion in dynamic scenarios.'}, 'zh': {'title': 'DreamVVT：提升视频虚拟试穿的创新框架', 'desc': 'DreamVVT是一种两阶段框架，利用扩散变换器和LoRA适配器，提升视频虚拟试穿技术。该方法通过使用未配对的人体中心数据和预训练模型，能够更好地保留服装细节和时间一致性。第一阶段通过多帧试穿模型生成高保真关键帧图像，第二阶段则利用预训练的视频生成模型确保动态运动的连贯性。实验结果表明，DreamVVT在真实场景中优于现有方法，能够更好地保持服装内容的细节和时间稳定性。'}}}, {'id': 'https://huggingface.co/papers/2508.04664', 'title': 'Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management', 'url': 'https://huggingface.co/papers/2508.04664', 'abstract': "Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.", 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '1575b65bda95c5f9', 'authors': ['Mo Li', 'L. H. Xu', 'Qitai Tan', 'Ting Cao', 'Yunxin Liu'], 'affiliations': ['Independent Researcher', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04664.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Sculptor: Умное управление контекстом для улучшения работы языковых моделей', 'desc': 'Статья представляет фреймворк Sculptor для активного управления контекстом в больших языковых моделях (LLM). Sculptor позволяет LLM проактивно управлять вниманием и рабочей памятью, что снижает проактивную интерференцию и улучшает надежность рассуждений. Фреймворк включает инструменты для фрагментации контекста, суммирования и интеллектуального поиска. Эксперименты показывают, что Sculptor значительно улучшает производительность LLM на длинных контекстах без специального обучения.'}, 'en': {'title': 'Sculptor: Mastering Memory for Better Long-Context Reasoning', 'desc': 'The paper introduces Sculptor, a framework designed to enhance the performance of Large Language Models (LLMs) when dealing with long contexts. It addresses the issue of proactive interference, where irrelevant information disrupts reasoning and memory recall. Sculptor provides LLMs with Active Context Management (ACM) tools, allowing them to manage their internal memory more effectively by fragmenting context, summarizing information, and intelligently searching for relevant data. Experimental results show that Sculptor improves reasoning reliability and performance on long-context tasks without requiring additional training, emphasizing the importance of context-control strategies.'}, 'zh': {'title': '主动上下文管理，提升LLM性能！', 'desc': 'Sculptor是一个用于主动上下文管理的框架，旨在提高大型语言模型（LLM）在处理长上下文时的表现。该框架通过主动管理注意力和工作记忆，减少了前期信息的干扰，从而改善推理的可靠性。Sculptor提供了三种工具：上下文碎片化、摘要、隐藏与恢复，以及智能搜索，帮助LLM更有效地处理信息。实验结果表明，Sculptor在没有特定训练的情况下，显著提升了模型的性能，强调了主动上下文管理的重要性。'}}}, {'id': 'https://huggingface.co/papers/2508.04586', 'title': 'Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference', 'url': 'https://huggingface.co/papers/2508.04586', 'abstract': 'The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.  \t\t\t\t\tAI-generated summary \t\t\t\t Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.', 'score': 6, 'issue_id': 5220, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '0e9d3ff69536a24d', 'authors': ['Nuo Chen', 'Moming Duan', 'Andre Huikai Lin', 'Qian Wang', 'Jiaying Wu', 'Bingsheng He'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2508.04586.jpg', 'data': {'categories': ['#ethics', '#survey'], 'emoji': '🌐', 'ru': {'title': 'Революция в формате конференций ИИ: от централизации к федерации', 'desc': 'Данная статья анализирует структурные проблемы конференций по искусственному интеллекту, включая рост числа публикаций, углеродный след, негативное настроение сообщества и логистические трудности. Авторы выделяют четыре ключевые области напряженности: научную, экологическую, психологическую и логистическую. Для решения этих проблем предлагается модель Community-Federated Conference (CFC), которая разделяет рецензирование, презентации и нетворкинг на глобально координируемые, но локально организованные компоненты. Эта модель предлагает более устойчивый, инклюзивный и гибкий подход к проведению исследований в области ИИ.'}, 'en': {'title': 'Towards Sustainable AI Conferences: A Community-Federated Approach', 'desc': 'This paper highlights critical issues facing AI conferences, such as unsustainable publication rates, high carbon emissions, negative community sentiment, and logistical challenges. It reveals that the number of papers published per author has significantly increased, leading to a strain on the scientific community. Additionally, the environmental impact of conferences is alarming, with emissions surpassing those of host cities. To address these challenges, the authors propose a Community-Federated Conference model that decentralizes the conference structure, enhancing sustainability and inclusivity in AI research.'}, 'zh': {'title': '构建可持续的人工智能会议新模式', 'desc': '这篇论文诊断了人工智能会议的结构性问题，包括发表率、碳足迹、负面社区情绪和后勤挑战。随着会议数量的快速增长，传统的集中式会议模式变得越来越不可持续。论文提出了一种基于社区的联合会议模型，旨在解决这些问题，促进科学传播的公平性和社区的福祉。该模型将同行评审、展示和网络交流分开，提供了一种更可持续、包容和有韧性的人工智能研究发展路径。'}}}, {'id': 'https://huggingface.co/papers/2508.01928', 'title': 'IAUNet: Instance-Aware U-Net', 'url': 'https://huggingface.co/papers/2508.01928', 'abstract': 'IAUNet, a query-based U-Net architecture with a lightweight convolutional Pixel decoder and Transformer decoder, outperforms state-of-the-art models in biomedical instance segmentation.  \t\t\t\t\tAI-generated summary \t\t\t\t Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet', 'score': 5, 'issue_id': 5226, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '0f9cff1ae25f175b', 'authors': ['Yaroslav Prytula', 'Illia Tsiporenko', 'Ali Zeynalli', 'Dmytro Fishman'], 'affiliations': ['Institute of Computer Science, University of Tartu', 'STACC U, Tartu, Estonia', 'Ukrainian Catholic University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01928.jpg', 'data': {'categories': ['#architecture', '#cv', '#games', '#benchmark', '#science', '#dataset', '#optimization', '#healthcare'], 'emoji': '🔬', 'ru': {'title': 'IAUNet: Прорыв в сегментации клеток с помощью гибридной архитектуры', 'desc': 'IAUNet - это новая архитектура сегментации изображений на основе U-Net с запросами. Она включает облегченный сверточный декодер пикселей и декодер на основе трансформеров для уточнения признаков объектов. Модель превосходит современные методы в задаче сегментации экземпляров биомедицинских изображений, особенно для перекрывающихся клеток. Авторы также представили новый набор данных для сегментации клеток, который станет эталоном в этой области.'}, 'en': {'title': 'IAUNet: Revolutionizing Biomedical Instance Segmentation with Query-Based U-Net', 'desc': 'IAUNet is a new architecture designed for biomedical instance segmentation, which helps in identifying and separating individual cells in images. It combines a traditional U-Net structure with a lightweight convolutional Pixel decoder and a Transformer decoder to enhance performance. This model efficiently reduces parameters while improving the segmentation of overlapping objects. The introduction of the 2025 Revvity Full Cell Segmentation Dataset provides a valuable resource for training and benchmarking segmentation models in this field.'}, 'zh': {'title': 'IAUNet：生物医学实例分割的新标杆', 'desc': 'IAUNet是一种基于查询的U-Net架构，结合了轻量级卷积像素解码器和Transformer解码器，能够在生物医学实例分割中超越现有的最先进模型。实例分割在生物医学成像中至关重要，因为它可以准确区分重叠且大小不一的细胞。IAUNet通过全新的轻量级卷积像素解码器提高了模型的效率，并减少了参数数量，同时引入的Transformer解码器能够在多个尺度上细化特定对象的特征。我们还推出了2025 Revvity全细胞分割数据集，为生物医学实例分割设定了新的基准。'}}}, {'id': 'https://huggingface.co/papers/2508.04295', 'title': 'EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation', 'url': 'https://huggingface.co/papers/2508.04295', 'abstract': "EvoC2Rust is an automated framework that translates entire C projects to Rust using a skeleton-guided approach, combining rule-based and LLM-based methods to improve syntax, semantics, and safety.  \t\t\t\t\tAI-generated summary \t\t\t\t Rust's compile-time safety guarantees make it ideal for safety-critical systems, creating demand for translating legacy C codebases to Rust. While various approaches have emerged for this task, they face inherent trade-offs: rule-based solutions face challenges in meeting code safety and idiomaticity requirements, while LLM-based solutions often fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting entire C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three evolutionary stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates the function, replacing the corresponding stub placeholder; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates EvoC2Rust's superior performance in project-level C-to-Rust translation. On average, it achieves 17.24% and 14.32% improvements in syntax and semantic accuracy over the LLM-based approaches, along with a 96.79% higher code safety rate than the rule-based tools. At the module level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates on industrial projects, even for complex codebases and long functions.", 'score': 4, 'issue_id': 5221, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '0dc09e7a8e2bad95', 'authors': ['Chaofan Wang', 'Tingrui Yu', 'Jie Wang', 'Dong Chen', 'Wenrui Zhang', 'Yuling Shi', 'Xiaodong Gu', 'Beijun Shen'], 'affiliations': ['Huawei Technologies Co., Ltd', 'Shanghai Jiao Tong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04295.jpg', 'data': {'categories': ['#open_source', '#architecture', '#plp', '#optimization'], 'emoji': '🔄', 'ru': {'title': 'EvoC2Rust: Эволюционный подход к автоматическому переводу C в Rust', 'desc': 'EvoC2Rust - это автоматизированный фреймворк для перевода проектов на C в Rust, использующий подход на основе скелета кода. Он сочетает методы на основе правил и языковых моделей для улучшения синтаксиса, семантики и безопасности кода. Фреймворк работает в три этапа: декомпозиция проекта, инкрементальный перевод функций и исправление ошибок компиляции. Оценка на открытых и промышленных проектах показала превосходство EvoC2Rust над существующими подходами в точности перевода и безопасности кода.'}, 'en': {'title': 'EvoC2Rust: Bridging C to Rust with Safety and Precision', 'desc': 'EvoC2Rust is a novel framework designed to automate the translation of entire C projects into Rust, leveraging a skeleton-guided approach that integrates both rule-based and LLM-based techniques. This method addresses the limitations of existing solutions by ensuring that the translated code maintains both safety and idiomatic Rust syntax. The framework operates in three stages: it first breaks down the C project into modules, then translates functions incrementally, and finally resolves any compilation errors using a combination of LLM and static analysis. Evaluation results show that EvoC2Rust significantly outperforms previous methods in terms of syntax, semantic accuracy, and code safety, making it a robust solution for converting legacy C code to Rust.'}, 'zh': {'title': 'EvoC2Rust：高效的C到Rust自动转换框架', 'desc': 'EvoC2Rust是一个自动化框架，旨在将整个C项目转换为Rust代码。它采用了骨架引导的方法，结合了基于规则和基于大语言模型（LLM）的方法，以提高代码的语法、语义和安全性。该框架通过三个进化阶段进行项目级翻译，首先将C项目分解为功能模块，然后逐步翻译函数，最后通过集成LLM和静态分析修复编译错误。评估结果显示，EvoC2Rust在C到Rust的翻译中表现优越，语法和语义准确性分别提高了17.24%和14.32%。'}}}, {'id': 'https://huggingface.co/papers/2508.00222', 'title': 'RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization', 'url': 'https://huggingface.co/papers/2508.00222', 'abstract': "RL-PLUS, a hybrid-policy optimization approach, enhances LLM reasoning capabilities by integrating Multiple Importance Sampling and Exploration-Based Advantage Function, outperforming RLVR on various benchmarks and resolving capability boundary collapse.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM's immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.", 'score': 4, 'issue_id': 5224, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'ec45fc053d8a3704', 'authors': ['Yihong Dong', 'Xue Jiang', 'Yongding Tao', 'Huanyu Liu', 'Kechi Zhang', 'Lili Mou', 'Rongyu Cao', 'Yingwei Ma', 'Jue Chen', 'Binhua Li', 'Zhi Jin', 'Fei Huang', 'Yongbin Li', 'Ge Li'], 'affiliations': ['Department of Computing Science, University of Alberta', 'School of Computer Science, Peking University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.00222.jpg', 'data': {'categories': ['#training', '#benchmark', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'RL-PLUS: Прорыв в обучении с подкреплением для улучшения рассуждений языковых моделей', 'desc': 'Статья представляет RL-PLUS - новый гибридный подход к оптимизации политики для улучшения способностей рассуждения больших языковых моделей (LLM). RL-PLUS объединяет метод множественной выборки по важности и функцию преимущества на основе исследования для преодоления ограничений базовой модели. Этот метод превосходит существующий подход RLVR на различных тестовых наборах данных по математическим рассуждениям и задачам вне распределения. RL-PLUS также эффективно решает проблему коллапса границ возможностей, характерную для RLVR.'}, 'en': {'title': 'Breaking Boundaries in LLM Reasoning with RL-PLUS', 'desc': "The paper introduces RL-PLUS, a new hybrid-policy optimization method designed to improve the reasoning abilities of Large Language Models (LLMs). It combines Multiple Importance Sampling and Exploration-Based Advantage Function to enhance the model's performance beyond the limitations of traditional Reinforcement Learning with Verifiable Reward (RLVR). By addressing the issues of distributional mismatch and guiding exploration towards valuable reasoning paths, RL-PLUS effectively prevents capability boundary collapse. Extensive experiments show that RL-PLUS outperforms existing methods on various benchmarks, achieving significant improvements in reasoning tasks."}, 'zh': {'title': 'RL-PLUS：突破推理能力边界的创新方法', 'desc': 'RL-PLUS是一种混合策略优化方法，旨在提升大型语言模型（LLM）的推理能力。它通过结合多重重要性采样和基于探索的优势函数，克服了传统强化学习方法（如RLVR）在能力边界崩溃方面的局限。RL-PLUS能够有效利用外部数据，指导模型探索高价值的推理路径，从而实现更强的推理能力。实验结果表明，RL-PLUS在多个基准测试中表现优异，显著超越了现有方法。'}}}, {'id': 'https://huggingface.co/papers/2507.21974', 'title': 'Reasoning Language Models for Root Cause Analysis in 5G Wireless\n  Networks', 'url': 'https://huggingface.co/papers/2507.21974', 'abstract': 'A lightweight framework using Large Language Models (LLMs) with TeleLogs dataset and a two-stage training methodology improves Root Cause Analysis (RCA) in mobile networks by enhancing interpretability and reasoning quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Root Cause Analysis (RCA) in mobile networks remains a challenging task due to the need for interpretability, domain expertise, and causal reasoning. In this work, we propose a lightweight framework that leverages Large Language Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of annotated troubleshooting problems designed to benchmark RCA capabilities. Our evaluation reveals that existing open-source reasoning LLMs struggle with these problems, underscoring the need for domain-specific adaptation. To address this issue, we propose a two-stage training methodology that combines supervised fine-tuning with reinforcement learning to improve the accuracy and reasoning quality of LLMs. The proposed approach fine-tunes a series of RCA models to integrate domain knowledge and generate structured, multi-step diagnostic explanations, improving both interpretability and effectiveness. Extensive experiments across multiple LLM sizes show significant performance gains over state-of-the-art reasoning and non-reasoning models, including strong generalization to randomized test variants. These results demonstrate the promise of domain-adapted, reasoning-enhanced LLMs for practical and explainable RCA in network operation and management.', 'score': 4, 'issue_id': 5227, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '91b127bf8dd7d610', 'authors': ['Mohamed Sana', 'Nicola Piovesan', 'Antonio De Domenico', 'Yibin Kang', 'Haozhe Zhang', 'Merouane Debbah', 'Fadhel Ayed'], 'affiliations': ['Huawei Technologies, China', 'Khalifa University of Science and Technology, Abu Dhabi, UAE', 'Paris Research Center, Huawei Technologies, Boulogne-Billancourt, France'], 'pdf_title_img': 'assets/pdf/title_img/2507.21974.jpg', 'data': {'categories': ['#reasoning', '#rl', '#interpretability', '#dataset', '#training', '#open_source', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Улучшение анализа первопричин в мобильных сетях с помощью адаптированных языковых моделей', 'desc': 'В статье представлен легковесный фреймворк, использующий большие языковые модели (LLM) с набором данных TeleLogs для улучшения анализа первопричин (RCA) в мобильных сетях. Авторы предлагают двухэтапную методологию обучения, сочетающую контролируемую тонкую настройку и обучение с подкреплением. Этот подход позволяет интегрировать доменные знания и генерировать структурированные многоступенчатые диагностические объяснения. Эксперименты показывают значительный прирост производительности по сравнению с современными моделями рассуждений и моделями без рассуждений.'}, 'en': {'title': 'Enhancing RCA in Mobile Networks with Domain-Specific LLMs', 'desc': 'This paper presents a lightweight framework that utilizes Large Language Models (LLMs) to enhance Root Cause Analysis (RCA) in mobile networks. It introduces the TeleLogs dataset, which contains annotated troubleshooting problems to evaluate RCA capabilities effectively. The authors propose a two-stage training methodology that combines supervised fine-tuning and reinforcement learning to improve the reasoning quality and interpretability of LLMs. Experimental results show that this approach significantly outperforms existing models, demonstrating the potential of domain-adapted LLMs for practical RCA applications.'}, 'zh': {'title': '领域适应与推理增强的根本原因分析新方法', 'desc': '本研究提出了一种轻量级框架，利用大型语言模型（LLMs）来改善移动网络中的根本原因分析（RCA）。我们引入了TeleLogs数据集，这是一个经过注释的故障排除问题集合，旨在评估RCA能力。通过两阶段训练方法，结合监督微调和强化学习，提升了LLMs的准确性和推理质量。实验结果显示，经过领域适应的LLMs在网络操作和管理中的RCA任务上表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2508.01197', 'title': 'A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding', 'url': 'https://huggingface.co/papers/2508.01197', 'abstract': 'A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual grounding aims to identify objects or regions in a scene based on natural language descriptions, essential for spatially aware perception in autonomous driving. However, existing visual grounding tasks typically depend on bounding boxes that often fail to capture fine-grained details. Not all voxels within a bounding box are occupied, resulting in inaccurate object representations. To address this, we introduce a benchmark for 3D occupancy grounding in challenging outdoor scenes. Built on the nuScenes dataset, it integrates natural language with voxel-level occupancy annotations, offering more precise object perception compared to the traditional grounding task. Moreover, we propose GroundingOcc, an end-to-end model designed for 3D occupancy grounding through multi-modal learning. It combines visual, textual, and point cloud features to predict object location and occupancy information from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder for feature extraction, an occupancy head for voxel-wise predictions, and a grounding head to refine localization. Additionally, a 2D grounding module and a depth estimation module enhance geometric understanding, thereby boosting model performance. Extensive experiments on the benchmark demonstrate that our method outperforms existing baselines on 3D occupancy grounding. The dataset is available at https://github.com/RONINGOD/GroundingOcc.', 'score': 3, 'issue_id': 5220, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': 'd7be41190836a7cc', 'authors': ['Zhan Shi', 'Song Wang', 'Junbo Chen', 'Jianke Zhu'], 'affiliations': ['College of Computer Science, Zhejiang University, Hangzhou 310027, China', 'College of Software Technology, Zhejiang University', 'Udeer.ai, Hangzhou 310000, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01197.jpg', 'data': {'categories': ['#multimodal', '#games', '#benchmark', '#3d', '#optimization'], 'emoji': '🚗', 'ru': {'title': 'Точное восприятие объектов для беспилотных автомобилей с помощью 3D грунтовки', 'desc': 'Статья представляет новый бенчмарк для задачи 3D грунтовки объектов с использованием естественного языка и воксельных аннотаций в контексте автономного вождения. Авторы предлагают модель GroundingOcc, которая объединяет визуальные, текстовые и облачные признаки для предсказания местоположения и занятости объектов. Модель включает мультимодальный энкодер, модули для предсказания воксельной занятости и уточнения локализации, а также дополнительные компоненты для 2D грунтовки и оценки глубины. Эксперименты показывают превосходство предложенного метода над существующими базовыми моделями в задаче 3D грунтовки занятости.'}, 'en': {'title': 'Enhancing Object Perception with 3D Occupancy Grounding', 'desc': 'This paper presents a new approach to 3D occupancy grounding, which is crucial for improving object perception in autonomous driving. It introduces a benchmark that uses voxel-level annotations combined with natural language descriptions, allowing for more accurate identification of objects in complex outdoor environments. The proposed model, GroundingOcc, employs multi-modal learning to integrate visual, textual, and point cloud data, enhancing the precision of object localization and occupancy predictions. Experimental results show that this method significantly outperforms existing techniques in the field, demonstrating its effectiveness in real-world applications.'}, 'zh': {'title': '提升自动驾驶物体感知的3D占用基础视觉定位', 'desc': '本论文提出了一种新的基准和模型，用于通过自然语言和体素级注释进行3D占用基础的视觉定位，旨在提高自动驾驶中的物体感知能力。现有的视觉定位任务通常依赖于边界框，这种方法无法捕捉到细粒度的细节，导致物体表示不准确。我们引入的GroundingOcc模型通过多模态学习，结合视觉、文本和点云特征，从粗到细地预测物体位置和占用信息。实验结果表明，我们的方法在3D占用基础的视觉定位任务中优于现有的基准。'}}}, {'id': 'https://huggingface.co/papers/2508.04632', 'title': 'IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2508.04632', 'abstract': 'Instruction Following Decorator enhances RLVR by improving sample efficiency, intent alignment, and reducing reward hacking in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.', 'score': 2, 'issue_id': 5228, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '1e91bd1d8a3387f3', 'authors': ['Xu Guo', 'Tianyi Liang', 'Tong Jian', 'Xiaogui Yang', 'Ling-I Wu', 'Chenhui Li', 'Zhihui Lu', 'Qipeng Guo', 'Kai Chen'], 'affiliations': ['East China Normal University', 'Fudan University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.04632.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#alignment', '#training', '#optimization', '#security', '#rl'], 'emoji': '🎓', 'ru': {'title': 'IFDecorator: Повышение эффективности и надежности обучения языковых моделей следованию инструкциям', 'desc': 'Данная статья представляет Instruction Following Decorator (IFDecorator) - фреймворк, улучшающий обучение с подкреплением с проверяемыми наградами (RLVR) для больших языковых моделей. IFDecorator включает в себя кооперативно-состязательный механизм генерации данных, модуль проверки соответствия намерениям и диагностический механизм для обнаружения эксплуатации наград. Авторы демонстрируют значительные улучшения в следовании инструкциям на различных бенчмарках, превосходя более крупные проприетарные модели. Также отмечается существенное снижение уровня эксплуатации наград благодаря применению IFDecorator.'}, 'en': {'title': 'Enhancing Instruction Following with Robust Reinforcement Learning', 'desc': 'The paper presents the Instruction Following Decorator (IFDecorator), a framework designed to enhance Reinforcement Learning with Verifiable Rewards (RLVR) for large language models (LLMs). IFDecorator improves sample efficiency by using a cooperative-adversarial data flywheel that generates increasingly difficult instruction-verification pairs. It also includes IntentCheck, which ensures that the model aligns with user intent, and trip wires that detect and mitigate reward hacking behaviors. The results show that the Qwen2.5-32B-Instruct-IFDecorator model achieves high accuracy and reduces reward hacking, outperforming larger models like GPT-4o.'}, 'zh': {'title': '提升指令跟随能力的创新框架', 'desc': '本论文介绍了一种名为指令跟随装饰器（IFDecorator）的框架，旨在提高大语言模型（LLMs）在可验证奖励（RLVR）下的指令跟随能力。该框架通过三个主要组件来增强样本效率和意图对齐，减少奖励黑客行为。首先，它利用合作对抗的数据飞轮生成越来越具挑战性的指令-验证对；其次，IntentCheck模块确保模型的意图对齐；最后，trip wires机制检测并捕捉奖励黑客行为。实验结果表明，使用IFDecorator的模型在多个基准测试中表现优异，显著提高了准确率和整体能力。'}}}, {'id': 'https://huggingface.co/papers/2508.04010', 'title': 'HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive\n  Policy Enhancement and Dual-Objective Optimization', 'url': 'https://huggingface.co/papers/2508.04010', 'abstract': 'HarmonyGuard is a multi-agent framework that enhances policy compliance and task completion in web environments by adaptively updating security policies and optimizing dual objectives of safety and utility.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard.', 'score': 2, 'issue_id': 5232, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '9a431a1adec12655', 'authors': ['Yurun Chen', 'Xavier Hu', 'Yuhan Liu', 'Keting Yin', 'Juncheng Li', 'Zhuosheng Zhang', 'Shengyu Zhang'], 'affiliations': ['Shanghai Jiao Tong University', 'Xiamen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.04010.jpg', 'data': {'categories': ['#optimization', '#security', '#benchmark', '#architecture', '#agents'], 'emoji': '🛡️', 'ru': {'title': 'Гармония безопасности и эффективности в веб-среде', 'desc': 'HarmonyGuard - это многоагентная система для повышения безопасности и эффективности веб-агентов на основе больших языковых моделей. Она адаптивно обновляет политики безопасности с помощью специального агента и оптимизирует двойные цели безопасности и полезности. Система включает агента полезности, который в реальном времени оценивает цели и использует метакогнитивные способности для их оптимизации. Эксперименты показали значительное улучшение соблюдения политик безопасности и выполнения задач по сравнению с существующими методами.'}, 'en': {'title': 'Balancing Safety and Utility in Web Tasks with HarmonyGuard', 'desc': 'HarmonyGuard is a multi-agent framework designed to improve how web agents comply with security policies while completing tasks. It addresses the challenge of balancing safety and utility in dynamic web environments by adaptively updating security policies. The framework includes a Policy Agent that extracts and updates security policies from external documents and a Utility Agent that optimizes task performance based on safety and utility objectives. Evaluations show that HarmonyGuard significantly enhances policy compliance and task completion compared to existing methods.'}, 'zh': {'title': 'HarmonyGuard：安全与效用的双重优化', 'desc': 'HarmonyGuard是一个多智能体框架，旨在通过自适应更新安全策略和优化安全性与效用的双重目标，增强网络环境中的政策合规性和任务完成度。该框架引入了政策代理，能够从非结构化文档中提取和维护结构化的安全政策，并根据不断变化的威胁持续更新这些政策。同时，效用代理通过马尔可夫实时推理评估安全性和效用的双重目标，并利用元认知能力进行优化。评估结果表明，HarmonyGuard在政策合规性和任务完成度方面显著优于现有基线，政策合规性超过90%。'}}}, {'id': 'https://huggingface.co/papers/2508.01778', 'title': 'DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving\n  via Online HD Map Diffusion', 'url': 'https://huggingface.co/papers/2508.01778', 'abstract': 'DiffSemanticFusion enhances autonomous driving by fusing semantic raster and graph-based representations using a map diffusion module, improving trajectory prediction and end-to-end driving performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at https://github.com/SunZhigang7/DiffSemanticFusion.', 'score': 2, 'issue_id': 5232, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'd5f701aadba28943', 'authors': ['Zhigang Sun', 'Yiru Wang', 'Anqing Jiang', 'Shuo Wang', 'Yu Gao', 'Yuwen Heng', 'Shouyi Zhang', 'An He', 'Hao Jiang', 'Jinhao Chai', 'Zichong Gu', 'Wang Jijun', 'Shichen Tang', 'Lavdim Halilaj', 'Juergen Luettin', 'Hao Sun'], 'affiliations': ['AIR, Tsinghua University, Beijing, China', 'Bosch Corporate Research, Bosch (China) Investment Ltd., Shanghai, China', 'Robert Bosch GmbH', 'School of Communication and Information Engineering, Shanghai University, Shanghai, China', 'Shanghai Jiaotong University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.01778.jpg', 'data': {'categories': ['#video', '#optimization', '#games', '#benchmark', '#multimodal', '#agents'], 'emoji': '🚗', 'ru': {'title': 'Слияние семантики для точного автопилота', 'desc': 'DiffSemanticFusion - это фреймворк для улучшения автономного вождения, объединяющий семантические растровые и графовые представления с помощью модуля диффузии карты. Он повышает точность прогнозирования траектории и эффективность сквозного автономного вождения. Эксперименты на реальных наборах данных показали значительное улучшение производительности по сравнению с современными методами. Фреймворк особенно эффективен в сценариях онлайн-генерации HD-карт, где он улучшает стабильность и выразительность представлений.'}, 'en': {'title': 'Fusing Data for Smarter Autonomous Driving', 'desc': 'DiffSemanticFusion is a novel framework designed to improve autonomous driving by combining two types of data representations: semantic raster and graph-based models. The framework uses a map diffusion module to enhance the stability and detail of high-definition maps, which are crucial for accurate trajectory prediction and driving performance. By integrating these representations, DiffSemanticFusion leverages their strengths to provide better scene understanding and decision-making for autonomous vehicles. Experiments show that this approach significantly outperforms existing methods in real-world driving scenarios, demonstrating its effectiveness in both trajectory prediction and end-to-end driving tasks.'}, 'zh': {'title': '融合语义与图形，提升自主驾驶性能', 'desc': 'DiffSemanticFusion 是一种增强自主驾驶的框架，通过融合语义栅格和基于图的表示，利用地图扩散模块来提高轨迹预测和端到端驾驶性能。该方法结合了栅格表示的视觉模型优势和图表示的结构细节，解决了在线高清地图生成中的不稳定性问题。我们在真实世界的自主驾驶基准测试中验证了该框架，结果显示在轨迹预测和规划任务上均优于多种先进方法。实验表明，DiffSemanticFusion 在多个场景中实现了显著的性能提升，特别是在复杂环境下的自主驾驶表现。'}}}, {'id': 'https://huggingface.co/papers/2508.01630', 'title': 'OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets', 'url': 'https://huggingface.co/papers/2508.01630', 'abstract': 'OpenMed NER, a suite of open-source transformer models using DAPT and LoRA, achieves state-of-the-art performance on diverse biomedical NER benchmarks with high efficiency and low computational cost.  \t\t\t\t\tAI-generated summary \t\t\t\t Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.', 'score': 2, 'issue_id': 5222, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'aca28561e07e250a', 'authors': ['Maziyar Panahi'], 'affiliations': ['CNRS Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2508.01630.jpg', 'data': {'categories': ['#healthcare', '#ethics', '#dataset', '#training', '#transfer_learning', '#open_source', '#benchmark', '#data'], 'emoji': '🧬', 'ru': {'title': 'OpenMed NER: Открытый прорыв в распознавании биомедицинских сущностей', 'desc': 'OpenMed NER - это набор моделей на основе трансформеров с открытым исходным кодом, использующих методы DAPT и LoRA для распознавания именованных сущностей в биомедицинских текстах. Модели достигают наилучших результатов на 10 из 12 эталонных наборов данных, охватывающих химические вещества, заболевания, гены и виды. Обучение проводится эффективно - менее 12 часов на одном GPU с низким углеродным следом. Проект демонстрирует, что стратегически адаптированные модели с открытым исходным кодом могут превзойти закрытые решения в области биомедицинского NER.'}, 'en': {'title': 'OpenMed NER: Efficiently Transforming Biomedical NER with Open-Source Innovation', 'desc': 'OpenMed NER is a collection of open-source transformer models designed for named-entity recognition (NER) in the biomedical field. It utilizes domain-adaptive pre-training (DAPT) and Low-Rank Adaptation (LoRA) to achieve high performance while being computationally efficient. The models were trained on a large dataset of clinical notes and research papers, and they excelled in identifying various biomedical entities, outperforming existing models on multiple benchmarks. This work highlights the potential of open-source solutions to achieve superior results compared to proprietary models, all while maintaining a low environmental impact.'}, 'zh': {'title': 'OpenMed NER：高效的生物医学命名实体识别解决方案', 'desc': 'OpenMed NER 是一套开源的变换器模型，结合了轻量级的领域自适应预训练（DAPT）和参数高效的低秩适应（LoRA），在生物医学命名实体识别（NER）基准测试中表现出色。该模型在350,000段来自伦理来源的临床笔记和研究文献的语料库上进行训练，能够高效提取医疗数据中的结构化信息。OpenMed NER 在12个生物医学NER基准测试中取得了10个数据集的新最优微F1分数，尤其在疾病和化学基准上有显著提升。该模型的训练效率高，单个GPU下训练时间少于12小时，且碳足迹低于1.2公斤CO2e，适合帮助从业者遵守数据保护和人工智能法规。'}}}, {'id': 'https://huggingface.co/papers/2508.00599', 'title': 'DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior', 'url': 'https://huggingface.co/papers/2508.00599', 'abstract': "DPoser-X, a diffusion-based model, addresses the complexity of 3D human poses using variational diffusion sampling and a novel truncated timestep scheduling method, outperforming existing models across various pose benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present DPoser-X, a diffusion-based prior model for 3D whole-body human poses. Building a versatile and robust full-body human pose prior remains challenging due to the inherent complexity of articulated human poses and the scarcity of high-quality whole-body pose datasets. To address these limitations, we introduce a Diffusion model as body Pose prior (DPoser) and extend it to DPoser-X for expressive whole-body human pose modeling. Our approach unifies various pose-centric tasks as inverse problems, solving them through variational diffusion sampling. To enhance performance on downstream applications, we introduce a novel truncated timestep scheduling method specifically designed for pose data characteristics. We also propose a masked training mechanism that effectively combines whole-body and part-specific datasets, enabling our model to capture interdependencies between body parts while avoiding overfitting to specific actions. Extensive experiments demonstrate DPoser-X's robustness and versatility across multiple benchmarks for body, hand, face, and full-body pose modeling. Our model consistently outperforms state-of-the-art alternatives, establishing a new benchmark for whole-body human pose prior modeling.", 'score': 2, 'issue_id': 5235, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '52b3dd3f31a8d796', 'authors': ['Junzhe Lu', 'Jing Lin', 'Hongkun Dou', 'Ailing Zeng', 'Yue Deng', 'Xian Liu', 'Zhongang Cai', 'Lei Yang', 'Yulun Zhang', 'Haoqian Wang', 'Ziwei Liu'], 'affiliations': ['Beihang University', 'Independent Researcher', 'NVIDIA Research', 'Nanyang Technological University', 'SenseTime Research', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00599.jpg', 'data': {'categories': ['#benchmark', '#3d', '#diffusion'], 'emoji': '🕺', 'ru': {'title': 'Революция в моделировании поз человека с помощью диффузионных моделей', 'desc': 'DPoser-X - это модель на основе диффузии для моделирования трехмерных поз человека. Она использует вариационную выборку диффузии и новый метод планирования усеченных временных шагов. Модель объединяет различные задачи, связанные с позами, как обратные задачи и решает их с помощью вариационной выборки диффузии. DPoser-X превосходит существующие модели по различным критериям оценки поз.'}, 'en': {'title': 'Revolutionizing 3D Human Pose Generation with DPoser-X', 'desc': 'DPoser-X is a diffusion-based model designed to improve the generation of 3D human poses by utilizing variational diffusion sampling. It addresses the challenges of modeling complex articulated poses and the lack of high-quality datasets by introducing a novel truncated timestep scheduling method tailored for pose data. The model treats various pose-related tasks as inverse problems, allowing it to effectively learn from both whole-body and part-specific datasets. Extensive testing shows that DPoser-X outperforms existing models, setting a new standard for whole-body human pose modeling.'}, 'zh': {'title': 'DPoser-X：突破3D人体姿态建模的极限', 'desc': 'DPoser-X是一种基于扩散的模型，旨在解决3D人体姿态建模的复杂性。它通过变分扩散采样和新颖的截断时间调度方法，提升了在各种姿态基准测试中的表现。该模型将多种姿态相关任务统一为逆问题，通过变分扩散采样进行求解。实验结果表明，DPoser-X在全身、手部、面部和全身姿态建模方面表现出色，超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2508.03983', 'title': 'MiDashengLM: Efficient Audio Understanding with General Audio Captions', 'url': 'https://huggingface.co/papers/2508.03983', 'abstract': 'MiDashengLM is an open audio-language model using general audio captions for efficient and comprehensive audio understanding, offering faster processing and higher throughput compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Current approaches for large audio language models (LALMs) often rely on closed data sources or proprietary models, limiting their generalization and accessibility. This paper introduces MiDashengLM, a novel open audio-language model designed for efficient and comprehensive audio understanding through the use of general audio captions using our novel ACAVCaps training dataset. MiDashengLM exclusively relies on publicly available pretraining and supervised fine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At its core, MiDashengLM integrates Dasheng, an open-source audio encoder, specifically engineered to process diverse auditory information effectively. Unlike previous works primarily focused on Automatic Speech Recognition (ASR) based audio-text alignment, our strategy centers on general audio captions, fusing speech, sound and music information into one textual representation, enabling a holistic textual representation of complex audio scenes. Lastly, MiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT) and up to 20x higher throughput than comparable models. Checkpoints are available online at https://huggingface.co/mispeech/midashenglm-7b and https://github.com/xiaomi-research/dasheng-lm.', 'score': 1, 'issue_id': 5227, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '6455762dc1eab458', 'authors': ['Heinrich Dinkel', 'Gang Li', 'Jizhong Liu', 'Jian Luan', 'Yadong Niu', 'Xingwei Sun', 'Tianzi Wang', 'Qiyang Xiao', 'Junbo Zhang', 'Jiahao Zhou'], 'affiliations': ['Horizon Team, MiLM Plus Xiaomi Inc., China'], 'pdf_title_img': 'assets/pdf/title_img/2508.03983.jpg', 'data': {'categories': ['#audio', '#dataset', '#open_source'], 'emoji': '🎧', 'ru': {'title': 'MiDashengLM: Открытая аудио-языковая модель для эффективного понимания звука', 'desc': 'MiDashengLM - это новая открытая аудио-языковая модель, разработанная для эффективного и всестороннего понимания аудио с использованием общих аудио-подписей. Модель интегрирует Dasheng, аудио-энкодер с открытым исходным кодом, специально разработанный для эффективной обработки разнообразной аудиоинформации. В отличие от предыдущих работ, сосредоточенных на выравнивании аудио-текста на основе автоматического распознавания речи (ASR), стратегия MiDashengLM фокусируется на общих аудио-подписях, объединяя информацию о речи, звуке и музыке в одно текстовое представление. MiDashengLM обеспечивает ускорение до 4 раз с точки зрения времени до первого токена (TTFT) и до 20 раз более высокую пропускную способность по сравнению с аналогичными моделями.'}, 'en': {'title': 'Revolutionizing Audio Understanding with MiDashengLM', 'desc': 'MiDashengLM is an innovative open audio-language model that enhances audio understanding by utilizing general audio captions. It leverages a unique training dataset called ACAVCaps, which is built from publicly available data, ensuring transparency and reproducibility in its development. Unlike traditional models that focus mainly on Automatic Speech Recognition (ASR), MiDashengLM integrates various audio elements like speech, sound, and music into a unified textual representation. This model significantly improves processing speed, achieving up to 4 times faster time-to-first-token and up to 20 times higher throughput compared to existing models.'}, 'zh': {'title': '高效音频理解的开放模型', 'desc': 'MiDashengLM是一种开放的音频语言模型，旨在通过使用通用音频标题实现高效和全面的音频理解。与现有模型相比，它提供了更快的处理速度和更高的吞吐量。该模型依赖于公开可用的预训练和监督微调数据集，确保了透明性和可重复性。MiDashengLM将语音、声音和音乐信息融合为一个文本表示，能够全面描述复杂的音频场景。'}}}, {'id': 'https://huggingface.co/papers/2508.03970', 'title': 'Data and AI governance: Promoting equity, ethics, and fairness in large\n  language models', 'url': 'https://huggingface.co/papers/2508.03970', 'abstract': 'Approaches to govern, assess, and quantify bias in machine learning models, particularly large language models, are discussed, emphasizing data and AI governance frameworks for ethical deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we cover approaches to systematically govern, assess and quantify bias across the complete life cycle of machine learning models, from initial development and validation to ongoing production monitoring and guardrail implementation. Building upon our foundational work on the Bias Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the authors share prevalent bias and fairness related gaps in Large Language Models (LLMs) and discuss data and AI governance framework to address Bias, Ethics, Fairness, and Factuality within LLMs. The data and AI governance approach discussed in this paper is suitable for practical, real-world applications, enabling rigorous benchmarking of LLMs prior to production deployment, facilitating continuous real-time evaluation, and proactively governing LLM generated responses. By implementing the data and AI governance across the life cycle of AI development, organizations can significantly enhance the safety and responsibility of their GenAI systems, effectively mitigating risks of discrimination and protecting against potential reputational or brand-related harm. Ultimately, through this article, we aim to contribute to advancement of the creation and deployment of socially responsible and ethically aligned generative artificial intelligence powered applications.', 'score': 1, 'issue_id': 5237, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'f7776b273b77fe9b', 'authors': ['Alok Abhishek', 'Lisa Erickson', 'Tushar Bandopadhyay'], 'affiliations': ['Independent Researcher', 'alum.mit.edu', 'kronml.com'], 'pdf_title_img': 'assets/pdf/title_img/2508.03970.jpg', 'data': {'categories': ['#multimodal', '#data', '#ethics', '#benchmark'], 'emoji': '⚖️', 'ru': {'title': 'Этичный ИИ: управление предвзятостью в языковых моделях', 'desc': 'В статье обсуждаются подходы к управлению, оценке и количественному измерению предвзятости в моделях машинного обучения, особенно в больших языковых моделях (LLM). Авторы представляют набор инструментов BEATS для оценки предвзятости в LLM и описывают структуру управления данными и ИИ для решения проблем этики, справедливости и фактической точности. Предлагаемый подход позволяет проводить тщательное тестирование LLM перед внедрением и обеспечивает непрерывную оценку в реальном времени. Цель статьи - способствовать созданию и внедрению социально ответственных и этичных приложений на основе генеративного ИИ.'}, 'en': {'title': 'Ensuring Ethical AI: Governing Bias in Large Language Models', 'desc': "This paper discusses methods to manage and measure bias in machine learning models, especially large language models (LLMs). It introduces the Bias Evaluation and Assessment Test Suite (BEATS) to identify fairness issues throughout the model's life cycle, from development to deployment. The authors propose a governance framework that ensures ethical practices in AI, focusing on bias, fairness, and factual accuracy. By applying these governance strategies, organizations can improve the safety and ethical standards of their AI systems, reducing risks of discrimination and reputational damage."}, 'zh': {'title': '治理偏见，构建负责任的人工智能', 'desc': '本文讨论了如何在机器学习模型，特别是大型语言模型中，系统性地管理、评估和量化偏见。我们提出了一种数据和人工智能治理框架，以确保在模型开发、验证和生产监控的整个生命周期中，能够有效应对偏见、伦理、公平性和事实性问题。通过实施这一治理框架，组织可以在生产部署前对大型语言模型进行严格的基准测试，并持续实时评估模型的输出。最终，我们希望推动社会责任和伦理对齐的生成式人工智能应用的创建和部署。'}}}, {'id': 'https://huggingface.co/papers/2508.03448', 'title': 'SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering', 'url': 'https://huggingface.co/papers/2508.03448', 'abstract': "SonicMaster, a unified generative model, improves music audio quality by addressing various artifacts using text-based control and a flow-matching generative training paradigm.  \t\t\t\t\tAI-generated summary \t\t\t\t Music recordings often suffer from audio quality issues such as excessive reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo image, especially when created in non-professional settings without specialized equipment or expertise. These problems are typically corrected using separate specialized tools and manual adjustments. In this paper, we introduce SonicMaster, the first unified generative model for music restoration and mastering that addresses a broad spectrum of audio artifacts with text-based control. SonicMaster is conditioned on natural language instructions to apply targeted enhancements, or can operate in an automatic mode for general restoration. To train this model, we construct the SonicMaster dataset, a large dataset of paired degraded and high-quality tracks by simulating common degradation types with nineteen degradation functions belonging to five enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our approach leverages a flow-matching generative training paradigm to learn an audio transformation that maps degraded inputs to their cleaned, mastered versions guided by text prompts. Objective audio quality metrics demonstrate that SonicMaster significantly improves sound quality across all artifact categories. Furthermore, subjective listening tests confirm that listeners prefer SonicMaster's enhanced outputs over the original degraded audio, highlighting the effectiveness of our unified approach.", 'score': 1, 'issue_id': 5223, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': '534674700c0af141', 'authors': ['Jan Melechovsky', 'Ambuj Mehrish', 'Dorien Herremans'], 'affiliations': ['Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2508.03448.jpg', 'data': {'categories': ['#audio', '#dataset', '#synthetic'], 'emoji': '🎵', 'ru': {'title': 'Универсальное улучшение качества музыки с помощью ИИ', 'desc': 'SonicMaster - это унифицированная генеративная модель для улучшения качества аудио в музыке. Она использует управление на основе текста и парадигму обучения с сопоставлением потоков для устранения различных артефактов. Модель может работать как в автоматическом режиме, так и с целевыми улучшениями на основе текстовых инструкций. SonicMaster обучается на специально созданном наборе данных, включающем пары деградированных и высококачественных треков.'}, 'en': {'title': 'SonicMaster: Revolutionizing Music Restoration with AI', 'desc': 'SonicMaster is a novel generative model designed to enhance music audio quality by correcting various audio artifacts. It utilizes text-based control to allow users to specify desired improvements, making it versatile for both targeted and automatic restoration. The model is trained on a large dataset that pairs degraded audio tracks with their high-quality counterparts, using a flow-matching generative training approach. Results show that SonicMaster significantly enhances sound quality, as confirmed by both objective metrics and subjective listener preferences.'}, 'zh': {'title': 'SonicMaster：音乐音频质量的统一生成模型', 'desc': 'SonicMaster是一种统一的生成模型，旨在通过文本控制和流匹配生成训练范式来改善音乐音频质量。该模型能够处理多种音频伪影，如混响过度、失真和音调不平衡等，尤其适用于非专业环境下录制的音乐。SonicMaster通过自然语言指令进行有针对性的增强，或在自动模式下进行一般修复。实验结果表明，SonicMaster在所有伪影类别中显著提高了音质，且听众更喜欢其增强的输出。'}}}, {'id': 'https://huggingface.co/papers/2508.03178', 'title': 'Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and\n  Self-Checking for Complex Instruction Following', 'url': 'https://huggingface.co/papers/2508.03178', 'abstract': 'A framework using entropy-preserving supervised fine-tuning and token-wise entropy-adaptive reinforcement learning improves instruction adherence in LLMs by fostering rigorous reasoning processes.  \t\t\t\t\tAI-generated summary \t\t\t\t While advancements in the reasoning abilities of LLMs have significantly enhanced their performance in solving mathematical problems, coding tasks, and general puzzles, their effectiveness in accurately adhering to instructions remains inconsistent, particularly with more complex directives. Our investigation identifies lazy reasoning during the thinking stage as the primary factor contributing to poor instruction adherence. To mitigate this issue, we propose a comprehensive framework designed to enable rigorous reasoning processes involving preview and self-checking, essential for satisfying strict instruction constraints. Specifically, we first generate instructions with complex constraints and apply a filtering process to obtain valid prompts, resulting in three distinct prompt datasets categorized as hard, easy, and pass. Then, we employ rejection sampling on the pass prompts to curate a small yet high-quality dataset, enabling a cold-start initialization of the model and facilitating its adaptation to effective reasoning patterns. Subsequently, we employ an entropy-preserving supervised fine-tuning (Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL) reinforcement learning guided by rule-based dense rewards. This approach encourages the model to transform its reasoning mechanism, ultimately fostering generalizable reasoning abilities that encompass preview and self-checking. Extensive experiments conducted on instruction-following benchmarks demonstrate remarkable performance improvements across various model scales. Notably, our Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1 and closed-source models like Doubao-1.6.', 'score': 1, 'issue_id': 5229, 'pub_date': '2025-08-05', 'pub_date_card': {'ru': '5 августа', 'en': 'August 5', 'zh': '8月5日'}, 'hash': 'b68fc5d4f6ed55e1', 'authors': ['Chenyang Wang', 'Liang Wen', 'Shousheng Jia', 'Xiangzheng Zhang', 'Liang Xu'], 'affiliations': ['CLUE', 'Harbin Institute of Technology', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2508.03178.jpg', 'data': {'categories': ['#optimization', '#rl', '#dataset', '#benchmark', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Точное следование инструкциям через глубокие рассуждения', 'desc': 'Статья представляет новый фреймворк для улучшения способности языковых моделей следовать инструкциям. Авторы используют энтропийно-сохраняющее обучение с учителем и токен-адаптивное обучение с подкреплением для развития навыков тщательных рассуждений. Метод включает предварительный просмотр и самопроверку для соблюдения сложных ограничений в инструкциях. Эксперименты показывают значительное улучшение производительности на различных бенчмарках, превосходя более крупные модели.'}, 'en': {'title': 'Enhancing Instruction Adherence in LLMs through Rigorous Reasoning', 'desc': 'This paper presents a new framework that enhances the ability of large language models (LLMs) to follow complex instructions by improving their reasoning processes. The authors identify that poor instruction adherence is often due to lazy reasoning, particularly during the initial thinking phase. To address this, they introduce an entropy-preserving supervised fine-tuning method and a token-wise entropy-adaptive reinforcement learning strategy, which together promote rigorous reasoning through self-checking and previewing. Their experiments show that this approach significantly boosts performance on instruction-following tasks, outperforming both larger open-source and closed-source models.'}, 'zh': {'title': '提升指令遵循能力的推理框架', 'desc': '本论文提出了一种新的框架，通过保持熵的监督微调和逐词熵自适应强化学习，来提高大型语言模型（LLMs）对指令的遵循能力。研究发现，懒惰推理是导致指令遵循不佳的主要原因，因此我们设计了一个综合框架，促进严格的推理过程，包括预览和自我检查。我们生成了具有复杂约束的指令，并通过过滤过程获得有效的提示，最终形成了三个不同的提示数据集。通过实验，我们的模型在遵循指令的基准测试中表现出显著的性能提升，尤其是我们的Light-IF-32B模型超越了许多更大的开源和闭源模型。'}}}, {'id': 'https://huggingface.co/papers/2508.01311', 'title': 'C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with\n  Learnable Advisor', 'url': 'https://huggingface.co/papers/2508.01311', 'abstract': 'A continual learning framework for 3D anomaly detection uses Kernel Attention mechanisms and parameter perturbation to handle multiple and emerging classes of point clouds.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Anomaly Detection (AD) has shown great potential in detecting anomalies or defects of high-precision industrial products. However, existing methods are typically trained in a class-specific manner and also lack the capability of learning from emerging classes. In this study, we proposed a continual learning framework named Continual 3D Anomaly Detection (C3D-AD), which can not only learn generalized representations for multi-class point clouds but also handle new classes emerging over time.Specifically, in the feature extraction module, to extract generalized local features from diverse product types of different tasks efficiently, Kernel Attention with random feature Layer (KAL) is introduced, which normalizes the feature space. Then, to reconstruct data correctly and continually, an efficient Kernel Attention with learnable Advisor (KAA) mechanism is proposed, which learns the information from new categories while discarding redundant old information within both the encoder and decoder. Finally, to keep the representation consistency over tasks, a Reconstruction with Parameter Perturbation (RPP) module is proposed by designing a representation rehearsal loss function, which ensures that the model remembers previous category information and returns category-adaptive representation.Extensive experiments on three public datasets demonstrate the effectiveness of the proposed method, achieving an average performance of 66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD, respectively.', 'score': 1, 'issue_id': 5230, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': 'af26fb044fbca284', 'authors': ['Haoquan Lu', 'Hanzhe Liang', 'Jie Zhang', 'Chenxi Hu', 'Jinbao Wang', 'Can Gao'], 'affiliations': ['College of Computer Science and Software Engineering, Shenzhen University', 'Faculty of Applied Sciences, Macao Polytechnic University', 'Guangdong Provincial Key Laboratory of Intelligent Information Processing', 'Ningbo Institute of Digital Twin, Eastern Institute of Technology', 'School of Artificial Intelligence, Shenzhen University', 'Shenzhen Audencia Financial Technology Institute, Shenzhen University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01311.jpg', 'data': {'categories': ['#training', '#3d', '#dataset', '#optimization'], 'emoji': '🔍', 'ru': {'title': 'Непрерывное обучение для обнаружения 3D-аномалий: от многоклассовости к новым классам', 'desc': 'Предложена система непрерывного обучения C3D-AD для обнаружения аномалий в 3D-облаках точек. Она использует механизмы ядерного внимания для извлечения обобщенных признаков из разных типов продуктов. Система способна эффективно обрабатывать как существующие, так и новые классы объектов. Эксперименты на трех наборах данных показали высокую эффективность предложенного метода.'}, 'en': {'title': 'Adapting to Anomalies: Continual Learning in 3D Detection', 'desc': 'This paper presents a continual learning framework called Continual 3D Anomaly Detection (C3D-AD) designed for detecting anomalies in 3D point clouds. It addresses the limitations of traditional methods that are class-specific and unable to adapt to new classes over time. The framework utilizes Kernel Attention mechanisms to efficiently extract generalized features and incorporates a parameter perturbation strategy to maintain representation consistency across tasks. Experimental results on multiple datasets show that C3D-AD significantly improves anomaly detection performance while adapting to emerging classes.'}, 'zh': {'title': '持续学习，智能检测3D异常', 'desc': '本研究提出了一种名为C3D-AD的持续学习框架，用于3D异常检测。该框架利用核注意力机制和参数扰动，能够处理多类点云数据并适应新出现的类别。通过引入随机特征层的核注意力，模型能够高效提取多样化产品类型的特征。同时，重建模块通过学习新类别的信息，确保模型在处理新任务时仍能保持对旧类别的记忆。'}}}, {'id': 'https://huggingface.co/papers/2508.00428', 'title': 'Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D\n  Generation', 'url': 'https://huggingface.co/papers/2508.00428', 'abstract': 'Sel3DCraft enhances text-to-3D generation through a dual-branch retrieval and generation system, multi-view hybrid scoring with MLLMs, and prompt-driven visual analytics, improving designer creativity.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and user studies demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers.', 'score': 1, 'issue_id': 5229, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '7c82937efeb350cb', 'authors': ['Nan Xiang', 'Tianyi Liang', 'Haiwen Huang', 'Shiqi Jiang', 'Hao Huang', 'Yifei Huang', 'Liangyu Chen', 'Changbo Wang', 'Chenhui Li'], 'affiliations': ['East China Normal University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00428.jpg', 'data': {'categories': ['#games', '#3d', '#optimization', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Sel3DCraft: революция в визуальной инженерии промптов для 3D-генерации', 'desc': 'Sel3DCraft - это система визуальной инженерии промптов для генерации текста в 3D, которая преобразует неструктурированное исследование в управляемый визуальный процесс. Она использует двухветвевую структуру, сочетающую поиск и генерацию для исследования разнообразных кандидатов. Система применяет многоракурсный гибридный подход к оценке с использованием мультимодальных языковых моделей (MLLM) и инновационных метрик высокого уровня. Sel3DCraft также включает набор инструментов визуальной аналитики на основе промптов, позволяющий интуитивно выявлять и устранять дефекты.'}, 'en': {'title': 'Transforming Text-to-3D Generation for Enhanced Creativity', 'desc': 'Sel3DCraft is a novel system designed to improve text-to-3D (T23D) generation by integrating a dual-branch approach that combines retrieval and generation methods. This system addresses the challenges of 3D model creation by implementing multi-view hybrid scoring, which utilizes machine learning language models (MLLMs) to ensure consistency and quality in the generated outputs. Additionally, Sel3DCraft features a prompt-driven visual analytics tool that helps designers identify and refine defects in their 3D models more intuitively. Overall, extensive testing shows that Sel3DCraft significantly enhances the creative process for designers compared to existing T23D systems.'}, 'zh': {'title': '提升设计师创造力的三维生成系统', 'desc': 'Sel3DCraft 是一个增强文本到三维生成的系统，采用双分支检索和生成结构，提升设计师的创造力。该系统通过多视角混合评分和大规模语言模型（MLLMs）来评估三维模型的一致性，解决了传统方法中的盲目试错问题。它还引入了基于提示的视觉分析工具，帮助用户直观地识别和改进缺陷。经过广泛测试，Sel3DCraft 在支持设计师创造力方面表现优于其他文本到三维生成系统。'}}}, {'id': 'https://huggingface.co/papers/2507.23313', 'title': 'The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models', 'url': 'https://huggingface.co/papers/2507.23313', 'abstract': 'Transformer-based text-to-image diffusion models show varying degrees of content-style separation in generated artworks, as revealed by cross-attention heatmaps.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at https://github.com/umilISLab/artistic-prompt-interpretation.', 'score': 1, 'issue_id': 5221, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': 'c584e9c932383ec6', 'authors': ['Alfio Ferrara', 'Sergio Picascia', 'Elisabetta Rocchetti'], 'affiliations': ['Department of Computer Science, Università degli Studi di Milano, Via Celoria, 18, 20133 Milan, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2507.23313.jpg', 'data': {'categories': ['#dataset', '#open_source', '#multimodal', '#interpretability', '#cv', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Скрытое понимание искусства: как ИИ разделяет содержание и стиль', 'desc': 'Исследование показало, что трансформерные модели диффузии для генерации изображений по тексту демонстрируют различную степень разделения содержания и стиля в создаваемых произведениях искусства. Анализ проводился с использованием тепловых карт кросс-внимания, которые позволяют соотнести пиксели сгенерированных изображений с конкретными токенами промпта. Результаты выявили, что во многих случаях токены содержания влияют преимущественно на области, связанные с объектами, в то время как токены стиля воздействуют на фон и текстуры. Это указывает на то, что модели диффузии формируют некоторое внутреннее представление о различии между содержанием и стилем без явного обучения этому.'}, 'en': {'title': 'Decoding Art: Understanding Content and Style in AI-Generated Images', 'desc': 'This paper explores how transformer-based text-to-image diffusion models generate artworks by analyzing their internal representation of content and style. Using cross-attention heatmaps, the authors investigate how different prompt tokens influence specific regions of generated images, revealing a separation between content and style. The study finds that content tokens mainly affect object-related areas, while style tokens influence backgrounds and textures, indicating an emergent understanding of these concepts. This research enhances our knowledge of generative models and their ability to represent complex artistic ideas without direct supervision.'}, 'zh': {'title': '探索内容与风格的分离：扩散模型的艺术生成', 'desc': '本研究探讨了基于变换器的文本到图像扩散模型在生成艺术作品时如何编码内容和风格的概念。通过交叉注意力热图，我们能够将生成图像中的像素归因于特定的提示令牌，从而区分受内容描述和风格描述影响的图像区域。研究发现，扩散模型在不同艺术提示和风格请求下表现出不同程度的内容与风格分离。结果表明，内容令牌主要影响与物体相关的区域，而风格令牌则影响背景和纹理区域，显示出模型对内容与风格区分的理解。'}}}, {'id': 'https://huggingface.co/papers/2508.04440', 'title': 'StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion', 'url': 'https://huggingface.co/papers/2508.04440', 'abstract': 'ThinkingF, a data synthesis and training pipeline, enhances autoformalization by improving formal knowledge and informal-to-formal reasoning, achieving state-of-the-art results in formalization tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoformalization aims to translate natural-language mathematical statements into a formal language. While LLMs have accelerated progress in this area, existing methods still suffer from low accuracy. We identify two key abilities for effective autoformalization: comprehensive mastery of formal-language domain knowledge, and reasoning capability of natural language problem understanding and informal-formal alignment. Without the former, a model cannot identify the correct formal objects; without the latter, it struggles to interpret real-world contexts and map them precisely into formal expressions. To address these gaps, we introduce ThinkingF, a data synthesis and training pipeline that improves both abilities. First, we construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge, and another by generating informal-to-formal reasoning trajectories guided by expert-designed templates. We then apply SFT and RLVR with these datasets to further fuse and refine the two abilities. The resulting 7B and 32B models exhibit both comprehensive formal knowledge and strong informal-to-formal reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior general-purpose and specialized models.', 'score': 0, 'issue_id': 5230, 'pub_date': '2025-08-06', 'pub_date_card': {'ru': '6 августа', 'en': 'August 6', 'zh': '8月6日'}, 'hash': '6d2a57f491626d3a', 'authors': ['Yutong Wu', 'Di Huang', 'Ruosi Wan', 'Yue Peng', 'Shijie Shang', 'Chenrui Cao', 'Lei Qi', 'Rui Zhang', 'Zidong Du', 'Jie Yan', 'Xing Hu'], 'affiliations': ['SKL of Processors, Institute of Computing Technology, CAS', 'StepFun Inc.', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2508.04440.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#reasoning', '#data', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'ThinkingF: мост между неформальной и формальной математикой', 'desc': 'ThinkingF - это новый подход к автоформализации, улучшающий формальные знания и рассуждения от неформального к формальному. Метод включает создание двух наборов данных: один с примерами, богатыми формальными знаниями, другой с траекториями рассуждений от неформального к формальному. Применяется обучение с учителем (SFT) и обучение с подкреплением (RLVR) для объединения этих способностей. Результирующие модели достигают наилучших результатов в задачах формализации на бенчмарках FormalMATH-Lite и ProverBench.'}, 'en': {'title': 'ThinkingF: Bridging Natural Language and Formal Knowledge', 'desc': 'The paper presents ThinkingF, a novel data synthesis and training pipeline designed to enhance autoformalization, which is the process of converting natural-language mathematical statements into formal language. It identifies two critical skills necessary for effective autoformalization: mastery of formal-language knowledge and the ability to reason from informal to formal contexts. To improve these skills, ThinkingF constructs two specialized datasets and employs techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning with Value Regression (RLVR). The resulting models, particularly StepFun-Formalizer-32B, achieve state-of-the-art performance on formalization tasks, demonstrating significant advancements over previous models.'}, 'zh': {'title': '提升自动形式化的ThinkingF', 'desc': '本文介绍了ThinkingF，一个数据合成和训练管道，旨在提升自动形式化的能力。自动形式化的目标是将自然语言数学陈述转换为形式语言。研究发现，成功的自动形式化需要对形式语言领域知识的全面掌握和自然语言问题理解的推理能力。通过构建两个数据集并应用SFT和RLVR，ThinkingF显著提高了模型在形式化任务中的表现，特别是在FormalMATH-Lite和ProverBench上取得了领先的成绩。'}}}, {'id': 'https://huggingface.co/papers/2508.00109', 'title': 'FACTORY: A Challenging Human-Verified Prompt Set for Long-Form\n  Factuality', 'url': 'https://huggingface.co/papers/2508.00109', 'abstract': 'FACTORY, a human-verified prompt set, evaluates the factuality of long-form responses from language models, revealing higher factual accuracy compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts.', 'score': 0, 'issue_id': 5234, 'pub_date': '2025-07-31', 'pub_date_card': {'ru': '31 июля', 'en': 'July 31', 'zh': '7月31日'}, 'hash': '6dd83d8a170db99d', 'authors': ['Mingda Chen', 'Yang Li', 'Xilun Chen', 'Adina Williams', 'Gargi Ghosh', 'Scott Yih'], 'affiliations': ['Meta'], 'pdf_title_img': 'assets/pdf/title_img/2508.00109.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#dataset', '#long_context', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'FACTORY: Новый стандарт оценки фактической точности языковых моделей', 'desc': 'FACTORY - это набор проверенных человеком промптов для оценки фактической точности длинных ответов языковых моделей. Он был разработан с использованием подхода model-in-the-loop и уточнен людьми. FACTORY включает сложные промпты, которые ориентированы на факты, имеют однозначные ответы и не допускают двусмысленности. Результаты показывают, что FACTORY является сложным бенчмарком: около 40% утверждений в ответах современных моделей не фактичны.'}, 'en': {'title': 'FACTORY: Elevating Factual Accuracy in Language Models', 'desc': 'The paper introduces FACTORY, a new benchmark for evaluating the factual accuracy of long-form responses generated by language models. Unlike existing datasets, FACTORY is human-verified, ensuring higher quality and reliability in assessing model outputs. The study reveals that state-of-the-art models struggle with factual accuracy, with around 40% of their claims being incorrect when evaluated with FACTORY. This highlights the importance of using robust, human-verified datasets to improve the reasoning capabilities of language models across diverse factual scenarios.'}, 'zh': {'title': 'FACTORY：提升语言模型的事实准确性', 'desc': 'FACTORY是一个经过人工验证的提示集，用于评估语言模型生成长文本响应的事实准确性。与现有数据集相比，FACTORY显示出更高的事实准确性，因为它采用了模型循环的方法，并经过人类的精细调整。通过对六种最先进的语言模型进行评估，我们发现大约40%的响应声明并不真实，而其他数据集的这一比例仅为10%。这表明FACTORY在评估模型的长尾事实推理能力方面具有更高的可靠性。'}}}, {'id': 'https://huggingface.co/papers/2508.11630', 'title': 'Thyme: Think Beyond Images', 'url': 'https://huggingface.co/papers/2508.11630', 'abstract': "Thyme, a novel paradigm, enables MLLMs to autonomously perform image manipulations and computations, enhancing performance in perception and reasoning tasks through a two-stage training strategy and GRPO-ATS algorithm.  \t\t\t\t\tAI-generated summary \t\t\t\t Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing ``think with images'' approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks.", 'score': 45, 'issue_id': 5394, 'pub_date': '2025-08-15', 'pub_date_card': {'ru': '15 августа', 'en': 'August 15', 'zh': '8月15日'}, 'hash': '9df408fb7ce360bf', 'authors': ['Yi-Fan Zhang', 'Xingyu Lu', 'Shukang Yin', 'Chaoyou Fu', 'Wei Chen', 'Xiao Hu', 'Bin Wen', 'Kaiyu Jiang', 'Changyi Liu', 'Tianke Zhang', 'Haonan Fan', 'Kaibing Chen', 'Jiankang Chen', 'Haojie Ding', 'Kaiyu Tang', 'Zhang Zhang', 'Liang Wang', 'Fan Yang', 'Tingting Gao', 'Guorui Zhou'], 'affiliations': ['CASIA', 'Kwai Keye', 'NJU', 'THU', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2508.11630.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training', '#optimization', '#agents', '#cv', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Тайм: новый уровень мышления ИИ с изображениями', 'desc': 'Тайм (Thyme) - это новая парадигма, позволяющая мультимодальным языковым моделям (MLLM) автономно выполнять манипуляции с изображениями и вычисления. Она использует двухэтапную стратегию обучения и алгоритм GRPO-ATS для улучшения восприятия и рассуждений. Тайм позволяет моделям самостоятельно генерировать и выполнять разнообразные операции обработки изображений и вычисления через исполняемый код. Эксперименты на почти 20 бенчмарках показали значительное и последовательное улучшение производительности, особенно в сложных задачах восприятия изображений высокого разрешения и комплексных рассуждений.'}, 'en': {'title': 'Think Beyond Images with Thyme!', 'desc': "Thyme is a new approach that allows Multi-Modal Language Models (MLLMs) to independently perform various image manipulations and computations, improving their ability to understand and reason about images. It uses a two-stage training method, starting with supervised fine-tuning on a large dataset to teach the model how to generate code for image processing. The second stage involves reinforcement learning to enhance the model's decision-making skills when applying these operations. The GRPO-ATS algorithm is introduced to optimize the balance between exploring reasoning tasks and executing code accurately, leading to better performance on numerous benchmarks."}, 'zh': {'title': '超越图像思维的自主处理能力', 'desc': 'Thyme是一种新颖的范式，旨在使多模态大语言模型（MLLMs）能够自主执行图像处理和计算操作，从而提升感知和推理任务的表现。该方法采用两阶段训练策略，首先在一个包含50万样本的精心策划数据集上进行监督微调（SFT），然后通过强化学习（RL）阶段来优化决策过程。我们提出的GRPO-ATS算法通过对文本和代码生成应用不同的温度，平衡推理探索与代码执行的精确性。实验结果表明，Thyme在近20个基准测试中表现出显著且一致的性能提升，尤其是在高分辨率感知和复杂推理任务中。'}}}, {'id': 'https://huggingface.co/papers/2508.10874', 'title': 'SSRL: Self-Search Reinforcement Learning', 'url': 'https://huggingface.co/papers/2508.10874', 'abstract': "LLMs can serve as efficient simulators for RL tasks, reducing reliance on external search engines through a method called Self-Search RL, which enhances internal knowledge utilization.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.", 'score': 39, 'issue_id': 5400, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': 'dbeffb2b7b345a96', 'authors': ['Yuchen Fan', 'Kaiyan Zhang', 'Heng Zhou', 'Yuxin Zuo', 'Yanxu Chen', 'Yu Fu', 'Xinwei Long', 'Xuekai Zhu', 'Che Jiang', 'Yuchen Zhang', 'Li Kang', 'Gang Chen', 'Cheng Huang', 'Zhizhou He', 'Bingning Wang', 'Lei Bai', 'Ning Ding', 'Bowen Zhou'], 'affiliations': ['CSCEC Third Bureau', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University College London', 'WeChat AI'], 'pdf_title_img': 'assets/pdf/title_img/2508.10874.jpg', 'data': {'categories': ['#rlhf', '#transfer_learning', '#rl', '#agents', '#hallucinations', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'LLM как эффективные симуляторы для обучения с подкреплением', 'desc': 'Исследование показывает, что большие языковые модели (LLM) могут эффективно симулировать задачи поиска в обучении с подкреплением (RL). Предложенный метод Self-Search RL (SSRL) улучшает использование внутренних знаний модели через структурированные подсказки и повторную выборку. SSRL позволяет моделям итеративно улучшать использование знаний без доступа к внешним инструментам. Эмпирические оценки демонстрируют, что модели, обученные с помощью SSRL, обеспечивают экономически эффективную и стабильную среду для обучения RL, уменьшая зависимость от внешних поисковых систем.'}, 'en': {'title': 'Harnessing LLMs for Efficient Reinforcement Learning Simulations', 'desc': "This paper explores how large language models (LLMs) can act as effective simulators for reinforcement learning (RL) tasks, minimizing the need for external search engines. The authors introduce a method called Self-Search RL (SSRL), which enhances the LLMs' ability to utilize their internal knowledge through structured prompting and rewards. The study shows that LLMs can achieve high performance on question-answering tasks, indicating their strong search capabilities. Ultimately, SSRL allows for more efficient and stable RL training by leveraging the LLMs' internal knowledge, leading to better sim-to-real transfer and reduced reliance on external tools."}, 'zh': {'title': '利用LLMs提升强化学习的效率', 'desc': '本文探讨了大型语言模型（LLMs）在强化学习（RL）任务中作为高效模拟器的潜力，减少对外部搜索引擎的依赖。我们提出了一种称为自搜索强化学习（Self-Search RL, SSRL）的方法，通过格式化和规则奖励来增强LLMs的自搜索能力。研究表明，LLMs在推理预算方面表现出强大的扩展性，能够在问答基准测试中取得高分。我们的实证评估显示，SSRL训练的策略模型为搜索驱动的RL训练提供了成本效益高且稳定的环境。'}}}, {'id': 'https://huggingface.co/papers/2508.10104', 'title': 'DINOv3', 'url': 'https://huggingface.co/papers/2508.10104', 'abstract': "DINOv3, a self-supervised learning model, achieves superior performance across various vision tasks by scaling datasets and models, addressing dense feature degradation, and enhancing flexibility with post-hoc strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.", 'score': 25, 'issue_id': 5400, 'pub_date': '2025-08-13', 'pub_date_card': {'ru': '13 августа', 'en': 'August 13', 'zh': '8月13日'}, 'hash': '98d721aa762199e4', 'authors': ['Oriane Siméoni', 'Huy V. Vo', 'Maximilian Seitzer', 'Federico Baldassarre', 'Maxime Oquab', 'Cijo Jose', 'Vasil Khalidov', 'Marc Szafraniec', 'Seungeun Yi', 'Michaël Ramamonjisoa', 'Francisco Massa', 'Daniel Haziza', 'Luca Wehrstedt', 'Jianyuan Wang', 'Timothée Darcet', 'Théo Moutakanni', 'Leonel Sentana', 'Claire Roberts', 'Andrea Vedaldi', 'Jamie Tolan', 'John Brandt', 'Camille Couprie', 'Julien Mairal', 'Hervé Jégou', 'Patrick Labatut', 'Piotr Bojanowski'], 'affiliations': ['Inria', 'Meta AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2508.10104.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#open_source', '#survey', '#dataset', '#cv'], 'emoji': '🔬', 'ru': {'title': 'DINOv3: Универсальная самообучающаяся модель для задач компьютерного зрения', 'desc': 'DINOv3 - это модель самоконтролируемого обучения, которая достигает превосходных результатов в различных задачах компьютерного зрения. Модель использует масштабирование наборов данных и архитектуры, решает проблему деградации плотных признаков с помощью метода Gram anchoring. DINOv3 применяет постобработку для повышения гибкости в отношении разрешения, размера модели и согласования с текстом. В результате получена универсальная модель компьютерного зрения, превосходящая специализированные решения в широком спектре задач без дополнительного обучения.'}, 'en': {'title': 'DINOv3: Scaling Self-Supervised Learning for Superior Vision Performance', 'desc': 'DINOv3 is a self-supervised learning model that excels in various vision tasks by effectively scaling datasets and model sizes. It addresses the challenge of dense feature degradation during long training periods through a novel technique called Gram anchoring. Additionally, DINOv3 enhances flexibility with post-hoc strategies that allow for adjustments in resolution and model size without the need for fine-tuning. This model demonstrates superior performance compared to existing self- and weakly-supervised models, making it a significant advancement in the field of vision foundation models.'}, 'zh': {'title': 'DINOv3：自监督学习的新里程碑', 'desc': 'DINOv3是一种自监督学习模型，通过扩展数据集和模型规模，解决了密集特征退化的问题，并通过后处理策略增强了灵活性，从而在各种视觉任务中取得了优异的表现。该模型不需要手动数据标注，能够轻松适应大规模数据集和更大架构，学习来自不同来源的视觉表示。DINOv3采用了简单而有效的策略，包括数据准备、设计和优化，以充分利用数据集和模型规模的优势。最终，DINOv3展示了其在多种设置下的卓越性能，显著超越了以往的自监督和弱监督基础模型。'}}}, {'id': 'https://huggingface.co/papers/2508.11116', 'title': 'PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical\n  Register Indexing', 'url': 'https://huggingface.co/papers/2508.11116', 'abstract': 'PaperRegister enhances paper search by using hierarchical indexing and adaptive retrieval, supporting flexible and fine-grained queries beyond traditional abstract-based systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Paper search is an important activity for researchers, typically involving using a query with description of a topic to find relevant papers. As research deepens, paper search requirements may become more flexible, sometimes involving specific details such as module configuration rather than being limited to coarse-grained topics. However, previous paper search systems are unable to meet these flexible-grained requirements, as these systems mainly collect paper abstracts to construct index of corpus, which lack detailed information to support retrieval by finer-grained queries. In this work, we propose PaperRegister, consisted of offline hierarchical indexing and online adaptive retrieval, transforming traditional abstract-based index into hierarchical index tree for paper search, thereby supporting queries at flexible granularity. Experiments on paper search tasks across a range of granularity demonstrate that PaperRegister achieves the state-of-the-art performance, and particularly excels in fine-grained scenarios, highlighting the good potential as an effective solution for flexible-grained paper search in real-world applications. Code for this work is in https://github.com/Li-Z-Q/PaperRegister.', 'score': 12, 'issue_id': 5394, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': 'f0b9bc66fdef20ad', 'authors': ['Zhuoqun Li', 'Xuanang Chen', 'Hongyu Lin', 'Yaojie Lu', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2508.11116.jpg', 'data': {'categories': ['#data', '#dataset', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'Гибкий поиск научных статей на любом уровне детализации', 'desc': 'PaperRegister - это новая система поиска научных статей, использующая иерархическую индексацию и адаптивный поиск. Она поддерживает гибкие и детализированные запросы, выходящие за рамки традиционных систем на основе аннотаций. PaperRegister преобразует обычный индекс на основе аннотаций в иерархическое дерево индексов, что позволяет выполнять поиск с различной степенью детализации. Эксперименты показали, что PaperRegister достигает наилучших результатов, особенно в сценариях с высокой степенью детализации запросов.'}, 'en': {'title': 'Revolutionizing Paper Search with Hierarchical Indexing', 'desc': 'PaperRegister is a novel system designed to improve the search for academic papers by utilizing hierarchical indexing and adaptive retrieval methods. Unlike traditional systems that rely solely on abstracts, PaperRegister allows for more detailed and flexible queries, accommodating specific research needs. The system organizes papers into a hierarchical index tree, enabling users to perform searches at various levels of granularity. Experimental results show that PaperRegister outperforms existing methods, particularly in scenarios requiring fine-grained search capabilities.'}, 'zh': {'title': 'PaperRegister：灵活细粒度论文搜索的新方法', 'desc': 'PaperRegister 是一种改进论文搜索的方法，它通过层次索引和自适应检索来增强搜索体验。传统的论文搜索系统主要依赖摘要来构建索引，无法满足细粒度查询的需求。PaperRegister 通过将传统的基于摘要的索引转变为层次索引树，支持更灵活的查询方式。实验结果表明，PaperRegister 在各种细粒度的论文搜索任务中表现优异，展示了其在实际应用中的良好潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.10395', 'title': 'XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization', 'url': 'https://huggingface.co/papers/2508.10395', 'abstract': 'XQuant and XQuant-CL reduce memory consumption in LLM inference through low-bit quantization and cross-layer similarity exploitation, achieving significant memory savings with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Although LLM inference has emerged as a critical workload for many downstream applications, efficiently inferring LLMs is challenging due to the substantial memory footprint and bandwidth requirements. In parallel, compute capabilities have steadily outpaced both memory capacity and bandwidth over the last few decades, a trend that remains evident in modern GPU hardware and exacerbates the challenge of LLM inference. As such, new algorithms are emerging that trade increased computation for reduced memory operations. To that end, we present XQuant, which takes advantage of this trend, enabling an order-of-magnitude reduction in memory consumption through low-bit quantization with substantial accuracy benefits relative to state-of-the-art KV cache quantization methods. We accomplish this by quantizing and caching the layer input activations X, instead of using standard KV caching, and then rematerializing the Keys and Values on-the-fly during inference. This results in an immediate 2times memory savings compared to KV caching. By applying XQuant, we achieve up to sim 7.7times memory savings with <0.1 perplexity degradation compared to the FP16 baseline. Furthermore, our approach leverages the fact that X values are similar across layers. Building on this observation, we introduce XQuant-CL, which exploits the cross-layer similarity in the X embeddings for extreme compression. Across different models, XQuant-CL attains up to 10times memory savings relative to the FP16 baseline with only 0.01 perplexity degradation, and 12.5times memory savings with only 0.1 perplexity degradation. XQuant exploits the rapidly increasing compute capabilities of hardware platforms to eliminate the memory bottleneck, while surpassing state-of-the-art KV cache quantization methods and achieving near-FP16 accuracy across a wide range of models.', 'score': 11, 'issue_id': 5400, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '85df7c343192e504', 'authors': ['Aditya Tomar', 'Coleman Hooper', 'Minjae Lee', 'Haocheng Xi', 'Rishabh Tiwari', 'Wonjun Kang', 'Luca Manolache', 'Michael W. Mahoney', 'Kurt Keutzer', 'Amir Gholami'], 'affiliations': ['KAIST', 'Lawrence Berkeley National Laboratory', 'UC Berkeley', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2508.10395.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Революция в эффективности LLM: меньше памяти, та же точность', 'desc': 'XQuant и XQuant-CL - это новые методы для уменьшения потребления памяти при инференсе больших языковых моделей (LLM). Они используют низкобитную квантизацию и эксплуатацию схожести между слоями модели. Эти подходы позволяют достичь значительной экономии памяти при минимальной потере точности по сравнению с базовыми моделями в формате FP16. XQuant-CL демонстрирует особенно впечатляющие результаты, достигая 10-кратной экономии памяти с деградацией перплексии всего на 0.01.'}, 'en': {'title': 'Revolutionizing Memory Efficiency in LLM Inference', 'desc': 'The paper introduces XQuant and XQuant-CL, two innovative methods designed to reduce memory usage during large language model (LLM) inference. By utilizing low-bit quantization and caching layer input activations instead of traditional key-value (KV) caching, these methods achieve significant memory savings while maintaining high accuracy. XQuant can reduce memory consumption by up to 7.7 times with minimal accuracy loss, while XQuant-CL further exploits cross-layer similarities to achieve up to 10 times memory savings. This approach effectively addresses the memory bottleneck in LLM inference, leveraging advancements in computational power to enhance efficiency.'}, 'zh': {'title': '高效内存节省，提升LLM推理能力', 'desc': 'XQuant和XQuant-CL通过低位量化和跨层相似性利用，显著减少了大规模语言模型（LLM）推理中的内存消耗，同时保持了较小的准确性损失。它们通过量化和缓存层输入激活值，而不是使用标准的键值缓存，从而实现了内存的显著节省。XQuant在内存消耗上实现了高达7.7倍的节省，而XQuant-CL则利用跨层相似性，达到了高达10倍的内存节省。这些方法充分利用了现代硬件的计算能力，解决了内存瓶颈问题。'}}}, {'id': 'https://huggingface.co/papers/2508.11203', 'title': 'StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image\n  Translation', 'url': 'https://huggingface.co/papers/2508.11203', 'abstract': 'StyleMM constructs stylized 3DMMs from text descriptions using a diffusion model for image-to-image translation while preserving facial attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce StyleMM, a novel framework that can construct a stylized 3D Morphable Model (3DMM) based on user-defined text descriptions specifying a target style. Building upon a pre-trained mesh deformation network and a texture generator for original 3DMM-based realistic human faces, our approach fine-tunes these models using stylized facial images generated via text-guided image-to-image (i2i) translation with a diffusion model, which serve as stylization targets for the rendered mesh. To prevent undesired changes in identity, facial alignment, or expressions during i2i translation, we introduce a stylization method that explicitly preserves the facial attributes of the source image. By maintaining these critical attributes during image stylization, the proposed approach ensures consistent 3D style transfer across the 3DMM parameter space through image-based training. Once trained, StyleMM enables feed-forward generation of stylized face meshes with explicit control over shape, expression, and texture parameters, producing meshes with consistent vertex connectivity and animatability. Quantitative and qualitative evaluations demonstrate that our approach outperforms state-of-the-art methods in terms of identity-level facial diversity and stylization capability. The code and videos are available at [kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).', 'score': 6, 'issue_id': 5395, 'pub_date': '2025-08-15', 'pub_date_card': {'ru': '15 августа', 'en': 'August 15', 'zh': '8月15日'}, 'hash': '9af71c9ddc545447', 'authors': ['Seungmi Lee', 'Kwan Yun', 'Junyong Noh'], 'affiliations': ['KAIST, Visual Media Lab'], 'pdf_title_img': 'assets/pdf/title_img/2508.11203.jpg', 'data': {'categories': ['#cv', '#3d', '#diffusion', '#open_source'], 'emoji': '🎭', 'ru': {'title': 'Стильные 3D-лица из текста: новый уровень контроля и реализма', 'desc': 'StyleMM - это новая система для создания стилизованных 3D-моделей лиц на основе текстовых описаний. Она использует диффузионную модель для преобразования изображений, сохраняя при этом ключевые черты лица. StyleMM обучается на стилизованных изображениях лиц и позволяет генерировать 3D-модели с контролем над формой, выражением и текстурой. Эксперименты показывают, что этот метод превосходит современные аналоги по разнообразию и возможностям стилизации лиц.'}, 'en': {'title': 'Transforming Text to Stylized 3D Faces with Precision', 'desc': 'StyleMM is a new framework that creates stylized 3D Morphable Models (3DMMs) from text descriptions, using a diffusion model for image-to-image translation. It fine-tunes a pre-trained mesh deformation network and a texture generator to ensure that the generated 3D faces maintain their original facial attributes while adopting new styles. The method prevents changes in identity and expression during the stylization process, allowing for consistent 3D style transfer. Ultimately, StyleMM enables the generation of customizable and animatable stylized face meshes, outperforming existing methods in facial diversity and stylization quality.'}, 'zh': {'title': '风格化3D模型的智能生成', 'desc': 'StyleMM是一个新颖的框架，可以根据用户定义的文本描述构建风格化的3D可变形模型（3DMM）。该方法利用预训练的网格变形网络和纹理生成器，通过扩散模型进行图像到图像的翻译，生成风格化的面部图像作为目标。为了保持面部特征不变，我们引入了一种显式保留源图像面部属性的风格化方法。经过训练后，StyleMM能够生成具有明确形状、表情和纹理参数控制的风格化面部网格，确保在3DMM参数空间中的一致性风格转移。'}}}, {'id': 'https://huggingface.co/papers/2508.11255', 'title': 'FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for\n  Audio-Driven Portrait Animation', 'url': 'https://huggingface.co/papers/2508.11255', 'abstract': 'A multimodal reward model and adaptive preference optimization framework improve audio-driven portrait animation by aligning with human preferences across multiple dimensions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in audio-driven portrait animation have demonstrated impressive capabilities. However, existing methods struggle to align with fine-grained human preferences across multiple dimensions, such as motion naturalness, lip-sync accuracy, and visual quality. This is due to the difficulty of optimizing among competing preference objectives, which often conflict with one another, and the scarcity of large-scale, high-quality datasets with multidimensional preference annotations. To address these, we first introduce Talking-Critic, a multimodal reward model that learns human-aligned reward functions to quantify how well generated videos satisfy multidimensional expectations. Leveraging this model, we curate Talking-NSQ, a large-scale multidimensional human preference dataset containing 410K preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert Preference Optimization (TLPO), a novel framework for aligning diffusion-based portrait animation models with fine-grained, multidimensional preferences. TLPO decouples preferences into specialized expert modules, which are then fused across timesteps and network layers, enabling comprehensive, fine-grained enhancement across all dimensions without mutual interference. Experiments demonstrate that Talking-Critic significantly outperforms existing methods in aligning with human preference ratings. Meanwhile, TLPO achieves substantial improvements over baseline models in lip-sync accuracy, motion naturalness, and visual quality, exhibiting superior performance in both qualitative and quantitative evaluations. Ours project page: https://fantasy-amap.github.io/fantasy-talking2/', 'score': 5, 'issue_id': 5398, 'pub_date': '2025-08-15', 'pub_date_card': {'ru': '15 августа', 'en': 'August 15', 'zh': '8月15日'}, 'hash': 'aff56c9a04ada350', 'authors': ['MengChao Wang', 'Qiang Wang', 'Fan Jiang', 'Mu Xu'], 'affiliations': ['AMAP, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.11255.jpg', 'data': {'categories': ['#data', '#training', '#optimization', '#multimodal', '#diffusion', '#alignment', '#dataset'], 'emoji': '🗣️', 'ru': {'title': 'Многомерная оптимизация анимации портретов с учетом человеческих предпочтений', 'desc': 'Статья представляет новый подход к улучшению анимации портретов, управляемой аудио. Авторы разработали мультимодальную модель вознаграждения Talking-Critic, которая учится оценивать качество генерируемых видео по нескольким измерениям. На основе этой модели был создан большой датасет Talking-NSQ с многомерными оценками предпочтений. Предложен фреймворк TLPO для точной настройки диффузионных моделей анимации портретов в соответствии с многомерными предпочтениями.'}, 'en': {'title': 'Enhancing Portrait Animation with Human-Aligned Preferences', 'desc': 'This paper presents a new approach to improve audio-driven portrait animation by focusing on human preferences. It introduces a multimodal reward model called Talking-Critic, which quantifies how well generated animations meet various human expectations like lip-sync accuracy and visual quality. Additionally, the authors create a large dataset, Talking-NSQ, with 410,000 preference pairs to train their model effectively. They also propose a novel optimization framework, TLPO, that allows for better alignment of animation models with these preferences by using specialized expert modules to enhance multiple dimensions simultaneously.'}, 'zh': {'title': '提升音频驱动肖像动画的多模态优化', 'desc': '本论文提出了一种多模态奖励模型和自适应偏好优化框架，以改善音频驱动的肖像动画。通过引入Talking-Critic模型，研究者能够量化生成视频在多个维度上与人类偏好的对齐程度。为了支持这一模型，研究团队还构建了一个包含41万个偏好对的大规模多维人类偏好数据集Talking-NSQ。最后，提出的TLPO框架通过将偏好解耦为专门的专家模块，实现了在不同时间步和网络层之间的融合，从而在不相互干扰的情况下全面提升动画的质量。'}}}, {'id': 'https://huggingface.co/papers/2508.10868', 'title': 'TexVerse: A Universe of 3D Objects with High-Resolution Textures', 'url': 'https://huggingface.co/papers/2508.10868', 'abstract': 'TexVerse is a large-scale 3D dataset with high-resolution textures, including PBR materials, rigged models, and animated models, suitable for texture synthesis, PBR material development, and 3D vision tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.', 'score': 5, 'issue_id': 5399, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '92f536825f2fff8b', 'authors': ['Yibo Zhang', 'Li Zhang', 'Rui Ma', 'Nan Cao'], 'affiliations': ['Fudan University', 'Jilin University', 'Shanghai Innovation Institute', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2508.10868.jpg', 'data': {'categories': ['#3d', '#dataset'], 'emoji': '🎨', 'ru': {'title': 'TexVerse: Революция в мире 3D-текстур и материалов', 'desc': 'TexVerse - это крупномасштабный набор данных 3D-моделей с высококачественными текстурами, включая PBR-материалы, риггированные и анимированные модели. Датасет содержит более 858 тысяч уникальных 3D-моделей с высоким разрешением, из которых более 158 тысяч имеют физически корректные материалы (PBR). TexVerse включает специализированные подмножества: TexVerse-Skeleton с 69 тысячами риггированных моделей и TexVerse-Animation с 54 тысячами анимированных моделей. Этот набор данных предоставляет широкие возможности для применения в задачах синтеза текстур, разработки PBR-материалов, анимации и различных задач компьютерного зрения и графики.'}, 'en': {'title': 'TexVerse: Elevating 3D Graphics with High-Resolution Textures', 'desc': 'TexVerse is a comprehensive 3D dataset that provides high-resolution textures and models, addressing the need for quality data in texture synthesis and PBR material development. It includes over 858,000 unique 3D models, with a significant portion featuring physically based rendering materials, making it ideal for advanced graphics tasks. The dataset also contains specialized subsets for rigged and animated models, ensuring that users have access to a variety of 3D instances with detailed annotations. This resource is designed to enhance machine learning applications in 3D vision and graphics, facilitating better model training and development.'}, 'zh': {'title': 'TexVerse：高分辨率3D纹理的未来', 'desc': 'TexVerse是一个大规模的3D数据集，包含高分辨率的纹理和PBR材料。该数据集汇集了超过858,000个独特的3D模型，其中包括158,000个具有物理基础渲染（PBR）材料的模型。TexVerse还提供了特定子集，如TexVerse-Skeleton和TexVerse-Animation，分别包含69,000个绑定模型和54,000个动画模型，保留了用户上传的原始骨骼和动画数据。这个数据集为纹理合成、PBR材料开发和3D视觉任务提供了高质量的数据资源。'}}}, {'id': 'https://huggingface.co/papers/2508.11616', 'title': 'Controlling Multimodal LLMs via Reward-guided Decoding', 'url': 'https://huggingface.co/papers/2508.11616', 'abstract': "A reward-guided decoding method for Multimodal Large Language Models (MLLMs) improves visual grounding by controlling object precision and recall, offering dynamic trade-offs between compute and grounding quality.  \t\t\t\t\tAI-generated summary \t\t\t\t As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods.", 'score': 3, 'issue_id': 5396, 'pub_date': '2025-08-15', 'pub_date_card': {'ru': '15 августа', 'en': 'August 15', 'zh': '8月15日'}, 'hash': 'a8b7ff59eca3faf3', 'authors': ['Oscar Mañas', "Pierluca D'Oro", 'Koustuv Sinha', 'Adriana Romero-Soriano', 'Michal Drozdzal', 'Aishwarya Agrawal'], 'affiliations': ['Canada CIFAR AI Chair', 'McGill University', 'Meta FAIR', 'Mila - Quebec AI Institute', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2508.11616.jpg', 'data': {'categories': ['#hallucinations', '#alignment', '#multimodal', '#benchmark', '#inference'], 'emoji': '🎛️', 'ru': {'title': 'Управляемое декодирование MLLM для точной визуальной привязки', 'desc': 'Статья представляет метод управляемого декодирования для мультимодальных больших языковых моделей (MLLM), улучшающий визуальную привязку. Авторы разработали модели вознаграждения для контроля точности и полноты распознавания объектов. Метод позволяет динамически регулировать баланс между вычислительными затратами и качеством визуальной привязки. Эксперименты показали превосходство предложенного подхода над существующими методами снижения галлюцинаций в MLLM.'}, 'en': {'title': 'Dynamic Control for Enhanced Visual Grounding in MLLMs', 'desc': 'This paper presents a novel reward-guided decoding method for Multimodal Large Language Models (MLLMs) that enhances visual grounding by managing object precision and recall. The authors introduce reward models that guide the decoding process, allowing users to adjust the importance of precision versus recall dynamically. This method not only improves the quality of image captioning tasks but also offers flexibility in balancing computational resources with grounding accuracy. The results demonstrate that this approach significantly outperforms existing methods for reducing object hallucination in MLLMs.'}, 'zh': {'title': '动态控制多模态语言模型的视觉定位', 'desc': '本文提出了一种基于奖励引导的解码方法，用于多模态大型语言模型（MLLMs），旨在提高视觉定位的精确度和召回率。我们构建了两个独立的奖励模型，分别控制模型输出中对象的精确度和召回率，从而实现动态的权衡。该方法允许用户在解码过程中实时调整奖励函数的重要性，以适应不同的任务需求。通过在标准对象幻觉基准上进行评估，我们的方法在控制MLLM推理方面表现出显著优势，超越了现有的幻觉缓解方法。'}}}, {'id': 'https://huggingface.co/papers/2508.10461', 'title': 'X-Node: Self-Explanation is All We Need', 'url': 'https://huggingface.co/papers/2508.10461', 'abstract': 'X-Node is a self-explaining GNN framework that generates per-node explanations by encoding local topology features and integrating them into the message-passing pipeline, maintaining accuracy while enhancing interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node\'s latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a "text-injection" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.', 'score': 3, 'issue_id': 5399, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': 'a9f5f6075fa0c601', 'authors': ['Prajit Sengupta', 'Islem Rekik'], 'affiliations': ['BASIRA Lab, Imperial-X (I-X) and Department of Computing, Imperial College London, London, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2508.10461.jpg', 'data': {'categories': ['#cv', '#interpretability', '#healthcare', '#graphs', '#dataset', '#architecture'], 'emoji': '🕸️', 'ru': {'title': 'X-Node: Самообъясняющиеся графовые нейронные сети для интерпретируемого анализа данных', 'desc': 'X-Node - это фреймворк для графовых нейронных сетей (GNN), который генерирует объяснения для каждого узла, кодируя особенности локальной топологии и интегрируя их в процесс передачи сообщений. Он создает структурированный вектор контекста, кодирующий интерпретируемые признаки, такие как степень, центральность и кластеризация узла. Легковесный модуль Reasoner преобразует этот контекст в компактный вектор объяснения, который используется для реконструкции скрытого представления узла, генерации текстового объяснения с помощью языковой модели и улучшения работы самой GNN. X-Node поддерживает конкурентоспособную точность классификации, одновременно предоставляя объяснения для каждого узла.'}, 'en': {'title': 'X-Node: Empowering GNNs with Self-Explanations for Trustworthy AI', 'desc': "X-Node is a novel framework for graph neural networks (GNNs) that enhances interpretability by providing explanations for each node's predictions. It encodes local topology features, such as degree and centrality, into a structured context vector that informs the decision-making process. A Reasoner module then translates this context into a compact explanation vector, which is used to reconstruct the node's embedding and generate natural language explanations. This approach allows X-Node to maintain high classification accuracy while offering clear insights into individual node decisions, making it suitable for critical applications like medical diagnostics."}, 'zh': {'title': 'X-Node：自解释的图神经网络框架', 'desc': 'X-Node是一个自解释的图神经网络（GNN）框架，能够为每个节点生成解释。它通过编码局部拓扑特征并将其整合到消息传递过程中，既保持了准确性，又增强了可解释性。与传统的后置全局解释技术不同，X-Node为每个节点提供了个性化的解释，帮助理解节点的决策过程。我们在MedMNIST和MorphoMNIST两个图数据集上评估了X-Node，结果表明它在分类准确性上具有竞争力，同时生成了可信的节点解释。'}}}, {'id': 'https://huggingface.co/papers/2508.06429', 'title': 'SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via\n  Class-Conditioned Image Translation', 'url': 'https://huggingface.co/papers/2508.06429', 'abstract': 'A GAN-based semi-supervised learning framework improves medical image classification with minimal labeled data by integrating specialized neural networks and ensemble-based pseudo-labeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.', 'score': 1, 'issue_id': 5395, 'pub_date': '2025-08-08', 'pub_date_card': {'ru': '8 августа', 'en': 'August 8', 'zh': '8月8日'}, 'hash': 'f1862f0be07f6b3d', 'authors': ['Guido Manni', 'Clemente Lauretti', 'Loredana Zollo', 'Paolo Soda'], 'affiliations': ['Unit of Advanced Robotics and Human-Centred Technologies, Department of Engineering, Università Campus Bio-Medico di Roma, Rome, Italy', 'Unit of Artificial Intelligence and Computer Systems, Department of Engineering, Università Campus Bio-Medico di Roma, Rome, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2508.06429.jpg', 'data': {'categories': ['#low_resource', '#dataset', '#healthcare', '#training', '#synthetic'], 'emoji': '🏥', 'ru': {'title': 'Эффективная классификация медицинских изображений при минимуме размеченных данных', 'desc': 'Статья представляет новый полу-контролируемый метод обучения на основе генеративно-состязательных сетей (GAN) для классификации медицинских изображений при ограниченном количестве размеченных данных. Подход включает три специализированные нейронные сети: генератор для условного преобразования изображений, дискриминатор для оценки подлинности и классификации, и отдельный классификатор. Метод использует ансамблевое псевдо-маркирование, комбинируя взвешенные по уверенности предсказания дискриминатора и классификатора. Эксперименты на 11 наборах данных MedMNIST показали статистически значимое улучшение по сравнению с шестью современными GAN-методами, особенно в условиях крайне малого количества размеченных примеров.'}, 'en': {'title': 'Enhancing Medical Image Classification with Minimal Labels Using GANs', 'desc': 'This paper presents a GAN-based semi-supervised learning framework that enhances medical image classification using very few labeled samples. It combines three specialized neural networks: a generator for transforming images based on class conditions, a discriminator for assessing image authenticity and classification, and a classifier for final predictions. The training process alternates between supervised learning on limited labeled data and unsupervised learning using abundant unlabeled images, employing image-to-image translation techniques. The method also utilizes ensemble-based pseudo-labeling to improve label estimation, demonstrating significant performance gains across various datasets, especially in scenarios with extremely limited labeled data.'}, 'zh': {'title': '基于GAN的半监督学习提升医疗图像分类', 'desc': '本文提出了一种基于生成对抗网络（GAN）的半监督学习框架，旨在改善医疗图像分类，尤其是在标注数据稀缺的情况下。该框架结合了三种专门的神经网络，包括用于图像转换的生成器、用于真实性评估和分类的判别器，以及专用分类器。通过在有限标注数据上进行监督训练和利用大量未标注图像进行无监督学习，该方法实现了有效的标签估计。实验结果表明，该方法在多个数据集上显著优于现有的半监督方法，尤其在标注样本极少的情况下表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.10894', 'title': 'MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and\n  Multispectral Earth Observation Data', 'url': 'https://huggingface.co/papers/2508.10894', 'abstract': 'MAESTRO, an adapted Masked Autoencoder with optimized fusion strategies and spectral prior normalization, achieves state-of-the-art performance on multitemporal Earth observation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-supervised learning holds great promise for remote sensing, but standard self-supervised methods must be adapted to the unique characteristics of Earth observation data. We take a step in this direction by conducting a comprehensive benchmark of fusion strategies and reconstruction target normalization schemes for multimodal, multitemporal, and multispectral Earth observation data. Based on our findings, we propose MAESTRO, a novel adaptation of the Masked Autoencoder, featuring optimized fusion strategies and a tailored target normalization scheme that introduces a spectral prior as a self-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO sets a new state-of-the-art on tasks that strongly rely on multitemporal dynamics, while remaining highly competitive on tasks dominated by a single mono-temporal modality. Code to reproduce all our experiments is available at https://github.com/ignf/maestro.', 'score': 0, 'issue_id': 5400, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 августа', 'en': 'August 14', 'zh': '8月14日'}, 'hash': '2dc651b08b24751f', 'authors': ['Antoine Labatie', 'Michael Vaccaro', 'Nina Lardiere', 'Anatol Garioud', 'Nicolas Gonthier'], 'affiliations': ['Institut national de linformation géographique et forestière (IGN), France', 'Univ Gustave Eiffel, ENSG, IGN, LASTIG, France'], 'pdf_title_img': 'assets/pdf/title_img/2508.10894.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#multimodal', '#open_source', '#dataset', '#cv'], 'emoji': '🛰️', 'ru': {'title': 'MAESTRO: прорыв в самообучении для анализа мультивременных спутниковых данных', 'desc': 'MAESTRO - это адаптированный маскированный автоэнкодер для задач мультивременного дистанционного зондирования Земли. Он использует оптимизированные стратегии слияния и схему нормализации с учетом спектрального приора. MAESTRO достигает наилучших результатов на задачах, сильно зависящих от мультивременной динамики. Модель была протестирована на четырех наборах данных дистанционного зондирования Земли.'}, 'en': {'title': 'MAESTRO: Revolutionizing Earth Observation with Self-Supervised Learning', 'desc': 'MAESTRO is a new machine learning model designed for analyzing Earth observation data using self-supervised learning. It adapts the Masked Autoencoder architecture by incorporating optimized fusion strategies and a unique spectral prior normalization method. This approach allows MAESTRO to effectively handle multimodal and multitemporal data, improving performance on tasks that require understanding changes over time. The model has been tested on multiple datasets and has achieved state-of-the-art results, demonstrating its effectiveness in remote sensing applications.'}, 'zh': {'title': 'MAESTRO：地球观测的自监督学习新突破', 'desc': 'MAESTRO是一种改进的掩码自编码器，采用优化的融合策略和光谱先验归一化，能够在多时相地球观测任务中实现最先进的性能。该研究针对地球观测数据的独特特性，进行了全面的融合策略和重建目标归一化方案的基准测试。MAESTRO引入了光谱先验作为自监督信号，优化了多模态、多时相和多光谱数据的处理。经过在四个地球观测数据集上的评估，MAESTRO在依赖多时相动态的任务中设定了新的最先进水平，同时在单一时相模态主导的任务中也保持了高度竞争力。'}}}, {'id': 'https://huggingface.co/papers/2508.02324', 'title': 'Qwen-Image Technical Report', 'url': 'https://huggingface.co/papers/2508.02324', 'abstract': "Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks.", 'score': 79, 'issue_id': 5179, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '4417ddebd50d6ca5', 'authors': ['Chenfei Wu', 'Jiahao Li', 'Jingren Zhou', 'Junyang Lin', 'Kaiyuan Gao', 'Kun Yan', 'Sheng-ming Yin', 'Shuai Bai', 'Xiao Xu', 'Yilei Chen', 'Yuxiang Chen', 'Zecheng Tang', 'Zekai Zhang', 'Zhengyi Wang', 'An Yang', 'Bowen Yu', 'Chen Cheng', 'Dayiheng Liu', 'Deqing Li', 'Hang Zhang', 'Hao Meng', 'Hu Wei', 'Jingyuan Ni', 'Kai Chen', 'Kuan Cao', 'Liang Peng', 'Lin Qu', 'Minggang Wu', 'Peng Wang', 'Shuting Yu', 'Tingkun Wen', 'Wensen Feng', 'Xiaoxiao Xu', 'Yi Wang', 'Yichang Zhang', 'Yongqiang Zhu', 'Yujia Wu', 'Yuxuan Cai', 'Zenan Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2508.02324.jpg', 'data': {'categories': ['#optimization', '#dataset', '#data', '#games', '#cv', '#training', '#multimodal', '#benchmark'], 'emoji': '🎨', 'ru': {'title': 'Qwen-Image: новый уровень генерации и редактирования изображений с текстом', 'desc': 'Qwen-Image - это модель генерации изображений, которая достигает значительных успехов в рендеринге сложного текста и точном редактировании изображений. Модель использует комплексный конвейер данных, включающий сбор, фильтрацию, аннотацию и балансировку данных. Применяется стратегия прогрессивного обучения, начиная с простых задач и постепенно переходя к более сложным. Qwen-Image вводит улучшенную парадигму многозадачного обучения и механизм двойного кодирования для повышения согласованности при редактировании изображений.'}, 'en': {'title': 'Revolutionizing Image Generation and Editing with Qwen-Image', 'desc': "Qwen-Image is an advanced image generation model that enhances text rendering and image editing through a sophisticated data pipeline and a dual-encoding mechanism. It employs a comprehensive approach to data collection and training, utilizing a progressive strategy that improves the model's ability to handle complex text inputs, including both alphabetic and logographic languages. The model's multi-task training paradigm integrates various tasks to ensure consistency in image editing while maintaining high-quality outputs. Overall, Qwen-Image sets new standards in image generation and editing performance across multiple benchmarks."}, 'zh': {'title': 'Qwen-Image：图像生成与编辑的突破性进展', 'desc': 'Qwen-Image是一种图像生成模型，旨在通过全面的数据处理流程和渐进式训练策略，提升文本渲染和图像编辑的能力。该模型采用了双编码机制，能够有效地处理复杂的文本输入，并在字母语言和表意文字（如中文）上都表现出色。通过多任务训练，Qwen-Image在文本到图像和图像重建任务中实现了更高的一致性，确保了语义和视觉的保真度。最终，Qwen-Image在多个基准测试中展现了其在图像生成和编辑方面的领先性能。'}}}, {'id': 'https://huggingface.co/papers/2508.01959', 'title': 'SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension', 'url': 'https://huggingface.co/papers/2508.01959', 'abstract': "A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.   We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications.", 'score': 37, 'issue_id': 5178, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '063b1bca074ff85f', 'authors': ['Junjie Wu', 'Jiangnan Li', 'Yuqing Li', 'Lemao Liu', 'Liyan Xu', 'Jiwei Li', 'Dit-Yan Yeung', 'Jie Zhou', 'Mo Yu'], 'affiliations': ['HKUST', 'IIE-CAS', 'WeChat AI, Tencent', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01959.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#rag', '#long_context', '#optimization', '#benchmark', '#training'], 'emoji': '🔍', 'ru': {'title': 'Контекст имеет значение: ситуативные эмбеддинги для точного информационного поиска', 'desc': 'Статья представляет новый подход к улучшению производительности информационного поиска путем контекстуализации коротких текстовых фрагментов в более широком контексте. Авторы предлагают новую парадигму обучения и модели ситуативных эмбеддингов (SitEmb), которые превосходят современные модели, имея меньше параметров. Метод особенно эффективен для задач, требующих понимания локального контекста в рамках более широкого документа. Результаты показывают значительное улучшение производительности на специально созданном наборе данных для оценки ситуативного поиска.'}, 'en': {'title': 'Enhancing Retrieval with Contextual Short Text Chunks', 'desc': 'This paper introduces a new training method and a model called situated embedding models (SitEmb) that improve the retrieval of information from text. By conditioning short text chunks on a broader context, the model captures the meaning of each chunk more effectively. The authors demonstrate that existing models struggle with this task due to their limitations in encoding context. Their SitEmb model outperforms leading models with significantly fewer parameters, showing better retrieval performance across various languages and applications.'}, 'zh': {'title': '情境嵌入，提升检索性能！', 'desc': '本文提出了一种新的训练范式和情境嵌入模型（SitEmb），通过将短文本片段与更广泛的上下文窗口相结合，提升了检索性能。传统的检索方法往往将长文档拆分为小块，但这些小块的理解需要上下文信息。我们的方法通过在更广泛的上下文中对短片段进行编码，来增强检索效果。实验结果表明，SitEmb模型在参数更少的情况下，显著超越了现有的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2508.02276', 'title': 'CellForge: Agentic Design of Virtual Cell Models', 'url': 'https://huggingface.co/papers/2508.02276', 'abstract': "CellForge, an agentic system using a multi-agent framework, transforms raw single-cell multi-omics data into optimized computational models for virtual cells, outperforming state-of-the-art methods in single-cell perturbation prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge.", 'score': 27, 'issue_id': 5176, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '04238c0793ca08e5', 'authors': ['Xiangru Tang', 'Zhuoyun Yu', 'Jiapeng Chen', 'Yan Cui', 'Daniel Shao', 'Weixu Wang', 'Fang Wu', 'Yuchen Zhuang', 'Wenqi Shi', 'Zhi Huang', 'Arman Cohan', 'Xihong Lin', 'Fabian Theis', 'Smita Krishnaswamy', 'Mark Gerstein'], 'affiliations': ['Google DeepMind', 'Harvard University', 'Helmholtz Zentrum Munchen', 'Stanford University', 'University of Pennsylvania', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02276.jpg', 'data': {'categories': ['#dataset', '#training', '#science', '#open_source', '#architecture', '#agents', '#multimodal'], 'emoji': '🧬', 'ru': {'title': 'CellForge: ИИ-агенты создают виртуальные клетки из одноклеточных данных', 'desc': 'CellForge - это агентная система, использующая мультиагентный фреймворк для преобразования необработанных одноклеточных мультиомных данных в оптимизированные вычислительные модели виртуальных клеток. Система состоит из трех основных модулей: анализа задач, разработки методов и выполнения экспериментов. CellForge превосходит современные методы в прогнозировании одноклеточных возмущений на шести разнообразных наборах данных. Это демонстрирует, как итеративное взаимодействие между агентами с различными перспективами обеспечивает лучшие решения, чем прямой подход к задаче моделирования.'}, 'en': {'title': 'Transforming Biology with Collaborative AI Models', 'desc': 'CellForge is an innovative system that uses a multi-agent framework to create computational models for virtual cells from raw single-cell multi-omics data. It addresses the complexities of biological systems by employing specialized agents that collaborate to analyze tasks, design methods, and execute experiments. This approach allows CellForge to generate optimized model architectures and executable code, significantly improving predictions for single-cell perturbations. By integrating diverse perspectives from its agents, CellForge consistently outperforms existing state-of-the-art methods in the field.'}, 'zh': {'title': 'CellForge：优化虚拟细胞建模的智能系统', 'desc': 'CellForge 是一个基于多智能体框架的系统，能够将原始的单细胞多组学数据转化为优化的虚拟细胞计算模型。该系统通过分析任务和数据集，自动生成可执行的代码，显著提高了单细胞扰动预测的准确性。CellForge 的设计模块由不同专业的智能体协作开发建模策略，确保了模型的优化。实验结果表明，CellForge 在多种数据集上均优于现有的最先进方法，展示了多智能体协作的优势。'}}}, {'id': 'https://huggingface.co/papers/2508.01059', 'title': 'Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report', 'url': 'https://huggingface.co/papers/2508.01059', 'abstract': 'Foundation-Sec-8B-Instruct is a cybersecurity-focused LLM designed for chat-style interactions and instruction-following, outperforming other models in cybersecurity tasks while matching their instruction-following capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown remarkable success across many domains, yet their integration into cybersecurity applications remains limited due to a lack of general-purpose cybersecurity data, representational complexity, and safety and regulatory concerns. To address this gap, we previously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable for fine-tuning on downstream tasks. That model, however, was not designed for chat-style interactions or instruction-following. In this report, we release Foundation-Sec-8B-Instruct: a model specifically trained for general-purpose cybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific knowledge with instruction-following, conversational capabilities, and alignment with human preferences to produce high-quality, relevant responses. Comprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms Llama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its instruction-following performance. It is also competitive with GPT-4o-mini on cyber threat intelligence and instruction-following tasks. We envision Foundation-Sec-8B-Instruct becoming an indispensable assistant in the daily workflows of cybersecurity professionals. We release the model publicly at https://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.', 'score': 21, 'issue_id': 5176, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '897c594bfa5630a6', 'authors': ['Sajana Weerawardhena', 'Paul Kassianik', 'Blaine Nelson', 'Baturay Saglam', 'Anu Vellore', 'Aman Priyanshu', 'Supriti Vijay', 'Massimo Aufiero', 'Arthur Goldblatt', 'Fraser Burch', 'Ed Li', 'Jianliang He', 'Dhruv Kedia', 'Kojin Oshiba', 'Zhouran Yang', 'Yaron Singer', 'Amin Karbasi'], 'affiliations': ['Carnegie Mellon University', 'Cisco Systems Inc.', 'Foundation AI', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01059.jpg', 'data': {'categories': ['#dataset', '#alignment', '#security', '#training', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Интеллектуальный помощник по кибербезопасности на базе ИИ', 'desc': 'Foundation-Sec-8B-Instruct - это языковая модель, специализирующаяся на кибербезопасности и предназначенная для диалогового взаимодействия. Модель сочетает в себе специализированные знания в области кибербезопасности с возможностями следования инструкциям и ведения разговора. Оценки показывают, что Foundation-Sec-8B-Instruct превосходит другие модели в задачах кибербезопасности, сохраняя при этом способность следовать инструкциям. Авторы предполагают, что эта модель станет незаменимым помощником в повседневной работе специалистов по кибербезопасности.'}, 'en': {'title': 'Empowering Cybersecurity with Conversational AI', 'desc': 'Foundation-Sec-8B-Instruct is a large language model (LLM) specifically designed for cybersecurity applications, enhancing chat-style interactions and instruction-following capabilities. It builds upon the previous Foundation-Sec-8B model, which was tailored for fine-tuning on cybersecurity tasks but lacked conversational features. This new model integrates domain-specific knowledge with the ability to follow instructions and engage in dialogue, resulting in high-quality responses relevant to cybersecurity. Evaluations demonstrate that it surpasses other models like Llama 3.1-8B-Instruct in cybersecurity tasks while maintaining competitive performance in instruction-following.'}, 'zh': {'title': '网络安全对话的智能助手', 'desc': 'Foundation-Sec-8B-Instruct 是一个专注于网络安全的语言模型，旨在进行对话式交互和遵循指令。该模型在网络安全任务上表现优于其他模型，同时在遵循指令的能力上与之相匹配。它结合了特定领域的知识和人类偏好的对齐，能够生成高质量和相关的响应。我们希望 Foundation-Sec-8B-Instruct 能成为网络安全专业人员日常工作中不可或缺的助手。'}}}, {'id': 'https://huggingface.co/papers/2508.02150', 'title': "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following", 'url': 'https://huggingface.co/papers/2508.02150', 'abstract': "A self-supervised RL framework enhances instruction following in reasoning models without external supervision, maintaining reasoning performance and offering scalability and cost-effectiveness.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models excel in complex problem solving but exhibit a concerning trade off between reasoning capabilities and instruction following abilities. Existing approaches for improving instruction following rely on stronger external models, creating methodological bottlenecks and practical limitations including increased costs and accessibility constraints. We propose a self-supervised RL framework that leverages reasoning models' own internal signals to improve instruction following capabilities without external supervision. Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance, offering a scalable and cost-effective approach to enhance instruction following in reasoning models. The data and code are publicly available at https://github.com/Rainier-rq/verl-if.", 'score': 20, 'issue_id': 5179, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '37d1b608fde6bd5f', 'authors': ['Qingyu Ren', 'Qianyu He', 'Bowei Zhang', 'Jie Zeng', 'Jiaqing Liang', 'Yanghua Xiao', 'Weikang Zhou', 'Zeye Sun', 'Fei Yu'], 'affiliations': ['Ant Group', 'School of Data Science, Fudan University', 'Shanghai Key Laboratory of Data Science, College of Computer Science and Artificial Intelligence, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02150.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Самообучение ИИ для лучшего выполнения инструкций', 'desc': 'Исследователи предложили новый подход к улучшению способности моделей машинного обучения следовать инструкциям без внешнего контроля. Они разработали систему самообучения с подкреплением, которая использует внутренние сигналы самой модели. Эксперименты показали, что этот метод значительно улучшает следование инструкциям, сохраняя при этом способности к рассуждению. Предложенный подход является масштабируемым и экономически эффективным решением для совершенствования моделей рассуждений.'}, 'en': {'title': 'Enhancing Instruction Following in Reasoning Models with Self-Supervised RL', 'desc': "This paper presents a self-supervised reinforcement learning (RL) framework designed to improve how reasoning models follow instructions. Traditional methods often depend on external models, which can be costly and limit accessibility. The proposed framework utilizes the internal signals of reasoning models to enhance their instruction-following abilities without needing external supervision. Experimental results show that this approach not only boosts instruction following but also preserves the models' reasoning performance, making it a scalable and cost-effective solution."}, 'zh': {'title': '自监督强化学习提升推理模型的指令遵循能力', 'desc': '本论文提出了一种自监督强化学习框架，旨在提升推理模型的指令遵循能力，而无需外部监督。传统方法通常依赖于更强大的外部模型，这导致了方法上的瓶颈和实际应用中的限制，如成本增加和可及性问题。我们的方法利用推理模型自身的内部信号来改善指令遵循能力，同时保持推理性能。实验结果表明，该框架在提升指令遵循能力的同时，提供了一种可扩展且具有成本效益的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2508.02317', 'title': 'VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo', 'url': 'https://huggingface.co/papers/2508.02317', 'abstract': 'A modular training framework accelerates the development of omni-modal LLMs through efficient 3D parallelism and flexible configuration.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \\veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \\veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \\veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.', 'score': 8, 'issue_id': 5176, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '2e96724e612a0eb6', 'authors': ['Qianli Ma', 'Yaowei Zheng', 'Zhelun Shi', 'Zhongkai Zhao', 'Bin Jia', 'Ziyue Huang', 'Zhiqi Lin', 'Youjie Li', 'Jiacheng Yang', 'Yanghua Peng', 'Zhi Zhang', 'Xin Liu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.02317.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'Ускоряем обучение омни-модальных LLM с помощью модульной архитектуры', 'desc': 'Эта статья представляет новую модульную систему обучения для омни-модальных языковых моделей (LLM). Система предлагает эффективное 3D-распараллеливание и гибкую конфигурацию, что ускоряет разработку омни-модальных LLM. Ключевые особенности включают разделение коммуникации и вычислений, а также простую интеграцию новых модальностей. Результаты показывают высокую эффективность и масштабируемость при обучении крупных омни-модальных моделей.'}, 'en': {'title': 'Accelerating Omni-Modal LLMs with Modular Training', 'desc': 'This paper introduces \textit{veomni}, a modular training framework designed to enhance the development of omni-modal large language models (LLMs). It addresses the challenges of training these models by separating model architecture from parallel processing logic, which allows for efficient 3D parallelism. The framework supports easy integration of new modalities, reducing the need for extensive code modifications. With \textit{veomni}, a mixture-of-experts model with 30 billion parameters can achieve high throughput and scalability, demonstrating its effectiveness in training large omni-modal LLMs.'}, 'zh': {'title': '模块化训练框架，提升全模态LLM开发效率', 'desc': '这篇论文介绍了一种名为\\veomni的模块化训练框架，旨在加速全模态大语言模型（LLM）的开发。该框架通过高效的三维并行处理和灵活的配置，解决了训练全模态LLM时面临的挑战。\\veomni将模型定义与并行逻辑解耦，使得在多种模态上进行大规模训练变得更加高效。使用\\veomni，研究人员能够以极高的速度训练具有30亿参数的全模态专家模型，展示了其在训练大型全模态LLM方面的优越效率和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2507.17520', 'title': 'InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation', 'url': 'https://huggingface.co/papers/2507.17520', 'abstract': "InstructVLA is an end-to-end vision-language-action model that enhances manipulation performance while preserving vision-language reasoning through multimodal training and mixture-of-experts adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t To operate effectively in the real world, robots must integrate multimodal reasoning with precise action generation. However, existing vision-language-action (VLA) models often sacrifice one for the other, narrow their abilities to task-specific manipulation data, and suffer catastrophic forgetting of pre-trained vision-language capabilities. To bridge this gap, we introduce InstructVLA, an end-to-end VLA model that preserves the flexible reasoning of large vision-language models (VLMs) while delivering leading manipulation performance. InstructVLA introduces a novel training paradigm, Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal training with mixture-of-experts adaptation to jointly optimize textual reasoning and action generation on both standard VLM corpora and a curated 650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves 30.5% improvement over SpatialVLA. To evaluate generalization, we introduce SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and high-level instruction understanding, where it outperforms a fine-tuned OpenVLA by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling by leveraging textual reasoning to boost manipulation performance in both simulated and real-world settings. These results demonstrate InstructVLA's potential for bridging intuitive and steerable human-robot interaction with efficient policy learning.", 'score': 8, 'issue_id': 5176, 'pub_date': '2025-07-23', 'pub_date_card': {'ru': '23 июля', 'en': 'July 23', 'zh': '7月23日'}, 'hash': '13d868fd7ad8ea42', 'authors': ['Shuai Yang', 'Hao Li', 'Yilun Chen', 'Bin Wang', 'Yang Tian', 'Tai Wang', 'Hanqing Wang', 'Feng Zhao', 'Yiyi Liao', 'Jiangmiao Pang'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'University of Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.17520.jpg', 'data': {'categories': ['#optimization', '#robotics', '#training', '#reasoning', '#multimodal', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'InstructVLA: Мост между интуитивным управлением роботами и эффективным обучением', 'desc': 'InstructVLA - это модель обработки зрения, языка и действий, которая улучшает манипуляционные способности роботов, сохраняя при этом рассуждения на основе зрения и языка. Модель использует мультимодальное обучение и адаптацию на основе смеси экспертов. InstructVLA демонстрирует значительное улучшение производительности по сравнению с существующими моделями на различных задачах, включая задачи в симулированной и реальной среде. Эта модель открывает возможности для более интуитивного и управляемого взаимодействия человека с роботом при эффективном обучении политикам.'}, 'en': {'title': 'Bridging Vision, Language, and Action for Smarter Robots', 'desc': 'InstructVLA is a new model that combines vision, language, and action to improve how robots perform tasks. It uses a special training method called Vision-Language-Action Instruction Tuning (VLA-IT) to enhance both understanding and action capabilities without losing previous knowledge. This model outperforms existing systems in various tasks, showing significant improvements in manipulation and reasoning. By integrating multimodal training and expert adaptation, InstructVLA enables better human-robot interaction and efficient learning of tasks.'}, 'zh': {'title': '提升机器人操作与推理的完美结合', 'desc': 'InstructVLA是一种端到端的视觉-语言-动作模型，旨在提高机器人操作性能，同时保持视觉-语言推理能力。该模型通过多模态训练和专家混合适应，解决了现有模型在任务特定数据上的局限性和灾难性遗忘问题。InstructVLA引入了一种新的训练范式，称为视觉-语言-动作指令调优（VLA-IT），在标准视觉-语言模型数据集和一个包含65万样本的VLA-IT数据集上共同优化文本推理和动作生成。实验结果表明，InstructVLA在多个任务上表现优异，展示了其在高效政策学习和人机交互中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.01548', 'title': 'A Glimpse to Compress: Dynamic Visual Token Pruning for Large\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2508.01548', 'abstract': "A dynamic pruning framework, GlimpsePrune, improves efficiency in Large Vision-Language Models by adaptively removing irrelevant visual tokens without degrading performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual token compression is critical for Large Vision-Language Models (LVLMs) to efficiently process high-resolution inputs. Existing methods that typically adopt fixed compression ratios cannot adapt to scenes of varying complexity, often causing imprecise pruning that discards informative visual tokens and results in degraded model performance. To address this issue, we introduce a dynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes a data-driven ''glimpse'' and prunes irrelevant visual tokens in a single forward pass before answer generation. This approach prunes 92.6% of visual tokens while on average fully retaining the baseline performance on free-form VQA tasks. The reduced computational cost also enables more effective fine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline performance while maintaining a similarly high pruning rate. Our work paves a new way for building more powerful and efficient LVLMs.", 'score': 7, 'issue_id': 5180, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '7611e04f9fa72eb7', 'authors': ['Quan-Sheng Zeng', 'Yunheng Li', 'Qilong Wang', 'Peng-Tao Jiang', 'Zuxuan Wu', 'Ming-Ming Cheng', 'Qibin Hou'], 'affiliations': ['Shanghai Innovation Institute', 'Tianjin University', 'VCIP, CS, Nankai University', 'vivo Mobile Communication Co., Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2508.01548.jpg', 'data': {'categories': ['#inference', '#cv', '#training', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка для эффективных визуально-языковых моделей', 'desc': 'GlimpsePrune - это динамическая система обрезки, которая повышает эффективность крупных визуально-языковых моделей (LVLM). Она адаптивно удаляет нерелевантные визуальные токены без ухудшения производительности модели. GlimpsePrune обрезает 92.6% визуальных токенов, сохраняя при этом базовую производительность на задачах визуальных вопросов и ответов (VQA). Улучшенная версия GlimpsePrune+ достигает 110% базовой производительности при сохранении высокого уровня обрезки.'}, 'en': {'title': 'Dynamic Pruning for Efficient Vision-Language Models', 'desc': 'GlimpsePrune is a dynamic pruning framework designed to enhance the efficiency of Large Vision-Language Models (LVLMs) by selectively removing irrelevant visual tokens. Unlike traditional methods that use fixed compression ratios, GlimpsePrune adapts to the complexity of different scenes, ensuring that important visual information is preserved. This framework prunes up to 92.6% of visual tokens while maintaining baseline performance on visual question answering tasks. Additionally, an improved version, GlimpsePrune+, not only retains high pruning rates but also boosts performance beyond the baseline, demonstrating a significant advancement in model efficiency.'}, 'zh': {'title': '动态剪枝，提升视觉语言模型效率', 'desc': 'GlimpsePrune是一个动态剪枝框架，旨在提高大型视觉语言模型的效率。它通过自适应地去除不相关的视觉标记，避免了固定压缩比带来的信息丢失问题。该方法在一次前向传播中进行剪枝，能够保留92.6%的视觉标记，同时在自由形式的视觉问答任务中保持基线性能。我们的研究为构建更强大和高效的视觉语言模型开辟了新路径。'}}}, {'id': 'https://huggingface.co/papers/2508.02137', 'title': 'Fitness aligned structural modeling enables scalable virtual screening\n  with AuroBind', 'url': 'https://huggingface.co/papers/2508.02137', 'abstract': 'AuroBind is a scalable virtual screening framework that fine-tunes atomic-level structural models to predict ligand-bound structures and binding fitness, achieving high hit rates in prospective screens across disease-relevant targets.  \t\t\t\t\tAI-generated summary \t\t\t\t Most human proteins remain undrugged, over 96% of human proteins remain unexploited by approved therapeutics. While structure-based virtual screening promises to expand the druggable proteome, existing methods lack atomic-level precision and fail to predict binding fitness, limiting translational impact. We present AuroBind, a scalable virtual screening framework that fine-tunes a custom atomic-level structural model on million-scale chemogenomic data. AuroBind integrates direct preference optimization, self-distillation from high-confidence complexes, and a teacher-student acceleration strategy to jointly predict ligand-bound structures and binding fitness. The proposed models outperform state-of-the-art models on structural and functional benchmarks while enabling 100,000-fold faster screening across ultra-large compound libraries. In a prospective screen across ten disease-relevant targets, AuroBind achieved experimental hit rates of 7-69%, with top compounds reaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and GPR160, AuroBind identified both agonists and antagonists with success rates of 16-30%, and functional assays confirmed GPR160 modulation in liver and prostate cancer models. AuroBind offers a generalizable framework for structure-function learning and high-throughput molecular screening, bridging the gap between structure prediction and therapeutic discovery.', 'score': 6, 'issue_id': 5190, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'f767500373dc2f12', 'authors': ['Zhongyue Zhang', 'Jiahua Rao', 'Jie Zhong', 'Weiqiang Bai', 'Dongxue Wang', 'Shaobo Ning', 'Lifeng Qiao', 'Sheng Xu', 'Runze Ma', 'Will Hua', 'Jack Xiaoyu Chen', 'Odin Zhang', 'Wei Lu', 'Hanyi Feng', 'He Yang', 'Xinchao Shi', 'Rui Li', 'Wanli Ouyang', 'Xinzhu Ma', 'Jiahao Wang', 'Jixian Zhang', 'Jia Duan', 'Siqi Sun', 'Jian Zhang', 'Shuangjia Zheng'], 'affiliations': ['Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China', 'Institute for Medical Engineering & Science, Department of Biological Engineering, Massachusetts Institute of Technology, Cambridge, MA, USA', 'Key Laboratory of Protection, Development and Utilization of Medicinal Resources in Liupanshan Area, Ministry of Education, Peptide & Protein Drug Research Center, School of Pharmacy, Ningxia Medical University, Ningxia, China', 'Lingang Laboratory, Shanghai, China', 'Medicinal Chemistry and Bioinformatics Center, School of Medicine, Shanghai Jiao Tong University, Shanghai, China', 'Research Institute of Intelligent Complex Systems, Fudan University, Shanghai, China', 'School of Computer Science and Engineering, Sun Yat-sen University, Guangdong, China', 'Shanghai Artificial Intelligence Laboratory, Shanghai, China', 'Zhongshan Institute for Drug Discovery, Shanghai Institute of Materia Medica, Chinese Academy of Sciences, Guangdong, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.02137.jpg', 'data': {'categories': ['#healthcare', '#data', '#optimization', '#science', '#dataset', '#benchmark'], 'emoji': '🧬', 'ru': {'title': 'AuroBind: прорыв в виртуальном скрининге лекарств на атомарном уровне', 'desc': 'AuroBind - это масштабируемая система виртуального скрининга, которая оптимизирует структурные модели на атомарном уровне для предсказания комплексов лиганд-белок и оценки аффинности связывания. Система использует обучение на миллионах хемогеномных данных, включая прямую оптимизацию предпочтений и самодистилляцию. AuroBind превосходит существующие методы по точности и скорости, позволяя проводить скрининг сверхбольших библиотек соединений в 100 000 раз быстрее. В проспективных экспериментах система продемонстрировала высокую эффективность для различных мишеней, включая орфанные GPCR-рецепторы.'}, 'en': {'title': 'AuroBind: Revolutionizing Drug Discovery with Precision and Speed', 'desc': 'AuroBind is a new framework designed for virtual screening in drug discovery, focusing on predicting how small molecules (ligands) bind to proteins. It fine-tunes detailed structural models using large datasets to improve the accuracy of binding predictions and identify potential drug candidates. By employing techniques like direct preference optimization and self-distillation, AuroBind significantly enhances screening speed and hit rates across various disease targets. This approach not only outperforms existing methods but also helps in discovering new therapeutic options for previously undrugged human proteins.'}, 'zh': {'title': 'AuroBind：高效的虚拟筛选新框架', 'desc': 'AuroBind 是一个可扩展的虚拟筛选框架，能够微调原子级结构模型，以预测配体结合结构和结合适应性。该方法通过优化直接偏好、自我蒸馏和教师-学生加速策略，联合预测配体结合结构和结合适应性。AuroBind 在结构和功能基准测试中超越了现有的最先进模型，并在超大化合物库中实现了100,000倍的筛选速度提升。通过对十个与疾病相关的靶点进行前瞻性筛选，AuroBind 实现了7-69%的实验命中率，显示出其在药物发现中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2508.01691', 'title': 'Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and\n  Regional Languages Around the Globe', 'url': 'https://huggingface.co/papers/2508.01691', 'abstract': 'Voxlect is a benchmark for evaluating speech foundation models on dialect classification and downstream applications across multiple languages and dialects.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.', 'score': 6, 'issue_id': 5185, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': 'bdb8f5a75702298d', 'authors': ['Tiantian Feng', 'Kevin Huang', 'Anfeng Xu', 'Xuan Shi', 'Thanathai Lertpetchpun', 'Jihwan Lee', 'Yoonjeong Lee', 'Dani Byrd', 'Shrikanth Narayanan'], 'affiliations': ['University of Southern California, Los Angeles, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.01691.jpg', 'data': {'categories': ['#multilingual', '#science', '#audio', '#benchmark', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Voxlect: Универсальный инструмент для анализа диалектов в речевых моделях', 'desc': 'Voxlect - это новый эталонный тест для оценки речевых моделей в задачах классификации диалектов и связанных приложениях для множества языков и диалектов. Исследование использует более 2 миллионов обучающих высказываний из 30 общедоступных речевых корпусов с диалектной информацией. Авторы оценивают производительность нескольких широко используемых речевых моделей в классификации диалектов, а также их устойчивость в условиях шума. Voxlect демонстрирует применимость для обогащения существующих наборов данных для распознавания речи диалектной информацией и оценки систем генерации речи.'}, 'en': {'title': 'Voxlect: Advancing Dialect Classification in Speech Models', 'desc': 'Voxlect is a benchmark designed to evaluate speech foundation models specifically for dialect classification across various languages. It utilizes over 2 million training utterances from 30 speech corpora that include dialectal information, allowing for comprehensive assessments of model performance. The study not only benchmarks dialect classification but also explores downstream applications, such as enhancing automatic speech recognition (ASR) datasets with dialectal data. Additionally, it evaluates the robustness of these models in noisy environments and provides insights into geographic continuity in dialectal variations.'}, 'zh': {'title': 'Voxlect：全球方言分类的基准测试', 'desc': 'Voxlect是一个用于评估语音基础模型在方言分类和下游应用中的基准测试。该研究涵盖了多种语言和方言，包括英语、阿拉伯语、普通话、粤语等，使用了超过200万个带有方言信息的语音样本。我们评估了多种广泛使用的语音基础模型在方言分类中的表现，并分析了模型在噪声条件下的鲁棒性。Voxlect不仅用于方言分类的基准测试，还可以增强现有的语音识别数据集，帮助分析不同方言的ASR性能。'}}}, {'id': 'https://huggingface.co/papers/2508.01151', 'title': 'Personalized Safety Alignment for Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2508.01151', 'abstract': "A personalized safety alignment framework integrates user-specific profiles into text-to-image diffusion models to better align generated content with individual safety preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models have revolutionized visual content generation, but current safety mechanisms apply uniform standards that often fail to account for individual user preferences. These models overlook the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the model's behavior to match individual safety preferences while preserving image quality. We introduce a new dataset, Sage, which captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism. Experiments show that PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores. Our code, data, and models are publicly available at https://torpedo2648.github.io/PSAlign/.", 'score': 6, 'issue_id': 5176, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '22d777cd71f123c6', 'authors': ['Yu Lei', 'Jinbin Bai', 'Qingyu Shi', 'Aosong Feng', 'Kaidong Yu'], 'affiliations': ['National University of Singapore', 'Peking University', 'TeleAI, China Telecom', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01151.jpg', 'data': {'categories': ['#cv', '#dataset', '#alignment', '#diffusion', '#open_source', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Персонализированная безопасность в генеративных моделях изображений', 'desc': 'Предложена новая система Personalized Safety Alignment (PSA) для настройки генеративных моделей изображений под индивидуальные предпочтения безопасности пользователей. PSA интегрирует персонализированные профили пользователей в процесс диффузии, адаптируя поведение модели к индивидуальным требованиям безопасности. Авторы представили новый датасет Sage, capturing пользовательские предпочтения безопасности. Эксперименты показали, что PSA превосходит существующие методы в подавлении вредного контента и лучше соответствует ограничениям пользователей.'}, 'en': {'title': 'Personalized Safety for Safer AI-Generated Images', 'desc': 'This paper presents a Personalized Safety Alignment (PSA) framework that enhances text-to-image diffusion models by incorporating individual user profiles. Current models apply a one-size-fits-all approach to safety, which does not consider the unique safety preferences shaped by personal factors. The PSA framework allows for user-specific adjustments in the generative process, ensuring that the generated images align with individual safety standards while maintaining high image quality. The authors also introduce a new dataset, Sage, to effectively capture and integrate these personalized safety preferences, demonstrating improved performance in harmful content suppression compared to existing methods.'}, 'zh': {'title': '个性化安全对齐：让生成内容更符合你的安全偏好', 'desc': '这篇论文提出了一种个性化安全对齐框架（PSA），旨在将用户特定的个人资料整合到文本到图像的扩散模型中，以更好地符合个体的安全偏好。当前的安全机制通常采用统一标准，无法考虑用户的多样化安全边界，如年龄、心理健康和个人信仰等因素。PSA通过在扩散过程中整合个性化用户资料，调整模型行为以匹配个体安全偏好，同时保持图像质量。实验结果表明，PSA在有害内容抑制和生成内容与用户约束的对齐方面优于现有方法，取得了更高的胜率和通过率。'}}}, {'id': 'https://huggingface.co/papers/2508.02271', 'title': 'Dynaword: From One-shot to Continuously Developed Datasets', 'url': 'https://huggingface.co/papers/2508.02271', 'abstract': 'A framework called Dynaword and its implementation Danish Dynaword enable community-driven, open, and continuously updated large-scale natural language datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.   To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.', 'score': 5, 'issue_id': 5193, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'efe4e9f51385893f', 'authors': ['Kenneth Enevoldsen', 'Kristian Nørgaard Jensen', 'Jan Kostkan', 'Balázs Szabó', 'Márton Kardos', 'Kirten Vad', 'Andrea Blasi Núñez', 'Gianluca Barmina', 'Jacob Nielsen', 'Rasmus Larsen', 'Peter Vahlstrup', 'Per Møldrup Dalum', 'Desmond Elliott', 'Lukas Galke', 'Peter Schneider-Kamp', 'Kristoffer Nielbo'], 'affiliations': ['Aarhus University', 'The Alexandra Institute', 'University of Copenhagen', 'University of Southern Denmark'], 'pdf_title_img': 'assets/pdf/title_img/2508.02271.jpg', 'data': {'categories': ['#dataset', '#survey', '#open_source', '#data'], 'emoji': '🌱', 'ru': {'title': 'Открытые и развивающиеся датасеты для NLP силами сообщества', 'desc': 'Dynaword - это фреймворк для создания больших открытых наборов данных на естественном языке с возможностью постоянного обновления сообществом. Danish Dynaword - это конкретная реализация этого подхода, содержащая в 4 раза больше токенов, чем аналоги, и имеющая открытую лицензию. Фреймворк решает проблемы ограничений лицензирования, статичности датасетов и закрытости процессов контроля качества. Он включает легковесные тесты для проверки форматирования, качества и документации данных, обеспечивая устойчивую основу для вклада сообщества и эволюции датасета.'}, 'en': {'title': 'Empowering Community-Driven Natural Language Datasets', 'desc': 'The paper presents Dynaword, a framework designed to create large-scale natural language datasets that are open and continuously updated through community involvement. It addresses three main challenges in current dataset practices: licensing issues, static releases, and limited quality assurance. The implementation, Danish Dynaword, showcases this framework by providing a dataset with significantly more tokens than similar datasets, all under open licenses. It also includes mechanisms for community contributions and quality checks, promoting a sustainable model for dataset development and maintenance.'}, 'zh': {'title': '构建开放、可持续的大规模语言数据集', 'desc': 'Dynaword是一个框架，旨在创建可持续更新的大规模自然语言数据集。它解决了当前数据集面临的三个主要挑战，包括模糊的许可限制、静态数据集发布和质量保证过程的局限性。Danish Dynaword是该框架的具体实现，展示了其潜力，并包含了比类似发布多四倍的标记。该项目通过轻量级测试确保数据格式、质量和文档，促进了社区的持续贡献和数据集的演变。'}}}, {'id': 'https://huggingface.co/papers/2508.02558', 'title': 'Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction', 'url': 'https://huggingface.co/papers/2508.02558', 'abstract': 'Sparse-dLLM improves the efficiency of diffusion large language models by implementing dynamic cache eviction and sparse attention, enhancing throughput without compromising performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10times higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.', 'score': 4, 'issue_id': 5189, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': 'abb2417fa4d42a82', 'authors': ['Yuerong Song', 'Xiaoran Liu', 'Ruixiao Li', 'Zhigeng Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2508.02558.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#training', '#inference', '#long_context'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей без потери качества', 'desc': 'Sparse-dLLM - это новый фреймворк для повышения эффективности диффузионных больших языковых моделей (dLLM). Он использует динамическое удаление кэша и разреженное внимание, что позволяет значительно увеличить пропускную способность без ущерба для производительности. Анализ паттернов внимания в dLLM показал устойчивую межслойную разреженность, где ключевые токены остаются важными на протяжении всего процесса декодирования. Эксперименты продемонстрировали, что Sparse-dLLM обеспечивает до 10 раз более высокую пропускную способность по сравнению с обычными dLLM при сопоставимой производительности и аналогичных затратах пиковой памяти.'}, 'en': {'title': 'Boosting Efficiency in Language Models with Sparse-dLLM', 'desc': 'Sparse-dLLM is a novel framework designed to enhance the efficiency of diffusion large language models (dLLMs) by utilizing dynamic cache eviction and sparse attention mechanisms. It addresses the high computational complexity and memory demands of traditional dLLMs during inference by selectively retaining important tokens while evicting less relevant ones. This approach leverages the consistent saliency of tokens across decoding steps, allowing for improved throughput without sacrificing performance. Experimental results show that Sparse-dLLM can achieve up to 10 times higher throughput compared to standard dLLMs, while maintaining similar performance levels and memory usage.'}, 'zh': {'title': 'Sparse-dLLM：提升扩散大语言模型效率的创新方案', 'desc': 'Sparse-dLLM通过动态缓存驱逐和稀疏注意力机制，提高了扩散大语言模型的效率。传统的缓存技术虽然加速了解码，但占用了大量内存，限制了长上下文的应用。我们的研究发现，扩散大语言模型中的注意力模式存在跨层稀疏性，关键的token在解码过程中始终保持重要性。Sparse-dLLM是首个不需要训练的框架，通过延迟双向稀疏缓存，动态保留重要token并驱逐不重要的前缀/后缀条目，从而实现了更高的解码吞吐量。'}}}, {'id': 'https://huggingface.co/papers/2508.01415', 'title': 'RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems', 'url': 'https://huggingface.co/papers/2508.01415', 'abstract': 'RoboMemory, a brain-inspired multi-memory framework, enhances lifelong learning in physical robots by integrating cognitive neuroscience principles and achieving state-of-the-art performance in real-world tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots.', 'score': 4, 'issue_id': 5177, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': 'ea0bbd88dcba8948', 'authors': ['Mingcong Lei', 'Honghao Cai', 'Zezhou Cui', 'Liangchen Tan', 'Junkun Hong', 'Gehan Hu', 'Shuangyu Zhu', 'Yimou Wu', 'Shaohan Jiang', 'Ge Wang', 'Zhen Li', 'Shuguang Cui', 'Yiming Zhao', 'Yatong Han'], 'affiliations': ['FNii-Shenzhen', 'Harbin Engineering University', 'Harbin Institute of Technology, Shenzhen', 'Infused Synapse AI', 'SSE', 'The Chinese University of Hong Kong, Shengzhen', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2508.01415.jpg', 'data': {'categories': ['#agents', '#optimization', '#open_source', '#training', '#agi', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'RoboMemory: Мозгоподобная память для непрерывного обучения роботов', 'desc': 'RoboMemory - это мультимодульная система памяти для непрерывного обучения роботов, вдохновленная принципами работы мозга. Она включает четыре основных модуля, имитирующих функции различных отделов мозга: препроцессор информации, систему долговременной памяти, модуль замкнутого планирования и исполнитель низкого уровня. Система показала значительное улучшение производительности по сравнению с существующими решениями на бенчмарке EmbodiedBench. RoboMemory решает проблемы высокой латентности и масштабируемости, что делает ее перспективной основой для интеграции мультимодальных систем памяти в физических роботах.'}, 'en': {'title': 'RoboMemory: Revolutionizing Lifelong Learning in Robots', 'desc': 'RoboMemory is a new framework designed to help robots learn continuously over time, inspired by how the human brain works. It uses four main components that mimic brain functions to improve memory and planning in robots, allowing them to handle complex tasks better. The framework addresses issues like slow memory access and the need for robots to remember different types of information effectively. Tests show that RoboMemory significantly outperforms existing systems in real-world scenarios, making it a promising advancement in robotic learning.'}, 'zh': {'title': 'RoboMemory：提升机器人终身学习的多记忆框架', 'desc': 'RoboMemory是一个受大脑启发的多记忆框架，旨在提高物理机器人在终身学习中的表现。它结合了认知神经科学的原理，解决了现实环境中的关键挑战，如持续学习和任务相关性捕捉。该框架包含四个核心模块，分别模拟大脑的不同部分，以实现长期规划和累积学习。通过在复杂记忆框架中并行更新和检索，RoboMemory显著提高了推理速度，并在实际任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2508.01408', 'title': 'Artificial Intelligence and Misinformation in Art: Can Vision Language\n  Models Judge the Hand or the Machine Behind the Canvas?', 'url': 'https://huggingface.co/papers/2508.01408', 'abstract': 'State-of-the-art vision language models struggle with accurately attributing artists and distinguishing AI-generated images, highlighting the need for improvement to prevent misinformation.  \t\t\t\t\tAI-generated summary \t\t\t\t The attribution of artworks in general and of paintings in particular has always been an issue in art. The advent of powerful artificial intelligence models that can generate and analyze images creates new challenges for painting attribution. On the one hand, AI models can create images that mimic the style of a painter, which can be incorrectly attributed, for example, by other AI models. On the other hand, AI models may not be able to correctly identify the artist for real paintings, inducing users to incorrectly attribute paintings. In this paper, both problems are experimentally studied using state-of-the-art AI models for image generation and analysis on a large dataset with close to 40,000 paintings from 128 artists. The results show that vision language models have limited capabilities to: 1) perform canvas attribution and 2) to identify AI generated images. As users increasingly rely on queries to AI models to get information, these results show the need to improve the capabilities of VLMs to reliably perform artist attribution and detection of AI generated images to prevent the spread of incorrect information.', 'score': 4, 'issue_id': 5185, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '31ac5fdf5a455427', 'authors': ['Tarian Fu', 'Javier Conde', 'Gonzalo Martínez', 'Pedro Reviriego', 'Elena Merino-Gómez', 'Fernando Moral'], 'affiliations': ['Nanjing University of Aeronautics and Astronautics', 'Universidad Antonio de Nebrija', 'Universidad Politécnica de Madrid', 'Universidad de Valladolid'], 'pdf_title_img': 'assets/pdf/title_img/2508.01408.jpg', 'data': {'categories': ['#cv', '#dataset', '#ethics', '#hallucinations', '#multimodal'], 'emoji': '🎨', 'ru': {'title': 'Искусственный интеллект vs Искусство: проблемы атрибуции в эпоху нейросетей', 'desc': 'Современные модели компьютерного зрения и обработки естественного языка (VLM) испытывают трудности с точной атрибуцией художников и распознаванием изображений, сгенерированных искусственным интеллектом. Исследование проводилось на большом наборе данных, включающем около 40 000 картин 128 художников. Результаты показывают ограниченные возможности VLM в атрибуции картин и идентификации изображений, созданных ИИ. Авторы подчеркивают необходимость улучшения этих моделей для предотвращения распространения неверной информации.'}, 'en': {'title': 'Enhancing AI for Accurate Art Attribution and Detection', 'desc': 'This paper investigates the limitations of state-of-the-art vision language models (VLMs) in accurately attributing artworks to their respective artists and distinguishing between real and AI-generated images. The study highlights that these models struggle with both canvas attribution and the identification of AI-generated content, which can lead to misinformation. Using a large dataset of nearly 40,000 paintings from 128 artists, the authors demonstrate the inadequacies of current AI models in these tasks. The findings emphasize the urgent need for advancements in VLMs to enhance their reliability in art attribution and image detection.'}, 'zh': {'title': '提升视觉语言模型以防止艺术作品错误归属', 'desc': '本论文探讨了当前视觉语言模型在艺术作品归属和区分AI生成图像方面的不足。研究发现，AI模型能够生成模仿画家风格的图像，导致错误归属的情况。与此同时，AI模型在识别真实画作的艺术家时也存在困难，可能导致用户错误地归属作品。通过对近40,000幅来自128位艺术家的画作进行实验，结果显示视觉语言模型在画布归属和AI生成图像识别方面的能力有限，强调了改进这些模型的重要性。'}}}, {'id': 'https://huggingface.co/papers/2508.01287', 'title': 'Exploitation Is All You Need... for Exploration', 'url': 'https://huggingface.co/papers/2508.01287', 'abstract': 'Meta-reinforcement learning agents can exhibit exploratory behavior when trained with a greedy objective, provided the environment has recurring structure, the agent has memory, and long-horizon credit assignment is possible.  \t\t\t\t\tAI-generated summary \t\t\t\t Ensuring sufficient exploration is a central challenge when training meta-reinforcement learning (meta-RL) agents to solve novel environments. Conventional solutions to the exploration-exploitation dilemma inject explicit incentives such as randomization, uncertainty bonuses, or intrinsic rewards to encourage exploration. In this work, we hypothesize that an agent trained solely to maximize a greedy (exploitation-only) objective can nonetheless exhibit emergent exploratory behavior, provided three conditions are met: (1) Recurring Environmental Structure, where the environment features repeatable regularities that allow past experience to inform future choices; (2) Agent Memory, enabling the agent to retain and utilize historical interaction data; and (3) Long-Horizon Credit Assignment, where learning propagates returns over a time frame sufficient for the delayed benefits of exploration to inform current decisions. Through experiments in stochastic multi-armed bandits and temporally extended gridworlds, we observe that, when both structure and memory are present, a policy trained on a strictly greedy objective exhibits information-seeking exploratory behavior. We further demonstrate, through controlled ablations, that emergent exploration vanishes if either environmental structure or agent memory is absent (Conditions 1 & 2). Surprisingly, removing long-horizon credit assignment (Condition 3) does not always prevent emergent exploration-a result we attribute to the pseudo-Thompson Sampling effect. These findings suggest that, under the right prerequisites, exploration and exploitation need not be treated as orthogonal objectives but can emerge from a unified reward-maximization process.', 'score': 3, 'issue_id': 5179, 'pub_date': '2025-08-02', 'pub_date_card': {'ru': '2 августа', 'en': 'August 2', 'zh': '8月2日'}, 'hash': '8867dd3f084db81e', 'authors': ['Micah Rentschler', 'Jesse Roberts'], 'affiliations': ['Tennessee Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01287.jpg', 'data': {'categories': ['#agents', '#optimization', '#games', '#reasoning', '#rl'], 'emoji': '🔍', 'ru': {'title': 'Исследование может возникнуть из чистой эксплуатации', 'desc': 'Это исследование показывает, что агенты мета-обучения с подкреплением могут демонстрировать исследовательское поведение при обучении с жадной целевой функцией. Для этого необходимы три условия: повторяющаяся структура среды, наличие памяти у агента и возможность долгосрочного назначения кредита. Эксперименты на многоруких бандитах и сетках подтверждают, что при наличии структуры и памяти политика, обученная только на максимизацию награды, проявляет поисковое поведение. Удивительно, но отсутствие долгосрочного назначения кредита не всегда препятствует возникновению исследования.'}, 'en': {'title': 'Exploration Emerges from Greedy Training with the Right Conditions', 'desc': 'This paper explores how meta-reinforcement learning agents can learn to explore their environments even when trained with a focus on maximizing immediate rewards. The authors propose that this exploratory behavior can emerge if the environment has recurring structures, the agent has memory to recall past experiences, and long-term credit assignment is possible. Through experiments, they show that when these conditions are met, agents can engage in information-seeking exploration without explicit incentives. The findings challenge the traditional view that exploration and exploitation are separate goals, suggesting they can coexist within a single reward-maximization framework.'}, 'zh': {'title': '探索与利用的统一：元强化学习的新视角', 'desc': '本研究探讨了元强化学习代理在特定条件下如何表现出探索行为。我们提出，当环境具有重复结构、代理具备记忆能力，并且能够进行长期信用分配时，即使代理仅以贪婪目标进行训练，也能自发地进行探索。实验结果表明，在随机多臂老虎机和时间扩展的网格世界中，满足这些条件的代理会表现出信息寻求的探索行为。我们的发现表明，探索和利用并非完全对立，而是可以通过统一的奖励最大化过程共同出现。'}}}, {'id': 'https://huggingface.co/papers/2508.00910', 'title': 'Cyber-Zero: Training Cybersecurity Agents without Runtime', 'url': 'https://huggingface.co/papers/2508.00910', 'abstract': 'Cyber-Zero synthesizes agent trajectories from CTF writeups to train runtime-free cybersecurity LLMs, achieving state-of-the-art performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.', 'score': 3, 'issue_id': 5177, 'pub_date': '2025-07-29', 'pub_date_card': {'ru': '29 июля', 'en': 'July 29', 'zh': '7月29日'}, 'hash': '181a31b28dfe8e6a', 'authors': ['Terry Yue Zhuo', 'Dingmin Wang', 'Hantian Ding', 'Varun Kumar', 'Zijian Wang'], 'affiliations': ['Amazon', 'Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2508.00910.jpg', 'data': {'categories': ['#agents', '#dataset', '#synthetic', '#benchmark', '#open_source'], 'emoji': '🛡️', 'ru': {'title': 'Синтез траекторий без среды выполнения для обучения передовых LLM в кибербезопасности', 'desc': 'Cyber-Zero - это первая система для синтеза траекторий агентов без использования среды выполнения для обучения языковых моделей в кибербезопасности. Она использует общедоступные отчеты CTF и симуляцию на основе LLM для создания реалистичных последовательностей взаимодействий. Обученные на синтезированных траекториях агенты на основе LLM достигают значительного улучшения производительности на трех ключевых бенчмарках CTF. Лучшая модель Cyber-Zero-32B устанавливает новый state-of-the-art среди открытых моделей, соответствуя возможностям проприетарных систем.'}, 'en': {'title': 'Revolutionizing Cybersecurity AI with Runtime-Free Trajectory Synthesis', 'desc': 'Cyber-Zero introduces a novel framework for training cybersecurity large language models (LLMs) without the need for executable runtime environments. It synthesizes agent trajectories from Capture The Flag (CTF) writeups, allowing the generation of realistic interaction sequences that mimic runtime behaviors. This approach enables the training of LLM-based agents that outperform existing models on key CTF benchmarks. By achieving state-of-the-art performance with a cost-effective solution, Cyber-Zero demonstrates the potential of runtime-free trajectory synthesis in advancing cybersecurity AI.'}, 'zh': {'title': 'Cyber-Zero：无运行时环境的网络安全代理训练新方法', 'desc': 'Cyber-Zero 是一个创新的框架，旨在通过合成高质量的代理轨迹来训练网络安全领域的语言模型（LLM），而无需实际的运行时环境。该框架利用公开的CTF（Capture The Flag）写作材料，采用基于角色的LLM模拟，逆向工程运行时行为，生成真实的长时间交互序列。通过使用Cyber-Zero合成的轨迹，我们训练的LLM代理在三个主要的CTF基准测试中，性能提升达13.1%。Cyber-Zero-32B模型在开放权重模型中创造了新的性能记录，展示了无运行时轨迹合成在网络安全代理开发中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2508.00890', 'title': 'AgentTTS: Large Language Model Agent for Test-time Compute-optimal\n  Scaling Strategy in Complex Tasks', 'url': 'https://huggingface.co/papers/2508.00890', 'abstract': 'AgentTTS, an LLM-agent-based framework, optimizes compute allocation for multi-stage complex tasks, improving performance and robustness compared to traditional methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.', 'score': 2, 'issue_id': 5178, 'pub_date': '2025-07-26', 'pub_date_card': {'ru': '26 июля', 'en': 'July 26', 'zh': '7月26日'}, 'hash': '359ff54230f7e0ba', 'authors': ['Fali Wang', 'Hui Liu', 'Zhenwei Dai', 'Jingying Zeng', 'Zhiwei Zhang', 'Zongyu Wu', 'Chen Luo', 'Zhen Li', 'Xianfeng Tang', 'Qi He', 'Suhang Wang'], 'affiliations': ['Amazon, Palo Alto, CA, USA', 'The Pennsylvania State University, University Park, PA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2508.00890.jpg', 'data': {'categories': ['#interpretability', '#rl', '#agents', '#optimization', '#inference'], 'emoji': '🤖', 'ru': {'title': 'Умное распределение ресурсов для сложных задач искусственного интеллекта', 'desc': 'AgentTTS - это фреймворк на основе LLM-агентов для оптимизации распределения вычислительных ресурсов в многоэтапных сложных задачах. Он использует итеративные взаимодействия с обратной связью для поиска оптимальных распределений моделей и бюджетов для каждой подзадачи. AgentTTS превосходит традиционные методы по эффективности поиска, устойчивости к размерам обучающих выборок и интерпретируемости. Фреймворк решает проблемы комбинаторного пространства поиска и взаимозависимости оптимальных распределений между подзадачами.'}, 'en': {'title': 'Optimizing Compute Allocation for Complex Multi-Stage Tasks with AgentTTS', 'desc': 'AgentTTS is a framework that optimizes how computing resources are allocated for complex tasks that involve multiple stages. It focuses on improving the performance of large language models (LLMs) by dynamically adjusting resource allocation during inference, especially for tasks that require different capabilities at each stage. The framework addresses challenges such as the combinatorial nature of model selection and budget allocation, which makes traditional search methods inefficient. Through extensive experiments, AgentTTS has shown to be more efficient and robust compared to existing methods, providing better performance across various datasets and tasks.'}, 'zh': {'title': 'AgentTTS：优化多阶段任务的计算分配', 'desc': 'AgentTTS是一个基于大语言模型（LLM）代理的框架，旨在优化多阶段复杂任务的计算资源分配。与传统方法相比，它在性能和鲁棒性上有显著提升。该框架通过迭代反馈与执行环境进行交互，自动搜索计算最优的分配方案。实验结果表明，AgentTTS在搜索效率上显著优于传统方法和其他基于LLM的基线，并且对训练集大小的变化表现出更好的鲁棒性和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2508.02605', 'title': 'ReMoMask: Retrieval-Augmented Masked Motion Generation', 'url': 'https://huggingface.co/papers/2508.02605', 'abstract': "ReMoMask, a unified framework, addresses limitations in text-to-motion generation by integrating a Bidirectional Momentum Text-Motion Model, Semantic Spatio-temporal Attention, and RAG-Classier-Free Guidance, achieving state-of-the-art performance on HumanML3D and KIT-ML benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Motion (T2M) generation aims to synthesize realistic and semantically aligned human motion sequences from natural language descriptions. However, current approaches face dual challenges: Generative models (e.g., diffusion models) suffer from limited diversity, error accumulation, and physical implausibility, while Retrieval-Augmented Generation (RAG) methods exhibit diffusion inertia, partial-mode collapse, and asynchronous artifacts. To address these limitations, we propose ReMoMask, a unified framework integrating three key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples negative sample scale from batch size via momentum queues, substantially improving cross-modal retrieval precision; 2) A Semantic Spatio-temporal Attention mechanism enforces biomechanical constraints during part-level fusion to eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates minor unconditional generation to enhance generalization. Built upon MoMask's RVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal steps. Extensive experiments on standard benchmarks demonstrate the state-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97% improvement in FID scores on HumanML3D and KIT-ML, respectively, compared to the previous SOTA method RAG-T2M. Code: https://github.com/AIGeeksGroup/ReMoMask. Website: https://aigeeksgroup.github.io/ReMoMask.", 'score': 1, 'issue_id': 5185, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '8fa0e8fe5289831e', 'authors': ['Zhengdao Li', 'Siheng Wang', 'Zeyu Zhang', 'Hao Tang'], 'affiliations': ['Jiangsu University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2508.02605.jpg', 'data': {'categories': ['#diffusion', '#games', '#benchmark', '#rag', '#video', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'ReMoMask: Революция в генерации движений из текста', 'desc': 'ReMoMask - это унифицированная система для генерации движений по текстовому описанию, решающая ограничения существующих подходов. Она включает в себя три ключевые инновации: двунаправленную моментную текстово-моторную модель, семантическое пространственно-временное внимание и RAG-безклассификаторное управление. ReMoMask достигает наилучших результатов на стандартных бенчмарках HumanML3D и KIT-ML, значительно улучшая показатели FID. Система эффективно генерирует согласованные во времени движения за минимальное количество шагов.'}, 'en': {'title': 'ReMoMask: Revolutionizing Text-to-Motion Generation!', 'desc': "ReMoMask is a new framework designed to improve text-to-motion generation, which creates human motion sequences from text descriptions. It combines a Bidirectional Momentum Text-Motion Model, which enhances retrieval accuracy, with a Semantic Spatio-temporal Attention mechanism that ensures realistic motion by applying biomechanical constraints. Additionally, it uses RAG-Classier-Free Guidance to boost the model's ability to generalize from data. The framework has shown significant improvements in generating coherent motions, outperforming previous methods on key benchmarks."}, 'zh': {'title': 'ReMoMask：文本到运动生成的新突破', 'desc': 'ReMoMask是一个统一框架，旨在解决文本到运动生成中的局限性。它通过集成双向动量文本-运动模型、语义时空注意力机制和无分类器引导，显著提高了在HumanML3D和KIT-ML基准测试中的表现。该框架通过动量队列解耦负样本规模与批量大小，提升了跨模态检索的精度。同时，语义时空注意力机制在部分融合过程中施加生物力学约束，消除了异步伪影。'}}}, {'id': 'https://huggingface.co/papers/2508.02268', 'title': 'SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic\n  Bidirectional Machine Translation System', 'url': 'https://huggingface.co/papers/2508.02268', 'abstract': 'A bidirectional machine translation system, SHAMI-MT, bridges the gap between Modern Standard Arabic and the Syrian dialect using AraT5v2-base-1024 architecture, achieving high-quality translations.  \t\t\t\t\tAI-generated summary \t\t\t\t The rich linguistic landscape of the Arab world is characterized by a significant gap between Modern Standard Arabic (MSA), the language of formal communication, and the diverse regional dialects used in everyday life. This diglossia presents a formidable challenge for natural language processing, particularly machine translation. This paper introduces SHAMI-MT, a bidirectional machine translation system specifically engineered to bridge the communication gap between MSA and the Syrian dialect. We present two specialized models, one for MSA-to-Shami and another for Shami-to-MSA translation, both built upon the state-of-the-art AraT5v2-base-1024 architecture. The models were fine-tuned on the comprehensive Nabra dataset and rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami model achieved an outstanding average quality score of 4.01 out of 5.0 when judged by OPENAI model GPT-4.1, demonstrating its ability to produce translations that are not only accurate but also dialectally authentic. This work provides a crucial, high-fidelity tool for a previously underserved language pair, advancing the field of dialectal Arabic translation and offering significant applications in content localization, cultural heritage, and intercultural communication.', 'score': 1, 'issue_id': 5189, 'pub_date': '2025-08-04', 'pub_date_card': {'ru': '4 августа', 'en': 'August 4', 'zh': '8月4日'}, 'hash': '0704696f67aca30f', 'authors': ['Serry Sibaee', 'Omer Nacar', 'Yasser Al-Habashi', 'Adel Ammar', 'Wadii Boulila'], 'affiliations': ['Prince Sultan University, Riyadh - Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2508.02268.jpg', 'data': {'categories': ['#training', '#low_resource', '#multilingual', '#dataset', '#machine_translation'], 'emoji': '🌉', 'ru': {'title': 'Мост между арабским языком и диалектом: революция в машинном переводе', 'desc': 'SHAMI-MT - это система машинного перевода, разработанная для преодоления разрыва между современным стандартным арабским языком и сирийским диалектом. Она использует архитектуру AraT5v2-base-1024 и включает две специализированные модели для двунаправленного перевода. Система была обучена на наборе данных Nabra и оценена на корпусе MADAR. Модель перевода с арабского на сирийский диалект достигла высокого среднего балла качества 4.01 из 5.0 по оценке GPT-4.1.'}, 'en': {'title': 'Bridging Dialects: SHAMI-MT Translates Arabic with Precision', 'desc': 'The paper presents SHAMI-MT, a bidirectional machine translation system designed to translate between Modern Standard Arabic (MSA) and the Syrian dialect. Utilizing the AraT5v2-base-1024 architecture, the system includes two specialized models for MSA-to-Shami and Shami-to-MSA translations. These models were fine-tuned on the Nabra dataset and evaluated using the MADAR corpus, achieving a high quality score of 4.01 out of 5.0. This work addresses the challenges of diglossia in Arabic, providing a valuable tool for accurate and culturally relevant translations.'}, 'zh': {'title': '弥合阿拉伯语与叙利亚方言的翻译桥梁', 'desc': '本文介绍了一种双向机器翻译系统SHAMI-MT，旨在弥合现代标准阿拉伯语与叙利亚方言之间的差距。该系统基于先进的AraT5v2-base-1024架构，分别针对现代标准阿拉伯语到叙利亚方言和叙利亚方言到现代标准阿拉伯语的翻译进行了优化。通过在全面的Nabra数据集上进行微调，并在MADAR语料库的未见数据上进行严格评估，模型表现出色。SHAMI-MT为阿拉伯方言翻译领域提供了重要的高保真工具，促进了内容本地化和跨文化交流。'}}}, {'id': 'https://huggingface.co/papers/2508.01109', 'title': 'Platonic Representations for Poverty Mapping: Unified Vision-Language\n  Codes or Agent-Induced Novelty?', 'url': 'https://huggingface.co/papers/2508.01109', 'abstract': 'A multimodal framework using satellite imagery and text data outperforms vision-only models in predicting household wealth, with LLM-generated text proving more effective than agent-retrieved text.  \t\t\t\t\tAI-generated summary \t\t\t\t We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts.', 'score': 1, 'issue_id': 5188, 'pub_date': '2025-08-01', 'pub_date_card': {'ru': '1 августа', 'en': 'August 1', 'zh': '8月1日'}, 'hash': '411bd35601db2ebd', 'authors': ['Satiyabooshan Murugaboopathy', 'Connor T. Jerzak', 'Adel Daoud'], 'affiliations': ['Chalmers & Linköping University', 'Fraunhofer Center', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2508.01109.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#dataset', '#science'], 'emoji': '🛰️', 'ru': {'title': 'Мультимодальный подход превосходит визуальные модели в оценке благосостояния', 'desc': 'Исследование показывает, что комбинация спутниковых снимков и текстовых данных превосходит модели, основанные только на изображениях, в прогнозировании благосостояния домохозяйств. Разработана мультимодальная система, использующая спутниковые снимки Landsat и текстовые описания, сгенерированные большой языковой моделью (LLM). Система включает пять различных подходов, включая модель компьютерного зрения, LLM и ансамбль всех сигналов. Результаты демонстрируют, что объединение визуальной и текстовой информации значительно улучшает точность прогнозирования благосостояния по сравнению с использованием только изображений.'}, 'en': {'title': 'Unlocking Wealth Insights: The Power of Multimodal Learning', 'desc': "This paper presents a multimodal framework that combines satellite imagery and text data to predict household wealth more accurately than models using only visual data. By analyzing Demographic and Health Survey data from African neighborhoods, the authors demonstrate that integrating LLM-generated text with satellite images significantly improves prediction performance. The study reveals that the internal knowledge of large language models (LLMs) is more effective than text retrieved by AI agents, enhancing the model's robustness across different contexts. Additionally, the research contributes a large-scale dataset that links over 60,000 clusters of socio-economic data with corresponding satellite images and textual descriptions."}, 'zh': {'title': '多模态框架提升家庭财富预测精度', 'desc': '本研究探讨了家庭财富等社会经济指标是否可以通过卫星图像和互联网文本数据进行预测。我们开发了一个多模态框架，结合了卫星图像和基于位置/年份的LLM生成文本，以提高财富预测的准确性。研究结果表明，融合视觉和文本信息的模型在财富预测上优于仅使用视觉的模型，且LLM生成的文本效果更佳。我们还发布了一个包含超过60,000个DHS集群的大规模多模态数据集，以支持未来的研究。'}}}, {'id': 'https://huggingface.co/papers/2508.00024', 'title': 'Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine\n  Learning', 'url': 'https://huggingface.co/papers/2508.00024', 'abstract': 'Combining Vision Transformer embeddings with quantum-classical pipelines achieves quantum advantage in classification tasks, demonstrating the importance of embedding choice in quantum machine learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Quantum Support Vector Machines face scalability challenges due to high-dimensional quantum states and hardware limitations. We propose an embedding-aware quantum-classical pipeline combining class-balanced k-means distillation with pretrained Vision Transformer embeddings. Our key finding: ViT embeddings uniquely enable quantum advantage, achieving up to 8.02% accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST, while CNN features show performance degradation. Using 16-qubit tensor network simulation via cuTensorNet, we provide the first systematic evidence that quantum kernel advantage depends critically on embedding choice, revealing fundamental synergy between transformer attention and quantum feature spaces. This provides a practical pathway for scalable quantum machine learning that leverages modern neural architectures.', 'score': 1, 'issue_id': 5185, 'pub_date': '2025-07-28', 'pub_date_card': {'ru': '28 июля', 'en': 'July 28', 'zh': '7月28日'}, 'hash': 'c9825e1a6f4d2a1c', 'authors': ['Sebastián Andrés Cajas Ordóñez', 'Luis Fernando Torres Torres', 'Mario Bifulco', 'Carlos Andrés Durán', 'Cristian Bosch', 'Ricardo Simón Carbajo'], 'affiliations': ['Corporation for Aerospace Initiatives (CASIRI), University of Cauca, Popayán, Colombia', 'Department of Computer Science, University of Torino, Torino, Italy', 'National Irish Centre for AI (CeADAR), University College Dublin (UCD), Dublin, Ireland', 'SISTEMIC Research Group, University of Antioquia, Medellín, Colombia'], 'pdf_title_img': 'assets/pdf/title_img/2508.00024.jpg', 'data': {'categories': ['#cv', '#games', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Квантовое преимущество через синергию трансформеров и квантовых пространств признаков', 'desc': 'Статья представляет новый подход к квантовому машинному обучению, объединяющий эмбеддинги Vision Transformer с квантово-классическими пайплайнами. Авторы демонстрируют, что такой метод позволяет достичь квантового преимущества в задачах классификации, превосходя классические SVM на наборах данных Fashion-MNIST и MNIST. Ключевым открытием является то, что эмбеддинги ViT уникальным образом обеспечивают квантовое преимущество, в то время как признаки CNN показывают снижение производительности. Исследование подчеркивает важность выбора эмбеддингов в квантовом машинном обучении и открывает путь к масштабируемым квантовым алгоритмам, использующим современные нейронные архитектуры.'}, 'en': {'title': 'Unlocking Quantum Advantage with Vision Transformers', 'desc': 'This paper explores how combining Vision Transformer (ViT) embeddings with quantum-classical pipelines can enhance classification tasks in machine learning. It highlights the challenges faced by Quantum Support Vector Machines (QSVMs) due to high-dimensional quantum states and hardware limitations. The authors introduce a method that uses class-balanced k-means distillation alongside pretrained ViT embeddings, which significantly improves accuracy compared to classical SVMs. Their findings suggest that the choice of embedding is crucial for achieving quantum advantage, demonstrating a beneficial interaction between transformer attention mechanisms and quantum feature spaces.'}, 'zh': {'title': '量子机器学习中的嵌入选择与优势', 'desc': '本文探讨了将视觉变换器（Vision Transformer）嵌入与量子-经典管道结合的方式，以在分类任务中实现量子优势。研究表明，嵌入的选择对量子机器学习至关重要，使用ViT嵌入可以在Fashion-MNIST数据集上提高8.02%的准确率，而在MNIST数据集上提高4.42%。相比之下，卷积神经网络（CNN）特征的表现却有所下降。通过使用16量子比特的张量网络模拟，本文首次系统性地证明了量子核优势与嵌入选择之间的关键关系，揭示了变换器注意力与量子特征空间之间的基本协同作用。'}}}, {'id': 'https://huggingface.co/papers/2508.01773', 'title': 'Uncertainty-Based Methods for Automated Process Reward Data Construction\n  and Output Aggregation in Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2508.01773', 'abstract': "An uncertainty-driven framework for automated process reward data construction and aggregation methods improves the effectiveness and efficiency of Process-Level Reward Models in mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.", 'score': 0, 'issue_id': 5186, 'pub_date': '2025-08-03', 'pub_date_card': {'ru': '3 августа', 'en': 'August 3', 'zh': '8月3日'}, 'hash': '872cba646c3b0c3d', 'authors': ['Jiuzhou Han', 'Wray Buntine', 'Ehsan Shareghi'], 'affiliations': ['College of Engineering and Computer Science, VinUniversity', 'Department of Data Science & AI, Monash University'], 'pdf_title_img': 'assets/pdf/title_img/2508.01773.jpg', 'data': {'categories': ['#math', '#training', '#dataset', '#reasoning', '#optimization', '#data'], 'emoji': '🧮', 'ru': {'title': 'Улучшение математических рассуждений ИИ через неопределенность', 'desc': 'Статья представляет новый подход к построению и агрегации данных для обучения моделей вознаграждения на уровне процесса (PRM) в задачах математических рассуждений. Авторы предлагают фреймворк, основанный на неопределенности, для автоматизированного создания данных о вознаграждениях процесса. Также введены два новых метода агрегации выходных данных: гибридное мажоритарное голосование по вознаграждениям и взвешенное частотное голосование по вознаграждениям. Эксперименты на нескольких наборах данных показывают эффективность предложенного подхода в улучшении способностей моделей к математическим рассуждениям.'}, 'en': {'title': 'Automating Reward Data for Smarter Math Reasoning', 'desc': 'This paper presents a new framework that automates the creation of process reward data, which is essential for training Process-Level Reward Models (PRMs) in mathematical reasoning tasks. The authors highlight the challenges of existing data construction methods, which are often time-consuming and inefficient. They introduce two innovative output aggregation techniques, Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, that enhance the performance of PRMs by effectively combining the strengths of traditional voting methods with PRM evaluations. Experimental results demonstrate that this uncertainty-driven approach significantly improves both the quality of the reward data and the reasoning capabilities of the models tested.'}, 'zh': {'title': '基于不确定性的自动化过程奖励数据构建框架', 'desc': '本文提出了一种基于不确定性的框架，用于自动化过程奖励数据的构建和聚合方法，以提高过程级奖励模型在数学推理任务中的有效性和效率。过程级奖励模型（PRMs）通过在每个中间步骤提供监督和评估，显著提升了模型的推理能力。然而，构建高质量的过程奖励数据通常需要耗费大量人力，现有方法效率低下。我们还提出了两种通用的不确定性感知输出聚合方法，进一步增强了PRMs的数学推理能力。'}}}, {'id': 'https://huggingface.co/papers/2507.16290', 'title': 'Dens3R: A Foundation Model for 3D Geometry Prediction', 'url': 'https://huggingface.co/papers/2507.16290', 'abstract': 'Dens3R is a 3D foundation model that jointly predicts multiple geometric quantities using a two-stage training framework, enhancing consistency and performance in dense 3D reconstruction tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in dense 3D reconstruction have led to significant progress, yet achieving accurate unified geometric prediction remains a major challenge. Most existing methods are limited to predicting a single geometry quantity from input images. However, geometric quantities such as depth, surface normals, and point maps are inherently correlated, and estimating them in isolation often fails to ensure consistency, thereby limiting both accuracy and practical applicability. This motivates us to explore a unified framework that explicitly models the structural coupling among different geometric properties to enable joint regression. In this paper, we present Dens3R, a 3D foundation model designed for joint geometric dense prediction and adaptable to a wide range of downstream tasks. Dens3R adopts a two-stage training framework to progressively build a pointmap representation that is both generalizable and intrinsically invariant. Specifically, we design a lightweight shared encoder-decoder backbone and introduce position-interpolated rotary positional encoding to maintain expressive power while enhancing robustness to high-resolution inputs. By integrating image-pair matching features with intrinsic invariance modeling, Dens3R accurately regresses multiple geometric quantities such as surface normals and depth, achieving consistent geometry perception from single-view to multi-view inputs. Additionally, we propose a post-processing pipeline that supports geometrically consistent multi-view inference. Extensive experiments demonstrate the superior performance of Dens3R across various dense 3D prediction tasks and highlight its potential for broader applications.', 'score': 0, 'issue_id': 5189, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': 'd47ef3bd9b4560f6', 'authors': ['Xianze Fang', 'Jingnan Gao', 'Zhe Wang', 'Zhuo Chen', 'Xingyu Ren', 'Jiangjing Lyu', 'Qiaomu Ren', 'Zhonglei Yang', 'Xiaokang Yang', 'Yichao Yan', 'Chengfei Lyu'], 'affiliations': ['Alibaba Group, China', 'Shanghai Jiao Tong University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.16290.jpg', 'data': {'categories': ['#3d'], 'emoji': '🧊', 'ru': {'title': 'Единая модель для точной 3D-реконструкции', 'desc': 'Dens3R - это модель для совместного предсказания нескольких геометрических характеристик в задачах плотной 3D-реконструкции. Она использует двухэтапную схему обучения для повышения согласованности и производительности. Dens3R применяет легковесную архитектуру энкодер-декодер и позиционно-интерполированное роторное позиционное кодирование. Модель способна точно регрессировать множество геометрических величин, таких как нормали поверхности и глубина, обеспечивая согласованное восприятие геометрии как для одного, так и для нескольких ракурсов.'}, 'en': {'title': 'Dens3R: Unified Predictions for Consistent 3D Geometry', 'desc': "Dens3R is a 3D foundation model that improves dense 3D reconstruction by predicting multiple geometric quantities together, such as depth and surface normals. It uses a two-stage training framework to enhance the consistency and accuracy of these predictions, addressing the limitations of existing methods that focus on single geometry predictions. By modeling the relationships between different geometric properties, Dens3R ensures that the predictions are coherent and reliable. The model's design includes a lightweight encoder-decoder and advanced encoding techniques, making it adaptable for various applications in 3D geometry tasks."}, 'zh': {'title': 'Dens3R：联合几何预测的3D基础模型', 'desc': 'Dens3R是一种3D基础模型，旨在通过两阶段训练框架联合预测多个几何量，从而提高密集3D重建任务中的一致性和性能。现有方法通常只能从输入图像中预测单一几何量，而Dens3R则通过建模不同几何属性之间的结构耦合，实现了联合回归。该模型采用轻量级共享编码器-解码器架构，并引入位置插值旋转位置编码，以增强对高分辨率输入的鲁棒性。实验结果表明，Dens3R在多种密集3D预测任务中表现优越，具有广泛的应用潜力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf moonly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (26)', '#agents (56)', '#agi (13)', '#alignment (22)', '#architecture (42)', '#audio (10)', '#benchmark (103)', '#cv (48)', '#data (39)', '#dataset (90)', '#diffusion (30)', '#ethics (13)', '#games (43)', '#graphs (3)', '#hallucinations (14)', '#healthcare (10)', '#inference (16)', '#interpretability (22)', '#leakage (1)', '#long_context (19)', '#low_resource (6)', '#machine_translation (4)', '#math (12)', '#multilingual (15)', '#multimodal (75)', '#open_source (75)', '#optimization (121)', '#plp (4)', '#rag (7)', '#reasoning (68)', '#rl (53)', '#rlhf (23)', '#robotics (10)', '#science (14)', '#security (12)', '#small_models (3)', '#story_generation (3)', '#survey (14)', '#synthetic (16)', '#training (112)', '#transfer_learning (10)', '#video (15)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-08-18 14:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-18 14:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-18 14:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('monthly'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    